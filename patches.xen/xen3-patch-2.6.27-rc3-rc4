From: Linux Kernel Mailing List <linux-kernel@vger.kernel.org>
Subject: Linux: Update to 2.6.27-rc4
Patch-mainline: 2.6.27-rc4

Automatically created from "patches.kernel.org/patch-2.6.27-rc3-rc4" by xen-port-patches.py
Acked-by: jbeulich@novell.com

Index: head-2008-09-15/arch/x86/kernel/acpi/sleep-xen.c
===================================================================
--- head-2008-09-15.orig/arch/x86/kernel/acpi/sleep-xen.c	2008-09-15 14:56:12.000000000 +0200
+++ head-2008-09-15/arch/x86/kernel/acpi/sleep-xen.c	2008-09-01 14:14:45.000000000 +0200
@@ -21,7 +21,7 @@ unsigned long acpi_realmode_flags;
 /* address in low memory of the wakeup routine. */
 static unsigned long acpi_realmode;
 
-#ifdef CONFIG_64BIT
+#if defined(CONFIG_SMP) && defined(CONFIG_64BIT)
 static char temp_stack[10240];
 #endif
 #endif
@@ -89,7 +89,7 @@ int acpi_save_state_mem(void)
 #endif /* !CONFIG_64BIT */
 
 	header->pmode_cr0 = read_cr0();
-	header->pmode_cr4 = read_cr4();
+	header->pmode_cr4 = read_cr4_safe();
 	header->realmode_flags = acpi_realmode_flags;
 	header->real_magic = 0x12345678;
 
Index: head-2008-09-15/arch/x86/kernel/head64-xen.c
===================================================================
--- head-2008-09-15.orig/arch/x86/kernel/head64-xen.c	2008-09-15 14:56:12.000000000 +0200
+++ head-2008-09-15/arch/x86/kernel/head64-xen.c	2008-09-01 14:14:45.000000000 +0200
@@ -110,6 +110,7 @@ void __init x86_64_start_kernel(char * r
 	BUILD_BUG_ON(!(MODULES_VADDR > __START_KERNEL));
 	BUILD_BUG_ON(!(((MODULES_END - 1) & PGDIR_MASK) ==
 				(__START_KERNEL & PGDIR_MASK)));
+	BUILD_BUG_ON(__fix_to_virt(__end_of_fixed_addresses) <= MODULES_END);
 
 	xen_setup_features();
 
Index: head-2008-09-15/arch/x86/kernel/mpparse-xen.c
===================================================================
--- head-2008-09-15.orig/arch/x86/kernel/mpparse-xen.c	2008-09-15 14:56:12.000000000 +0200
+++ head-2008-09-15/arch/x86/kernel/mpparse-xen.c	2008-09-03 10:38:58.000000000 +0200
@@ -50,7 +50,7 @@ static int __init mpf_checksum(unsigned 
 }
 
 #ifndef CONFIG_XEN
-static void __cpuinit MP_processor_info(struct mpc_config_processor *m)
+static void __init MP_processor_info(struct mpc_config_processor *m)
 {
 	int apicid;
 	char *bootup_cpu = "";
@@ -74,7 +74,7 @@ static void __cpuinit MP_processor_info(
 	generic_processor_info(apicid, m->mpc_apicver);
 }
 #else
-static void __cpuinit MP_processor_info(struct mpc_config_processor *m)
+static void __init MP_processor_info(struct mpc_config_processor *m)
 {
 	num_processors++;
 }
@@ -491,7 +491,7 @@ static void __init construct_default_ioi
 }
 
 
-static void construct_ioapic_table(int mpc_default_type)
+static void __init construct_ioapic_table(int mpc_default_type)
 {
 	struct mpc_config_ioapic ioapic;
 	struct mpc_config_bus bus;
@@ -536,7 +536,7 @@ static void construct_ioapic_table(int m
 	construct_default_ioirq_mptable(mpc_default_type);
 }
 #else
-static inline void construct_ioapic_table(int mpc_default_type) { }
+static inline void __init construct_ioapic_table(int mpc_default_type) { }
 #endif
 
 static inline void __init construct_default_ISA_mptable(int mpc_default_type)
Index: head-2008-09-15/arch/x86/kernel/setup-xen.c
===================================================================
--- head-2008-09-15.orig/arch/x86/kernel/setup-xen.c	2008-09-23 17:26:42.000000000 +0200
+++ head-2008-09-15/arch/x86/kernel/setup-xen.c	2008-09-01 14:14:45.000000000 +0200
@@ -508,7 +508,7 @@ static void __init reserve_early_setup_d
  * @size: Size of the crashkernel memory to reserve.
  * Returns the base address on success, and -1ULL on failure.
  */
-unsigned long long find_and_reserve_crashkernel(unsigned long long size)
+unsigned long long __init find_and_reserve_crashkernel(unsigned long long size)
 {
 	const unsigned long long alignment = 16<<20; 	/* 16M */
 	unsigned long long start = 0LL;
Index: head-2008-09-15/arch/x86/kernel/traps_64-xen.c
===================================================================
--- head-2008-09-15.orig/arch/x86/kernel/traps_64-xen.c	2008-09-15 14:56:12.000000000 +0200
+++ head-2008-09-15/arch/x86/kernel/traps_64-xen.c	2008-09-01 14:14:45.000000000 +0200
@@ -1135,7 +1135,14 @@ asmlinkage void math_state_restore(void)
 
         /* clts(); */ /* 'clts' is done for us by Xen during virtual trap. */
 
-	restore_fpu_checking(&me->thread.xstate->fxsave);
+	/*
+	 * Paranoid restore. send a SIGSEGV if we fail to restore the state.
+	 */
+	if (unlikely(restore_fpu_checking(&me->thread.xstate->fxsave))) {
+		stts();
+		force_sig(SIGSEGV, me);
+		return;
+	}
 	task_thread_info(me)->status |= TS_USEDFPU;
 	me->fpu_counter++;
 }
Index: head-2008-09-15/arch/x86/mm/init_64-xen.c
===================================================================
--- head-2008-09-15.orig/arch/x86/mm/init_64-xen.c	2008-09-24 13:55:10.000000000 +0200
+++ head-2008-09-15/arch/x86/mm/init_64-xen.c	2008-09-24 13:58:48.000000000 +0200
@@ -75,7 +75,7 @@ DEFINE_PER_CPU(struct mmu_gather, mmu_ga
 extern pmd_t level2_fixmap_pgt[PTRS_PER_PMD];
 extern pte_t level1_fixmap_pgt[PTRS_PER_PTE];
 
-int direct_gbpages __meminitdata
+int direct_gbpages
 #ifdef CONFIG_DIRECT_GBPAGES
 				= 1
 #endif
@@ -159,7 +159,11 @@ static unsigned long __meminitdata table
 static unsigned long __meminitdata table_cur;
 static unsigned long __meminitdata table_end;
 
-static __init void *spp_getpage(void)
+/*
+ * NOTE: This function is marked __ref because it calls __init function
+ * (alloc_bootmem_pages). It's safe to do it ONLY when after_bootmem == 0.
+ */
+static __ref void *spp_getpage(void)
 {
 	void *ptr;
 
@@ -417,6 +421,7 @@ phys_pmd_init(pmd_t *pmd_page, unsigned 
 {
 	unsigned long pages = 0;
 	unsigned long last_map_addr = end;
+	unsigned long start = address;
 
 	int i = pmd_index(address);
 
@@ -432,6 +437,9 @@ phys_pmd_init(pmd_t *pmd_page, unsigned 
 			if (!pmd_large(*pmd))
 				last_map_addr = phys_pte_update(pmd, address,
 								 end);
+			/* Count entries we're using from level2_ident_pgt */
+			if (start == 0)
+				pages++;
 			continue;
 		}
 
Index: head-2008-09-15/arch/x86/mm/ioremap-xen.c
===================================================================
--- head-2008-09-15.orig/arch/x86/mm/ioremap-xen.c	2008-09-15 14:56:12.000000000 +0200
+++ head-2008-09-15/arch/x86/mm/ioremap-xen.c	2008-09-01 14:14:45.000000000 +0200
@@ -328,7 +328,7 @@ static void __iomem *__ioremap_caller(re
 	phys_addr &= PAGE_MASK;
 	size = PAGE_ALIGN(last_addr+1) - phys_addr;
 
-	retval = reserve_memtype(phys_addr, phys_addr + size,
+	retval = reserve_memtype(phys_addr, (u64)phys_addr + size,
 						prot_val, &new_prot_val);
 	if (retval) {
 		pr_debug("Warning: reserve_memtype returned %d\n", retval);
Index: head-2008-09-15/arch/x86/mm/pageattr-xen.c
===================================================================
--- head-2008-09-15.orig/arch/x86/mm/pageattr-xen.c	2008-09-15 14:56:12.000000000 +0200
+++ head-2008-09-15/arch/x86/mm/pageattr-xen.c	2008-09-01 14:14:45.000000000 +0200
@@ -55,13 +55,19 @@ static void split_page_count(int level)
 
 int arch_report_meminfo(char *page)
 {
-	int n = sprintf(page, "DirectMap4k:  %8lu\n"
-			"DirectMap2M:  %8lu\n",
-			direct_pages_count[PG_LEVEL_4K],
-			direct_pages_count[PG_LEVEL_2M]);
+	int n = sprintf(page, "DirectMap4k:  %8lu kB\n",
+			direct_pages_count[PG_LEVEL_4K] << 2);
+#if defined(CONFIG_X86_64) || defined(CONFIG_X86_PAE)
+	n += sprintf(page + n, "DirectMap2M:  %8lu kB\n",
+			direct_pages_count[PG_LEVEL_2M] << 11);
+#else
+	n += sprintf(page + n, "DirectMap4M:  %8lu kB\n",
+			direct_pages_count[PG_LEVEL_2M] << 12);
+#endif
 #ifdef CONFIG_X86_64
-	n += sprintf(page + n, "DirectMap1G:  %8lu\n",
-		     direct_pages_count[PG_LEVEL_1G]);
+	if (direct_gbpages)
+		n += sprintf(page + n, "DirectMap1G:  %8lu kB\n",
+			direct_pages_count[PG_LEVEL_1G] << 20);
 #endif
 	return n;
 }
@@ -611,10 +617,9 @@ repeat:
 	if (!__pte_val(old_pte)) {
 		if (!primary)
 			return 0;
-		printk(KERN_WARNING "CPA: called for zero pte. "
+		WARN(1, KERN_WARNING "CPA: called for zero pte. "
 		       "vaddr = %lx cpa->vaddr = %lx\n", address,
 		       cpa->vaddr);
-		WARN_ON(1);
 		return -EINVAL;
 	}
 
Index: head-2008-09-15/arch/x86/pci/irq-xen.c
===================================================================
--- head-2008-09-15.orig/arch/x86/pci/irq-xen.c	2008-09-15 14:56:12.000000000 +0200
+++ head-2008-09-15/arch/x86/pci/irq-xen.c	2008-09-01 14:14:45.000000000 +0200
@@ -596,6 +596,8 @@ static __init int intel_router_probe(str
 	case PCI_DEVICE_ID_INTEL_ICH10_1:
 	case PCI_DEVICE_ID_INTEL_ICH10_2:
 	case PCI_DEVICE_ID_INTEL_ICH10_3:
+	case PCI_DEVICE_ID_INTEL_PCH_0:
+	case PCI_DEVICE_ID_INTEL_PCH_1:
 		r->name = "PIIX/ICH";
 		r->get = pirq_piix_get;
 		r->set = pirq_piix_set;
Index: head-2008-09-15/include/asm-x86/mach-xen/asm/io.h
===================================================================
--- head-2008-09-15.orig/include/asm-x86/mach-xen/asm/io.h	2008-09-15 14:56:12.000000000 +0200
+++ head-2008-09-15/include/asm-x86/mach-xen/asm/io.h	2008-09-01 14:14:45.000000000 +0200
@@ -21,7 +21,7 @@ extern void __iomem *fix_ioremap(unsigne
 
 #define build_mmio_read(name, size, type, reg, barrier) \
 static inline type name(const volatile void __iomem *addr) \
-{ type ret; asm volatile("mov" size " %1,%0":"=" reg (ret) \
+{ type ret; asm volatile("mov" size " %1,%0":reg (ret) \
 :"m" (*(volatile type __force *)addr) barrier); return ret; }
 
 #define build_mmio_write(name, size, type, reg, barrier) \
@@ -29,13 +29,13 @@ static inline void name(type val, volati
 { asm volatile("mov" size " %0,%1": :reg (val), \
 "m" (*(volatile type __force *)addr) barrier); }
 
-build_mmio_read(readb, "b", unsigned char, "q", :"memory")
-build_mmio_read(readw, "w", unsigned short, "r", :"memory")
-build_mmio_read(readl, "l", unsigned int, "r", :"memory")
-
-build_mmio_read(__readb, "b", unsigned char, "q", )
-build_mmio_read(__readw, "w", unsigned short, "r", )
-build_mmio_read(__readl, "l", unsigned int, "r", )
+build_mmio_read(readb, "b", unsigned char, "=q", :"memory")
+build_mmio_read(readw, "w", unsigned short, "=r", :"memory")
+build_mmio_read(readl, "l", unsigned int, "=r", :"memory")
+
+build_mmio_read(__readb, "b", unsigned char, "=q", )
+build_mmio_read(__readw, "w", unsigned short, "=r", )
+build_mmio_read(__readl, "l", unsigned int, "=r", )
 
 build_mmio_write(writeb, "b", unsigned char, "q", :"memory")
 build_mmio_write(writew, "w", unsigned short, "r", :"memory")
@@ -59,8 +59,8 @@ build_mmio_write(__writel, "l", unsigned
 #define mmiowb() barrier()
 
 #ifdef CONFIG_X86_64
-build_mmio_read(readq, "q", unsigned long, "r", :"memory")
-build_mmio_read(__readq, "q", unsigned long, "r", )
+build_mmio_read(readq, "q", unsigned long, "=r", :"memory")
+build_mmio_read(__readq, "q", unsigned long, "=r", )
 build_mmio_write(writeq, "q", unsigned long, "r", :"memory")
 build_mmio_write(__writeq, "q", unsigned long, "r", )
 
Index: head-2008-09-15/include/asm-x86/mach-xen/asm/pgtable_64.h
===================================================================
--- head-2008-09-15.orig/include/asm-x86/mach-xen/asm/pgtable_64.h	2008-09-15 14:56:12.000000000 +0200
+++ head-2008-09-15/include/asm-x86/mach-xen/asm/pgtable_64.h	2008-09-01 14:14:45.000000000 +0200
@@ -156,7 +156,7 @@ static inline void xen_pgd_clear(pgd_t *
 #define VMALLOC_END      _AC(0xffffe1ffffffffff, UL)
 #define VMEMMAP_START	 _AC(0xffffe20000000000, UL)
 #define MODULES_VADDR    _AC(0xffffffffa0000000, UL)
-#define MODULES_END      _AC(0xfffffffffff00000, UL)
+#define MODULES_END      _AC(0xffffffffff000000, UL)
 #define MODULES_LEN   (MODULES_END - MODULES_VADDR)
 
 #ifndef __ASSEMBLY__
Index: head-2008-09-15/include/asm-x86/mach-xen/asm/processor.h
===================================================================
--- head-2008-09-15.orig/include/asm-x86/mach-xen/asm/processor.h	2008-09-15 14:56:12.000000000 +0200
+++ head-2008-09-15/include/asm-x86/mach-xen/asm/processor.h	2008-09-01 14:14:45.000000000 +0200
@@ -670,6 +670,31 @@ extern unsigned long		boot_option_idle_o
 extern unsigned long		idle_halt;
 extern unsigned long		idle_nomwait;
 
+#ifndef CONFIG_XEN
+/*
+ * on systems with caches, caches must be flashed as the absolute
+ * last instruction before going into a suspended halt.  Otherwise,
+ * dirty data can linger in the cache and become stale on resume,
+ * leading to strange errors.
+ *
+ * perform a variety of operations to guarantee that the compiler
+ * will not reorder instructions.  wbinvd itself is serializing
+ * so the processor will not reorder.
+ *
+ * Systems without cache can just go into halt.
+ */
+static inline void wbinvd_halt(void)
+{
+	mb();
+	/* check for clflush to determine if wbinvd is legal */
+	if (cpu_has_clflush)
+		asm volatile("cli; wbinvd; 1: hlt; jmp 1b" : : : "memory");
+	else
+		while (1)
+			halt();
+}
+#endif
+
 extern void enable_sep_cpu(void);
 extern int sysenter_setup(void);
 
