From: jbeulich@novell.com
Subject: Go into polling mode early if lock owner is not running
Patch-mainline: n/a

This could be merged into the original ticket spinlock code once
validated, if there wasn't the dependency on smp-processor-id.h, which
only gets introduced in the 2.6.32 merge.

--- head-2011-12-21.orig/arch/x86/include/mach-xen/asm/spinlock.h	2011-11-21 14:16:04.000000000 +0100
+++ head-2011-12-21/arch/x86/include/mach-xen/asm/spinlock.h	2012-01-04 16:55:11.000000000 +0100
@@ -44,11 +44,12 @@
 #ifdef TICKET_SHIFT
 
 #include <asm/irqflags.h>
+#include <asm/smp-processor-id.h>
 
 int xen_spinlock_init(unsigned int cpu);
 void xen_spinlock_cleanup(unsigned int cpu);
-bool xen_spin_wait(arch_spinlock_t *, struct __raw_tickets *,
-		   unsigned int flags);
+unsigned int xen_spin_wait(arch_spinlock_t *, struct __raw_tickets *,
+			   unsigned int flags);
 struct __raw_tickets xen_spin_adjust(const arch_spinlock_t *,
 				     struct __raw_tickets);
 void xen_spin_kick(const arch_spinlock_t *, __ticket_t);
@@ -66,27 +67,31 @@ void xen_spin_kick(const arch_spinlock_t
  * in the high part, because a wide xadd increment of the low part would carry
  * up and contaminate the high part.
  */
+#define __spin_count_dec(c, l) (vcpu_running((l)->owner) ? --(c) : ((c) >>= 1))
+
 static __always_inline void __ticket_spin_lock(arch_spinlock_t *lock)
 {
 	struct __raw_tickets inc = { .tail = 1 };
 	unsigned int count, flags = arch_local_irq_save();
 
 	inc = xadd(&lock->tickets, inc);
-	if (likely(inc.head == inc.tail)) {
+	if (likely(inc.head == inc.tail))
+		arch_local_irq_restore(flags);
+	else {
+		inc = xen_spin_adjust(lock, inc);
 		arch_local_irq_restore(flags);
-		return;
-	}
-	inc = xen_spin_adjust(lock, inc);
-	arch_local_irq_restore(flags);
-
-	do {
 		count = 1 << 12;
-		while (inc.head != inc.tail && --count) {
-			cpu_relax();
-			inc.head = ACCESS_ONCE(lock->tickets.head);
-		}
-	} while (unlikely(!count) && !xen_spin_wait(lock, &inc, flags));
+		do {
+			while (inc.head != inc.tail
+			       && __spin_count_dec(count, lock)) {
+				cpu_relax();
+				inc.head = ACCESS_ONCE(lock->tickets.head);
+			}
+		} while (unlikely(!count)
+			 && (count = xen_spin_wait(lock, &inc, flags)));
+	}
 	barrier();		/* make sure nothing creeps before the lock is taken */
+	lock->owner = raw_smp_processor_id();
 }
 
 static __always_inline void __ticket_spin_lock_flags(arch_spinlock_t *lock,
@@ -96,20 +101,24 @@ static __always_inline void __ticket_spi
 	unsigned int count;
 
 	inc = xadd(&lock->tickets, inc);
-	if (likely(inc.head == inc.tail))
-		return;
-	inc = xen_spin_adjust(lock, inc);
-
-	do {
+	if (unlikely(inc.head != inc.tail)) {
+		inc = xen_spin_adjust(lock, inc);
 		count = 1 << 12;
-		while (inc.head != inc.tail && --count) {
-			cpu_relax();
-			inc.head = ACCESS_ONCE(lock->tickets.head);
-		}
-	} while (unlikely(!count) && !xen_spin_wait(lock, &inc, flags));
+		do {
+			while (inc.head != inc.tail
+			       && __spin_count_dec(count, lock)) {
+				cpu_relax();
+				inc.head = ACCESS_ONCE(lock->tickets.head);
+			}
+		} while (unlikely(!count)
+			 && (count = xen_spin_wait(lock, &inc, flags)));
+	}
 	barrier();		/* make sure nothing creeps before the lock is taken */
+	lock->owner = raw_smp_processor_id();
 }
 
+#undef __spin_count_dec
+
 static __always_inline int __ticket_spin_trylock(arch_spinlock_t *lock)
 {
 	arch_spinlock_t old, new;
@@ -121,7 +130,10 @@ static __always_inline int __ticket_spin
 	new.head_tail = old.head_tail + (1 << TICKET_SHIFT);
 
 	/* cmpxchg is a full barrier, so nothing can move before it */
-	return cmpxchg(&lock->head_tail, old.head_tail, new.head_tail) == old.head_tail;
+	if (cmpxchg(&lock->head_tail, old.head_tail, new.head_tail) != old.head_tail)
+		return 0;
+	lock->owner = raw_smp_processor_id();
+	return 1;
 }
 
 static __always_inline void __ticket_spin_unlock(arch_spinlock_t *lock)
--- head-2011-12-21.orig/arch/x86/include/mach-xen/asm/spinlock_types.h	2011-11-21 14:18:48.000000000 +0100
+++ head-2011-12-21/arch/x86/include/mach-xen/asm/spinlock_types.h	2011-11-21 14:20:12.000000000 +0100
@@ -30,9 +30,16 @@ typedef u32 __ticketpair_t;
 
 typedef union {
 	__ticketpair_t head_tail;
-	struct __raw_tickets {
-		__ticket_t head, tail;
-	} tickets;
+	struct {
+		struct __raw_tickets {
+			__ticket_t head, tail;
+		} tickets;
+#if CONFIG_NR_CPUS <= 256
+		u8 owner;
+#else
+		u16 owner;
+#endif
+	};
 #else /* CONFIG_XEN_COMPAT < 0x030200 */
 typedef struct {
 /*
--- head-2011-12-21.orig/drivers/xen/core/spinlock.c	2011-11-18 16:11:35.000000000 +0100
+++ head-2011-12-21/drivers/xen/core/spinlock.c	2012-01-04 17:23:09.000000000 +0100
@@ -39,6 +39,8 @@ int __cpuinit xen_spinlock_init(unsigned
 	struct evtchn_bind_ipi bind_ipi;
 	int rc;
 
+	setup_runstate_area(cpu);
+
  	WARN_ON(per_cpu(poll_evtchn, cpu));
 	bind_ipi.vcpu = cpu;
 	rc = HYPERVISOR_event_channel_op(EVTCHNOP_bind_ipi, &bind_ipi);
@@ -112,18 +114,17 @@ struct __raw_tickets xen_spin_adjust(con
 	return token;
 }
 
-bool xen_spin_wait(arch_spinlock_t *lock, struct __raw_tickets *ptok,
-		   unsigned int flags)
+unsigned int xen_spin_wait(arch_spinlock_t *lock, struct __raw_tickets *ptok,
+			   unsigned int flags)
 {
+	unsigned int rm_idx, cpu = raw_smp_processor_id();
 	bool rc;
 	typeof(vcpu_info(0)->evtchn_upcall_mask) upcall_mask;
-	unsigned int rm_idx;
 	struct spinning spinning, *other;
 
 	/* If kicker interrupt not initialized yet, just spin. */
-	if (unlikely(!cpu_online(raw_smp_processor_id()))
-	    || unlikely(!percpu_read(poll_evtchn)))
-		return false;
+	if (unlikely(!cpu_online(cpu)) || unlikely(!percpu_read(poll_evtchn)))
+		return UINT_MAX;
 
 	/* announce we're spinning */
 	spinning.ticket = ptok->tail;
@@ -143,6 +144,7 @@ bool xen_spin_wait(arch_spinlock_t *lock
 		 * we weren't looking.
 		 */
 		if (lock->tickets.head == spinning.ticket) {
+			lock->owner = cpu;
 			/*
 			 * If we interrupted another spinlock while it was
 			 * blocking, make sure it doesn't block (again)
@@ -175,6 +177,7 @@ bool xen_spin_wait(arch_spinlock_t *lock
 			while (lk->tickets.head == other->ticket) {
 				struct __raw_tickets token;
 
+				lk->owner = cpu;
 				other->ticket = -1;
 				asm volatile(UNLOCK_LOCK_PREFIX
 					     "inc" UNLOCK_SUFFIX(0) " %0"
@@ -238,6 +241,8 @@ bool xen_spin_wait(arch_spinlock_t *lock
 					? token.tail
 					: spin_adjust(other->prev, lk,
 						      token.tail);
+			if (lk->tickets.head == other->ticket)
+				lk->owner = cpu;
 		} while ((other = other->prev) != NULL);
 	}
 
@@ -248,7 +253,7 @@ bool xen_spin_wait(arch_spinlock_t *lock
 	ptok->head = lock->tickets.head;
 	ptok->tail = spinning.ticket;
 
-	return rc;
+	return rc ? 0 : 1 << 10;
 }
 
 void xen_spin_kick(const arch_spinlock_t *lock, __ticket_t ticket)
