From: jbeulich@novell.com
Subject: Go into polling mode early if lock owner is not running
Patch-mainline: n/a

This could be merged into the original ticket spinlock code once
validated, if there wasn't the dependency on smp-processor-id.h, which
only gets introduced in the 2.6.32 merge.

--- head-2010-01-19.orig/arch/x86/include/mach-xen/asm/spinlock.h	2010-01-22 14:38:13.000000000 +0100
+++ head-2010-01-19/arch/x86/include/mach-xen/asm/spinlock.h	2009-11-23 10:58:32.000000000 +0100
@@ -40,6 +40,11 @@
 
 #ifdef TICKET_SHIFT
 
+#include <asm/smp-processor-id.h>
+#include <xen/interface/vcpu.h>
+
+DECLARE_PER_CPU(struct vcpu_runstate_info, runstate);
+
 int xen_spinlock_init(unsigned int cpu);
 void xen_spinlock_cleanup(unsigned int cpu);
 int xen_spin_wait(arch_spinlock_t *, unsigned int token);
@@ -105,6 +110,9 @@ static __always_inline int __ticket_spin
 	    :
 	    : "memory", "cc");
 
+	if (tmp)
+		lock->owner = raw_smp_processor_id();
+
 	return tmp;
 }
 
@@ -174,6 +182,9 @@ static __always_inline int __ticket_spin
 	    :
 	    : "memory", "cc");
 
+	if (tmp)
+		lock->owner = raw_smp_processor_id();
+
 	return tmp;
 }
 
@@ -195,6 +206,10 @@ static __always_inline void __ticket_spi
 }
 #endif
 
+#define __ticket_spin_count(lock) \
+	(per_cpu(runstate.state, (lock)->owner) == RUNSTATE_running \
+	 ? 1 << 10 : 1)
+
 static inline int __ticket_spin_is_locked(arch_spinlock_t *lock)
 {
 	int tmp = ACCESS_ONCE(lock->slock);
@@ -219,9 +234,10 @@ static __always_inline void __ticket_spi
 		return;
 	token = xen_spin_adjust(lock, token);
 	do {
-		count = 1 << 10;
+		count = __ticket_spin_count(lock);
 		__ticket_spin_lock_body;
 	} while (unlikely(!count) && !xen_spin_wait(lock, token));
+	lock->owner = raw_smp_processor_id();
 }
 
 static __always_inline void __ticket_spin_lock_flags(arch_spinlock_t *lock,
@@ -235,13 +251,15 @@ static __always_inline void __ticket_spi
 		return;
 	token = xen_spin_adjust(lock, token);
 	do {
-		count = 1 << 10;
+		count = __ticket_spin_count(lock);
 		__ticket_spin_lock_body;
 	} while (unlikely(!count) && !xen_spin_wait_flags(lock, &token, flags));
+	lock->owner = raw_smp_processor_id();
 }
 
 #undef __ticket_spin_lock_preamble
 #undef __ticket_spin_lock_body
+#undef __ticket_spin_count
 
 #define __arch_spin(n) __ticket_spin_##n
 
--- head-2010-01-19.orig/arch/x86/include/mach-xen/asm/spinlock_types.h	2010-01-26 11:24:00.000000000 +0100
+++ head-2010-01-19/arch/x86/include/mach-xen/asm/spinlock_types.h	2010-01-26 11:27:24.000000000 +0100
@@ -26,6 +26,11 @@ typedef union {
 # define TICKET_SHIFT 16
 		u16 cur, seq;
 #endif
+#if CONFIG_NR_CPUS <= 256
+		u8 owner;
+#else
+		u16 owner;
+#endif
 #else
 /*
  * This differs from the pre-2.6.24 spinlock by always using xchgb
--- head-2010-01-19.orig/arch/x86/kernel/time-xen.c	2009-11-23 10:49:09.000000000 +0100
+++ head-2010-01-19/arch/x86/kernel/time-xen.c	2009-11-23 10:58:42.000000000 +0100
@@ -65,7 +65,7 @@ static DEFINE_PER_CPU(u64, processed_sto
 static DEFINE_PER_CPU(u64, processed_blocked_time);
 
 /* Current runstate of each CPU (updated automatically by the hypervisor). */
-static DEFINE_PER_CPU(struct vcpu_runstate_info, runstate);
+DEFINE_PER_CPU(struct vcpu_runstate_info, runstate);
 
 /* Must be signed, as it's compared with s64 quantities which can be -ve. */
 #define NS_PER_TICK (1000000000LL/HZ)
--- head-2010-01-19.orig/drivers/xen/core/spinlock.c	2009-11-06 11:10:22.000000000 +0100
+++ head-2010-01-19/drivers/xen/core/spinlock.c	2010-01-26 08:39:49.000000000 +0100
@@ -38,6 +38,8 @@ int __cpuinit xen_spinlock_init(unsigned
 	};
 	int rc;
 
+	setup_runstate_area(cpu);
+
 	rc = bind_ipi_to_irqaction(SPIN_UNLOCK_VECTOR,
 				   cpu,
 				   &spinlock_action);
