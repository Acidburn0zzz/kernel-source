Subject: patch-2.6.13-rc5
From: torvalds@osdl.org


Signed-off-by: Olaf Hering <olh@suse.de>

Automatically created from "patches.fixes/patch-2.6.13-rc4-rc5" by xen-port-patches.py
Further porting to arch/xen by jbeulich

Index: xen-2005-08-20/arch/xen/i386/kernel/mpparse.c
===================================================================
--- xen-2005-08-20.orig/arch/xen/i386/kernel/mpparse.c
+++ xen-2005-08-20/arch/xen/i386/kernel/mpparse.c
@@ -1130,9 +1130,17 @@ int mp_register_gsi (u32 gsi, int edge_l
 		 * due to unused I/O APIC pins.
 		 */
 		int irq = gsi;
 		if (gsi < MAX_GSI_NUM) {
-			gsi = pci_irq++;
+			if (gsi > 15)
+				gsi = pci_irq++;
+#ifdef CONFIG_ACPI_BUS
+			/*
+			 * Don't assign IRQ used by ACPI SCI
+			 */
+			if (gsi == acpi_fadt.sci_int)
+				gsi = pci_irq++;
+#endif
 			gsi_to_irq[irq] = gsi;
 		} else {
 			printk(KERN_ERR "GSI %u is too high\n", gsi);
 			return gsi;
Index: xen-2005-08-20/arch/xen/i386/pci/irq.c
===================================================================
--- xen-2005-08-20.orig/arch/xen/i386/pci/irq.c
+++ xen-2005-08-20/arch/xen/i386/pci/irq.c
@@ -55,8 +55,9 @@ struct irq_router_handler {
 	int (*probe)(struct irq_router *r, struct pci_dev *router, u16 device);
 };
 
 int (*pcibios_enable_irq)(struct pci_dev *dev) = NULL;
+void (*pcibios_disable_irq)(struct pci_dev *dev) = NULL;
 
 #ifdef CONFIG_XEN_PRIVILEGED_GUEST
 /*
  *  Check passed address for the PCI IRQ Routing Table signature
Index: xen-2005-08-20/arch/xen/x86_64/Kconfig
===================================================================
--- xen-2005-08-20.orig/arch/xen/x86_64/Kconfig
+++ xen-2005-08-20/arch/xen/x86_64/Kconfig
@@ -329,14 +329,17 @@ config HPET_EMULATE_RTC
 	depends on HPET_TIMER && RTC=y
 
 config GART_IOMMU
 	bool "IOMMU support"
+	default y
 	depends on PCI
 	help
-	  Support the K8 IOMMU. Needed to run systems with more than 4GB of memory
+	  Support the IOMMU. Needed to run systems with more than 3GB of memory
 	  properly with 32-bit PCI devices that do not support DAC (Double Address
 	  Cycle). The IOMMU can be turned off at runtime with the iommu=off parameter.
 	  Normally the kernel will take the right choice by itself.
+	  This option includes a driver for the AMD Opteron/Athlon64 IOMMU
+	  and a software emulation used on some other systems.
 	  If unsure, say Y.
 
 # need this always enabled with GART_IOMMU for the VIA workaround
 config SWIOTLB
Index: xen-2005-08-20/arch/xen/x86_64/ia32/syscall32.c
===================================================================
--- xen-2005-08-20.orig/arch/xen/x86_64/ia32/syscall32.c
+++ xen-2005-08-20/arch/xen/x86_64/ia32/syscall32.c
@@ -13,36 +13,13 @@
 #include <asm/proto.h>
 #include <asm/tlbflush.h>
 #include <asm/ia32_unistd.h>
 
-#define USE_INT80
+#include "syscall32_syscall.S"
 
 #ifdef USE_INT80
-/* 32bit VDSOs mapped into user space. */ 
-asm(".section \".init.data\",\"aw\"\n"
-    "syscall32_int80:\n"
-    ".incbin \"arch/xen/x86_64/ia32/vsyscall-int80.so\"\n"
-    "syscall32_int80_end:\n"
-    "syscall32_syscall:\n"
-    ".incbin \"arch/xen/x86_64/ia32/vsyscall-syscall.so\"\n"
-    "syscall32_syscall_end:\n"
-    "syscall32_sysenter:\n"
-    ".incbin \"arch/xen/x86_64/ia32/vsyscall-sysenter.so\"\n"
-    "syscall32_sysenter_end:\n"
-    ".previous");
-
 extern unsigned char syscall32_int80[], syscall32_int80_end[];
 #else
-/* 32bit VDSOs mapped into user space. */ 
-asm(".section \".init.data\",\"aw\"\n"
-    "syscall32_syscall:\n"
-    ".incbin \"arch/xen/x86_64/ia32/vsyscall-syscall.so\"\n"
-    "syscall32_syscall_end:\n"
-    "syscall32_sysenter:\n"
-    ".incbin \"arch/xen/x86_64/ia32/vsyscall-sysenter.so\"\n"
-    "syscall32_sysenter_end:\n"
-    ".previous");
-
 static int use_sysenter = -1;
 #endif
 
 extern unsigned char syscall32_syscall[], syscall32_syscall_end[];
Index: xen-2005-08-20/arch/xen/x86_64/kernel/entry.S
===================================================================
--- xen-2005-08-20.orig/arch/xen/x86_64/kernel/entry.S
+++ xen-2005-08-20/arch/xen/x86_64/kernel/entry.S
@@ -76,9 +76,9 @@
 	.endm
 
 	.macro FAKE_STACK_FRAME child_rip
 	/* push in order ss, rsp, eflags, cs, rip */
-	xorq %rax, %rax
+	xorl %eax, %eax
 	pushq %rax /* ss */
 	CFI_ADJUST_CFA_OFFSET	8
 	pushq %rax /* rsp */
 	CFI_ADJUST_CFA_OFFSET	8
@@ -451,9 +451,9 @@ ENTRY(stub_rt_sigreturn)
 	testl $3,CS(%rdi)
 	je 1f
 	swapgs	
 #endif        
-1:	addl $1,%gs:pda_irqcount	# RED-PEN should check preempt count
+1:	incl %gs:pda_irqcount	# RED-PEN should check preempt count
 	movq %gs:pda_irqstackptr,%rax
 	cmoveq %rax,%rsp							
 	pushq %rdi			# save old stack	
 	call \func
@@ -500,9 +500,9 @@ retint_signal:
 	jz    retint_restore_args
         XEN_UNBLOCK_EVENTS(%rsi)
 	SAVE_REST
 	movq $-1,ORIG_RAX(%rsp) 			
-	xorq %rsi,%rsi		# oldset
+	xorl %esi,%esi		# oldset
 	movq %rsp,%rdi		# &pt_regs
 	call do_notify_resume
 	RESTORE_REST
         XEN_BLOCK_EVENTS(%rsi)		
@@ -840,9 +840,9 @@ child_rip:
 	movq %rdi, %rax
 	movq %rsi, %rdi
 	call *%rax
 	# exit
-	xorq %rdi, %rdi
+	xorl %edi, %edi
 	call do_exit
 
 /*
  * execve(). This function needs to use IRET, not SYSRET, to set up all state properly.
@@ -1057,4 +1057,16 @@ ENTRY(machine_check)
 ENTRY(call_debug)
        zeroentry do_call_debug
 
 
+ENTRY(call_softirq)
+	movq %gs:pda_irqstackptr,%rax
+	pushq %r15
+	movq %rsp,%r15
+	incl %gs:pda_irqcount
+	cmoveq %rax,%rsp
+	call __do_softirq
+	movq %r15,%rsp
+	decl %gs:pda_irqcount
+	popq %r15
+	ret
+
Index: xen-2005-08-20/arch/xen/x86_64/kernel/genapic.c
===================================================================
--- xen-2005-08-20.orig/arch/xen/x86_64/kernel/genapic.c
+++ xen-2005-08-20/arch/xen/x86_64/kernel/genapic.c
@@ -30,8 +30,9 @@ EXPORT_SYMBOL(x86_cpu_to_apicid);
 u8 x86_cpu_to_log_apicid[NR_CPUS] = { [0 ... NR_CPUS-1] = BAD_APICID };
 
 extern struct genapic apic_cluster;
 extern struct genapic apic_flat;
+extern struct genapic apic_physflat;
 
 #ifndef CONFIG_XEN
 struct genapic *genapic = &apic_flat;
 #else
@@ -49,14 +50,9 @@ void __init clustered_apic_check(void)
 	long i;
 	u8 clusters, max_cluster;
 	u8 id;
 	u8 cluster_cnt[NUM_APIC_CLUSTERS];
-
-	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD) {
-		/* AMD always uses flat mode right now */
-		genapic = &apic_flat;
-		goto print;
-	}
+	int num_cpus = 0;
 
 #if defined(CONFIG_ACPI_BUS)
 	/*
 	 * Some x86_64 machines use physical APIC mode regardless of how many
@@ -69,17 +65,36 @@ void __init clustered_apic_check(void)
 		}
 #endif
 
 	memset(cluster_cnt, 0, sizeof(cluster_cnt));
-
 	for (i = 0; i < NR_CPUS; i++) {
 		id = bios_cpu_apicid[i];
-		if (id != BAD_APICID)
-			cluster_cnt[APIC_CLUSTERID(id)]++;
+		if (id == BAD_APICID)
+			continue;
+		num_cpus++;
+		cluster_cnt[APIC_CLUSTERID(id)]++;
 	}
 
+	/* Don't use clustered mode on AMD platforms. */
+ 	if (boot_cpu_data.x86_vendor == X86_VENDOR_AMD) {
+		genapic = &apic_physflat;
+#ifndef CONFIG_CPU_HOTPLUG
+		/* In the CPU hotplug case we cannot use broadcast mode
+		   because that opens a race when a CPU is removed.
+		   Stay at physflat mode in this case.
+		   It is bad to do this unconditionally though. Once
+		   we have ACPI platform support for CPU hotplug
+		   we should detect hotplug capablity from ACPI tables and
+		   only do this when really needed. -AK */
+		if (num_cpus <= 8)
+			genapic = &apic_flat;
+#endif
+ 		goto print;
+ 	}
+
 	clusters = 0;
 	max_cluster = 0;
+
 	for (i = 0; i < NUM_APIC_CLUSTERS; i++) {
 		if (cluster_cnt[i] > 0) {
 			++clusters;
 			if (cluster_cnt[i] > max_cluster)
Index: xen-2005-08-20/arch/xen/x86_64/kernel/irq.c
===================================================================
--- xen-2005-08-20.orig/arch/xen/x86_64/kernel/irq.c
+++ xen-2005-08-20/arch/xen/x86_64/kernel/irq.c
@@ -136,4 +136,23 @@ void fixup_irqs(cpumask_t map)
 	mdelay(1);
 	local_irq_disable();
 }
 #endif
+
+extern void call_softirq(void);
+
+asmlinkage void do_softirq(void)
+{
+ 	__u32 pending;
+ 	unsigned long flags;
+
+ 	if (in_interrupt())
+ 		return;
+
+ 	local_irq_save(flags);
+ 	pending = local_softirq_pending();
+ 	/* Switch to interrupt stack */
+ 	if (pending)
+		call_softirq();
+ 	local_irq_restore(flags);
+}
+EXPORT_SYMBOL(do_softirq);
Index: xen-2005-08-20/arch/xen/x86_64/kernel/mpparse.c
===================================================================
--- xen-2005-08-20.orig/arch/xen/x86_64/kernel/mpparse.c
+++ xen-2005-08-20/arch/xen/x86_64/kernel/mpparse.c
@@ -109,9 +109,9 @@ static int __init mpf_checksum(unsigned 
 
 #ifndef CONFIG_XEN
 static void __init MP_processor_info (struct mpc_config_processor *m)
 {
-	int ver;
+	int ver, cpu;
 	static int found_bsp=0;
 
 	if (!(m->mpc_cpuflag & CPU_ENABLED))
 		return;
@@ -131,9 +131,9 @@ static void __init MP_processor_info (st
 			" Processor ignored.\n", NR_CPUS);
 		return;
 	}
 
-	num_processors++;
+	cpu = num_processors++;
 
 	if (m->mpc_apicid > MAX_APICS) {
 		printk(KERN_ERR "Processor #%d INVALID. (Max ID: %d).\n",
 			m->mpc_apicid, MAX_APICS);
@@ -155,15 +155,20 @@ static void __init MP_processor_info (st
  		 * bios_cpu_apicid is required to have processors listed
  		 * in same order as logical cpu numbers. Hence the first
  		 * entry is BSP, and so on.
  		 */
+		cpu = 0;
+
  		bios_cpu_apicid[0] = m->mpc_apicid;
  		x86_cpu_to_apicid[0] = m->mpc_apicid;
  		found_bsp = 1;
- 	} else {
- 		bios_cpu_apicid[num_processors - found_bsp] = m->mpc_apicid;
- 		x86_cpu_to_apicid[num_processors - found_bsp] = m->mpc_apicid;
- 	}
+ 	} else
+		cpu = num_processors - found_bsp;
+	bios_cpu_apicid[cpu] = m->mpc_apicid;
+	x86_cpu_to_apicid[cpu] = m->mpc_apicid;
+
+	cpu_set(cpu, cpu_possible_map);
+	cpu_set(cpu, cpu_present_map);
 }
 #else
 void __init MP_processor_info (struct mpc_config_processor *m)
 {
@@ -969,10 +974,23 @@ int mp_register_gsi(u32 gsi, int edge_le
 		 * For PCI devices assign IRQs in order, avoiding gaps
 		 * due to unused I/O APIC pins.
 		 */
 		int irq = gsi;
-		gsi = pci_irq++;
-		gsi_to_irq[irq] = gsi;
+		if (gsi < MAX_GSI_NUM) {
+			if (gsi > 15)
+				gsi = pci_irq++;
+#ifdef CONFIG_ACPI_BUS
+			/*
+			 * Don't assign IRQ used by ACPI SCI
+			 */
+			if (gsi == acpi_fadt.sci_int)
+				gsi = pci_irq++;
+#endif
+			gsi_to_irq[irq] = gsi;
+		} else {
+			printk(KERN_ERR "GSI %u is too high\n", gsi);
+			return gsi;
+		}
 	}
 
 	io_apic_set_pci_routing(ioapic, ioapic_pin, gsi,
 		edge_level == ACPI_EDGE_SENSITIVE ? 0 : 1,
Index: xen-2005-08-20/arch/xen/x86_64/kernel/setup.c
===================================================================
--- xen-2005-08-20.orig/arch/xen/x86_64/kernel/setup.c
+++ xen-2005-08-20/arch/xen/x86_64/kernel/setup.c
@@ -985,10 +985,8 @@ static void __init amd_detect_cmp(struct
 #ifdef CONFIG_SMP
 	int cpu = smp_processor_id();
 	int node = 0;
 	unsigned bits;
-	if (c->x86_num_cores == 1)
-		return;
 
 	bits = 0;
 	while ((1 << bits) < c->x86_num_cores)
 		bits++;
@@ -1301,10 +1299,9 @@ void __cpuinit identify_cpu(struct cpuin
 		mtrr_bp_init();
 	else
 		mtrr_ap_init();
 #ifdef CONFIG_NUMA
-	if (c != &boot_cpu_data)
-		numa_add_cpu(c - cpu_data);
+	numa_add_cpu(smp_processor_id());
 #endif
 }
  
 
Index: xen-2005-08-20/arch/xen/x86_64/kernel/setup64.c
===================================================================
--- xen-2005-08-20.orig/arch/xen/x86_64/kernel/setup64.c
+++ xen-2005-08-20/arch/xen/x86_64/kernel/setup64.c
@@ -15,8 +15,9 @@
 #include <linux/sched.h>
 #include <linux/string.h>
 #include <linux/bootmem.h>
 #include <linux/bitops.h>
+#include <linux/module.h>
 #include <asm/bootsetup.h>
 #include <asm/pda.h>
 #include <asm/pgtable.h>
 #include <asm/processor.h>
@@ -25,12 +26,10 @@
 #include <asm/mmu_context.h>
 #include <asm/smp.h>
 #include <asm/i387.h>
 #include <asm/percpu.h>
-#include <asm/mtrr.h>
 #include <asm/proto.h>
-#include <asm/mman.h>
-#include <asm/numa.h>
+#include <asm/sections.h>
 #ifdef CONFIG_XEN
 #include <asm-xen/hypervisor.h>
 #endif
 char x86_boot_params[BOOT_PARAM_SIZE] __initdata = {0,};
@@ -38,13 +37,8 @@ char x86_boot_params[BOOT_PARAM_SIZE] __
 cpumask_t cpu_initialized __cpuinitdata = CPU_MASK_NONE;
 
 struct x8664_pda cpu_pda[NR_CPUS] __cacheline_aligned; 
 
-extern struct task_struct init_task;
-
-extern unsigned char __per_cpu_start[], __per_cpu_end[]; 
-
-extern struct desc_ptr cpu_gdt_descr[];
 struct desc_ptr idt_descr = { 256 * 16, (unsigned long) idt_table }; 
 
 char boot_cpu_stack[IRQSTACKSIZE] __attribute__((section(".bss.page_aligned")));
 
@@ -106,9 +100,9 @@ void __init setup_per_cpu_areas(void)
 		size = PERCPU_ENOUGH_ROOM;
 #endif
 
 	for (i = 0; i < NR_CPUS; i++) { 
-		unsigned char *ptr;
+		char *ptr;
 
 		if (!NODE_DATA(cpu_to_node(i))) {
 			printk("cpu with no node %d, num_online_nodes %d\n",
 			       i, num_online_nodes());
@@ -245,13 +239,9 @@ void __cpuinit check_efer(void)
  * A lot of state is already set up in PDA init.
  */
 void __cpuinit cpu_init (void)
 {
-#ifdef CONFIG_SMP
 	int cpu = stack_smp_processor_id();
-#else
-	int cpu = smp_processor_id();
-#endif
 	struct tss_struct *t = &per_cpu(init_tss, cpu);
 	unsigned long v; 
 	char *estacks = NULL; 
 	struct task_struct *me;
@@ -269,9 +259,9 @@ void __cpuinit cpu_init (void)
 		panic("CPU#%d already initialized!\n", cpu);
 
 	printk("Initializing CPU#%d\n", cpu);
 
-		clear_in_cr4(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
+	clear_in_cr4(X86_CR4_VME|X86_CR4_PVI|X86_CR4_TSD|X86_CR4_DE);
 
 	/*
 	 * Initialize the per-CPU GDT with the boot GDT,
 	 * and set up the GDT descriptor:
Index: xen-2005-08-20/arch/xen/x86_64/kernel/smp.c
===================================================================
--- xen-2005-08-20.orig/arch/xen/x86_64/kernel/smp.c
+++ xen-2005-08-20/arch/xen/x86_64/kernel/smp.c
@@ -135,12 +135,11 @@ asmlinkage void smp_invalidate_interrupt
 				__flush_tlb_one(flush_va);
 		} else
 			leave_mm(cpu);
 	}
+out:
 	ack_APIC_irq();
 	cpu_clear(cpu, flush_cpumask);
-
-out:
 	put_cpu_no_resched();
 }
 
 static void flush_tlb_others(cpumask_t cpumask, struct mm_struct *mm,
@@ -312,8 +311,71 @@ void unlock_ipi_call_lock(void)
 	spin_unlock_irq(&call_lock);
 }
 
 /*
+ * this function sends a 'generic call function' IPI to one other CPU
+ * in the system.
+ */
+static void __smp_call_function_single (int cpu, void (*func) (void *info), void *info,
+				int nonatomic, int wait)
+{
+	struct call_data_struct data;
+	int cpus = 1;
+
+	data.func = func;
+	data.info = info;
+	atomic_set(&data.started, 0);
+	data.wait = wait;
+	if (wait)
+		atomic_set(&data.finished, 0);
+
+	call_data = &data;
+	wmb();
+	/* Send a message to all other CPUs and wait for them to respond */
+	send_IPI_mask(cpumask_of_cpu(cpu), CALL_FUNCTION_VECTOR);
+
+	/* Wait for response */
+	while (atomic_read(&data.started) != cpus)
+		cpu_relax();
+
+	if (!wait)
+		return;
+
+	while (atomic_read(&data.finished) != cpus)
+		cpu_relax();
+}
+
+/*
+ * smp_call_function_single - Run a function on another CPU
+ * @func: The function to run. This must be fast and non-blocking.
+ * @info: An arbitrary pointer to pass to the function.
+ * @nonatomic: Currently unused.
+ * @wait: If true, wait until function has completed on other CPUs.
+ *
+ * Retrurns 0 on success, else a negative status code.
+ *
+ * Does not return until the remote CPU is nearly ready to execute <func>
+ * or is or has executed.
+ */
+
+int smp_call_function_single (int cpu, void (*func) (void *info), void *info,
+	int nonatomic, int wait)
+{
+	/* prevent preemption and reschedule on another processor */
+	int me = get_cpu();
+	if (cpu == me) {
+		WARN_ON(1);
+		put_cpu();
+		return -EBUSY;
+	}
+	spin_lock_bh(&call_lock);
+	__smp_call_function_single(cpu, func, info, nonatomic, wait);
+	spin_unlock_bh(&call_lock);
+	put_cpu();
+	return 0;
+}
+
+/*
  * this function sends a 'generic call function' IPI to all other CPUs
  * in the system.
  */
 static void __smp_call_function (void (*func) (void *info), void *info,
Index: xen-2005-08-20/arch/xen/x86_64/kernel/smpboot.c
===================================================================
--- xen-2005-08-20.orig/arch/xen/x86_64/kernel/smpboot.c
+++ xen-2005-08-20/arch/xen/x86_64/kernel/smpboot.c
@@ -116,26 +116,8 @@ struct task_struct *idle_thread_array[NR
 
 #define get_idle_for_cpu(x)     (idle_thread_array[(x)])
 #define set_idle_for_cpu(x,p)   (idle_thread_array[(x)] = (p))
 
-/*
- * cpu_possible_map should be static, it cannot change as cpu's
- * are onlined, or offlined. The reason is per-cpu data-structures
- * are allocated by some modules at init time, and dont expect to
- * do this dynamically on cpu arrival/departure.
- * cpu_present_map on the other hand can change dynamically.
- * In case when cpu_hotplug is not compiled, then we resort to current
- * behaviour, which is cpu_possible == cpu_present.
- * If cpu-hotplug is supported, then we need to preallocate for all
- * those NR_CPUS, hence cpu_possible_map represents entire NR_CPUS range.
- * - Ashok Raj
- */
-#ifdef CONFIG_HOTPLUG_CPU
-#define fixup_cpu_possible_map(x)	cpu_set((x), cpu_possible_map)
-#else
-#define fixup_cpu_possible_map(x)
-#endif
-
 #ifndef CONFIG_XEN
 /*
  * Trampoline 80x86 program as an array.
  */
@@ -243,11 +225,8 @@ static int notscsync __cpuinitdata;
 static __cpuinit void sync_master(void *arg)
 {
 	unsigned long flags, i;
 
-	if (smp_processor_id() != 0)
-		return;
-
 	go[MASTER] = 0;
 
 	local_irq_save(flags);
 	{
@@ -294,9 +273,9 @@ get_delta(long *rt, long *master)
 		++tcenter;
 	return tcenter - best_tm;
 }
 
-static __cpuinit void sync_tsc(void)
+static __cpuinit void sync_tsc(unsigned int master)
 {
 	int i, done = 0;
 	long delta, adj, adjust_latency = 0;
 	unsigned long flags, rt, master_time_stamp, bound;
@@ -308,11 +287,19 @@ static __cpuinit void sync_tsc(void)
 		long lat;	/* estimate of tsc adjustment latency */
 	} t[NUM_ROUNDS] __cpuinitdata;
 #endif
 
+	printk(KERN_INFO "CPU %d: Syncing TSC to CPU %u.\n",
+		smp_processor_id(), master);
+
 	go[MASTER] = 1;
 
-	smp_call_function(sync_master, NULL, 1, 0);
+	/* It is dangerous to broadcast IPI as cpus are coming up,
+	 * as they may not be ready to accept them.  So since
+	 * we only need to send the ipi to the boot cpu direct
+	 * the message, and avoid the race.
+	 */
+	smp_call_function_single(master, sync_master, NULL, 1, 0);
 
 	while (go[MASTER])	/* wait for master to be ready */
 		no_cpu_relax();
 
@@ -354,18 +341,16 @@ static __cpuinit void sync_tsc(void)
 
 	printk(KERN_INFO
 	       "CPU %d: synchronized TSC with CPU %u (last diff %ld cycles, "
 	       "maxerr %lu cycles)\n",
-	       smp_processor_id(), boot_cpu_id, delta, rt);
+	       smp_processor_id(), master, delta, rt);
 }
 
 static void __cpuinit tsc_sync_wait(void)
 {
 	if (notscsync || !cpu_has_tsc)
 		return;
-	printk(KERN_INFO "CPU %d: Syncing TSC to CPU %u.\n", smp_processor_id(),
-			boot_cpu_id);
-	sync_tsc();
+	sync_tsc(boot_cpu_id);
 }
 
 static __init int notscsync_setup(char *s)
 {
@@ -844,10 +829,11 @@ do_rest:
 	per_cpu(init_tss,cpu).rsp0 = init_rsp;
 	initial_code = start_secondary;
 	clear_ti_thread_flag(c_idle.idle->thread_info, TIF_FORK);
 
-	printk(KERN_INFO "Booting processor %d/%d rip %lx rsp %lx\n", cpu, apicid,
-	       start_rip, init_rsp);
+	printk(KERN_INFO "Booting processor %d/%d APIC 0x%x\n", cpu,
+		cpus_weight(cpu_present_map),
+		apicid);
 
 	/*
 	 * This grunge runs the startup process for
 	 * the targeted processor.
@@ -1087,8 +1073,29 @@ static __init void enforce_max_cpus(unsi
 		}
 	}
 }
 
+#ifdef CONFIG_HOTPLUG_CPU
+/*
+ * cpu_possible_map should be static, it cannot change as cpu's
+ * are onlined, or offlined. The reason is per-cpu data-structures
+ * are allocated by some modules at init time, and dont expect to
+ * do this dynamically on cpu arrival/departure.
+ * cpu_present_map on the other hand can change dynamically.
+ * In case when cpu_hotplug is not compiled, then we resort to current
+ * behaviour, which is cpu_possible == cpu_present.
+ * If cpu-hotplug is supported, then we need to preallocate for all
+ * those NR_CPUS, hence cpu_possible_map represents entire NR_CPUS range.
+ * - Ashok Raj
+ */
+static void prefill_possible_map(void)
+{
+	int i;
+	for (i = 0; i < NR_CPUS; i++)
+		cpu_set(i, cpu_possible_map);
+}
+#endif
+
 /*
  * Various sanity checks.
  */
 static int __init smp_sanity_check(unsigned max_cpus)
@@ -1157,10 +1164,8 @@ static int __init smp_sanity_check(unsig
  * earlier.  Just do some sanity checking here and enable APIC mode.
  */
 void __init smp_prepare_cpus(unsigned int max_cpus)
 {
-	int i;
-
 #if defined(CONFIG_XEN) && !defined(CONFIG_XEN_PRIVILEGED_GUEST)
 #else
 	nmi_watchdog_default();
 #endif
@@ -1168,23 +1173,11 @@ void __init smp_prepare_cpus(unsigned in
 	current_thread_info()->cpu = 0;  /* needed? */
 
 	enforce_max_cpus(max_cpus);
 
-	/*
-	 * Fill in cpu_present_mask
-	 */
-	for (i = 0; i < NR_CPUS; i++) {
-#ifndef CONFIG_XEN
-		int apicid = cpu_present_to_apicid(i);
-		if (physid_isset(apicid, phys_cpu_present_map)) {
-#else
-		if (i < HYPERVISOR_shared_info->n_vcpu) {
+#ifdef CONFIG_HOTPLUG_CPU
+	prefill_possible_map();
 #endif
-			cpu_set(i, cpu_present_map);
-			cpu_set(i, cpu_possible_map);
-		}
-		fixup_cpu_possible_map(i);
-	}
 
 	if (smp_sanity_check(max_cpus) < 0) {
 		printk(KERN_INFO "SMP disabled\n");
 		disable_smp();
@@ -1385,10 +1378,9 @@ void __cpu_die(unsigned int cpu)
 		if (per_cpu(cpu_state, cpu) == CPU_DEAD) {
 			printk ("CPU %d is now offline\n", cpu);
 			return;
 		}
-		current->state = TASK_UNINTERRUPTIBLE;
-		schedule_timeout(HZ/10);
+		msleep(100);
 	}
 	printk(KERN_ERR "CPU %u didn't die...\n", cpu);
 }
 
Index: xen-2005-08-20/arch/xen/x86_64/kernel/traps.c
===================================================================
--- xen-2005-08-20.orig/arch/xen/x86_64/kernel/traps.c
+++ xen-2005-08-20/arch/xen/x86_64/kernel/traps.c
@@ -595,11 +595,8 @@ asmlinkage void default_do_nmi(struct pt
 	/* Only the BSP gets external NMIs from the system.  */
 	if (!cpu)
 		reason = get_nmi_reason();
 
-	if (!cpu_online(cpu))
-		return;
-
 	if (!(reason & 0xc0)) {
 		if (notify_die(DIE_NMI_IPI, "nmi_ipi", regs, reason, 0, SIGINT)
 								== NOTIFY_STOP)
 			return;
Index: xen-2005-08-20/arch/xen/x86_64/mm/fault.c
===================================================================
--- xen-2005-08-20.orig/arch/xen/x86_64/mm/fault.c
+++ xen-2005-08-20/arch/xen/x86_64/mm/fault.c
@@ -23,9 +23,8 @@
 #include <linux/vt_kern.h>		/* For unblank_screen() */
 #include <linux/compiler.h>
 #include <linux/module.h>
 #include <linux/percpu.h>
-#include <linux/kprobes.h>
 
 #include <asm/system.h>
 #include <asm/uaccess.h>
 #include <asm/pgalloc.h>
Index: xen-2005-08-20/include/asm-xen/asm-x86_64/desc.h
===================================================================
--- xen-2005-08-20.orig/include/asm-xen/asm-x86_64/desc.h
+++ xen-2005-08-20/include/asm-xen/asm-x86_64/desc.h
@@ -90,8 +90,9 @@ static inline void clear_LDT(void)
  * something other than this.
  */
 extern struct desc_struct default_ldt[];
 extern struct gate_struct idt_table[]; 
+extern struct desc_ptr cpu_gdt_descr[];
 
 static inline void _set_gate(void *adr, unsigned type, unsigned long func, unsigned dpl, unsigned ist)  
 {
 	struct gate_struct s; 	
Index: xen-2005-08-20/include/asm-xen/asm-x86_64/irq.h
===================================================================
--- xen-2005-08-20.orig/include/asm-xen/asm-x86_64/irq.h
+++ xen-2005-08-20/include/asm-xen/asm-x86_64/irq.h
@@ -37,5 +37,7 @@ int handle_IRQ_event(unsigned int, struc
 #include <linux/cpumask.h>
 extern void fixup_irqs(cpumask_t map);
 #endif
 
+#define __ARCH_HAS_DO_SOFTIRQ 1
+
 #endif /* _ASM_IRQ_H */
Index: xen-2005-08-20/include/asm-xen/asm-x86_64/pgtable.h
===================================================================
--- xen-2005-08-20.orig/include/asm-xen/asm-x86_64/pgtable.h
+++ xen-2005-08-20/include/asm-xen/asm-x86_64/pgtable.h
@@ -215,8 +215,10 @@ static inline pte_t ptep_get_and_clear(s
 #define __PAGE_KERNEL_VSYSCALL_NOCACHE \
 	(_PAGE_PRESENT | _PAGE_USER | _PAGE_ACCESSED | _PAGE_PCD | _PAGE_USER )
 #define __PAGE_KERNEL_LARGE \
 	(__PAGE_KERNEL | _PAGE_PSE | _PAGE_USER )
+#define __PAGE_KERNEL_LARGE_EXEC \
+	(__PAGE_KERNEL_EXEC | _PAGE_PSE | _PAGE_USER )
 
 
 /*
  * We don't support GLOBAL page in xenolinux64
Index: xen-2005-08-20/include/asm-xen/asm-x86_64/smp.h
===================================================================
--- xen-2005-08-20.orig/include/asm-xen/asm-x86_64/smp.h
+++ xen-2005-08-20/include/asm-xen/asm-x86_64/smp.h
@@ -45,14 +45,14 @@ extern volatile unsigned long smp_invali
 extern int pic_mode;
 extern void lock_ipi_call_lock(void);
 extern void unlock_ipi_call_lock(void);
 extern int smp_num_siblings;
-extern void smp_flush_tlb(void);
-extern void smp_message_irq(int cpl, void *dev_id, struct pt_regs *regs);
 extern void smp_send_reschedule(int cpu);
-extern void smp_invalidate_rcv(void);		/* Process an NMI */
 extern void zap_low_mappings(void);
 void smp_stop_cpu(void);
+extern int smp_call_function_single(int cpuid, void (*func) (void *info),
+				void *info, int retry, int wait);
+
 extern cpumask_t cpu_sibling_map[NR_CPUS];
 extern cpumask_t cpu_core_map[NR_CPUS];
 extern u8 phys_proc_id[NR_CPUS];
 extern u8 cpu_core_id[NR_CPUS];
Index: xen-2005-08-20/include/asm-xen/asm-x86_64/system.h
===================================================================
--- xen-2005-08-20.orig/include/asm-xen/asm-x86_64/system.h
+++ xen-2005-08-20/include/asm-xen/asm-x86_64/system.h
@@ -119,14 +119,14 @@ struct alt_instr { 
 
 /*
  * Alternative inline assembly with input.
  * 
- * Pecularities:
+ * Peculiarities:
  * No memory clobber here. 
  * Argument numbers start with 1.
  * Best is to use constraints that are fixed size (like (%1) ... "r")
  * If you use variable sized constraints like "m" or "g" in the 
- * replacement maake sure to pad to the worst case length.
+ * replacement make sure to pad to the worst case length.
  */
 #define alternative_input(oldinstr, newinstr, feature, input...)	\
 	asm volatile ("661:\n\t" oldinstr "\n662:\n"			\
 		      ".section .altinstructions,\"a\"\n"		\
@@ -396,10 +396,7 @@ void cpu_idle_wait(void);
 #define HAVE_DISABLE_HLT
 void disable_hlt(void);
 void enable_hlt(void);
 
-#define HAVE_EAT_KEY
-void eat_key(void);
-
 extern unsigned long arch_align_stack(unsigned long sp);
 
 #endif
Index: xen-2005-08-20/include/asm-xen/asm-x86_64/tlbflush.h
===================================================================
--- xen-2005-08-20.orig/include/asm-xen/asm-x86_64/tlbflush.h
+++ xen-2005-08-20/include/asm-xen/asm-x86_64/tlbflush.h
@@ -31,10 +31,11 @@ extern unsigned long pgkern_mask;
  *  - flush_tlb_range(vma, start, end) flushes a range of pages
  *  - flush_tlb_kernel_range(start, end) flushes a range of kernel pages
  *  - flush_tlb_pgtables(mm, start, end) flushes a range of page tables
  *
- * ..but the x86_64 has somewhat limited tlb flushing capabilities,
- * and page-granular flushes are available only on i486 and up.
+ * x86-64 can only flush individual pages or full VMs. For a range flush
+ * we always do the full VM. Might be worth trying if for a small
+ * range a few INVLPGs in a row are a win.
  */
 
 #ifndef CONFIG_SMP
 
@@ -90,8 +91,10 @@ static inline void flush_tlb_range(struc
 
 static inline void flush_tlb_pgtables(struct mm_struct *mm,
 				      unsigned long start, unsigned long end)
 {
-	/* x86_64 does not keep any page table caches in TLB */
+	/* x86_64 does not keep any page table caches in a software TLB.
+	   The CPUs do in their hardware TLBs, but they are handled
+	   by the normal TLB flushing algorithms. */
 }
 
 #endif /* _X8664_TLBFLUSH_H */
Index: xen-2005-08-20/arch/xen/x86_64/ia32/syscall32_syscall.S
===================================================================
--- /dev/null
+++ xen-2005-08-20/arch/xen/x86_64/ia32/syscall32_syscall.S
@@ -0,0 +1,34 @@
+#define USE_INT80
+
+#ifdef __ASSEMBLY__
+
+/* 32bit VDSOs mapped into user space. */
+
+	.section ".init.data","aw"
+
+#ifdef USE_INT80
+
+	.globl syscall32_int80
+	.globl syscall32_int80_end
+
+syscall32_int80:
+	.incbin "arch/xen/x86_64/ia32/vsyscall-int80.so"
+syscall32_int80_end:
+
+#endif
+
+	.globl syscall32_syscall
+	.globl syscall32_syscall_end
+
+syscall32_syscall:
+	.incbin "arch/xen/x86_64/ia32/vsyscall-syscall.so"
+syscall32_syscall_end:
+
+	.globl syscall32_sysenter
+	.globl syscall32_sysenter_end
+
+syscall32_sysenter:
+	.incbin "arch/xen/x86_64/ia32/vsyscall-sysenter.so"
+syscall32_sysenter_end:
+
+#endif
Index: xen-2005-08-20/arch/xen/x86_64/ia32/Makefile
===================================================================
--- xen-2005-08-20.orig/arch/xen/x86_64/ia32/Makefile
+++ xen-2005-08-20/arch/xen/x86_64/ia32/Makefile
@@ -4,9 +4,9 @@
 XENARCH	:= $(subst ",,$(CONFIG_XENARCH))
 
 CFLAGS	+= -Iarch/$(XENARCH)/kernel
 
-obj-$(CONFIG_IA32_EMULATION) := ia32entry.o syscall32.o
+obj-$(CONFIG_IA32_EMULATION) := ia32entry.o syscall32.o syscall32_syscall.o
 
 c-obj-$(CONFIG_IA32_EMULATION) := sys_ia32.o ia32_ioctl.o \
 	ia32_signal.o tls32.o \
 	ia32_binfmt.o fpu32.o ptrace32.o 
@@ -17,9 +17,9 @@ sysv-$(CONFIG_SYSVIPC) := ipc32.o
 c-obj-$(CONFIG_IA32_EMULATION) += $(sysv-y)
 
 c-obj-$(CONFIG_IA32_AOUT) += ia32_aout.o
 
-$(obj)/syscall32.o: $(src)/syscall32.c \
+$(obj)/syscall32_syscall.o: \
 	$(foreach F,int80 sysenter syscall,$(obj)/vsyscall-$F.so)
 
 # Teach kbuild about targets
 targets := $(foreach F,int80 sysenter syscall,vsyscall-$F.o vsyscall-$F.so)
