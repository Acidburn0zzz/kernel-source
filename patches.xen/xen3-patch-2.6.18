From: www.kernel.org
Subject: Linux 2.6.18
Patch-mainline: 2.6.18

Automatically created from "patches.kernel.org/patch-2.6.18" by xen-port-patches.py

Acked-by: jbeulich@novell.com

Index: head-2006-11-06/arch/i386/kernel/acpi/boot-xen.c
===================================================================
--- head-2006-11-06.orig/arch/i386/kernel/acpi/boot-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/i386/kernel/acpi/boot-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -24,7 +24,6 @@
  */
 
 #include <linux/init.h>
-#include <linux/config.h>
 #include <linux/acpi.h>
 #include <linux/efi.h>
 #include <linux/module.h>
@@ -60,7 +59,7 @@ static inline int gsi_irq_sharing(int gs
 
 #define BAD_MADT_ENTRY(entry, end) (					    \
 		(!entry) || (unsigned long)entry + sizeof(*entry) > end ||  \
-		((acpi_table_entry_header *)entry)->length != sizeof(*entry))
+		((acpi_table_entry_header *)entry)->length < sizeof(*entry))
 
 #define PREFIX			"ACPI: "
 
@@ -204,6 +203,8 @@ int __init acpi_parse_mcfg(unsigned long
 		if (mcfg->config[i].base_reserved) {
 			printk(KERN_ERR PREFIX
 			       "MMCONFIG not in low 4GB of memory\n");
+			kfree(pci_mmcfg_config);
+			pci_mmcfg_config_num = 0;
 			return -ENODEV;
 		}
 	}
@@ -217,7 +218,7 @@ static int __init acpi_parse_madt(unsign
 {
 	struct acpi_table_madt *madt = NULL;
 
-	if (!phys_addr || !size)
+	if (!phys_addr || !size || !cpu_has_apic)
 		return -EINVAL;
 
 	madt = (struct acpi_table_madt *)__acpi_map_table(phys_addr, size);
@@ -624,9 +625,9 @@ extern u32 pmtmr_ioport;
 
 static int __init acpi_parse_fadt(unsigned long phys, unsigned long size)
 {
-	struct fadt_descriptor_rev2 *fadt = NULL;
+	struct fadt_descriptor *fadt = NULL;
 
-	fadt = (struct fadt_descriptor_rev2 *)__acpi_map_table(phys, size);
+	fadt = (struct fadt_descriptor *)__acpi_map_table(phys, size);
 	if (!fadt) {
 		printk(KERN_WARNING PREFIX "Unable to map FADT\n");
 		return 0;
Index: head-2006-11-06/arch/i386/kernel/alternative.c
===================================================================
--- head-2006-11-06.orig/arch/i386/kernel/alternative.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/arch/i386/kernel/alternative.c	2006-10-16 10:19:17.000000000 +0200
@@ -151,7 +151,7 @@ void apply_alternatives(struct alt_instr
 #ifdef CONFIG_X86_64
 		/* vsyscall code is not mapped yet. resolve it manually. */
 		if (instr >= (u8 *)VSYSCALL_START && instr < (u8*)VSYSCALL_END) {
-			instr = __va(instr - (u8*)VSYSCALL_START + (u8*)__pa_symbol(&__vsyscall_0));
+			instr -= VSYSCALL_START - (unsigned long)&__vsyscall_0;
 			DPRINTK("%s: vsyscall fixup: %p => %p\n",
 				__FUNCTION__, a->instr, instr);
 		}
Index: head-2006-11-06/arch/i386/kernel/apic-xen.c
===================================================================
--- head-2006-11-06.orig/arch/i386/kernel/apic-xen.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/arch/i386/kernel/apic-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -14,7 +14,6 @@
  *	Mikael Pettersson	:	PM converted to driver model.
  */
 
-#include <linux/config.h>
 #include <linux/init.h>
 
 #include <linux/mm.h>
Index: head-2006-11-06/arch/i386/kernel/cpu/common-xen.c
===================================================================
--- head-2006-11-06.orig/arch/i386/kernel/cpu/common-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/i386/kernel/cpu/common-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -11,6 +11,8 @@
 #include <asm/msr.h>
 #include <asm/io.h>
 #include <asm/mmu_context.h>
+#include <asm/mtrr.h>
+#include <asm/mce.h>
 #ifdef CONFIG_X86_LOCAL_APIC
 #include <asm/mpspec.h>
 #include <asm/apic.h>
@@ -295,7 +297,7 @@ void __cpuinit generic_identify(struct c
 			if (c->x86 >= 0x6)
 				c->x86_model += ((tfms >> 16) & 0xF) << 4;
 			c->x86_mask = tfms & 15;
-#ifdef CONFIG_SMP
+#ifdef CONFIG_X86_HT
 			c->apicid = phys_pkg_id((ebx >> 24) & 0xFF, 0);
 #else
 			c->apicid = (ebx >> 24) & 0xFF;
@@ -320,7 +322,7 @@ void __cpuinit generic_identify(struct c
 	early_intel_workaround(c);
 
 #ifdef CONFIG_X86_HT
-	phys_proc_id[smp_processor_id()] = (cpuid_ebx(1) >> 24) & 0xff;
+	c->phys_proc_id = (cpuid_ebx(1) >> 24) & 0xff;
 #endif
 }
 
@@ -478,11 +480,9 @@ void __cpuinit detect_ht(struct cpuinfo_
 {
 	u32 	eax, ebx, ecx, edx;
 	int 	index_msb, core_bits;
-	int 	cpu = smp_processor_id();
 
 	cpuid(1, &eax, &ebx, &ecx, &edx);
 
-
 	if (!cpu_has(c, X86_FEATURE_HT) || cpu_has(c, X86_FEATURE_CMP_LEGACY))
 		return;
 
@@ -493,16 +493,17 @@ void __cpuinit detect_ht(struct cpuinfo_
 	} else if (smp_num_siblings > 1 ) {
 
 		if (smp_num_siblings > NR_CPUS) {
-			printk(KERN_WARNING "CPU: Unsupported number of the siblings %d", smp_num_siblings);
+			printk(KERN_WARNING "CPU: Unsupported number of the "
+					"siblings %d", smp_num_siblings);
 			smp_num_siblings = 1;
 			return;
 		}
 
 		index_msb = get_count_order(smp_num_siblings);
-		phys_proc_id[cpu] = phys_pkg_id((ebx >> 24) & 0xFF, index_msb);
+		c->phys_proc_id = phys_pkg_id((ebx >> 24) & 0xFF, index_msb);
 
 		printk(KERN_INFO  "CPU: Physical Processor ID: %d\n",
-		       phys_proc_id[cpu]);
+		       c->phys_proc_id);
 
 		smp_num_siblings = smp_num_siblings / c->x86_max_cores;
 
@@ -510,12 +511,12 @@ void __cpuinit detect_ht(struct cpuinfo_
 
 		core_bits = get_count_order(c->x86_max_cores);
 
-		cpu_core_id[cpu] = phys_pkg_id((ebx >> 24) & 0xFF, index_msb) &
+		c->cpu_core_id = phys_pkg_id((ebx >> 24) & 0xFF, index_msb) &
 					       ((1 << core_bits) - 1);
 
 		if (c->x86_max_cores > 1)
 			printk(KERN_INFO  "CPU: Processor Core ID: %d\n",
-			       cpu_core_id[cpu]);
+			       c->cpu_core_id);
 	}
 }
 #endif
@@ -634,6 +635,12 @@ void __cpuinit cpu_init(void)
 	}
 
 #ifndef CONFIG_XEN
+	/* The CPU hotplug case */
+	if (cpu_gdt_descr->address) {
+		gdt = (struct desc_struct *)cpu_gdt_descr->address;
+		memset(gdt, 0, PAGE_SIZE);
+		goto old_gdt;
+	}
 	/*
 	 * This is a horrible hack to allocate the GDT.  The problem
 	 * is that cpu_init() is called really early for the boot CPU
@@ -652,7 +659,7 @@ void __cpuinit cpu_init(void)
 				local_irq_enable();
 		}
 	}
-
+old_gdt:
 	/*
 	 * Initialize the per-CPU GDT with the boot GDT,
 	 * and set up the GDT descriptor:
Index: head-2006-11-06/arch/i386/kernel/entry-xen.S
===================================================================
--- head-2006-11-06.orig/arch/i386/kernel/entry-xen.S	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/i386/kernel/entry-xen.S	2006-10-16 10:19:17.000000000 +0200
@@ -40,14 +40,15 @@
  * "current" is in register %ebx during any slow entries.
  */
 
-#include <linux/config.h>
 #include <linux/linkage.h>
 #include <asm/thread_info.h>
+#include <asm/irqflags.h>
 #include <asm/errno.h>
 #include <asm/segment.h>
 #include <asm/smp.h>
 #include <asm/page.h>
 #include <asm/desc.h>
+#include <asm/dwarf2.h>
 #include "irq_vectors.h"
 #include <xen/interface/xen.h>
 
@@ -106,40 +107,93 @@ NMI_MASK	= 0x80000000
 #endif
 
 #ifdef CONFIG_PREEMPT
-#define preempt_stop		cli
+#define preempt_stop		cli; TRACE_IRQS_OFF
 #else
 #define preempt_stop
 #define resume_kernel		restore_nocheck
 #endif
 
+#ifndef CONFIG_XEN
+.macro TRACE_IRQS_IRET
+#ifdef CONFIG_TRACE_IRQFLAGS
+	testl $IF_MASK,EFLAGS(%esp)     # interrupts off?
+	jz 1f
+	TRACE_IRQS_ON
+1:
+#endif
+.endm
+#endif
+
+#ifdef CONFIG_VM86
+#define resume_userspace_sig	check_userspace
+#else
+#define resume_userspace_sig	resume_userspace
+#endif
+
 #define SAVE_ALL \
 	cld; \
 	pushl %es; \
+	CFI_ADJUST_CFA_OFFSET 4;\
+	/*CFI_REL_OFFSET es, 0;*/\
 	pushl %ds; \
+	CFI_ADJUST_CFA_OFFSET 4;\
+	/*CFI_REL_OFFSET ds, 0;*/\
 	pushl %eax; \
+	CFI_ADJUST_CFA_OFFSET 4;\
+	CFI_REL_OFFSET eax, 0;\
 	pushl %ebp; \
+	CFI_ADJUST_CFA_OFFSET 4;\
+	CFI_REL_OFFSET ebp, 0;\
 	pushl %edi; \
+	CFI_ADJUST_CFA_OFFSET 4;\
+	CFI_REL_OFFSET edi, 0;\
 	pushl %esi; \
+	CFI_ADJUST_CFA_OFFSET 4;\
+	CFI_REL_OFFSET esi, 0;\
 	pushl %edx; \
+	CFI_ADJUST_CFA_OFFSET 4;\
+	CFI_REL_OFFSET edx, 0;\
 	pushl %ecx; \
+	CFI_ADJUST_CFA_OFFSET 4;\
+	CFI_REL_OFFSET ecx, 0;\
 	pushl %ebx; \
+	CFI_ADJUST_CFA_OFFSET 4;\
+	CFI_REL_OFFSET ebx, 0;\
 	movl $(__USER_DS), %edx; \
 	movl %edx, %ds; \
 	movl %edx, %es;
 
 #define RESTORE_INT_REGS \
 	popl %ebx;	\
+	CFI_ADJUST_CFA_OFFSET -4;\
+	CFI_RESTORE ebx;\
 	popl %ecx;	\
+	CFI_ADJUST_CFA_OFFSET -4;\
+	CFI_RESTORE ecx;\
 	popl %edx;	\
+	CFI_ADJUST_CFA_OFFSET -4;\
+	CFI_RESTORE edx;\
 	popl %esi;	\
+	CFI_ADJUST_CFA_OFFSET -4;\
+	CFI_RESTORE esi;\
 	popl %edi;	\
+	CFI_ADJUST_CFA_OFFSET -4;\
+	CFI_RESTORE edi;\
 	popl %ebp;	\
-	popl %eax
+	CFI_ADJUST_CFA_OFFSET -4;\
+	CFI_RESTORE ebp;\
+	popl %eax;	\
+	CFI_ADJUST_CFA_OFFSET -4;\
+	CFI_RESTORE eax
 
 #define RESTORE_REGS	\
 	RESTORE_INT_REGS; \
 1:	popl %ds;	\
+	CFI_ADJUST_CFA_OFFSET -4;\
+	/*CFI_RESTORE ds;*/\
 2:	popl %es;	\
+	CFI_ADJUST_CFA_OFFSET -4;\
+	/*CFI_RESTORE es;*/\
 .section .fixup,"ax";	\
 3:	movl $0,(%esp);	\
 	jmp 1b;		\
@@ -152,13 +206,47 @@ NMI_MASK	= 0x80000000
 	.long 2b,4b;	\
 .previous
 
+#define RING0_INT_FRAME \
+	CFI_STARTPROC simple;\
+	CFI_DEF_CFA esp, 3*4;\
+	/*CFI_OFFSET cs, -2*4;*/\
+	CFI_OFFSET eip, -3*4
+
+#define RING0_EC_FRAME \
+	CFI_STARTPROC simple;\
+	CFI_DEF_CFA esp, 4*4;\
+	/*CFI_OFFSET cs, -2*4;*/\
+	CFI_OFFSET eip, -3*4
+
+#define RING0_PTREGS_FRAME \
+	CFI_STARTPROC simple;\
+	CFI_DEF_CFA esp, OLDESP-EBX;\
+	/*CFI_OFFSET cs, CS-OLDESP;*/\
+	CFI_OFFSET eip, EIP-OLDESP;\
+	/*CFI_OFFSET es, ES-OLDESP;*/\
+	/*CFI_OFFSET ds, DS-OLDESP;*/\
+	CFI_OFFSET eax, EAX-OLDESP;\
+	CFI_OFFSET ebp, EBP-OLDESP;\
+	CFI_OFFSET edi, EDI-OLDESP;\
+	CFI_OFFSET esi, ESI-OLDESP;\
+	CFI_OFFSET edx, EDX-OLDESP;\
+	CFI_OFFSET ecx, ECX-OLDESP;\
+	CFI_OFFSET ebx, EBX-OLDESP
 
 ENTRY(ret_from_fork)
+	CFI_STARTPROC
 	pushl %eax
+	CFI_ADJUST_CFA_OFFSET 4
 	call schedule_tail
 	GET_THREAD_INFO(%ebp)
 	popl %eax
+	CFI_ADJUST_CFA_OFFSET -4
+	pushl $0x0202			# Reset kernel eflags
+	CFI_ADJUST_CFA_OFFSET 4
+	popfl
+	CFI_ADJUST_CFA_OFFSET -4
 	jmp syscall_exit
+	CFI_ENDPROC
 
 /*
  * Return to user mode is not as complex as all this looks,
@@ -169,10 +257,12 @@ ENTRY(ret_from_fork)
 
 	# userspace resumption stub bypassing syscall exit tracing
 	ALIGN
+	RING0_PTREGS_FRAME
 ret_from_exception:
 	preempt_stop
 ret_from_intr:
 	GET_THREAD_INFO(%ebp)
+check_userspace:
 	movl EFLAGS(%esp), %eax		# mix EFLAGS and CS
 	movb CS(%esp), %al
 	testl $(VM_MASK | 2), %eax
@@ -201,20 +291,42 @@ need_resched:
 	call preempt_schedule_irq
 	jmp need_resched
 #endif
+	CFI_ENDPROC
 
 /* SYSENTER_RETURN points to after the "sysenter" instruction in
    the vsyscall page.  See vsyscall-sysentry.S, which defines the symbol.  */
 
 	# sysenter call handler stub
 ENTRY(sysenter_entry)
+	CFI_STARTPROC simple
+	CFI_DEF_CFA esp, 0
+	CFI_REGISTER esp, ebp
 	movl SYSENTER_stack_esp0(%esp),%esp
 sysenter_past_esp:
+	/*
+	 * No need to follow this irqs on/off section: the syscall
+	 * disabled irqs and here we enable it straight after entry:
+	 */
 	sti
 	pushl $(__USER_DS)
+	CFI_ADJUST_CFA_OFFSET 4
+	/*CFI_REL_OFFSET ss, 0*/
 	pushl %ebp
+	CFI_ADJUST_CFA_OFFSET 4
+	CFI_REL_OFFSET esp, 0
 	pushfl
+	CFI_ADJUST_CFA_OFFSET 4
 	pushl $(__USER_CS)
-	pushl $SYSENTER_RETURN
+	CFI_ADJUST_CFA_OFFSET 4
+	/*CFI_REL_OFFSET cs, 0*/
+	/*
+	 * Push current_thread_info()->sysenter_return to the stack.
+	 * A tiny bit of offset fixup is necessary - 4*4 means the 4 words
+	 * pushed above; +8 corresponds to copy_thread's esp0 setting.
+	 */
+	pushl (TI_sysenter_return-THREAD_SIZE+8+4*4)(%esp)
+	CFI_ADJUST_CFA_OFFSET 4
+	CFI_REL_OFFSET eip, 0
 
 /*
  * Load the potential sixth argument from user stack.
@@ -229,6 +341,7 @@ sysenter_past_esp:
 .previous
 
 	pushl %eax
+	CFI_ADJUST_CFA_OFFSET 4
 	SAVE_ALL
 	GET_THREAD_INFO(%ebp)
 
@@ -240,6 +353,7 @@ sysenter_past_esp:
 	call *sys_call_table(,%eax,4)
 	movl %eax,EAX(%esp)
 	DISABLE_INTERRUPTS
+	TRACE_IRQS_OFF
 	movl TI_flags(%ebp), %ecx
 	testw $_TIF_ALLWORK_MASK, %cx
 	jne syscall_exit_work
@@ -247,6 +361,7 @@ sysenter_past_esp:
 	movl EIP(%esp), %edx
 	movl OLDESP(%esp), %ecx
 	xorl %ebp,%ebp
+	TRACE_IRQS_ON
 #ifdef CONFIG_XEN
 	__ENABLE_INTERRUPTS
 sysexit_scrit:	/**** START OF SYSEXIT CRITICAL REGION ****/
@@ -255,6 +370,7 @@ sysexit_scrit:	/**** START OF SYSEXIT CR
 	movl ESI(%esp), %esi
 	sysexit
 14:	__DISABLE_INTERRUPTS
+	TRACE_IRQS_OFF
 sysexit_ecrit:	/**** END OF SYSEXIT CRITICAL REGION ****/
 	push %esp
 	call evtchn_do_upcall
@@ -264,11 +380,14 @@ sysexit_ecrit:	/**** END OF SYSEXIT CRIT
 	sti
 	sysexit
 #endif /* !CONFIG_XEN */
+	CFI_ENDPROC
 
 
 	# system call handler stub
 ENTRY(system_call)
+	RING0_INT_FRAME			# can't unwind into user space anyway
 	pushl %eax			# save orig_eax
+	CFI_ADJUST_CFA_OFFSET 4
 	SAVE_ALL
 	GET_THREAD_INFO(%ebp)
 	testl $TF_MASK,EFLAGS(%esp)
@@ -288,6 +407,7 @@ syscall_exit:
 	DISABLE_INTERRUPTS		# make sure we don't miss an interrupt
 					# setting need_resched or sigpending
 					# between sampling and the iret
+	TRACE_IRQS_OFF
 	movl TI_flags(%ebp), %ecx
 	testw $_TIF_ALLWORK_MASK, %cx	# current->work
 	jne syscall_exit_work
@@ -302,25 +422,32 @@ restore_all:
 	movb CS(%esp), %al
 	andl $(VM_MASK | (4 << 8) | 3), %eax
 	cmpl $((4 << 8) | 3), %eax
+	CFI_REMEMBER_STATE
 	je ldt_ss			# returning to user-space with LDT SS
 restore_nocheck:
+	TRACE_IRQS_IRET
 #else
 restore_nocheck:
 	movl EFLAGS(%esp), %eax
 	testl $(VM_MASK|NMI_MASK), %eax
+	CFI_REMEMBER_STATE
 	jnz hypervisor_iret
 	shr $9, %eax			# EAX[0] == IRET_EFLAGS.IF
 	GET_VCPU_INFO
 	andb evtchn_upcall_mask(%esi),%al
 	andb $1,%al			# EAX[0] == IRET_EFLAGS.IF & event_mask
+	CFI_REMEMBER_STATE
 	jnz restore_all_enable_events	#        != 0 => enable event delivery
 #endif
+restore_nocheck_notrace:
 	RESTORE_REGS
 	addl $4, %esp
+	CFI_ADJUST_CFA_OFFSET -4
 1:	iret
 .section .fixup,"ax"
 iret_exc:
 #ifndef CONFIG_XEN
+	TRACE_IRQS_ON
 	sti
 #endif
 	pushl $0			# no error code
@@ -332,6 +459,7 @@ iret_exc:
 	.long 1b,iret_exc
 .previous
 
+	CFI_RESTORE_STATE
 #ifndef CONFIG_XEN
 ldt_ss:
 	larl OLDSS(%esp), %eax
@@ -345,11 +473,15 @@ ldt_ss:
 	 * CPUs, which we can try to work around to make
 	 * dosemu and wine happy. */
 	subl $8, %esp		# reserve space for switch16 pointer
+	CFI_ADJUST_CFA_OFFSET 8
 	cli
+	TRACE_IRQS_OFF
 	movl %esp, %eax
 	/* Set up the 16bit stack frame with switch32 pointer on top,
 	 * and a switch16 pointer on top of the current frame. */
 	call setup_x86_bogus_stack
+	CFI_ADJUST_CFA_OFFSET -8	# frame has moved
+	TRACE_IRQS_IRET
 	RESTORE_REGS
 	lss 20+4(%esp), %esp	# switch to 16bit stack
 1:	iret
@@ -358,15 +490,39 @@ ldt_ss:
 	.long 1b,iret_exc
 .previous
 #else
+        ALIGN
+restore_all_enable_events:
+	TRACE_IRQS_ON
+	__ENABLE_INTERRUPTS
+scrit:	/**** START OF CRITICAL REGION ****/
+	__TEST_PENDING
+	jnz  14f			# process more events if necessary...
+	RESTORE_REGS
+	addl $4, %esp
+	CFI_ADJUST_CFA_OFFSET -4
+1:	iret
+.section __ex_table,"a"
+	.align 4
+	.long 1b,iret_exc
+.previous
+14:	__DISABLE_INTERRUPTS
+	TRACE_IRQS_OFF
+	jmp  11f
+ecrit:  /**** END OF CRITICAL REGION ****/
+
+	CFI_RESTORE_STATE
 hypervisor_iret:
 	andl $~NMI_MASK, EFLAGS(%esp)
 	RESTORE_REGS
 	addl $4, %esp
+	CFI_ADJUST_CFA_OFFSET -4
 	jmp  hypercall_page + (__HYPERVISOR_iret * 32)
 #endif
+	CFI_ENDPROC
 
 	# perform work that needs to be done immediately before resumption
 	ALIGN
+	RING0_PTREGS_FRAME		# can't unwind into user space anyway
 work_pending:
 	testb $_TIF_NEED_RESCHED, %cl
 	jz work_notifysig
@@ -375,6 +531,7 @@ work_resched:
 	DISABLE_INTERRUPTS		# make sure we don't miss an interrupt
 					# setting need_resched or sigpending
 					# between sampling and the iret
+	TRACE_IRQS_OFF
 	movl TI_flags(%ebp), %ecx
 	andl $_TIF_WORK_MASK, %ecx	# is there any work to be done other
 					# than syscall tracing?
@@ -390,18 +547,20 @@ work_notifysig:				# deal with pending s
 					# vm86-space
 	xorl %edx, %edx
 	call do_notify_resume
-	jmp resume_userspace
+	jmp resume_userspace_sig
 
 	ALIGN
 work_notifysig_v86:
 #ifdef CONFIG_VM86
 	pushl %ecx			# save ti_flags for do_notify_resume
+	CFI_ADJUST_CFA_OFFSET 4
 	call save_v86_state		# %eax contains pt_regs pointer
 	popl %ecx
+	CFI_ADJUST_CFA_OFFSET -4
 	movl %eax, %esp
 	xorl %edx, %edx
 	call do_notify_resume
-	jmp resume_userspace
+	jmp resume_userspace_sig
 #endif
 
 	# perform syscall exit tracing
@@ -424,25 +583,28 @@ syscall_trace_entry:
 syscall_exit_work:
 	testb $(_TIF_SYSCALL_TRACE|_TIF_SYSCALL_AUDIT|_TIF_SINGLESTEP), %cl
 	jz work_pending
+	TRACE_IRQS_ON
 	ENABLE_INTERRUPTS		# could let do_syscall_trace() call
 					# schedule() instead
 	movl %esp, %eax
 	movl $1, %edx
 	call do_syscall_trace
 	jmp resume_userspace
+	CFI_ENDPROC
 
-	ALIGN
+	RING0_INT_FRAME			# can't unwind into user space anyway
 syscall_fault:
 	pushl %eax			# save orig_eax
+	CFI_ADJUST_CFA_OFFSET 4
 	SAVE_ALL
 	GET_THREAD_INFO(%ebp)
 	movl $-EFAULT,EAX(%esp)
 	jmp resume_userspace
 
-	ALIGN
 syscall_badsys:
 	movl $-ENOSYS,EAX(%esp)
 	jmp resume_userspace
+	CFI_ENDPROC
 
 #ifndef CONFIG_XEN
 #define FIXUP_ESPFIX_STACK \
@@ -455,16 +617,21 @@ syscall_badsys:
 	movl %eax, %esp;
 #define UNWIND_ESPFIX_STACK \
 	pushl %eax; \
+	CFI_ADJUST_CFA_OFFSET 4; \
 	movl %ss, %eax; \
 	/* see if on 16bit stack */ \
 	cmpw $__ESPFIX_SS, %ax; \
-	jne 28f; \
-	movl $__KERNEL_DS, %edx; \
-	movl %edx, %ds; \
-	movl %edx, %es; \
+	je 28f; \
+27:	popl %eax; \
+	CFI_ADJUST_CFA_OFFSET -4; \
+.section .fixup,"ax"; \
+28:	movl $__KERNEL_DS, %eax; \
+	movl %eax, %ds; \
+	movl %eax, %es; \
 	/* switch to 32bit stack */ \
-	FIXUP_ESPFIX_STACK \
-28:	popl %eax;
+	FIXUP_ESPFIX_STACK; \
+	jmp 27b; \
+.previous
 
 /*
  * Build the entry stubs and pointer table with
@@ -476,9 +643,14 @@ ENTRY(interrupt)
 
 vector=0
 ENTRY(irq_entries_start)
+	RING0_INT_FRAME
 .rept NR_IRQS
 	ALIGN
+ .if vector
+	CFI_ADJUST_CFA_OFFSET -4
+ .endif
 1:	pushl $~(vector)
+	CFI_ADJUST_CFA_OFFSET 4
 	jmp common_interrupt
 .data
 	.long 1b
@@ -486,20 +658,30 @@ ENTRY(irq_entries_start)
 vector=vector+1
 .endr
 
+/*
+ * the CPU automatically disables interrupts when executing an IRQ vector,
+ * so IRQ-flags tracing has to follow that:
+ */
 	ALIGN
 common_interrupt:
 	SAVE_ALL
+	TRACE_IRQS_OFF
 	movl %esp,%eax
 	call do_IRQ
 	jmp ret_from_intr
+	CFI_ENDPROC
 
 #define BUILD_INTERRUPT(name, nr)	\
 ENTRY(name)				\
+	RING0_INT_FRAME;		\
 	pushl $~(nr);			\
+	CFI_ADJUST_CFA_OFFSET 4;	\
 	SAVE_ALL			\
+	TRACE_IRQS_OFF			\
 	movl %esp,%eax;			\
 	call smp_/**/name;		\
-	jmp ret_from_intr;
+	jmp ret_from_intr;		\
+	CFI_ENDPROC
 
 /* The include is where all of the SMP etc. interrupts come from */
 #include "entry_arch.h"
@@ -508,34 +690,59 @@ ENTRY(name)				\
 #endif
 
 ENTRY(divide_error)
+	RING0_INT_FRAME
 	pushl $0			# no error code
+	CFI_ADJUST_CFA_OFFSET 4
 	pushl $do_divide_error
+	CFI_ADJUST_CFA_OFFSET 4
 	ALIGN
 error_code:
 	pushl %ds
+	CFI_ADJUST_CFA_OFFSET 4
+	/*CFI_REL_OFFSET ds, 0*/
 	pushl %eax
+	CFI_ADJUST_CFA_OFFSET 4
+	CFI_REL_OFFSET eax, 0
 	xorl %eax, %eax
 	pushl %ebp
+	CFI_ADJUST_CFA_OFFSET 4
+	CFI_REL_OFFSET ebp, 0
 	pushl %edi
+	CFI_ADJUST_CFA_OFFSET 4
+	CFI_REL_OFFSET edi, 0
 	pushl %esi
+	CFI_ADJUST_CFA_OFFSET 4
+	CFI_REL_OFFSET esi, 0
 	pushl %edx
+	CFI_ADJUST_CFA_OFFSET 4
+	CFI_REL_OFFSET edx, 0
 	decl %eax			# eax = -1
 	pushl %ecx
+	CFI_ADJUST_CFA_OFFSET 4
+	CFI_REL_OFFSET ecx, 0
 	pushl %ebx
+	CFI_ADJUST_CFA_OFFSET 4
+	CFI_REL_OFFSET ebx, 0
 	cld
 	pushl %es
+	CFI_ADJUST_CFA_OFFSET 4
+	/*CFI_REL_OFFSET es, 0*/
 	UNWIND_ESPFIX_STACK
 	popl %ecx
+	CFI_ADJUST_CFA_OFFSET -4
+	/*CFI_REGISTER es, ecx*/
 	movl ES(%esp), %edi		# get the function address
 	movl ORIG_EAX(%esp), %edx	# get the error code
 	movl %eax, ORIG_EAX(%esp)
 	movl %ecx, ES(%esp)
+	/*CFI_REL_OFFSET es, ES*/
 	movl $(__USER_DS), %ecx
 	movl %ecx, %ds
 	movl %ecx, %es
 	movl %esp,%eax			# pt_regs pointer
 	call *%edi
 	jmp ret_from_exception
+	CFI_ENDPROC
 
 #ifdef CONFIG_XEN
 # A note on the "critical region" in our callback handler.
@@ -555,7 +762,9 @@ error_code:
 # critical region we know that the entire frame is present and correct
 # so we can simply throw away the new one.
 ENTRY(hypervisor_callback)
+	RING0_INT_FRAME
 	pushl %eax
+	CFI_ADJUST_CFA_OFFSET 4
 	SAVE_ALL
 	movl EIP(%esp),%eax
 	cmpl $scrit,%eax
@@ -567,27 +776,15 @@ ENTRY(hypervisor_callback)
 	cmpl $sysexit_ecrit,%eax
 	ja   11f
 	addl $0x34,%esp			# Remove cs...ebx from stack frame.
+	CFI_ADJUST_CFA_OFFSET -0x34
 11:	push %esp
+	CFI_ADJUST_CFA_OFFSET 4
 	call evtchn_do_upcall
 	add  $4,%esp
+	CFI_ADJUST_CFA_OFFSET -4
 	jmp  ret_from_intr
+	CFI_ENDPROC
 
-        ALIGN
-restore_all_enable_events:
-	__ENABLE_INTERRUPTS
-scrit:	/**** START OF CRITICAL REGION ****/
-	__TEST_PENDING
-	jnz  14f			# process more events if necessary...
-	RESTORE_REGS
-	addl $4, %esp
-1:	iret
-.section __ex_table,"a"
-	.align 4
-	.long 1b,iret_exc
-.previous
-14:	__DISABLE_INTERRUPTS
-	jmp  11b
-ecrit:  /**** END OF CRITICAL REGION ****/
 # [How we do the fixup]. We want to merge the current stack frame with the
 # just-interrupted frame. How we do this depends on where in the critical
 # region the interrupted handler was executing, and so how many saved
@@ -657,6 +854,7 @@ ENTRY(failsafe_callback)
 	addl $16,%esp		# EAX != 0 => Category 2 (Bad IRET)
 	jmp iret_exc
 5:	addl $16,%esp		# EAX == 0 => Category 1 (Bad segment)
+	RING0_INT_FRAME
 	pushl $0
 	SAVE_ALL
 	jmp ret_from_exception
@@ -682,33 +880,47 @@ ENTRY(failsafe_callback)
 	.long 4b,9b;		\
 .previous
 #endif
+	CFI_ENDPROC
 
 ENTRY(coprocessor_error)
+	RING0_INT_FRAME
 	pushl $0
+	CFI_ADJUST_CFA_OFFSET 4
 	pushl $do_coprocessor_error
+	CFI_ADJUST_CFA_OFFSET 4
 	jmp error_code
+	CFI_ENDPROC
 
 ENTRY(simd_coprocessor_error)
+	RING0_INT_FRAME
 	pushl $0
+	CFI_ADJUST_CFA_OFFSET 4
 	pushl $do_simd_coprocessor_error
+	CFI_ADJUST_CFA_OFFSET 4
 	jmp error_code
+	CFI_ENDPROC
 
 ENTRY(device_not_available)
+	RING0_INT_FRAME
 	pushl $-1			# mark this as an int
+	CFI_ADJUST_CFA_OFFSET 4
 	SAVE_ALL
 #ifndef CONFIG_XEN
 	movl %cr0, %eax
 	testl $0x4, %eax		# EM (math emulation bit)
 	je device_available_emulate
 	pushl $0			# temporary storage for ORIG_EIP
+	CFI_ADJUST_CFA_OFFSET 4
 	call math_emulate
 	addl $4, %esp
+	CFI_ADJUST_CFA_OFFSET -4
 	jmp ret_from_exception
 device_available_emulate:
 #endif
 	preempt_stop
 	call math_state_restore
 	jmp ret_from_exception
+	CFI_ENDPROC
 
 #ifndef CONFIG_XEN
 /*
@@ -735,6 +947,7 @@ label:						\
 #endif /* CONFIG_XEN */
 
 KPROBE_ENTRY(debug)
+	RING0_INT_FRAME
 #ifndef CONFIG_XEN
 	cmpl $sysenter_entry,(%esp)
 	jne debug_stack_correct
@@ -742,11 +955,13 @@ KPROBE_ENTRY(debug)
 debug_stack_correct:
 #endif /* !CONFIG_XEN */
 	pushl $-1			# mark this as an int
+	CFI_ADJUST_CFA_OFFSET 4
 	SAVE_ALL
 	xorl %edx,%edx			# error code 0
 	movl %esp,%eax			# pt_regs pointer
 	call do_debug
 	jmp ret_from_exception
+	CFI_ENDPROC
 	.previous .text
 
 #ifndef CONFIG_XEN
@@ -759,14 +974,18 @@ debug_stack_correct:
  * fault happened on the sysenter path.
  */
 ENTRY(nmi)
+	RING0_INT_FRAME
 	pushl %eax
+	CFI_ADJUST_CFA_OFFSET 4
 	movl %ss, %eax
 	cmpw $__ESPFIX_SS, %ax
 	popl %eax
+	CFI_ADJUST_CFA_OFFSET -4
 	je nmi_16bit_stack
 	cmpl $sysenter_entry,(%esp)
 	je nmi_stack_fixup
 	pushl %eax
+	CFI_ADJUST_CFA_OFFSET 4
 	movl %esp,%eax
 	/* Do not access memory above the end of our stack page,
 	 * it might not exist.
@@ -774,16 +993,19 @@ ENTRY(nmi)
 	andl $(THREAD_SIZE-1),%eax
 	cmpl $(THREAD_SIZE-20),%eax
 	popl %eax
+	CFI_ADJUST_CFA_OFFSET -4
 	jae nmi_stack_correct
 	cmpl $sysenter_entry,12(%esp)
 	je nmi_debug_stack_check
 nmi_stack_correct:
 	pushl %eax
+	CFI_ADJUST_CFA_OFFSET 4
 	SAVE_ALL
 	xorl %edx,%edx		# zero error code
 	movl %esp,%eax		# pt_regs pointer
 	call do_nmi
-	jmp restore_all
+	jmp restore_nocheck_notrace
+	CFI_ENDPROC
 
 nmi_stack_fixup:
 	FIX_STACK(12,nmi_stack_correct, 1)
@@ -799,103 +1021,188 @@ nmi_debug_stack_check:
 	jmp nmi_stack_correct
 
 nmi_16bit_stack:
+	RING0_INT_FRAME
 	/* create the pointer to lss back */
 	pushl %ss
+	CFI_ADJUST_CFA_OFFSET 4
 	pushl %esp
+	CFI_ADJUST_CFA_OFFSET 4
 	movzwl %sp, %esp
 	addw $4, (%esp)
 	/* copy the iret frame of 12 bytes */
 	.rept 3
 	pushl 16(%esp)
+	CFI_ADJUST_CFA_OFFSET 4
 	.endr
 	pushl %eax
+	CFI_ADJUST_CFA_OFFSET 4
 	SAVE_ALL
 	FIXUP_ESPFIX_STACK		# %eax == %esp
+	CFI_ADJUST_CFA_OFFSET -20	# the frame has now moved
 	xorl %edx,%edx			# zero error code
 	call do_nmi
 	RESTORE_REGS
 	lss 12+4(%esp), %esp		# back to 16bit stack
 1:	iret
+	CFI_ENDPROC
 .section __ex_table,"a"
 	.align 4
 	.long 1b,iret_exc
 .previous
 #else
 ENTRY(nmi)
+	RING0_INT_FRAME
 	pushl %eax
+	CFI_ADJUST_CFA_OFFSET 4
 	SAVE_ALL
 	xorl %edx,%edx		# zero error code
 	movl %esp,%eax		# pt_regs pointer
 	call do_nmi
 	orl  $NMI_MASK, EFLAGS(%esp)
 	jmp restore_all
+	CFI_ENDPROC
 #endif
 
 KPROBE_ENTRY(int3)
+	RING0_INT_FRAME
 	pushl $-1			# mark this as an int
+	CFI_ADJUST_CFA_OFFSET 4
 	SAVE_ALL
 	xorl %edx,%edx		# zero error code
 	movl %esp,%eax		# pt_regs pointer
 	call do_int3
 	jmp ret_from_exception
+	CFI_ENDPROC
 	.previous .text
 
 ENTRY(overflow)
+	RING0_INT_FRAME
 	pushl $0
+	CFI_ADJUST_CFA_OFFSET 4
 	pushl $do_overflow
+	CFI_ADJUST_CFA_OFFSET 4
 	jmp error_code
+	CFI_ENDPROC
 
 ENTRY(bounds)
+	RING0_INT_FRAME
 	pushl $0
+	CFI_ADJUST_CFA_OFFSET 4
 	pushl $do_bounds
+	CFI_ADJUST_CFA_OFFSET 4
 	jmp error_code
+	CFI_ENDPROC
 
 ENTRY(invalid_op)
+	RING0_INT_FRAME
 	pushl $0
+	CFI_ADJUST_CFA_OFFSET 4
 	pushl $do_invalid_op
+	CFI_ADJUST_CFA_OFFSET 4
 	jmp error_code
+	CFI_ENDPROC
 
 ENTRY(coprocessor_segment_overrun)
+	RING0_INT_FRAME
 	pushl $0
+	CFI_ADJUST_CFA_OFFSET 4
 	pushl $do_coprocessor_segment_overrun
+	CFI_ADJUST_CFA_OFFSET 4
 	jmp error_code
+	CFI_ENDPROC
 
 ENTRY(invalid_TSS)
+	RING0_EC_FRAME
 	pushl $do_invalid_TSS
+	CFI_ADJUST_CFA_OFFSET 4
 	jmp error_code
+	CFI_ENDPROC
 
 ENTRY(segment_not_present)
+	RING0_EC_FRAME
 	pushl $do_segment_not_present
+	CFI_ADJUST_CFA_OFFSET 4
 	jmp error_code
+	CFI_ENDPROC
 
 ENTRY(stack_segment)
+	RING0_EC_FRAME
 	pushl $do_stack_segment
+	CFI_ADJUST_CFA_OFFSET 4
 	jmp error_code
+	CFI_ENDPROC
 
 KPROBE_ENTRY(general_protection)
+	RING0_EC_FRAME
 	pushl $do_general_protection
+	CFI_ADJUST_CFA_OFFSET 4
 	jmp error_code
+	CFI_ENDPROC
 	.previous .text
 
 ENTRY(alignment_check)
+	RING0_EC_FRAME
 	pushl $do_alignment_check
+	CFI_ADJUST_CFA_OFFSET 4
 	jmp error_code
+	CFI_ENDPROC
 
 KPROBE_ENTRY(page_fault)
+	RING0_EC_FRAME
 	pushl $do_page_fault
+	CFI_ADJUST_CFA_OFFSET 4
 	jmp error_code
+	CFI_ENDPROC
 	.previous .text
 
 #ifdef CONFIG_X86_MCE
 ENTRY(machine_check)
+	RING0_INT_FRAME
 	pushl $0
+	CFI_ADJUST_CFA_OFFSET 4
 	pushl machine_check_vector
+	CFI_ADJUST_CFA_OFFSET 4
 	jmp error_code
+	CFI_ENDPROC
 #endif
 
 ENTRY(fixup_4gb_segment)
+	RING0_EC_FRAME
 	pushl $do_fixup_4gb_segment
+	CFI_ADJUST_CFA_OFFSET 4
 	jmp error_code
+	CFI_ENDPROC
+
+#ifdef CONFIG_STACK_UNWIND
+ENTRY(arch_unwind_init_running)
+	CFI_STARTPROC
+	movl	4(%esp), %edx
+	movl	(%esp), %ecx
+	leal	4(%esp), %eax
+	movl	%ebx, EBX(%edx)
+	xorl	%ebx, %ebx
+	movl	%ebx, ECX(%edx)
+	movl	%ebx, EDX(%edx)
+	movl	%esi, ESI(%edx)
+	movl	%edi, EDI(%edx)
+	movl	%ebp, EBP(%edx)
+	movl	%ebx, EAX(%edx)
+	movl	$__USER_DS, DS(%edx)
+	movl	$__USER_DS, ES(%edx)
+	movl	%ebx, ORIG_EAX(%edx)
+	movl	%ecx, EIP(%edx)
+	movl	12(%esp), %ecx
+	movl	$__KERNEL_CS, CS(%edx)
+	movl	%ebx, EFLAGS(%edx)
+	movl	%eax, OLDESP(%edx)
+	movl	8(%esp), %eax
+	movl	%ecx, 8(%esp)
+	movl	EBX(%edx), %ebx
+	movl	$__KERNEL_DS, OLDSS(%edx)
+	jmpl	*%eax
+	CFI_ENDPROC
+ENDPROC(arch_unwind_init_running)
+#endif
 
 .section .rodata,"a"
 #include "syscall_table.S"
Index: head-2006-11-06/arch/i386/kernel/head-xen.S
===================================================================
--- head-2006-11-06.orig/arch/i386/kernel/head-xen.S	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/i386/kernel/head-xen.S	2006-10-16 10:19:17.000000000 +0200
@@ -1,7 +1,6 @@
 
 
 .text
-#include <linux/config.h>
 #include <linux/elfnote.h>
 #include <linux/threads.h>
 #include <linux/linkage.h>
@@ -62,10 +61,8 @@ ENTRY(startup_32)
 	movl %eax,%gs
 	cld			# gcc2 wants the direction flag cleared at all times
 
-	call start_kernel
-L6:
-	jmp L6			# main should never return here, but
-				# just in case, we know what happens.
+	pushl %eax		# fake return address
+	jmp start_kernel
 
 #define HYPERCALL_PAGE_OFFSET 0x1000
 .org HYPERCALL_PAGE_OFFSET
Index: head-2006-11-06/arch/i386/kernel/io_apic-xen.c
===================================================================
--- head-2006-11-06.orig/arch/i386/kernel/io_apic-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/i386/kernel/io_apic-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -25,7 +25,6 @@
 #include <linux/init.h>
 #include <linux/delay.h>
 #include <linux/sched.h>
-#include <linux/config.h>
 #include <linux/smp_lock.h>
 #include <linux/mc146818rtc.h>
 #include <linux/compiler.h>
@@ -38,6 +37,7 @@
 #include <asm/desc.h>
 #include <asm/timer.h>
 #include <asm/i8259.h>
+#include <asm/nmi.h>
 
 #include <mach_apic.h>
 
@@ -310,7 +310,7 @@ static void set_ioapic_affinity_irq(unsi
 # include <linux/slab.h>		/* kmalloc() */
 # include <linux/timer.h>	/* time_after() */
  
-# ifdef CONFIG_BALANCED_IRQ_DEBUG
+#ifdef CONFIG_BALANCED_IRQ_DEBUG
 #  define TDprintk(x...) do { printk("<%ld:%s:%d>: ", jiffies, __FILE__, __LINE__); printk(x); } while (0)
 #  define Dprintk(x...) do { TDprintk(x); } while (0)
 # else
@@ -318,10 +318,15 @@ static void set_ioapic_affinity_irq(unsi
 #  define Dprintk(x...) 
 # endif
 
-
 #define IRQBALANCE_CHECK_ARCH -999
-static int irqbalance_disabled = IRQBALANCE_CHECK_ARCH;
-static int physical_balance = 0;
+#define MAX_BALANCED_IRQ_INTERVAL	(5*HZ)
+#define MIN_BALANCED_IRQ_INTERVAL	(HZ/2)
+#define BALANCED_IRQ_MORE_DELTA		(HZ/10)
+#define BALANCED_IRQ_LESS_DELTA		(HZ)
+
+static int irqbalance_disabled __read_mostly = IRQBALANCE_CHECK_ARCH;
+static int physical_balance __read_mostly;
+static long balanced_irq_interval __read_mostly = MAX_BALANCED_IRQ_INTERVAL;
 
 static struct irq_cpu_info {
 	unsigned long * last_irq;
@@ -340,12 +345,14 @@ static struct irq_cpu_info {
 
 #define CPU_TO_PACKAGEINDEX(i) (first_cpu(cpu_sibling_map[i]))
 
-#define MAX_BALANCED_IRQ_INTERVAL	(5*HZ)
-#define MIN_BALANCED_IRQ_INTERVAL	(HZ/2)
-#define BALANCED_IRQ_MORE_DELTA		(HZ/10)
-#define BALANCED_IRQ_LESS_DELTA		(HZ)
+static cpumask_t balance_irq_affinity[NR_IRQS] = {
+	[0 ... NR_IRQS-1] = CPU_MASK_ALL
+};
 
-static long balanced_irq_interval = MAX_BALANCED_IRQ_INTERVAL;
+void set_balance_irq_affinity(unsigned int irq, cpumask_t mask)
+{
+	balance_irq_affinity[irq] = mask;
+}
 
 static unsigned long move(int curr_cpu, cpumask_t allowed_mask,
 			unsigned long now, int direction)
@@ -383,7 +390,7 @@ static inline void balance_irq(int cpu, 
 	if (irqbalance_disabled)
 		return; 
 
-	cpus_and(allowed_mask, cpu_online_map, irq_affinity[irq]);
+	cpus_and(allowed_mask, cpu_online_map, balance_irq_affinity[irq]);
 	new_cpu = move(cpu, allowed_mask, now, 1);
 	if (cpu != new_cpu) {
 		set_pending_irq(irq, cpumask_of_cpu(new_cpu));
@@ -572,7 +579,9 @@ tryanotherirq:
 		}
 	}
 
-	cpus_and(allowed_mask, cpu_online_map, irq_affinity[selected_irq]);
+	cpus_and(allowed_mask,
+		cpu_online_map,
+		balance_irq_affinity[selected_irq]);
 	target_cpu_mask = cpumask_of_cpu(min_loaded);
 	cpus_and(tmp, target_cpu_mask, allowed_mask);
 
@@ -613,7 +622,7 @@ static int balanced_irq(void *unused)
 	
 	/* push everything to CPU 0 to give us a starting point.  */
 	for (i = 0 ; i < NR_IRQS ; i++) {
-		pending_irq_cpumask[i] = cpumask_of_cpu(0);
+		irq_desc[i].pending_mask = cpumask_of_cpu(0);
 		set_pending_irq(i, cpumask_of_cpu(0));
 	}
 
@@ -1201,7 +1210,7 @@ int assign_irq_vector(int irq)
 {
 	struct physdev_irq irq_op;
 
-	BUG_ON(irq >= NR_IRQ_VECTORS);
+	BUG_ON(irq != AUTO_ASSIGN && (unsigned)irq >= NR_IRQ_VECTORS);
 	if (irq != AUTO_ASSIGN && IO_APIC_VECTOR(irq) > 0)
 		return IO_APIC_VECTOR(irq);
 
@@ -1224,23 +1233,18 @@ static struct hw_interrupt_type ioapic_e
 #define IOAPIC_EDGE	0
 #define IOAPIC_LEVEL	1
 
-static inline void ioapic_register_intr(int irq, int vector, unsigned long trigger)
+static void ioapic_register_intr(int irq, int vector, unsigned long trigger)
 {
-	if (use_pci_vector() && !platform_legacy_irq(irq)) {
-		if ((trigger == IOAPIC_AUTO && IO_APIC_irq_trigger(irq)) ||
-				trigger == IOAPIC_LEVEL)
-			irq_desc[vector].handler = &ioapic_level_type;
-		else
-			irq_desc[vector].handler = &ioapic_edge_type;
-		set_intr_gate(vector, interrupt[vector]);
-	} else	{
-		if ((trigger == IOAPIC_AUTO && IO_APIC_irq_trigger(irq)) ||
-				trigger == IOAPIC_LEVEL)
-			irq_desc[irq].handler = &ioapic_level_type;
-		else
-			irq_desc[irq].handler = &ioapic_edge_type;
-		set_intr_gate(vector, interrupt[irq]);
-	}
+	unsigned idx;
+
+	idx = use_pci_vector() && !platform_legacy_irq(irq) ? vector : irq;
+
+	if ((trigger == IOAPIC_AUTO && IO_APIC_irq_trigger(irq)) ||
+			trigger == IOAPIC_LEVEL)
+		irq_desc[idx].chip = &ioapic_level_type;
+	else
+		irq_desc[idx].chip = &ioapic_edge_type;
+	set_intr_gate(vector, interrupt[idx]);
 }
 #else
 #define ioapic_register_intr(_irq,_vector,_trigger) ((void)0)
@@ -1355,7 +1359,7 @@ static void __init setup_ExtINT_IRQ0_pin
 	 * The timer IRQ doesn't have to know that behind the
 	 * scene we have a 8259A-master in AEOI mode ...
 	 */
-	irq_desc[0].handler = &ioapic_edge_type;
+	irq_desc[0].chip = &ioapic_edge_type;
 
 	/*
 	 * Add it to the IO-APIC irq-routing table:
@@ -2106,6 +2110,13 @@ static void set_ioapic_affinity_vector (
 #endif
 #endif
 
+static int ioapic_retrigger(unsigned int irq)
+{
+	send_IPI_self(IO_APIC_VECTOR(irq));
+
+	return 1;
+}
+
 /*
  * Level and edge triggered IO-APIC interrupts need different handling,
  * so we use two separate IRQ descriptors. Edge triggered IRQs can be
@@ -2125,6 +2136,7 @@ static struct hw_interrupt_type ioapic_e
 #ifdef CONFIG_SMP
 	.set_affinity 	= set_ioapic_affinity,
 #endif
+	.retrigger	= ioapic_retrigger,
 };
 
 static struct hw_interrupt_type ioapic_level_type __read_mostly = {
@@ -2138,6 +2150,7 @@ static struct hw_interrupt_type ioapic_l
 #ifdef CONFIG_SMP
 	.set_affinity 	= set_ioapic_affinity,
 #endif
+	.retrigger	= ioapic_retrigger,
 };
 #endif /* !CONFIG_XEN */
 
@@ -2174,7 +2187,7 @@ static inline void init_IO_APIC_traps(vo
 #ifndef CONFIG_XEN
 			else
 				/* Strange. Oh, well.. */
-				irq_desc[irq].handler = &no_irq_type;
+				irq_desc[irq].chip = &no_irq_type;
 #endif
 		}
 	}
@@ -2392,7 +2405,7 @@ static inline void check_timer(void)
 	printk(KERN_INFO "...trying to set up timer as Virtual Wire IRQ...");
 
 	disable_8259A_irq(0);
-	irq_desc[0].handler = &lapic_irq_type;
+	irq_desc[0].chip = &lapic_irq_type;
 	apic_write_around(APIC_LVT0, APIC_DM_FIXED | vector);	/* Fixed mode */
 	enable_8259A_irq(0);
 
Index: head-2006-11-06/arch/i386/kernel/ioport-xen.c
===================================================================
--- head-2006-11-06.orig/arch/i386/kernel/ioport-xen.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/arch/i386/kernel/ioport-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -79,6 +79,7 @@ asmlinkage long sys_ioperm(unsigned long
 
 		memset(bitmap, 0xff, IO_BITMAP_BYTES);
 		t->io_bitmap_ptr = bitmap;
+		set_thread_flag(TIF_IO_BITMAP);
 
 		set_iobitmap.bitmap   = (char *)bitmap;
 		set_iobitmap.nr_ports = IO_BITMAP_BITS;
Index: head-2006-11-06/arch/i386/kernel/irq-xen.c
===================================================================
--- head-2006-11-06.orig/arch/i386/kernel/irq-xen.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/arch/i386/kernel/irq-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -42,8 +42,8 @@ union irq_ctx {
 	u32                     stack[THREAD_SIZE/sizeof(u32)];
 };
 
-static union irq_ctx *hardirq_ctx[NR_CPUS];
-static union irq_ctx *softirq_ctx[NR_CPUS];
+static union irq_ctx *hardirq_ctx[NR_CPUS] __read_mostly;
+static union irq_ctx *softirq_ctx[NR_CPUS] __read_mostly;
 #endif
 
 /*
@@ -60,6 +60,12 @@ fastcall unsigned int do_IRQ(struct pt_r
 	u32 *isp;
 #endif
 
+	if (unlikely((unsigned)irq >= NR_IRQS)) {
+		printk(KERN_EMERG "%s: cannot handle IRQ %d\n",
+					__FUNCTION__, irq);
+		BUG();
+	}
+
 	irq_enter();
 #ifdef CONFIG_DEBUG_STACKOVERFLOW
 	/* Debugging check for stack overflow: is there less than 1KB free? */
@@ -95,6 +101,14 @@ fastcall unsigned int do_IRQ(struct pt_r
 		irqctx->tinfo.task = curctx->tinfo.task;
 		irqctx->tinfo.previous_esp = current_stack_pointer;
 
+		/*
+		 * Copy the softirq bits in preempt_count so that the
+		 * softirq checks work in the hardirq context.
+		 */
+		irqctx->tinfo.preempt_count =
+			(irqctx->tinfo.preempt_count & ~SOFTIRQ_MASK) |
+			(curctx->tinfo.preempt_count & SOFTIRQ_MASK);
+
 		asm volatile(
 			"       xchgl   %%ebx,%%esp      \n"
 			"       call    __do_IRQ         \n"
@@ -147,7 +161,7 @@ void irq_ctx_init(int cpu)
 	irqctx->tinfo.task              = NULL;
 	irqctx->tinfo.exec_domain       = NULL;
 	irqctx->tinfo.cpu               = cpu;
-	irqctx->tinfo.preempt_count     = SOFTIRQ_OFFSET;
+	irqctx->tinfo.preempt_count     = 0;
 	irqctx->tinfo.addr_limit        = MAKE_MM_SEG(0);
 
 	softirq_ctx[cpu] = irqctx;
@@ -192,6 +206,10 @@ asmlinkage void do_softirq(void)
 			: "0"(isp)
 			: "memory", "cc", "edx", "ecx", "eax"
 		);
+		/*
+		 * Shouldnt happen, we returned above if in_interrupt():
+	 	 */
+		WARN_ON_ONCE(softirq_count());
 	}
 
 	local_irq_restore(flags);
@@ -219,7 +237,7 @@ int show_interrupts(struct seq_file *p, 
 	if (i == 0) {
 		seq_printf(p, "           ");
 		for_each_online_cpu(j)
-			seq_printf(p, "CPU%d       ",j);
+			seq_printf(p, "CPU%-8d",j);
 		seq_putc(p, '\n');
 	}
 
@@ -235,7 +253,7 @@ int show_interrupts(struct seq_file *p, 
 		for_each_online_cpu(j)
 			seq_printf(p, "%10u ", kstat_cpu(j).irqs[i]);
 #endif
-		seq_printf(p, " %14s", irq_desc[i].handler->typename);
+		seq_printf(p, " %14s", irq_desc[i].chip->typename);
 		seq_printf(p, "  %s", action->name);
 
 		for (action=action->next; action; action = action->next)
@@ -276,13 +294,13 @@ void fixup_irqs(cpumask_t map)
 		if (irq == 2)
 			continue;
 
-		cpus_and(mask, irq_affinity[irq], map);
+		cpus_and(mask, irq_desc[irq].affinity, map);
 		if (any_online_cpu(mask) == NR_CPUS) {
 			/*printk("Breaking affinity for irq %i\n", irq);*/
 			mask = map;
 		}
-		if (irq_desc[irq].handler->set_affinity)
-			irq_desc[irq].handler->set_affinity(irq, mask);
+		if (irq_desc[irq].chip->set_affinity)
+			irq_desc[irq].chip->set_affinity(irq, mask);
 		else if (irq_desc[irq].action && !(warned++))
 			printk("Cannot set affinity for irq %i\n", irq);
 	}
Index: head-2006-11-06/arch/i386/kernel/Makefile
===================================================================
--- head-2006-11-06.orig/arch/i386/kernel/Makefile	2006-09-21 10:20:54.000000000 +0200
+++ head-2006-11-06/arch/i386/kernel/Makefile	2006-10-16 10:19:17.000000000 +0200
@@ -74,6 +74,52 @@ $(obj)/vsyscall-%.so: $(src)/vsyscall.ld
 		      $(obj)/vsyscall-%.o $(obj)/$(vsyscall_note) FORCE
 	$(call if_changed,syscall)
 
+ifeq ($(CONFIG_XEN)$(CONFIG_COMPAT_VDSO),yy)
+
+# vsyscall.o also contains the vsyscall DSO relocation info as __initdata.
+# We must build both the alternative images before we can assemble it.
+# Note: kbuild does not track this dependency due to usage of .include
+$(obj)/vsyscall.o: $(obj)/vsyscall-int80.rel $(obj)/vsyscall-sysenter.rel
+targets += $(foreach F,int80 sysenter,vsyscall-$F.so.alt vsyscall-$F.rel)
+targets += vsyscall.lds.alt
+
+# The alternative DSO images are built using an alternate base address.
+quiet_cmd_syscall_alt = REBASE  $@
+      cmd_syscall_alt = sed 's,^\([[:space:]]*\.[[:space:]]*=[[:space:]]*\),\1-0x55AA0000 + ,' $< >$@
+
+quiet_cmd_syscall_rel = COMPARE $@
+      cmd_syscall_rel = set -e; \
+			cmp -l $(basename $<) $< \
+			| { read off1 old1 new1; \
+			    read off2 old2 new2; \
+			    test $$(expr $$off1 + 1) = $$off2; \
+			    echo " .long $$(expr $$off1 - 3)"; \
+			    while read off1 old3 new3; do \
+				read off2 old4 new4; \
+				test $$(expr $$off1 + 1) = $$off2; \
+				test $$old1 = $$old3 -a $$new1 = $$new3; \
+				test $$old2 = $$old4 -a $$new2 = $$new4; \
+				echo " .long $$(expr $$off1 - 3)"; \
+			    done; \
+			  } >$@
+
+SYSCFLAGS_vsyscall-sysenter.so.alt = $(vsyscall-flags)
+SYSCFLAGS_vsyscall-int80.so.alt    = $(vsyscall-flags)
+
+$(obj)/vsyscall.lds.alt: $(obj)/vsyscall.lds FORCE
+	$(call if_changed,syscall_alt)
+
+$(obj)/vsyscall-int80.so.alt $(obj)/vsyscall-sysenter.so.alt: \
+$(obj)/vsyscall-%.so.alt: $(obj)/vsyscall.lds.alt \
+		      $(obj)/vsyscall-%.o $(obj)/$(vsyscall_note) FORCE
+	$(call if_changed,syscall)
+
+$(obj)/vsyscall-int80.rel $(obj)/vsyscall-sysenter.rel: \
+$(obj)/vsyscall-%.rel: $(obj)/vsyscall-%.so.alt FORCE
+	$(call if_changed,syscall_rel)
+
+endif
+
 # We also create a special relocatable object that should mirror the symbol
 # table and layout of the linked DSO.  With ld -R we can then refer to
 # these symbols in the kernel code rather than hand-coded addresses.
@@ -93,7 +139,7 @@ include $(srctree)/scripts/Makefile.xen
 
 obj-y += fixup.o
 microcode-$(subst m,y,$(CONFIG_MICROCODE)) := microcode-xen.o
-n-obj-xen := i8259.o timers/ reboot.o smpboot.o trampoline.o
+n-obj-xen := i8259.o reboot.o smpboot.o trampoline.o tsc.o
 
 obj-y := $(call filterxen, $(obj-y), $(n-obj-xen))
 obj-y := $(call cherrypickxen, $(obj-y))
Index: head-2006-11-06/arch/i386/kernel/microcode-xen.c
===================================================================
--- head-2006-11-06.orig/arch/i386/kernel/microcode-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/i386/kernel/microcode-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -117,7 +117,6 @@ static struct file_operations microcode_
 static struct miscdevice microcode_dev = {
 	.minor		= MICROCODE_MINOR,
 	.name		= "microcode",
-	.devfs_name	= "cpu/microcode",
 	.fops		= &microcode_fops,
 };
 
Index: head-2006-11-06/arch/i386/kernel/mpparse-xen.c
===================================================================
--- head-2006-11-06.orig/arch/i386/kernel/mpparse-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/i386/kernel/mpparse-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -17,7 +17,6 @@
 #include <linux/init.h>
 #include <linux/acpi.h>
 #include <linux/delay.h>
-#include <linux/config.h>
 #include <linux/bootmem.h>
 #include <linux/smp_lock.h>
 #include <linux/kernel_stat.h>
Index: head-2006-11-06/arch/i386/kernel/process-xen.c
===================================================================
--- head-2006-11-06.orig/arch/i386/kernel/process-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/i386/kernel/process-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -28,7 +28,6 @@
 #include <linux/user.h>
 #include <linux/a.out.h>
 #include <linux/interrupt.h>
-#include <linux/config.h>
 #include <linux/utsname.h>
 #include <linux/delay.h>
 #include <linux/reboot.h>
@@ -108,10 +107,10 @@ void xen_idle(void)
 	if (need_resched())
 		local_irq_enable();
 	else {
-		clear_thread_flag(TIF_POLLING_NRFLAG);
+		current_thread_info()->status &= ~TS_POLLING;
 		smp_mb__after_clear_bit();
 		safe_halt();
-		set_thread_flag(TIF_POLLING_NRFLAG);
+		current_thread_info()->status |= TS_POLLING;
 	}
 }
 #ifdef CONFIG_APM_MODULE
@@ -146,7 +145,7 @@ void cpu_idle(void)
 {
 	int cpu = smp_processor_id();
 
-	set_thread_flag(TIF_POLLING_NRFLAG);
+	current_thread_info()->status |= TS_POLLING;
 
 	/* endless idle loop with no priority at all */
 	while (1) {
@@ -228,7 +227,7 @@ void show_regs(struct pt_regs * regs)
 	cr3 = read_cr3();
 	cr4 = read_cr4_safe();
 	printk("CR0: %08lx CR2: %08lx CR3: %08lx CR4: %08lx\n", cr0, cr2, cr3, cr4);
-	show_trace(NULL, &regs->esp);
+	show_trace(NULL, regs, &regs->esp);
 }
 
 /*
@@ -276,15 +275,15 @@ EXPORT_SYMBOL(kernel_thread);
  */
 void exit_thread(void)
 {
-	struct task_struct *tsk = current;
-	struct thread_struct *t = &tsk->thread;
-
 	/* The process may have allocated an io port bitmap... nuke it. */
-	if (unlikely(NULL != t->io_bitmap_ptr)) {
+	if (unlikely(test_thread_flag(TIF_IO_BITMAP))) {
+		struct task_struct *tsk = current;
+		struct thread_struct *t = &tsk->thread;
 		struct physdev_set_iobitmap set_iobitmap = { 0 };
 		HYPERVISOR_physdev_op(PHYSDEVOP_set_iobitmap, &set_iobitmap);
 		kfree(t->io_bitmap_ptr);
 		t->io_bitmap_ptr = NULL;
+		clear_thread_flag(TIF_IO_BITMAP);
 	}
 }
 
@@ -294,6 +293,7 @@ void flush_thread(void)
 
 	memset(tsk->thread.debugreg, 0, sizeof(unsigned long)*8);
 	memset(tsk->thread.tls_array, 0, sizeof(tsk->thread.tls_array));	
+	clear_tsk_thread_flag(tsk, TIF_DEBUG);
 	/*
 	 * Forget coprocessor state..
 	 */
@@ -338,7 +338,7 @@ int copy_thread(int nr, unsigned long cl
 	savesegment(gs,p->thread.gs);
 
 	tsk = current;
-	if (unlikely(NULL != tsk->thread.io_bitmap_ptr)) {
+	if (unlikely(test_tsk_thread_flag(tsk, TIF_IO_BITMAP))) {
 		p->thread.io_bitmap_ptr = kmalloc(IO_BITMAP_BYTES, GFP_KERNEL);
 		if (!p->thread.io_bitmap_ptr) {
 			p->thread.io_bitmap_max = 0;
@@ -346,6 +346,7 @@ int copy_thread(int nr, unsigned long cl
 		}
 		memcpy(p->thread.io_bitmap_ptr, tsk->thread.io_bitmap_ptr,
 			IO_BITMAP_BYTES);
+		set_tsk_thread_flag(p, TIF_IO_BITMAP);
 	}
 
 	/*
@@ -559,7 +560,8 @@ struct task_struct fastcall * __switch_t
 		mcl++;
 	}
 
-	if (unlikely(prev->io_bitmap_ptr || next->io_bitmap_ptr)) {
+	if (unlikely(test_tsk_thread_flag(prev_p, TIF_IO_BITMAP)
+	             || test_tsk_thread_flag(next_p, TIF_IO_BITMAP))) {
 		iobmp_op.bitmap   = (char *)next->io_bitmap_ptr;
 		iobmp_op.nr_ports = next->io_bitmap_ptr ? IO_BITMAP_BITS : 0;
 		mcl->op      = __HYPERVISOR_physdev_op;
@@ -585,7 +587,7 @@ struct task_struct fastcall * __switch_t
 	/*
 	 * Now maybe reload the debug registers
 	 */
-	if (unlikely(next->debugreg[7])) {
+	if (unlikely(test_tsk_thread_flag(next_p, TIF_DEBUG))) {
 		set_debugreg(next->debugreg[0], 0);
 		set_debugreg(next->debugreg[1], 1);
 		set_debugreg(next->debugreg[2], 2);
Index: head-2006-11-06/arch/i386/kernel/quirks-xen.c
===================================================================
--- head-2006-11-06.orig/arch/i386/kernel/quirks-xen.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/arch/i386/kernel/quirks-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -1,7 +1,6 @@
 /*
  * This file contains work-arounds for x86 and x86_64 platform bugs.
  */
-#include <linux/config.h>
 #include <linux/pci.h>
 #include <linux/irq.h>
 
Index: head-2006-11-06/arch/i386/kernel/setup-xen.c
===================================================================
--- head-2006-11-06.orig/arch/i386/kernel/setup-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/i386/kernel/setup-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -23,11 +23,10 @@
  * This file handles the architecture-dependent parts of initialization
  */
 
-#include <linux/config.h>
 #include <linux/sched.h>
 #include <linux/mm.h>
 #include <linux/mmzone.h>
-#include <linux/tty.h>
+#include <linux/screen_info.h>
 #include <linux/ioport.h>
 #include <linux/acpi.h>
 #include <linux/apm_bios.h>
@@ -68,7 +67,7 @@
 #include <xen/interface/memory.h>
 #include <xen/features.h>
 #include <xen/xencons.h>
-#include "setup_arch_pre.h"
+#include <setup_arch.h>
 #include <bios_ebda.h>
 
 /* Forward Declaration. */
@@ -400,8 +399,8 @@ EXPORT_SYMBOL(phys_to_machine_mapping);
 start_info_t *xen_start_info;
 EXPORT_SYMBOL(xen_start_info);
 
-static void __init add_memory_region(unsigned long long start,
-                                  unsigned long long size, int type)
+void __init add_memory_region(unsigned long long start,
+			      unsigned long long size, int type)
 {
 	int x;
 
@@ -522,7 +521,7 @@ static struct change_member *change_poin
 static struct e820entry *overlap_list[E820MAX] __initdata;
 static struct e820entry new_bios[E820MAX] __initdata;
 
-static int __init sanitize_e820_map(struct e820entry * biosmap, char * pnr_map)
+int __init sanitize_e820_map(struct e820entry * biosmap, char * pnr_map)
 {
 	struct change_member *change_tmp;
 	unsigned long current_type, last_type;
@@ -691,7 +690,7 @@ static int __init sanitize_e820_map(stru
  * thinkpad 560x, for example, does not cooperate with the memory
  * detection code.)
  */
-static int __init copy_e820_map(struct e820entry * biosmap, int nr_map)
+int __init copy_e820_map(struct e820entry * biosmap, int nr_map)
 {
 #ifndef CONFIG_XEN
 	/* Only one memory region (or negative)? Ignore it */
@@ -755,12 +754,6 @@ static inline void copy_edd(void)
 }
 #endif
 
-/*
- * Do NOT EVER look at the BIOS memory size location.
- * It does not work on many machines.
- */
-#define LOWMEMSIZE()	(0x9f000)
-
 static void __init parse_cmdline_early (char ** cmdline_p)
 {
 	char c = ' ', *to = command_line, *from = saved_command_line;
@@ -1407,8 +1400,10 @@ legacy_init_iomem_resources(struct e820e
 
 	for (i = 0; i < nr_map; i++) {
 		struct resource *res;
+#ifndef CONFIG_RESOURCES_64BIT
 		if (e820[i].addr + e820[i].size > 0x100000000ULL)
 			continue;
+#endif
 		res = kzalloc(sizeof(struct resource), GFP_ATOMIC);
 		switch (e820[i].type) {
 		case E820_RAM:	res->name = "System RAM"; break;
@@ -1419,7 +1414,10 @@ legacy_init_iomem_resources(struct e820e
 		res->start = e820[i].addr;
 		res->end = res->start + e820[i].size - 1;
 		res->flags = IORESOURCE_MEM | IORESOURCE_BUSY;
-		request_resource(&iomem_resource, res);
+		if (request_resource(&iomem_resource, res)) {
+			kfree(res);
+			continue;
+		}
 		if (e820[i].type == E820_RAM) {
 			/*
 			 *  We don't know which RAM region contains kernel data,
@@ -1546,8 +1544,6 @@ static void __init register_memory(void)
 		e820_setup_gap(e820.map, e820.nr_map);
 }
 
-static char * __init machine_specific_memory_setup(void);
-
 #ifdef CONFIG_MCA
 static void set_mca_bus(int x)
 {
@@ -1814,6 +1810,8 @@ void __init setup_arch(char **cmdline_p)
 		extern int console_use_vt;
 		console_use_vt = 0;
 	}
+
+	tsc_init();
 }
 
 static __init int add_pcspkr(void)
@@ -1844,7 +1842,6 @@ xen_panic_event(struct notifier_block *t
 	return NOTIFY_DONE;
 }
 
-#include "setup_arch_post.h"
 /*
  * Local Variables:
  * mode:c
Index: head-2006-11-06/arch/i386/kernel/smp-xen.c
===================================================================
--- head-2006-11-06.orig/arch/i386/kernel/smp-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/i386/kernel/smp-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -117,7 +117,17 @@ DEFINE_PER_CPU(struct tlb_state, cpu_tlb
 
 static inline int __prepare_ICR (unsigned int shortcut, int vector)
 {
-	return APIC_DM_FIXED | shortcut | vector | APIC_DEST_LOGICAL;
+	unsigned int icr = shortcut | APIC_DEST_LOGICAL;
+
+	switch (vector) {
+	default:
+		icr |= APIC_DM_FIXED | vector;
+		break;
+	case NMI_VECTOR:
+		icr |= APIC_DM_NMI;
+		break;
+	}
+	return icr;
 }
 
 static inline int __prepare_ICR2 (unsigned int mask)
Index: head-2006-11-06/arch/i386/kernel/sysenter.c
===================================================================
--- head-2006-11-06.orig/arch/i386/kernel/sysenter.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/arch/i386/kernel/sysenter.c	2006-10-16 10:19:17.000000000 +0200
@@ -71,6 +71,142 @@ void enable_sep_cpu(void)
 #endif
 }
 
+#if defined(CONFIG_XEN) && defined(CONFIG_COMPAT_VDSO)
+static void __init relocate_vdso(Elf32_Ehdr *ehdr, unsigned long old_base, unsigned long new_base,
+                                 const unsigned long *reloc_start, const unsigned long *reloc_end)
+{
+#if 1
+	const unsigned long *reloc;
+
+	for (reloc = reloc_start; reloc < reloc_end; ++reloc) {
+		unsigned long *ptr = (void *)((unsigned long)ehdr + *reloc);
+
+		*ptr += new_base - old_base;
+	}
+#else
+	unsigned i, ndynsym = 0, szdynsym = 0;
+	unsigned long dynsym = 0;
+
+	BUG_ON(ehdr->e_ident[EI_MAG0] != ELFMAG0);
+	BUG_ON(ehdr->e_ident[EI_MAG1] != ELFMAG1);
+	BUG_ON(ehdr->e_ident[EI_MAG2] != ELFMAG2);
+	BUG_ON(ehdr->e_ident[EI_MAG3] != ELFMAG3);
+	BUG_ON(ehdr->e_ident[EI_CLASS] != ELFCLASS32);
+	BUG_ON(ehdr->e_ident[EI_DATA] != ELFDATA2LSB);
+	BUG_ON(ehdr->e_ehsize < sizeof(*ehdr));
+	ehdr->e_entry += new_base - old_base;
+	BUG_ON(ehdr->e_phentsize < sizeof(Elf32_Phdr));
+	for (i = 0; i < ehdr->e_phnum; ++i) {
+		Elf32_Phdr *phdr = (void *)((unsigned long)ehdr + ehdr->e_phoff + i * ehdr->e_phentsize);
+
+		phdr->p_vaddr += new_base - old_base;
+		switch(phdr->p_type) {
+		case PT_LOAD:
+		case PT_NOTE:
+			break;
+		case PT_DYNAMIC: {
+				Elf32_Dyn *dyn = (void *)(phdr->p_vaddr - new_base + (unsigned long)ehdr);
+				unsigned j;
+
+				for(j = 0; dyn[j].d_tag != DT_NULL; ++j) {
+					switch(dyn[j].d_tag) {
+					case DT_HASH:
+					case DT_STRTAB:
+					case DT_SYMTAB:
+					case 0x6ffffff0: /* DT_VERSYM */
+					case 0x6ffffffc: /* DT_VERDEF */
+						break;
+					case DT_SONAME:
+					case DT_STRSZ:
+					case 0x6ffffffd: /* DT_VERDEFNUM */
+						continue;
+					case DT_SYMENT:
+						szdynsym = dyn[j].d_un.d_val;
+						continue;
+					default:
+						if (dyn[j].d_tag >= 0x60000000 /* OLD_DT_LOOS */
+						    || dyn[j].d_tag < 31 /* DT_ENCODING */
+						    || !(dyn[j].d_tag & 1)) {
+							printk(KERN_WARNING "vDSO dynamic info %u has unsupported tag %08X\n", j, dyn[j].d_tag);
+							WARN_ON(1);
+							continue;
+						}
+						break;
+					}
+					dyn[j].d_un.d_ptr += new_base - old_base;
+					switch(dyn[j].d_tag) {
+					case DT_HASH:
+						ndynsym = ((Elf32_Word *)dyn[j].d_un.d_ptr)[1];
+						break;
+					case DT_SYMTAB:
+						dynsym = dyn[j].d_un.d_ptr;
+						break;
+					}
+				}
+			}
+			break;
+		case PT_GNU_EH_FRAME:
+			/* XXX */
+			break;
+		default:
+			printk(KERN_WARNING "vDSO program header %u has unsupported type %08X\n", i, phdr->p_type);
+			WARN_ON(1);
+			break;
+		}
+	}
+	BUG_ON(ehdr->e_shentsize < sizeof(Elf32_Shdr));
+	BUG_ON(ehdr->e_shnum >= SHN_LORESERVE);
+	for (i = 1; i < ehdr->e_shnum; ++i) {
+		Elf32_Shdr *shdr = (void *)((unsigned long)ehdr + ehdr->e_shoff + i * ehdr->e_shentsize);
+
+		if (!(shdr->sh_flags & SHF_ALLOC))
+			continue;
+		shdr->sh_addr += new_base - old_base;
+		switch(shdr->sh_type) {
+		case SHT_DYNAMIC:
+		case SHT_HASH:
+		case SHT_NOBITS:
+		case SHT_NOTE:
+		case SHT_PROGBITS:
+		case SHT_STRTAB:
+		case 0x6ffffffd: /* SHT_GNU_verdef */
+		case 0x6fffffff: /* SHT_GNU_versym */
+			break;
+		case SHT_DYNSYM:
+			BUG_ON(shdr->sh_entsize < sizeof(Elf32_Sym));
+			if (!szdynsym)
+				szdynsym = shdr->sh_entsize;
+			else
+				WARN_ON(szdynsym != shdr->sh_entsize);
+			if (!ndynsym)
+				ndynsym = shdr->sh_size / szdynsym;
+			else
+				WARN_ON(ndynsym != shdr->sh_size / szdynsym);
+			if (!dynsym)
+				dynsym = shdr->sh_addr;
+			else
+				WARN_ON(dynsym != shdr->sh_addr);
+			break;
+		default:
+			printk(KERN_WARNING "vDSO section %u has unsupported type %08X\n", i, shdr->sh_type);
+			WARN_ON(shdr->sh_size);
+			break;
+		}
+	}
+	dynsym += (unsigned long)ehdr - new_base;
+	for(i = 1; i < ndynsym; ++i) {
+		Elf32_Sym *sym = (void *)(dynsym + i * szdynsym);
+
+		if (sym->st_shndx == SHN_ABS)
+			continue;
+		sym->st_value += new_base - old_base;
+	}
+#endif
+}
+#else
+#define relocate_vdso(ehdr, old, new, start, end) ((void)0)
+#endif
+
 /*
  * These symbols are defined by vsyscall.o to mark the bounds
  * of the ELF DSO images included therein.
@@ -79,18 +215,25 @@ extern const char vsyscall_int80_start, 
 extern const char vsyscall_sysenter_start, vsyscall_sysenter_end;
 static void *syscall_page;
 
+#ifndef CONFIG_XEN
+#define virt_to_machine(x) __pa(x)
+#elif defined(CONFIG_COMPAT_VDSO)
+extern const unsigned long vdso_rel_int80_start[], vdso_rel_int80_end[];
+extern const unsigned long vdso_rel_sysenter_start[], vdso_rel_sysenter_end[];
+#endif
+
 int __init sysenter_setup(void)
 {
 	syscall_page = (void *)get_zeroed_page(GFP_ATOMIC);
 
 #ifdef CONFIG_COMPAT_VDSO
-	__set_fixmap(FIX_VDSO, __pa(syscall_page), PAGE_READONLY);
+	__set_fixmap(FIX_VDSO, virt_to_machine(syscall_page), PAGE_READONLY);
 	printk("Compat vDSO mapped to %08lx.\n", __fix_to_virt(FIX_VDSO));
 #else
 	/*
 	 * In the non-compat case the ELF coredumping code needs the fixmap:
 	 */
-	__set_fixmap(FIX_VDSO, __pa(syscall_page), PAGE_KERNEL_RO);
+	__set_fixmap(FIX_VDSO, virt_to_machine(syscall_page), PAGE_KERNEL_RO);
 #endif
 
 #ifdef CONFIG_XEN
@@ -103,12 +246,16 @@ int __init sysenter_setup(void)
 		memcpy(syscall_page,
 		       &vsyscall_int80_start,
 		       &vsyscall_int80_end - &vsyscall_int80_start);
+		relocate_vdso(syscall_page, VDSO_PRELINK, __fix_to_virt(FIX_VDSO),
+		              vdso_rel_int80_start, vdso_rel_int80_end);
 		return 0;
 	}
 
 	memcpy(syscall_page,
 	       &vsyscall_sysenter_start,
 	       &vsyscall_sysenter_end - &vsyscall_sysenter_start);
+	relocate_vdso(syscall_page, VDSO_PRELINK, __fix_to_virt(FIX_VDSO),
+	              vdso_rel_sysenter_start, vdso_rel_sysenter_end);
 
 	return 0;
 }
Index: head-2006-11-06/arch/i386/kernel/time-xen.c
===================================================================
--- head-2006-11-06.orig/arch/i386/kernel/time-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/i386/kernel/time-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -65,7 +65,6 @@
 #include "mach_time.h"
 
 #include <linux/timex.h>
-#include <linux/config.h>
 
 #include <asm/hpet.h>
 
@@ -97,18 +96,6 @@ extern unsigned long wall_jiffies;
 DEFINE_SPINLOCK(rtc_lock);
 EXPORT_SYMBOL(rtc_lock);
 
-#if defined (__i386__)
-#include <asm/i8253.h>
-#endif
-
-DEFINE_SPINLOCK(i8253_lock);
-EXPORT_SYMBOL(i8253_lock);
-
-extern struct init_timer_opts timer_tsc_init;
-extern struct timer_opts timer_tsc;
-#define timer_none timer_tsc
-struct timer_opts *cur_timer __read_mostly = &timer_tsc;
-
 /* These are peridically updated in shared_info, and then copied here. */
 struct shadow_time_info {
 	u64 tsc_timestamp;     /* TSC at last update of time vals.  */
@@ -166,23 +153,7 @@ static int __init __permitted_clock_jitt
 }
 __setup("permitted_clock_jitter=", __permitted_clock_jitter);
 
-int tsc_disable __devinitdata = 0;
-
-static void delay_tsc(unsigned long loops)
-{
-	unsigned long bclock, now;
-
-	rdtscl(bclock);
-	do {
-		rep_nop();
-		rdtscl(now);
-	} while ((now - bclock) < loops);
-}
-
-struct timer_opts timer_tsc = {
-	.name = "tsc",
-	.delay = delay_tsc,
-};
+int tsc_disable __cpuinitdata = 0;
 
 /*
  * Scale a 64-bit delta by scaling and multiplying by a 32-bit fraction,
@@ -220,14 +191,6 @@ static inline u64 scale_delta(u64 delta,
 	return product;
 }
 
-#if defined (__i386__)
-int read_current_timer(unsigned long *timer_val)
-{
-	rdtscl(*timer_val);
-	return 0;
-}
-#endif
-
 void init_cpu_khz(void)
 {
 	u64 __cpu_khz = 1000000ULL << 32;
@@ -248,6 +211,7 @@ static u64 get_nsec_offset(struct shadow
 	return scale_delta(delta, shadow->tsc_to_nsec_mul, shadow->tsc_shift);
 }
 
+#ifdef CONFIG_X86_64
 static unsigned long get_usec_offset(struct shadow_time_info *shadow)
 {
 	u64 now, delta;
@@ -255,6 +219,7 @@ static unsigned long get_usec_offset(str
 	delta = now - shadow->tsc_timestamp;
 	return scale_delta(delta, shadow->tsc_to_usec_mul, shadow->tsc_shift);
 }
+#endif
 
 static void __update_wallclock(time_t sec, long nsec)
 {
@@ -365,6 +330,8 @@ void rtc_cmos_write(unsigned char val, u
 }
 EXPORT_SYMBOL(rtc_cmos_write);
 
+#ifdef CONFIG_X86_64
+
 /*
  * This version of gettimeofday has microsecond resolution
  * and better than microsecond precision on fast x86 machines with TSC.
@@ -493,6 +460,8 @@ int do_settimeofday(struct timespec *tv)
 
 EXPORT_SYMBOL(do_settimeofday);
 
+#endif
+
 static void sync_xen_wallclock(unsigned long dummy);
 static DEFINE_TIMER(sync_xen_wallclock_timer, sync_xen_wallclock, 0, 0);
 static void sync_xen_wallclock(unsigned long dummy)
@@ -527,28 +496,32 @@ static void sync_xen_wallclock(unsigned 
 static int set_rtc_mmss(unsigned long nowtime)
 {
 	int retval;
-
-	WARN_ON(irqs_disabled());
+	unsigned long flags;
 
 	if (independent_wallclock || !is_initial_xendomain())
 		return 0;
 
 	/* gets recalled with irq locally disabled */
-	spin_lock_irq(&rtc_lock);
+	/* XXX - does irqsave resolve this? -johnstul */
+	spin_lock_irqsave(&rtc_lock, flags);
 	if (efi_enabled)
 		retval = efi_set_rtc_mmss(nowtime);
 	else
 		retval = mach_set_rtc_mmss(nowtime);
-	spin_unlock_irq(&rtc_lock);
+	spin_unlock_irqrestore(&rtc_lock, flags);
 
 	return retval;
 }
 
+#ifdef CONFIG_X86_64
 /* monotonic_clock(): returns # of nanoseconds passed since time_init()
  *		Note: This function is required to return accurate
  *		time even in the absence of multiple timer ticks.
  */
 unsigned long long monotonic_clock(void)
+#else
+unsigned long long sched_clock(void)
+#endif
 {
 	int cpu = get_cpu();
 	struct shadow_time_info *shadow = &per_cpu(shadow_time, cpu);
@@ -568,12 +541,14 @@ unsigned long long monotonic_clock(void)
 
 	return time;
 }
+#ifdef CONFIG_X86_64
 EXPORT_SYMBOL(monotonic_clock);
 
 unsigned long long sched_clock(void)
 {
 	return monotonic_clock();
 }
+#endif
 
 #if defined(CONFIG_SMP) && defined(CONFIG_FRAME_POINTER)
 unsigned long profile_pc(struct pt_regs *regs)
@@ -587,7 +562,7 @@ unsigned long profile_pc(struct pt_regs 
 	   is just accounted to the spinlock function.
 	   Better would be to write these functions in assembler again
 	   and check exactly. */
-	if (in_lock_functions(pc)) {
+	if (!user_mode(regs) && in_lock_functions(pc)) {
 		char *v = *(char **)regs->rsp;
 		if ((v >= _stext && v <= _etext) ||
 			(v >= _sinittext && v <= _einittext) ||
@@ -596,7 +571,7 @@ unsigned long profile_pc(struct pt_regs 
 		return ((unsigned long *)regs->rsp)[1];
 	}
 #else
-	if (in_lock_functions(pc))
+	if (!user_mode_vm(regs) && in_lock_functions(pc))
 		return *(unsigned long *)(regs->ebp + 4);
 #endif
 
@@ -741,15 +716,16 @@ static void init_missing_ticks_accountin
 unsigned long get_cmos_time(void)
 {
 	unsigned long retval;
+	unsigned long flags;
 
-	spin_lock(&rtc_lock);
+	spin_lock_irqsave(&rtc_lock, flags);
 
 	if (efi_enabled)
 		retval = efi_get_time();
 	else
 		retval = mach_get_cmos_time();
 
-	spin_unlock(&rtc_lock);
+	spin_unlock_irqrestore(&rtc_lock, flags);
 
 	return retval;
 }
@@ -807,7 +783,6 @@ void notify_arch_cmos_timer(void)
 
 static long clock_cmos_diff, sleep_start;
 
-static struct timer_opts *last_timer;
 static int timer_suspend(struct sys_device *dev, pm_message_t state)
 {
 	/*
@@ -816,10 +791,6 @@ static int timer_suspend(struct sys_devi
 	clock_cmos_diff = -get_cmos_time();
 	clock_cmos_diff += get_seconds();
 	sleep_start = get_cmos_time();
-	last_timer = cur_timer;
-	cur_timer = &timer_none;
-	if (last_timer->suspend)
-		last_timer->suspend(state);
 	return 0;
 }
 
@@ -841,10 +812,6 @@ static int timer_resume(struct sys_devic
 	jiffies_64 += sleep_length;
 	wall_jiffies += sleep_length;
 	write_sequnlock_irqrestore(&xtime_lock, flags);
-	if (last_timer->resume)
-		last_timer->resume();
-	cur_timer = last_timer;
-	last_timer = NULL;
 	touch_softlockup_watchdog();
 	return 0;
 }
@@ -886,9 +853,6 @@ static void __init hpet_time_init(void)
 		printk("Using HPET for base-timer\n");
 	}
 
-	cur_timer = select_timer();
-	printk(KERN_INFO "Using %s for high-res timesource\n",cur_timer->name);
-
 	time_init_hook();
 }
 #endif
@@ -930,11 +894,11 @@ void __init time_init(void)
 
 	update_wallclock();
 
+#if defined(__x86_64__)
 	init_cpu_khz();
 	printk(KERN_INFO "Xen reported: %u.%03u MHz processor.\n",
 	       cpu_khz / 1000, cpu_khz % 1000);
 
-#if defined(__x86_64__)
 	vxtime.mode = VXTIME_TSC;
 	vxtime.quot = (1000000L << 32) / vxtime_hz;
 	vxtime.tsc_quot = (1000L << 32) / cpu_khz;
@@ -1009,14 +973,14 @@ static void start_hz_timer(void)
 	cpu_clear(smp_processor_id(), nohz_cpu_mask);
 }
 
-void safe_halt(void)
+void raw_safe_halt(void)
 {
 	stop_hz_timer();
 	/* Blocking includes an implicit local_irq_enable(). */
 	HYPERVISOR_block();
 	start_hz_timer();
 }
-EXPORT_SYMBOL(safe_halt);
+EXPORT_SYMBOL(raw_safe_halt);
 
 void halt(void)
 {
@@ -1075,6 +1039,58 @@ void local_teardown_timer(unsigned int c
 }
 #endif
 
+#ifndef CONFIG_X86_64
+
+void tsc_init(void)
+{
+	init_cpu_khz();
+	printk(KERN_INFO "Xen reported: %u.%03u MHz processor.\n",
+	       cpu_khz / 1000, cpu_khz % 1000);
+
+	use_tsc_delay();
+}
+
+#include <linux/clocksource.h>
+
+void mark_tsc_unstable(void)
+{
+#ifndef CONFIG_XEN /* XXX Should tell the hypervisor about this fact. */
+	tsc_unstable = 1;
+#endif
+}
+EXPORT_SYMBOL_GPL(mark_tsc_unstable);
+
+static cycle_t read_tsc(void)
+{
+	cycle_t ret;
+
+	rdtscll(ret);
+
+	return ret;
+}
+
+static struct clocksource clocksource_xen = {
+	.name			= "xen",
+	.rating			= 300,
+	.read			= read_tsc,
+	.mask			= CLOCKSOURCE_MASK(64),
+	.mult			= 0, /* to be set */
+	.shift			= 22,
+	.is_continuous		= 1,
+};
+
+static int __init init_xen_clocksource(void)
+{
+	clocksource_xen.mult = clocksource_khz2mult(cpu_khz,
+						clocksource_xen.shift);
+
+	return clocksource_register(&clocksource_xen);
+}
+
+module_init(init_xen_clocksource);
+
+#endif
+
 /*
  * /proc/sys/xen: This really belongs in another file. It can stay here for
  * now however.
Index: head-2006-11-06/arch/i386/kernel/traps-xen.c
===================================================================
--- head-2006-11-06.orig/arch/i386/kernel/traps-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/i386/kernel/traps-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -11,7 +11,6 @@
  * 'Traps.c' handles hardware traps and faults after we have saved some
  * state in 'asm.s'.
  */
-#include <linux/config.h>
 #include <linux/sched.h>
 #include <linux/kernel.h>
 #include <linux/string.h>
@@ -28,6 +27,7 @@
 #include <linux/utsname.h>
 #include <linux/kprobes.h>
 #include <linux/kexec.h>
+#include <linux/unwind.h>
 
 #ifdef CONFIG_EISA
 #include <linux/ioport.h>
@@ -47,7 +47,7 @@
 #include <asm/desc.h>
 #include <asm/i387.h>
 #include <asm/nmi.h>
-
+#include <asm/unwind.h>
 #include <asm/smp.h>
 #include <asm/arch_hooks.h>
 #include <asm/kdebug.h>
@@ -98,6 +98,11 @@ asmlinkage void fixup_4gb_segment(void);
 asmlinkage void machine_check(void);
 
 static int kstack_depth_to_print = 24;
+#ifdef CONFIG_STACK_UNWIND
+static int call_trace = 1;
+#else
+#define call_trace (-1)
+#endif
 ATOMIC_NOTIFIER_HEAD(i386die_chain);
 
 int register_die_notifier(struct notifier_block *nb)
@@ -105,13 +110,13 @@ int register_die_notifier(struct notifie
 	vmalloc_sync_all();
 	return atomic_notifier_chain_register(&i386die_chain, nb);
 }
-EXPORT_SYMBOL(register_die_notifier);
+EXPORT_SYMBOL(register_die_notifier); /* used modular by kdb */
 
 int unregister_die_notifier(struct notifier_block *nb)
 {
 	return atomic_notifier_chain_unregister(&i386die_chain, nb);
 }
-EXPORT_SYMBOL(unregister_die_notifier);
+EXPORT_SYMBOL(unregister_die_notifier); /* used modular by kdb */
 
 static inline int valid_stack_ptr(struct thread_info *tinfo, void *p)
 {
@@ -120,28 +125,13 @@ static inline int valid_stack_ptr(struct
 }
 
 /*
- * Print CONFIG_STACK_BACKTRACE_COLS address/symbol entries per line.
+ * Print one address/symbol entries per line.
  */
-static inline int print_addr_and_symbol(unsigned long addr, char *log_lvl,
-					int printed)
+static inline void print_addr_and_symbol(unsigned long addr, char *log_lvl)
 {
-	if (!printed)
-		printk(log_lvl);
-
-#if CONFIG_STACK_BACKTRACE_COLS == 1
 	printk(" [<%08lx>] ", addr);
-#else
-	printk(" <%08lx> ", addr);
-#endif
-	print_symbol("%s", addr);
-
-	printed = (printed + 1) % CONFIG_STACK_BACKTRACE_COLS;
-	if (printed)
-		printk(" ");
-	else
-		printk("\n");
 
-	return printed;
+	print_symbol("%s\n", addr);
 }
 
 static inline unsigned long print_context_stack(struct thread_info *tinfo,
@@ -149,28 +139,44 @@ static inline unsigned long print_contex
 				char *log_lvl)
 {
 	unsigned long addr;
-	int printed = 0; /* nr of entries already printed on current line */
 
 #ifdef	CONFIG_FRAME_POINTER
 	while (valid_stack_ptr(tinfo, (void *)ebp)) {
 		addr = *(unsigned long *)(ebp + 4);
-		printed = print_addr_and_symbol(addr, log_lvl, printed);
+		print_addr_and_symbol(addr, log_lvl);
+		/*
+		 * break out of recursive entries (such as
+		 * end_of_stack_stop_unwind_function):
+	 	 */
+		if (ebp == *(unsigned long *)ebp)
+			break;
 		ebp = *(unsigned long *)ebp;
 	}
 #else
 	while (valid_stack_ptr(tinfo, stack)) {
 		addr = *stack++;
 		if (__kernel_text_address(addr))
-			printed = print_addr_and_symbol(addr, log_lvl, printed);
+			print_addr_and_symbol(addr, log_lvl);
 	}
 #endif
-	if (printed)
-		printk("\n");
-
 	return ebp;
 }
 
-static void show_trace_log_lvl(struct task_struct *task,
+static asmlinkage int
+show_trace_unwind(struct unwind_frame_info *info, void *log_lvl)
+{
+	int n = 0;
+
+	while (unwind(info) == 0 && UNW_PC(info)) {
+		n++;
+		print_addr_and_symbol(UNW_PC(info), log_lvl);
+		if (arch_unw_user_mode(info))
+			break;
+	}
+	return n;
+}
+
+static void show_trace_log_lvl(struct task_struct *task, struct pt_regs *regs,
 			       unsigned long *stack, char *log_lvl)
 {
 	unsigned long ebp;
@@ -178,6 +184,36 @@ static void show_trace_log_lvl(struct ta
 	if (!task)
 		task = current;
 
+	if (call_trace >= 0) {
+		int unw_ret = 0;
+		struct unwind_frame_info info;
+
+		if (regs) {
+			if (unwind_init_frame_info(&info, task, regs) == 0)
+				unw_ret = show_trace_unwind(&info, log_lvl);
+		} else if (task == current)
+			unw_ret = unwind_init_running(&info, show_trace_unwind, log_lvl);
+		else {
+			if (unwind_init_blocked(&info, task) == 0)
+				unw_ret = show_trace_unwind(&info, log_lvl);
+		}
+		if (unw_ret > 0) {
+			if (call_trace == 1 && !arch_unw_user_mode(&info)) {
+				print_symbol("DWARF2 unwinder stuck at %s\n",
+					     UNW_PC(&info));
+				if (UNW_SP(&info) >= PAGE_OFFSET) {
+					printk("Leftover inexact backtrace:\n");
+					stack = (void *)UNW_SP(&info);
+				} else
+					printk("Full inexact backtrace again:\n");
+			} else if (call_trace >= 1)
+				return;
+			else
+				printk("Full inexact backtrace again:\n");
+		} else
+			printk("Inexact backtrace:\n");
+	}
+
 	if (task == current) {
 		/* Grab ebp right from our regs */
 		asm ("movl %%ebp, %0" : "=r" (ebp) : );
@@ -198,13 +234,13 @@ static void show_trace_log_lvl(struct ta
 	}
 }
 
-void show_trace(struct task_struct *task, unsigned long * stack)
+void show_trace(struct task_struct *task, struct pt_regs *regs, unsigned long * stack)
 {
-	show_trace_log_lvl(task, stack, "");
+	show_trace_log_lvl(task, regs, stack, "");
 }
 
-static void show_stack_log_lvl(struct task_struct *task, unsigned long *esp,
-			       char *log_lvl)
+static void show_stack_log_lvl(struct task_struct *task, struct pt_regs *regs,
+			       unsigned long *esp, char *log_lvl)
 {
 	unsigned long *stack;
 	int i;
@@ -225,13 +261,13 @@ static void show_stack_log_lvl(struct ta
 		printk("%08lx ", *stack++);
 	}
 	printk("\n%sCall Trace:\n", log_lvl);
-	show_trace_log_lvl(task, esp, log_lvl);
+	show_trace_log_lvl(task, regs, esp, log_lvl);
 }
 
 void show_stack(struct task_struct *task, unsigned long *esp)
 {
 	printk("       ");
-	show_stack_log_lvl(task, esp, "");
+	show_stack_log_lvl(task, NULL, esp, "");
 }
 
 /*
@@ -241,7 +277,7 @@ void dump_stack(void)
 {
 	unsigned long stack;
 
-	show_trace(current, &stack);
+	show_trace(current, NULL, &stack);
 }
 
 EXPORT_SYMBOL(dump_stack);
@@ -274,8 +310,9 @@ void show_registers(struct pt_regs *regs
 		regs->esi, regs->edi, regs->ebp, esp);
 	printk(KERN_EMERG "ds: %04x   es: %04x   ss: %04x\n",
 		regs->xds & 0xffff, regs->xes & 0xffff, ss);
-	printk(KERN_EMERG "Process %s (pid: %d, threadinfo=%p task=%p)",
-		current->comm, current->pid, current_thread_info(), current);
+	printk(KERN_EMERG "Process %.*s (pid: %d, ti=%p task=%p task.ti=%p)",
+		TASK_COMM_LEN, current->comm, current->pid,
+		current_thread_info(), current, current->thread_info);
 	/*
 	 * When in-kernel, we also print out the stack and code at the
 	 * time of the fault..
@@ -284,7 +321,7 @@ void show_registers(struct pt_regs *regs
 		u8 __user *eip;
 
 		printk("\n" KERN_EMERG "Stack: ");
-		show_stack_log_lvl(NULL, (unsigned long *)esp, KERN_EMERG);
+		show_stack_log_lvl(NULL, regs, (unsigned long *)esp, KERN_EMERG);
 
 		printk(KERN_EMERG "Code: ");
 
@@ -307,35 +344,35 @@ void show_registers(struct pt_regs *regs
 
 static void handle_BUG(struct pt_regs *regs)
 {
+	unsigned long eip = regs->eip;
 	unsigned short ud2;
-	unsigned short line;
-	char *file;
-	char c;
-	unsigned long eip;
-
-	eip = regs->eip;
 
 	if (eip < PAGE_OFFSET)
-		goto no_bug;
+		return;
 	if (__get_user(ud2, (unsigned short __user *)eip))
-		goto no_bug;
+		return;
 	if (ud2 != 0x0b0f)
-		goto no_bug;
-	if (__get_user(line, (unsigned short __user *)(eip + 2)))
-		goto bug;
-	if (__get_user(file, (char * __user *)(eip + 4)) ||
-		(unsigned long)file < PAGE_OFFSET || __get_user(c, file))
-		file = "<bad filename>";
+		return;
 
 	printk(KERN_EMERG "------------[ cut here ]------------\n");
-	printk(KERN_EMERG "kernel BUG at %s:%d!\n", file, line);
 
-no_bug:
-	return;
+#ifdef CONFIG_DEBUG_BUGVERBOSE
+	do {
+		unsigned short line;
+		char *file;
+		char c;
+
+		if (__get_user(line, (unsigned short __user *)(eip + 2)))
+			break;
+		if (__get_user(file, (char * __user *)(eip + 4)) ||
+		    (unsigned long)file < PAGE_OFFSET || __get_user(c, file))
+			file = "<bad filename>";
 
-	/* Here we know it was a BUG but file-n-line is unavailable */
-bug:
-	printk(KERN_EMERG "Kernel BUG\n");
+		printk(KERN_EMERG "kernel BUG at %s:%d!\n", file, line);
+		return;
+	} while (0);
+#endif
+	printk(KERN_EMERG "Kernel BUG at [verbose debug info unavailable]\n");
 }
 
 /* This is gone through when something in the kernel
@@ -425,11 +462,9 @@ void die(const char * str, struct pt_reg
 	if (in_interrupt())
 		panic("Fatal exception in interrupt");
 
-	if (panic_on_oops) {
-		printk(KERN_EMERG "Fatal exception: panic in 5 seconds\n");
-		ssleep(5);
+	if (panic_on_oops)
 		panic("Fatal exception");
-	}
+
 	oops_exit();
 	do_exit(SIGSEGV);
 }
@@ -1133,3 +1168,19 @@ static int __init kstack_setup(char *s)
 	return 1;
 }
 __setup("kstack=", kstack_setup);
+
+#ifdef CONFIG_STACK_UNWIND
+static int __init call_trace_setup(char *s)
+{
+	if (strcmp(s, "old") == 0)
+		call_trace = -1;
+	else if (strcmp(s, "both") == 0)
+		call_trace = 0;
+	else if (strcmp(s, "newfallback") == 0)
+		call_trace = 1;
+	else if (strcmp(s, "new") == 2)
+		call_trace = 2;
+	return 1;
+}
+__setup("call_trace=", call_trace_setup);
+#endif
Index: head-2006-11-06/arch/i386/kernel/vsyscall.S
===================================================================
--- head-2006-11-06.orig/arch/i386/kernel/vsyscall.S	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/arch/i386/kernel/vsyscall.S	2006-10-16 10:19:17.000000000 +0200
@@ -12,4 +12,20 @@ vsyscall_sysenter_start:
 	.incbin "arch/i386/kernel/vsyscall-sysenter.so"
 vsyscall_sysenter_end:
 
+#if defined(CONFIG_XEN) && defined(CONFIG_COMPAT_VDSO)
+
+	.align 4
+
+	.globl vdso_rel_int80_start, vdso_rel_int80_end
+vdso_rel_int80_start:
+	.include "arch/i386/kernel/vsyscall-int80.rel"
+vdso_rel_int80_end:
+
+	.globl vdso_rel_sysenter_start, vdso_rel_sysenter_end
+vdso_rel_sysenter_start:
+	.include "arch/i386/kernel/vsyscall-sysenter.rel"
+vdso_rel_sysenter_end:
+
+#endif
+
 __FINIT
Index: head-2006-11-06/arch/i386/mach-xen/irqflags.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ head-2006-11-06/arch/i386/mach-xen/irqflags.c	2006-10-16 10:19:17.000000000 +0200
@@ -0,0 +1,99 @@
+#include <linux/module.h>
+#include <linux/smp.h>
+#include <asm/irqflags.h>
+#include <asm/hypervisor.h>
+
+/* interrupt control.. */
+
+/*
+ * The use of 'barrier' in the following reflects their use as local-lock
+ * operations. Reentrancy must be prevented (e.g., __cli()) /before/ following
+ * critical operations are executed. All critical operations must complete
+ * /before/ reentrancy is permitted (e.g., __sti()). Alpha architecture also
+ * includes these barriers, for example.
+ */
+
+unsigned long __raw_local_save_flags(void)
+{
+	struct vcpu_info *_vcpu;
+	unsigned long flags;
+
+	preempt_disable();
+	_vcpu = &HYPERVISOR_shared_info->vcpu_info[__vcpu_id];
+	flags = _vcpu->evtchn_upcall_mask;
+	preempt_enable();
+
+	return flags;
+}
+EXPORT_SYMBOL(__raw_local_save_flags);
+
+void raw_local_irq_restore(unsigned long flags)
+{
+	struct vcpu_info *_vcpu;
+
+	preempt_disable();
+	_vcpu = &HYPERVISOR_shared_info->vcpu_info[__vcpu_id];
+	if ((_vcpu->evtchn_upcall_mask = flags) == 0) {
+		barrier(); /* unmask then check (avoid races) */
+		if (unlikely(_vcpu->evtchn_upcall_pending))
+			force_evtchn_callback();
+		preempt_enable();
+	} else
+		preempt_enable_no_resched();
+
+}
+EXPORT_SYMBOL(raw_local_irq_restore);
+
+void raw_local_irq_disable(void)
+{
+	struct vcpu_info *_vcpu;
+
+	preempt_disable();
+	_vcpu = &HYPERVISOR_shared_info->vcpu_info[__vcpu_id];
+	_vcpu->evtchn_upcall_mask = 1;
+	preempt_enable_no_resched();
+}
+EXPORT_SYMBOL(raw_local_irq_disable);
+
+void raw_local_irq_enable(void)
+{
+	struct vcpu_info *_vcpu;
+
+	preempt_disable();
+	_vcpu = &HYPERVISOR_shared_info->vcpu_info[__vcpu_id];
+	_vcpu->evtchn_upcall_mask = 0;
+	barrier(); /* unmask then check (avoid races) */
+	if (unlikely(_vcpu->evtchn_upcall_pending))
+		force_evtchn_callback();
+	preempt_enable();
+}
+EXPORT_SYMBOL(raw_local_irq_enable);
+
+/* Cannot use preempt_enable() here as we would recurse in preempt_sched(). */
+int raw_irqs_disabled(void)
+{
+	struct vcpu_info *_vcpu;
+	int disabled;
+
+	preempt_disable();
+	_vcpu = &HYPERVISOR_shared_info->vcpu_info[__vcpu_id];
+	disabled = (_vcpu->evtchn_upcall_mask != 0);
+	preempt_enable_no_resched();
+	return disabled;
+}
+EXPORT_SYMBOL(raw_irqs_disabled);
+
+unsigned long __raw_local_irq_save(void)
+{
+	struct vcpu_info *_vcpu;
+	unsigned long flags;
+
+	preempt_disable();
+	_vcpu = &HYPERVISOR_shared_info->vcpu_info[__vcpu_id];
+	flags = _vcpu->evtchn_upcall_mask;
+	_vcpu->evtchn_upcall_mask = 1;
+	preempt_enable_no_resched();
+
+	return flags;
+}
+EXPORT_SYMBOL(__raw_local_irq_save);
Index: head-2006-11-06/arch/i386/mach-xen/Makefile
===================================================================
--- head-2006-11-06.orig/arch/i386/mach-xen/Makefile	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/arch/i386/mach-xen/Makefile	2006-10-16 10:19:17.000000000 +0200
@@ -2,4 +2,4 @@
 # Makefile for the linux kernel.
 #
 
-obj-y				:= setup.o
+obj-y				:= setup.o irqflags.o
Index: head-2006-11-06/arch/i386/mach-xen/setup.c
===================================================================
--- head-2006-11-06.orig/arch/i386/mach-xen/setup.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/arch/i386/mach-xen/setup.c	2006-10-16 10:19:17.000000000 +0200
@@ -2,12 +2,17 @@
  *	Machine specific setup for generic
  */
 
-#include <linux/config.h>
 #include <linux/smp.h>
 #include <linux/init.h>
 #include <linux/interrupt.h>
+#include <linux/module.h>
 #include <asm/acpi.h>
 #include <asm/arch_hooks.h>
+#include <asm/e820.h>
+#include <asm/fixmap.h>
+#include <asm/setup.h>
+#include <xen/interface/callback.h>
+#include <xen/interface/memory.h>
 
 #ifdef CONFIG_HOTPLUG_CPU
 #define DEFAULT_SEND_IPI	(1)
@@ -35,3 +40,94 @@ static int __init print_ipi_mode(void)
 }
 
 late_initcall(print_ipi_mode);
+
+char * __init machine_specific_memory_setup(void)
+{
+	int rc;
+	struct xen_memory_map memmap;
+	/*
+	 * This is rather large for a stack variable but this early in
+	 * the boot process we know we have plenty slack space.
+	 */
+	struct e820entry map[E820MAX];
+
+	memmap.nr_entries = E820MAX;
+	set_xen_guest_handle(memmap.buffer, map);
+
+	rc = HYPERVISOR_memory_op(XENMEM_memory_map, &memmap);
+	if ( rc == -ENOSYS ) {
+		memmap.nr_entries = 1;
+		map[0].addr = 0ULL;
+		map[0].size = PFN_PHYS((unsigned long long)xen_start_info->nr_pages);
+		/* 8MB slack (to balance backend allocations). */
+		map[0].size += 8ULL << 20;
+		map[0].type = E820_RAM;
+		rc = 0;
+	}
+	BUG_ON(rc);
+
+	sanitize_e820_map(map, (char *)&memmap.nr_entries);
+
+	BUG_ON(copy_e820_map(map, (char)memmap.nr_entries) < 0);
+
+	return "Xen";
+}
+
+extern void hypervisor_callback(void);
+extern void failsafe_callback(void);
+extern void nmi(void);
+
+unsigned long *machine_to_phys_mapping;
+EXPORT_SYMBOL(machine_to_phys_mapping);
+unsigned int machine_to_phys_order;
+EXPORT_SYMBOL(machine_to_phys_order);
+
+void __init machine_specific_arch_setup(void)
+{
+	int ret;
+	struct xen_machphys_mapping mapping;
+	unsigned long machine_to_phys_nr_ents;
+	struct xen_platform_parameters pp;
+	struct callback_register event = {
+		.type = CALLBACKTYPE_event,
+		.address = { __KERNEL_CS, (unsigned long)hypervisor_callback },
+	};
+	struct callback_register failsafe = {
+		.type = CALLBACKTYPE_failsafe,
+		.address = { __KERNEL_CS, (unsigned long)failsafe_callback },
+	};
+	struct callback_register nmi_cb = {
+		.type = CALLBACKTYPE_nmi,
+		.address = { __KERNEL_CS, (unsigned long)nmi },
+	};
+
+	ret = HYPERVISOR_callback_op(CALLBACKOP_register, &event);
+	if (ret == 0)
+		ret = HYPERVISOR_callback_op(CALLBACKOP_register, &failsafe);
+	if (ret == -ENOSYS)
+		ret = HYPERVISOR_set_callbacks(
+			event.address.cs, event.address.eip,
+			failsafe.address.cs, failsafe.address.eip);
+	BUG_ON(ret);
+
+	ret = HYPERVISOR_callback_op(CALLBACKOP_register, &nmi_cb);
+	if (ret == -ENOSYS) {
+		struct xennmi_callback cb;
+
+		cb.handler_address = nmi_cb.address.eip;
+		HYPERVISOR_nmi_op(XENNMI_register_callback, &cb);
+	}
+
+	if (HYPERVISOR_xen_version(XENVER_platform_parameters,
+				   &pp) == 0)
+		set_fixaddr_top(pp.virt_start - PAGE_SIZE);
+
+	machine_to_phys_mapping = (unsigned long *)MACH2PHYS_VIRT_START;
+	machine_to_phys_nr_ents = MACH2PHYS_NR_ENTRIES;
+	if (HYPERVISOR_memory_op(XENMEM_machphys_mapping, &mapping) == 0) {
+		machine_to_phys_mapping = (unsigned long *)mapping.v_start;
+		machine_to_phys_nr_ents = mapping.max_mfn + 1;
+	}
+	while ((1UL << machine_to_phys_order) < machine_to_phys_nr_ents )
+		machine_to_phys_order++;
+}
Index: head-2006-11-06/arch/i386/mm/fault-xen.c
===================================================================
--- head-2006-11-06.orig/arch/i386/mm/fault-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/i386/mm/fault-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -30,6 +30,40 @@
 
 extern void die(const char *,struct pt_regs *,long);
 
+#ifdef CONFIG_KPROBES
+ATOMIC_NOTIFIER_HEAD(notify_page_fault_chain);
+int register_page_fault_notifier(struct notifier_block *nb)
+{
+	vmalloc_sync_all();
+	return atomic_notifier_chain_register(&notify_page_fault_chain, nb);
+}
+
+int unregister_page_fault_notifier(struct notifier_block *nb)
+{
+	return atomic_notifier_chain_unregister(&notify_page_fault_chain, nb);
+}
+
+static inline int notify_page_fault(enum die_val val, const char *str,
+			struct pt_regs *regs, long err, int trap, int sig)
+{
+	struct die_args args = {
+		.regs = regs,
+		.str = str,
+		.err = err,
+		.trapnr = trap,
+		.signr = sig
+	};
+	return atomic_notifier_call_chain(&notify_page_fault_chain, val, &args);
+}
+#else
+static inline int notify_page_fault(enum die_val val, const char *str,
+			struct pt_regs *regs, long err, int trap, int sig)
+{
+	return NOTIFY_DONE;
+}
+#endif
+
+
 /*
  * Unlock any spinlocks which will prevent us from getting the
  * message out 
@@ -77,13 +111,16 @@ static inline unsigned long get_segment_
 	unsigned seg = regs->xcs & 0xffff;
 	u32 seg_ar, seg_limit, base, *desc;
 
+	/* Unlikely, but must come before segment checks. */
+	if (unlikely(regs->eflags & VM_MASK)) {
+		base = seg << 4;
+		*eip_limit = base + 0xffff;
+		return base + (eip & 0xffff);
+	}
+
 	/* The standard kernel/user address space limit. */
 	*eip_limit = (seg & 2) ? USER_DS.seg : KERNEL_DS.seg;
 
-	/* Unlikely, but must come before segment checks. */
-	if (unlikely((regs->eflags & VM_MASK) != 0))
-		return eip + (seg << 4);
-	
 	/* By far the most common cases. */
 	if (likely(seg == __USER_CS || seg == GET_KERNEL_CS()))
 		return eip;
@@ -445,7 +482,7 @@ fastcall void __kprobes do_page_fault(st
 		/* Can take a spurious fault if mapping changes R/O -> R/W. */
 		if (spurious_fault(regs, address, error_code))
 			return;
-		if (notify_die(DIE_PAGE_FAULT, "page fault", regs, error_code, 14,
+		if (notify_page_fault(DIE_PAGE_FAULT, "page fault", regs, error_code, 14,
 						SIGSEGV) == NOTIFY_STOP)
 			return;
 		/*
@@ -455,7 +492,7 @@ fastcall void __kprobes do_page_fault(st
 		goto bad_area_nosemaphore;
 	}
 
-	if (notify_die(DIE_PAGE_FAULT, "page fault", regs, error_code, 14,
+	if (notify_page_fault(DIE_PAGE_FAULT, "page fault", regs, error_code, 14,
 					SIGSEGV) == NOTIFY_STOP)
 		return;
 
@@ -476,7 +513,7 @@ fastcall void __kprobes do_page_fault(st
 	/* When running in the kernel we expect faults to occur only to
 	 * addresses in user space.  All other faults represent errors in the
 	 * kernel and should generate an OOPS.  Unfortunatly, in the case of an
-	 * erroneous fault occuring in a code path which already holds mmap_sem
+	 * erroneous fault occurring in a code path which already holds mmap_sem
 	 * we will deadlock attempting to validate the fault against the
 	 * address space.  Luckily the kernel only validly references user
 	 * space from well defined areas of code, which are listed in the
@@ -504,12 +541,12 @@ fastcall void __kprobes do_page_fault(st
 		goto bad_area;
 	if (error_code & 4) {
 		/*
-		 * accessing the stack below %esp is always a bug.
-		 * The "+ 32" is there due to some instructions (like
-		 * pusha) doing post-decrement on the stack and that
-		 * doesn't show up until later..
+		 * Accessing the stack below %esp is always a bug.
+		 * The large cushion allows instructions like enter
+		 * and pusha to work.  ("enter $65535,$31" pushes
+		 * 32 pointers and then decrements %esp by 65535.)
 		 */
-		if (address + 32 < regs->esp)
+		if (address + 65536 + 32 * sizeof(unsigned long) < regs->esp)
 			goto bad_area;
 	}
 	if (expand_stack(vma, address))
Index: head-2006-11-06/arch/i386/mm/init-xen.c
===================================================================
--- head-2006-11-06.orig/arch/i386/mm/init-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/i386/mm/init-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -6,7 +6,6 @@
  *  Support of BIGMEM added by Gerhard Wichert, Siemens AG, July 1999
  */
 
-#include <linux/config.h>
 #include <linux/module.h>
 #include <linux/signal.h>
 #include <linux/sched.h>
@@ -23,12 +22,14 @@
 #include <linux/init.h>
 #include <linux/highmem.h>
 #include <linux/pagemap.h>
+#include <linux/poison.h>
 #include <linux/bootmem.h>
 #include <linux/slab.h>
 #include <linux/proc_fs.h>
 #include <linux/efi.h>
 #include <linux/memory_hotplug.h>
 #include <linux/initrd.h>
+#include <linux/cpumask.h>
 #include <linux/dma-mapping.h>
 #include <linux/scatterlist.h>
 
@@ -415,7 +416,7 @@ static void __init pagetable_init (void)
 	permanent_kmaps_init(pgd_base);
 }
 
-#ifdef CONFIG_SOFTWARE_SUSPEND
+#if defined(CONFIG_SOFTWARE_SUSPEND) || defined(CONFIG_ACPI_SLEEP)
 /*
  * Swap suspend & friends need this for resume because things like the intel-agp
  * driver might have split up a kernel 4MB mapping.
@@ -723,7 +724,7 @@ void __init mem_init(void)
  */
 #ifdef CONFIG_MEMORY_HOTPLUG
 #ifndef CONFIG_NEED_MULTIPLE_NODES
-int add_memory(u64 start, u64 size)
+int arch_add_memory(int nid, u64 start, u64 size)
 {
 	struct pglist_data *pgdata = &contig_page_data;
 	struct zone *zone = pgdata->node_zones + MAX_NR_ZONES-1;
@@ -799,16 +800,15 @@ static int noinline do_test_wp_bit(void)
 
 #ifdef CONFIG_DEBUG_RODATA
 
-extern char __start_rodata, __end_rodata;
 void mark_rodata_ro(void)
 {
-	unsigned long addr = (unsigned long)&__start_rodata;
+	unsigned long addr = (unsigned long)__start_rodata;
 
-	for (; addr < (unsigned long)&__end_rodata; addr += PAGE_SIZE)
+	for (; addr < (unsigned long)__end_rodata; addr += PAGE_SIZE)
 		change_page_attr(virt_to_page(addr), 1, PAGE_KERNEL_RO);
 
-	printk ("Write protecting the kernel read-only data: %luk\n",
-			(unsigned long)(&__end_rodata - &__start_rodata) >> 10);
+	printk("Write protecting the kernel read-only data: %uk\n",
+			(__end_rodata - __start_rodata) >> 10);
 
 	/*
 	 * change_page_attr() requires a global_flush_tlb() call after it.
@@ -827,7 +827,7 @@ void free_init_pages(char *what, unsigne
 	for (addr = begin; addr < end; addr += PAGE_SIZE) {
 		ClearPageReserved(virt_to_page(addr));
 		init_page_count(virt_to_page(addr));
-		memset((void *)addr, 0xcc, PAGE_SIZE);
+		memset((void *)addr, POISON_FREE_INITMEM, PAGE_SIZE);
 		free_page(addr);
 		totalram_pages++;
 	}
Index: head-2006-11-06/arch/i386/mm/pgtable-xen.c
===================================================================
--- head-2006-11-06.orig/arch/i386/mm/pgtable-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/i386/mm/pgtable-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -2,7 +2,6 @@
  *  linux/arch/i386/mm/pgtable.c
  */
 
-#include <linux/config.h>
 #include <linux/sched.h>
 #include <linux/kernel.h>
 #include <linux/errno.h>
@@ -39,7 +38,6 @@ void show_mem(void)
 	struct page *page;
 	pg_data_t *pgdat;
 	unsigned long i;
-	struct page_state ps;
 	unsigned long flags;
 
 	printk(KERN_INFO "Mem-info:\n");
@@ -67,12 +65,13 @@ void show_mem(void)
 	printk(KERN_INFO "%d pages shared\n", shared);
 	printk(KERN_INFO "%d pages swap cached\n", cached);
 
-	get_page_state(&ps);
-	printk(KERN_INFO "%lu pages dirty\n", ps.nr_dirty);
-	printk(KERN_INFO "%lu pages writeback\n", ps.nr_writeback);
-	printk(KERN_INFO "%lu pages mapped\n", ps.nr_mapped);
-	printk(KERN_INFO "%lu pages slab\n", ps.nr_slab);
-	printk(KERN_INFO "%lu pages pagetables\n", ps.nr_page_table_pages);
+	printk(KERN_INFO "%lu pages dirty\n", global_page_state(NR_FILE_DIRTY));
+	printk(KERN_INFO "%lu pages writeback\n",
+					global_page_state(NR_WRITEBACK));
+	printk(KERN_INFO "%lu pages mapped\n", global_page_state(NR_FILE_MAPPED));
+	printk(KERN_INFO "%lu pages slab\n", global_page_state(NR_SLAB));
+	printk(KERN_INFO "%lu pages pagetables\n",
+					global_page_state(NR_PAGETABLE));
 }
 
 /*
Index: head-2006-11-06/arch/i386/oprofile/xenoprof.c
===================================================================
--- head-2006-11-06.orig/arch/i386/oprofile/xenoprof.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/arch/i386/oprofile/xenoprof.c	2006-10-16 10:19:17.000000000 +0200
@@ -202,7 +202,7 @@ static void unbind_virq(void)
 {
 	int i;
 
-	for_each_cpu(i) {
+	for_each_possible_cpu(i) {
 		if (ovf_irq[i] >= 0) {
 			unbind_from_irqhandler(ovf_irq[i], NULL);
 			ovf_irq[i] = -1;
@@ -215,7 +215,7 @@ static int bind_virq(void)
 {
 	int i, result;
 
-	for_each_cpu(i) {
+	for_each_possible_cpu(i) {
 		result = bind_virq_to_irqhandler(VIRQ_XENOPROF,
 						 i,
 						 xenoprof_ovf_interrupt,
Index: head-2006-11-06/arch/i386/pci/irq-xen.c
===================================================================
--- head-2006-11-06.orig/arch/i386/pci/irq-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/i386/pci/irq-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -4,7 +4,6 @@
  *	(c) 1999--2000 Martin Mares <mj@ucw.cz>
  */
 
-#include <linux/config.h>
 #include <linux/types.h>
 #include <linux/kernel.h>
 #include <linux/pci.h>
@@ -203,14 +202,14 @@ static void write_config_nybble(struct p
  */
 static int pirq_ali_get(struct pci_dev *router, struct pci_dev *dev, int pirq)
 {
-	static unsigned char irqmap[16] = { 0, 9, 3, 10, 4, 5, 7, 6, 1, 11, 0, 12, 0, 14, 0, 15 };
+	static const unsigned char irqmap[16] = { 0, 9, 3, 10, 4, 5, 7, 6, 1, 11, 0, 12, 0, 14, 0, 15 };
 
 	return irqmap[read_config_nybble(router, 0x48, pirq-1)];
 }
 
 static int pirq_ali_set(struct pci_dev *router, struct pci_dev *dev, int pirq, int irq)
 {
-	static unsigned char irqmap[16] = { 0, 8, 0, 2, 4, 5, 7, 6, 0, 1, 3, 9, 11, 0, 13, 15 };
+	static const unsigned char irqmap[16] = { 0, 8, 0, 2, 4, 5, 7, 6, 0, 1, 3, 9, 11, 0, 13, 15 };
 	unsigned int val = irqmap[irq];
 		
 	if (val) {
@@ -261,13 +260,13 @@ static int pirq_via_set(struct pci_dev *
  */
 static int pirq_via586_get(struct pci_dev *router, struct pci_dev *dev, int pirq)
 {
-	static unsigned int pirqmap[4] = { 3, 2, 5, 1 };
+	static const unsigned int pirqmap[4] = { 3, 2, 5, 1 };
 	return read_config_nybble(router, 0x55, pirqmap[pirq-1]);
 }
 
 static int pirq_via586_set(struct pci_dev *router, struct pci_dev *dev, int pirq, int irq)
 {
-	static unsigned int pirqmap[4] = { 3, 2, 5, 1 };
+	static const unsigned int pirqmap[4] = { 3, 2, 5, 1 };
 	write_config_nybble(router, 0x55, pirqmap[pirq-1], irq);
 	return 1;
 }
@@ -279,13 +278,13 @@ static int pirq_via586_set(struct pci_de
  */
 static int pirq_ite_get(struct pci_dev *router, struct pci_dev *dev, int pirq)
 {
-	static unsigned char pirqmap[4] = { 1, 0, 2, 3 };
+	static const unsigned char pirqmap[4] = { 1, 0, 2, 3 };
 	return read_config_nybble(router,0x43, pirqmap[pirq-1]);
 }
 
 static int pirq_ite_set(struct pci_dev *router, struct pci_dev *dev, int pirq, int irq)
 {
-	static unsigned char pirqmap[4] = { 1, 0, 2, 3 };
+	static const unsigned char pirqmap[4] = { 1, 0, 2, 3 };
 	write_config_nybble(router, 0x43, pirqmap[pirq-1], irq);
 	return 1;
 }
@@ -510,7 +509,7 @@ static int pirq_bios_set(struct pci_dev 
 
 static __init int intel_router_probe(struct irq_router *r, struct pci_dev *router, u16 device)
 {
-	static struct pci_device_id pirq_440gx[] = {
+	static struct pci_device_id __initdata pirq_440gx[] = {
 		{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_82443GX_0) },
 		{ PCI_DEVICE(PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_82443GX_2) },
 		{ },
@@ -870,7 +869,7 @@ static int pcibios_lookup_irq(struct pci
 		for (i = 0; i < 16; i++) {
 			if (!(mask & (1 << i)))
 				continue;
-			if (pirq_penalty[i] < pirq_penalty[newirq] && can_request_irq(i, SA_SHIRQ))
+			if (pirq_penalty[i] < pirq_penalty[newirq] && can_request_irq(i, IRQF_SHARED))
 				newirq = i;
 		}
 	}
@@ -885,6 +884,7 @@ static int pcibios_lookup_irq(struct pci
 	((!(pci_probe & PCI_USE_PIRQ_MASK)) || ((1 << irq) & mask)) ) {
 		DBG(" -> got IRQ %d\n", irq);
 		msg = "Found";
+		eisa_set_level_irq(irq);
 	} else if (newirq && r->set && (dev->class >> 8) != PCI_CLASS_DISPLAY_VGA) {
 		DBG(" -> assigning IRQ %d", newirq);
 		if (r->set(pirq_router_dev, dev, pirq, newirq)) {
Index: head-2006-11-06/arch/x86_64/ia32/ia32entry-xen.S
===================================================================
--- head-2006-11-06.orig/arch/x86_64/ia32/ia32entry-xen.S	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/x86_64/ia32/ia32entry-xen.S	2006-10-16 10:19:17.000000000 +0200
@@ -13,6 +13,7 @@
 #include <asm/thread_info.h>	
 #include <asm/segment.h>
 #include <asm/vsyscall32.h>
+#include <asm/irqflags.h>
 #include <linux/linkage.h>
 
 #define __XEN_X86_64 1
@@ -92,6 +93,10 @@ ENTRY(ia32_sysenter_target)
 	__swapgs 
 	movq	%gs:pda_kernelstack, %rsp
 	addq	$(PDA_STACKOFFSET),%rsp
+	/*
+	 * No need to follow this irqs on/off section: the syscall
+	 * disabled irqs, here we enable it straight after entry:
+	 */
 	XEN_UNBLOCK_EVENTS(%r11)	
 	__sti
  	movl	%ebp,%ebp		/* zero extension */
@@ -116,7 +121,7 @@ ENTRY(ia32_sysenter_target)
 	pushq	%rax
 	CFI_ADJUST_CFA_OFFSET 8
 	cld
-	SAVE_ARGS 0,0,1
+	SAVE_ARGS 0,0,0
  	/* no need to do an access_ok check here because rbp has been
  	   32bit zero extended */ 
 1:	movl	(%rbp),%r9d
@@ -137,6 +142,7 @@ sysenter_do_call:	
 	GET_THREAD_INFO(%r10)
 	XEN_BLOCK_EVENTS(%r11)	
 	__cli
+	TRACE_IRQS_OFF
 	testl	$_TIF_ALLWORK_MASK,threadinfo_flags(%r10)
 	jnz	int_ret_from_sys_call
 	andl    $~TS_COMPAT,threadinfo_status(%r10)
@@ -151,6 +157,7 @@ sysenter_do_call:	
 	CFI_REGISTER rsp,rcx
 	movl	$VSYSCALL32_SYSEXIT,%edx	/* User %eip */
 	CFI_REGISTER rip,rdx
+	TRACE_IRQS_ON
 	__swapgs
 	XEN_UNBLOCK_EVENTS(%r11)		
 	__sti		/* sti only takes effect after the next instruction */
@@ -175,6 +182,7 @@ sysenter_tracesys:
 	.previous
 	jmp	sysenter_do_call
 	CFI_ENDPROC
+ENDPROC(ia32_sysenter_target)
 
 /*
  * 32bit SYSCALL instruction entry.
@@ -198,13 +206,17 @@ sysenter_tracesys:
  */ 	
 ENTRY(ia32_cstar_target)
 	CFI_STARTPROC32	simple
-	CFI_DEF_CFA	rsp,0
+	CFI_DEF_CFA	rsp,PDA_STACKOFFSET
 	CFI_REGISTER	rip,rcx
 	/*CFI_REGISTER	rflags,r11*/
 	__swapgs
 	movl	%esp,%r8d
 	CFI_REGISTER	rsp,r8
 	movq	%gs:pda_kernelstack,%rsp
+	/*
+	 * No need to follow this irqs on/off section: the syscall
+	 * disabled irqs and here we enable it straight after entry:
+	 */
 	XEN_UNBLOCK_EVENTS(%r11)	
 	__sti
 	SAVE_ARGS 8,1,1
@@ -241,6 +253,7 @@ cstar_do_call:	
 	GET_THREAD_INFO(%r10)
 	XEN_BLOCK_EVENTS(%r11)		
 	__cli
+	TRACE_IRQS_OFF
 	testl $_TIF_ALLWORK_MASK,threadinfo_flags(%r10)
 	jnz  int_ret_from_sys_call
 	andl $~TS_COMPAT,threadinfo_status(%r10)
@@ -249,6 +262,7 @@ cstar_do_call:	
 	CFI_REGISTER rip,rcx
 	movl EFLAGS-ARGOFFSET(%rsp),%r11d	
 	/*CFI_REGISTER rflags,r11*/
+	TRACE_IRQS_ON
 	movl RSP-ARGOFFSET(%rsp),%esp
 	CFI_RESTORE rsp
 	__swapgs
@@ -271,6 +285,7 @@ cstar_tracesys:	
 	.quad 1b,ia32_badarg
 	.previous
 	jmp cstar_do_call
+END(ia32_cstar_target)
 				
 ia32_badarg:
 	movq $-EFAULT,%rax
@@ -307,6 +322,10 @@ ENTRY(ia32_syscall)
 	/*CFI_REL_OFFSET	cs,CS-RIP*/
 	CFI_REL_OFFSET	rip,RIP-RIP
 	__swapgs
+	/*
+	 * No need to follow this irqs on/off section: the syscall
+	 * disabled irqs and here we enable it straight after entry:
+	 */
 	XEN_UNBLOCK_EVENTS(%r11)
 	__sti
 	movq (%rsp),%rcx
@@ -341,16 +360,13 @@ ia32_tracesys:			 
 	LOAD_ARGS ARGOFFSET  /* reload args from stack in case ptrace changed it */
 	RESTORE_REST
 	jmp ia32_do_syscall
+END(ia32_syscall)
 
 ia32_badsys:
 	movq $0,ORIG_RAX-ARGOFFSET(%rsp)
 	movq $-ENOSYS,RAX-ARGOFFSET(%rsp)
 	jmp int_ret_from_sys_call
 
-ni_syscall:
-	movq %rax,%rdi
-	jmp  sys32_ni_syscall			
-
 quiet_ni_syscall:
 	movq $-ENOSYS,%rax
 	ret
@@ -397,10 +413,10 @@ ENTRY(ia32_ptregs_common)
 	RESTORE_REST
 	jmp  ia32_sysret	/* misbalances the return cache */
 	CFI_ENDPROC
+END(ia32_ptregs_common)
 
 	.section .rodata,"a"
 	.align 8
-	.globl ia32_sys_call_table
 ia32_sys_call_table:
 	.quad sys_restart_syscall
 	.quad sys_exit
@@ -723,4 +739,5 @@ ia32_sys_call_table:
 	.quad sys_sync_file_range
 	.quad sys_tee
 	.quad compat_sys_vmsplice
+	.quad compat_sys_move_pages
 ia32_syscall_end:		
Index: head-2006-11-06/arch/x86_64/kernel/apic-xen.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/apic-xen.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/arch/x86_64/kernel/apic-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -14,7 +14,6 @@
  *	Mikael Pettersson	:	PM converted to driver model.
  */
 
-#include <linux/config.h>
 #include <linux/init.h>
 
 #include <linux/mm.h>
Index: head-2006-11-06/arch/x86_64/kernel/e820-xen.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/e820-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/x86_64/kernel/e820-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -1,7 +1,6 @@
 /* 
  * Handle the memory map.
  * The functions here do the job until bootmem takes over.
- * $Id: e820.c,v 1.4 2002/09/19 19:25:32 ak Exp $
  *
  *  Getting sanitize_e820_map() in sync with i386 version by applying change:
  *  -  Provisions for empty E820 memory regions (reported by certain BIOSes).
@@ -9,7 +8,6 @@
  *  Venkatesh Pallipadi <venkatesh.pallipadi@intel.com>
  *
  */
-#include <linux/config.h>
 #include <linux/kernel.h>
 #include <linux/types.h>
 #include <linux/init.h>
@@ -19,6 +17,7 @@
 #include <linux/kexec.h>
 #include <linux/module.h>
 
+#include <asm/pgtable.h>
 #include <asm/page.h>
 #include <asm/e820.h>
 #include <asm/proto.h>
@@ -74,7 +73,11 @@ static inline int bad_addr(unsigned long
 #endif
 	/* kernel code + 640k memory hole (later should not be needed, but 
 	   be paranoid for now) */
-	if (last >= 640*1024 && addr < __pa_symbol(&_end)) { 
+	if (last >= 640*1024 && addr < 1024*1024) {
+		*addrp = 1024*1024;
+		return 1;
+	}
+	if (last >= __pa_symbol(&_text) && last < __pa_symbol(&_end)) {
 		*addrp = __pa_symbol(&_end);
 		return 1;
 	}
@@ -707,6 +710,7 @@ void __init parse_memmapopt(char *p, cha
 }
 
 unsigned long pci_mem_start = 0xaeedbabe;
+EXPORT_SYMBOL(pci_mem_start);
 
 /*
  * Search for the biggest gap in the low 32 bits of the e820
Index: head-2006-11-06/arch/x86_64/kernel/early_printk-xen.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/early_printk-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/x86_64/kernel/early_printk-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -3,7 +3,7 @@
 #include <linux/kernel.h>
 #include <linux/init.h>
 #include <linux/string.h>
-#include <linux/tty.h>
+#include <linux/screen_info.h>
 #include <asm/io.h>
 #include <asm/processor.h>
 #include <asm/fcntl.h>
Index: head-2006-11-06/arch/x86_64/kernel/entry-xen.S
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/entry-xen.S	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/x86_64/kernel/entry-xen.S	2006-10-16 10:19:17.000000000 +0200
@@ -32,7 +32,6 @@
  */
 
 #define ASSEMBLY 1
-#include <linux/config.h>
 #include <linux/linkage.h>
 #include <asm/segment.h>
 #include <asm/smp.h>
@@ -46,6 +45,7 @@
 #include <asm/thread_info.h>
 #include <asm/hw_irq.h>
 #include <asm/page.h>
+#include <asm/irqflags.h>
 #include <asm/errno.h>
 #include <xen/interface/arch-x86_64.h>
 #include <xen/interface/features.h>
@@ -60,6 +60,18 @@
 #define retint_kernel retint_restore_args
 #endif	
 
+
+#ifndef CONFIG_XEN
+.macro TRACE_IRQS_IRETQ offset=ARGOFFSET
+#ifdef CONFIG_TRACE_IRQFLAGS
+	bt   $9,EFLAGS-\offset(%rsp)	/* interrupts off? */
+	jnc  1f
+	TRACE_IRQS_ON
+1:
+#endif
+.endm
+#endif
+
 NMI_MASK = 0x80000000
 	
 /*
@@ -106,19 +118,21 @@ NMI_MASK = 0x80000000
 	CFI_ADJUST_CFA_OFFSET	-(6*8)
 	.endm
 
-	.macro	CFI_DEFAULT_STACK start=1
+	.macro	CFI_DEFAULT_STACK start=1,adj=0
 	.if \start
 	CFI_STARTPROC	simple
-	CFI_DEF_CFA	rsp,SS+8
+	CFI_DEF_CFA	rsp,SS+8-(\adj*ARGOFFSET)
 	.else
-	CFI_DEF_CFA_OFFSET SS+8
+	CFI_DEF_CFA_OFFSET SS+8-(\adj*ARGOFFSET)
 	.endif
+	.if \adj == 0
 	CFI_REL_OFFSET	r15,R15
 	CFI_REL_OFFSET	r14,R14
 	CFI_REL_OFFSET	r13,R13
 	CFI_REL_OFFSET	r12,R12
 	CFI_REL_OFFSET	rbp,RBP
 	CFI_REL_OFFSET	rbx,RBX
+	.endif
 	CFI_REL_OFFSET	r11,R11
 	CFI_REL_OFFSET	r10,R10
 	CFI_REL_OFFSET	r9,R9
@@ -192,6 +206,7 @@ rff_trace:
 	GET_THREAD_INFO(%rcx)	
 	jmp rff_action
 	CFI_ENDPROC
+END(ret_from_fork)
 
 /*
  * System call entry. Upto 6 arguments in registers are supported.
@@ -226,11 +241,15 @@ rff_trace:
 
 ENTRY(system_call)
 	CFI_STARTPROC	simple
-	CFI_DEF_CFA	rsp,0
+	CFI_DEF_CFA	rsp,PDA_STACKOFFSET
 	CFI_REGISTER	rip,rcx
 	/*CFI_REGISTER	rflags,r11*/
 	SAVE_ARGS -8,0
 	movq  %rax,ORIG_RAX-ARGOFFSET(%rsp) 
+	/*
+	 * No need to follow this irqs off/on section - it's straight
+	 * and short:
+	 */
         XEN_UNBLOCK_EVENTS(%r11)        
 	GET_THREAD_INFO(%rcx)
 	testl $(_TIF_SYSCALL_TRACE|_TIF_SYSCALL_AUDIT|_TIF_SECCOMP),threadinfo_flags(%rcx)
@@ -252,10 +271,12 @@ ret_from_sys_call:
 sysret_check:		
 	GET_THREAD_INFO(%rcx)
         XEN_BLOCK_EVENTS(%rsi)        
+	TRACE_IRQS_OFF
 	movl threadinfo_flags(%rcx),%edx
 	andl %edi,%edx
 	CFI_REMEMBER_STATE
 	jnz  sysret_careful 
+	TRACE_IRQS_ON
         XEN_UNBLOCK_EVENTS(%rsi)                
 	CFI_REGISTER	rip,rcx
 	RESTORE_ARGS 0,8,0
@@ -268,6 +289,7 @@ sysret_careful:
 	CFI_RESTORE_STATE
 	bt $TIF_NEED_RESCHED,%edx
 	jnc sysret_signal
+	TRACE_IRQS_ON
 	XEN_UNBLOCK_EVENTS(%rsi)
 	pushq %rdi
 	CFI_ADJUST_CFA_OFFSET 8
@@ -278,6 +300,7 @@ sysret_careful:
 
 	/* Handle a signal */ 
 sysret_signal:
+	TRACE_IRQS_ON
 /*	sti */
         XEN_UNBLOCK_EVENTS(%rsi)        
 	testl $(_TIF_SIGPENDING|_TIF_NOTIFY_RESUME|_TIF_SINGLESTEP),%edx
@@ -293,6 +316,7 @@ sysret_signal:
 	/* Use IRET because user could have changed frame. This
 	   works because ptregscall_common has called FIXUP_TOP_OF_STACK. */
 	XEN_BLOCK_EVENTS(%rsi)
+	TRACE_IRQS_OFF
 	jmp int_with_check
 	
 badsys:
@@ -317,6 +341,7 @@ tracesys:			 
 	/* Use IRET because user could have changed frame */
 	jmp int_ret_from_sys_call
 	CFI_ENDPROC
+END(system_call)
 		
 /* 
  * Syscall return path ending with IRET.
@@ -340,6 +365,7 @@ ENTRY(int_ret_from_sys_call)
 	CFI_REL_OFFSET	r10,R10-ARGOFFSET
 	CFI_REL_OFFSET	r11,R11-ARGOFFSET
         XEN_BLOCK_EVENTS(%rsi)
+	TRACE_IRQS_OFF
 	testb $3,CS-ARGOFFSET(%rsp)
         jnz 1f
         /* Need to set the proper %ss (not NULL) for ring 3 iretq */
@@ -362,6 +388,7 @@ int_with_check:
 int_careful:
 	bt $TIF_NEED_RESCHED,%edx
 	jnc  int_very_careful
+	TRACE_IRQS_ON
 /*	sti */
         XEN_UNBLOCK_EVENTS(%rsi)
 	pushq %rdi
@@ -370,10 +397,12 @@ int_careful:
 	popq %rdi
 	CFI_ADJUST_CFA_OFFSET -8
 	XEN_BLOCK_EVENTS(%rsi)
+	TRACE_IRQS_OFF
 	jmp int_with_check
 
 	/* handle signals and tracing -- both require a full stack frame */
 int_very_careful:
+	TRACE_IRQS_ON
 /*	sti */
         XEN_UNBLOCK_EVENTS(%rsi)
 	SAVE_REST
@@ -388,6 +417,7 @@ int_very_careful:
 	CFI_ADJUST_CFA_OFFSET -8
 	andl $~(_TIF_SYSCALL_TRACE|_TIF_SYSCALL_AUDIT|_TIF_SINGLESTEP),%edi
 	XEN_BLOCK_EVENTS(%rsi)
+	TRACE_IRQS_OFF
 	jmp int_restore_rest
 	
 int_signal:
@@ -400,8 +430,10 @@ int_signal:
 int_restore_rest:
 	RESTORE_REST
 	XEN_BLOCK_EVENTS(%rsi)
+	TRACE_IRQS_OFF
 	jmp int_with_check
 	CFI_ENDPROC
+END(int_ret_from_sys_call)
 		
 /* 
  * Certain special system calls that need to save a complete full stack frame.
@@ -413,6 +445,7 @@ int_restore_rest:
 	leaq	\func(%rip),%rax
 	leaq    -ARGOFFSET+8(%rsp),\arg /* 8 for return address */
 	jmp	ptregscall_common
+END(\label)
 	.endm
 
 	CFI_STARTPROC
@@ -442,6 +475,7 @@ ENTRY(ptregscall_common)
 	CFI_REL_OFFSET rip, 0
 	ret
 	CFI_ENDPROC
+END(ptregscall_common)
 	
 ENTRY(stub_execve)
 	CFI_STARTPROC
@@ -456,6 +490,7 @@ ENTRY(stub_execve)
 	RESTORE_REST
 	jmp int_ret_from_sys_call
 	CFI_ENDPROC
+END(stub_execve)
 	
 /*
  * sigreturn is special because it needs to restore all registers on return.
@@ -473,6 +508,7 @@ ENTRY(stub_rt_sigreturn)
 	RESTORE_REST
 	jmp int_ret_from_sys_call
 	CFI_ENDPROC
+END(stub_rt_sigreturn)
 
 /*
  * initial frame state for interrupts and exceptions
@@ -499,7 +535,7 @@ ENTRY(stub_rt_sigreturn)
  */ 
 
 retint_check:
-	CFI_DEFAULT_STACK
+	CFI_DEFAULT_STACK adj=1
 	movl threadinfo_flags(%rcx),%edx
 	andl %edi,%edx
 	CFI_REMEMBER_STATE
@@ -521,6 +557,7 @@ retint_careful:
 	CFI_RESTORE_STATE
 	bt    $TIF_NEED_RESCHED,%edx
 	jnc   retint_signal
+	TRACE_IRQS_ON
 	XEN_UNBLOCK_EVENTS(%rsi)
 /*	sti */        
 	pushq %rdi
@@ -531,11 +568,13 @@ retint_careful:
 	GET_THREAD_INFO(%rcx)
 	XEN_BLOCK_EVENTS(%rsi)		
 /*	cli */
+	TRACE_IRQS_OFF
 	jmp retint_check
 	
 retint_signal:
 	testl $(_TIF_SIGPENDING|_TIF_NOTIFY_RESUME|_TIF_SINGLESTEP),%edx
 	jz    retint_restore_args
+	TRACE_IRQS_ON
         XEN_UNBLOCK_EVENTS(%rsi)
 	SAVE_REST
 	movq $-1,ORIG_RAX(%rsp) 			
@@ -544,6 +583,7 @@ retint_signal:
 	call do_notify_resume
 	RESTORE_REST
         XEN_BLOCK_EVENTS(%rsi)		
+	TRACE_IRQS_OFF
 	movl $_TIF_NEED_RESCHED,%edi
 	GET_THREAD_INFO(%rcx)
 	jmp retint_check
@@ -562,6 +602,7 @@ retint_kernel:	
 	call preempt_schedule_irq
 	jmp retint_kernel       /* check again */
 #endif	
+
 	CFI_ENDPROC
 	
 /*
@@ -579,17 +620,21 @@ retint_kernel:	
 #ifndef CONFIG_XEN
 ENTRY(thermal_interrupt)
 	apicinterrupt THERMAL_APIC_VECTOR,smp_thermal_interrupt
+END(thermal_interrupt)
 
 ENTRY(threshold_interrupt)
 	apicinterrupt THRESHOLD_APIC_VECTOR,mce_threshold_interrupt
+END(threshold_interrupt)
 
 #ifdef CONFIG_SMP	
 ENTRY(reschedule_interrupt)
 	apicinterrupt RESCHEDULE_VECTOR,smp_reschedule_interrupt
+END(reschedule_interrupt)
 
 	.macro INVALIDATE_ENTRY num
 ENTRY(invalidate_interrupt\num)
 	apicinterrupt INVALIDATE_TLB_VECTOR_START+\num,smp_invalidate_interrupt	
+END(invalidate_interrupt\num)
 	.endm
 
 	INVALIDATE_ENTRY 0
@@ -603,17 +648,21 @@ ENTRY(invalidate_interrupt\num)
 
 ENTRY(call_function_interrupt)
 	apicinterrupt CALL_FUNCTION_VECTOR,smp_call_function_interrupt
+END(call_function_interrupt)
 #endif
 
 #ifdef CONFIG_X86_LOCAL_APIC	
 ENTRY(apic_timer_interrupt)
 	apicinterrupt LOCAL_TIMER_VECTOR,smp_apic_timer_interrupt
+END(apic_timer_interrupt)
 
 ENTRY(error_interrupt)
 	apicinterrupt ERROR_APIC_VECTOR,smp_error_interrupt
+END(error_interrupt)
 
 ENTRY(spurious_interrupt)
 	apicinterrupt SPURIOUS_APIC_VECTOR,smp_spurious_interrupt
+END(spurious_interrupt)
 #endif
 #endif /* !CONFIG_XEN */
 				
@@ -649,7 +698,7 @@ ENTRY(spurious_interrupt)
 #if 0 /* not XEN */
 	/* error code is on the stack already */
 	/* handle NMI like exceptions that can happen everywhere */
-	.macro paranoidentry sym, ist=0
+	.macro paranoidentry sym, ist=0, irqtrace=1
         movq (%rsp),%rcx
         movq 8(%rsp),%r11
         addq $0x10,%rsp /* skip rcx and r11 */        
@@ -680,9 +729,74 @@ ENTRY(spurious_interrupt)
 	.endif
 /*	cli */
 	XEN_BLOCK_EVENTS(%rsi)		
+	.if \irqtrace
+	TRACE_IRQS_OFF
+	.endif
+	.endm
+
+	/*
+ 	 * "Paranoid" exit path from exception stack.
+  	 * Paranoid because this is used by NMIs and cannot take
+	 * any kernel state for granted.
+	 * We don't do kernel preemption checks here, because only
+	 * NMI should be common and it does not enable IRQs and
+	 * cannot get reschedule ticks.
+	 *
+	 * "trace" is 0 for the NMI handler only, because irq-tracing
+	 * is fundamentally NMI-unsafe. (we cannot change the soft and
+	 * hard flags at once, atomically)
+	 */
+	.macro paranoidexit trace=1
+	/* ebx:	no swapgs flag */
+paranoid_exit\trace:
+	testl %ebx,%ebx				/* swapgs needed? */
+	jnz paranoid_restore\trace
+	testl $3,CS(%rsp)
+	jnz   paranoid_userspace\trace
+paranoid_swapgs\trace:
+	TRACE_IRQS_IRETQ 0
+	swapgs
+paranoid_restore\trace:
+	RESTORE_ALL 8
+	iretq
+paranoid_userspace\trace:
+	GET_THREAD_INFO(%rcx)
+	movl threadinfo_flags(%rcx),%ebx
+	andl $_TIF_WORK_MASK,%ebx
+	jz paranoid_swapgs\trace
+	movq %rsp,%rdi			/* &pt_regs */
+	call sync_regs
+	movq %rax,%rsp			/* switch stack for scheduling */
+	testl $_TIF_NEED_RESCHED,%ebx
+	jnz paranoid_schedule\trace
+	movl %ebx,%edx			/* arg3: thread flags */
+	.if \trace
+	TRACE_IRQS_ON
+	.endif
+	sti
+	xorl %esi,%esi 			/* arg2: oldset */
+	movq %rsp,%rdi 			/* arg1: &pt_regs */
+	call do_notify_resume
+	cli
+	.if \trace
+	TRACE_IRQS_OFF
+	.endif
+	jmp paranoid_userspace\trace
+paranoid_schedule\trace:
+	.if \trace
+	TRACE_IRQS_ON
+	.endif
+	sti
+	call schedule
+	cli
+	.if \trace
+	TRACE_IRQS_OFF
+	.endif
+	jmp paranoid_userspace\trace
+	CFI_ENDPROC
 	.endm
 #endif
-	
+
 /*
  * Exception entry point. This expects an error code/orig_rax on the stack
  * and the exception handler in %rax.	
@@ -724,6 +838,7 @@ ENTRY(error_entry)
 	CFI_REL_OFFSET	r15,R15
 #if 0        
 	cmpl $__KERNEL_CS,CS(%rsp)
+	CFI_REMEMBER_STATE
 	je  error_kernelspace
 #endif        
 error_call_handler:
@@ -736,6 +851,7 @@ error_exit:		
 	RESTORE_REST
 /*	cli */
 	XEN_BLOCK_EVENTS(%rsi)		
+	TRACE_IRQS_OFF
 	GET_THREAD_INFO(%rcx)	
 	testb $3,CS-ARGOFFSET(%rsp)
 	jz retint_kernel
@@ -745,7 +861,7 @@ error_exit:		
 	jnz   retint_careful
 	jmp   retint_restore_args
 
-error_kernelspace:
+#if 0
          /*
          * We need to re-write the logic here because we don't do iretq to 
          * to return to user mode. It's still possible that we get trap/fault
@@ -753,7 +869,8 @@ error_kernelspace:
          * for example).
          *
          */           
-#if 0
+	CFI_RESTORE_STATE
+error_kernelspace:
 	incl %ebx
        /* There are two places in the kernel that can potentially fault with
           usergs. Handle them here. The exception handlers after
@@ -769,10 +886,13 @@ error_kernelspace:
 	cmpq $gs_change,RIP(%rsp)
         je   error_swapgs
 	jmp  error_sti
-#endif        
+#endif
+	CFI_ENDPROC
+END(error_entry)
 	
 ENTRY(hypervisor_callback)
 	zeroentry do_hypervisor_callback
+END(hypervisor_callback)
         
 /*
  * Copied from arch/xen/i386/kernel/entry.S
@@ -800,37 +920,47 @@ ENTRY(do_hypervisor_callback)   # do_hyp
 	popq %rsp
 	decl %gs:pda_irqcount
 	jmp  error_exit
+END(do_hypervisor_callback)
 
 #ifdef CONFIG_X86_LOCAL_APIC
 KPROBE_ENTRY(nmi)
 	zeroentry do_nmi_callback
 ENTRY(do_nmi_callback)
+	CFI_STARTPROC
         addq $8, %rsp
+	CFI_DEFAULT_STACK start=0,adj=1
         call do_nmi
         orl  $NMI_MASK,EFLAGS(%rsp)
         RESTORE_REST
         XEN_BLOCK_EVENTS(%rsi)
         GET_THREAD_INFO(%rcx)
         jmp  retint_restore_args
+	CFI_ENDPROC
 	.previous .text
+END(nmi)
 #endif
 
         ALIGN
 restore_all_enable_events:  
+	CFI_DEFAULT_STACK adj=1
+	TRACE_IRQS_ON
 	XEN_UNBLOCK_EVENTS(%rsi)        # %rsi is already set up...
 
 scrit:	/**** START OF CRITICAL REGION ****/
 	XEN_TEST_PENDING(%rsi)
+	CFI_REMEMBER_STATE
 	jnz  14f			# process more events if necessary...
 	XEN_PUT_VCPU_INFO(%rsi)
         RESTORE_ARGS 0,8,0
         HYPERVISOR_IRET 0
         
+	CFI_RESTORE_STATE
 14:	XEN_LOCKED_BLOCK_EVENTS(%rsi)
 	XEN_PUT_VCPU_INFO(%rsi)
 	SAVE_REST
         movq %rsp,%rdi                  # set the argument again
 	jmp  11b
+	CFI_ENDPROC
 ecrit:  /**** END OF CRITICAL REGION ****/
 # At this point, unlike on x86-32, we don't do the fixup to simplify the 
 # code and the stack frame is more complex on x86-64.
@@ -850,8 +980,12 @@ ecrit:  /**** END OF CRITICAL REGION ***
 # We distinguish between categories by comparing each saved segment register
 # with its current contents: any discrepancy means we in category 1.
 ENTRY(failsafe_callback)
+	_frame (RIP-0x30)
+	CFI_REL_OFFSET rcx, 0
+	CFI_REL_OFFSET r11, 8
 	movw %ds,%cx
 	cmpw %cx,0x10(%rsp)
+	CFI_REMEMBER_STATE
 	jne 1f
 	movw %es,%cx
 	cmpw %cx,0x18(%rsp)
@@ -864,17 +998,26 @@ ENTRY(failsafe_callback)
 	jne 1f
 	/* All segments match their saved values => Category 2 (Bad IRET). */
 	movq (%rsp),%rcx
+	CFI_RESTORE rcx
 	movq 8(%rsp),%r11
+	CFI_RESTORE r11
 	addq $0x30,%rsp
+	CFI_ADJUST_CFA_OFFSET -0x30
 	movq $-9999,%rdi	/* better code? */
 	jmp do_exit			
+	CFI_RESTORE_STATE
 1:	/* Segment mismatch => Category 1 (Bad segment). Retry the IRET. */
 	movq (%rsp),%rcx
+	CFI_RESTORE rcx
 	movq 8(%rsp),%r11
+	CFI_RESTORE r11
 	addq $0x30,%rsp
+	CFI_ADJUST_CFA_OFFSET -0x30
 	pushq $0
+	CFI_ADJUST_CFA_OFFSET 8
 	SAVE_ALL
 	jmp error_exit
+	CFI_ENDPROC
 #if 0	      
         .section __ex_table,"a"
         .align 8
@@ -929,9 +1072,11 @@ ENTRY(kernel_thread)
 	UNFAKE_STACK_FRAME
 	ret
 	CFI_ENDPROC
-
+ENDPROC(kernel_thread)
 	
 child_rip:
+	pushq $0		# fake return address
+	CFI_STARTPROC
 	/*
 	 * Here we are in the child and the registers are set as they were
 	 * at kernel_thread() invocation in the parent.
@@ -942,6 +1087,8 @@ child_rip:
 	# exit
 	xorl %edi, %edi
 	call do_exit
+	CFI_ENDPROC
+ENDPROC(child_rip)
 
 /*
  * execve(). This function needs to use IRET, not SYSRET, to set up all state properly.
@@ -972,28 +1119,33 @@ ENTRY(execve)
 	UNFAKE_STACK_FRAME
 	ret
 	CFI_ENDPROC
+ENDPROC(execve)
 
 KPROBE_ENTRY(page_fault)
 	errorentry do_page_fault
+END(page_fault)
 	.previous .text
 
 ENTRY(coprocessor_error)
 	zeroentry do_coprocessor_error
+END(coprocessor_error)
 
 ENTRY(simd_coprocessor_error)
 	zeroentry do_simd_coprocessor_error	
+END(simd_coprocessor_error)
 
 ENTRY(device_not_available)
 	zeroentry math_state_restore
+END(device_not_available)
 
 	/* runs on exception stack */
 KPROBE_ENTRY(debug)
- 	INTR_FRAME
-/*	pushq $0
+/* 	INTR_FRAME
+	pushq $0
 	CFI_ADJUST_CFA_OFFSET 8	*/
 	zeroentry do_debug
-/*	jmp paranoid_exit */
-	CFI_ENDPROC
+/*	paranoidexit */
+END(debug)
 	.previous .text
 
 #if 0
@@ -1002,109 +1154,90 @@ KPROBE_ENTRY(nmi)
 	INTR_FRAME
 	pushq $-1
 	CFI_ADJUST_CFA_OFFSET 8
-	paranoidentry do_nmi
-	/*
- 	 * "Paranoid" exit path from exception stack.
-  	 * Paranoid because this is used by NMIs and cannot take
-	 * any kernel state for granted.
-	 * We don't do kernel preemption checks here, because only
-	 * NMI should be common and it does not enable IRQs and
-	 * cannot get reschedule ticks.
-	 */
-	/* ebx:	no swapgs flag */
-paranoid_exit:
-	testl %ebx,%ebx				/* swapgs needed? */
-	jnz paranoid_restore
-	testl $3,CS(%rsp)
-	jnz   paranoid_userspace
-paranoid_swapgs:	
-	swapgs
-paranoid_restore:	
-	RESTORE_ALL 8
-	iretq
-paranoid_userspace:	
-	GET_THREAD_INFO(%rcx)
-	movl threadinfo_flags(%rcx),%ebx
-	andl $_TIF_WORK_MASK,%ebx
-	jz paranoid_swapgs
-	movq %rsp,%rdi			/* &pt_regs */
-	call sync_regs
-	movq %rax,%rsp			/* switch stack for scheduling */
-	testl $_TIF_NEED_RESCHED,%ebx
-	jnz paranoid_schedule
-	movl %ebx,%edx			/* arg3: thread flags */
-	sti
-	xorl %esi,%esi 			/* arg2: oldset */
-	movq %rsp,%rdi 			/* arg1: &pt_regs */
-	call do_notify_resume
-	cli
-	jmp paranoid_userspace
-paranoid_schedule:
-	sti
-	call schedule
-	cli
-	jmp paranoid_userspace
-	CFI_ENDPROC
+	paranoidentry do_nmi, 0, 0
+#ifdef CONFIG_TRACE_IRQFLAGS
+	paranoidexit 0
+#else
+	jmp paranoid_exit1
+ 	CFI_ENDPROC
+#endif
+END(nmi)
 	.previous .text
 #endif        
 
 KPROBE_ENTRY(int3)
- 	INTR_FRAME
-/* 	pushq $0
+/* 	INTR_FRAME
+ 	pushq $0
  	CFI_ADJUST_CFA_OFFSET 8 */
  	zeroentry do_int3
-/* 	jmp paranoid_exit */
- 	CFI_ENDPROC
+/* 	jmp paranoid_exit1
+ 	CFI_ENDPROC */
+END(int3)
 	.previous .text
 
 ENTRY(overflow)
 	zeroentry do_overflow
+END(overflow)
 
 ENTRY(bounds)
 	zeroentry do_bounds
+END(bounds)
 
 ENTRY(invalid_op)
 	zeroentry do_invalid_op	
+END(invalid_op)
 
 ENTRY(coprocessor_segment_overrun)
 	zeroentry do_coprocessor_segment_overrun
+END(coprocessor_segment_overrun)
 
 ENTRY(reserved)
 	zeroentry do_reserved
+END(reserved)
 
 #if 0
 	/* runs on exception stack */
 ENTRY(double_fault)
 	XCPT_FRAME
 	paranoidentry do_double_fault
-	jmp paranoid_exit
+	jmp paranoid_exit1
 	CFI_ENDPROC
+END(double_fault)
 #endif
 
 ENTRY(invalid_TSS)
 	errorentry do_invalid_TSS
+END(invalid_TSS)
 
 ENTRY(segment_not_present)
 	errorentry do_segment_not_present
+END(segment_not_present)
 
 	/* runs on exception stack */
 ENTRY(stack_segment)
-	XCPT_FRAME
+/*	XCPT_FRAME
+	paranoidentry do_stack_segment */
 	errorentry do_stack_segment
-	CFI_ENDPROC
+/*	jmp paranoid_exit1
+	CFI_ENDPROC */
+END(stack_segment)
 
 KPROBE_ENTRY(general_protection)
 	errorentry do_general_protection
+END(general_protection)
 	.previous .text
 
 ENTRY(alignment_check)
 	errorentry do_alignment_check
+END(alignment_check)
 
 ENTRY(divide_error)
 	zeroentry do_divide_error
+END(divide_error)
 
 ENTRY(spurious_interrupt_bug)
 	zeroentry do_spurious_interrupt_bug
+END(spurious_interrupt_bug)
 
 #ifdef CONFIG_X86_MCE
 	/* runs on exception stack */
@@ -1113,22 +1246,60 @@ ENTRY(machine_check)
 	pushq $0
 	CFI_ADJUST_CFA_OFFSET 8	
 	paranoidentry do_machine_check
-	jmp paranoid_exit
+	jmp paranoid_exit1
 	CFI_ENDPROC
+END(machine_check)
 #endif
 
+/* Call softirq on interrupt stack. Interrupts are off. */
 ENTRY(call_softirq)
 	CFI_STARTPROC
-	movq %gs:pda_irqstackptr,%rax
-	movq %rsp,%rdx
-	CFI_DEF_CFA_REGISTER	rdx
+	push %rbp
+	CFI_ADJUST_CFA_OFFSET	8
+	CFI_REL_OFFSET rbp,0
+	mov  %rsp,%rbp
+	CFI_DEF_CFA_REGISTER rbp
 	incl %gs:pda_irqcount
-	cmove %rax,%rsp
-	pushq %rdx
-	/*todo CFI_DEF_CFA_EXPRESSION ...*/
+	cmove %gs:pda_irqstackptr,%rsp
+	push  %rbp			# backlink for old unwinder
 	call __do_softirq
-	popq %rsp
+	leaveq
 	CFI_DEF_CFA_REGISTER	rsp
+	CFI_ADJUST_CFA_OFFSET   -8
 	decl %gs:pda_irqcount
 	ret
 	CFI_ENDPROC
+ENDPROC(call_softirq)
+
+#ifdef CONFIG_STACK_UNWIND
+ENTRY(arch_unwind_init_running)
+	CFI_STARTPROC
+	movq	%r15, R15(%rdi)
+	movq	%r14, R14(%rdi)
+	xchgq	%rsi, %rdx
+	movq	%r13, R13(%rdi)
+	movq	%r12, R12(%rdi)
+	xorl	%eax, %eax
+	movq	%rbp, RBP(%rdi)
+	movq	%rbx, RBX(%rdi)
+	movq	(%rsp), %rcx
+	movq	%rax, R11(%rdi)
+	movq	%rax, R10(%rdi)
+	movq	%rax, R9(%rdi)
+	movq	%rax, R8(%rdi)
+	movq	%rax, RAX(%rdi)
+	movq	%rax, RCX(%rdi)
+	movq	%rax, RDX(%rdi)
+	movq	%rax, RSI(%rdi)
+	movq	%rax, RDI(%rdi)
+	movq	%rax, ORIG_RAX(%rdi)
+	movq	%rcx, RIP(%rdi)
+	leaq	8(%rsp), %rcx
+	movq	$__KERNEL_CS, CS(%rdi)
+	movq	%rax, EFLAGS(%rdi)
+	movq	%rcx, RSP(%rdi)
+	movq	$__KERNEL_DS, SS(%rdi)
+	jmpq	*%rdx
+	CFI_ENDPROC
+ENDPROC(arch_unwind_init_running)
+#endif
Index: head-2006-11-06/arch/x86_64/kernel/genapic-xen.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/genapic-xen.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/arch/x86_64/kernel/genapic-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -8,7 +8,6 @@
  * Martin Bligh, Andi Kleen, James Bottomley, John Stultz, and
  * James Cleverdon.
  */
-#include <linux/config.h>
 #include <linux/threads.h>
 #include <linux/cpumask.h>
 #include <linux/string.h>
Index: head-2006-11-06/arch/x86_64/kernel/genapic_xen.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/genapic_xen.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/arch/x86_64/kernel/genapic_xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -10,7 +10,6 @@
  *
  * Hacked to pieces for Xen by Chris Wright.
  */
-#include <linux/config.h>
 #include <linux/threads.h>
 #include <linux/cpumask.h>
 #include <linux/string.h>
Index: head-2006-11-06/arch/x86_64/kernel/head-xen.S
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/head-xen.S	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/x86_64/kernel/head-xen.S	2006-10-16 10:19:17.000000000 +0200
@@ -40,6 +40,7 @@ ENTRY(_start)
 	/* rsi is pointer to startup info structure.
 	   pass it to C */
 	movq %rsi,%rdi
+	pushq $0		# fake return address
 	jmp x86_64_start_kernel
 
 ENTRY(stext)
Index: head-2006-11-06/arch/x86_64/kernel/head64-xen.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/head64-xen.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/arch/x86_64/kernel/head64-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -3,8 +3,6 @@
  *
  *  Copyright (C) 2000 Andrea Arcangeli <andrea@suse.de> SuSE
  *
- *  $Id: head64.c,v 1.22 2001/07/06 14:28:20 ak Exp $
- *
  *  Jun Nakajima <jun.nakajima@intel.com>
  *	Modified for Xen.
  */
@@ -130,6 +128,11 @@ void __init x86_64_start_kernel(char * r
 	asm volatile("lidt %0" :: "m" (idt_descr));
 #endif
 
+	/*
+	 * This must be called really, really early:
+	 */
+	lockdep_init();
+
  	for (i = 0; i < NR_CPUS; i++)
  		cpu_pda(i) = &boot_cpu_pda[i];
 
Index: head-2006-11-06/arch/x86_64/kernel/init_task.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/init_task.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/arch/x86_64/kernel/init_task.c	2006-10-16 10:19:17.000000000 +0200
@@ -47,11 +47,11 @@ EXPORT_SYMBOL(init_task);
  * on exact cacheline boundaries, to eliminate cacheline ping-pong.
  */ 
 DEFINE_PER_CPU(struct tss_struct, init_tss) ____cacheline_internodealigned_in_smp = INIT_TSS;
-#endif
 
 /* Copies of the original ist values from the tss are only accessed during
  * debugging, no special alignment required.
  */
 DEFINE_PER_CPU(struct orig_ist, orig_ist);
+#endif
 
 #define ALIGN_TO_4K __attribute__((section(".data.init_task")))
Index: head-2006-11-06/arch/x86_64/kernel/io_apic-xen.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/io_apic-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/x86_64/kernel/io_apic-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -25,7 +25,6 @@
 #include <linux/init.h>
 #include <linux/delay.h>
 #include <linux/sched.h>
-#include <linux/config.h>
 #include <linux/smp_lock.h>
 #include <linux/mc146818rtc.h>
 #include <linux/acpi.h>
@@ -41,6 +40,7 @@
 #include <asm/mach_apic.h>
 #include <asm/acpi.h>
 #include <asm/dma.h>
+#include <asm/nmi.h>
 
 #define __apicdebuginit  __init
 
@@ -367,7 +367,7 @@ void __init check_ioapic(void) 
 				vendor &= 0xffff;
 				switch (vendor) { 
 				case PCI_VENDOR_ID_VIA:
-#ifdef CONFIG_GART_IOMMU
+#ifdef CONFIG_IOMMU
 					if ((end_pfn > MAX_DMA32_PFN ||
 					     force_iommu) &&
 					    !iommu_aperture_allowed) {
@@ -913,23 +913,18 @@ static struct hw_interrupt_type ioapic_e
 #define IOAPIC_EDGE	0
 #define IOAPIC_LEVEL	1
 
-static inline void ioapic_register_intr(int irq, int vector, unsigned long trigger)
+static void ioapic_register_intr(int irq, int vector, unsigned long trigger)
 {
-	if (use_pci_vector() && !platform_legacy_irq(irq)) {
-		if ((trigger == IOAPIC_AUTO && IO_APIC_irq_trigger(irq)) ||
-				trigger == IOAPIC_LEVEL)
-			irq_desc[vector].handler = &ioapic_level_type;
-		else
-			irq_desc[vector].handler = &ioapic_edge_type;
-		set_intr_gate(vector, interrupt[vector]);
-	} else	{
-		if ((trigger == IOAPIC_AUTO && IO_APIC_irq_trigger(irq)) ||
-				trigger == IOAPIC_LEVEL)
-			irq_desc[irq].handler = &ioapic_level_type;
-		else
-			irq_desc[irq].handler = &ioapic_edge_type;
-		set_intr_gate(vector, interrupt[irq]);
-	}
+	unsigned idx;
+
+	idx = use_pci_vector() && !platform_legacy_irq(irq) ? vector : irq;
+
+	if ((trigger == IOAPIC_AUTO && IO_APIC_irq_trigger(irq)) ||
+			trigger == IOAPIC_LEVEL)
+		irq_desc[idx].chip = &ioapic_level_type;
+	else
+		irq_desc[idx].chip = &ioapic_edge_type;
+	set_intr_gate(vector, interrupt[idx]);
 }
 #else
 #define ioapic_register_intr(_irq,_vector,_trigger) ((void)0)
@@ -1034,7 +1029,7 @@ static void __init setup_ExtINT_IRQ0_pin
 	 * The timer IRQ doesn't have to know that behind the
 	 * scene we have a 8259A-master in AEOI mode ...
 	 */
-	irq_desc[0].handler = &ioapic_edge_type;
+	irq_desc[0].chip = &ioapic_edge_type;
 
 	/*
 	 * Add it to the IO-APIC irq-routing table:
@@ -1679,6 +1674,13 @@ static void set_ioapic_affinity_vector (
 #endif // CONFIG_SMP
 #endif // CONFIG_PCI_MSI
 
+static int ioapic_retrigger(unsigned int irq)
+{
+	send_IPI_self(IO_APIC_VECTOR(irq));
+
+	return 1;
+}
+
 /*
  * Level and edge triggered IO-APIC interrupts need different handling,
  * so we use two separate IRQ descriptors. Edge triggered IRQs can be
@@ -1699,6 +1701,7 @@ static struct hw_interrupt_type ioapic_e
 #ifdef CONFIG_SMP
 	.set_affinity = set_ioapic_affinity,
 #endif
+	.retrigger	= ioapic_retrigger,
 };
 
 static struct hw_interrupt_type ioapic_level_type __read_mostly = {
@@ -1712,6 +1715,7 @@ static struct hw_interrupt_type ioapic_l
 #ifdef CONFIG_SMP
 	.set_affinity = set_ioapic_affinity,
 #endif
+	.retrigger	= ioapic_retrigger,
 };
 #endif /* !CONFIG_XEN */
 
@@ -1748,7 +1752,7 @@ static inline void init_IO_APIC_traps(vo
 #ifndef CONFIG_XEN
 			else
 				/* Strange. Oh, well.. */
-				irq_desc[irq].handler = &no_irq_type;
+				irq_desc[irq].chip = &no_irq_type;
 #endif
 		}
 	}
@@ -1967,7 +1971,7 @@ static inline void check_timer(void)
 	apic_printk(APIC_VERBOSE, KERN_INFO "...trying to set up timer as Virtual Wire IRQ...");
 
 	disable_8259A_irq(0);
-	irq_desc[0].handler = &lapic_irq_type;
+	irq_desc[0].chip = &lapic_irq_type;
 	apic_write(APIC_LVT0, APIC_DM_FIXED | vector);	/* Fixed mode */
 	enable_8259A_irq(0);
 
Index: head-2006-11-06/arch/x86_64/kernel/irq-xen.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/irq-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/x86_64/kernel/irq-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -26,6 +26,30 @@ atomic_t irq_mis_count;
 #endif
 #endif
 
+#ifdef CONFIG_DEBUG_STACKOVERFLOW
+/*
+ * Probabilistic stack overflow check:
+ *
+ * Only check the stack in process context, because everything else
+ * runs on the big interrupt stacks. Checking reliably is too expensive,
+ * so we just check from interrupts.
+ */
+static inline void stack_overflow_check(struct pt_regs *regs)
+{
+	u64 curbase = (u64) current->thread_info;
+	static unsigned long warned = -60*HZ;
+
+	if (regs->rsp >= curbase && regs->rsp <= curbase + THREAD_SIZE &&
+	    regs->rsp <  curbase + sizeof(struct thread_info) + 128 &&
+	    time_after(jiffies, warned + 60*HZ)) {
+		printk("do_IRQ: %s near stack overflow (cur:%Lx,rsp:%lx)\n",
+		       current->comm, curbase, regs->rsp);
+		show_stack(NULL,NULL);
+		warned = jiffies;
+	}
+}
+#endif
+
 /*
  * Generic, controller-independent functions:
  */
@@ -39,7 +63,7 @@ int show_interrupts(struct seq_file *p, 
 	if (i == 0) {
 		seq_printf(p, "           ");
 		for_each_online_cpu(j)
-			seq_printf(p, "CPU%d       ",j);
+			seq_printf(p, "CPU%-8d",j);
 		seq_putc(p, '\n');
 	}
 
@@ -55,7 +79,7 @@ int show_interrupts(struct seq_file *p, 
 		for_each_online_cpu(j)
 			seq_printf(p, "%10u ", kstat_cpu(j).irqs[i]);
 #endif
-		seq_printf(p, " %14s", irq_desc[i].handler->typename);
+		seq_printf(p, " %14s", irq_desc[i].chip->typename);
 
 		seq_printf(p, "  %s", action->name);
 		for (action=action->next; action; action = action->next)
@@ -94,9 +118,17 @@ asmlinkage unsigned int do_IRQ(struct pt
 	/* high bit used in ret_from_ code  */
 	unsigned irq = ~regs->orig_rax;
 
+	if (unlikely(irq >= NR_IRQS)) {
+		printk(KERN_EMERG "%s: cannot handle IRQ %d\n",
+					__FUNCTION__, irq);
+		BUG();
+	}
+
 	exit_idle();
 	irq_enter();
-
+#ifdef CONFIG_DEBUG_STACKOVERFLOW
+	stack_overflow_check(regs);
+#endif
 	__do_IRQ(irq, regs);
 	irq_exit();
 
@@ -114,13 +146,13 @@ void fixup_irqs(cpumask_t map)
 		if (irq == 2)
 			continue;
 
-		cpus_and(mask, irq_affinity[irq], map);
+		cpus_and(mask, irq_desc[irq].affinity, map);
 		if (any_online_cpu(mask) == NR_CPUS) {
 			printk("Breaking affinity for irq %i\n", irq);
 			mask = map;
 		}
-		if (irq_desc[irq].handler->set_affinity)
-			irq_desc[irq].handler->set_affinity(irq, mask);
+		if (irq_desc[irq].chip->set_affinity)
+			irq_desc[irq].chip->set_affinity(irq, mask);
 		else if (irq_desc[irq].action && !(warned++))
 			printk("Cannot set affinity for irq %i\n", irq);
 	}
@@ -145,8 +177,10 @@ asmlinkage void do_softirq(void)
  	local_irq_save(flags);
  	pending = local_softirq_pending();
  	/* Switch to interrupt stack */
- 	if (pending)
+ 	if (pending) {
 		call_softirq();
+		WARN_ON_ONCE(softirq_count());
+	}
  	local_irq_restore(flags);
 }
 EXPORT_SYMBOL(do_softirq);
Index: head-2006-11-06/arch/x86_64/kernel/Makefile
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/Makefile	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/arch/x86_64/kernel/Makefile	2006-10-16 10:19:17.000000000 +0200
@@ -39,6 +39,7 @@ obj-$(CONFIG_X86_PM_TIMER)	+= pmtimer.o
 obj-$(CONFIG_X86_VSMP)		+= vsmp.o
 obj-$(CONFIG_K8_NB)		+= k8.o
 obj-$(CONFIG_AUDIT)		+= audit.o
+obj-$(CONFIG_XEN)		+= irqflags.o
 
 obj-$(CONFIG_MODULES)		+= module.o
 
@@ -58,6 +59,7 @@ msr-$(subst m,y,$(CONFIG_X86_MSR))  += .
 alternative-y			+= ../../i386/kernel/alternative.o
 
 ifdef CONFIG_XEN
+irqflags-y			+= ../../i386/mach-xen/irqflags.o
 time-y				+= ../../i386/kernel/time-xen.o
 pci-dma-y			+= ../../i386/kernel/pci-dma-xen.o
 microcode-$(subst m,y,$(CONFIG_MICROCODE))  := ../../i386/kernel/microcode-xen.o
Index: head-2006-11-06/arch/x86_64/kernel/mpparse-xen.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/mpparse-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/x86_64/kernel/mpparse-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -16,7 +16,6 @@
 #include <linux/mm.h>
 #include <linux/init.h>
 #include <linux/delay.h>
-#include <linux/config.h>
 #include <linux/bootmem.h>
 #include <linux/smp_lock.h>
 #include <linux/kernel_stat.h>
Index: head-2006-11-06/arch/x86_64/kernel/pci-nommu.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/pci-nommu.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/arch/x86_64/kernel/pci-nommu.c	2006-10-16 10:19:17.000000000 +0200
@@ -93,6 +93,8 @@ void __init no_iommu_init(void)
 	if (dma_ops)
 		return;
 
+#ifndef CONFIG_XEN
 	force_iommu = 0; /* no HW IOMMU */
+#endif
 	dma_ops = &nommu_dma_ops;
 }
Index: head-2006-11-06/arch/x86_64/kernel/pci-swiotlb-xen.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/pci-swiotlb-xen.c	2006-10-16 10:35:51.000000000 +0200
+++ head-2006-11-06/arch/x86_64/kernel/pci-swiotlb-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -38,9 +38,10 @@ void pci_swiotlb_init(void)
 {
 #if 0
 	/* don't initialize swiotlb if iommu=off (no_iommu=1) */
-	if (!iommu_aperture && !no_iommu &&
-	    (end_pfn > MAX_DMA32_PFN || force_iommu))
+	if (!iommu_detected && !no_iommu && end_pfn > MAX_DMA32_PFN)
 	       swiotlb = 1;
+	if (swiotlb_force)
+		swiotlb = 1;
 	if (swiotlb) {
 		printk(KERN_INFO "PCI-DMA: Using software bounce buffering for IO (SWIOTLB)\n");
 		swiotlb_init();
Index: head-2006-11-06/arch/x86_64/kernel/process-xen.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/process-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/x86_64/kernel/process-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -10,7 +10,6 @@
  *	Andi Kleen.
  *
  *	CPU hotplug support - ashok.raj@intel.com
- *  $Id: process.c,v 1.38 2002/01/15 10:08:03 ak Exp $
  * 
  *  Jun Nakajima <jun.nakajima@intel.com> 
  *     Modified for Xen
@@ -73,6 +72,7 @@ EXPORT_SYMBOL(boot_option_idle_override)
  * Powermanagement idle function, if any..
  */
 void (*pm_idle)(void);
+EXPORT_SYMBOL(pm_idle);
 static DEFINE_PER_CPU(unsigned int, cpu_idle_state);
 
 static ATOMIC_NOTIFIER_HEAD(idle_notifier);
@@ -120,10 +120,10 @@ static void xen_idle(void)
 	if (need_resched())
 		local_irq_enable();
 	else {
-		clear_thread_flag(TIF_POLLING_NRFLAG);
+		current_thread_info()->status &= ~TS_POLLING;
 		smp_mb__after_clear_bit();
 		safe_halt();
-		set_thread_flag(TIF_POLLING_NRFLAG);
+		current_thread_info()->status |= TS_POLLING;
 	}
 }
 
@@ -152,8 +152,7 @@ static inline void play_dead(void)
  */
 void cpu_idle (void)
 {
-	set_thread_flag(TIF_POLLING_NRFLAG);
-
+	current_thread_info()->status |= TS_POLLING;
 	/* endless idle loop with no priority at all */
 	while (1) {
 		while (!need_resched()) {
@@ -223,7 +222,7 @@ void __show_regs(struct pt_regs * regs)
 		system_utsname.version);
 	printk("RIP: %04lx:[<%016lx>] ", regs->cs & 0xffff, regs->rip);
 	printk_address(regs->rip); 
-	printk("\nRSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss, regs->rsp,
+	printk("RSP: %04lx:%016lx  EFLAGS: %08lx\n", regs->ss, regs->rsp,
 		regs->eflags);
 	printk("RAX: %016lx RBX: %016lx RCX: %016lx\n",
 	       regs->rax, regs->rbx, regs->rcx);
@@ -256,7 +255,7 @@ void show_regs(struct pt_regs *regs)
 {
 	printk("CPU %d:", smp_processor_id());
 	__show_regs(regs);
-	show_trace(&regs->rsp);
+	show_trace(NULL, regs, (void *)(regs + 1));
 }
 
 /*
@@ -301,8 +300,11 @@ void flush_thread(void)
 	struct task_struct *tsk = current;
 	struct thread_info *t = current_thread_info();
 
-	if (t->flags & _TIF_ABI_PENDING)
+	if (t->flags & _TIF_ABI_PENDING) {
 		t->flags ^= (_TIF_ABI_PENDING | _TIF_IA32);
+		if (t->flags & _TIF_IA32)
+			current_thread_info()->status |= TS_COMPAT;
+	}
 
 	tsk->thread.debugreg0 = 0;
 	tsk->thread.debugreg1 = 0;
Index: head-2006-11-06/arch/x86_64/kernel/setup-xen.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/setup-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/x86_64/kernel/setup-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -5,8 +5,6 @@
  *
  *  Nov 2001 Dave Jones <davej@suse.de>
  *  Forked from i386 setup code.
- *
- *  $Id$
  */
 
 /*
@@ -23,10 +21,9 @@
 #include <linux/slab.h>
 #include <linux/user.h>
 #include <linux/a.out.h>
-#include <linux/tty.h>
+#include <linux/screen_info.h>
 #include <linux/ioport.h>
 #include <linux/delay.h>
-#include <linux/config.h>
 #include <linux/init.h>
 #include <linux/initrd.h>
 #include <linux/highmem.h>
@@ -65,9 +62,7 @@
 #include <asm/setup.h>
 #include <asm/mach_apic.h>
 #include <asm/numa.h>
-#include <asm/swiotlb.h>
 #include <asm/sections.h>
-#include <asm/gart-mapping.h>
 #include <asm/dmi.h>
 #ifdef CONFIG_XEN
 #include <linux/percpu.h>
@@ -117,6 +112,7 @@ EXPORT_SYMBOL(xen_start_info);
  */
 
 struct cpuinfo_x86 boot_cpu_data __read_mostly;
+EXPORT_SYMBOL(boot_cpu_data);
 
 unsigned long mmu_cr4_features;
 
@@ -145,12 +141,14 @@ char dmi_alloc_data[DMI_MAX_DATA];
  * Setup options
  */
 struct screen_info screen_info;
+EXPORT_SYMBOL(screen_info);
 struct sys_desc_table_struct {
 	unsigned short length;
 	unsigned char table[0];
 };
 
 struct edid_info edid_info;
+EXPORT_SYMBOL_GPL(edid_info);
 struct e820map e820;
 #ifdef CONFIG_XEN
 struct e820map machine_e820;
@@ -525,80 +523,6 @@ contig_initmem_init(unsigned long start_
 } 
 #endif
 
-/* Use inline assembly to define this because the nops are defined 
-   as inline assembly strings in the include files and we cannot 
-   get them easily into strings. */
-asm("\t.data\nk8nops: " 
-    K8_NOP1 K8_NOP2 K8_NOP3 K8_NOP4 K8_NOP5 K8_NOP6
-    K8_NOP7 K8_NOP8); 
-    
-extern unsigned char k8nops[];
-static unsigned char *k8_nops[ASM_NOP_MAX+1] = { 
-     NULL,
-     k8nops,
-     k8nops + 1,
-     k8nops + 1 + 2,
-     k8nops + 1 + 2 + 3,
-     k8nops + 1 + 2 + 3 + 4,
-     k8nops + 1 + 2 + 3 + 4 + 5,
-     k8nops + 1 + 2 + 3 + 4 + 5 + 6,
-     k8nops + 1 + 2 + 3 + 4 + 5 + 6 + 7,
-}; 
-
-extern char __vsyscall_0;
-
-/* Replace instructions with better alternatives for this CPU type.
-
-   This runs before SMP is initialized to avoid SMP problems with
-   self modifying code. This implies that assymetric systems where
-   APs have less capabilities than the boot processor are not handled. 
-   In this case boot with "noreplacement". */ 
-void apply_alternatives(void *start, void *end) 
-{ 
-	struct alt_instr *a; 
-	int diff, i, k;
-	for (a = start; (void *)a < end; a++) { 
-		u8 *instr;
-
-		if (!boot_cpu_has(a->cpuid))
-			continue;
-
-		BUG_ON(a->replacementlen > a->instrlen); 
-		instr = a->instr;
-		/* vsyscall code is not mapped yet. resolve it manually. */
-		if (instr >= (u8 *)VSYSCALL_START && instr < (u8*)VSYSCALL_END)
-			instr -= VSYSCALL_START - (unsigned long)&__vsyscall_0;
-		__inline_memcpy(instr, a->replacement, a->replacementlen);
-		diff = a->instrlen - a->replacementlen; 
-
-		/* Pad the rest with nops */
-		for (i = a->replacementlen; diff > 0; diff -= k, i += k) {
-			k = diff;
-			if (k > ASM_NOP_MAX)
-				k = ASM_NOP_MAX;
-			__inline_memcpy(instr + i, k8_nops[k], k);
-		} 
-	}
-} 
-
-static int no_replacement __initdata = 0; 
- 
-void __init alternative_instructions(void)
-{
-	extern struct alt_instr __alt_instructions[], __alt_instructions_end[];
-	if (no_replacement) 
-		return;
-	apply_alternatives(__alt_instructions, __alt_instructions_end);
-}
-
-static int __init noreplacement_setup(char *s)
-{ 
-     no_replacement = 1; 
-     return 1;
-} 
-
-__setup("noreplacement", noreplacement_setup); 
-
 #if defined(CONFIG_EDD) || defined(CONFIG_EDD_MODULE)
 struct edd edd;
 #ifdef CONFIG_EDD_MODULE
@@ -653,14 +577,11 @@ static void discover_ebda(void)
 
 void __init setup_arch(char **cmdline_p)
 {
-	unsigned long kernel_end;
-
 #ifdef CONFIG_XEN
 	/* Register a call for panic conditions. */
 	notifier_chain_register(&panic_notifier_list, &xen_panic_block);
 
  	ROOT_DEV = MKDEV(RAMDISK_MAJOR,0); 
-	kernel_end = 0;		/* dummy */
  	screen_info = SCREEN_INFO;
 
 	if (is_initial_xendomain()) {
@@ -765,18 +686,19 @@ void __init setup_arch(char **cmdline_p)
 	contig_initmem_init(0, end_pfn);
 #endif
 
+#ifdef CONFIG_XEN
+	/* reserve kernel, physmap, start info and initial page tables */
+	reserve_bootmem_generic(__pa_symbol(&_text),
+	                        (table_end << PAGE_SHIFT) - __pa_symbol(&_text));
+#else
 	/* Reserve direct mapping */
 	reserve_bootmem_generic(table_start << PAGE_SHIFT, 
 				(table_end - table_start) << PAGE_SHIFT);
 
 	/* reserve kernel */
-	kernel_end = round_up(__pa_symbol(&_end),PAGE_SIZE);
-	reserve_bootmem_generic(HIGH_MEMORY, kernel_end - HIGH_MEMORY);
+	reserve_bootmem_generic(__pa_symbol(&_text),
+				__pa_symbol(&_end) - __pa_symbol(&_text));
 
-#ifdef CONFIG_XEN
-	/* reserve physmap, start info and initial page tables */
-	reserve_bootmem(kernel_end, (table_start<<PAGE_SHIFT)-kernel_end);
-#else
 	/*
 	 * reserve physical page 0 - it's a special BIOS page on many boxes,
 	 * enabling clean reboots, SMP operation, laptop functions.
@@ -844,7 +766,7 @@ void __init setup_arch(char **cmdline_p)
 #endif	/* !CONFIG_XEN */
 #ifdef CONFIG_KEXEC
 	if (crashk_res.start != crashk_res.end) {
-		reserve_bootmem(crashk_res.start,
+		reserve_bootmem_generic(crashk_res.start,
 			crashk_res.end - crashk_res.start + 1);
 	}
 #endif
@@ -986,10 +908,6 @@ void __init setup_arch(char **cmdline_p)
 	e820_setup_gap(e820.map, e820.nr_map);
 #endif
 
-#ifdef CONFIG_GART_IOMMU
-	iommu_hole_init();
-#endif
-
 #ifdef CONFIG_XEN
 	{
 		struct physdev_set_iopl set_iopl;
@@ -1109,24 +1027,32 @@ static int nearby_node(int apicid)
 static void __init amd_detect_cmp(struct cpuinfo_x86 *c)
 {
 #ifdef CONFIG_SMP
-	int cpu = smp_processor_id();
 	unsigned bits;
 #ifdef CONFIG_NUMA
+	int cpu = smp_processor_id();
 	int node = 0;
 	unsigned apicid = hard_smp_processor_id();
 #endif
+	unsigned ecx = cpuid_ecx(0x80000008);
+
+	c->x86_max_cores = (ecx & 0xff) + 1;
 
-	bits = 0;
-	while ((1 << bits) < c->x86_max_cores)
-		bits++;
+	/* CPU telling us the core id bits shift? */
+	bits = (ecx >> 12) & 0xF;
+
+	/* Otherwise recompute */
+	if (bits == 0) {
+		while ((1 << bits) < c->x86_max_cores)
+			bits++;
+	}
 
 	/* Low order bits define the core id (index of core in socket) */
-	cpu_core_id[cpu] = phys_proc_id[cpu] & ((1 << bits)-1);
+	c->cpu_core_id = c->phys_proc_id & ((1 << bits)-1);
 	/* Convert the APIC ID into the socket ID */
-	phys_proc_id[cpu] = phys_pkg_id(bits);
+	c->phys_proc_id = phys_pkg_id(bits);
 
 #ifdef CONFIG_NUMA
-  	node = phys_proc_id[cpu];
+  	node = c->phys_proc_id;
  	if (apicid_to_node[apicid] != NUMA_NO_NODE)
  		node = apicid_to_node[apicid];
  	if (!node_online(node)) {
@@ -1139,7 +1065,7 @@ static void __init amd_detect_cmp(struct
  		   but in the same order as the HT nodeids.
  		   If that doesn't result in a usable node fall back to the
  		   path for the previous case.  */
- 		int ht_nodeid = apicid - (phys_proc_id[0] << bits);
+ 		int ht_nodeid = apicid - (cpu_data[0].phys_proc_id << bits);
  		if (ht_nodeid >= 0 &&
  		    apicid_to_node[ht_nodeid] != NUMA_NO_NODE)
  			node = apicid_to_node[ht_nodeid];
@@ -1149,15 +1075,13 @@ static void __init amd_detect_cmp(struct
  	}
 	numa_set_node(cpu, node);
 
-  	printk(KERN_INFO "CPU %d/%x(%d) -> Node %d -> Core %d\n",
-  			cpu, apicid, c->x86_max_cores, node, cpu_core_id[cpu]);
+	printk(KERN_INFO "CPU %d/%x -> Node %d\n", cpu, apicid, node);
 #endif
 #endif
 }
 
-static int __init init_amd(struct cpuinfo_x86 *c)
+static void __init init_amd(struct cpuinfo_x86 *c)
 {
-	int r;
 	unsigned level;
 
 #ifdef CONFIG_SMP
@@ -1194,8 +1118,8 @@ static int __init init_amd(struct cpuinf
 	if (c->x86 >= 6)
 		set_bit(X86_FEATURE_FXSAVE_LEAK, &c->x86_capability);
 
-	r = get_model_name(c);
-	if (!r) { 
+	level = get_model_name(c);
+	if (!level) {
 		switch (c->x86) { 
 		case 15:
 			/* Should distinguish Models here, but this is only
@@ -1210,13 +1134,12 @@ static int __init init_amd(struct cpuinf
 	if (c->x86_power & (1<<8))
 		set_bit(X86_FEATURE_CONSTANT_TSC, &c->x86_capability);
 
-	if (c->extended_cpuid_level >= 0x80000008) {
-		c->x86_max_cores = (cpuid_ecx(0x80000008) & 0xff) + 1;
-
+	/* Multi core CPU? */
+	if (c->extended_cpuid_level >= 0x80000008)
 		amd_detect_cmp(c);
-	}
 
-	return r;
+	/* Fix cpuid4 emulation for more */
+	num_cache_leaves = 3;
 }
 
 static void __cpuinit detect_ht(struct cpuinfo_x86 *c)
@@ -1224,13 +1147,14 @@ static void __cpuinit detect_ht(struct c
 #ifdef CONFIG_SMP
 	u32 	eax, ebx, ecx, edx;
 	int 	index_msb, core_bits;
-	int 	cpu = smp_processor_id();
 
 	cpuid(1, &eax, &ebx, &ecx, &edx);
 
 
-	if (!cpu_has(c, X86_FEATURE_HT) || cpu_has(c, X86_FEATURE_CMP_LEGACY))
+	if (!cpu_has(c, X86_FEATURE_HT))
 		return;
+ 	if (cpu_has(c, X86_FEATURE_CMP_LEGACY))
+		goto out;
 
 	smp_num_siblings = (ebx & 0xff0000) >> 16;
 
@@ -1245,10 +1169,7 @@ static void __cpuinit detect_ht(struct c
 		}
 
 		index_msb = get_count_order(smp_num_siblings);
-		phys_proc_id[cpu] = phys_pkg_id(index_msb);
-
-		printk(KERN_INFO  "CPU: Physical Processor ID: %d\n",
-		       phys_proc_id[cpu]);
+		c->phys_proc_id = phys_pkg_id(index_msb);
 
 		smp_num_siblings = smp_num_siblings / c->x86_max_cores;
 
@@ -1256,13 +1177,15 @@ static void __cpuinit detect_ht(struct c
 
 		core_bits = get_count_order(c->x86_max_cores);
 
-		cpu_core_id[cpu] = phys_pkg_id(index_msb) &
+		c->cpu_core_id = phys_pkg_id(index_msb) &
 					       ((1 << core_bits) - 1);
-
-		if (c->x86_max_cores > 1)
-			printk(KERN_INFO  "CPU: Processor Core ID: %d\n",
-			       cpu_core_id[cpu]);
 	}
+out:
+	if ((c->x86_max_cores * smp_num_siblings) > 1) {
+		printk(KERN_INFO  "CPU: Physical Processor ID: %d\n", c->phys_proc_id);
+		printk(KERN_INFO  "CPU: Processor Core ID: %d\n", c->cpu_core_id);
+	}
+
 #endif
 }
 
@@ -1271,15 +1194,12 @@ static void __cpuinit detect_ht(struct c
  */
 static int __cpuinit intel_num_cpu_cores(struct cpuinfo_x86 *c)
 {
-	unsigned int eax;
+	unsigned int eax, t;
 
 	if (c->cpuid_level < 4)
 		return 1;
 
-	__asm__("cpuid"
-		: "=a" (eax)
-		: "0" (4), "c" (0)
-		: "bx", "dx");
+	cpuid_count(4, 0, &eax, &t, &t, &t);
 
 	if (eax & 0x1f)
 		return ((eax >> 26) + 1);
@@ -1292,16 +1212,17 @@ static void srat_detect_node(void)
 #ifdef CONFIG_NUMA
 	unsigned node;
 	int cpu = smp_processor_id();
+	int apicid = hard_smp_processor_id();
 
 	/* Don't do the funky fallback heuristics the AMD version employs
 	   for now. */
-	node = apicid_to_node[hard_smp_processor_id()];
+	node = apicid_to_node[apicid];
 	if (node == NUMA_NO_NODE)
 		node = first_node(node_online_map);
 	numa_set_node(cpu, node);
 
 	if (acpi_numa > 0)
-		printk(KERN_INFO "CPU %d -> Node %d\n", cpu, node);
+		printk(KERN_INFO "CPU %d/%x -> Node %d\n", cpu, apicid, node);
 #endif
 }
 
@@ -1311,6 +1232,13 @@ static void __cpuinit init_intel(struct 
 	unsigned n;
 
 	init_intel_cacheinfo(c);
+	if (c->cpuid_level > 9 ) {
+		unsigned eax = cpuid_eax(10);
+		/* Check for version and the number of counters */
+		if ((eax & 0xff) && (((eax>>8) & 0xff) > 1))
+			set_bit(X86_FEATURE_ARCH_PERFMON, &c->x86_capability);
+	}
+
 	n = c->extended_cpuid_level;
 	if (n >= 0x80000008) {
 		unsigned eax = cpuid_eax(0x80000008);
@@ -1402,7 +1330,7 @@ void __cpuinit early_identify_cpu(struct
 	}
 
 #ifdef CONFIG_SMP
-	phys_proc_id[smp_processor_id()] = (cpuid_ebx(1) >> 24) & 0xff;
+	c->phys_proc_id = (cpuid_ebx(1) >> 24) & 0xff;
 #endif
 }
 
@@ -1529,7 +1457,7 @@ static int show_cpuinfo(struct seq_file 
 		NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL,
 		NULL, NULL, NULL, "syscall", NULL, NULL, NULL, NULL,
 		NULL, NULL, NULL, NULL, "nx", NULL, "mmxext", NULL,
-		NULL, "fxsr_opt", "rdtscp", NULL, NULL, "lm", "3dnowext", "3dnow",
+		NULL, "fxsr_opt", NULL, "rdtscp", NULL, "lm", "3dnowext", "3dnow",
 
 		/* Transmeta-defined */
 		"recovery", "longrun", NULL, "lrti", NULL, NULL, NULL, NULL,
@@ -1540,7 +1468,7 @@ static int show_cpuinfo(struct seq_file 
 		/* Other (Linux-defined) */
 		"cxmmx", NULL, "cyrix_arr", "centaur_mcr", NULL,
 		"constant_tsc", NULL, NULL,
-		NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL,
+		"up", NULL, NULL, NULL, NULL, NULL, NULL, NULL,
 		NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL,
 		NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL,
 
@@ -1610,9 +1538,9 @@ static int show_cpuinfo(struct seq_file 
 #ifdef CONFIG_SMP
 	if (smp_num_siblings * c->x86_max_cores > 1) {
 		int cpu = c - cpu_data;
-		seq_printf(m, "physical id\t: %d\n", phys_proc_id[cpu]);
+		seq_printf(m, "physical id\t: %d\n", c->phys_proc_id);
 		seq_printf(m, "siblings\t: %d\n", cpus_weight(cpu_core_map[cpu]));
-		seq_printf(m, "core id\t\t: %d\n", cpu_core_id[cpu]);
+		seq_printf(m, "core id\t\t: %d\n", c->cpu_core_id);
 		seq_printf(m, "cpu cores\t: %d\n", c->booted_cores);
 	}
 #endif	
@@ -1686,7 +1614,7 @@ struct seq_operations cpuinfo_op = {
 	.show =	show_cpuinfo,
 };
 
-#ifdef CONFIG_INPUT_PCSPKR
+#if defined(CONFIG_INPUT_PCSPKR) || defined(CONFIG_INPUT_PCSPKR_MODULE)
 #include <linux/platform_device.h>
 static __init int add_pcspkr(void)
 {
Index: head-2006-11-06/arch/x86_64/kernel/setup64-xen.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/setup64-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/x86_64/kernel/setup64-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -3,13 +3,11 @@
  * Copyright (C) 1995  Linus Torvalds
  * Copyright 2001, 2002, 2003 SuSE Labs / Andi Kleen.
  * See setup.c for older changelog.
- * $Id: setup64.c,v 1.12 2002/03/21 10:09:17 ak Exp $
  *
  * Jun Nakajima <jun.nakajima@intel.com> 
  *   Modified for Xen
  *
  */ 
-#include <linux/config.h>
 #include <linux/init.h>
 #include <linux/kernel.h>
 #include <linux/sched.h>
@@ -38,6 +36,7 @@ char x86_boot_params[BOOT_PARAM_SIZE] __
 cpumask_t cpu_initialized __cpuinitdata = CPU_MASK_NONE;
 
 struct x8664_pda *_cpu_pda[NR_CPUS] __read_mostly;
+EXPORT_SYMBOL(_cpu_pda);
 struct x8664_pda boot_cpu_pda[NR_CPUS] __cacheline_aligned;
 
 #ifndef CONFIG_X86_NO_IDT
@@ -47,6 +46,7 @@ struct desc_ptr idt_descr = { 256 * 16 -
 char boot_cpu_stack[IRQSTACKSIZE] __attribute__((section(".bss.page_aligned")));
 
 unsigned long __supported_pte_mask __read_mostly = ~0UL;
+EXPORT_SYMBOL(__supported_pte_mask);
 static int do_not_nx __cpuinitdata = 0;
 
 /* noexec=on|off
@@ -246,6 +246,7 @@ void __cpuinit cpu_init (void)
 	int cpu = stack_smp_processor_id();
 #ifndef CONFIG_X86_NO_TSS
 	struct tss_struct *t = &per_cpu(init_tss, cpu);
+	struct orig_ist *orig_ist = &per_cpu(orig_ist, cpu);
 	unsigned long v; 
 	char *estacks = NULL; 
 	unsigned i;
@@ -319,7 +320,7 @@ void __cpuinit cpu_init (void)
 			estacks += EXCEPTION_STKSZ;
 			break;
 		}
-		t->ist[v] = (unsigned long)estacks;
+		orig_ist->ist[v] = t->ist[v] = (unsigned long)estacks;
 	}
 
 	t->io_bitmap_base = offsetof(struct tss_struct, io_bitmap);
Index: head-2006-11-06/arch/x86_64/kernel/smp-xen.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/smp-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/x86_64/kernel/smp-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -141,10 +141,10 @@ asmlinkage void smp_invalidate_interrupt
 
 	cpu = smp_processor_id();
 	/*
-	 * orig_rax contains the interrupt vector - 256.
+	 * orig_rax contains the negated interrupt vector.
 	 * Use that to determine where the sender put the data.
 	 */
-	sender = regs->orig_rax + 256 - INVALIDATE_TLB_VECTOR_START;
+	sender = ~regs->orig_rax - INVALIDATE_TLB_VECTOR_START;
 	f = &per_cpu(flush_state, sender);
 
 	if (!cpu_isset(cpu, f->flush_cpumask))
@@ -209,7 +209,7 @@ int __cpuinit init_smp_flush(void)
 {
 	int i;
 	for_each_cpu_mask(i, cpu_possible_map) {
-		spin_lock_init(&per_cpu(flush_state.tlbstate_lock, i));
+		spin_lock_init(&per_cpu(flush_state, i).tlbstate_lock);
 	}
 	return 0;
 }
@@ -230,6 +230,7 @@ void flush_tlb_current_task(void)
 		flush_tlb_others(cpu_mask, mm, FLUSH_ALL);
 	preempt_enable();
 }
+EXPORT_SYMBOL(flush_tlb_current_task);
 
 void flush_tlb_mm (struct mm_struct * mm)
 {
@@ -250,6 +251,7 @@ void flush_tlb_mm (struct mm_struct * mm
 
 	preempt_enable();
 }
+EXPORT_SYMBOL(flush_tlb_mm);
 
 void flush_tlb_page(struct vm_area_struct * vma, unsigned long va)
 {
@@ -272,6 +274,7 @@ void flush_tlb_page(struct vm_area_struc
 
 	preempt_enable();
 }
+EXPORT_SYMBOL(flush_tlb_page);
 
 static void do_flush_tlb_all(void* info)
 {
@@ -469,6 +472,7 @@ int smp_call_function (void (*func) (voi
 	spin_unlock(&call_lock);
 	return 0;
 }
+EXPORT_SYMBOL(smp_call_function);
 
 void smp_stop_cpu(void)
 {
@@ -500,7 +504,7 @@ void smp_send_stop(void)
 #endif
 	/* Don't deadlock on the call lock in panic */
 	if (!spin_trylock(&call_lock)) {
-		/* ignore locking because we have paniced anyways */
+		/* ignore locking because we have panicked anyways */
 		nolock = 1;
 	}
 	__smp_call_function(smp_really_stop_cpu, NULL, 0, 0);
@@ -572,13 +576,13 @@ int safe_smp_processor_id(void)
 #ifdef CONFIG_XEN
 	return smp_processor_id();
 #else
-	int apicid, i;
+	unsigned apicid, i;
 
 	if (disable_apic)
 		return 0;
 
 	apicid = hard_smp_processor_id();
-	if (x86_cpu_to_apicid[apicid] == apicid)
+	if (apicid < NR_CPUS && x86_cpu_to_apicid[apicid] == apicid)
 		return apicid;
 
 	for (i = 0; i < NR_CPUS; ++i) {
Index: head-2006-11-06/arch/x86_64/kernel/traps-xen.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/traps-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/x86_64/kernel/traps-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -6,15 +6,12 @@
  *
  *  Pentium III FXSR, SSE support
  *	Gareth Hughes <gareth@valinux.com>, May 2000
- *
- *  $Id: traps.c,v 1.36 2002/03/24 11:09:10 ak Exp $
  */
 
 /*
  * 'Traps.c' handles hardware traps and faults after we have saved some
  * state in 'entry.S'.
  */
-#include <linux/config.h>
 #include <linux/sched.h>
 #include <linux/kernel.h>
 #include <linux/string.h>
@@ -31,6 +28,7 @@
 #include <linux/nmi.h>
 #include <linux/kprobes.h>
 #include <linux/kexec.h>
+#include <linux/unwind.h>
 
 #include <asm/system.h>
 #include <asm/uaccess.h>
@@ -41,7 +39,7 @@
 #include <asm/i387.h>
 #include <asm/kdebug.h>
 #include <asm/processor.h>
-
+#include <asm/unwind.h>
 #include <asm/smp.h>
 #include <asm/pgalloc.h>
 #include <asm/pda.h>
@@ -71,19 +69,20 @@ asmlinkage void machine_check(void);
 asmlinkage void spurious_interrupt_bug(void);
 
 ATOMIC_NOTIFIER_HEAD(die_chain);
+EXPORT_SYMBOL(die_chain);
 
 int register_die_notifier(struct notifier_block *nb)
 {
 	vmalloc_sync_all();
 	return atomic_notifier_chain_register(&die_chain, nb);
 }
-EXPORT_SYMBOL(register_die_notifier);
+EXPORT_SYMBOL(register_die_notifier); /* used modular by kdb */
 
 int unregister_die_notifier(struct notifier_block *nb)
 {
 	return atomic_notifier_chain_unregister(&die_chain, nb);
 }
-EXPORT_SYMBOL(unregister_die_notifier);
+EXPORT_SYMBOL(unregister_die_notifier); /* used modular by kdb */
 
 static inline void conditional_sti(struct pt_regs *regs)
 {
@@ -107,31 +106,39 @@ static inline void preempt_conditional_c
 	preempt_enable_no_resched();
 }
 
-static int kstack_depth_to_print = 10;
+static int kstack_depth_to_print = 12;
+#ifdef CONFIG_STACK_UNWIND
+static int call_trace = 1;
+#else
+#define call_trace (-1)
+#endif
 
 #ifdef CONFIG_KALLSYMS
-#include <linux/kallsyms.h> 
-int printk_address(unsigned long address)
-{ 
+# include <linux/kallsyms.h>
+void printk_address(unsigned long address)
+{
 	unsigned long offset = 0, symsize;
 	const char *symname;
 	char *modname;
-	char *delim = ":"; 
+	char *delim = ":";
 	char namebuf[128];
 
-	symname = kallsyms_lookup(address, &symsize, &offset, &modname, namebuf); 
-	if (!symname) 
-		return printk("[<%016lx>]", address);
-	if (!modname) 
+	symname = kallsyms_lookup(address, &symsize, &offset,
+					&modname, namebuf);
+	if (!symname) {
+		printk(" [<%016lx>]\n", address);
+		return;
+	}
+	if (!modname)
 		modname = delim = ""; 		
-        return printk("<%016lx>{%s%s%s%s%+ld}",
-		      address, delim, modname, delim, symname, offset); 
-} 
+	printk(" [<%016lx>] %s%s%s%s+0x%lx/0x%lx\n",
+		address, delim, modname, delim, symname, offset, symsize);
+}
 #else
-int printk_address(unsigned long address)
-{ 
-	return printk("[<%016lx>]", address);
-} 
+void printk_address(unsigned long address)
+{
+	printk(" [<%016lx>]\n", address);
+}
 #endif
 
 static unsigned long *in_exception_stack(unsigned cpu, unsigned long stack,
@@ -150,32 +157,68 @@ static unsigned long *in_exception_stack
 	};
 	unsigned k;
 
+	/*
+	 * Iterate over all exception stacks, and figure out whether
+	 * 'stack' is in one of them:
+	 */
 	for (k = 0; k < N_EXCEPTION_STACKS; k++) {
 		unsigned long end;
 
+		/*
+		 * set 'end' to the end of the exception stack.
+		 */
 		switch (k + 1) {
+		/*
+		 * TODO: this block is not needed i think, because
+		 * setup64.c:cpu_init() sets up t->ist[DEBUG_STACK]
+		 * properly too.
+		 */
 #if DEBUG_STKSZ > EXCEPTION_STKSZ
 		case DEBUG_STACK:
 			end = cpu_pda(cpu)->debugstack + DEBUG_STKSZ;
 			break;
 #endif
 		default:
-			end = per_cpu(init_tss, cpu).ist[k];
+			end = per_cpu(orig_ist, cpu).ist[k];
 			break;
 		}
+		/*
+		 * Is 'stack' above this exception frame's end?
+		 * If yes then skip to the next frame.
+		 */
 		if (stack >= end)
 			continue;
+		/*
+		 * Is 'stack' above this exception frame's start address?
+		 * If yes then we found the right frame.
+		 */
 		if (stack >= end - EXCEPTION_STKSZ) {
+			/*
+			 * Make sure we only iterate through an exception
+			 * stack once. If it comes up for the second time
+			 * then there's something wrong going on - just
+			 * break out and return NULL:
+			 */
 			if (*usedp & (1U << k))
 				break;
 			*usedp |= 1U << k;
 			*idp = ids[k];
 			return (unsigned long *)end;
 		}
+		/*
+		 * If this is a debug stack, and if it has a larger size than
+		 * the usual exception stacks, then 'stack' might still
+		 * be within the lower portion of the debug stack:
+		 */
 #if DEBUG_STKSZ > EXCEPTION_STKSZ
 		if (k == DEBUG_STACK - 1 && stack >= end - DEBUG_STKSZ) {
 			unsigned j = N_EXCEPTION_STACKS - 1;
 
+			/*
+			 * Black magic. A large debug stack is composed of
+			 * multiple exception stack entries, which we
+			 * iterate through now. Dont look:
+			 */
 			do {
 				++j;
 				end -= EXCEPTION_STKSZ;
@@ -193,6 +236,19 @@ static unsigned long *in_exception_stack
 	return NULL;
 }
 
+static int show_trace_unwind(struct unwind_frame_info *info, void *context)
+{
+	int n = 0;
+
+	while (unwind(info) == 0 && UNW_PC(info)) {
+		n++;
+		printk_address(UNW_PC(info));
+		if (arch_unw_user_mode(info))
+			break;
+	}
+	return n;
+}
+
 /*
  * x86-64 can have upto three kernel stacks: 
  * process stack
@@ -200,25 +256,56 @@ static unsigned long *in_exception_stack
  * severe exception (double fault, nmi, stack fault, debug, mce) hardware stack
  */
 
-void show_trace(unsigned long *stack)
+void show_trace(struct task_struct *tsk, struct pt_regs *regs, unsigned long * stack)
 {
 	const unsigned cpu = safe_smp_processor_id();
 	unsigned long *irqstack_end = (unsigned long *)cpu_pda(cpu)->irqstackptr;
-	int i;
 	unsigned used = 0;
 
-	printk("\nCall Trace:");
+	printk("\nCall Trace:\n");
+
+	if (!tsk)
+		tsk = current;
 
+	if (call_trace >= 0) {
+		int unw_ret = 0;
+		struct unwind_frame_info info;
+
+		if (regs) {
+			if (unwind_init_frame_info(&info, tsk, regs) == 0)
+				unw_ret = show_trace_unwind(&info, NULL);
+		} else if (tsk == current)
+			unw_ret = unwind_init_running(&info, show_trace_unwind, NULL);
+		else {
+			if (unwind_init_blocked(&info, tsk) == 0)
+				unw_ret = show_trace_unwind(&info, NULL);
+		}
+		if (unw_ret > 0) {
+			if (call_trace == 1 && !arch_unw_user_mode(&info)) {
+				print_symbol("DWARF2 unwinder stuck at %s\n",
+					     UNW_PC(&info));
+				if ((long)UNW_SP(&info) < 0) {
+					printk("Leftover inexact backtrace:\n");
+					stack = (unsigned long *)UNW_SP(&info);
+				} else
+					printk("Full inexact backtrace again:\n");
+			} else if (call_trace >= 1)
+				return;
+			else
+				printk("Full inexact backtrace again:\n");
+		} else
+			printk("Inexact backtrace:\n");
+	}
+
+	/*
+	 * Print function call entries within a stack. 'cond' is the
+	 * "end of stackframe" condition, that the 'stack++'
+	 * iteration will eventually trigger.
+	 */
 #define HANDLE_STACK(cond) \
 	do while (cond) { \
 		unsigned long addr = *stack++; \
 		if (kernel_text_address(addr)) { \
-			if (i > 50) { \
-				printk("\n       "); \
-				i = 0; \
-			} \
-			else \
-				i += printk(" "); \
 			/* \
 			 * If the address is either in the text segment of the \
 			 * kernel, or in the region which contains vmalloc'ed \
@@ -227,20 +314,30 @@ void show_trace(unsigned long *stack)
 			 * down the cause of the crash will be able to figure \
 			 * out the call path that was taken. \
 			 */ \
-			i += printk_address(addr); \
+			printk_address(addr); \
 		} \
 	} while (0)
 
-	for(i = 11; ; ) {
+	/*
+	 * Print function call entries in all stacks, starting at the
+	 * current stack address. If the stacks consist of nested
+	 * exceptions
+	 */
+	for ( ; ; ) {
 		const char *id;
 		unsigned long *estack_end;
 		estack_end = in_exception_stack(cpu, (unsigned long)stack,
 						&used, &id);
 
 		if (estack_end) {
-			i += printk(" <%s>", id);
+			printk(" <%s>", id);
 			HANDLE_STACK (stack < estack_end);
-			i += printk(" <EOE>");
+			printk(" <EOE>");
+			/*
+			 * We link to the next stack via the
+			 * second-to-last pointer (index -2 to end) in the
+			 * exception stack:
+			 */
 			stack = (unsigned long *) estack_end[-2];
 			continue;
 		}
@@ -250,23 +347,32 @@ void show_trace(unsigned long *stack)
 				(IRQSTACKSIZE - 64) / sizeof(*irqstack);
 
 			if (stack >= irqstack && stack < irqstack_end) {
-				i += printk(" <IRQ>");
+				printk(" <IRQ>");
 				HANDLE_STACK (stack < irqstack_end);
+				/*
+				 * We link to the next stack (which would be
+				 * the process stack normally) the last
+				 * pointer (index -1 to end) in the IRQ stack:
+				 */
 				stack = (unsigned long *) (irqstack_end[-1]);
 				irqstack_end = NULL;
-				i += printk(" <EOI>");
+				printk(" <EOI>");
 				continue;
 			}
 		}
 		break;
 	}
 
+	/*
+	 * This prints the process stack:
+	 */
 	HANDLE_STACK (((long) stack & (THREAD_SIZE-1)) != 0);
 #undef HANDLE_STACK
+
 	printk("\n");
 }
 
-void show_stack(struct task_struct *tsk, unsigned long * rsp)
+static void _show_stack(struct task_struct *tsk, struct pt_regs *regs, unsigned long * rsp)
 {
 	unsigned long *stack;
 	int i;
@@ -296,11 +402,16 @@ void show_stack(struct task_struct *tsk,
 			break;
 		}
 		if (i && ((i % 4) == 0))
-			printk("\n       ");
-		printk("%016lx ", *stack++);
+			printk("\n");
+		printk(" %016lx", *stack++);
 		touch_nmi_watchdog();
 	}
-	show_trace((unsigned long *)rsp);
+	show_trace(tsk, regs, rsp);
+}
+
+void show_stack(struct task_struct *tsk, unsigned long * rsp)
+{
+	_show_stack(tsk, NULL, rsp);
 }
 
 /*
@@ -309,7 +420,7 @@ void show_stack(struct task_struct *tsk,
 void dump_stack(void)
 {
 	unsigned long dummy;
-	show_trace(&dummy);
+	show_trace(NULL, NULL, &dummy);
 }
 
 EXPORT_SYMBOL(dump_stack);
@@ -336,7 +447,7 @@ void show_registers(struct pt_regs *regs
 	if (in_kernel) {
 
 		printk("Stack: ");
-		show_stack(NULL, (unsigned long*)rsp);
+		_show_stack(NULL, regs, (unsigned long*)rsp);
 
 		printk("\nCode: ");
 		if (regs->rip < PAGE_OFFSET)
@@ -385,6 +496,7 @@ void out_of_line_bug(void)
 { 
 	BUG(); 
 } 
+EXPORT_SYMBOL(out_of_line_bug);
 #endif
 
 static DEFINE_SPINLOCK(die_lock);
@@ -423,7 +535,7 @@ void __kprobes oops_end(unsigned long fl
 		/* Nest count reaches zero, release the lock. */
 		spin_unlock_irqrestore(&die_lock, flags);
 	if (panic_on_oops)
-		panic("Oops");
+		panic("Fatal exception");
 }
 
 void __kprobes __die(const char * str, struct pt_regs * regs, long err)
@@ -1046,3 +1158,18 @@ static int __init kstack_setup(char *s)
 }
 __setup("kstack=", kstack_setup);
 
+#ifdef CONFIG_STACK_UNWIND
+static int __init call_trace_setup(char *s)
+{
+	if (strcmp(s, "old") == 0)
+		call_trace = -1;
+	else if (strcmp(s, "both") == 0)
+		call_trace = 0;
+	else if (strcmp(s, "newfallback") == 0)
+		call_trace = 1;
+	else if (strcmp(s, "new") == 0)
+		call_trace = 2;
+	return 1;
+}
+__setup("call_trace=", call_trace_setup);
+#endif
Index: head-2006-11-06/arch/x86_64/kernel/vsyscall-xen.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/vsyscall-xen.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/arch/x86_64/kernel/vsyscall-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -107,7 +107,7 @@ static __always_inline long time_syscall
 
 int __vsyscall(0) vgettimeofday(struct timeval * tv, struct timezone * tz)
 {
-	if (unlikely(!__sysctl_vsyscall))
+	if (!__sysctl_vsyscall)
 		return gettimeofday(tv,tz);
 	if (tv)
 		do_vgettimeofday(tv);
@@ -120,7 +120,7 @@ int __vsyscall(0) vgettimeofday(struct t
  * unlikely */
 time_t __vsyscall(1) vtime(time_t *t)
 {
-	if (unlikely(!__sysctl_vsyscall))
+	if (!__sysctl_vsyscall)
 		return time_syscall(t);
 	else if (t)
 		*t = __xtime.tv_sec;		
Index: head-2006-11-06/arch/x86_64/kernel/x8664_ksyms-xen.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/kernel/x8664_ksyms-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ /dev/null	1970-01-01 00:00:00.000000000 +0000
@@ -1,154 +0,0 @@
-#include <linux/config.h>
-#include <linux/module.h>
-#include <linux/smp.h>
-#include <linux/user.h>
-#include <linux/sched.h>
-#include <linux/in6.h>
-#include <linux/interrupt.h>
-#include <linux/smp_lock.h>
-#include <linux/pm.h>
-#include <linux/pci.h>
-#include <linux/apm_bios.h>
-#include <linux/kernel.h>
-#include <linux/string.h>
-#include <linux/syscalls.h>
-#include <linux/tty.h>
-
-#include <asm/semaphore.h>
-#include <asm/processor.h>
-#include <asm/i387.h>
-#include <asm/uaccess.h>
-#include <asm/checksum.h>
-#include <asm/io.h>
-#include <asm/delay.h>
-#include <asm/irq.h>
-#include <asm/mmx.h>
-#include <asm/desc.h>
-#include <asm/pgtable.h>
-#include <asm/pgalloc.h>
-#include <asm/nmi.h>
-#include <asm/kdebug.h>
-#include <asm/unistd.h>
-#include <asm/tlbflush.h>
-#include <asm/kdebug.h>
-
-#ifdef CONFIG_SMP
-extern void __write_lock_failed(rwlock_t *rw);
-extern void __read_lock_failed(rwlock_t *rw);
-#endif
-
-/* platform dependent support */
-EXPORT_SYMBOL(boot_cpu_data);
-//EXPORT_SYMBOL(dump_fpu);
-EXPORT_SYMBOL(kernel_thread);
-EXPORT_SYMBOL(pm_idle);
-
-EXPORT_SYMBOL(__down_failed);
-EXPORT_SYMBOL(__down_failed_interruptible);
-EXPORT_SYMBOL(__down_failed_trylock);
-EXPORT_SYMBOL(__up_wakeup);
-/* Networking helper routines. */
-EXPORT_SYMBOL(csum_partial_copy_nocheck);
-EXPORT_SYMBOL(ip_compute_csum);
-/* Delay loops */
-EXPORT_SYMBOL(__udelay);
-EXPORT_SYMBOL(__ndelay);
-EXPORT_SYMBOL(__delay);
-EXPORT_SYMBOL(__const_udelay);
-
-EXPORT_SYMBOL(__get_user_1);
-EXPORT_SYMBOL(__get_user_2);
-EXPORT_SYMBOL(__get_user_4);
-EXPORT_SYMBOL(__get_user_8);
-EXPORT_SYMBOL(__put_user_1);
-EXPORT_SYMBOL(__put_user_2);
-EXPORT_SYMBOL(__put_user_4);
-EXPORT_SYMBOL(__put_user_8);
-
-EXPORT_SYMBOL(strncpy_from_user);
-EXPORT_SYMBOL(__strncpy_from_user);
-EXPORT_SYMBOL(clear_user);
-EXPORT_SYMBOL(__clear_user);
-EXPORT_SYMBOL(copy_user_generic);
-EXPORT_SYMBOL(copy_from_user);
-EXPORT_SYMBOL(copy_to_user);
-EXPORT_SYMBOL(copy_in_user);
-EXPORT_SYMBOL(strnlen_user);
-
-#ifdef CONFIG_PCI
-EXPORT_SYMBOL(pci_mem_start);
-#endif
-
-EXPORT_SYMBOL(copy_page);
-EXPORT_SYMBOL(clear_page);
-
-EXPORT_SYMBOL(_cpu_pda);
-#ifdef CONFIG_SMP
-EXPORT_SYMBOL(__write_lock_failed);
-EXPORT_SYMBOL(__read_lock_failed);
-
-EXPORT_SYMBOL(smp_call_function);
-#endif
-
-#ifdef CONFIG_VT
-EXPORT_SYMBOL(screen_info);
-#endif
-
-#ifdef CONFIG_X86_LOCAL_APIC
-EXPORT_SYMBOL_GPL(set_nmi_callback);
-EXPORT_SYMBOL_GPL(unset_nmi_callback);
-#endif
-
-/* Export string functions. We normally rely on gcc builtin for most of these,
-   but gcc sometimes decides not to inline them. */    
-#undef memcpy
-#undef memset
-#undef memmove
-
-extern void * memset(void *,int,__kernel_size_t);
-extern size_t strlen(const char *);
-extern void * memmove(void * dest,const void *src,size_t count);
-extern void * memcpy(void *,const void *,__kernel_size_t);
-extern void * __memcpy(void *,const void *,__kernel_size_t);
-
-EXPORT_SYMBOL(memset);
-EXPORT_SYMBOL(memmove);
-EXPORT_SYMBOL(memcpy);
-EXPORT_SYMBOL(__memcpy);
-
-#ifdef CONFIG_RWSEM_XCHGADD_ALGORITHM
-/* prototypes are wrong, these are assembly with custom calling functions */
-extern void rwsem_down_read_failed_thunk(void);
-extern void rwsem_wake_thunk(void);
-extern void rwsem_downgrade_thunk(void);
-extern void rwsem_down_write_failed_thunk(void);
-EXPORT_SYMBOL(rwsem_down_read_failed_thunk);
-EXPORT_SYMBOL(rwsem_wake_thunk);
-EXPORT_SYMBOL(rwsem_downgrade_thunk);
-EXPORT_SYMBOL(rwsem_down_write_failed_thunk);
-#endif
-
-EXPORT_SYMBOL(empty_zero_page);
-
-EXPORT_SYMBOL(die_chain);
-
-#ifdef CONFIG_SMP
-EXPORT_SYMBOL(cpu_sibling_map);
-EXPORT_SYMBOL(smp_num_siblings);
-#endif
-
-#ifdef CONFIG_BUG
-EXPORT_SYMBOL(out_of_line_bug);
-#endif
-
-EXPORT_SYMBOL(init_level4_pgt);
-
-extern unsigned long __supported_pte_mask;
-EXPORT_SYMBOL(__supported_pte_mask);
-
-#ifdef CONFIG_SMP
-EXPORT_SYMBOL(flush_tlb_page);
-#endif
-
-EXPORT_SYMBOL(load_gs_index);
-
Index: head-2006-11-06/arch/x86_64/mm/fault-xen.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/mm/fault-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/x86_64/mm/fault-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -5,7 +5,6 @@
  *  Copyright (C) 2001,2002 Andi Kleen, SuSE Labs.
  */
 
-#include <linux/config.h>
 #include <linux/signal.h>
 #include <linux/sched.h>
 #include <linux/kernel.h>
@@ -41,6 +40,41 @@
 #define PF_RSVD	(1<<3)
 #define PF_INSTR	(1<<4)
 
+#ifdef CONFIG_KPROBES
+ATOMIC_NOTIFIER_HEAD(notify_page_fault_chain);
+
+/* Hook to register for page fault notifications */
+int register_page_fault_notifier(struct notifier_block *nb)
+{
+	vmalloc_sync_all();
+	return atomic_notifier_chain_register(&notify_page_fault_chain, nb);
+}
+
+int unregister_page_fault_notifier(struct notifier_block *nb)
+{
+	return atomic_notifier_chain_unregister(&notify_page_fault_chain, nb);
+}
+
+static inline int notify_page_fault(enum die_val val, const char *str,
+			struct pt_regs *regs, long err, int trap, int sig)
+{
+	struct die_args args = {
+		.regs = regs,
+		.str = str,
+		.err = err,
+		.trapnr = trap,
+		.signr = sig
+	};
+	return atomic_notifier_call_chain(&notify_page_fault_chain, val, &args);
+}
+#else
+static inline int notify_page_fault(enum die_val val, const char *str,
+			struct pt_regs *regs, long err, int trap, int sig)
+{
+	return NOTIFY_DONE;
+}
+#endif
+
 void bust_spinlocks(int yes)
 {
 	int loglevel_save = console_loglevel;
@@ -158,7 +192,7 @@ void dump_pagetable(unsigned long addres
 	printk("PGD %lx ", pgd_val(*pgd));
 	if (!pgd_present(*pgd)) goto ret; 
 
-	pud = __pud_offset_k((pud_t *)pgd_page(*pgd), address);
+	pud = pud_offset(pgd, address);
 	if (bad_address(pud)) goto bad;
 	printk("PUD %lx ", pud_val(*pud));
 	if (!pud_present(*pud))	goto ret;
@@ -410,7 +444,7 @@ asmlinkage void __kprobes do_page_fault(
 		/* Can take a spurious fault if mapping changes R/O -> R/W. */
 		if (spurious_fault(regs, address, error_code))
 			return;
-		if (notify_die(DIE_PAGE_FAULT, "page fault", regs, error_code, 14,
+		if (notify_page_fault(DIE_PAGE_FAULT, "page fault", regs, error_code, 14,
 						SIGSEGV) == NOTIFY_STOP)
 			return;
 		/*
@@ -420,7 +454,7 @@ asmlinkage void __kprobes do_page_fault(
 		goto bad_area_nosemaphore;
 	}
 
-	if (notify_die(DIE_PAGE_FAULT, "page fault", regs, error_code, 14,
+	if (notify_page_fault(DIE_PAGE_FAULT, "page fault", regs, error_code, 14,
 					SIGSEGV) == NOTIFY_STOP)
 		return;
 
@@ -445,7 +479,7 @@ asmlinkage void __kprobes do_page_fault(
 	/* When running in the kernel we expect faults to occur only to
 	 * addresses in user space.  All other faults represent errors in the
 	 * kernel and should generate an OOPS.  Unfortunatly, in the case of an
-	 * erroneous fault occuring in a code path which already holds mmap_sem
+	 * erroneous fault occurring in a code path which already holds mmap_sem
 	 * we will deadlock attempting to validate the fault against the
 	 * address space.  Luckily the kernel only validly references user
 	 * space from well defined areas of code, which are listed in the
@@ -472,8 +506,10 @@ asmlinkage void __kprobes do_page_fault(
 	if (!(vma->vm_flags & VM_GROWSDOWN))
 		goto bad_area;
 	if (error_code & 4) {
-		// XXX: align red zone size with ABI 
-		if (address + 128 < regs->rsp)
+		/* Allow userspace just enough access below the stack pointer
+		 * to let the 'enter' instruction work.
+		 */
+		if (address + 65536 + 32 * sizeof(unsigned long) < regs->rsp)
 			goto bad_area;
 	}
 	if (expand_stack(vma, address))
@@ -596,7 +632,6 @@ no_context:
 		printk(KERN_ALERT "Unable to handle kernel paging request");
 	printk(" at %016lx RIP: \n" KERN_ALERT,address);
 	printk_address(regs->rip);
-	printk("\n");
 	dump_pagetable(address);
 	tsk->thread.cr2 = address;
 	tsk->thread.trap_no = 14;
Index: head-2006-11-06/arch/x86_64/mm/init-xen.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/mm/init-xen.c	2006-11-15 13:38:08.000000000 +0100
+++ head-2006-11-06/arch/x86_64/mm/init-xen.c	2006-11-15 13:38:55.000000000 +0100
@@ -9,7 +9,6 @@
  *	Modified for Xen.
  */
 
-#include <linux/config.h>
 #include <linux/signal.h>
 #include <linux/sched.h>
 #include <linux/kernel.h>
@@ -26,6 +25,7 @@
 #include <linux/bootmem.h>
 #include <linux/proc_fs.h>
 #include <linux/pci.h>
+#include <linux/poison.h>
 #include <linux/dma-mapping.h>
 #include <linux/module.h>
 #include <linux/memory_hotplug.h>
@@ -44,8 +44,6 @@
 #include <asm/proto.h>
 #include <asm/smp.h>
 #include <asm/sections.h>
-#include <asm/dma-mapping.h>
-#include <asm/swiotlb.h>
 
 #include <xen/features.h>
 
@@ -203,8 +201,6 @@ void show_mem(void)
 	printk(KERN_INFO "%lu pages swap cached\n",cached);
 }
 
-/* References to section boundaries */
-
 int after_bootmem;
 
 static __init void *spp_getpage(void)
@@ -385,7 +381,7 @@ unsigned long __initdata table_start, ta
 
 unsigned long get_machine_pfn(unsigned long addr)
 {
-	pud_t* pud = pud_offset_k(NULL, addr);
+	pud_t* pud = pud_offset(pgd_offset_k(addr), addr);
 	pmd_t* pmd = pmd_offset(pud, addr);
 	pte_t *pte = pte_offset_kernel(pmd, addr);
 
@@ -481,8 +477,9 @@ phys_pmd_init(pmd_t *pmd, unsigned long 
 		pte_t *pte, *pte_save;
 
 		if (address >= end) {
-			for (; i < PTRS_PER_PMD; i++, pmd++)
-				set_pmd(pmd, __pmd(0));
+			if (!after_bootmem)
+				for (; i < PTRS_PER_PMD; i++, pmd++)
+					set_pmd(pmd, __pmd(0));
 			break;
 		}
 		pte = alloc_static_page(&pte_phys);
@@ -729,7 +726,7 @@ void __meminit init_memory_mapping(unsig
 		pud_t *pud;
 
 		if (after_bootmem) {
-			pud = pud_offset_k(pgd, start & PGDIR_MASK);
+			pud = pud_offset(pgd, start & PGDIR_MASK);
 			make_page_readonly(pud, XENFEAT_writable_page_tables);
 			pud_phys = __pa(pud);
 		} else {
@@ -916,8 +913,6 @@ void __init clear_kernel_mapping(unsigne
 /*
  * Memory hotplug specific functions
  */
-#if defined(CONFIG_ACPI_HOTPLUG_MEMORY) || defined(CONFIG_ACPI_HOTPLUG_MEMORY_MODULE)
-
 void online_page(struct page *page)
 {
 	ClearPageReserved(page);
@@ -927,31 +922,17 @@ void online_page(struct page *page)
 	num_physpages++;
 }
 
-#ifndef CONFIG_MEMORY_HOTPLUG
+#ifdef CONFIG_MEMORY_HOTPLUG
 /*
- * Memory Hotadd without sparsemem. The mem_maps have been allocated in advance,
- * just online the pages.
+ * XXX: memory_add_physaddr_to_nid() is to find node id from physical address
+ *	via probe interface of sysfs. If acpi notifies hot-add event, then it
+ *	can tell node id by searching dsdt. But, probe interface doesn't have
+ *	node id. So, return 0 as node id at this time.
  */
-int __add_pages(struct zone *z, unsigned long start_pfn, unsigned long nr_pages)
+#ifdef CONFIG_NUMA
+int memory_add_physaddr_to_nid(u64 start)
 {
-	int err = -EIO;
-	unsigned long pfn;
-	unsigned long total = 0, mem = 0;
-	for (pfn = start_pfn; pfn < start_pfn + nr_pages; pfn++) {
-		if (pfn_valid(pfn)) {
-			online_page(pfn_to_page(pfn));
-			err = 0;
-			mem++;
-		}
-		total++;
-	}
-	if (!err) {
-		z->spanned_pages += total;
-		z->present_pages += mem;
-		z->zone_pgdat->node_spanned_pages += total;
-		z->zone_pgdat->node_present_pages += mem;
-	}
-	return err;
+	return 0;
 }
 #endif
 
@@ -959,9 +940,9 @@ int __add_pages(struct zone *z, unsigned
  * Memory is added always to NORMAL zone. This means you will never get
  * additional DMA/DMA32 memory.
  */
-int add_memory(u64 start, u64 size)
+int arch_add_memory(int nid, u64 start, u64 size)
 {
-	struct pglist_data *pgdat = NODE_DATA(0);
+	struct pglist_data *pgdat = NODE_DATA(nid);
 	struct zone *zone = pgdat->node_zones + MAX_NR_ZONES-2;
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
@@ -978,7 +959,7 @@ error:
 	printk("%s: Problem encountered in __add_pages!\n", __func__);
 	return ret;
 }
-EXPORT_SYMBOL_GPL(add_memory);
+EXPORT_SYMBOL_GPL(arch_add_memory);
 
 int remove_memory(u64 start, u64 size)
 {
@@ -986,7 +967,33 @@ int remove_memory(u64 start, u64 size)
 }
 EXPORT_SYMBOL_GPL(remove_memory);
 
-#endif
+#else /* CONFIG_MEMORY_HOTPLUG */
+/*
+ * Memory Hotadd without sparsemem. The mem_maps have been allocated in advance,
+ * just online the pages.
+ */
+int __add_pages(struct zone *z, unsigned long start_pfn, unsigned long nr_pages)
+{
+	int err = -EIO;
+	unsigned long pfn;
+	unsigned long total = 0, mem = 0;
+	for (pfn = start_pfn; pfn < start_pfn + nr_pages; pfn++) {
+		if (pfn_valid(pfn)) {
+			online_page(pfn_to_page(pfn));
+			err = 0;
+			mem++;
+		}
+		total++;
+	}
+	if (!err) {
+		z->spanned_pages += total;
+		z->present_pages += mem;
+		z->zone_pgdat->node_spanned_pages += total;
+		z->zone_pgdat->node_present_pages += mem;
+	}
+	return err;
+}
+#endif /* CONFIG_MEMORY_HOTPLUG */
 
 static struct kcore_list kcore_mem, kcore_vmalloc, kcore_kernel, kcore_modules,
 			 kcore_vsyscall;
@@ -1001,10 +1008,14 @@ void __init mem_init(void)
 	BUG_ON(!contiguous_bitmap);
 	memset(contiguous_bitmap, 0, (end_pfn + 2*BITS_PER_LONG) >> 3);
 
+#ifndef CONFIG_XEN
+	pci_iommu_alloc();
+#else
 #if defined(CONFIG_SWIOTLB)
-	pci_swiotlb_init();	
+	pci_swiotlb_init();
 #endif
 	no_iommu_init();
+#endif
 
 	/* How many end-of-memory variables you have, grandma! */
 	max_low_pfn = end_pfn;
@@ -1066,20 +1077,19 @@ void __init mem_init(void)
 #endif
 }
 
-void free_initmem(void)
+void free_init_pages(char *what, unsigned long begin, unsigned long end)
 {
-#ifdef __DO_LATER__
-	/*
-	 * Some pages can be pinned, but some are not. Unpinning such pages 
-	 * triggers BUG(). 
-	 */
 	unsigned long addr;
 
-	addr = (unsigned long)(&__init_begin);
-	for (; addr < (unsigned long)(&__init_end); addr += PAGE_SIZE) {
+	if (begin >= end)
+		return;
+
+	printk(KERN_INFO "Freeing %s: %ldk freed\n", what, (end - begin) >> 10);
+	for (addr = begin; addr < end; addr += PAGE_SIZE) {
 		ClearPageReserved(virt_to_page(addr));
 		init_page_count(virt_to_page(addr));
-		memset((void *)(addr & ~(PAGE_SIZE-1)), 0xcc, PAGE_SIZE); 
+		memset((void *)(addr & ~(PAGE_SIZE-1)),
+			POISON_FREE_INITMEM, PAGE_SIZE);
 		make_page_writable(
 			__va(__pa(addr)), XENFEAT_writable_page_tables);
 		/*
@@ -1090,23 +1100,34 @@ void free_initmem(void)
 		free_page(addr);
 		totalram_pages++;
 	}
-	memset(__initdata_begin, 0xba, __initdata_end - __initdata_begin);
-	printk ("Freeing unused kernel memory: %luk freed\n", (__init_end - __init_begin) >> 10);
+}
+
+void free_initmem(void)
+{
+	memset(__initdata_begin, POISON_FREE_INITDATA,
+		__initdata_end - __initdata_begin);
+#ifdef __DO_LATER__
+	/*
+	 * Some pages can be pinned, but some are not. Unpinning such pages
+	 * triggers BUG().
+	 */
+	free_init_pages("unused kernel memory",
+			(unsigned long)(&__init_begin),
+			(unsigned long)(&__init_end));
 #endif
 }
 
 #ifdef CONFIG_DEBUG_RODATA
 
-extern char __start_rodata, __end_rodata;
 void mark_rodata_ro(void)
 {
-	unsigned long addr = (unsigned long)&__start_rodata;
+	unsigned long addr = (unsigned long)__start_rodata;
 
-	for (; addr < (unsigned long)&__end_rodata; addr += PAGE_SIZE)
+	for (; addr < (unsigned long)__end_rodata; addr += PAGE_SIZE)
 		change_page_attr_addr(addr, 1, PAGE_KERNEL_RO);
 
 	printk ("Write protecting the kernel read-only data: %luk\n",
-			(&__end_rodata - &__start_rodata) >> 10);
+			(__end_rodata - __start_rodata) >> 10);
 
 	/*
 	 * change_page_attr_addr() requires a global_flush_tlb() call after it.
@@ -1121,15 +1142,7 @@ void mark_rodata_ro(void)
 #ifdef CONFIG_BLK_DEV_INITRD
 void free_initrd_mem(unsigned long start, unsigned long end)
 {
-	if (start >= end)
-		return;
-	printk ("Freeing initrd memory: %ldk freed\n", (end - start) >> 10);
-	for (; start < end; start += PAGE_SIZE) {
-		ClearPageReserved(virt_to_page(start));
-		init_page_count(virt_to_page(start));
-		free_page(start);
-		totalram_pages++;
-	}
+	free_init_pages("initrd memory", start, end);
 }
 #endif
 
@@ -1161,7 +1174,7 @@ int kern_addr_valid(unsigned long addr) 
 	if (pgd_none(*pgd))
 		return 0;
 
-	pud = pud_offset_k(pgd, addr);
+	pud = pud_offset(pgd, addr);
 	if (pud_none(*pud))
 		return 0; 
 
Index: head-2006-11-06/arch/x86_64/mm/pageattr-xen.c
===================================================================
--- head-2006-11-06.orig/arch/x86_64/mm/pageattr-xen.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/arch/x86_64/mm/pageattr-xen.c	2006-10-16 10:19:17.000000000 +0200
@@ -3,7 +3,6 @@
  * Thanks to Ben LaHaise for precious feedback.
  */ 
 
-#include <linux/config.h>
 #include <linux/mm.h>
 #include <linux/sched.h>
 #include <linux/highmem.h>
Index: head-2006-11-06/drivers/oprofile/oprof.c
===================================================================
--- head-2006-11-06.orig/drivers/oprofile/oprof.c	2006-10-16 10:14:37.000000000 +0200
+++ head-2006-11-06/drivers/oprofile/oprof.c	2006-10-16 10:19:17.000000000 +0200
@@ -45,9 +45,9 @@ int oprofile_set_active(int active_domai
 	if (!oprofile_ops.set_active)
 		return -EINVAL;
 
-	down(&start_sem);
+	mutex_lock(&start_mutex);
 	err = oprofile_ops.set_active(active_domains, adomains);
-	up(&start_sem);
+	mutex_unlock(&start_mutex);
 	return err;
 }
 
@@ -58,9 +58,9 @@ int oprofile_set_passive(int passive_dom
 	if (!oprofile_ops.set_passive)
 		return -EINVAL;
 
-	down(&start_sem);
+	mutex_lock(&start_mutex);
 	err = oprofile_ops.set_passive(passive_domains, pdomains);
-	up(&start_sem);
+	mutex_unlock(&start_mutex);
 	return err;
 }
 #endif
Index: head-2006-11-06/drivers/xen/blkfront/block.h
===================================================================
--- head-2006-11-06.orig/drivers/xen/blkfront/block.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/drivers/xen/blkfront/block.h	2006-10-16 10:19:17.000000000 +0200
@@ -47,7 +47,6 @@
 #include <linux/hdreg.h>
 #include <linux/blkdev.h>
 #include <linux/major.h>
-#include <linux/devfs_fs_kernel.h>
 #include <asm/hypervisor.h>
 #include <xen/xenbus.h>
 #include <xen/gnttab.h>
Index: head-2006-11-06/drivers/xen/blkfront/vbd.c
===================================================================
--- head-2006-11-06.orig/drivers/xen/blkfront/vbd.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/drivers/xen/blkfront/vbd.c	2006-10-16 10:19:17.000000000 +0200
@@ -130,7 +130,6 @@ xlbd_alloc_major_info(int major, int min
 		return NULL;
 	}
 
-	devfs_mk_dir(ptr->type->devname);
 	major_info[index] = ptr;
 	return ptr;
 }
Index: head-2006-11-06/drivers/xen/blktap/blktap.c
===================================================================
--- head-2006-11-06.orig/drivers/xen/blktap/blktap.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/drivers/xen/blktap/blktap.c	2006-10-16 10:19:17.000000000 +0200
@@ -49,7 +49,6 @@
 #include <linux/gfp.h>
 #include <linux/poll.h>
 #include <asm/tlbflush.h>
-#include <linux/devfs_fs_kernel.h>
 
 #define MAX_TAP_DEV 100     /*the maximum number of tapdisk ring devices    */
 #define MAX_DEV_NAME 100    /*the max tapdisk ring device name e.g. blktap0 */
@@ -330,7 +329,7 @@ static int blktap_ioctl(struct inode *in
                         unsigned int cmd, unsigned long arg);
 static unsigned int blktap_poll(struct file *file, poll_table *wait);
 
-static struct file_operations blktap_fops = {
+static const struct file_operations blktap_fops = {
 	.owner   = THIS_MODULE,
 	.poll    = blktap_poll,
 	.ioctl   = blktap_ioctl,
@@ -1319,7 +1318,7 @@ static void make_response(blkif_t *blkif
 
 static int __init blkif_init(void)
 {
-	int i,ret,blktap_dir;
+	int i,ret;
 	tap_blkif_t *info;
 
 	if (!is_running_on_xen())
@@ -1345,9 +1344,8 @@ static int __init blkif_init(void)
 
 	/* Dynamically allocate a major for this device */
 	ret = register_chrdev(0, "blktap", &blktap_fops);
-	blktap_dir = devfs_mk_dir(NULL, "xen", 0, NULL);
 
-	if ( (ret < 0)||(blktap_dir < 0) ) {
+	if ( (ret < 0) ) {
 		WPRINTK("Couldn't register /dev/xen/blktap\n");
 		return -ENOMEM;
 	}	
@@ -1362,13 +1360,10 @@ static int __init blkif_init(void)
 		info->pid = 0;
 		info->blkif = NULL;
 
-		ret = devfs_mk_cdev(MKDEV(blktap_major, i),
-			S_IFCHR|S_IRUGO|S_IWUSR, "xen/blktap%d", i);
-
-		if(ret != 0)
-			return -ENOMEM;
 		info->dev_pending = info->dev_inuse = 0;
 
+		//?? class_device_create(blktap_class, NULL, MKDEV(BLKTAP_DEV_MAJOR, i), NULL, "blktap%d", i);
+
 		DPRINTK("Created misc_dev [/dev/xen/blktap%d]\n",i);
 	}
 	
Index: head-2006-11-06/drivers/xen/char/mem.c
===================================================================
--- head-2006-11-06.orig/drivers/xen/char/mem.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/drivers/xen/char/mem.c	2006-10-16 10:19:17.000000000 +0200
@@ -8,7 +8,6 @@
  *  Shared /dev/zero mmaping support, Feb 2000, Kanoj Sarcar <kanoj@sgi.com>
  */
 
-#include <linux/config.h>
 #include <linux/mm.h>
 #include <linux/miscdevice.h>
 #include <linux/slab.h>
@@ -20,7 +19,6 @@
 #include <linux/tty.h>
 #include <linux/capability.h>
 #include <linux/smp_lock.h>
-#include <linux/devfs_fs_kernel.h>
 #include <linux/ptrace.h>
 #include <linux/device.h>
 #include <asm/pgalloc.h>
@@ -173,7 +171,7 @@ static int open_mem(struct inode * inode
 	return capable(CAP_SYS_RAWIO) ? 0 : -EPERM;
 }
 
-struct file_operations mem_fops = {
+const struct file_operations mem_fops = {
 	.llseek		= memory_lseek,
 	.read		= read_mem,
 	.write		= write_mem,
Index: head-2006-11-06/drivers/xen/console/console.c
===================================================================
--- head-2006-11-06.orig/drivers/xen/console/console.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/drivers/xen/console/console.c	2006-10-16 10:19:17.000000000 +0200
@@ -39,6 +39,7 @@
 #include <linux/interrupt.h>
 #include <linux/tty.h>
 #include <linux/tty_flip.h>
+#include <linux/vt.h>
 #include <linux/serial.h>
 #include <linux/major.h>
 #include <linux/ptrace.h>
Index: head-2006-11-06/drivers/xen/core/cpu_hotplug.c
===================================================================
--- head-2006-11-06.orig/drivers/xen/core/cpu_hotplug.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/drivers/xen/core/cpu_hotplug.c	2006-10-16 10:19:17.000000000 +0200
@@ -93,7 +93,7 @@ static int setup_cpu_watcher(struct noti
 	(void)register_xenbus_watch(&cpu_watch);
 
 	if (!is_initial_xendomain()) {
-		for_each_cpu(i)
+		for_each_possible_cpu(i)
 			vcpu_hotplug(i);
 		printk(KERN_INFO "Brought up %ld CPUs\n",
 		       (long)num_online_cpus());
@@ -139,7 +139,7 @@ int smp_suspend(void)
 			if (err) {
 				printk(KERN_CRIT "Failed to take all CPUs "
 				       "down: %d.\n", err);
-				for_each_cpu(i)
+				for_each_possible_cpu(i)
 					vcpu_hotplug(i);
 				return err;
 			}
@@ -154,12 +154,12 @@ void smp_resume(void)
 {
 	int cpu;
 
-	for_each_cpu(cpu)
+	for_each_possible_cpu(cpu)
 		cpu_initialize_context(cpu);
 
 	unlock_cpu_hotplug();
 
-	for_each_cpu(cpu)
+	for_each_possible_cpu(cpu)
 		vcpu_hotplug(cpu);
 }
 
Index: head-2006-11-06/drivers/xen/core/evtchn.c
===================================================================
--- head-2006-11-06.orig/drivers/xen/core/evtchn.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/drivers/xen/core/evtchn.c	2006-10-16 10:19:17.000000000 +0200
@@ -473,6 +473,19 @@ static void set_affinity_irq(unsigned ir
 	rebind_irq_to_cpu(irq, tcpu);
 }
 
+static int retrigger_irq(unsigned int i)
+{
+	int evtchn = evtchn_from_irq(i);
+	shared_info_t *s = HYPERVISOR_shared_info;
+
+	if (!VALID_EVTCHN(evtchn))
+		return 0;
+	BUG_ON(!synch_test_bit(evtchn, &s->evtchn_mask[0]));
+	synch_set_bit(evtchn, &s->evtchn_pending[0]);
+
+	return 1;
+}
+
 /*
  * Interface to generic handling in irq.c
  */
@@ -531,14 +544,15 @@ static void end_dynirq(unsigned int irq)
 }
 
 static struct hw_interrupt_type dynirq_type = {
-	"Dynamic-irq",
-	startup_dynirq,
-	shutdown_dynirq,
-	enable_dynirq,
-	disable_dynirq,
-	ack_dynirq,
-	end_dynirq,
-	set_affinity_irq
+	.typename = "Dynamic-irq",
+	.startup = startup_dynirq,
+	.shutdown = shutdown_dynirq,
+	.enable = enable_dynirq,
+	.disable = disable_dynirq,
+	.ack = ack_dynirq,
+	.end = end_dynirq,
+	.set_affinity = set_affinity_irq,
+	.retrigger = retrigger_irq
 };
 
 static inline void pirq_unmask_notify(int pirq)
@@ -656,14 +670,15 @@ static void end_pirq(unsigned int irq)
 }
 
 static struct hw_interrupt_type pirq_type = {
-	"Phys-irq",
-	startup_pirq,
-	shutdown_pirq,
-	enable_pirq,
-	disable_pirq,
-	ack_pirq,
-	end_pirq,
-	set_affinity_irq
+	.typename = "Phys-irq",
+	.startup = startup_pirq,
+	.shutdown = shutdown_pirq,
+	.enable = enable_pirq,
+	.disable = disable_pirq,
+	.ack = ack_pirq,
+	.end = end_pirq,
+	.set_affinity = set_affinity_irq,
+	.retrigger = retrigger_irq
 };
 
 int irq_ignore_unhandled(unsigned int irq)
@@ -677,16 +692,6 @@ int irq_ignore_unhandled(unsigned int ir
 	return !!(irq_status.flags & XENIRQSTAT_shared);
 }
 
-void resend_irq_on_evtchn(struct hw_interrupt_type *h, unsigned int i)
-{
-	int evtchn = evtchn_from_irq(i);
-	shared_info_t *s = HYPERVISOR_shared_info;
-	if (!VALID_EVTCHN(evtchn))
-		return;
-	BUG_ON(!synch_test_bit(evtchn, &s->evtchn_mask[0]));
-	synch_set_bit(evtchn, &s->evtchn_pending[0]);
-}
-
 void notify_remote_via_irq(int irq)
 {
 	int evtchn = evtchn_from_irq(irq);
@@ -831,7 +836,7 @@ void __init xen_init_IRQ(void)
 		irq_desc[dynirq_to_irq(i)].status  = IRQ_DISABLED;
 		irq_desc[dynirq_to_irq(i)].action  = NULL;
 		irq_desc[dynirq_to_irq(i)].depth   = 1;
-		irq_desc[dynirq_to_irq(i)].handler = &dynirq_type;
+		irq_desc[dynirq_to_irq(i)].chip    = &dynirq_type;
 	}
 
 	/* Phys IRQ space is statically bound (1:1 mapping). Nail refcnts. */
@@ -847,6 +852,6 @@ void __init xen_init_IRQ(void)
 		irq_desc[pirq_to_irq(i)].status  = IRQ_DISABLED;
 		irq_desc[pirq_to_irq(i)].action  = NULL;
 		irq_desc[pirq_to_irq(i)].depth   = 1;
-		irq_desc[pirq_to_irq(i)].handler = &pirq_type;
+		irq_desc[pirq_to_irq(i)].chip    = &pirq_type;
 	}
 }
Index: head-2006-11-06/drivers/xen/core/smpboot.c
===================================================================
--- head-2006-11-06.orig/drivers/xen/core/smpboot.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/drivers/xen/core/smpboot.c	2006-10-16 10:19:17.000000000 +0200
@@ -257,7 +257,7 @@ void __init smp_prepare_cpus(unsigned in
 		cpu_clear(cpu, cpu_possible_map);
 	}
 
-	for_each_cpu (cpu) {
+	for_each_possible_cpu(cpu) {
 		if (cpu == 0)
 			continue;
 
Index: head-2006-11-06/drivers/xen/evtchn/evtchn.c
===================================================================
--- head-2006-11-06.orig/drivers/xen/evtchn/evtchn.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/drivers/xen/evtchn/evtchn.c	2006-10-16 10:19:17.000000000 +0200
@@ -407,7 +407,7 @@ static int evtchn_release(struct inode *
 	return 0;
 }
 
-static struct file_operations evtchn_fops = {
+static const struct file_operations evtchn_fops = {
 	.owner   = THIS_MODULE,
 	.read    = evtchn_read,
 	.write   = evtchn_write,
Index: head-2006-11-06/drivers/xen/pcifront/pci_op.c
===================================================================
--- head-2006-11-06.orig/drivers/xen/pcifront/pci_op.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/drivers/xen/pcifront/pci_op.c	2006-10-16 10:19:17.000000000 +0200
@@ -237,19 +237,20 @@ int pcifront_scan_root(struct pcifront_d
 
 static void free_root_bus_devs(struct pci_bus *bus)
 {
+	extern struct rw_semaphore pci_bus_sem;
 	struct pci_dev *dev;
 
-	spin_lock(&pci_bus_lock);
+	down_read(&pci_bus_sem);
 	while (!list_empty(&bus->devices)) {
 		dev = container_of(bus->devices.next, struct pci_dev, bus_list);
-		spin_unlock(&pci_bus_lock);
+		up_read(&pci_bus_sem);
 
 		dev_dbg(&dev->dev, "removing device\n");
 		pci_remove_bus_device(dev);
 
-		spin_lock(&pci_bus_lock);
+		down_read(&pci_bus_sem);
 	}
-	spin_unlock(&pci_bus_lock);
+	up_read(&pci_bus_sem);
 }
 
 void pcifront_free_roots(struct pcifront_device *pdev)
Index: head-2006-11-06/drivers/xen/privcmd/privcmd.c
===================================================================
--- head-2006-11-06.orig/drivers/xen/privcmd/privcmd.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/drivers/xen/privcmd/privcmd.c	2006-10-16 10:19:17.000000000 +0200
@@ -258,7 +258,7 @@ static int privcmd_enforce_singleshot_ma
 }
 #endif
 
-static struct file_operations privcmd_file_ops = {
+static const struct file_operations privcmd_file_ops = {
 	.ioctl = privcmd_ioctl,
 	.mmap  = privcmd_mmap,
 };
Index: head-2006-11-06/drivers/xen/tpmback/tpmback.c
===================================================================
--- head-2006-11-06.orig/drivers/xen/tpmback/tpmback.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/drivers/xen/tpmback/tpmback.c	2006-10-16 10:19:17.000000000 +0200
@@ -629,7 +629,7 @@ static unsigned int vtpm_op_poll(struct 
 	return flags;
 }
 
-static struct file_operations vtpm_ops = {
+static const struct file_operations vtpm_ops = {
 	.owner = THIS_MODULE,
 	.llseek = no_llseek,
 	.open = vtpm_op_open,
Index: head-2006-11-06/drivers/xen/xenbus/xenbus_client.c
===================================================================
--- head-2006-11-06.orig/drivers/xen/xenbus/xenbus_client.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/drivers/xen/xenbus/xenbus_client.c	2006-10-16 10:19:17.000000000 +0200
@@ -30,14 +30,12 @@
  * IN THE SOFTWARE.
  */
 
+#include <linux/kernel.h>
 #include <xen/evtchn.h>
 #include <xen/gnttab.h>
 #include <xen/xenbus.h>
 #include <xen/driver_util.h>
 
-/* xenbus_probe.c */
-extern char *kasprintf(const char *fmt, ...);
-
 #define DPRINTK(fmt, args...) \
     pr_debug("xenbus_client (%s:%d) " fmt ".\n", __FUNCTION__, __LINE__, ##args)
 
@@ -84,7 +82,7 @@ int xenbus_watch_path2(struct xenbus_dev
 					const char **, unsigned int))
 {
 	int err;
-	char *state = kasprintf("%s/%s", path, path2);
+	char *state = kasprintf(GFP_KERNEL, "%s/%s", path, path2);
 	if (!state) {
 		xenbus_dev_fatal(dev, -ENOMEM, "allocating path for watch");
 		return -ENOMEM;
@@ -152,7 +150,7 @@ EXPORT_SYMBOL_GPL(xenbus_frontend_closed
  */
 static char *error_path(struct xenbus_device *dev)
 {
-	return kasprintf("error/%s", dev->nodename);
+	return kasprintf(GFP_KERNEL, "error/%s", dev->nodename);
 }
 
 
Index: head-2006-11-06/drivers/xen/xenbus/xenbus_dev.c
===================================================================
--- head-2006-11-06.orig/drivers/xen/xenbus/xenbus_dev.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/drivers/xen/xenbus/xenbus_dev.c	2006-10-16 10:19:17.000000000 +0200
@@ -338,7 +338,7 @@ static unsigned int xenbus_dev_poll(stru
 	return 0;
 }
 
-static struct file_operations xenbus_dev_file_ops = {
+static const struct file_operations xenbus_dev_file_ops = {
 	.read = xenbus_dev_read,
 	.write = xenbus_dev_write,
 	.open = xenbus_dev_open,
Index: head-2006-11-06/drivers/xen/xenbus/xenbus_probe.c
===================================================================
--- head-2006-11-06.orig/drivers/xen/xenbus/xenbus_probe.c	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/drivers/xen/xenbus/xenbus_probe.c	2006-10-16 10:19:17.000000000 +0200
@@ -559,27 +559,6 @@ static void xenbus_dev_release(struct de
 		kfree(to_xenbus_device(dev));
 }
 
-/* Simplified asprintf. */
-char *kasprintf(const char *fmt, ...)
-{
-	va_list ap;
-	unsigned int len;
-	char *p, dummy[1];
-
-	va_start(ap, fmt);
-	/* FIXME: vsnprintf has a bug, NULL should work */
-	len = vsnprintf(dummy, 0, fmt, ap);
-	va_end(ap);
-
-	p = kmalloc(len + 1, GFP_KERNEL);
-	if (!p)
-		return NULL;
-	va_start(ap, fmt);
-	vsprintf(p, fmt, ap);
-	va_end(ap);
-	return p;
-}
-
 static ssize_t xendev_show_nodename(struct device *dev,
 				    struct device_attribute *attr, char *buf)
 {
@@ -656,7 +635,7 @@ static int xenbus_probe_frontend(const c
 	char *nodename;
 	int err;
 
-	nodename = kasprintf("%s/%s/%s", xenbus_frontend.root, type, name);
+	nodename = kasprintf(GFP_KERNEL, "%s/%s/%s", xenbus_frontend.root, type, name);
 	if (!nodename)
 		return -ENOMEM;
 
@@ -675,7 +654,7 @@ static int xenbus_probe_backend_unit(con
 	char *nodename;
 	int err;
 
-	nodename = kasprintf("%s/%s", dir, name);
+	nodename = kasprintf(GFP_KERNEL, "%s/%s", dir, name);
 	if (!nodename)
 		return -ENOMEM;
 
@@ -696,7 +675,7 @@ static int xenbus_probe_backend(const ch
 
 	DPRINTK("");
 
-	nodename = kasprintf("%s/%s/%s", xenbus_backend.root, type, domid);
+	nodename = kasprintf(GFP_KERNEL, "%s/%s/%s", xenbus_backend.root, type, domid);
 	if (!nodename)
 		return -ENOMEM;
 
@@ -802,7 +781,7 @@ static void dev_changed(const char *node
 	rootlen = strsep_len(node, '/', bus->levels);
 	if (rootlen < 0)
 		return;
-	root = kasprintf("%.*s", rootlen, node);
+	root = kasprintf(GFP_KERNEL, "%.*s", rootlen, node);
 	if (!root)
 		return;
 
Index: head-2006-11-06/drivers/xen/xenbus/xenbus_xs.c
===================================================================
--- head-2006-11-06.orig/drivers/xen/xenbus/xenbus_xs.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/drivers/xen/xenbus/xenbus_xs.c	2006-10-16 10:19:17.000000000 +0200
@@ -45,9 +45,6 @@
 #include <xen/xenbus.h>
 #include "xenbus_comms.h"
 
-/* xenbus_probe.c */
-extern char *kasprintf(const char *fmt, ...);
-
 struct xs_stored_msg {
 	struct list_head list;
 
@@ -289,9 +286,9 @@ static char *join(const char *dir, const
 	char *buffer;
 
 	if (strlen(name) == 0)
-		buffer = kasprintf("%s", dir);
+		buffer = kasprintf(GFP_KERNEL, "%s", dir);
 	else
-		buffer = kasprintf("%s/%s", dir, name);
+		buffer = kasprintf(GFP_KERNEL, "%s/%s", dir, name);
 	return (!buffer) ? ERR_PTR(-ENOMEM) : buffer;
 }
 
Index: head-2006-11-06/fs/aio.c
===================================================================
--- head-2006-11-06.orig/fs/aio.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/fs/aio.c	2006-10-16 10:19:17.000000000 +0200
@@ -1294,7 +1294,7 @@ static unsigned int aio_queue_fd_poll(st
 	return pollflags;
 }
 
-static struct file_operations aioq_fops = {
+static const struct file_operations aioq_fops = {
 	.release	= aio_queue_fd_close,
 	.poll		= aio_queue_fd_poll
 };
Index: head-2006-11-06/fs/eventpoll.c
===================================================================
--- head-2006-11-06.orig/fs/eventpoll.c	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/fs/eventpoll.c	2006-10-16 10:19:17.000000000 +0200
@@ -265,7 +265,7 @@ static int ep_events_transfer(struct eve
 static int ep_poll(struct eventpoll *ep, struct epoll_event __user *events,
 		   int maxevents, long timeout);
 static int eventpollfs_delete_dentry(struct dentry *dentry);
-static struct inode *ep_eventpoll_inode(struct file_operations *fops);
+static struct inode *ep_eventpoll_inode(const struct file_operations *fops);
 static int eventpollfs_get_sb(struct file_system_type *fs_type,
 			      int flags, const char *dev_name,
 			      void *data, struct vfsmount *mnt);
@@ -701,7 +701,7 @@ eexit_1:
  * Creates the file descriptor to be used by the epoll interface.
  */
 int ep_getfd(int *efd, struct inode **einode, struct file **efile,
-		    struct eventpoll *ep, struct file_operations *fops)
+		    struct eventpoll *ep, const struct file_operations *fops)
 {
 	struct qstr this;
 	char name[32];
@@ -1567,7 +1567,7 @@ static int eventpollfs_delete_dentry(str
 }
 
 
-static struct inode *ep_eventpoll_inode(struct file_operations *fops)
+static struct inode *ep_eventpoll_inode(const struct file_operations *fops)
 {
 	int error = -ENOMEM;
 	struct inode *inode = new_inode(eventpoll_mnt->mnt_sb);
Index: head-2006-11-06/include/asm-i386/elf.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/elf.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/asm-i386/elf.h	2006-10-16 10:19:17.000000000 +0200
@@ -137,7 +137,11 @@ extern int dump_task_extended_fpu (struc
 
 #ifdef CONFIG_COMPAT_VDSO
 # define VDSO_COMPAT_BASE	VDSO_HIGH_BASE
-# define VDSO_PRELINK		VDSO_HIGH_BASE
+# ifndef CONFIG_XEN
+#  define VDSO_PRELINK		VDSO_HIGH_BASE
+# else
+#  define VDSO_PRELINK		(0UL - FIX_VDSO * PAGE_SIZE)
+# endif
 #else
 # define VDSO_COMPAT_BASE	VDSO_BASE
 # define VDSO_PRELINK		0
Index: head-2006-11-06/include/asm-i386/mach-xen/asm/fixmap.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/asm/fixmap.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/asm-i386/mach-xen/asm/fixmap.h	2006-10-16 10:19:17.000000000 +0200
@@ -13,7 +13,6 @@
 #ifndef _ASM_FIXMAP_H
 #define _ASM_FIXMAP_H
 
-#include <linux/config.h>
 
 /* used by vmalloc.c, vsyscall.lds.S.
  *
@@ -53,6 +52,7 @@ extern unsigned long __FIXADDR_TOP;
  */
 enum fixed_addresses {
 	FIX_HOLE,
+	FIX_VDSO,
 #ifdef CONFIG_X86_LOCAL_APIC
 	FIX_APIC_BASE,	/* local (CPU) APIC) -- required for SMP or not */
 #endif
Index: head-2006-11-06/include/asm-i386/mach-xen/asm/floppy.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/asm/floppy.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/asm-i386/mach-xen/asm/floppy.h	2006-10-16 10:19:17.000000000 +0200
@@ -88,7 +88,7 @@ static void fd_disable_dma(void)
 
 static int fd_request_irq(void)
 {
-	return request_irq(FLOPPY_IRQ, floppy_hardint,SA_INTERRUPT,
+	return request_irq(FLOPPY_IRQ, floppy_hardint, IRQF_DISABLED,
 					   "floppy", NULL);
 }
 
Index: head-2006-11-06/include/asm-i386/mach-xen/asm/highmem.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/asm/highmem.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/asm-i386/mach-xen/asm/highmem.h	2006-10-16 10:19:17.000000000 +0200
@@ -20,7 +20,6 @@
 
 #ifdef __KERNEL__
 
-#include <linux/config.h>
 #include <linux/interrupt.h>
 #include <linux/threads.h>
 #include <asm/kmap_types.h>
Index: head-2006-11-06/include/asm-i386/mach-xen/asm/hw_irq.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/asm/hw_irq.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/asm-i386/mach-xen/asm/hw_irq.h	2006-10-16 10:19:17.000000000 +0200
@@ -12,7 +12,6 @@
  *	<tomsoft@informatik.tu-chemnitz.de>
  */
 
-#include <linux/config.h>
 #include <linux/profile.h>
 #include <asm/atomic.h>
 #include <asm/irq.h>
@@ -20,6 +19,8 @@
 
 struct hw_interrupt_type;
 
+#define NMI_VECTOR		0x02
+
 /*
  * Various low-level irq details needed by irq.c, process.c,
  * time.c, io_apic.c and smp.c
@@ -68,10 +69,4 @@ extern atomic_t irq_mis_count;
 
 #define IO_APIC_IRQ(x) (((x) >= 16) || ((1<<(x)) & io_apic_irqs))
 
-extern void resend_irq_on_evtchn(struct hw_interrupt_type *h, unsigned int i);
-static inline void hw_resend_irq(struct hw_interrupt_type *h, unsigned int i)
-{
-	resend_irq_on_evtchn(h, i);
-}
-
 #endif /* _ASM_HW_IRQ_H */
Index: head-2006-11-06/include/asm-i386/mach-xen/asm/io.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/asm/io.h	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/include/asm-i386/mach-xen/asm/io.h	2006-10-16 10:19:17.000000000 +0200
@@ -1,7 +1,6 @@
 #ifndef _ASM_IO_H
 #define _ASM_IO_H
 
-#include <linux/config.h>
 #include <linux/string.h>
 #include <linux/compiler.h>
 
Index: head-2006-11-06/include/asm-i386/mach-xen/asm/irqflags.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ head-2006-11-06/include/asm-i386/mach-xen/asm/irqflags.h	2006-10-16 10:19:17.000000000 +0200
@@ -0,0 +1,80 @@
+/*
+ * include/asm-i386/irqflags.h
+ *
+ * IRQ flags handling
+ *
+ * This file gets included from lowlevel asm headers too, to provide
+ * wrapped versions of the local_irq_*() APIs, based on the
+ * raw_local_irq_*() functions from the lowlevel headers.
+ */
+#ifndef _ASM_IRQFLAGS_H
+#define _ASM_IRQFLAGS_H
+
+#ifndef __ASSEMBLY__
+
+#define raw_local_save_flags(flags) \
+		do { (flags) = __raw_local_save_flags(); } while (0)
+
+unsigned long __raw_local_save_flags(void);
+void raw_local_irq_restore(unsigned long flags);
+void raw_local_irq_disable(void);
+void raw_local_irq_enable(void);
+
+/*
+ * Used in the idle loop; sti takes one instruction cycle
+ * to complete:
+ */
+void raw_safe_halt(void);
+
+/*
+ * Used when interrupts are already enabled or to
+ * shutdown the processor:
+ */
+void halt(void);
+
+static inline int raw_irqs_disabled_flags(unsigned long flags)
+{
+	return flags != 0;
+}
+
+int raw_irqs_disabled(void);
+
+/*
+ * For spinlocks, etc:
+ */
+unsigned long __raw_local_irq_save(void);
+#define raw_local_irq_save(flags) \
+		do { (flags) = __raw_local_irq_save(); } while (0)
+
+#endif /* __ASSEMBLY__ */
+
+/*
+ * Do the CPU's IRQ-state tracing from assembly code. We call a
+ * C function, so save all the C-clobbered registers:
+ */
+#ifdef CONFIG_TRACE_IRQFLAGS
+
+# define TRACE_IRQS_ON				\
+	pushl %eax;				\
+	pushl %ecx;				\
+	pushl %edx;				\
+	call trace_hardirqs_on;			\
+	popl %edx;				\
+	popl %ecx;				\
+	popl %eax;
+
+# define TRACE_IRQS_OFF				\
+	pushl %eax;				\
+	pushl %ecx;				\
+	pushl %edx;				\
+	call trace_hardirqs_off;		\
+	popl %edx;				\
+	popl %ecx;				\
+	popl %eax;
+
+#else
+# define TRACE_IRQS_ON
+# define TRACE_IRQS_OFF
+#endif
+
+#endif
Index: head-2006-11-06/include/asm-i386/mach-xen/asm/kmap_types.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/asm/kmap_types.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/asm-i386/mach-xen/asm/kmap_types.h	2006-10-16 10:19:17.000000000 +0200
@@ -1,7 +1,6 @@
 #ifndef _ASM_KMAP_TYPES_H
 #define _ASM_KMAP_TYPES_H
 
-#include <linux/config.h>
 
 #ifdef CONFIG_DEBUG_HIGHMEM
 # define D(n) __KM_FENCE_##n ,
Index: head-2006-11-06/include/asm-i386/mach-xen/asm/mmu.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/asm/mmu.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/asm-i386/mach-xen/asm/mmu.h	2006-10-16 10:19:17.000000000 +0200
@@ -12,6 +12,7 @@ typedef struct { 
 	int size;
 	struct semaphore sem;
 	void *ldt;
+	void *vdso;
 #ifdef CONFIG_XEN
 	int has_foreign_mappings;
 #endif
Index: head-2006-11-06/include/asm-i386/mach-xen/asm/mmu_context.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/asm/mmu_context.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/asm-i386/mach-xen/asm/mmu_context.h	2006-10-16 10:19:17.000000000 +0200
@@ -1,7 +1,6 @@
 #ifndef __I386_SCHED_H
 #define __I386_SCHED_H
 
-#include <linux/config.h>
 #include <asm/desc.h>
 #include <asm/atomic.h>
 #include <asm/pgalloc.h>
Index: head-2006-11-06/include/asm-i386/mach-xen/asm/page.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/asm/page.h	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/include/asm-i386/mach-xen/asm/page.h	2006-10-16 10:19:17.000000000 +0200
@@ -12,7 +12,6 @@
 #ifdef __KERNEL__
 #ifndef __ASSEMBLY__
 
-#include <linux/config.h>
 #include <linux/string.h>
 #include <linux/types.h>
 #include <linux/kernel.h>
@@ -161,6 +160,8 @@ static inline unsigned long pgd_val(pgd_
 
 #ifndef __ASSEMBLY__
 
+struct vm_area_struct;
+
 /*
  * This much address space is reserved for vmalloc() and iomap()
  * as well as fixmap mappings.
@@ -205,11 +206,10 @@ extern int page_is_ram(unsigned long pag
 	((current->personality & READ_IMPLIES_EXEC) ? VM_EXEC : 0 ) | \
 		 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)
 
-#define __HAVE_ARCH_GATE_AREA 1
-
-#endif /* __KERNEL__ */
-
 #include <asm-generic/memory_model.h>
 #include <asm-generic/page.h>
 
+#define __HAVE_ARCH_GATE_AREA 1
+#endif /* __KERNEL__ */
+
 #endif /* _I386_PAGE_H */
Index: head-2006-11-06/include/asm-i386/mach-xen/asm/param.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/asm/param.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/asm-i386/mach-xen/asm/param.h	2006-10-16 10:19:17.000000000 +0200
@@ -2,7 +2,6 @@
 #define _ASMi386_PARAM_H
 
 #ifdef __KERNEL__
-# include <linux/config.h>
 # define HZ		CONFIG_HZ	/* Internal kernel timer frequency */
 # define USER_HZ	100		/* .. some user interfaces are in "ticks" */
 # define CLOCKS_PER_SEC		(USER_HZ)	/* like times() */
Index: head-2006-11-06/include/asm-i386/mach-xen/asm/pci.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/asm/pci.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/asm-i386/mach-xen/asm/pci.h	2006-10-16 10:19:17.000000000 +0200
@@ -1,7 +1,6 @@
 #ifndef __i386_PCI_H
 #define __i386_PCI_H
 
-#include <linux/config.h>
 
 #ifdef __KERNEL__
 #include <linux/mm.h>		/* for struct page */
Index: head-2006-11-06/include/asm-i386/mach-xen/asm/pgalloc.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/asm/pgalloc.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/asm-i386/mach-xen/asm/pgalloc.h	2006-10-16 10:19:17.000000000 +0200
@@ -1,7 +1,6 @@
 #ifndef _I386_PGALLOC_H
 #define _I386_PGALLOC_H
 
-#include <linux/config.h>
 #include <asm/fixmap.h>
 #include <linux/threads.h>
 #include <linux/mm.h>		/* for struct page */
Index: head-2006-11-06/include/asm-i386/mach-xen/asm/pgtable.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/asm/pgtable.h	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/include/asm-i386/mach-xen/asm/pgtable.h	2006-10-16 10:19:17.000000000 +0200
@@ -1,7 +1,6 @@
 #ifndef _I386_PGTABLE_H
 #define _I386_PGTABLE_H
 
-#include <linux/config.h>
 #include <asm/hypervisor.h>
 
 /*
Index: head-2006-11-06/include/asm-i386/mach-xen/asm/processor.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/asm/processor.h	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/include/asm-i386/mach-xen/asm/processor.h	2006-10-16 10:19:17.000000000 +0200
@@ -17,7 +17,6 @@
 #include <asm/msr.h>
 #include <asm/system.h>
 #include <linux/cache.h>
-#include <linux/config.h>
 #include <linux/threads.h>
 #include <asm/percpu.h>
 #include <linux/cpumask.h>
@@ -73,8 +72,12 @@ struct cpuinfo_x86 {
 	cpumask_t llc_shared_map;	/* cpus sharing the last level cache */
 #endif
 	unsigned char x86_max_cores;	/* cpuid returned max cores value */
-	unsigned char booted_cores;	/* number of cores as seen by OS */
 	unsigned char apicid;
+#ifdef CONFIG_SMP
+	unsigned char booted_cores;	/* number of cores as seen by OS */
+	__u8 phys_proc_id; 		/* Physical processor id. */
+	__u8 cpu_core_id;  		/* Core id */
+#endif
 } __attribute__((__aligned__(SMP_CACHE_BYTES)));
 
 #define X86_VENDOR_INTEL 0
@@ -108,14 +111,13 @@ extern struct cpuinfo_x86 cpu_data[];
 #define current_cpu_data boot_cpu_data
 #endif
 
-extern	int phys_proc_id[NR_CPUS];
-extern	int cpu_core_id[NR_CPUS];
 extern	int cpu_llc_id[NR_CPUS];
 extern char ignore_fpu_irq;
 
 extern void identify_cpu(struct cpuinfo_x86 *);
 extern void print_cpu_info(struct cpuinfo_x86 *);
 extern unsigned int init_intel_cacheinfo(struct cpuinfo_x86 *c);
+extern unsigned short num_cache_leaves;
 
 #ifdef CONFIG_X86_HT
 extern void detect_ht(struct cpuinfo_x86 *c);
@@ -562,7 +564,7 @@ extern void prepare_to_copy(struct task_
 extern int kernel_thread(int (*fn)(void *), void * arg, unsigned long flags);
 
 extern unsigned long thread_saved_pc(struct task_struct *tsk);
-void show_trace(struct task_struct *task, unsigned long *stack);
+void show_trace(struct task_struct *task, struct pt_regs *regs, unsigned long *stack);
 
 unsigned long get_wchan(struct task_struct *p);
 
@@ -736,18 +738,4 @@ extern unsigned long boot_option_idle_ov
 extern void enable_sep_cpu(void);
 extern int sysenter_setup(void);
 
-#ifdef CONFIG_MTRR
-extern void mtrr_ap_init(void);
-extern void mtrr_bp_init(void);
-#else
-#define mtrr_ap_init() do {} while (0)
-#define mtrr_bp_init() do {} while (0)
-#endif
-
-#ifdef CONFIG_X86_MCE
-extern void mcheck_init(struct cpuinfo_x86 *c);
-#else
-#define mcheck_init(c) do {} while(0)
-#endif
-
 #endif /* __ASM_I386_PROCESSOR_H */
Index: head-2006-11-06/include/asm-i386/mach-xen/asm/setup.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/asm/setup.h	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/include/asm-i386/mach-xen/asm/setup.h	2006-10-16 10:19:17.000000000 +0200
@@ -6,6 +6,7 @@
 #ifndef _i386_SETUP_H
 #define _i386_SETUP_H
 
+#ifdef __KERNEL__
 #include <linux/pfn.h>
 
 /*
@@ -13,6 +14,7 @@
  */
 #define MAXMEM_PFN	PFN_DOWN(MAXMEM)
 #define MAX_NONPAE_PFN	(1 << 20)
+#endif
 
 #define PARAM_SIZE 4096
 #define COMMAND_LINE_SIZE 256
@@ -59,6 +61,22 @@ extern unsigned char boot_params[PARAM_S
 #define EDD_MBR_SIGNATURE ((unsigned int *) (PARAM+EDD_MBR_SIG_BUF))
 #define EDD_BUF     ((struct edd_info *) (PARAM+EDDBUF))
 
+/*
+ * Do NOT EVER look at the BIOS memory size location.
+ * It does not work on many machines.
+ */
+#define LOWMEMSIZE()	(0x9f000)
+
+struct e820entry;
+
+char * __init machine_specific_memory_setup(void);
+void __init machine_specific_arch_setup(void);
+
+int __init copy_e820_map(struct e820entry * biosmap, int nr_map);
+int __init sanitize_e820_map(struct e820entry * biosmap, char * pnr_map);
+void __init add_memory_region(unsigned long long start,
+			      unsigned long long size, int type);
+
 #endif /* __ASSEMBLY__ */
 
 #endif /* _i386_SETUP_H */
Index: head-2006-11-06/include/asm-i386/mach-xen/asm/smp.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/asm/smp.h	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/include/asm-i386/mach-xen/asm/smp.h	2006-10-16 10:19:17.000000000 +0200
@@ -5,7 +5,6 @@
  * We need the APIC definitions automatically as part of 'smp.h'
  */
 #ifndef __ASSEMBLY__
-#include <linux/config.h>
 #include <linux/kernel.h>
 #include <linux/threads.h>
 #include <linux/cpumask.h>
Index: head-2006-11-06/include/asm-i386/mach-xen/asm/spinlock.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/asm/spinlock.h	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/include/asm-i386/mach-xen/asm/spinlock.h	2006-10-16 10:19:17.000000000 +0200
@@ -4,7 +4,6 @@
 #include <asm/atomic.h>
 #include <asm/rwlock.h>
 #include <asm/page.h>
-#include <linux/config.h>
 #include <linux/compiler.h>
 
 /*
@@ -23,7 +22,7 @@
 
 #define __raw_spin_lock_string \
 	"\n1:\n" \
-	"lock decb %0\n\t" \
+	LOCK_PREFIX " decb %0\n\t" \
 	"jns 3f\n" \
 	"2:\t" \
 	"rep;nop\n\t" \
@@ -32,9 +31,14 @@
 	"jmp 1b\n" \
 	"3:\n\t"
 
+/*
+ * NOTE: there's an irqs-on section here, which normally would have to be
+ * irq-traced, but on CONFIG_TRACE_IRQFLAGS we never use
+ * __raw_spin_lock_string_flags().
+ */
 #define __raw_spin_lock_string_flags \
 	"\n1:\n" \
-	"lock decb %0\n\t" \
+	LOCK_PREFIX " decb %0\n\t" \
 	"jns 5f\n" \
 	"2:\t" \
 	"testl $0x200, %1\n\t" \
@@ -53,31 +57,29 @@
 	"jmp 4b\n" \
 	"5:\n\t"
 
-#define __raw_spin_lock_string_up \
-	"\n\tdecb %0"
-
 static inline void __raw_spin_lock(raw_spinlock_t *lock)
 {
-	alternative_smp(
-		__raw_spin_lock_string,
-		__raw_spin_lock_string_up,
-		"=m" (lock->slock) : : "memory");
+	asm(__raw_spin_lock_string : "+m" (lock->slock) : : "memory");
 }
 
+/*
+ * It is easier for the lock validator if interrupts are not re-enabled
+ * in the middle of a lock-acquire. This is a performance feature anyway
+ * so we turn it off:
+ */
+#ifndef CONFIG_PROVE_LOCKING
 static inline void __raw_spin_lock_flags(raw_spinlock_t *lock, unsigned long flags)
 {
-	alternative_smp(
-		__raw_spin_lock_string_flags,
-		__raw_spin_lock_string_up,
-		"=m" (lock->slock) : "r" (flags) : "memory");
+	asm(__raw_spin_lock_string_flags : "+m" (lock->slock) : "r" (flags) : "memory");
 }
+#endif
 
 static inline int __raw_spin_trylock(raw_spinlock_t *lock)
 {
 	char oldval;
 	__asm__ __volatile__(
 		"xchgb %b0,%1"
-		:"=q" (oldval), "=m" (lock->slock)
+		:"=q" (oldval), "+m" (lock->slock)
 		:"0" (0) : "memory");
 	return oldval > 0;
 }
@@ -93,7 +95,7 @@ static inline int __raw_spin_trylock(raw
 
 #define __raw_spin_unlock_string \
 	"movb $1,%0" \
-		:"=m" (lock->slock) : : "memory"
+		:"+m" (lock->slock) : : "memory"
 
 
 static inline void __raw_spin_unlock(raw_spinlock_t *lock)
@@ -107,7 +109,7 @@ static inline void __raw_spin_unlock(raw
 
 #define __raw_spin_unlock_string \
 	"xchgb %b0, %1" \
-		:"=q" (oldval), "=m" (lock->slock) \
+		:"=q" (oldval), "+m" (lock->slock) \
 		:"0" (oldval) : "memory"
 
 static inline void __raw_spin_unlock(raw_spinlock_t *lock)
@@ -188,13 +190,13 @@ static inline int __raw_write_trylock(ra
 
 static inline void __raw_read_unlock(raw_rwlock_t *rw)
 {
-	asm volatile(LOCK_PREFIX "incl %0" :"=m" (rw->lock) : : "memory");
+	asm volatile(LOCK_PREFIX "incl %0" :"+m" (rw->lock) : : "memory");
 }
 
 static inline void __raw_write_unlock(raw_rwlock_t *rw)
 {
 	asm volatile(LOCK_PREFIX "addl $" RW_LOCK_BIAS_STR ", %0"
-				 : "=m" (rw->lock) : : "memory");
+				 : "+m" (rw->lock) : : "memory");
 }
 
 #endif /* __ASM_SPINLOCK_H */
Index: head-2006-11-06/include/asm-i386/mach-xen/asm/system.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/asm/system.h	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/include/asm-i386/mach-xen/asm/system.h	2006-10-16 10:19:17.000000000 +0200
@@ -1,7 +1,6 @@
 #ifndef __ASM_SYSTEM_H
 #define __ASM_SYSTEM_H
 
-#include <linux/config.h>
 #include <linux/kernel.h>
 #include <linux/bitops.h>
 #include <asm/synch_bitops.h>
@@ -20,9 +19,14 @@
 struct task_struct;	/* one of the stranger aspects of C forward declarations.. */
 extern struct task_struct * FASTCALL(__switch_to(struct task_struct *prev, struct task_struct *next));
 
+/*
+ * Saving eflags is important. It switches not only IOPL between tasks,
+ * it also protects other tasks from NT leaking through sysenter etc.
+ */
 #define switch_to(prev,next,last) do {					\
 	unsigned long esi,edi;						\
-	asm volatile("pushl %%ebp\n\t"					\
+	asm volatile("pushfl\n\t"		/* Save flags */	\
+		     "pushl %%ebp\n\t"					\
 		     "movl %%esp,%0\n\t"	/* save ESP */		\
 		     "movl %5,%%esp\n\t"	/* restore ESP */	\
 		     "movl $1f,%1\n\t"		/* save EIP */		\
@@ -30,6 +34,7 @@ extern struct task_struct * FASTCALL(__s
 		     "jmp __switch_to\n"				\
 		     "1:\t"						\
 		     "popl %%ebp\n\t"					\
+		     "popfl"						\
 		     :"=m" (prev->thread.esp),"=m" (prev->thread.eip),	\
 		      "=a" (last),"=S" (esi),"=D" (edi)			\
 		     :"m" (next->thread.esp),"m" (next->thread.eip),	\
@@ -91,10 +96,6 @@ __asm__ __volatile__ ("movw %%dx,%1\n\t"
 #define savesegment(seg, value) \
 	asm volatile("mov %%" #seg ",%0":"=rm" (value))
 
-/*
- * Clear and set 'TS' bit respectively
- */
-#define clts() (HYPERVISOR_fpu_taskswitch(0))
 #define read_cr0() ({ \
 	unsigned int __dummy; \
 	__asm__ __volatile__( \
@@ -103,12 +104,12 @@ __asm__ __volatile__ ("movw %%dx,%1\n\t"
 	__dummy; \
 })
 #define write_cr0(x) \
-	__asm__ __volatile__("movl %0,%%cr0": :"r" (x));
+	__asm__ __volatile__("movl %0,%%cr0": :"r" (x))
 
 #define read_cr2() \
 	(HYPERVISOR_shared_info->vcpu_info[smp_processor_id()].arch.cr2)
 #define write_cr2(x) \
-	__asm__ __volatile__("movl %0,%%cr2": :"r" (x));
+	__asm__ __volatile__("movl %0,%%cr2": :"r" (x))
 
 #define read_cr3() ({ \
 	unsigned int __dummy; \
@@ -131,7 +132,6 @@ __asm__ __volatile__ ("movw %%dx,%1\n\t"
 		:"=r" (__dummy)); \
 	__dummy; \
 })
-
 #define read_cr4_safe() ({			      \
 	unsigned int __dummy;			      \
 	/* This could fault if %cr4 does not exist */ \
@@ -143,15 +143,19 @@ __asm__ __volatile__ ("movw %%dx,%1\n\t"
 		: "=r" (__dummy): "0" (0));	      \
 	__dummy;				      \
 })
-
 #define write_cr4(x) \
-	__asm__ __volatile__("movl %0,%%cr4": :"r" (x));
+	__asm__ __volatile__("movl %0,%%cr4": :"r" (x))
+
+/*
+ * Clear and set 'TS' bit respectively
+ */
+#define clts() (HYPERVISOR_fpu_taskswitch(0))
 #define stts() (HYPERVISOR_fpu_taskswitch(1))
 
 #endif	/* __KERNEL__ */
 
 #define wbinvd() \
-	__asm__ __volatile__ ("wbinvd": : :"memory");
+	__asm__ __volatile__ ("wbinvd": : :"memory")
 
 static inline unsigned long get_limit(unsigned long segment)
 {
@@ -435,7 +439,7 @@ static inline unsigned long long __cmpxc
  * does not enforce ordering, since there is no data dependency between
  * the read of "a" and the read of "b".  Therefore, on some CPUs, such
  * as Alpha, "y" could be set to 3 and "x" to 0.  Use rmb()
- * in cases like thiswhere there are no data dependencies.
+ * in cases like this where there are no data dependencies.
  **/
 
 #define read_barrier_depends()	do { } while(0)
@@ -462,94 +466,7 @@ static inline unsigned long long __cmpxc
 #define set_mb(var, value) do { var = value; barrier(); } while (0)
 #endif
 
-#define set_wmb(var, value) do { var = value; wmb(); } while (0)
-
-/* interrupt control.. */
-
-/* 
- * The use of 'barrier' in the following reflects their use as local-lock
- * operations. Reentrancy must be prevented (e.g., __cli()) /before/ following
- * critical operations are executed. All critical operations must complete
- * /before/ reentrancy is permitted (e.g., __sti()). Alpha architecture also
- * includes these barriers, for example.
- */
-
-#define __cli()								\
-do {									\
-	vcpu_info_t *_vcpu;						\
-	preempt_disable();						\
-	_vcpu = &HYPERVISOR_shared_info->vcpu_info[__vcpu_id];		\
-	_vcpu->evtchn_upcall_mask = 1;					\
-	preempt_enable_no_resched();					\
-	barrier();							\
-} while (0)
-
-#define __sti()								\
-do {									\
-	vcpu_info_t *_vcpu;						\
-	barrier();							\
-	preempt_disable();						\
-	_vcpu = &HYPERVISOR_shared_info->vcpu_info[__vcpu_id];		\
-	_vcpu->evtchn_upcall_mask = 0;					\
-	barrier(); /* unmask then check (avoid races) */		\
-	if (unlikely(_vcpu->evtchn_upcall_pending))			\
-		force_evtchn_callback();				\
-	preempt_enable();						\
-} while (0)
-
-#define __save_flags(x)							\
-do {									\
-	vcpu_info_t *_vcpu;						\
-	preempt_disable();						\
-	_vcpu = &HYPERVISOR_shared_info->vcpu_info[__vcpu_id];		\
-	(x) = _vcpu->evtchn_upcall_mask;				\
-	preempt_enable();						\
-} while (0)
-
-#define __restore_flags(x)						\
-do {									\
-	vcpu_info_t *_vcpu;						\
-	barrier();							\
-	preempt_disable();						\
-	_vcpu = &HYPERVISOR_shared_info->vcpu_info[__vcpu_id];		\
-	if ((_vcpu->evtchn_upcall_mask = (x)) == 0) {			\
-		barrier(); /* unmask then check (avoid races) */	\
-		if (unlikely(_vcpu->evtchn_upcall_pending))		\
-			force_evtchn_callback();			\
-		preempt_enable();					\
-	} else								\
-		preempt_enable_no_resched();				\
-} while (0)
-
-void safe_halt(void);
-void halt(void);
-
-#define __save_and_cli(x)						\
-do {									\
-	vcpu_info_t *_vcpu;						\
-	preempt_disable();						\
-	_vcpu = &HYPERVISOR_shared_info->vcpu_info[__vcpu_id];		\
-	(x) = _vcpu->evtchn_upcall_mask;				\
-	_vcpu->evtchn_upcall_mask = 1;					\
-	preempt_enable_no_resched();					\
-	barrier();							\
-} while (0)
-
-#define local_irq_save(x)	__save_and_cli(x)
-#define local_irq_restore(x)	__restore_flags(x)
-#define local_save_flags(x)	__save_flags(x)
-#define local_irq_disable()	__cli()
-#define local_irq_enable()	__sti()
-
-/* Cannot use preempt_enable() here as we would recurse in preempt_sched(). */
-#define irqs_disabled()							\
-({	int ___x;							\
-	vcpu_info_t *_vcpu;						\
-	preempt_disable();						\
-	_vcpu = &HYPERVISOR_shared_info->vcpu_info[__vcpu_id];		\
-	___x = (_vcpu->evtchn_upcall_mask != 0);			\
-	preempt_enable_no_resched();					\
-	___x; })
+#include <linux/irqflags.h>
 
 /*
  * disable hlt during certain critical i/o operations
Index: head-2006-11-06/include/asm-i386/mach-xen/asm/tlbflush.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/asm/tlbflush.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/asm-i386/mach-xen/asm/tlbflush.h	2006-10-16 10:19:17.000000000 +0200
@@ -1,7 +1,6 @@
 #ifndef _I386_TLBFLUSH_H
 #define _I386_TLBFLUSH_H
 
-#include <linux/config.h>
 #include <linux/mm.h>
 #include <asm/processor.h>
 
Index: head-2006-11-06/include/asm-i386/mach-xen/asm/vga.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/asm/vga.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/asm-i386/mach-xen/asm/vga.h	2006-10-16 10:19:17.000000000 +0200
@@ -12,7 +12,7 @@
  *	access the videoram directly without any black magic.
  */
 
-#define VGA_MAP_MEM(x) (unsigned long)isa_bus_to_virt(x)
+#define VGA_MAP_MEM(x,s) (unsigned long)isa_bus_to_virt(x)
 
 #define vga_readb(x) (*(x))
 #define vga_writeb(x,y) (*(y) = (x))
Index: head-2006-11-06/include/asm-i386/mach-xen/setup_arch.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ head-2006-11-06/include/asm-i386/mach-xen/setup_arch.h	2006-10-16 10:19:17.000000000 +0200
@@ -0,0 +1,3 @@
+/* Hook to call BIOS initialisation function */
+
+#define ARCH_SETUP machine_specific_arch_setup();
Index: head-2006-11-06/include/asm-i386/mach-xen/setup_arch_post.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/setup_arch_post.h	2006-10-16 10:15:19.000000000 +0200
+++ /dev/null	1970-01-01 00:00:00.000000000 +0000
@@ -1,101 +0,0 @@
-/**
- * machine_specific_memory_setup - Hook for machine specific memory setup.
- *
- * Description:
- *	This is included late in kernel/setup.c so that it can make
- *	use of all of the static functions.
- **/
-
-#include <xen/interface/callback.h>
-#include <xen/interface/memory.h>
-
-static char * __init machine_specific_memory_setup(void)
-{
-	int rc;
-	struct xen_memory_map memmap;
-	/*
-	 * This is rather large for a stack variable but this early in
-	 * the boot process we know we have plenty slack space.
-	 */
-	struct e820entry map[E820MAX];
-
-	memmap.nr_entries = E820MAX;
-	set_xen_guest_handle(memmap.buffer, map);
-
-	rc = HYPERVISOR_memory_op(XENMEM_memory_map, &memmap);
-	if ( rc == -ENOSYS ) {
-		memmap.nr_entries = 1;
-		map[0].addr = 0ULL;
-		map[0].size = PFN_PHYS((unsigned long long)xen_start_info->nr_pages);
-		/* 8MB slack (to balance backend allocations). */
-		map[0].size += 8ULL << 20;
-		map[0].type = E820_RAM;
-		rc = 0;
-	}
-	BUG_ON(rc);
-
-	sanitize_e820_map(map, (char *)&memmap.nr_entries);
-
-	BUG_ON(copy_e820_map(map, (char)memmap.nr_entries) < 0);
-
-	return "Xen";
-}
-
-extern void hypervisor_callback(void);
-extern void failsafe_callback(void);
-extern void nmi(void);
-
-unsigned long *machine_to_phys_mapping;
-EXPORT_SYMBOL(machine_to_phys_mapping);
-unsigned int machine_to_phys_order;
-EXPORT_SYMBOL(machine_to_phys_order);
-
-static void __init machine_specific_arch_setup(void)
-{
-	int ret;
-	struct xen_machphys_mapping mapping;
-	unsigned long machine_to_phys_nr_ents;
-	struct xen_platform_parameters pp;
-	struct callback_register event = {
-		.type = CALLBACKTYPE_event,
-		.address = { __KERNEL_CS, (unsigned long)hypervisor_callback },
-	};
-	struct callback_register failsafe = {
-		.type = CALLBACKTYPE_failsafe,
-		.address = { __KERNEL_CS, (unsigned long)failsafe_callback },
-	};
-	struct callback_register nmi_cb = {
-		.type = CALLBACKTYPE_nmi,
-		.address = { __KERNEL_CS, (unsigned long)nmi },
-	};
-
-	ret = HYPERVISOR_callback_op(CALLBACKOP_register, &event);
-	if (ret == 0)
-		ret = HYPERVISOR_callback_op(CALLBACKOP_register, &failsafe);
-	if (ret == -ENOSYS)
-		ret = HYPERVISOR_set_callbacks(
-			event.address.cs, event.address.eip,
-			failsafe.address.cs, failsafe.address.eip);
-	BUG_ON(ret);
-
-	ret = HYPERVISOR_callback_op(CALLBACKOP_register, &nmi_cb);
-	if (ret == -ENOSYS) {
-		struct xennmi_callback cb;
-
-		cb.handler_address = nmi_cb.address.eip;
-		HYPERVISOR_nmi_op(XENNMI_register_callback, &cb);
-	}
-
-	if (HYPERVISOR_xen_version(XENVER_platform_parameters,
-				   &pp) == 0)
-		set_fixaddr_top(pp.virt_start - PAGE_SIZE);
-
-	machine_to_phys_mapping = (unsigned long *)MACH2PHYS_VIRT_START;
-	machine_to_phys_nr_ents = MACH2PHYS_NR_ENTRIES;
-	if (HYPERVISOR_memory_op(XENMEM_machphys_mapping, &mapping) == 0) {
-		machine_to_phys_mapping = (unsigned long *)mapping.v_start;
-		machine_to_phys_nr_ents = mapping.max_mfn + 1;
-	}
-	while ((1UL << machine_to_phys_order) < machine_to_phys_nr_ents )
-		machine_to_phys_order++;
-}
Index: head-2006-11-06/include/asm-i386/mach-xen/setup_arch_pre.h
===================================================================
--- head-2006-11-06.orig/include/asm-i386/mach-xen/setup_arch_pre.h	2006-11-15 13:35:56.000000000 +0100
+++ /dev/null	1970-01-01 00:00:00.000000000 +0000
@@ -1,5 +0,0 @@
-/* Hook to call BIOS initialisation function */
-
-#define ARCH_SETUP machine_specific_arch_setup();
-
-static void __init machine_specific_arch_setup(void);
Index: head-2006-11-06/include/asm-x86_64/mach-xen/asm/dma-mapping.h
===================================================================
--- head-2006-11-06.orig/include/asm-x86_64/mach-xen/asm/dma-mapping.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/asm-x86_64/mach-xen/asm/dma-mapping.h	2006-10-16 10:19:17.000000000 +0200
@@ -6,7 +6,6 @@
  * documentation.
  */
 
-#include <linux/config.h>
 
 #include <asm/scatterlist.h>
 #include <asm/io.h>
@@ -57,6 +56,13 @@ extern struct dma_mapping_ops* dma_ops;
 extern int iommu_merge;
 
 #if 0
+static inline int valid_dma_direction(int dma_direction)
+{
+	return ((dma_direction == DMA_BIDIRECTIONAL) ||
+		(dma_direction == DMA_TO_DEVICE) ||
+		(dma_direction == DMA_FROM_DEVICE));
+}
+
 static inline int dma_mapping_error(dma_addr_t dma_addr)
 {
 	if (dma_ops->mapping_error)
@@ -74,6 +80,7 @@ static inline dma_addr_t
 dma_map_single(struct device *hwdev, void *ptr, size_t size,
 	       int direction)
 {
+	BUG_ON(!valid_dma_direction(direction));
 	return dma_ops->map_single(hwdev, ptr, size, direction);
 }
 
@@ -81,6 +88,7 @@ static inline void
 dma_unmap_single(struct device *dev, dma_addr_t addr,size_t size,
 		 int direction)
 {
+	BUG_ON(!valid_dma_direction(direction));
 	dma_ops->unmap_single(dev, addr, size, direction);
 }
 
@@ -93,6 +101,7 @@ static inline void
 dma_sync_single_for_cpu(struct device *hwdev, dma_addr_t dma_handle,
 			size_t size, int direction)
 {
+	BUG_ON(!valid_dma_direction(direction));
 	if (dma_ops->sync_single_for_cpu)
 		dma_ops->sync_single_for_cpu(hwdev, dma_handle, size,
 					     direction);
@@ -103,6 +112,7 @@ static inline void
 dma_sync_single_for_device(struct device *hwdev, dma_addr_t dma_handle,
 			   size_t size, int direction)
 {
+	BUG_ON(!valid_dma_direction(direction));
 	if (dma_ops->sync_single_for_device)
 		dma_ops->sync_single_for_device(hwdev, dma_handle, size,
 						direction);
@@ -113,6 +123,7 @@ static inline void
 dma_sync_single_range_for_cpu(struct device *hwdev, dma_addr_t dma_handle,
 			      unsigned long offset, size_t size, int direction)
 {
+	BUG_ON(!valid_dma_direction(direction));
 	if (dma_ops->sync_single_range_for_cpu) {
 		dma_ops->sync_single_range_for_cpu(hwdev, dma_handle, offset, size, direction);
 	}
@@ -124,6 +135,7 @@ static inline void
 dma_sync_single_range_for_device(struct device *hwdev, dma_addr_t dma_handle,
 				 unsigned long offset, size_t size, int direction)
 {
+	BUG_ON(!valid_dma_direction(direction));
 	if (dma_ops->sync_single_range_for_device)
 		dma_ops->sync_single_range_for_device(hwdev, dma_handle,
 						      offset, size, direction);
@@ -135,6 +147,7 @@ static inline void
 dma_sync_sg_for_cpu(struct device *hwdev, struct scatterlist *sg,
 		    int nelems, int direction)
 {
+	BUG_ON(!valid_dma_direction(direction));
 	if (dma_ops->sync_sg_for_cpu)
 		dma_ops->sync_sg_for_cpu(hwdev, sg, nelems, direction);
 	flush_write_buffers();
@@ -144,6 +157,7 @@ static inline void
 dma_sync_sg_for_device(struct device *hwdev, struct scatterlist *sg,
 		       int nelems, int direction)
 {
+	BUG_ON(!valid_dma_direction(direction));
 	if (dma_ops->sync_sg_for_device) {
 		dma_ops->sync_sg_for_device(hwdev, sg, nelems, direction);
 	}
@@ -154,6 +168,7 @@ dma_sync_sg_for_device(struct device *hw
 static inline int
 dma_map_sg(struct device *hwdev, struct scatterlist *sg, int nents, int direction)
 {
+	BUG_ON(!valid_dma_direction(direction));
 	return dma_ops->map_sg(hwdev, sg, nents, direction);
 }
 
@@ -161,6 +176,7 @@ static inline void
 dma_unmap_sg(struct device *hwdev, struct scatterlist *sg, int nents,
 	     int direction)
 {
+	BUG_ON(!valid_dma_direction(direction));
 	dma_ops->unmap_sg(hwdev, sg, nents, direction);
 }
 
Index: head-2006-11-06/include/asm-x86_64/mach-xen/asm/fixmap.h
===================================================================
--- head-2006-11-06.orig/include/asm-x86_64/mach-xen/asm/fixmap.h	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/include/asm-x86_64/mach-xen/asm/fixmap.h	2006-10-16 10:19:17.000000000 +0200
@@ -11,7 +11,6 @@
 #ifndef _ASM_FIXMAP_H
 #define _ASM_FIXMAP_H
 
-#include <linux/config.h>
 #include <linux/kernel.h>
 #include <asm/apicdef.h>
 #include <xen/gnttab.h>
Index: head-2006-11-06/include/asm-x86_64/mach-xen/asm/floppy.h
===================================================================
--- head-2006-11-06.orig/include/asm-x86_64/mach-xen/asm/floppy.h	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/include/asm-x86_64/mach-xen/asm/floppy.h	2006-10-16 10:19:17.000000000 +0200
@@ -134,7 +134,7 @@ static int vdma_get_dma_residue(unsigned
 
 static int fd_request_irq(void)
 {
-	return request_irq(FLOPPY_IRQ, floppy_hardint,SA_INTERRUPT,
+	return request_irq(FLOPPY_IRQ, floppy_hardint, IRQF_DISABLED,
 					   "floppy", NULL);
 }
 
Index: head-2006-11-06/include/asm-x86_64/mach-xen/asm/hw_irq.h
===================================================================
--- head-2006-11-06.orig/include/asm-x86_64/mach-xen/asm/hw_irq.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/asm-x86_64/mach-xen/asm/hw_irq.h	2006-10-16 10:19:17.000000000 +0200
@@ -12,12 +12,9 @@
  *	<tomsoft@informatik.tu-chemnitz.de>
  *
  *	hacked by Andi Kleen for x86-64.
- * 
- *  $Id: hw_irq.h,v 1.24 2001/09/14 20:55:03 vojtech Exp $
  */
 
 #ifndef __ASSEMBLY__
-#include <linux/config.h>
 #include <asm/atomic.h>
 #include <asm/irq.h>
 #include <linux/profile.h>
@@ -129,15 +126,9 @@ asmlinkage void IRQ_NAME(nr); \
 __asm__( \
 "\n.p2align\n" \
 "IRQ" #nr "_interrupt:\n\t" \
-	"push $" #nr "-256 ; " \
+	"push $~(" #nr ") ; " \
 	"jmp common_interrupt");
 
-extern void resend_irq_on_evtchn(struct hw_interrupt_type *h, unsigned int i);
-static inline void hw_resend_irq(struct hw_interrupt_type *h, unsigned int i)
-{
-	resend_irq_on_evtchn(h, i);
-}
-
 #define platform_legacy_irq(irq)	((irq) < 16)
 
 #endif
Index: head-2006-11-06/include/asm-x86_64/mach-xen/asm/io.h
===================================================================
--- head-2006-11-06.orig/include/asm-x86_64/mach-xen/asm/io.h	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/include/asm-x86_64/mach-xen/asm/io.h	2006-10-16 10:19:17.000000000 +0200
@@ -1,7 +1,6 @@
 #ifndef _ASM_IO_H
 #define _ASM_IO_H
 
-#include <linux/config.h>
 #include <asm/fixmap.h>
 
 /*
Index: head-2006-11-06/include/asm-x86_64/mach-xen/asm/irqflags.h
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ head-2006-11-06/include/asm-x86_64/mach-xen/asm/irqflags.h	2006-10-16 10:19:17.000000000 +0200
@@ -0,0 +1,65 @@
+/*
+ * include/asm-x86_64/irqflags.h
+ *
+ * IRQ flags handling
+ *
+ * This file gets included from lowlevel asm headers too, to provide
+ * wrapped versions of the local_irq_*() APIs, based on the
+ * raw_local_irq_*() functions from the lowlevel headers.
+ */
+#ifndef _ASM_IRQFLAGS_H
+#define _ASM_IRQFLAGS_H
+
+#ifndef __ASSEMBLY__
+/*
+ * Interrupt control:
+ */
+
+unsigned long __raw_local_save_flags(void);
+#define raw_local_save_flags(flags) \
+		do { (flags) = __raw_local_save_flags(); } while (0)
+
+void raw_local_irq_restore(unsigned long flags);
+void raw_local_irq_disable(void);
+void raw_local_irq_enable(void);
+
+static inline int raw_irqs_disabled_flags(unsigned long flags)
+{
+	return flags != 0;
+}
+
+/*
+ * For spinlocks, etc.:
+ */
+
+unsigned long __raw_local_irq_save(void);
+
+#define raw_local_irq_save(flags) \
+		do { (flags) = __raw_local_irq_save(); } while (0)
+
+int raw_irqs_disabled(void);
+
+/*
+ * Used in the idle loop; sti takes one instruction cycle
+ * to complete:
+ */
+void raw_safe_halt(void);
+
+
+/*
+ * Used when interrupts are already enabled or to
+ * shutdown the processor:
+ */
+void halt(void);
+
+#else /* __ASSEMBLY__: */
+# ifdef CONFIG_TRACE_IRQFLAGS
+#  define TRACE_IRQS_ON		call trace_hardirqs_on_thunk
+#  define TRACE_IRQS_OFF	call trace_hardirqs_off_thunk
+# else
+#  define TRACE_IRQS_ON
+#  define TRACE_IRQS_OFF
+# endif
+#endif
+
+#endif
Index: head-2006-11-06/include/asm-x86_64/mach-xen/asm/mmu_context.h
===================================================================
--- head-2006-11-06.orig/include/asm-x86_64/mach-xen/asm/mmu_context.h	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/include/asm-x86_64/mach-xen/asm/mmu_context.h	2006-10-16 10:19:17.000000000 +0200
@@ -1,7 +1,6 @@
 #ifndef __X86_64_MMU_CONTEXT_H
 #define __X86_64_MMU_CONTEXT_H
 
-#include <linux/config.h>
 #include <asm/desc.h>
 #include <asm/atomic.h>
 #include <asm/pgalloc.h>
Index: head-2006-11-06/include/asm-x86_64/mach-xen/asm/nmi.h
===================================================================
--- head-2006-11-06.orig/include/asm-x86_64/mach-xen/asm/nmi.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/asm-x86_64/mach-xen/asm/nmi.h	2006-10-16 10:19:17.000000000 +0200
@@ -5,28 +5,29 @@
 #define ASM_NMI_H
 
 #include <linux/pm.h>
+#include <asm/io.h>
 
 #include <xen/interface/nmi.h>
 
 struct pt_regs;
- 
+
 typedef int (*nmi_callback_t)(struct pt_regs * regs, int cpu);
- 
-/** 
+
+/**
  * set_nmi_callback
  *
  * Set a handler for an NMI. Only one handler may be
  * set. Return 1 if the NMI was handled.
  */
 void set_nmi_callback(nmi_callback_t callback);
- 
-/** 
+
+/**
  * unset_nmi_callback
  *
  * Remove the handler previously set.
  */
 void unset_nmi_callback(void);
- 
+
 #ifdef CONFIG_PM
  
 /** Replace the PM callback routine for NMI. */
@@ -72,4 +73,21 @@ extern int unknown_nmi_panic;
 
 extern int check_nmi_watchdog(void);
  
+extern void setup_apic_nmi_watchdog (void);
+extern int reserve_lapic_nmi(void);
+extern void release_lapic_nmi(void);
+extern void disable_timer_nmi_watchdog(void);
+extern void enable_timer_nmi_watchdog(void);
+extern void nmi_watchdog_tick (struct pt_regs * regs, unsigned reason);
+
+extern void nmi_watchdog_default(void);
+extern int setup_nmi_watchdog(char *);
+
+extern unsigned int nmi_watchdog;
+#define NMI_DEFAULT	-1
+#define NMI_NONE	0
+#define NMI_IO_APIC	1
+#define NMI_LOCAL_APIC	2
+#define NMI_INVALID	3
+
 #endif /* ASM_NMI_H */
Index: head-2006-11-06/include/asm-x86_64/mach-xen/asm/page.h
===================================================================
--- head-2006-11-06.orig/include/asm-x86_64/mach-xen/asm/page.h	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/include/asm-x86_64/mach-xen/asm/page.h	2006-10-16 10:19:17.000000000 +0200
@@ -1,7 +1,6 @@
 #ifndef _X86_64_PAGE_H
 #define _X86_64_PAGE_H
 
-#include <linux/config.h>
 /* #include <linux/string.h> */
 #ifndef __ASSEMBLY__
 #include <linux/kernel.h>
@@ -42,7 +41,7 @@
 #define EXCEPTION_STACK_ORDER 0
 #define EXCEPTION_STKSZ (PAGE_SIZE << EXCEPTION_STACK_ORDER)
 
-#define DEBUG_STACK_ORDER EXCEPTION_STACK_ORDER
+#define DEBUG_STACK_ORDER (EXCEPTION_STACK_ORDER + 1)
 #define DEBUG_STKSZ (PAGE_SIZE << DEBUG_STACK_ORDER)
 
 #define IRQSTACK_ORDER 2
@@ -201,9 +200,9 @@ static inline pgd_t __pgd(unsigned long 
 
 #define __HAVE_ARCH_GATE_AREA 1	
 
-#endif /* __KERNEL__ */
-
 #include <asm-generic/memory_model.h>
 #include <asm-generic/page.h>
 
+#endif /* __KERNEL__ */
+
 #endif /* _X86_64_PAGE_H */
Index: head-2006-11-06/include/asm-x86_64/mach-xen/asm/pci.h
===================================================================
--- head-2006-11-06.orig/include/asm-x86_64/mach-xen/asm/pci.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/asm-x86_64/mach-xen/asm/pci.h	2006-10-16 10:19:17.000000000 +0200
@@ -1,7 +1,6 @@
 #ifndef __x8664_PCI_H
 #define __x8664_PCI_H
 
-#include <linux/config.h>
 #include <asm/io.h>
 
 #ifdef __KERNEL__
@@ -40,8 +39,8 @@ int pcibios_set_irq_routing(struct pci_d
 #include <asm/scatterlist.h>
 #include <linux/string.h>
 #include <asm/page.h>
-#include <linux/dma-mapping.h> /* for have_iommu */
 
+extern void pci_iommu_alloc(void);
 extern int iommu_setup(char *opt);
 
 /* The PCI address space does equal the physical memory
@@ -53,7 +52,7 @@ extern int iommu_setup(char *opt);
  */
 #define PCI_DMA_BUS_IS_PHYS (dma_ops->is_phys)
 
-#ifdef CONFIG_GART_IOMMU
+#if defined(CONFIG_IOMMU) || defined(CONFIG_CALGARY_IOMMU)
 
 /*
  * x86-64 always supports DAC, but sometimes it is useful to force
Index: head-2006-11-06/include/asm-x86_64/mach-xen/asm/pgtable.h
===================================================================
--- head-2006-11-06.orig/include/asm-x86_64/mach-xen/asm/pgtable.h	2006-11-15 13:38:23.000000000 +0100
+++ head-2006-11-06/include/asm-x86_64/mach-xen/asm/pgtable.h	2006-11-15 13:38:46.000000000 +0100
@@ -402,19 +402,6 @@ static inline int pmd_large(pmd_t pte) {
 /* to find an entry in a page-table-directory. */
 #define pud_index(address) (((address) >> PUD_SHIFT) & (PTRS_PER_PUD-1))
 #define pud_offset(pgd, address) ((pud_t *) pgd_page(*(pgd)) + pud_index(address))
-static inline pud_t *__pud_offset_k(pud_t *pud, unsigned long address)
-{ 
-	return pud + pud_index(address);
-} 
-
-/* Find correct pud via the hidden fourth level page level: */
-
-/* This accesses the reference page table of the boot cpu. 
-   Other CPUs get synced lazily via the page fault handler. */
-static inline pud_t *pud_offset_k(pgd_t *pgd, unsigned long address)
-{
-	return pud_offset(pgd_offset_k(address), address);
-}
 
 /* PMD  - Level 2 access */
 #define pmd_page_kernel(pmd) ((unsigned long) __va(pmd_val(pmd) & PTE_MASK))
Index: head-2006-11-06/include/asm-x86_64/mach-xen/asm/processor.h
===================================================================
--- head-2006-11-06.orig/include/asm-x86_64/mach-xen/asm/processor.h	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/include/asm-x86_64/mach-xen/asm/processor.h	2006-10-16 10:19:17.000000000 +0200
@@ -12,7 +12,6 @@
 #include <asm/types.h>
 #include <asm/sigcontext.h>
 #include <asm/cpufeature.h>
-#include <linux/config.h>
 #include <linux/threads.h>
 #include <asm/msr.h>
 #include <asm/current.h>
@@ -70,7 +69,11 @@ struct cpuinfo_x86 {
 	cpumask_t llc_shared_map;	/* cpus sharing the last level cache */
 #endif
 	__u8	apicid;
+#ifdef CONFIG_SMP
 	__u8	booted_cores;	/* number of cores as seen by OS */
+	__u8	phys_proc_id;	/* Physical Processor id. */
+	__u8	cpu_core_id;	/* Core id. */
+#endif
 } ____cacheline_aligned;
 
 #define X86_VENDOR_INTEL 0
@@ -97,6 +100,7 @@ extern char ignore_irq13;
 extern void identify_cpu(struct cpuinfo_x86 *);
 extern void print_cpu_info(struct cpuinfo_x86 *);
 extern unsigned int init_intel_cacheinfo(struct cpuinfo_x86 *c);
+extern unsigned short num_cache_leaves;
 
 /*
  * EFLAGS bits
@@ -238,6 +242,11 @@ struct tss_struct {
 } __attribute__((packed)) ____cacheline_aligned;
 
 DECLARE_PER_CPU(struct tss_struct,init_tss);
+/* Save the original ist values for checking stack pointers during debugging */
+struct orig_ist {
+	unsigned long ist[7];
+};
+DECLARE_PER_CPU(struct orig_ist, orig_ist);
 #endif
 
 extern struct cpuinfo_x86 boot_cpu_data;
Index: head-2006-11-06/include/asm-x86_64/mach-xen/asm/smp.h
===================================================================
--- head-2006-11-06.orig/include/asm-x86_64/mach-xen/asm/smp.h	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/include/asm-x86_64/mach-xen/asm/smp.h	2006-10-16 10:19:17.000000000 +0200
@@ -5,7 +5,6 @@
  * We need the APIC definitions automatically as part of 'smp.h'
  */
 #ifndef __ASSEMBLY__
-#include <linux/config.h>
 #include <linux/threads.h>
 #include <linux/cpumask.h>
 #include <linux/bitops.h>
@@ -53,8 +52,6 @@ extern int smp_call_function_single(int 
 
 extern cpumask_t cpu_sibling_map[NR_CPUS];
 extern cpumask_t cpu_core_map[NR_CPUS];
-extern int phys_proc_id[NR_CPUS];
-extern int cpu_core_id[NR_CPUS];
 extern int cpu_llc_id[NR_CPUS];
 
 #define SMP_TRAMPOLINE_BASE 0x6000
Index: head-2006-11-06/include/asm-x86_64/mach-xen/asm/system.h
===================================================================
--- head-2006-11-06.orig/include/asm-x86_64/mach-xen/asm/system.h	2006-10-16 10:15:19.000000000 +0200
+++ head-2006-11-06/include/asm-x86_64/mach-xen/asm/system.h	2006-10-16 10:19:17.000000000 +0200
@@ -1,9 +1,9 @@
 #ifndef __ASM_SYSTEM_H
 #define __ASM_SYSTEM_H
 
-#include <linux/config.h>
 #include <linux/kernel.h>
 #include <asm/segment.h>
+#include <asm/alternative.h>
 #include <asm/synch_bitops.h>
 #include <asm/hypervisor.h>
 #include <xen/interface/arch-x86_64.h>
@@ -16,12 +16,6 @@
 #define __vcpu_id 0
 #endif
 
-#ifdef CONFIG_SMP
-#define LOCK_PREFIX "lock ; "
-#else
-#define LOCK_PREFIX ""
-#endif
-
 #define __STR(x) #x
 #define STR(x) __STR(x)
 
@@ -44,7 +38,7 @@
 		     "thread_return:\n\t"					    \
 		     "movq %%gs:%P[pda_pcurrent],%%rsi\n\t"			  \
 		     "movq %P[thread_info](%%rsi),%%r8\n\t"			  \
-		     LOCK "btr  %[tif_fork],%P[ti_flags](%%r8)\n\t"		  \
+		     LOCK_PREFIX "btr  %[tif_fork],%P[ti_flags](%%r8)\n\t"	  \
 		     "movq %%rax,%%rdi\n\t" 					  \
 		     "jc   ret_from_fork\n\t"					  \
 		     RESTORE_CONTEXT						    \
@@ -80,82 +74,6 @@ extern void load_gs_index(unsigned);
 		".previous"			\
 		: :"r" (value), "r" (0))
 
-#ifdef __KERNEL__
-struct alt_instr { 
-	__u8 *instr; 		/* original instruction */
-	__u8 *replacement;
-	__u8  cpuid;		/* cpuid bit set for replacement */
-	__u8  instrlen;		/* length of original instruction */
-	__u8  replacementlen; 	/* length of new instruction, <= instrlen */ 
-	__u8  pad[5];
-}; 
-#endif
-
-/*
- * Alternative instructions for different CPU types or capabilities.
- * 
- * This allows to use optimized instructions even on generic binary
- * kernels.
- * 
- * length of oldinstr must be longer or equal the length of newinstr
- * It can be padded with nops as needed.
- * 
- * For non barrier like inlines please define new variants
- * without volatile and memory clobber.
- */
-#define alternative(oldinstr, newinstr, feature) 	\
-	asm volatile ("661:\n\t" oldinstr "\n662:\n" 		     \
-		      ".section .altinstructions,\"a\"\n"     	     \
-		      "  .align 8\n"				       \
-		      "  .quad 661b\n"            /* label */          \
-		      "  .quad 663f\n"		  /* new instruction */ \
-		      "  .byte %c0\n"             /* feature bit */    \
-		      "  .byte 662b-661b\n"       /* sourcelen */      \
-		      "  .byte 664f-663f\n"       /* replacementlen */ \
-		      ".previous\n"					\
-		      ".section .altinstr_replacement,\"ax\"\n"		\
-		      "663:\n\t" newinstr "\n664:\n"   /* replacement */ \
-		      ".previous" :: "i" (feature) : "memory")  
-
-/*
- * Alternative inline assembly with input.
- * 
- * Peculiarities:
- * No memory clobber here. 
- * Argument numbers start with 1.
- * Best is to use constraints that are fixed size (like (%1) ... "r")
- * If you use variable sized constraints like "m" or "g" in the 
- * replacement make sure to pad to the worst case length.
- */
-#define alternative_input(oldinstr, newinstr, feature, input...)	\
-	asm volatile ("661:\n\t" oldinstr "\n662:\n"			\
-		      ".section .altinstructions,\"a\"\n"		\
-		      "  .align 8\n"					\
-		      "  .quad 661b\n"            /* label */		\
-		      "  .quad 663f\n"		  /* new instruction */	\
-		      "  .byte %c0\n"             /* feature bit */	\
-		      "  .byte 662b-661b\n"       /* sourcelen */	\
-		      "  .byte 664f-663f\n"       /* replacementlen */	\
-		      ".previous\n"					\
-		      ".section .altinstr_replacement,\"ax\"\n"		\
-		      "663:\n\t" newinstr "\n664:\n"   /* replacement */ \
-		      ".previous" :: "i" (feature), ##input)
-
-/* Like alternative_input, but with a single output argument */
-#define alternative_io(oldinstr, newinstr, feature, output, input...) \
-	asm volatile ("661:\n\t" oldinstr "\n662:\n"			\
-		      ".section .altinstructions,\"a\"\n"		\
-		      "  .align 8\n"					\
-		      "  .quad 661b\n"            /* label */		\
-		      "  .quad 663f\n"		  /* new instruction */	\
-		      "  .byte %c[feat]\n"        /* feature bit */	\
-		      "  .byte 662b-661b\n"       /* sourcelen */	\
-		      "  .byte 664f-663f\n"       /* replacementlen */	\
-		      ".previous\n"					\
-		      ".section .altinstr_replacement,\"ax\"\n"		\
-		      "663:\n\t" newinstr "\n664:\n"   /* replacement */ \
-		      ".previous" : output : [feat] "i" (feature), ##input)
-
 /*
  * Clear and set 'TS' bit respectively
  */
@@ -331,98 +249,14 @@ static inline unsigned long __cmpxchg(vo
 #endif
 #define read_barrier_depends()	do {} while(0)
 #define set_mb(var, value) do { (void) xchg(&var, value); } while (0)
-#define set_wmb(var, value) do { var = value; wmb(); } while (0)
 
 #define warn_if_not_ulong(x) do { unsigned long foo; (void) (&(x) == &foo); } while (0)
 
-
-/* 
- * The use of 'barrier' in the following reflects their use as local-lock
- * operations. Reentrancy must be prevented (e.g., __cli()) /before/ following
- * critical operations are executed. All critical operations must complete
- * /before/ reentrancy is permitted (e.g., __sti()). Alpha architecture also
- * includes these barriers, for example.
- */
-
-#define __cli()								\
-do {									\
-	vcpu_info_t *_vcpu;						\
-	preempt_disable();						\
-	_vcpu = &HYPERVISOR_shared_info->vcpu_info[__vcpu_id];		\
-	_vcpu->evtchn_upcall_mask = 1;					\
-	preempt_enable_no_resched();					\
-	barrier();							\
-} while (0)
-
-#define __sti()								\
-do {									\
-	vcpu_info_t *_vcpu;						\
-	barrier();							\
-	preempt_disable();						\
-	_vcpu = &HYPERVISOR_shared_info->vcpu_info[__vcpu_id];		\
-	_vcpu->evtchn_upcall_mask = 0;					\
-	barrier(); /* unmask then check (avoid races) */		\
-	if ( unlikely(_vcpu->evtchn_upcall_pending) )			\
-		force_evtchn_callback();				\
-	preempt_enable();						\
-} while (0)
-
-#define __save_flags(x)							\
-do {									\
-	vcpu_info_t *_vcpu;						\
-	preempt_disable();						\
-	_vcpu = &HYPERVISOR_shared_info->vcpu_info[__vcpu_id];		\
-	(x) = _vcpu->evtchn_upcall_mask;				\
-	preempt_enable();						\
-} while (0)
-
-#define __restore_flags(x)						\
-do {									\
-	vcpu_info_t *_vcpu;						\
-	barrier();							\
-	preempt_disable();						\
-	_vcpu = &HYPERVISOR_shared_info->vcpu_info[__vcpu_id];		\
-	if ((_vcpu->evtchn_upcall_mask = (x)) == 0) {			\
-		barrier(); /* unmask then check (avoid races) */	\
-		if ( unlikely(_vcpu->evtchn_upcall_pending) )		\
-			force_evtchn_callback();			\
-		preempt_enable();					\
-	} else								\
-		preempt_enable_no_resched();				\
-} while (0)
-
-#define __save_and_cli(x)						\
-do {									\
-	vcpu_info_t *_vcpu;						\
-	preempt_disable();						\
-	_vcpu = &HYPERVISOR_shared_info->vcpu_info[__vcpu_id];		\
-	(x) = _vcpu->evtchn_upcall_mask;				\
-	_vcpu->evtchn_upcall_mask = 1;					\
-	preempt_enable_no_resched();					\
-	barrier();							\
-} while (0)
-
-#define local_irq_save(x)	__save_and_cli(x)
-#define local_irq_restore(x)	__restore_flags(x)
-#define local_save_flags(x)	__save_flags(x)
-#define local_irq_disable()	__cli()
-#define local_irq_enable()	__sti()
-
-/* Cannot use preempt_enable() here as we would recurse in preempt_sched(). */
-#define irqs_disabled()							\
-({	int ___x;							\
-	vcpu_info_t *_vcpu;						\
-	preempt_disable();						\
-	_vcpu = &HYPERVISOR_shared_info->vcpu_info[__vcpu_id];		\
-	___x = (_vcpu->evtchn_upcall_mask != 0);			\
-	preempt_enable_no_resched();					\
-	___x; })
-
-void safe_halt(void);
-void halt(void);
+#include <linux/irqflags.h>
 
 void cpu_idle_wait(void);
 
 extern unsigned long arch_align_stack(unsigned long sp);
+extern void free_init_pages(char *what, unsigned long begin, unsigned long end);
 
 #endif
Index: head-2006-11-06/include/asm-x86_64/mach-xen/asm/timer.h
===================================================================
--- head-2006-11-06.orig/include/asm-x86_64/mach-xen/asm/timer.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/asm-x86_64/mach-xen/asm/timer.h	2006-10-16 10:19:17.000000000 +0200
@@ -16,25 +16,9 @@
  *                   timer.
  * @delay: delays this many clock cycles.
  */
-struct timer_opts {
-	char* name;
-	void (*mark_offset)(void);
-	unsigned long (*get_offset)(void);
-	unsigned long long (*monotonic_clock)(void);
-	void (*delay)(unsigned long);
-	unsigned long (*read_timer)(void);
-	int (*suspend)(pm_message_t state);
-	int (*resume)(void);
-};
-
-struct init_timer_opts {
-	int (*init)(char *override);
-	struct timer_opts *opts;
-};
 
 #define TICK_SIZE (tick_nsec / 1000)
 
-extern struct timer_opts* __init select_timer(void);
 extern void clock_fallback(void);
 void setup_pit_timer(void);
 
@@ -42,26 +26,13 @@ void setup_pit_timer(void);
 
 extern int pit_latch_buggy;
 
-extern struct timer_opts *cur_timer;
 extern int timer_ack;
 
 /* list of externed timers */
-extern struct timer_opts timer_none;
-extern struct timer_opts timer_pit;
-extern struct init_timer_opts timer_pit_init;
-extern struct init_timer_opts timer_tsc_init;
-#ifdef CONFIG_X86_CYCLONE_TIMER
-extern struct init_timer_opts timer_cyclone_init;
-#endif
-
 extern unsigned long calibrate_tsc(void);
 extern void init_cpu_khz(void);
 #ifdef CONFIG_HPET_TIMER
-extern struct init_timer_opts timer_hpet_init;
 extern unsigned long calibrate_tsc_hpet(unsigned long *tsc_hpet_quotient_ptr);
 #endif
 
-#ifdef CONFIG_X86_PM_TIMER
-extern struct init_timer_opts timer_pmtmr_init;
-#endif
 #endif
Index: head-2006-11-06/include/asm-x86_64/mach-xen/asm/tlbflush.h
===================================================================
--- head-2006-11-06.orig/include/asm-x86_64/mach-xen/asm/tlbflush.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/asm-x86_64/mach-xen/asm/tlbflush.h	2006-10-16 10:19:17.000000000 +0200
@@ -1,7 +1,6 @@
 #ifndef _X8664_TLBFLUSH_H
 #define _X8664_TLBFLUSH_H
 
-#include <linux/config.h>
 #include <linux/mm.h>
 #include <asm/processor.h>
 
Index: head-2006-11-06/include/asm-x86_64/mach-xen/asm/vga.h
===================================================================
--- head-2006-11-06.orig/include/asm-x86_64/mach-xen/asm/vga.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/asm-x86_64/mach-xen/asm/vga.h	2006-10-16 10:19:17.000000000 +0200
@@ -12,7 +12,7 @@
  *	access the videoram directly without any black magic.
  */
 
-#define VGA_MAP_MEM(x) (unsigned long)isa_bus_to_virt(x)
+#define VGA_MAP_MEM(x,s) (unsigned long)isa_bus_to_virt(x)
 
 #define vga_readb(x) (*(x))
 #define vga_writeb(x,y) (*(y) = (x))
Index: head-2006-11-06/include/asm-x86_64/mach-xen/mach_timer.h
===================================================================
--- head-2006-11-06.orig/include/asm-x86_64/mach-xen/mach_timer.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/asm-x86_64/mach-xen/mach_timer.h	2006-10-16 10:19:17.000000000 +0200
@@ -15,7 +15,9 @@
 #ifndef _MACH_TIMER_H
 #define _MACH_TIMER_H
 
-#define CALIBRATE_LATCH	(5 * LATCH)
+#define CALIBRATE_TIME_MSEC 30 /* 30 msecs */
+#define CALIBRATE_LATCH	\
+	((CLOCK_TICK_RATE * CALIBRATE_TIME_MSEC + 1000/2)/1000)
 
 static inline void mach_prepare_counter(void)
 {
Index: head-2006-11-06/include/linux/eventpoll.h
===================================================================
--- head-2006-11-06.orig/include/linux/eventpoll.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/linux/eventpoll.h	2006-10-16 10:19:17.000000000 +0200
@@ -95,7 +95,7 @@ static inline void eventpoll_release(str
  */
 struct eventpoll;
 int ep_getfd(int *efd, struct inode **einode, struct file **efile,
-             struct eventpoll *ep, struct file_operations *fops);
+             struct eventpoll *ep, const struct file_operations *fops);
 #else
 
 static inline void eventpoll_init_file(struct file *file) {}
Index: head-2006-11-06/include/linux/skbuff.h
===================================================================
--- head-2006-11-06.orig/include/linux/skbuff.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/linux/skbuff.h	2006-10-16 10:19:17.000000000 +0200
@@ -1095,6 +1095,7 @@ static inline void __skb_queue_purge(str
 		kfree_skb(skb);
 }
 
+#ifndef CONFIG_HAVE_ARCH_DEV_ALLOC_SKB
 /**
  *	__dev_alloc_skb - allocate an skbuff for receiving
  *	@length: length to allocate
@@ -1115,6 +1116,9 @@ static inline struct sk_buff *__dev_allo
 		skb_reserve(skb, NET_SKB_PAD);
 	return skb;
 }
+#else
+extern struct sk_buff *__dev_alloc_skb(unsigned int length, gfp_t gfp_mask);
+#endif
 
 /**
  *	dev_alloc_skb - allocate an skbuff for receiving
Index: head-2006-11-06/include/xen/pcifront.h
===================================================================
--- head-2006-11-06.orig/include/xen/pcifront.h	2006-11-15 13:35:56.000000000 +0100
+++ head-2006-11-06/include/xen/pcifront.h	2006-10-16 10:19:17.000000000 +0200
@@ -70,8 +70,6 @@ static inline void pcifront_init_sd(stru
 
 #endif /* __ia64__ */
 
-extern spinlock_t pci_bus_lock;
-
 #endif /* __KERNEL__ */
 
 #endif /* __XEN_ASM_PCIFRONT_H__ */
Index: head-2006-11-06/include/asm-x86_64/swiotlb.h
===================================================================
--- head-2006-11-06.orig/include/asm-x86_64/swiotlb.h	2006-10-16 10:35:51.000000000 +0200
+++ head-2006-11-06/include/asm-x86_64/swiotlb.h	2006-10-16 10:19:17.000000000 +0200
@@ -42,9 +42,9 @@ extern void swiotlb_free_coherent (struc
 extern int swiotlb_dma_supported(struct device *hwdev, u64 mask);
 extern void swiotlb_init(void);
 
+#ifndef CONFIG_XEN
 extern int swiotlb_force;
-
-#ifdef CONFIG_XEN
+#else
 extern dma_addr_t swiotlb_map_page(struct device *hwdev, struct page *page,
                                    unsigned long offset, size_t size,
                                    enum dma_data_direction direction);
