Subject: xen3: common code changes
From: kraxel

mostly autogenerated from xen-unstable.hg

Signed-off-by: Gerd Knorr <kraxel@suse.de>
Index: linux-2.6.13/drivers/acpi/tables.c
===================================================================
--- linux-2.6.13.orig/drivers/acpi/tables.c
+++ linux-2.6.13/drivers/acpi/tables.c
@@ -564,8 +564,14 @@ acpi_table_get_sdt (
  * checksum all tables, print SDT/XSDT
  * 
  * result: sdt_entry[] is initialized
  */
+#if CONFIG_XEN
+#define acpi_rsdp_phys_to_va(rsdp_phys) (__fix_to_virt(FIX_ACPI_RSDP_PAGE) + \
+					   (rsdp_phys & ~PAGE_MASK))
+#else
+#define acpi_rsdp_phys_to_va(rsdp_phys) __va(rsdp_phys)
+#endif
 
 int __init
 acpi_table_init (void)
 {
@@ -580,9 +586,9 @@ acpi_table_init (void)
 		printk(KERN_ERR PREFIX "Unable to locate RSDP\n");
 		return -ENODEV;
 	}
 
-	rsdp = (struct acpi_table_rsdp *) __va(rsdp_phys);
+	rsdp = (struct acpi_table_rsdp *) acpi_rsdp_phys_to_va(rsdp_phys);
 	if (!rsdp) {
 		printk(KERN_WARNING PREFIX "Unable to map RSDP\n");
 		return -ENODEV;
 	}
Index: linux-2.6.13/drivers/char/mem.c
===================================================================
--- linux-2.6.13.orig/drivers/char/mem.c
+++ linux-2.6.13/drivers/char/mem.c
@@ -106,8 +106,9 @@ static inline int valid_phys_addr_range(
 	return 1;
 }
 #endif
 
+#ifndef ARCH_HAS_DEV_MEM
 /*
  * This funcion reads the *physical* memory. The f_pos points directly to the 
  * memory location. 
  */
@@ -230,8 +231,9 @@ static ssize_t write_mem(struct file * f
 
 	*ppos += written;
 	return written;
 }
+#endif
 
 static int mmap_mem(struct file * file, struct vm_area_struct * vma)
 {
 #if defined(__HAVE_PHYS_MEM_ACCESS_PROT)
@@ -763,15 +765,19 @@ static int open_port(struct inode * inod
 #define open_mem	open_port
 #define open_kmem	open_mem
 #define open_oldmem	open_mem
 
+#ifndef ARCH_HAS_DEV_MEM
 static struct file_operations mem_fops = {
 	.llseek		= memory_lseek,
 	.read		= read_mem,
 	.write		= write_mem,
 	.mmap		= mmap_mem,
 	.open		= open_mem,
 };
+#else
+extern struct file_operations mem_fops;
+#endif
 
 static struct file_operations kmem_fops = {
 	.llseek		= memory_lseek,
 	.read		= read_kmem,
Index: linux-2.6.13/drivers/char/tty_io.c
===================================================================
--- linux-2.6.13.orig/drivers/char/tty_io.c
+++ linux-2.6.13/drivers/char/tty_io.c
@@ -1801,9 +1801,9 @@ retry_open:
 		/* noctty = 1; */
 		goto got_driver;
 	}
 #ifdef CONFIG_VT
-	if (console_use_vt && device == MKDEV(TTY_MAJOR,0)) {
+	if (console_use_vt && (device == MKDEV(TTY_MAJOR,0))) {
 		extern struct tty_driver *console_driver;
 		driver = console_driver;
 		index = fg_console;
 		noctty = 1;
Index: linux-2.6.13/include/asm-generic/pgtable.h
===================================================================
--- linux-2.6.13.orig/include/asm-generic/pgtable.h
+++ linux-2.6.13/include/asm-generic/pgtable.h
@@ -41,8 +41,18 @@ do {				  					  \
 	flush_tlb_page(__vma, __address);				  \
 } while (0)
 #endif
 
+#ifndef __HAVE_ARCH_PTEP_ESTABLISH_NEW
+/*
+ * Establish a mapping where none previously existed
+ */
+#define ptep_establish_new(__vma, __address, __ptep, __entry)		\
+do {									\
+	set_pte(__ptep, __entry);					\
+} while (0)
+#endif
+
 #ifndef __HAVE_ARCH_PTEP_TEST_AND_CLEAR_YOUNG
 #define ptep_test_and_clear_young(__vma, __address, __ptep)		\
 ({									\
 	pte_t __pte = *(__ptep);					\
Index: linux-2.6.13/include/linux/gfp.h
===================================================================
--- linux-2.6.13.orig/include/linux/gfp.h
+++ linux-2.6.13/include/linux/gfp.h
@@ -77,10 +77,14 @@ struct vm_area_struct;
  * For the normal case of non-DISCONTIGMEM systems the NODE_DATA() gets
  * optimized to &contig_page_data at compile-time.
  */
 
+/*
+ * If arch_free_page returns non-zero then the generic free_page code can
+ * immediately bail: the arch-specific function has done all the work.
+ */
 #ifndef HAVE_ARCH_FREE_PAGE
-static inline void arch_free_page(struct page *page, int order) { }
+#define arch_free_page(page, order) 0
 #endif
 
 extern struct page *
 FASTCALL(__alloc_pages(unsigned int, unsigned int, struct zonelist *));
Index: linux-2.6.13/include/linux/highmem.h
===================================================================
--- linux-2.6.13.orig/include/linux/highmem.h
+++ linux-2.6.13/include/linux/highmem.h
@@ -12,12 +12,14 @@
 #include <asm/highmem.h>
 
 /* declarations for linux/mm/highmem.c */
 unsigned int nr_free_highpages(void);
+void kmap_flush_unused(void);
 
 #else /* CONFIG_HIGHMEM */
 
 static inline unsigned int nr_free_highpages(void) { return 0; }
+static inline void kmap_flush_unused(void) { }
 
 static inline void *kmap(struct page *page)
 {
 	might_sleep();
Index: linux-2.6.13/include/linux/irq.h
===================================================================
--- linux-2.6.13.orig/include/linux/irq.h
+++ linux-2.6.13/include/linux/irq.h
@@ -77,8 +77,9 @@ extern irq_desc_t irq_desc [NR_IRQS];
 
 #include <asm/hw_irq.h> /* the arch dependent stuff */
 
 extern int setup_irq(unsigned int irq, struct irqaction * new);
+extern int teardown_irq(unsigned int irq, struct irqaction * old);
 
 #ifdef CONFIG_GENERIC_HARDIRQS
 extern cpumask_t irq_affinity[NR_IRQS];
 extern int no_irq_affinity;
Index: linux-2.6.13/include/linux/mm.h
===================================================================
--- linux-2.6.13.orig/include/linux/mm.h
+++ linux-2.6.13/include/linux/mm.h
@@ -160,8 +160,9 @@ extern unsigned int kobjsize(const void 
 #define VM_ACCOUNT	0x00100000	/* Is a VM accounted object */
 #define VM_HUGETLB	0x00400000	/* Huge TLB Page VM */
 #define VM_NONLINEAR	0x00800000	/* Is non-linear (remap_file_pages) */
 #define VM_MAPPED_COPY	0x01000000	/* T if mapped copy of data (nommu mmap) */
+#define VM_FOREIGN	0x02000000	/* Has pages belonging to another VM */
 
 #ifndef VM_STACK_DEFAULT_FLAGS		/* arch can override this */
 #define VM_STACK_DEFAULT_FLAGS VM_DATA_DEFAULT_FLAGS
 #endif
@@ -947,8 +948,14 @@ int remap_page_range(struct vm_area_stru
 {
 	return remap_pfn_range(vma, uvaddr, paddr >> PAGE_SHIFT, size, prot);
 }
 
+typedef int (*pte_fn_t)(pte_t *pte, struct page *pte_page, unsigned long addr, 
+                        void *data);
+extern int generic_page_range(struct mm_struct *mm, unsigned long address, 
+                              unsigned long size, pte_fn_t fn, void *data);
+
+
 #ifdef CONFIG_PROC_FS
 void __vm_stat_account(struct mm_struct *, unsigned long, struct file *, long);
 #else
 static inline void __vm_stat_account(struct mm_struct *mm,
Index: linux-2.6.13/include/linux/skbuff.h
===================================================================
--- linux-2.6.13.orig/include/linux/skbuff.h
+++ linux-2.6.13/include/linux/skbuff.h
@@ -177,8 +177,10 @@ struct skb_shared_info {
  *	@csum: Checksum
  *	@local_df: allow local fragmentation
  *	@cloned: Head may be cloned (check refcnt to be sure)
  *	@nohdr: Payload reference only, must not modify header
+ *	@proto_csum_valid: Protocol csum validated since arriving at localhost
+ *	@proto_csum_blank: Protocol csum must be added before leaving localhost
  *	@pkt_type: Packet class
  *	@ip_summed: Driver fed us an IP checksum
  *	@priority: Packet queueing priority
  *	@users: User count - see {datagram,tcp}.c
@@ -251,10 +253,12 @@ struct sk_buff {
 	__u32			priority;
 	__u8			local_df:1,
 				cloned:1,
 				ip_summed:2,
-				nohdr:1;
-				/* 3 bits spare */
+				nohdr:1,
+				proto_csum_valid:1,
+				proto_csum_blank:1;
+				/* 1 bits spare */
 	__u8			pkt_type;
 	__be16			protocol;
 
 	void			(*destructor)(struct sk_buff *skb);
Index: linux-2.6.13/kernel/irq/manage.c
===================================================================
--- linux-2.6.13.orig/kernel/irq/manage.c
+++ linux-2.6.13/kernel/irq/manage.c
@@ -146,11 +146,16 @@ int can_request_irq(unsigned int irq, un
 
 	return !action;
 }
 
-/*
- * Internal function to register an irqaction - typically used to
- * allocate special interrupts that are part of the architecture.
+/**
+ *	setup_irq - register an irqaction structure
+ *	@irq: Interrupt to register
+ *	@irqaction: The irqaction structure to be registered
+ *
+ *	Normally called by request_irq, this function can be used
+ *	directly to allocate special interrupts that are part of the
+ *	architecture.
  */
 int setup_irq(unsigned int irq, struct irqaction * new)
 {
 	struct irq_desc *desc = irq_desc + irq;
@@ -217,30 +222,29 @@ int setup_irq(unsigned int irq, struct i
 
 	return 0;
 }
 
-/**
- *	free_irq - free an interrupt
- *	@irq: Interrupt line to free
- *	@dev_id: Device identity to free
- *
- *	Remove an interrupt handler. The handler is removed and if the
- *	interrupt line is no longer in use by any driver it is disabled.
- *	On a shared IRQ the caller must ensure the interrupt is disabled
- *	on the card it drives before calling this function. The function
- *	does not return until any executing interrupts for this IRQ
- *	have completed.
+/*
+ *	teardown_irq - unregister an irqaction
+ *	@irq: Interrupt line being freed
+ *	@old: Pointer to the irqaction that is to be unregistered
+ *
+ *	This function is called by free_irq and does the actual
+ *	business of unregistering the handler. It exists as a 
+ *	seperate function to enable handlers to be unregistered 
+ *	for irqactions that have been allocated statically at 
+ *	boot time.
  *
  *	This function must not be called from interrupt context.
  */
-void free_irq(unsigned int irq, void *dev_id)
+int teardown_irq(unsigned int irq, struct irqaction * old)
 {
 	struct irq_desc *desc;
 	struct irqaction **p;
 	unsigned long flags;
 
 	if (irq >= NR_IRQS)
-		return;
+		return -ENOENT;
 
 	desc = irq_desc + irq;
 	spin_lock_irqsave(&desc->lock,flags);
 	p = &desc->action;
@@ -250,9 +254,9 @@ void free_irq(unsigned int irq, void *de
 		if (action) {
 			struct irqaction **pp = p;
 
 			p = &action->next;
-			if (action->dev_id != dev_id)
+			if (action != old)
 				continue;
 
 			/* Found it - now remove it from the list of entries */
 			*pp = action->next;
@@ -274,15 +278,54 @@ void free_irq(unsigned int irq, void *de
 			unregister_handler_proc(irq, action);
 
 			/* Make sure it's not being used on another CPU */
 			synchronize_irq(irq);
-			kfree(action);
-			return;
+			return 0;
 		}
-		printk(KERN_ERR "Trying to free free IRQ%d\n",irq);
+		printk(KERN_ERR "Trying to teardown free IRQ%d\n",irq);
 		spin_unlock_irqrestore(&desc->lock,flags);
+		return -ENOENT;
+	}
+}
+
+/**
+ *	free_irq - free an interrupt
+ *	@irq: Interrupt line to free
+ *	@dev_id: Device identity to free
+ *
+ *	Remove an interrupt handler. The handler is removed and if the
+ *	interrupt line is no longer in use by any driver it is disabled.
+ *	On a shared IRQ the caller must ensure the interrupt is disabled
+ *	on the card it drives before calling this function. The function
+ *	does not return until any executing interrupts for this IRQ
+ *	have completed.
+ *
+ *	This function must not be called from interrupt context.
+ */
+void free_irq(unsigned int irq, void *dev_id)
+{
+	struct irq_desc *desc;
+	struct irqaction *action;
+	unsigned long flags;
+
+	if (irq >= NR_IRQS)
+		return;
+
+	desc = irq_desc + irq;
+	spin_lock_irqsave(&desc->lock,flags);
+	for (action = desc->action; action != NULL; action = action->next) {
+		if (action->dev_id != dev_id)
+			continue;
+
+		spin_unlock_irqrestore(&desc->lock,flags);
+
+		if (teardown_irq(irq, action) == 0)
+			kfree(action);
 		return;
 	}
+	printk(KERN_ERR "Trying to free free IRQ%d\n",irq);
+	spin_unlock_irqrestore(&desc->lock,flags);
+	return;
 }
 
 EXPORT_SYMBOL(free_irq);
 
Index: linux-2.6.13/mm/highmem.c
===================================================================
--- linux-2.6.13.orig/mm/highmem.c
+++ linux-2.6.13/mm/highmem.c
@@ -147,8 +147,17 @@ start:
 
 	return vaddr;
 }
 
+void kmap_flush_unused(void)
+{
+	spin_lock(&kmap_lock);
+	flush_all_zero_pkmaps();
+	spin_unlock(&kmap_lock);
+}
+
+EXPORT_SYMBOL(kmap_flush_unused);
+
 void fastcall *kmap_high(struct page *page)
 {
 	unsigned long vaddr;
 
Index: linux-2.6.13/mm/memory.c
===================================================================
--- linux-2.6.13.orig/mm/memory.c
+++ linux-2.6.13/mm/memory.c
@@ -933,8 +933,28 @@ int get_user_pages(struct task_struct *t
 			len--;
 			continue;
 		}
 
+                if (vma && (vma->vm_flags & VM_FOREIGN))
+                {
+                    struct page **map = vma->vm_private_data;
+                    int offset = (start - vma->vm_start) >> PAGE_SHIFT;
+
+                    if (map[offset] != NULL) {
+                        if (pages) {
+                            pages[i] = map[offset];
+                        } 
+                        if (vmas) 
+                            vmas[i] = vma;
+                        i++;
+                        start += PAGE_SIZE;
+                        len--;
+printk(KERN_ALERT "HIT  0x%lx\n", start);
+                        continue;
+                    } 
+else printk(KERN_ALERT "MISS 0x%lx\n", start);
+                }
+
 		if (!vma || (vma->vm_flags & VM_IO)
 				|| !(flags & vma->vm_flags))
 			return i ? : -EFAULT;
 
@@ -1152,9 +1172,9 @@ int remap_pfn_range(struct vm_area_struc
 		    unsigned long pfn, unsigned long size, pgprot_t prot)
 {
 	pgd_t *pgd;
 	unsigned long next;
-	unsigned long end = addr + PAGE_ALIGN(size);
+	unsigned long end = addr + size;
 	struct mm_struct *mm = vma->vm_mm;
 	int err;
 
 	/*
@@ -1183,8 +1203,106 @@ int remap_pfn_range(struct vm_area_struc
 	return err;
 }
 EXPORT_SYMBOL(remap_pfn_range);
 
+static inline int generic_pte_range(struct mm_struct *mm,
+                                    pmd_t *pmd, 
+                                    unsigned long addr, 
+                                    unsigned long end,
+                                    pte_fn_t fn, void *data)
+{
+	pte_t *pte;
+        int err;
+        struct page *pte_page;
+
+        pte = (mm == &init_mm) ? 
+                pte_alloc_kernel(mm, pmd, addr) :
+                pte_alloc_map(mm, pmd, addr);
+        if (!pte)
+                return -ENOMEM;
+
+        pte_page = pmd_page(*pmd);
+
+        do {
+                err = fn(pte, pte_page, addr, data);
+		if (err)
+                        break;
+        } while (pte++, addr += PAGE_SIZE, addr != end);
+
+        if (mm != &init_mm)
+                pte_unmap(pte-1);
+        return err;
+
+}
+
+static inline int generic_pmd_range(struct mm_struct *mm,
+                                    pud_t *pud, 
+                                    unsigned long addr, 
+                                    unsigned long end,
+                                    pte_fn_t fn, void *data)
+{
+	pmd_t *pmd;
+	unsigned long next;
+        int err;
+
+	pmd = pmd_alloc(mm, pud, addr);
+	if (!pmd)
+		return -ENOMEM;
+	do {
+		next = pmd_addr_end(addr, end);
+                err = generic_pte_range(mm, pmd, addr, next, fn, data);
+                if (err)
+                    break;
+	} while (pmd++, addr = next, addr != end);
+	return err;
+}
+
+static inline int generic_pud_range(struct mm_struct *mm, pgd_t *pgd, 
+                                    unsigned long addr,
+                                    unsigned long end,
+                                    pte_fn_t fn, void *data)
+{
+	pud_t *pud;
+	unsigned long next;
+        int err;
+
+	pud = pud_alloc(mm, pgd, addr);
+	if (!pud)
+		return -ENOMEM;
+	do {
+		next = pud_addr_end(addr, end);
+		err = generic_pmd_range(mm, pud, addr, next, fn, data);
+                if (err)
+			break;
+	} while (pud++, addr = next, addr != end);
+	return err;
+}
+
+/*
+ * Scan a region of virtual memory, filling in page tables as necessary
+ * and calling a provided function on each leaf page table.
+ */
+int generic_page_range(struct mm_struct *mm, unsigned long addr, 
+                  unsigned long size, pte_fn_t fn, void *data)
+{
+	pgd_t *pgd;
+	unsigned long next;
+	unsigned long end = addr + size;
+	int err;
+
+	BUG_ON(addr >= end);
+	pgd = pgd_offset(mm, addr);
+	spin_lock(&mm->page_table_lock);
+	do {
+		next = pgd_addr_end(addr, end);
+		err = generic_pud_range(mm, pgd, addr, next, fn, data);
+		if (err)
+			break;
+	} while (pgd++, addr = next, addr != end);
+	spin_unlock(&mm->page_table_lock);
+	return err;
+}
+
 /*
  * Do pte_mkwrite, but only if the vma says VM_WRITE.  We do this when
  * servicing faults for write access.  In the normal case, do always want
  * pte_mkwrite.  But get_user_pages can cause write faults for mappings
@@ -1238,22 +1356,17 @@ static int do_wp_page(struct mm_struct *
 	struct page *old_page, *new_page;
 	unsigned long pfn = pte_pfn(pte);
 	pte_t entry;
 	int ret;
+	struct page invalid_page;
 
 	if (unlikely(!pfn_valid(pfn))) {
-		/*
-		 * This should really halt the system so it can be debugged or
-		 * at least the kernel stops what it's doing before it corrupts
-		 * data, but for the moment just pretend this is OOM.
-		 */
-		pte_unmap(page_table);
-		printk(KERN_ERR "do_wp_page: bogus page at address %08lx\n",
-				address);
-		spin_unlock(&mm->page_table_lock);
-		return VM_FAULT_OOM;
+		/* This can happen with /dev/mem (PROT_WRITE, MAP_PRIVATE). */
+		invalid_page.flags = (1<<PG_reserved) | (1<<PG_locked);
+		old_page = &invalid_page;
+	} else {
+		old_page = pfn_to_page(pfn);
 	}
-	old_page = pfn_to_page(pfn);
 
 	if (PageAnon(old_page) && !TestSetPageLocked(old_page)) {
 		int reuse = can_share_swap_page(old_page);
 		unlock_page(old_page);
@@ -1287,9 +1400,15 @@ static int do_wp_page(struct mm_struct *
 	} else {
 		new_page = alloc_page_vma(GFP_HIGHUSER, vma, address);
 		if (!new_page)
 			goto no_new_page;
-		copy_user_highpage(new_page, old_page, address);
+		if (old_page == &invalid_page) {
+			char *vto = kmap_atomic(new_page, KM_USER1);
+			copy_page(vto, (void *)(address & PAGE_MASK));
+			kunmap_atomic(vto, KM_USER1);
+		} else {
+			copy_user_highpage(new_page, old_page, address);
+		}
 	}
 	/*
 	 * Re-check the pte - we dropped the lock
 	 */
Index: linux-2.6.13/mm/mmap.c
===================================================================
--- linux-2.6.13.orig/mm/mmap.c
+++ linux-2.6.13/mm/mmap.c
@@ -1954,8 +1954,12 @@ void exit_mmap(struct mm_struct *mm)
 	struct vm_area_struct *vma = mm->mmap;
 	unsigned long nr_accounted = 0;
 	unsigned long end;
 
+#ifdef arch_exit_mmap
+	arch_exit_mmap(mm);
+#endif
+
 	lru_add_drain();
 
 	spin_lock(&mm->page_table_lock);
 
Index: linux-2.6.13/mm/page_alloc.c
===================================================================
--- linux-2.6.13.orig/mm/page_alloc.c
+++ linux-2.6.13/mm/page_alloc.c
@@ -369,9 +369,10 @@ void __free_pages_ok(struct page *page, 
 {
 	LIST_HEAD(list);
 	int i;
 
-	arch_free_page(page, order);
+	if (arch_free_page(page, order))
+		return;
 
 	mod_page_state(pgfree, 1 << order);
 
 #ifndef CONFIG_MMU
@@ -642,9 +643,10 @@ static void fastcall free_hot_cold_page(
 	struct zone *zone = page_zone(page);
 	struct per_cpu_pages *pcp;
 	unsigned long flags;
 
-	arch_free_page(page, 0);
+	if (arch_free_page(page, 0))
+		return;
 
 	kernel_map_pages(page, 1, 0);
 	inc_page_state(pgfree);
 	if (PageAnon(page))
Index: linux-2.6.13/net/core/dev.c
===================================================================
--- linux-2.6.13.orig/net/core/dev.c
+++ linux-2.6.13/net/core/dev.c
@@ -114,8 +114,13 @@
 #include <net/iw_handler.h>
 #endif	/* CONFIG_NET_RADIO */
 #include <asm/current.h>
 
+#include <net/ip.h>
+#include <linux/tcp.h>
+#include <linux/udp.h>
+
+
 /*
  *	The list of packet types we will receive (as opposed to discard)
  *	and the routines to invoke.
  *
@@ -1242,8 +1247,32 @@ int dev_queue_xmit(struct sk_buff *skb)
 	    (!(dev->features & NETIF_F_SG) || illegal_highdma(dev, skb)) &&
 	    __skb_linearize(skb, GFP_ATOMIC))
 		goto out_kfree_skb;
 
+	/* If a checksum-deferred packet is forwarded to a device that needs a
+	 * checksum, correct the pointers and force checksumming.
+	 */
+	if (skb->proto_csum_blank) {
+		if (skb->protocol != htons(ETH_P_IP))
+			goto out_kfree_skb;
+		skb->h.raw = (unsigned char *)skb->nh.iph + 4*skb->nh.iph->ihl;
+		if (skb->h.raw >= skb->tail)
+			goto out_kfree_skb;
+		switch (skb->nh.iph->protocol) {
+		case IPPROTO_TCP:
+			skb->csum = offsetof(struct tcphdr, check);
+			break;
+		case IPPROTO_UDP:
+			skb->csum = offsetof(struct udphdr, check);
+			break;
+		default:
+			goto out_kfree_skb;
+		}
+		if ((skb->h.raw + skb->csum + 2) > skb->tail)
+			goto out_kfree_skb;
+		skb->ip_summed = CHECKSUM_HW;
+	}
+
 	/* If packet is not checksummed and device does not support
 	 * checksumming for this protocol, complete checksumming here.
 	 */
 	if (skb->ip_summed == CHECKSUM_HW &&
@@ -1586,8 +1615,19 @@ int netif_receive_skb(struct sk_buff *sk
 		goto ncls;
 	}
 #endif
 
+	switch (skb->ip_summed) {
+	case CHECKSUM_UNNECESSARY:
+		skb->proto_csum_valid = 1;
+		break;
+	case CHECKSUM_HW:
+		/* XXX Implement me. */
+	default:
+		skb->proto_csum_valid = 0;
+		break;
+	}
+
 	list_for_each_entry_rcu(ptype, &ptype_all, list) {
 		if (!ptype->dev || ptype->dev == skb->dev) {
 			if (pt_prev) 
 				ret = deliver_skb(skb, pt_prev);
Index: linux-2.6.13/net/core/skbuff.c
===================================================================
--- linux-2.6.13.orig/net/core/skbuff.c
+++ linux-2.6.13/net/core/skbuff.c
@@ -356,8 +356,10 @@ struct sk_buff *skb_clone(struct sk_buff
 	C(csum);
 	C(local_df);
 	n->cloned = 1;
 	n->nohdr = 0;
+	C(proto_csum_valid);
+	C(proto_csum_blank);
 	C(pkt_type);
 	C(ip_summed);
 	C(priority);
 	C(protocol);
