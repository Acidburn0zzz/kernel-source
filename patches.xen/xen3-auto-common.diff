Subject: xen: common code changes.
From: kraxel@suse.de

autogenerated from http://xenbits.xensource.com/linux-2.6-xen.hg

Signed-off-by: Gerd Knorr <kraxel@suse.de>
--- vanilla-2.6.14/drivers/Kconfig	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/drivers/Kconfig	2005-11-08 15:06:47.000000000 +0100
@@ -4,6 +4,8 @@ menu "Device Drivers"
 
 source "drivers/base/Kconfig"
 
+if !XEN || XEN_PHYSDEV_ACCESS
+
 source "drivers/connector/Kconfig"
 
 source "drivers/mtd/Kconfig"
@@ -11,17 +13,23 @@ source "drivers/mtd/Kconfig"
 source "drivers/parport/Kconfig"
 
 source "drivers/pnp/Kconfig"
+endif
 
 source "drivers/block/Kconfig"
 
+if !XEN || XEN_PHYSDEV_ACCESS
 source "drivers/ide/Kconfig"
+endif
 
 source "drivers/scsi/Kconfig"
 
+if !XEN || XEN_PHYSDEV_ACCESS
 source "drivers/cdrom/Kconfig"
+endif
 
 source "drivers/md/Kconfig"
 
+if !XEN || XEN_PHYSDEV_ACCESS
 source "drivers/message/fusion/Kconfig"
 
 source "drivers/ieee1394/Kconfig"
@@ -29,9 +37,11 @@ source "drivers/ieee1394/Kconfig"
 source "drivers/message/i2o/Kconfig"
 
 source "drivers/macintosh/Kconfig"
+endif
 
 source "drivers/net/Kconfig"
 
+if !XEN || XEN_PHYSDEV_ACCESS
 source "drivers/isdn/Kconfig"
 
 source "drivers/telephony/Kconfig"
@@ -63,6 +73,7 @@ source "drivers/usb/Kconfig"
 source "drivers/mmc/Kconfig"
 
 source "drivers/infiniband/Kconfig"
+endif
 
 source "drivers/sn/Kconfig"
 
--- vanilla-2.6.14/drivers/acpi/Kconfig	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/drivers/acpi/Kconfig	2005-11-08 15:06:47.000000000 +0100
@@ -46,7 +46,7 @@ if ACPI
 
 config ACPI_SLEEP
 	bool "Sleep States"
-	depends on X86 && (!SMP || SUSPEND_SMP)
+	depends on X86 && (!SMP || SUSPEND_SMP) && !XEN
 	depends on PM
 	default y
 	---help---
--- vanilla-2.6.14/drivers/acpi/tables.c	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/drivers/acpi/tables.c	2005-11-08 15:06:47.000000000 +0100
@@ -572,6 +572,12 @@ static int __init acpi_table_get_sdt(str
  * 
  * result: sdt_entry[] is initialized
  */
+#if CONFIG_XEN
+#define acpi_rsdp_phys_to_va(rsdp_phys) (__fix_to_virt(FIX_ACPI_RSDP_PAGE) + \
+					   (rsdp_phys & ~PAGE_MASK))
+#else
+#define acpi_rsdp_phys_to_va(rsdp_phys) __va(rsdp_phys)
+#endif
 
 int __init acpi_table_init(void)
 {
@@ -587,7 +593,7 @@ int __init acpi_table_init(void)
 		return -ENODEV;
 	}
 
-	rsdp = (struct acpi_table_rsdp *)__va(rsdp_phys);
+	rsdp = (struct acpi_table_rsdp *)acpi_rsdp_phys_to_va(rsdp_phys);
 	if (!rsdp) {
 		printk(KERN_WARNING PREFIX "Unable to map RSDP\n");
 		return -ENODEV;
--- vanilla-2.6.14/drivers/char/mem.c	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/drivers/char/mem.c	2005-11-08 15:06:49.000000000 +0100
@@ -103,6 +103,7 @@ static inline int valid_phys_addr_range(
 }
 #endif
 
+#ifndef ARCH_HAS_DEV_MEM
 /*
  * This funcion reads the *physical* memory. The f_pos points directly to the 
  * memory location. 
@@ -227,6 +228,7 @@ static ssize_t write_mem(struct file * f
 	*ppos += written;
 	return written;
 }
+#endif
 
 static int mmap_mem(struct file * file, struct vm_area_struct * vma)
 {
@@ -760,6 +762,7 @@ static int open_port(struct inode * inod
 #define open_kmem	open_mem
 #define open_oldmem	open_mem
 
+#ifndef ARCH_HAS_DEV_MEM
 static struct file_operations mem_fops = {
 	.llseek		= memory_lseek,
 	.read		= read_mem,
@@ -767,6 +770,9 @@ static struct file_operations mem_fops =
 	.mmap		= mmap_mem,
 	.open		= open_mem,
 };
+#else
+extern struct file_operations mem_fops;
+#endif
 
 static struct file_operations kmem_fops = {
 	.llseek		= memory_lseek,
--- vanilla-2.6.14/drivers/char/tpm/Kconfig	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/drivers/char/tpm/Kconfig	2005-11-08 15:06:49.000000000 +0100
@@ -6,7 +6,7 @@ menu "TPM devices"
 
 config TCG_TPM
 	tristate "TPM Hardware Support"
-	depends on EXPERIMENTAL && PCI
+	depends on EXPERIMENTAL && (PCI || XEN)
 	---help---
 	  If you have a TPM security chip in your system, which
 	  implements the Trusted Computing Group's specification,
@@ -22,7 +22,7 @@ config TCG_TPM
 
 config TCG_NSC
 	tristate "National Semiconductor TPM Interface"
-	depends on TCG_TPM
+	depends on TCG_TPM && !XEN_UNPRIVILEGED_GUEST
 	---help---
 	  If you have a TPM security chip from National Semicondutor 
 	  say Yes and it will be accessible from within Linux.  To 
@@ -31,7 +31,7 @@ config TCG_NSC
 
 config TCG_ATMEL
 	tristate "Atmel TPM Interface"
-	depends on TCG_TPM
+	depends on TCG_TPM && !XEN_UNPRIVILEGED_GUEST
 	---help---
 	  If you have a TPM security chip from Atmel say Yes and it 
 	  will be accessible from within Linux.  To compile this driver 
@@ -49,5 +49,15 @@ config TCG_INFINEON
 	  Further information on this driver and the supported hardware
 	  can be found at http://www.prosec.rub.de/tpm
 
+config TCG_XEN
+	tristate "XEN TPM Interface"
+	depends on TCG_TPM && XEN && XEN_TPMDEV_FRONTEND
+	---help---
+	  If you want to make TPM support available to a Xen
+	  user domain, say Yes and it will
+          be accessible from within Linux. To compile this driver
+          as a module, choose M here; the module will be called
+          tpm_xen.
+
 endmenu
 
--- vanilla-2.6.14/drivers/char/tpm/Makefile	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/drivers/char/tpm/Makefile	2005-11-08 15:06:49.000000000 +0100
@@ -5,3 +5,4 @@ obj-$(CONFIG_TCG_TPM) += tpm.o
 obj-$(CONFIG_TCG_NSC) += tpm_nsc.o
 obj-$(CONFIG_TCG_ATMEL) += tpm_atmel.o
 obj-$(CONFIG_TCG_INFINEON) += tpm_infineon.o
+obj-$(CONFIG_TCG_XEN) += tpm_xen.o
--- vanilla-2.6.14/drivers/char/tpm/tpm.c	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/drivers/char/tpm/tpm.c	2005-11-08 15:06:49.000000000 +0100
@@ -28,9 +28,13 @@
 #include <linux/spinlock.h>
 #include "tpm.h"
 
+#define TPM_CHIP_NUM_MASK	0x0000ffff
+#define TPM_CHIP_TYPE_SHIFT	16	
+
 enum tpm_const {
 	TPM_MINOR = 224,	/* officially assigned */
-	TPM_BUFSIZE = 2048,
+	TPM_MIN_BUFSIZE = 2048,
+	TPM_MAX_BUFSIZE = 64 * 1024,
 	TPM_NUM_DEVICES = 256,
 	TPM_NUM_MASK_ENTRIES = TPM_NUM_DEVICES / (8 * sizeof(int))
 };
@@ -45,26 +49,29 @@ static void user_reader_timeout(unsigned
 
 	down(&chip->buffer_mutex);
 	atomic_set(&chip->data_pending, 0);
-	memset(chip->data_buffer, 0, TPM_BUFSIZE);
+	memset(chip->data_buffer, 0, chip->vendor->buffersize);
 	up(&chip->buffer_mutex);
 }
 
 /*
  * Internal kernel interface to transmit TPM commands
  */
-static ssize_t tpm_transmit(struct tpm_chip *chip, const char *buf,
+static ssize_t tpm_transmit(struct tpm_chip * chip, const char *buf,
 			    size_t bufsiz)
 {
 	ssize_t rc;
 	u32 count;
 	unsigned long stop;
 
+	if (!chip)
+		return -ENODEV;
+
 	count = be32_to_cpu(*((__be32 *) (buf + 2)));
 
 	if (count == 0)
 		return -ENODATA;
 	if (count > bufsiz) {
-		dev_err(&chip->pci_dev->dev,
+		dev_err(chip->dev,
 			"invalid count value %x %zx \n", count, bufsiz);
 		return -E2BIG;
 	}
@@ -72,21 +79,21 @@ static ssize_t tpm_transmit(struct tpm_c
 	down(&chip->tpm_mutex);
 
 	if ((rc = chip->vendor->send(chip, (u8 *) buf, count)) < 0) {
-		dev_err(&chip->pci_dev->dev,
+		dev_err(chip->dev,
 			"tpm_transmit: tpm_send: error %zd\n", rc);
 		goto out;
 	}
 
 	stop = jiffies + 2 * 60 * HZ;
 	do {
-		u8 status = inb(chip->vendor->base + 1);
+		u8 status = chip->vendor->status(chip);
 		if ((status & chip->vendor->req_complete_mask) ==
 		    chip->vendor->req_complete_val) {
 			goto out_recv;
 		}
 
 		if ((status == chip->vendor->req_canceled)) {
-			dev_err(&chip->pci_dev->dev, "Operation Canceled\n");
+			dev_err(chip->dev, "Operation Canceled\n");
 			rc = -ECANCELED;
 			goto out;
 		}
@@ -97,14 +104,14 @@ static ssize_t tpm_transmit(struct tpm_c
 
 
 	chip->vendor->cancel(chip);
-	dev_err(&chip->pci_dev->dev, "Operation Timed out\n");
+	dev_err(chip->dev, "Operation Timed out\n");
 	rc = -ETIME;
 	goto out;
 
 out_recv:
 	rc = chip->vendor->recv(chip, (u8 *) buf, bufsiz);
 	if (rc < 0)
-		dev_err(&chip->pci_dev->dev,
+		dev_err(chip->dev,
 			"tpm_transmit: tpm_recv: error %zd\n", rc);
 out:
 	up(&chip->tpm_mutex);
@@ -147,7 +154,7 @@ ssize_t tpm_show_pcrs(struct device *dev
 	memcpy(data, cap_pcr, sizeof(cap_pcr));
 	if ((len = tpm_transmit(chip, data, sizeof(data)))
 	    < CAP_PCR_RESULT_SIZE) {
-		dev_dbg(&chip->pci_dev->dev, "A TPM error (%d) occurred "
+		dev_dbg(chip->dev, "A TPM error (%d) occurred "
 				"attempting to determine the number of PCRS\n",
 			be32_to_cpu(*((__be32 *) (data + 6))));
 		return 0;
@@ -161,7 +168,7 @@ ssize_t tpm_show_pcrs(struct device *dev
 		memcpy(data + 10, &index, 4);
 		if ((len = tpm_transmit(chip, data, sizeof(data)))
 		    < READ_PCR_RESULT_SIZE){
-			dev_dbg(&chip->pci_dev->dev, "A TPM error (%d) occurred"
+			dev_dbg(chip->dev, "A TPM error (%d) occurred"
 				" attempting to read PCR %d of %d\n",
 				be32_to_cpu(*((__be32 *) (data + 6))), i, num_pcrs);
 			goto out;
@@ -205,7 +212,7 @@ ssize_t tpm_show_pubek(struct device *de
 
 	if ((len = tpm_transmit(chip, data, READ_PUBEK_RESULT_SIZE)) <
 	    READ_PUBEK_RESULT_SIZE) {
-		dev_dbg(&chip->pci_dev->dev, "A TPM error (%d) occurred "
+		dev_dbg(chip->dev, "A TPM error (%d) occurred "
 				"attempting to read the PUBEK\n",
 			    be32_to_cpu(*((__be32 *) (data + 6))));
 		rc = 0;
@@ -339,21 +346,21 @@ int tpm_open(struct inode *inode, struct
 	}
 
 	if (chip->num_opens) {
-		dev_dbg(&chip->pci_dev->dev,
+		dev_dbg(chip->dev,
 			"Another process owns this TPM\n");
 		rc = -EBUSY;
 		goto err_out;
 	}
 
 	chip->num_opens++;
-	pci_dev_get(chip->pci_dev);
+	get_device(chip->dev);
 
 	spin_unlock(&driver_lock);
 
-	chip->data_buffer = kmalloc(TPM_BUFSIZE * sizeof(u8), GFP_KERNEL);
+	chip->data_buffer = kmalloc(chip->vendor->buffersize * sizeof(u8), GFP_KERNEL);
 	if (chip->data_buffer == NULL) {
 		chip->num_opens--;
-		pci_dev_put(chip->pci_dev);
+		put_device(chip->dev);
 		return -ENOMEM;
 	}
 
@@ -378,7 +385,7 @@ int tpm_release(struct inode *inode, str
 	chip->num_opens--;
 	del_singleshot_timer_sync(&chip->user_read_timer);
 	atomic_set(&chip->data_pending, 0);
-	pci_dev_put(chip->pci_dev);
+	put_device(chip->dev);
 	kfree(chip->data_buffer);
 	spin_unlock(&driver_lock);
 	return 0;
@@ -399,8 +406,8 @@ ssize_t tpm_write(struct file * file, co
 
 	down(&chip->buffer_mutex);
 
-	if (in_size > TPM_BUFSIZE)
-		in_size = TPM_BUFSIZE;
+	if (in_size > chip->vendor->buffersize)
+		in_size = chip->vendor->buffersize;
 
 	if (copy_from_user
 	    (chip->data_buffer, (void __user *) buf, in_size)) {
@@ -409,9 +416,11 @@ ssize_t tpm_write(struct file * file, co
 	}
 
 	/* atomic tpm command send and result receive */
-	out_size = tpm_transmit(chip, chip->data_buffer, TPM_BUFSIZE);
+	out_size = tpm_transmit(chip, chip->data_buffer, 
+	                        chip->vendor->buffersize);
 
 	atomic_set(&chip->data_pending, out_size);
+	atomic_set(&chip->data_position, 0);
 	up(&chip->buffer_mutex);
 
 	/* Set a timeout by which the reader must come claim the result */
@@ -427,32 +436,45 @@ ssize_t tpm_read(struct file * file, cha
 {
 	struct tpm_chip *chip = file->private_data;
 	int ret_size;
+	int pos, pending = 0;
 
-	del_singleshot_timer_sync(&chip->user_read_timer);
 	ret_size = atomic_read(&chip->data_pending);
-	atomic_set(&chip->data_pending, 0);
 	if (ret_size > 0) {	/* relay data */
 		if (size < ret_size)
 			ret_size = size;
 
+		pos = atomic_read(&chip->data_position);
+
 		down(&chip->buffer_mutex);
 		if (copy_to_user
-		    ((void __user *) buf, chip->data_buffer, ret_size))
+		    ((void __user *) buf, &chip->data_buffer[pos], ret_size)) {
 			ret_size = -EFAULT;
+		} else {
+			pending = atomic_read(&chip->data_pending) - ret_size;
+			if ( pending ) {
+				atomic_set( &chip->data_pending, pending );
+				atomic_set( &chip->data_position, pos+ret_size );
+			}
+		}
 		up(&chip->buffer_mutex);
 	}
+	
+	if ( ret_size <= 0 || pending == 0 ) {
+		atomic_set( &chip->data_pending, 0 );
+		del_singleshot_timer_sync(&chip->user_read_timer);
+	}
 
 	return ret_size;
 }
 
 EXPORT_SYMBOL_GPL(tpm_read);
 
-void __devexit tpm_remove(struct pci_dev *pci_dev)
+void tpm_remove_hardware(struct device *dev)
 {
-	struct tpm_chip *chip = pci_get_drvdata(pci_dev);
+	struct tpm_chip *chip = dev_get_drvdata(dev);
 
 	if (chip == NULL) {
-		dev_err(&pci_dev->dev, "No device data found\n");
+		dev_err(dev, "No device data found\n");
 		return;
 	}
 
@@ -462,22 +484,20 @@ void __devexit tpm_remove(struct pci_dev
 
 	spin_unlock(&driver_lock);
 
-	pci_set_drvdata(pci_dev, NULL);
+	dev_set_drvdata(dev, NULL);
 	misc_deregister(&chip->vendor->miscdev);
 	kfree(chip->vendor->miscdev.name);
 
-	sysfs_remove_group(&pci_dev->dev.kobj, chip->vendor->attr_group);
-
-	pci_disable_device(pci_dev);
+	sysfs_remove_group(&dev->kobj, chip->vendor->attr_group);
 
 	dev_mask[chip->dev_num / TPM_NUM_MASK_ENTRIES ] &= !(1 << (chip->dev_num % TPM_NUM_MASK_ENTRIES));
 
 	kfree(chip);
 
-	pci_dev_put(pci_dev);
+	put_device(dev);
 }
 
-EXPORT_SYMBOL_GPL(tpm_remove);
+EXPORT_SYMBOL_GPL(tpm_remove_hardware);
 
 static u8 savestate[] = {
 	0, 193,			/* TPM_TAG_RQU_COMMAND */
@@ -524,7 +544,7 @@ EXPORT_SYMBOL_GPL(tpm_pm_resume);
  * upon errant exit from this function specific probe function should call
  * pci_disable_device
  */
-int tpm_register_hardware(struct pci_dev *pci_dev,
+int tpm_register_hardware(struct device *dev,
 			  struct tpm_vendor_specific *entry)
 {
 #define DEVNAME_SIZE 7
@@ -549,6 +569,12 @@ int tpm_register_hardware(struct pci_dev
 	chip->user_read_timer.data = (unsigned long) chip;
 
 	chip->vendor = entry;
+	
+	if (entry->buffersize < TPM_MIN_BUFSIZE) {
+		entry->buffersize = TPM_MIN_BUFSIZE;
+	} else if (entry->buffersize > TPM_MAX_BUFSIZE) {
+		entry->buffersize = TPM_MAX_BUFSIZE;
+	}
 
 	chip->dev_num = -1;
 
@@ -563,7 +589,7 @@ int tpm_register_hardware(struct pci_dev
 
 dev_num_search_complete:
 	if (chip->dev_num < 0) {
-		dev_err(&pci_dev->dev,
+		dev_err(dev,
 			"No available tpm device numbers\n");
 		kfree(chip);
 		return -ENODEV;
@@ -576,15 +602,15 @@ dev_num_search_complete:
 	scnprintf(devname, DEVNAME_SIZE, "%s%d", "tpm", chip->dev_num);
 	chip->vendor->miscdev.name = devname;
 
-	chip->vendor->miscdev.dev = &(pci_dev->dev);
-	chip->pci_dev = pci_dev_get(pci_dev);
+	chip->vendor->miscdev.dev = dev;
+	chip->dev = get_device(dev);
 
 	if (misc_register(&chip->vendor->miscdev)) {
-		dev_err(&chip->pci_dev->dev,
+		dev_err(chip->dev,
 			"unable to misc_register %s, minor %d\n",
 			chip->vendor->miscdev.name,
 			chip->vendor->miscdev.minor);
-		pci_dev_put(pci_dev);
+		put_device(dev);
 		kfree(chip);
 		dev_mask[i] &= !(1 << j);
 		return -ENODEV;
@@ -592,13 +618,13 @@ dev_num_search_complete:
 
 	spin_lock(&driver_lock);
 
-	pci_set_drvdata(pci_dev, chip);
+	dev_set_drvdata(dev, chip);
 
 	list_add(&chip->list, &tpm_chip_list);
 
 	spin_unlock(&driver_lock);
 
-	sysfs_create_group(&pci_dev->dev.kobj, chip->vendor->attr_group);
+	sysfs_create_group(&dev->kobj, chip->vendor->attr_group);
 
 	return 0;
 }
--- vanilla-2.6.14/drivers/char/tpm/tpm.h	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/drivers/char/tpm/tpm.h	2005-11-08 15:06:49.000000000 +0100
@@ -51,16 +51,19 @@ struct tpm_vendor_specific {
 	u8 req_complete_val;
 	u8 req_canceled;
 	u16 base;		/* TPM base address */
+	int drv_type;
+	u32 buffersize;
 
 	int (*recv) (struct tpm_chip *, u8 *, size_t);
 	int (*send) (struct tpm_chip *, u8 *, size_t);
 	void (*cancel) (struct tpm_chip *);
+	u8 (*status) (struct tpm_chip *);
 	struct miscdevice miscdev;
 	struct attribute_group *attr_group;
 };
 
 struct tpm_chip {
-	struct pci_dev *pci_dev;	/* PCI device stuff */
+	struct device *dev;	/* Device stuff */
 
 	int dev_num;		/* /dev/tpm# */
 	int num_opens;		/* only one allowed */
@@ -69,6 +72,7 @@ struct tpm_chip {
 	/* Data passed to and from the tpm via the read/write calls */
 	u8 *data_buffer;
 	atomic_t data_pending;
+	atomic_t data_position;
 	struct semaphore buffer_mutex;
 
 	struct timer_list user_read_timer;	/* user needs to claim result */
@@ -91,13 +95,13 @@ static inline void tpm_write_index(int b
 	outb(value & 0xFF, base+1);
 }
 
-extern int tpm_register_hardware(struct pci_dev *,
+extern int tpm_register_hardware(struct device *,
 				 struct tpm_vendor_specific *);
 extern int tpm_open(struct inode *, struct file *);
 extern int tpm_release(struct inode *, struct file *);
 extern ssize_t tpm_write(struct file *, const char __user *, size_t,
 			 loff_t *);
 extern ssize_t tpm_read(struct file *, char __user *, size_t, loff_t *);
-extern void __devexit tpm_remove(struct pci_dev *);
+extern void tpm_remove_hardware(struct device *);
 extern int tpm_pm_suspend(struct pci_dev *, pm_message_t);
 extern int tpm_pm_resume(struct pci_dev *);
--- vanilla-2.6.14/drivers/char/tpm/tpm_atmel.c	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/drivers/char/tpm/tpm_atmel.c	2005-11-08 15:06:49.000000000 +0100
@@ -54,7 +54,7 @@ static int tpm_atml_recv(struct tpm_chip
 	for (i = 0; i < 6; i++) {
 		status = inb(chip->vendor->base + 1);
 		if ((status & ATML_STATUS_DATA_AVAIL) == 0) {
-			dev_err(&chip->pci_dev->dev,
+			dev_err(chip->dev,
 				"error reading header\n");
 			return -EIO;
 		}
@@ -66,12 +66,12 @@ static int tpm_atml_recv(struct tpm_chip
 	size = be32_to_cpu(*native_size);
 
 	if (count < size) {
-		dev_err(&chip->pci_dev->dev,
+		dev_err(chip->dev,
 			"Recv size(%d) less than available space\n", size);
 		for (; i < size; i++) {	/* clear the waiting data anyway */
 			status = inb(chip->vendor->base + 1);
 			if ((status & ATML_STATUS_DATA_AVAIL) == 0) {
-				dev_err(&chip->pci_dev->dev,
+				dev_err(chip->dev,
 					"error reading data\n");
 				return -EIO;
 			}
@@ -83,7 +83,7 @@ static int tpm_atml_recv(struct tpm_chip
 	for (; i < size; i++) {
 		status = inb(chip->vendor->base + 1);
 		if ((status & ATML_STATUS_DATA_AVAIL) == 0) {
-			dev_err(&chip->pci_dev->dev,
+			dev_err(chip->dev,
 				"error reading data\n");
 			return -EIO;
 		}
@@ -93,7 +93,7 @@ static int tpm_atml_recv(struct tpm_chip
 	/* make sure data available is gone */
 	status = inb(chip->vendor->base + 1);
 	if (status & ATML_STATUS_DATA_AVAIL) {
-		dev_err(&chip->pci_dev->dev, "data available is stuck\n");
+		dev_err(chip->dev, "data available is stuck\n");
 		return -EIO;
 	}
 
@@ -104,9 +104,9 @@ static int tpm_atml_send(struct tpm_chip
 {
 	int i;
 
-	dev_dbg(&chip->pci_dev->dev, "tpm_atml_send: ");
+	dev_dbg(chip->dev, "tpm_atml_send:\n");
 	for (i = 0; i < count; i++) {
-		dev_dbg(&chip->pci_dev->dev, "0x%x(%d) ", buf[i], buf[i]);
+		dev_dbg(chip->dev, "%d 0x%x(%d)\n",  i, buf[i], buf[i]);
 		outb(buf[i], chip->vendor->base);
 	}
 
@@ -118,6 +118,11 @@ static void tpm_atml_cancel(struct tpm_c
 	outb(ATML_STATUS_ABORT, chip->vendor->base + 1);
 }
 
+static u8 tpm_atml_status(struct tpm_chip *chip)
+{
+	return inb(chip->vendor->base + 1);
+}
+
 static struct file_operations atmel_ops = {
 	.owner = THIS_MODULE,
 	.llseek = no_llseek,
@@ -146,6 +151,7 @@ static struct tpm_vendor_specific tpm_at
 	.recv = tpm_atml_recv,
 	.send = tpm_atml_send,
 	.cancel = tpm_atml_cancel,
+	.status = tpm_atml_status,
 	.req_complete_mask = ATML_STATUS_BUSY | ATML_STATUS_DATA_AVAIL,
 	.req_complete_val = ATML_STATUS_DATA_AVAIL,
 	.req_canceled = ATML_STATUS_READY,
@@ -187,7 +193,7 @@ static int __devinit tpm_atml_init(struc
 		goto out_err;
 	}
 
-	if ((rc = tpm_register_hardware(pci_dev, &tpm_atmel)) < 0)
+	if ((rc = tpm_register_hardware(&pci_dev->dev, &tpm_atmel)) < 0)
 		goto out_err;
 
 	dev_info(&pci_dev->dev,
@@ -200,6 +206,14 @@ out_err:
 	return rc;
 }
 
+static void __devexit tpm_atml_remove(struct pci_dev *pci_dev) 
+{
+	struct tpm_chip *chip = pci_get_drvdata(pci_dev);
+
+	if ( chip )
+		tpm_remove_hardware(chip->dev);
+}
+
 static struct pci_device_id tpm_pci_tbl[] __devinitdata = {
 	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_82801BA_0)},
 	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_82801CA_12)},
@@ -220,7 +234,7 @@ static struct pci_driver atmel_pci_drive
 	.name = "tpm_atmel",
 	.id_table = tpm_pci_tbl,
 	.probe = tpm_atml_init,
-	.remove = __devexit_p(tpm_remove),
+	.remove = __devexit_p(tpm_atml_remove),
 	.suspend = tpm_pm_suspend,
 	.resume = tpm_pm_resume,
 };
@@ -235,7 +249,7 @@ static void __exit cleanup_atmel(void)
 	pci_unregister_driver(&atmel_pci_driver);
 }
 
-module_init(init_atmel);
+fs_initcall(init_atmel);
 module_exit(cleanup_atmel);
 
 MODULE_AUTHOR("Leendert van Doorn (leendert@watson.ibm.com)");
--- vanilla-2.6.14/drivers/char/tpm/tpm_nsc.c	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/drivers/char/tpm/tpm_nsc.c	2005-11-08 15:06:49.000000000 +0100
@@ -111,7 +111,7 @@ static int nsc_wait_for_ready(struct tpm
 	}
 	while (time_before(jiffies, stop));
 
-	dev_info(&chip->pci_dev->dev, "wait for ready failed\n");
+	dev_info(chip->dev, "wait for ready failed\n");
 	return -EBUSY;
 }
 
@@ -127,12 +127,12 @@ static int tpm_nsc_recv(struct tpm_chip 
 		return -EIO;
 
 	if (wait_for_stat(chip, NSC_STATUS_F0, NSC_STATUS_F0, &data) < 0) {
-		dev_err(&chip->pci_dev->dev, "F0 timeout\n");
+		dev_err(chip->dev, "F0 timeout\n");
 		return -EIO;
 	}
 	if ((data =
 	     inb(chip->vendor->base + NSC_DATA)) != NSC_COMMAND_NORMAL) {
-		dev_err(&chip->pci_dev->dev, "not in normal mode (0x%x)\n",
+		dev_err(chip->dev, "not in normal mode (0x%x)\n",
 			data);
 		return -EIO;
 	}
@@ -141,7 +141,7 @@ static int tpm_nsc_recv(struct tpm_chip 
 	for (p = buffer; p < &buffer[count]; p++) {
 		if (wait_for_stat
 		    (chip, NSC_STATUS_OBF, NSC_STATUS_OBF, &data) < 0) {
-			dev_err(&chip->pci_dev->dev,
+			dev_err(chip->dev,
 				"OBF timeout (while reading data)\n");
 			return -EIO;
 		}
@@ -152,11 +152,11 @@ static int tpm_nsc_recv(struct tpm_chip 
 
 	if ((data & NSC_STATUS_F0) == 0 &&
 	(wait_for_stat(chip, NSC_STATUS_F0, NSC_STATUS_F0, &data) < 0)) {
-		dev_err(&chip->pci_dev->dev, "F0 not set\n");
+		dev_err(chip->dev, "F0 not set\n");
 		return -EIO;
 	}
 	if ((data = inb(chip->vendor->base + NSC_DATA)) != NSC_COMMAND_EOC) {
-		dev_err(&chip->pci_dev->dev,
+		dev_err(chip->dev,
 			"expected end of command(0x%x)\n", data);
 		return -EIO;
 	}
@@ -187,19 +187,19 @@ static int tpm_nsc_send(struct tpm_chip 
 		return -EIO;
 
 	if (wait_for_stat(chip, NSC_STATUS_IBF, 0, &data) < 0) {
-		dev_err(&chip->pci_dev->dev, "IBF timeout\n");
+		dev_err(chip->dev, "IBF timeout\n");
 		return -EIO;
 	}
 
 	outb(NSC_COMMAND_NORMAL, chip->vendor->base + NSC_COMMAND);
 	if (wait_for_stat(chip, NSC_STATUS_IBR, NSC_STATUS_IBR, &data) < 0) {
-		dev_err(&chip->pci_dev->dev, "IBR timeout\n");
+		dev_err(chip->dev, "IBR timeout\n");
 		return -EIO;
 	}
 
 	for (i = 0; i < count; i++) {
 		if (wait_for_stat(chip, NSC_STATUS_IBF, 0, &data) < 0) {
-			dev_err(&chip->pci_dev->dev,
+			dev_err(chip->dev,
 				"IBF timeout (while writing data)\n");
 			return -EIO;
 		}
@@ -207,7 +207,7 @@ static int tpm_nsc_send(struct tpm_chip 
 	}
 
 	if (wait_for_stat(chip, NSC_STATUS_IBF, 0, &data) < 0) {
-		dev_err(&chip->pci_dev->dev, "IBF timeout\n");
+		dev_err(chip->dev, "IBF timeout\n");
 		return -EIO;
 	}
 	outb(NSC_COMMAND_EOC, chip->vendor->base + NSC_COMMAND);
@@ -220,6 +220,11 @@ static void tpm_nsc_cancel(struct tpm_ch
 	outb(NSC_COMMAND_CANCEL, chip->vendor->base + NSC_COMMAND);
 }
 
+static u8 tpm_nsc_status(struct tpm_chip *chip)
+{
+	return inb(chip->vendor->base + NSC_STATUS);
+}
+
 static struct file_operations nsc_ops = {
 	.owner = THIS_MODULE,
 	.llseek = no_llseek,
@@ -248,6 +253,7 @@ static struct tpm_vendor_specific tpm_ns
 	.recv = tpm_nsc_recv,
 	.send = tpm_nsc_send,
 	.cancel = tpm_nsc_cancel,
+	.status = tpm_nsc_status,
 	.req_complete_mask = NSC_STATUS_OBF,
 	.req_complete_val = NSC_STATUS_OBF,
 	.req_canceled = NSC_STATUS_RDY,
@@ -319,7 +325,7 @@ static int __devinit tpm_nsc_init(struct
 	/* enable the DPM module */
 	tpm_write_index(nscAddrBase, NSC_LDC_INDEX, 0x01);
 
-	if ((rc = tpm_register_hardware(pci_dev, &tpm_nsc)) < 0)
+	if ((rc = tpm_register_hardware(&pci_dev->dev, &tpm_nsc)) < 0)
 		goto out_err;
 
 	return 0;
@@ -329,6 +335,14 @@ out_err:
 	return rc;
 }
 
+static void __devexit tpm_nsc_remove(struct pci_dev *pci_dev) 
+{
+	struct tpm_chip *chip = pci_get_drvdata(pci_dev);
+
+	if ( chip )
+		tpm_remove_hardware(chip->dev);
+}
+
 static struct pci_device_id tpm_pci_tbl[] __devinitdata = {
 	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_82801BA_0)},
 	{PCI_DEVICE(PCI_VENDOR_ID_INTEL, PCI_DEVICE_ID_INTEL_82801CA_12)},
@@ -348,7 +362,7 @@ static struct pci_driver nsc_pci_driver 
 	.name = "tpm_nsc",
 	.id_table = tpm_pci_tbl,
 	.probe = tpm_nsc_init,
-	.remove = __devexit_p(tpm_remove),
+	.remove = __devexit_p(tpm_nsc_remove),
 	.suspend = tpm_pm_suspend,
 	.resume = tpm_pm_resume,
 };
@@ -363,7 +377,7 @@ static void __exit cleanup_nsc(void)
 	pci_unregister_driver(&nsc_pci_driver);
 }
 
-module_init(init_nsc);
+fs_initcall(init_nsc);
 module_exit(cleanup_nsc);
 
 MODULE_AUTHOR("Leendert van Doorn (leendert@watson.ibm.com)");
--- /dev/null	2005-10-26 14:15:28.920057500 +0200
+++ xen-hg12171/drivers/char/tpm/tpm_xen.c	2005-11-08 15:06:49.000000000 +0100
@@ -0,0 +1,531 @@
+/*
+ * Copyright (C) 2004 IBM Corporation
+ *
+ * Authors:
+ * Leendert van Doorn <leendert@watson.ibm.com>
+ * Dave Safford <safford@watson.ibm.com>
+ * Reiner Sailer <sailer@watson.ibm.com>
+ * Kylene Hall <kjhall@us.ibm.com>
+ * Stefan Berger <stefanb@us.ibm.com>
+ *
+ * Maintained by: <tpmdd_devel@lists.sourceforge.net>
+ *
+ * Device driver for TCG/TCPA TPM (trusted platform module) for XEN.
+ * Specifications at www.trustedcomputinggroup.org
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation, version 2 of the
+ * License.
+ *
+ */
+
+#include <asm/uaccess.h>
+#include <linux/list.h>
+#include <linux/tpmfe.h>
+#include <linux/device.h>
+#include <linux/interrupt.h>
+#include "tpm.h"
+
+/* read status bits */
+enum {
+	STATUS_BUSY = 0x01,
+	STATUS_DATA_AVAIL = 0x02,
+	STATUS_READY = 0x04
+};
+
+#define MIN(x,y)  ((x) < (y)) ? (x) : (y)
+
+struct transmission {
+	struct list_head next;
+	unsigned char *request;
+	unsigned int request_len;
+	unsigned char *rcv_buffer;
+	unsigned int  buffersize;
+	struct tpm_chip     *chip;
+	unsigned int flags;
+};
+
+enum {
+	TRANSMISSION_FLAG_WAS_QUEUED = 0x1
+};
+
+struct data_exchange {
+	struct transmission *current_request;
+	spinlock_t           req_list_lock;
+	wait_queue_head_t    req_wait_queue;
+
+	struct list_head     queued_requests;
+
+	struct transmission *current_response;
+	spinlock_t           resp_list_lock;
+	wait_queue_head_t    resp_wait_queue;     // processes waiting for responses
+
+	struct transmission *req_cancelled;       // if a cancellation was encounterd
+
+	unsigned int         fe_status;
+	unsigned int         flags;
+};
+
+enum {
+	DATAEX_FLAG_QUEUED_ONLY = 0x1
+};
+
+static struct data_exchange dataex;
+
+static unsigned long disconnect_time;
+
+/* local function prototypes */
+static void __exit cleanup_xen(void);
+
+
+/* =============================================================
+ * Some utility functions
+ * =============================================================
+ */
+static inline struct transmission *
+transmission_alloc(void)
+{
+	struct transmission *t = kmalloc(sizeof(*t), GFP_KERNEL);
+	if (t) {
+		memset(t, 0x0, sizeof(*t));
+	}
+	return t;
+}
+
+static inline unsigned char *
+transmission_set_buffer(struct transmission *t,
+                        unsigned char *buffer, unsigned int len)
+{
+	if (NULL != t->request) {
+		kfree(t->request);
+	}
+	t->request = kmalloc(len, GFP_KERNEL);
+	if (t->request) {
+		memcpy(t->request,
+		       buffer,
+		       len);
+		t->request_len = len;
+	}
+	return t->request;
+}
+
+static inline void
+transmission_free(struct transmission *t)
+{
+	if (t->request) {
+		kfree(t->request);
+	}
+	if (t->rcv_buffer) {
+		kfree(t->rcv_buffer);
+	}
+	kfree(t);
+}
+
+/* =============================================================
+ * Interface with the TPM shared memory driver for XEN
+ * =============================================================
+ */
+static int tpm_recv(const u8 *buffer, size_t count, const void *ptr)
+{
+	int ret_size = 0;
+	struct transmission *t, *temp;
+
+	/*
+	 * The list with requests must contain one request
+	 * only and the element there must be the one that
+	 * was passed to me from the front-end.
+	 */
+	if (dataex.current_request != ptr) {
+		printk("WARNING: The request pointer is different than the pointer "
+		       "the shared memory driver returned to me. %p != %p\n",
+		       dataex.current_request, ptr);
+	}
+
+	/*
+	 * If the request has been cancelled, just quit here
+	 */
+	if (dataex.req_cancelled == (struct transmission *)ptr) {
+		if (dataex.current_request == dataex.req_cancelled) {
+			dataex.current_request = NULL;
+		}
+		transmission_free(dataex.req_cancelled);
+		dataex.req_cancelled = NULL;
+		return 0;
+	}
+
+	if (NULL != (temp = dataex.current_request)) {
+		transmission_free(temp);
+		dataex.current_request = NULL;
+	}
+
+	t = transmission_alloc();
+	if (NULL != t) {
+		unsigned long flags;
+		t->rcv_buffer = kmalloc(count, GFP_KERNEL);
+		if (NULL == t->rcv_buffer) {
+			transmission_free(t);
+			return -ENOMEM;
+		}
+		t->buffersize = count;
+		memcpy(t->rcv_buffer, buffer, count);
+		ret_size = count;
+
+		spin_lock_irqsave(&dataex.resp_list_lock ,flags);
+		dataex.current_response = t;
+		spin_unlock_irqrestore(&dataex.resp_list_lock, flags);
+		wake_up_interruptible(&dataex.resp_wait_queue);
+	}
+	return ret_size;
+}
+
+
+static void tpm_fe_status(unsigned int flags)
+{
+	dataex.fe_status = flags;
+	if ((dataex.fe_status & TPMFE_STATUS_CONNECTED) == 0) {
+		disconnect_time = jiffies;
+	}
+}
+
+/* =============================================================
+ * Interface with the generic TPM driver
+ * =============================================================
+ */
+static int tpm_xen_recv(struct tpm_chip *chip, u8 * buf, size_t count)
+{
+	unsigned long flags;
+	int rc = 0;
+
+	spin_lock_irqsave(&dataex.resp_list_lock, flags);
+	/*
+	 * Check if the previous operation only queued the command
+	 * In this case there won't be a response, so I just
+	 * return from here and reset that flag. In any other
+	 * case I should receive a response from the back-end.
+	 */
+	if ((dataex.flags & DATAEX_FLAG_QUEUED_ONLY) != 0) {
+		dataex.flags &= ~DATAEX_FLAG_QUEUED_ONLY;
+		spin_unlock_irqrestore(&dataex.resp_list_lock, flags);
+		/*
+		 * a little hack here. The first few measurements
+		 * are queued since there's no way to talk to the
+		 * TPM yet (due to slowness of the control channel)
+		 * So we just make IMA happy by giving it 30 NULL
+		 * bytes back where the most important part is
+		 * that the result code is '0'.
+		 */
+
+		count = MIN(count, 30);
+		memset(buf, 0x0, count);
+		return count;
+	}
+	/*
+	 * Check whether something is in the responselist and if
+	 * there's nothing in the list wait for something to appear.
+	 */
+
+	if (NULL == dataex.current_response) {
+		spin_unlock_irqrestore(&dataex.resp_list_lock, flags);
+		interruptible_sleep_on_timeout(&dataex.resp_wait_queue,
+		                               1000);
+		spin_lock_irqsave(&dataex.resp_list_lock ,flags);
+	}
+
+	if (NULL != dataex.current_response) {
+		struct transmission *t = dataex.current_response;
+		dataex.current_response = NULL;
+		rc = MIN(count, t->buffersize);
+		memcpy(buf, t->rcv_buffer, rc);
+		transmission_free(t);
+	}
+
+	spin_unlock_irqrestore(&dataex.resp_list_lock, flags);
+	return rc;
+}
+
+static int tpm_xen_send(struct tpm_chip *chip, u8 * buf, size_t count)
+{
+	/*
+	 * We simply pass the packet onto the XEN shared
+	 * memory driver.
+	 */
+	unsigned long flags;
+	int rc;
+	struct transmission *t = transmission_alloc();
+
+	spin_lock_irqsave(&dataex.req_list_lock, flags);
+	/*
+	 * If there's a current request, it must be the
+	 * previous request that has timed out.
+	 */
+	if (dataex.current_request != NULL) {
+		printk("WARNING: Sending although there is a request outstanding.\n"
+		       "         Previous request must have timed out.\n");
+		transmission_free(dataex.current_request);
+		dataex.current_request = NULL;
+	}
+
+	if (t != NULL) {
+		unsigned int error = 0;
+		t->rcv_buffer = NULL;
+		t->buffersize = 0;
+		t->chip = chip;
+
+		/*
+		 * Queue the packet if the driver below is not
+		 * ready, yet, or there is any packet already
+		 * in the queue.
+		 * If the driver below is ready, unqueue all
+		 * packets first before sending our current
+		 * packet.
+		 * For each unqueued packet, except for the
+		 * last (=current) packet, call the function
+		 * tpm_xen_recv to wait for the response to come
+		 * back.
+		 */
+		if ((dataex.fe_status & TPMFE_STATUS_CONNECTED) == 0) {
+			if (time_after(jiffies, disconnect_time + HZ * 10)) {
+				rc = -ENOENT;
+			} else {
+				/*
+				 * copy the request into the buffer
+				 */
+				if (transmission_set_buffer(t, buf, count)
+				    == NULL) {
+					transmission_free(t);
+					rc = -ENOMEM;
+					goto exit;
+				}
+				dataex.flags |= DATAEX_FLAG_QUEUED_ONLY;
+				list_add_tail(&t->next, &dataex.queued_requests);
+				rc = 0;
+			}
+		} else {
+			/*
+			 * Check whether there are any packets in the queue
+			 */
+			while (!list_empty(&dataex.queued_requests)) {
+				/*
+				 * Need to dequeue them.
+				 * Read the result into a dummy buffer.
+				 */
+				unsigned char buffer[1];
+				struct transmission *qt = (struct transmission *) dataex.queued_requests.next;
+				list_del(&qt->next);
+				dataex.current_request = qt;
+				spin_unlock_irqrestore(&dataex.req_list_lock, flags);
+
+				rc = tpm_fe_send(qt->request,
+				                 qt->request_len,
+				                 qt);
+
+				if (rc < 0) {
+					spin_lock_irqsave(&dataex.req_list_lock, flags);
+					if ((qt = dataex.current_request) != NULL) {
+						/*
+						 * requeue it at the beginning
+						 * of the list
+						 */
+						list_add(&qt->next,
+						         &dataex.queued_requests);
+					}
+					dataex.current_request = NULL;
+					error = 1;
+					break;
+				}
+				/*
+				 * After this point qt is not valid anymore!
+				 * It is freed when the front-end is delivering the data
+				 * by calling tpm_recv
+				 */
+
+				/*
+				 * Try to receive the response now into the provided dummy
+				 * buffer (I don't really care about this response since
+				 * there is no receiver anymore for this response)
+				 */
+				rc = tpm_xen_recv(chip, buffer, sizeof(buffer));
+
+				spin_lock_irqsave(&dataex.req_list_lock, flags);
+			}
+
+			if (error == 0) {
+				/*
+				 * Finally, send the current request.
+				 */
+				dataex.current_request = t;
+				/*
+				 * Call the shared memory driver
+				 * Pass to it the buffer with the request, the
+				 * amount of bytes in the request and
+				 * a void * pointer (here: transmission structure)
+				 */
+				rc = tpm_fe_send(buf, count, t);
+				/*
+				 * The generic TPM driver will call
+				 * the function to receive the response.
+				 */
+				if (rc < 0) {
+					dataex.current_request = NULL;
+					goto queue_it;
+				}
+			} else {
+queue_it:
+				if (transmission_set_buffer(t, buf, count) == NULL) {
+					transmission_free(t);
+					rc = -ENOMEM;
+					goto exit;
+				}
+				/*
+				 * An error occurred. Don't event try
+				 * to send the current request. Just
+				 * queue it.
+				 */
+				dataex.flags |= DATAEX_FLAG_QUEUED_ONLY;
+				list_add_tail(&t->next, &dataex.queued_requests);
+				rc = 0;
+			}
+		}
+	} else {
+		rc = -ENOMEM;
+	}
+
+exit:
+	spin_unlock_irqrestore(&dataex.req_list_lock, flags);
+	return rc;
+}
+
+static void tpm_xen_cancel(struct tpm_chip *chip)
+{
+	unsigned long flags;
+	spin_lock_irqsave(&dataex.resp_list_lock,flags);
+
+	dataex.req_cancelled = dataex.current_request;
+
+	spin_unlock_irqrestore(&dataex.resp_list_lock,flags);
+}
+
+static u8 tpm_xen_status(struct tpm_chip *chip)
+{
+	unsigned long flags;
+	u8 rc = 0;
+	spin_lock_irqsave(&dataex.resp_list_lock, flags);
+	/*
+	 * Data are available if:
+	 *  - there's a current response
+	 *  - the last packet was queued only (this is fake, but necessary to
+	 *      get the generic TPM layer to call the receive function.)
+	 */
+	if (NULL != dataex.current_response ||
+	    0 != (dataex.flags & DATAEX_FLAG_QUEUED_ONLY)) {
+		rc = STATUS_DATA_AVAIL;
+	}
+	spin_unlock_irqrestore(&dataex.resp_list_lock, flags);
+	return rc;
+}
+
+static struct file_operations tpm_xen_ops = {
+	.owner = THIS_MODULE,
+	.llseek = no_llseek,
+	.open = tpm_open,
+	.read = tpm_read,
+	.write = tpm_write,
+	.release = tpm_release,
+};
+
+static DEVICE_ATTR(pubek, S_IRUGO, tpm_show_pubek, NULL);
+static DEVICE_ATTR(pcrs, S_IRUGO, tpm_show_pcrs, NULL);
+static DEVICE_ATTR(caps, S_IRUGO, tpm_show_caps, NULL);
+static DEVICE_ATTR(cancel, S_IWUSR |S_IWGRP, NULL, tpm_store_cancel);
+
+static struct attribute* xen_attrs[] = {
+	&dev_attr_pubek.attr,
+	&dev_attr_pcrs.attr,
+	&dev_attr_caps.attr,
+	&dev_attr_cancel.attr,
+	0,
+};
+
+static struct attribute_group xen_attr_grp = { .attrs = xen_attrs };
+
+static struct tpm_vendor_specific tpm_xen = {
+	.recv = tpm_xen_recv,
+	.send = tpm_xen_send,
+	.cancel = tpm_xen_cancel,
+	.status = tpm_xen_status,
+	.req_complete_mask = STATUS_BUSY | STATUS_DATA_AVAIL,
+	.req_complete_val  = STATUS_DATA_AVAIL,
+	.req_canceled = STATUS_READY,
+	.base = 0,
+	.attr_group = &xen_attr_grp,
+	.miscdev.fops = &tpm_xen_ops,
+	.buffersize = 64 * 1024,
+};
+
+static struct device tpm_device = {
+	.bus_id = "vtpm",
+};
+
+static struct tpmfe_device tpmfe = {
+	.receive = tpm_recv,
+	.status  = tpm_fe_status,
+};
+
+
+static int __init init_xen(void)
+{
+	int rc;
+
+	/*
+	 * Register device with the low lever front-end
+	 * driver
+	 */
+	if ((rc = tpm_fe_register_receiver(&tpmfe)) < 0) {
+		return rc;
+	}
+
+	/*
+	 * Register our device with the system.
+	 */
+	if ((rc = device_register(&tpm_device)) < 0) {
+		tpm_fe_unregister_receiver();
+		return rc;
+	}
+
+	tpm_xen.buffersize = tpmfe.max_tx_size;
+
+	if ((rc = tpm_register_hardware(&tpm_device, &tpm_xen)) < 0) {
+		device_unregister(&tpm_device);
+		tpm_fe_unregister_receiver();
+		return rc;
+	}
+
+	dataex.current_request = NULL;
+	spin_lock_init(&dataex.req_list_lock);
+	init_waitqueue_head(&dataex.req_wait_queue);
+	INIT_LIST_HEAD(&dataex.queued_requests);
+
+	dataex.current_response = NULL;
+	spin_lock_init(&dataex.resp_list_lock);
+	init_waitqueue_head(&dataex.resp_wait_queue);
+
+	disconnect_time = jiffies;
+
+	return 0;
+}
+
+static void __exit cleanup_xen(void)
+{
+	tpm_remove_hardware(&tpm_device);
+	device_unregister(&tpm_device);
+	tpm_fe_unregister_receiver();
+}
+
+fs_initcall(init_xen);
+module_exit(cleanup_xen);
+
+MODULE_AUTHOR("Stefan Berger (stefanb@us.ibm.com)");
+MODULE_DESCRIPTION("TPM Driver for XEN (shared memory)");
+MODULE_VERSION("1.0");
+MODULE_LICENSE("GPL");
--- vanilla-2.6.14/drivers/char/tty_io.c	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/drivers/char/tty_io.c	2005-11-08 15:06:49.000000000 +0100
@@ -132,6 +132,8 @@ LIST_HEAD(tty_drivers);			/* linked list
    vt.c for deeply disgusting hack reasons */
 DECLARE_MUTEX(tty_sem);
 
+int console_use_vt = 1;
+
 #ifdef CONFIG_UNIX98_PTYS
 extern struct tty_driver *ptm_driver;	/* Unix98 pty masters; for /dev/ptmx */
 extern int pty_limit;		/* Config limit on Unix98 ptys */
@@ -1828,7 +1830,7 @@ retry_open:
 		goto got_driver;
 	}
 #ifdef CONFIG_VT
-	if (device == MKDEV(TTY_MAJOR,0)) {
+	if (console_use_vt && (device == MKDEV(TTY_MAJOR,0))) {
 		extern struct tty_driver *console_driver;
 		driver = console_driver;
 		index = fg_console;
@@ -3002,6 +3004,8 @@ static int __init tty_init(void)
 #endif
 
 #ifdef CONFIG_VT
+	if (!console_use_vt)
+		goto out_vt;
 	cdev_init(&vc0_cdev, &console_fops);
 	if (cdev_add(&vc0_cdev, MKDEV(TTY_MAJOR, 0), 1) ||
 	    register_chrdev_region(MKDEV(TTY_MAJOR, 0), 1, "/dev/vc/0") < 0)
@@ -3010,6 +3014,7 @@ static int __init tty_init(void)
 	class_device_create(tty_class, MKDEV(TTY_MAJOR, 0), NULL, "tty0");
 
 	vty_init();
+ out_vt:
 #endif
 	return 0;
 }
--- vanilla-2.6.14/drivers/serial/Kconfig	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/drivers/serial/Kconfig	2005-11-08 15:07:02.000000000 +0100
@@ -10,7 +10,7 @@ menu "Serial drivers"
 # The new 8250/16550 serial drivers
 config SERIAL_8250
 	tristate "8250/16550 and compatible serial support"
-	depends on (BROKEN || !(SPARC64 || SPARC32))
+	depends on (BROKEN || !(SPARC64 || SPARC32 || XEN_DISABLE_SERIAL))
 	select SERIAL_CORE
 	---help---
 	  This selects whether you want to include the driver for the standard
--- vanilla-2.6.14/fs/Kconfig	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/fs/Kconfig	2005-11-08 15:07:05.000000000 +0100
@@ -811,6 +811,7 @@ config TMPFS
 config HUGETLBFS
 	bool "HugeTLB file system support"
 	depends X86 || IA64 || PPC64 || SPARC64 || SUPERH || X86_64 || BROKEN
+	depends !XEN
 
 config HUGETLB_PAGE
 	def_bool HUGETLBFS
--- vanilla-2.6.14/include/asm-um/page.h	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/include/asm-um/page.h	2005-11-08 15:07:17.000000000 +0100
@@ -118,7 +118,7 @@ extern unsigned long uml_physmem;
 extern struct page *arch_validate(struct page *page, int mask, int order);
 #define HAVE_ARCH_VALIDATE
 
-extern void arch_free_page(struct page *page, int order);
+extern int arch_free_page(struct page *page, int order);
 #define HAVE_ARCH_FREE_PAGE
 
 #include <asm-generic/page.h>
--- vanilla-2.6.14/include/linux/gfp.h	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/include/linux/gfp.h	2005-11-08 15:07:18.000000000 +0100
@@ -81,7 +81,11 @@ struct vm_area_struct;
  */
 
 #ifndef HAVE_ARCH_FREE_PAGE
-static inline void arch_free_page(struct page *page, int order) { }
+/*
+ * If arch_free_page returns non-zero then the generic free_page code can
+ * immediately bail: the arch-specific function has done all the work.
+ */
+static inline int arch_free_page(struct page *page, int order) { return 0; }
 #endif
 
 extern struct page *
--- vanilla-2.6.14/include/linux/highmem.h	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/include/linux/highmem.h	2005-11-08 15:07:18.000000000 +0100
@@ -13,10 +13,16 @@
 
 /* declarations for linux/mm/highmem.c */
 unsigned int nr_free_highpages(void);
+#ifdef CONFIG_XEN
+void kmap_flush_unused(void);
+#endif
 
 #else /* CONFIG_HIGHMEM */
 
 static inline unsigned int nr_free_highpages(void) { return 0; }
+#ifdef CONFIG_XEN
+static inline void kmap_flush_unused(void) { }
+#endif
 
 static inline void *kmap(struct page *page)
 {
--- vanilla-2.6.14/include/linux/irq.h	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/include/linux/irq.h	2005-11-08 15:07:18.000000000 +0100
@@ -93,6 +93,9 @@ irq_descp (int irq)
 #include <asm/hw_irq.h> /* the arch dependent stuff */
 
 extern int setup_irq(unsigned int irq, struct irqaction * new);
+#ifdef CONFIG_XEN
+extern int teardown_irq(unsigned int irq, struct irqaction * old);
+#endif
 
 #ifdef CONFIG_GENERIC_HARDIRQS
 extern cpumask_t irq_affinity[NR_IRQS];
--- vanilla-2.6.14/include/linux/mm.h	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/include/linux/mm.h	2005-11-08 15:07:19.000000000 +0100
@@ -162,6 +162,9 @@ extern unsigned int kobjsize(const void 
 #define VM_HUGETLB	0x00400000	/* Huge TLB Page VM */
 #define VM_NONLINEAR	0x00800000	/* Is non-linear (remap_file_pages) */
 #define VM_MAPPED_COPY	0x01000000	/* T if mapped copy of data (nommu mmap) */
+#ifdef CONFIG_XEN
+#define VM_FOREIGN	0x02000000	/* Has pages belonging to another VM */
+#endif
 
 #ifndef VM_STACK_DEFAULT_FLAGS		/* arch can override this */
 #define VM_STACK_DEFAULT_FLAGS VM_DATA_DEFAULT_FLAGS
@@ -927,6 +930,13 @@ extern int check_user_page_readable(stru
 int remap_pfn_range(struct vm_area_struct *, unsigned long,
 		unsigned long, unsigned long, pgprot_t);
 
+#ifdef CONFIG_XEN
+typedef int (*pte_fn_t)(pte_t *pte, struct page *pte_page, unsigned long addr, 
+                        void *data);
+extern int generic_page_range(struct mm_struct *mm, unsigned long address, 
+                              unsigned long size, pte_fn_t fn, void *data);
+#endif
+
 #ifdef CONFIG_PROC_FS
 void __vm_stat_account(struct mm_struct *, unsigned long, struct file *, long);
 #else
--- vanilla-2.6.14/include/linux/skbuff.h	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/include/linux/skbuff.h	2005-11-09 10:13:25.000000000 +0100
@@ -189,6 +189,8 @@ enum {
  *	@local_df: allow local fragmentation
  *	@cloned: Head may be cloned (check refcnt to be sure)
  *	@nohdr: Payload reference only, must not modify header
+ *	@proto_csum_valid: Protocol csum validated since arriving at localhost
+ *	@proto_csum_blank: Protocol csum must be added before leaving localhost
  *	@pkt_type: Packet class
  *	@ip_summed: Driver fed us an IP checksum
  *	@priority: Packet queueing priority
@@ -261,7 +263,14 @@ struct sk_buff {
 				nohdr:1,
 				nfctinfo:3;
 	__u8			pkt_type:3,
+#ifndef CONFIG_XEN
 				fclone:2;
+#else
+				fclone:2,
+				proto_csum_valid:1,
+				proto_csum_blank:1;
+				/* 1 bit spare */
+#endif
 	__be16			protocol;
 
 	void			(*destructor)(struct sk_buff *skb);
@@ -317,7 +326,8 @@ static inline struct sk_buff *alloc_skb_
 
 extern struct sk_buff *alloc_skb_from_cache(kmem_cache_t *cp,
 					    unsigned int size,
-					    gfp_t priority);
+					    gfp_t priority,
+					    int fclone);
 extern void	       kfree_skbmem(struct sk_buff *skb);
 extern struct sk_buff *skb_clone(struct sk_buff *skb,
 				 gfp_t priority);
--- vanilla-2.6.14/kernel/irq/manage.c	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/kernel/irq/manage.c	2005-11-08 15:07:20.000000000 +0100
@@ -151,9 +151,14 @@ int can_request_irq(unsigned int irq, un
 	return !action;
 }
 
-/*
- * Internal function to register an irqaction - typically used to
- * allocate special interrupts that are part of the architecture.
+/**
+ *	setup_irq - register an irqaction structure
+ *	@irq: Interrupt to register
+ *	@irqaction: The irqaction structure to be registered
+ *
+ *	Normally called by request_irq, this function can be used
+ *	directly to allocate special interrupts that are part of the
+ *	architecture.
  */
 int setup_irq(unsigned int irq, struct irqaction * new)
 {
@@ -222,28 +227,30 @@ int setup_irq(unsigned int irq, struct i
 	return 0;
 }
 
-/**
- *	free_irq - free an interrupt
- *	@irq: Interrupt line to free
- *	@dev_id: Device identity to free
+/*
+ *	teardown_irq - unregister an irqaction
+ *	@irq: Interrupt line being freed
+ *	@old: Pointer to the irqaction that is to be unregistered
  *
- *	Remove an interrupt handler. The handler is removed and if the
- *	interrupt line is no longer in use by any driver it is disabled.
- *	On a shared IRQ the caller must ensure the interrupt is disabled
- *	on the card it drives before calling this function. The function
- *	does not return until any executing interrupts for this IRQ
- *	have completed.
+ *	This function is called by free_irq and does the actual
+ *	business of unregistering the handler. It exists as a 
+ *	seperate function to enable handlers to be unregistered 
+ *	for irqactions that have been allocated statically at 
+ *	boot time.
  *
  *	This function must not be called from interrupt context.
  */
-void free_irq(unsigned int irq, void *dev_id)
+#ifndef CONFIG_XEN
+static
+#endif
+int teardown_irq(unsigned int irq, struct irqaction * old)
 {
 	struct irq_desc *desc;
 	struct irqaction **p;
 	unsigned long flags;
 
 	if (irq >= NR_IRQS)
-		return;
+		return -ENOENT;
 
 	desc = irq_desc + irq;
 	spin_lock_irqsave(&desc->lock,flags);
@@ -255,7 +262,7 @@ void free_irq(unsigned int irq, void *de
 			struct irqaction **pp = p;
 
 			p = &action->next;
-			if (action->dev_id != dev_id)
+			if (action != old)
 				continue;
 
 			/* Found it - now remove it from the list of entries */
@@ -279,13 +286,52 @@ void free_irq(unsigned int irq, void *de
 
 			/* Make sure it's not being used on another CPU */
 			synchronize_irq(irq);
-			kfree(action);
-			return;
+			return 0;
 		}
-		printk(KERN_ERR "Trying to free free IRQ%d\n",irq);
+		printk(KERN_ERR "Trying to teardown free IRQ%d\n",irq);
+		spin_unlock_irqrestore(&desc->lock,flags);
+		return -ENOENT;
+	}
+}
+
+/**
+ *	free_irq - free an interrupt
+ *	@irq: Interrupt line to free
+ *	@dev_id: Device identity to free
+ *
+ *	Remove an interrupt handler. The handler is removed and if the
+ *	interrupt line is no longer in use by any driver it is disabled.
+ *	On a shared IRQ the caller must ensure the interrupt is disabled
+ *	on the card it drives before calling this function. The function
+ *	does not return until any executing interrupts for this IRQ
+ *	have completed.
+ *
+ *	This function must not be called from interrupt context.
+ */
+void free_irq(unsigned int irq, void *dev_id)
+{
+	struct irq_desc *desc;
+	struct irqaction *action;
+	unsigned long flags;
+
+	if (irq >= NR_IRQS)
+		return;
+
+	desc = irq_desc + irq;
+	spin_lock_irqsave(&desc->lock,flags);
+	for (action = desc->action; action != NULL; action = action->next) {
+		if (action->dev_id != dev_id)
+			continue;
+
 		spin_unlock_irqrestore(&desc->lock,flags);
+
+		if (teardown_irq(irq, action) == 0)
+			kfree(action);
 		return;
 	}
+	printk(KERN_ERR "Trying to free free IRQ%d\n",irq);
+	spin_unlock_irqrestore(&desc->lock,flags);
+	return;
 }
 
 EXPORT_SYMBOL(free_irq);
--- vanilla-2.6.14/lib/Kconfig.debug	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/lib/Kconfig.debug	2005-11-08 15:07:20.000000000 +0100
@@ -137,7 +137,7 @@ config DEBUG_BUGVERBOSE
 
 config DEBUG_INFO
 	bool "Compile the kernel with debug info"
-	depends on DEBUG_KERNEL
+	depends on DEBUG_KERNEL && !X86_64_XEN
 	help
           If you say Y here the resulting kernel image will include
 	  debugging info resulting in a larger kernel image.
--- vanilla-2.6.14/mm/highmem.c	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/mm/highmem.c	2005-11-08 15:07:21.000000000 +0100
@@ -148,6 +148,17 @@ start:
 	return vaddr;
 }
 
+#ifdef CONFIG_XEN
+void kmap_flush_unused(void)
+{
+	spin_lock(&kmap_lock);
+	flush_all_zero_pkmaps();
+	spin_unlock(&kmap_lock);
+}
+
+EXPORT_SYMBOL(kmap_flush_unused);
+#endif
+
 void fastcall *kmap_high(struct page *page)
 {
 	unsigned long vaddr;
--- vanilla-2.6.14/mm/memory.c	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/mm/memory.c	2005-11-08 15:07:21.000000000 +0100
@@ -945,6 +945,23 @@ int get_user_pages(struct task_struct *t
 			continue;
 		}
 
+#ifdef CONFIG_XEN
+                if (vma && (vma->vm_flags & VM_FOREIGN)) {
+			struct page **map = vma->vm_private_data;
+			int offset = (start - vma->vm_start) >> PAGE_SHIFT;
+
+			if (map[offset] != NULL) {
+				if (pages)
+					pages[i] = map[offset];
+				if (vmas)
+					vmas[i] = vma;
+				i++;
+				start += PAGE_SIZE;
+				len--;
+				continue;
+			}
+                }
+#endif
 		if (!vma || (vma->vm_flags & VM_IO)
 				|| !(flags & vma->vm_flags))
 			return i ? : -EFAULT;
@@ -1195,6 +1212,106 @@ int remap_pfn_range(struct vm_area_struc
 }
 EXPORT_SYMBOL(remap_pfn_range);
 
+#ifdef CONFIG_XEN
+static inline int generic_pte_range(struct mm_struct *mm,
+                                    pmd_t *pmd,
+                                    unsigned long addr,
+                                    unsigned long end,
+                                    pte_fn_t fn, void *data)
+{
+	pte_t *pte;
+        int err;
+        struct page *pte_page;
+
+        pte = (mm == &init_mm) ?
+                pte_alloc_kernel(mm, pmd, addr) :
+                pte_alloc_map(mm, pmd, addr);
+        if (!pte)
+                return -ENOMEM;
+
+        pte_page = pmd_page(*pmd);
+
+        do {
+                err = fn(pte, pte_page, addr, data);
+		if (err)
+                        break;
+        } while (pte++, addr += PAGE_SIZE, addr != end);
+
+        if (mm != &init_mm)
+                pte_unmap(pte-1);
+        return err;
+
+}
+
+static inline int generic_pmd_range(struct mm_struct *mm,
+                                    pud_t *pud,
+                                    unsigned long addr,
+                                    unsigned long end,
+                                    pte_fn_t fn, void *data)
+{
+	pmd_t *pmd;
+	unsigned long next;
+        int err;
+
+	pmd = pmd_alloc(mm, pud, addr);
+	if (!pmd)
+		return -ENOMEM;
+	do {
+		next = pmd_addr_end(addr, end);
+                err = generic_pte_range(mm, pmd, addr, next, fn, data);
+                if (err)
+                    break;
+	} while (pmd++, addr = next, addr != end);
+	return err;
+}
+
+static inline int generic_pud_range(struct mm_struct *mm, pgd_t *pgd,
+                                    unsigned long addr,
+                                    unsigned long end,
+                                    pte_fn_t fn, void *data)
+{
+	pud_t *pud;
+	unsigned long next;
+        int err;
+
+	pud = pud_alloc(mm, pgd, addr);
+	if (!pud)
+		return -ENOMEM;
+	do {
+		next = pud_addr_end(addr, end);
+		err = generic_pmd_range(mm, pud, addr, next, fn, data);
+                if (err)
+			break;
+	} while (pud++, addr = next, addr != end);
+	return err;
+}
+
+/*
+ * Scan a region of virtual memory, filling in page tables as necessary
+ * and calling a provided function on each leaf page table.
+ */
+int generic_page_range(struct mm_struct *mm, unsigned long addr,
+                  unsigned long size, pte_fn_t fn, void *data)
+{
+	pgd_t *pgd;
+	unsigned long next;
+	unsigned long end = addr + size;
+	int err;
+
+	BUG_ON(addr >= end);
+	pgd = pgd_offset(mm, addr);
+	spin_lock(&mm->page_table_lock);
+	do {
+		next = pgd_addr_end(addr, end);
+		err = generic_pud_range(mm, pgd, addr, next, fn, data);
+		if (err)
+			break;
+	} while (pgd++, addr = next, addr != end);
+	spin_unlock(&mm->page_table_lock);
+	return err;
+}
+#endif
+
 /*
  * Do pte_mkwrite, but only if the vma says VM_WRITE.  We do this when
  * servicing faults for write access.  In the normal case, do always want
@@ -1250,8 +1367,18 @@ static int do_wp_page(struct mm_struct *
 	unsigned long pfn = pte_pfn(pte);
 	pte_t entry;
 	int ret;
+#ifdef CONFIG_XEN
+	struct page invalid_page;
+#endif
 
 	if (unlikely(!pfn_valid(pfn))) {
+#ifdef CONFIG_XEN
+		/* This can happen with /dev/mem (PROT_WRITE, MAP_PRIVATE). */
+		invalid_page.flags = (1<<PG_reserved) | (1<<PG_locked);
+		old_page = &invalid_page;
+	} else {
+		old_page = pfn_to_page(pfn);
+#else
 		/*
 		 * This should really halt the system so it can be debugged or
 		 * at least the kernel stops what it's doing before it corrupts
@@ -1262,8 +1389,11 @@ static int do_wp_page(struct mm_struct *
 				address);
 		spin_unlock(&mm->page_table_lock);
 		return VM_FAULT_OOM;
+#endif
 	}
+#ifndef CONFIG_XEN
 	old_page = pfn_to_page(pfn);
+#endif
 
 	if (PageAnon(old_page) && !TestSetPageLocked(old_page)) {
 		int reuse = can_share_swap_page(old_page);
@@ -1299,7 +1429,17 @@ static int do_wp_page(struct mm_struct *
 		new_page = alloc_page_vma(GFP_HIGHUSER, vma, address);
 		if (!new_page)
 			goto no_new_page;
+#ifndef CONFIG_XEN
 		copy_user_highpage(new_page, old_page, address);
+#else
+		if (old_page == &invalid_page) {
+			char *vto = kmap_atomic(new_page, KM_USER1);
+			copy_page(vto, (void *)(address & PAGE_MASK));
+			kunmap_atomic(vto, KM_USER1);
+		} else {
+			copy_user_highpage(new_page, old_page, address);
+		}
+#endif
 	}
 	/*
 	 * Re-check the pte - we dropped the lock
--- vanilla-2.6.14/mm/mmap.c	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/mm/mmap.c	2005-11-08 15:07:21.000000000 +0100
@@ -1932,6 +1932,10 @@ void exit_mmap(struct mm_struct *mm)
 	unsigned long nr_accounted = 0;
 	unsigned long end;
 
+#ifdef arch_exit_mmap
+	arch_exit_mmap(mm);
+#endif
+
 	lru_add_drain();
 
 	spin_lock(&mm->page_table_lock);
--- vanilla-2.6.14/mm/page_alloc.c	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/mm/page_alloc.c	2005-11-08 15:07:21.000000000 +0100
@@ -371,7 +371,8 @@ void __free_pages_ok(struct page *page, 
 	LIST_HEAD(list);
 	int i;
 
-	arch_free_page(page, order);
+	if (arch_free_page(page, order))
+		return;
 
 	mod_page_state(pgfree, 1 << order);
 
@@ -644,7 +645,8 @@ static void fastcall free_hot_cold_page(
 	struct per_cpu_pages *pcp;
 	unsigned long flags;
 
-	arch_free_page(page, 0);
+	if (arch_free_page(page, 0))
+		return;
 
 	kernel_map_pages(page, 1, 0);
 	inc_page_state(pgfree);
--- vanilla-2.6.14/net/core/dev.c	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/net/core/dev.c	2005-11-08 15:07:21.000000000 +0100
@@ -115,6 +115,12 @@
 #endif	/* CONFIG_NET_RADIO */
 #include <asm/current.h>
 
+#ifdef CONFIG_XEN
+#include <net/ip.h>
+#include <linux/tcp.h>
+#include <linux/udp.h>
+#endif
+
 /*
  *	The list of packet types we will receive (as opposed to discard)
  *	and the routines to invoke.
@@ -1249,6 +1255,32 @@ int dev_queue_xmit(struct sk_buff *skb)
 	    __skb_linearize(skb, GFP_ATOMIC))
 		goto out_kfree_skb;
 
+#ifdef CONFIG_XEN
+	/* If a checksum-deferred packet is forwarded to a device that needs a
+	 * checksum, correct the pointers and force checksumming.
+	 */
+	if (skb->proto_csum_blank) {
+		if (skb->protocol != htons(ETH_P_IP))
+			goto out_kfree_skb;
+		skb->h.raw = (unsigned char *)skb->nh.iph + 4*skb->nh.iph->ihl;
+		if (skb->h.raw >= skb->tail)
+			goto out_kfree_skb;
+		switch (skb->nh.iph->protocol) {
+		case IPPROTO_TCP:
+			skb->csum = offsetof(struct tcphdr, check);
+			break;
+		case IPPROTO_UDP:
+			skb->csum = offsetof(struct udphdr, check);
+			break;
+		default:
+			goto out_kfree_skb;
+		}
+		if ((skb->h.raw + skb->csum + 2) > skb->tail)
+			goto out_kfree_skb;
+		skb->ip_summed = CHECKSUM_HW;
+	}
+#endif
+
 	/* If packet is not checksummed and device does not support
 	 * checksumming for this protocol, complete checksumming here.
 	 */
@@ -1598,6 +1630,19 @@ int netif_receive_skb(struct sk_buff *sk
 	}
 #endif
 
+#ifdef CONFIG_XEN
+	switch (skb->ip_summed) {
+	case CHECKSUM_UNNECESSARY:
+		skb->proto_csum_valid = 1;
+		break;
+	case CHECKSUM_HW:
+		/* XXX Implement me. */
+	default:
+		skb->proto_csum_valid = 0;
+		break;
+	}
+#endif
+
 	list_for_each_entry_rcu(ptype, &ptype_all, list) {
 		if (!ptype->dev || ptype->dev == skb->dev) {
 			if (pt_prev) 
--- vanilla-2.6.14/net/core/skbuff.c	2005-10-28 02:02:08.000000000 +0200
+++ xen-hg12171/net/core/skbuff.c	2005-11-09 10:13:25.000000000 +0100
@@ -130,6 +130,7 @@ void skb_under_panic(struct sk_buff *skb
  *	Buffers may only be allocated from interrupts using a @gfp_mask of
  *	%GFP_ATOMIC.
  */
+#ifndef CONFIG_HAVE_ARCH_ALLOC_SKB
 struct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,
 			    int fclone)
 {
@@ -177,10 +178,14 @@ struct sk_buff *__alloc_skb(unsigned int
 out:
 	return skb;
 nodata:
-	kmem_cache_free(skbuff_head_cache, skb);
+	if (fclone)
+		kmem_cache_free(skbuff_fclone_cache, skb);
+	else
+		kmem_cache_free(skbuff_head_cache, skb);
 	skb = NULL;
 	goto out;
 }
+#endif /* !CONFIG_HAVE_ARCH_ALLOC_SKB */
 
 /**
  *	alloc_skb_from_cache	-	allocate a network buffer
@@ -198,14 +203,20 @@ nodata:
  */
 struct sk_buff *alloc_skb_from_cache(kmem_cache_t *cp,
 				     unsigned int size,
-				     gfp_t gfp_mask)
+				     gfp_t gfp_mask,
+				     int fclone)
 {
 	struct sk_buff *skb;
 	u8 *data;
 
 	/* Get the HEAD */
-	skb = kmem_cache_alloc(skbuff_head_cache,
-			       gfp_mask & ~__GFP_DMA);
+	if (fclone)
+		skb = kmem_cache_alloc(skbuff_fclone_cache,
+				       gfp_mask & ~__GFP_DMA);
+	else
+		skb = kmem_cache_alloc(skbuff_head_cache,
+				       gfp_mask & ~__GFP_DMA);
+
 	if (!skb)
 		goto out;
 
@@ -222,7 +233,15 @@ struct sk_buff *alloc_skb_from_cache(kme
 	skb->data = data;
 	skb->tail = data;
 	skb->end  = data + size;
+	if (fclone) {
+		struct sk_buff *child = skb + 1;
+		atomic_t *fclone_ref = (atomic_t *) (child + 1);
+
+		skb->fclone = SKB_FCLONE_ORIG;
+		atomic_set(fclone_ref, 1);
 
+		child->fclone = SKB_FCLONE_UNAVAILABLE;
+	}
 	atomic_set(&(skb_shinfo(skb)->dataref), 1);
 	skb_shinfo(skb)->nr_frags  = 0;
 	skb_shinfo(skb)->tso_size = 0;
@@ -231,7 +250,10 @@ struct sk_buff *alloc_skb_from_cache(kme
 out:
 	return skb;
 nodata:
-	kmem_cache_free(skbuff_head_cache, skb);
+	if (fclone)
+		kmem_cache_free(skbuff_fclone_cache, skb);
+	else
+		kmem_cache_free(skbuff_head_cache, skb);
 	skb = NULL;
 	goto out;
 }
@@ -400,6 +422,10 @@ struct sk_buff *skb_clone(struct sk_buff
 	C(local_df);
 	n->cloned = 1;
 	n->nohdr = 0;
+#ifdef CONFIG_XEN
+	C(proto_csum_valid);
+	C(proto_csum_blank);
+#endif
 	C(pkt_type);
 	C(ip_summed);
 	C(priority);
