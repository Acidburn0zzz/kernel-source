Subject: xen3 common
From: kraxel@suse.de

autogenerated from http://xenbits.xensource.com/ext/linux-2.6-merge.hg (tip 15332)

Index: head-2006-01-16/drivers/acpi/tables.c
===================================================================
--- head-2006-01-16.orig/drivers/acpi/tables.c	2006-01-16 15:39:50.789465080 +0100
+++ head-2006-01-16/drivers/acpi/tables.c	2006-01-16 15:44:39.512572496 +0100
@@ -572,6 +572,12 @@ static int __init acpi_table_get_sdt(str
  * 
  * result: sdt_entry[] is initialized
  */
+#if defined(CONFIG_X86_XEN) || defined(CONFIG_X86_64_XEN)
+#define acpi_rsdp_phys_to_va(rsdp_phys) (__fix_to_virt(FIX_ACPI_RSDP_PAGE) + \
+					   (rsdp_phys & ~PAGE_MASK))
+#else
+#define acpi_rsdp_phys_to_va(rsdp_phys) __va(rsdp_phys)
+#endif
 
 int __init acpi_table_init(void)
 {
@@ -587,7 +593,7 @@ int __init acpi_table_init(void)
 		return -ENODEV;
 	}
 
-	rsdp = (struct acpi_table_rsdp *)__va(rsdp_phys);
+	rsdp = (struct acpi_table_rsdp *)acpi_rsdp_phys_to_va(rsdp_phys);
 	if (!rsdp) {
 		printk(KERN_WARNING PREFIX "Unable to map RSDP\n");
 		return -ENODEV;
Index: head-2006-01-16/drivers/char/mem.c
===================================================================
--- head-2006-01-16.orig/drivers/char/mem.c	2006-01-16 15:39:51.261393336 +0100
+++ head-2006-01-16/drivers/char/mem.c	2006-01-16 15:44:39.515572040 +0100
@@ -108,6 +108,7 @@ static inline int valid_mmap_phys_addr_r
 }
 #endif
 
+#ifndef ARCH_HAS_DEV_MEM
 /*
  * This funcion reads the *physical* memory. The f_pos points directly to the 
  * memory location. 
@@ -232,6 +233,7 @@ static ssize_t write_mem(struct file * f
 	*ppos += written;
 	return written;
 }
+#endif
 
 #ifndef __HAVE_PHYS_MEM_ACCESS_PROT
 static pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
@@ -773,6 +775,7 @@ static int open_port(struct inode * inod
 #define open_kmem	open_mem
 #define open_oldmem	open_mem
 
+#ifndef ARCH_HAS_DEV_MEM
 static struct file_operations mem_fops = {
 	.llseek		= memory_lseek,
 	.read		= read_mem,
@@ -780,6 +783,9 @@ static struct file_operations mem_fops =
 	.mmap		= mmap_mem,
 	.open		= open_mem,
 };
+#else
+extern struct file_operations mem_fops;
+#endif
 
 static struct file_operations kmem_fops = {
 	.llseek		= memory_lseek,
Index: head-2006-01-16/include/asm-um/page.h
===================================================================
--- head-2006-01-16.orig/include/asm-um/page.h	2006-01-03 04:21:10.000000000 +0100
+++ head-2006-01-16/include/asm-um/page.h	2006-01-16 15:44:39.517571736 +0100
@@ -118,7 +118,7 @@ extern unsigned long uml_physmem;
 extern struct page *arch_validate(struct page *page, gfp_t mask, int order);
 #define HAVE_ARCH_VALIDATE
 
-extern void arch_free_page(struct page *page, int order);
+extern int arch_free_page(struct page *page, int order);
 #define HAVE_ARCH_FREE_PAGE
 
 #include <asm-generic/page.h>
Index: head-2006-01-16/include/linux/gfp.h
===================================================================
--- head-2006-01-16.orig/include/linux/gfp.h	2006-01-16 15:40:07.270959512 +0100
+++ head-2006-01-16/include/linux/gfp.h	2006-01-16 15:44:39.519571432 +0100
@@ -98,7 +98,11 @@ static inline int gfp_zone(gfp_t gfp)
  */
 
 #ifndef HAVE_ARCH_FREE_PAGE
-static inline void arch_free_page(struct page *page, int order) { }
+/*
+ * If arch_free_page returns non-zero then the generic free_page code can
+ * immediately bail: the arch-specific function has done all the work.
+ */
+static inline int arch_free_page(struct page *page, int order) { return 0; }
 #endif
 
 extern struct page *
Index: head-2006-01-16/include/linux/highmem.h
===================================================================
--- head-2006-01-16.orig/include/linux/highmem.h	2006-01-03 04:21:10.000000000 +0100
+++ head-2006-01-16/include/linux/highmem.h	2006-01-16 15:44:39.521571128 +0100
@@ -13,10 +13,16 @@
 
 /* declarations for linux/mm/highmem.c */
 unsigned int nr_free_highpages(void);
+#ifdef CONFIG_XEN
+void kmap_flush_unused(void);
+#endif
 
 #else /* CONFIG_HIGHMEM */
 
 static inline unsigned int nr_free_highpages(void) { return 0; }
+#ifdef CONFIG_XEN
+static inline void kmap_flush_unused(void) { }
+#endif
 
 static inline void *kmap(struct page *page)
 {
Index: head-2006-01-16/include/linux/irq.h
===================================================================
--- head-2006-01-16.orig/include/linux/irq.h	2006-01-16 15:40:07.307953888 +0100
+++ head-2006-01-16/include/linux/irq.h	2006-01-16 15:44:39.522570976 +0100
@@ -94,6 +94,9 @@ irq_descp (int irq)
 #include <asm/hw_irq.h> /* the arch dependent stuff */
 
 extern int setup_irq(unsigned int irq, struct irqaction * new);
+#ifdef CONFIG_XEN
+extern int teardown_irq(unsigned int irq, struct irqaction * old);
+#endif
 
 #ifdef CONFIG_GENERIC_HARDIRQS
 extern cpumask_t irq_affinity[NR_IRQS];
Index: head-2006-01-16/include/linux/mm.h
===================================================================
--- head-2006-01-16.orig/include/linux/mm.h	2006-01-16 15:40:25.470192808 +0100
+++ head-2006-01-16/include/linux/mm.h	2006-01-16 15:44:39.526570368 +0100
@@ -166,6 +166,9 @@ extern unsigned int kobjsize(const void 
 #define VM_NONLINEAR	0x00800000	/* Is non-linear (remap_file_pages) */
 #define VM_MAPPED_COPY	0x01000000	/* T if mapped copy of data (nommu mmap) */
 #define VM_INSERTPAGE	0x02000000	/* The vma has had "vm_insert_page()" done on it */
+#ifdef CONFIG_XEN
+#define VM_FOREIGN	0x04000000	/* Has pages belonging to another VM */
+#endif
 
 #ifndef VM_STACK_DEFAULT_FLAGS		/* arch can override this */
 #define VM_STACK_DEFAULT_FLAGS VM_DATA_DEFAULT_FLAGS
@@ -1015,6 +1018,13 @@ struct page *follow_page(struct vm_area_
 #define FOLL_GET	0x04	/* do get_page on page */
 #define FOLL_ANON	0x08	/* give ZERO_PAGE if no pgtable */
 
+#ifdef CONFIG_XEN
+typedef int (*pte_fn_t)(pte_t *pte, struct page *pte_page, unsigned long addr,
+                        void *data);
+extern int generic_page_range(struct mm_struct *mm, unsigned long address,
+                              unsigned long size, pte_fn_t fn, void *data);
+#endif
+
 #ifdef CONFIG_PROC_FS
 void vm_stat_account(struct mm_struct *, unsigned long, struct file *, long);
 #else
Index: head-2006-01-16/include/linux/skbuff.h
===================================================================
--- head-2006-01-16.orig/include/linux/skbuff.h	2006-01-16 15:40:25.597173504 +0100
+++ head-2006-01-16/include/linux/skbuff.h	2006-01-16 15:49:04.959218472 +0100
@@ -190,6 +190,8 @@ enum {
  *	@local_df: allow local fragmentation
  *	@cloned: Head may be cloned (check refcnt to be sure)
  *	@nohdr: Payload reference only, must not modify header
+ *	@proto_csum_valid: Protocol csum validated since arriving at localhost
+ *	@proto_csum_blank: Protocol csum must be added before leaving localhost
  *	@pkt_type: Packet class
  *	@fclone: skbuff clone status
  *	@ip_summed: Driver fed us an IP checksum
@@ -287,6 +289,10 @@ struct sk_buff {
 	__u16			tc_verd;	/* traffic control verdict */
 #endif
 #endif
+#ifdef CONFIG_XEN
+	__u8			proto_csum_valid:1,
+				proto_csum_blank:1;
+#endif
 #ifdef CONFIG_NET_DMA
 	dma_cookie_t		dma_cookie;
 #endif
@@ -326,7 +332,8 @@ static inline struct sk_buff *alloc_skb_
 
 extern struct sk_buff *alloc_skb_from_cache(kmem_cache_t *cp,
 					    unsigned int size,
-					    gfp_t priority);
+					    gfp_t priority,
+					    int fclone);
 extern void	       kfree_skbmem(struct sk_buff *skb);
 extern struct sk_buff *skb_clone(struct sk_buff *skb,
 				 gfp_t priority);
@@ -1056,7 +1063,7 @@ static inline struct sk_buff *__dev_allo
 	return skb;
 }
 #else
-extern struct sk_buff *__dev_alloc_skb(unsigned int length, int gfp_mask);
+extern struct sk_buff *__dev_alloc_skb(unsigned int length, gfp_t gfp_mask);
 #endif
 
 /**
Index: head-2006-01-16/kernel/irq/manage.c
===================================================================
--- head-2006-01-16.orig/kernel/irq/manage.c	2006-01-16 15:40:08.835721632 +0100
+++ head-2006-01-16/kernel/irq/manage.c	2006-01-16 15:44:39.534569152 +0100
@@ -164,9 +164,14 @@ int can_request_irq(unsigned int irq, un
 	return !action;
 }
 
-/*
- * Internal function to register an irqaction - typically used to
- * allocate special interrupts that are part of the architecture.
+/**
+ *	setup_irq - register an irqaction structure
+ *	@irq: Interrupt to register
+ *	@irqaction: The irqaction structure to be registered
+ *
+ *	Normally called by request_irq, this function can be used
+ *	directly to allocate special interrupts that are part of the
+ *	architecture.
  */
 int setup_irq(unsigned int irq, struct irqaction * new)
 {
@@ -238,28 +243,30 @@ int setup_irq(unsigned int irq, struct i
 	return 0;
 }
 
-/**
- *	free_irq - free an interrupt
- *	@irq: Interrupt line to free
- *	@dev_id: Device identity to free
+/*
+ *	teardown_irq - unregister an irqaction
+ *	@irq: Interrupt line being freed
+ *	@old: Pointer to the irqaction that is to be unregistered
  *
- *	Remove an interrupt handler. The handler is removed and if the
- *	interrupt line is no longer in use by any driver it is disabled.
- *	On a shared IRQ the caller must ensure the interrupt is disabled
- *	on the card it drives before calling this function. The function
- *	does not return until any executing interrupts for this IRQ
- *	have completed.
+ *	This function is called by free_irq and does the actual
+ *	business of unregistering the handler. It exists as a
+ *	seperate function to enable handlers to be unregistered
+ *	for irqactions that have been allocated statically at
+ *	boot time.
  *
  *	This function must not be called from interrupt context.
  */
-void free_irq(unsigned int irq, void *dev_id)
+#ifndef CONFIG_XEN
+static
+#endif
+int teardown_irq(unsigned int irq, struct irqaction * old)
 {
 	struct irq_desc *desc;
 	struct irqaction **p;
 	unsigned long flags;
 
 	if (irq >= NR_IRQS)
-		return;
+		return -ENOENT;
 
 	desc = irq_desc + irq;
 	spin_lock_irqsave(&desc->lock,flags);
@@ -271,7 +278,7 @@ void free_irq(unsigned int irq, void *de
 			struct irqaction **pp = p;
 
 			p = &action->next;
-			if (action->dev_id != dev_id)
+			if (action != old)
 				continue;
 
 			/* Found it - now remove it from the list of entries */
@@ -295,13 +302,52 @@ void free_irq(unsigned int irq, void *de
 
 			/* Make sure it's not being used on another CPU */
 			synchronize_irq(irq);
-			kfree(action);
-			return;
+			return 0;
 		}
-		printk(KERN_ERR "Trying to free free IRQ%d\n",irq);
+		printk(KERN_ERR "Trying to teardown free IRQ%d\n",irq);
+		spin_unlock_irqrestore(&desc->lock,flags);
+		return -ENOENT;
+	}
+}
+
+/**
+ *	free_irq - free an interrupt
+ *	@irq: Interrupt line to free
+ *	@dev_id: Device identity to free
+ *
+ *	Remove an interrupt handler. The handler is removed and if the
+ *	interrupt line is no longer in use by any driver it is disabled.
+ *	On a shared IRQ the caller must ensure the interrupt is disabled
+ *	on the card it drives before calling this function. The function
+ *	does not return until any executing interrupts for this IRQ
+ *	have completed.
+ *
+ *	This function must not be called from interrupt context.
+ */
+void free_irq(unsigned int irq, void *dev_id)
+{
+	struct irq_desc *desc;
+	struct irqaction *action;
+	unsigned long flags;
+
+	if (irq >= NR_IRQS)
+		return;
+
+	desc = irq_desc + irq;
+	spin_lock_irqsave(&desc->lock,flags);
+	for (action = desc->action; action != NULL; action = action->next) {
+		if (action->dev_id != dev_id)
+			continue;
+
 		spin_unlock_irqrestore(&desc->lock,flags);
+
+		if (teardown_irq(irq, action) == 0)
+			kfree(action);
 		return;
 	}
+	printk(KERN_ERR "Trying to free free IRQ%d\n",irq);
+	spin_unlock_irqrestore(&desc->lock,flags);
+	return;
 }
 
 EXPORT_SYMBOL(free_irq);
Index: head-2006-01-16/lib/Makefile
===================================================================
--- head-2006-01-16.orig/lib/Makefile	2006-01-03 04:21:10.000000000 +0100
+++ head-2006-01-16/lib/Makefile	2006-01-16 15:44:39.536568848 +0100
@@ -44,7 +44,9 @@ obj-$(CONFIG_TEXTSEARCH_KMP) += ts_kmp.o
 obj-$(CONFIG_TEXTSEARCH_BM) += ts_bm.o
 obj-$(CONFIG_TEXTSEARCH_FSM) += ts_fsm.o
 
+ifneq ($(CONFIG_XEN),y)
 obj-$(CONFIG_SWIOTLB) += swiotlb.o
+endif
 
 hostprogs-y	:= gen_crc32table
 clean-files	:= crc32table.h
Index: head-2006-01-16/mm/highmem.c
===================================================================
--- head-2006-01-16.orig/mm/highmem.c	2006-01-03 04:21:10.000000000 +0100
+++ head-2006-01-16/mm/highmem.c	2006-01-16 15:44:39.539568392 +0100
@@ -152,6 +152,17 @@ start:
 	return vaddr;
 }
 
+#ifdef CONFIG_XEN
+void kmap_flush_unused(void)
+{
+	spin_lock(&kmap_lock);
+	flush_all_zero_pkmaps();
+	spin_unlock(&kmap_lock);
+}
+
+EXPORT_SYMBOL(kmap_flush_unused);
+#endif
+
 void fastcall *kmap_high(struct page *page)
 {
 	unsigned long vaddr;
Index: head-2006-01-16/mm/memory.c
===================================================================
--- head-2006-01-16.orig/mm/memory.c	2006-01-16 15:40:30.767387512 +0100
+++ head-2006-01-16/mm/memory.c	2006-01-16 15:44:39.545567480 +0100
@@ -395,7 +395,8 @@ struct page *vm_normal_page(struct vm_ar
 	 * Remove this test eventually!
 	 */
 	if (unlikely(!pfn_valid(pfn))) {
-		print_bad_pte(vma, pte, addr);
+		if (!vma->vm_flags & VM_RESERVED)
+			print_bad_pte(vma, pte, addr);
 		return NULL;
 	}
 
@@ -1010,6 +1011,23 @@ int get_user_pages(struct task_struct *t
 			continue;
 		}
 
+#ifdef CONFIG_XEN
+                if (vma && (vma->vm_flags & VM_FOREIGN)) {
+			struct page **map = vma->vm_private_data;
+			int offset = (start - vma->vm_start) >> PAGE_SHIFT;
+
+			if (map[offset] != NULL) {
+				if (pages)
+					pages[i] = map[offset];
+				if (vmas)
+					vmas[i] = vma;
+				i++;
+				start += PAGE_SIZE;
+				len--;
+				continue;
+			}
+                }
+#endif
 		if (!vma || (vma->vm_flags & (VM_IO | VM_PFNMAP))
 				|| !(vm_flags & vma->vm_flags))
 			return i ? : -EFAULT;
@@ -1349,6 +1367,104 @@ int remap_pfn_range(struct vm_area_struc
 }
 EXPORT_SYMBOL(remap_pfn_range);
 
+#ifdef CONFIG_XEN
+static inline int generic_pte_range(struct mm_struct *mm,
+                                    pmd_t *pmd,
+                                    unsigned long addr,
+                                    unsigned long end,
+                                    pte_fn_t fn, void *data)
+{
+	pte_t *pte;
+        int err;
+        struct page *pte_page;
+
+        pte = (mm == &init_mm) ?
+                pte_alloc_kernel(pmd, addr) :
+                pte_alloc_map(mm, pmd, addr);
+        if (!pte)
+                return -ENOMEM;
+
+        pte_page = pmd_page(*pmd);
+
+        do {
+                err = fn(pte, pte_page, addr, data);
+		if (err)
+                        break;
+        } while (pte++, addr += PAGE_SIZE, addr != end);
+
+        if (mm != &init_mm)
+                pte_unmap(pte-1);
+        return err;
+
+}
+
+static inline int generic_pmd_range(struct mm_struct *mm,
+                                    pud_t *pud,
+                                    unsigned long addr,
+                                    unsigned long end,
+                                    pte_fn_t fn, void *data)
+{
+	pmd_t *pmd;
+	unsigned long next;
+        int err;
+
+	pmd = pmd_alloc(mm, pud, addr);
+	if (!pmd)
+		return -ENOMEM;
+	do {
+		next = pmd_addr_end(addr, end);
+                err = generic_pte_range(mm, pmd, addr, next, fn, data);
+                if (err)
+                    break;
+	} while (pmd++, addr = next, addr != end);
+	return err;
+}
+
+static inline int generic_pud_range(struct mm_struct *mm, pgd_t *pgd,
+                                    unsigned long addr,
+                                    unsigned long end,
+                                    pte_fn_t fn, void *data)
+{
+	pud_t *pud;
+	unsigned long next;
+        int err;
+
+	pud = pud_alloc(mm, pgd, addr);
+	if (!pud)
+		return -ENOMEM;
+	do {
+		next = pud_addr_end(addr, end);
+		err = generic_pmd_range(mm, pud, addr, next, fn, data);
+                if (err)
+			break;
+	} while (pud++, addr = next, addr != end);
+	return err;
+}
+
+/*
+ * Scan a region of virtual memory, filling in page tables as necessary
+ * and calling a provided function on each leaf page table.
+ */
+int generic_page_range(struct mm_struct *mm, unsigned long addr,
+                  unsigned long size, pte_fn_t fn, void *data)
+{
+	pgd_t *pgd;
+	unsigned long next;
+	unsigned long end = addr + size;
+	int err;
+
+	BUG_ON(addr >= end);
+	pgd = pgd_offset(mm, addr);
+	do {
+		next = pgd_addr_end(addr, end);
+		err = generic_pud_range(mm, pgd, addr, next, fn, data);
+		if (err)
+			break;
+	} while (pgd++, addr = next, addr != end);
+	return err;
+}
+#endif
+
 /*
  * handle_pte_fault chooses page fault handler according to an entry
  * which was read non-atomically.  Before making any commitment, on
Index: head-2006-01-16/mm/mmap.c
===================================================================
--- head-2006-01-16.orig/mm/mmap.c	2006-01-16 15:40:09.220663112 +0100
+++ head-2006-01-16/mm/mmap.c	2006-01-16 15:44:39.549566872 +0100
@@ -1937,6 +1937,10 @@ void exit_mmap(struct mm_struct *mm)
 	unsigned long nr_accounted = 0;
 	unsigned long end;
 
+#ifdef arch_exit_mmap
+	arch_exit_mmap(mm);
+#endif
+
 	lru_add_drain();
 	flush_cache_mm(mm);
 	tlb = tlb_gather_mmu(mm, 1);
Index: head-2006-01-16/mm/page_alloc.c
===================================================================
--- head-2006-01-16.orig/mm/page_alloc.c	2006-01-16 15:40:09.310649432 +0100
+++ head-2006-01-16/mm/page_alloc.c	2006-01-16 15:44:39.555565960 +0100
@@ -414,7 +414,8 @@ static void __free_pages_ok(struct page 
 	int i;
 	int reserved = 0;
 
-	arch_free_page(page, order);
+	if (arch_free_page(page, order))
+		return;
 	if (!PageHighMem(page))
 		mutex_debug_check_no_locks_freed(page_address(page),
 						 PAGE_SIZE<<order);
@@ -459,7 +460,8 @@ void fastcall __init __free_pages_bootme
 			set_page_count(p, 0);
 		}
 
-		arch_free_page(page, order);
+		if (arch_free_page(page, order))
+			return;
 
 		mod_page_state(pgfree, 1 << order);
 
@@ -708,7 +710,8 @@ static void fastcall free_hot_cold_page(
 	struct per_cpu_pages *pcp;
 	unsigned long flags;
 
-	arch_free_page(page, 0);
+	if (arch_free_page(page, 0))
+		return;
 
 	if (PageAnon(page))
 		page->mapping = NULL;
Index: head-2006-01-16/net/core/dev.c
===================================================================
--- head-2006-01-16.orig/net/core/dev.c	2006-01-16 15:40:25.568177912 +0100
+++ head-2006-01-16/net/core/dev.c	2006-01-16 15:44:39.564564592 +0100
@@ -117,6 +117,12 @@
 #include <linux/dmaengine.h>
 #include <asm/current.h>
 
+#ifdef CONFIG_XEN
+#include <net/ip.h>
+#include <linux/tcp.h>
+#include <linux/udp.h>
+#endif
+
 /*
  *	The list of packet types we will receive (as opposed to discard)
  *	and the routines to invoke.
@@ -1266,6 +1272,37 @@ int dev_queue_xmit(struct sk_buff *skb)
 	    __skb_linearize(skb, GFP_ATOMIC))
 		goto out_kfree_skb;
 
+#ifdef CONFIG_XEN
+	/* If a checksum-deferred packet is forwarded to a device that needs a
+	 * checksum, correct the pointers and force checksumming.
+	 */
+	if (skb->proto_csum_blank) {
+		if (skb->protocol != htons(ETH_P_IP))
+			goto out_kfree_skb;
+		skb->h.raw = (unsigned char *)skb->nh.iph + 4*skb->nh.iph->ihl;
+		if (skb->h.raw >= skb->tail)
+			goto out_kfree_skb;
+		switch (skb->nh.iph->protocol) {
+		case IPPROTO_TCP:
+			skb->csum = offsetof(struct tcphdr, check);
+			break;
+		case IPPROTO_UDP:
+			skb->csum = offsetof(struct udphdr, check);
+			break;
+		default:
+			if (net_ratelimit())
+				printk(KERN_ERR "Attempting to checksum a non-"
+				       "TCP/UDP packet, dropping a protocol"
+				       " %d packet", skb->nh.iph->protocol);
+			rc = -EPROTO;
+			goto out_kfree_skb;
+		}
+		if ((skb->h.raw + skb->csum + 2) > skb->tail)
+			goto out_kfree_skb;
+		skb->ip_summed = CHECKSUM_HW;
+	}
+#endif
+
 	/* If packet is not checksummed and device does not support
 	 * checksumming for this protocol, complete checksumming here.
 	 */
@@ -1615,6 +1652,19 @@ int netif_receive_skb(struct sk_buff *sk
 	}
 #endif
 
+#ifdef CONFIG_XEN
+	switch (skb->ip_summed) {
+	case CHECKSUM_UNNECESSARY:
+		skb->proto_csum_valid = 1;
+		break;
+	case CHECKSUM_HW:
+		/* XXX Implement me. */
+	default:
+		skb->proto_csum_valid = 0;
+		break;
+	}
+#endif
+
 	list_for_each_entry_rcu(ptype, &ptype_all, list) {
 		if (!ptype->dev || ptype->dev == skb->dev) {
 			if (pt_prev) 
Index: head-2006-01-16/net/core/skbuff.c
===================================================================
--- head-2006-01-16.orig/net/core/skbuff.c	2006-01-16 15:40:25.610171528 +0100
+++ head-2006-01-16/net/core/skbuff.c	2006-01-16 15:50:44.983012544 +0100
@@ -132,6 +132,7 @@ void skb_under_panic(struct sk_buff *skb
  *	Buffers may only be allocated from interrupts using a @gfp_mask of
  *	%GFP_ATOMIC.
  */
+#ifndef CONFIG_HAVE_ARCH_ALLOC_SKB
 struct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,
 			    int fclone)
 {
@@ -180,10 +181,14 @@ struct sk_buff *__alloc_skb(unsigned int
 out:
 	return skb;
 nodata:
-	kmem_cache_free(skbuff_head_cache, skb);
+	if (fclone)
+		kmem_cache_free(skbuff_fclone_cache, skb);
+	else
+		kmem_cache_free(skbuff_head_cache, skb);
 	skb = NULL;
 	goto out;
 }
+#endif /* !CONFIG_HAVE_ARCH_ALLOC_SKB */
 
 /**
  *	alloc_skb_from_cache	-	allocate a network buffer
@@ -201,14 +206,20 @@ nodata:
  */
 struct sk_buff *alloc_skb_from_cache(kmem_cache_t *cp,
 				     unsigned int size,
-				     gfp_t gfp_mask)
+				     gfp_t gfp_mask,
+				     int fclone)
 {
 	struct sk_buff *skb;
 	u8 *data;
 
 	/* Get the HEAD */
-	skb = kmem_cache_alloc(skbuff_head_cache,
-			       gfp_mask & ~__GFP_DMA);
+	if (fclone)
+		skb = kmem_cache_alloc(skbuff_fclone_cache,
+				       gfp_mask & ~__GFP_DMA);
+	else
+		skb = kmem_cache_alloc(skbuff_head_cache,
+				       gfp_mask & ~__GFP_DMA);
+
 	if (!skb)
 		goto out;
 
@@ -225,7 +236,15 @@ struct sk_buff *alloc_skb_from_cache(kme
 	skb->data = data;
 	skb->tail = data;
 	skb->end  = data + size;
+	if (fclone) {
+		struct sk_buff *child = skb + 1;
+		atomic_t *fclone_ref = (atomic_t *) (child + 1);
 
+		skb->fclone = SKB_FCLONE_ORIG;
+		atomic_set(fclone_ref, 1);
+
+		child->fclone = SKB_FCLONE_UNAVAILABLE;
+	}
 	atomic_set(&(skb_shinfo(skb)->dataref), 1);
 	skb_shinfo(skb)->nr_frags  = 0;
 	skb_shinfo(skb)->tso_size = 0;
@@ -234,7 +253,10 @@ struct sk_buff *alloc_skb_from_cache(kme
 out:
 	return skb;
 nodata:
-	kmem_cache_free(skbuff_head_cache, skb);
+	if (fclone)
+		kmem_cache_free(skbuff_fclone_cache, skb);
+	else
+		kmem_cache_free(skbuff_head_cache, skb);
 	skb = NULL;
 	goto out;
 }
@@ -443,6 +465,10 @@ struct sk_buff *skb_clone(struct sk_buff
 #endif
 
 #endif
+#ifdef CONFIG_XEN
+	C(proto_csum_valid);
+	C(proto_csum_blank);
+#endif
 	C(truesize);
 	atomic_set(&n->users, 1);
 	C(head);
