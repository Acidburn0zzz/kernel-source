Subject: xen3 common
From: jbeulich@novell.com

autogenerated from http://xenbits.xensource.com/xen-3.0-testing.hg (tip 9646)

Index: head-2006-04-28/drivers/acpi/tables.c
===================================================================
--- head-2006-04-28.orig/drivers/acpi/tables.c	2006-04-28 09:18:11.000000000 +0200
+++ head-2006-04-28/drivers/acpi/tables.c	2006-04-28 09:19:28.000000000 +0200
@@ -572,6 +572,11 @@ static int __init acpi_table_get_sdt(str
  * 
  * result: sdt_entry[] is initialized
  */
+#if defined(CONFIG_X86_XEN) || defined(CONFIG_X86_64_XEN)
+#define acpi_rsdp_phys_to_va(rsdp_phys) isa_bus_to_virt(rsdp_phys)
+#else
+#define acpi_rsdp_phys_to_va(rsdp_phys) __va(rsdp_phys)
+#endif
 
 int __init acpi_table_init(void)
 {
@@ -587,7 +592,7 @@ int __init acpi_table_init(void)
 		return -ENODEV;
 	}
 
-	rsdp = (struct acpi_table_rsdp *)__va(rsdp_phys);
+	rsdp = (struct acpi_table_rsdp *)acpi_rsdp_phys_to_va(rsdp_phys);
 	if (!rsdp) {
 		printk(KERN_WARNING PREFIX "Unable to map RSDP\n");
 		return -ENODEV;
Index: head-2006-04-28/drivers/char/mem.c
===================================================================
--- head-2006-04-28.orig/drivers/char/mem.c	2006-04-28 09:18:11.000000000 +0200
+++ head-2006-04-28/drivers/char/mem.c	2006-04-28 09:19:28.000000000 +0200
@@ -108,6 +108,7 @@ static inline int valid_mmap_phys_addr_r
 }
 #endif
 
+#ifndef ARCH_HAS_DEV_MEM
 /*
  * This funcion reads the *physical* memory. The f_pos points directly to the 
  * memory location. 
@@ -232,6 +233,7 @@ static ssize_t write_mem(struct file * f
 	*ppos += written;
 	return written;
 }
+#endif
 
 #ifndef __HAVE_PHYS_MEM_ACCESS_PROT
 static pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
@@ -773,6 +775,7 @@ static int open_port(struct inode * inod
 #define open_kmem	open_mem
 #define open_oldmem	open_mem
 
+#ifndef ARCH_HAS_DEV_MEM
 static struct file_operations mem_fops = {
 	.llseek		= memory_lseek,
 	.read		= read_mem,
@@ -780,6 +783,9 @@ static struct file_operations mem_fops =
 	.mmap		= mmap_mem,
 	.open		= open_mem,
 };
+#else
+extern struct file_operations mem_fops;
+#endif
 
 static struct file_operations kmem_fops = {
 	.llseek		= memory_lseek,
Index: head-2006-04-28/drivers/char/tpm/Makefile
===================================================================
--- head-2006-04-28.orig/drivers/char/tpm/Makefile	2006-04-28 09:18:11.000000000 +0200
+++ head-2006-04-28/drivers/char/tpm/Makefile	2006-04-28 09:19:28.000000000 +0200
@@ -8,3 +8,4 @@ endif
 obj-$(CONFIG_TCG_NSC) += tpm_nsc.o
 obj-$(CONFIG_TCG_ATMEL) += tpm_atmel.o
 obj-$(CONFIG_TCG_INFINEON) += tpm_infineon.o
+obj-$(CONFIG_TCG_XEN) += tpm_xen.o
Index: head-2006-04-28/drivers/char/tpm/tpm.c
===================================================================
--- head-2006-04-28.orig/drivers/char/tpm/tpm.c	2006-04-28 09:18:11.000000000 +0200
+++ head-2006-04-28/drivers/char/tpm/tpm.c	2006-04-28 09:19:28.000000000 +0200
@@ -30,7 +30,8 @@
 
 enum tpm_const {
 	TPM_MINOR = 224,	/* officially assigned */
-	TPM_BUFSIZE = 2048,
+	TPM_MIN_BUFSIZE = 2048,
+	TPM_MAX_BUFSIZE = 64 * 1024,
 	TPM_NUM_DEVICES = 256,
 	TPM_NUM_MASK_ENTRIES = TPM_NUM_DEVICES / (8 * sizeof(int))
 };
@@ -52,14 +53,14 @@ static void timeout_work(void * ptr)
 
 	down(&chip->buffer_mutex);
 	atomic_set(&chip->data_pending, 0);
-	memset(chip->data_buffer, 0, TPM_BUFSIZE);
+	memset(chip->data_buffer, 0, get_chip_buffersize(chip));
 	up(&chip->buffer_mutex);
 }
 
 /*
  * Internal kernel interface to transmit TPM commands
  */
-static ssize_t tpm_transmit(struct tpm_chip *chip, const char *buf,
+static ssize_t tpm_transmit(struct tpm_chip * chip, const char *buf,
 			    size_t bufsiz)
 {
 	ssize_t rc;
@@ -351,7 +352,7 @@ int tpm_open(struct inode *inode, struct
 
 	spin_unlock(&driver_lock);
 
-	chip->data_buffer = kmalloc(TPM_BUFSIZE * sizeof(u8), GFP_KERNEL);
+	chip->data_buffer = kmalloc(get_chip_buffersize(chip) * sizeof(u8), GFP_KERNEL);
 	if (chip->data_buffer == NULL) {
 		chip->num_opens--;
 		put_device(chip->dev);
@@ -399,8 +400,8 @@ ssize_t tpm_write(struct file *file, con
 
 	down(&chip->buffer_mutex);
 
-	if (in_size > TPM_BUFSIZE)
-		in_size = TPM_BUFSIZE;
+	if (in_size > get_chip_buffersize(chip))
+		in_size = get_chip_buffersize(chip);
 
 	if (copy_from_user
 	    (chip->data_buffer, (void __user *) buf, in_size)) {
@@ -409,9 +410,11 @@ ssize_t tpm_write(struct file *file, con
 	}
 
 	/* atomic tpm command send and result receive */
-	out_size = tpm_transmit(chip, chip->data_buffer, TPM_BUFSIZE);
+	out_size = tpm_transmit(chip, chip->data_buffer, 
+	                        get_chip_buffersize(chip));
 
 	atomic_set(&chip->data_pending, out_size);
+	atomic_set(&chip->data_position, 0);
 	up(&chip->buffer_mutex);
 
 	/* Set a timeout by which the reader must come claim the result */
@@ -427,20 +430,33 @@ ssize_t tpm_read(struct file * file, cha
 {
 	struct tpm_chip *chip = file->private_data;
 	int ret_size;
+	int pos, pending = 0;
 
-	del_singleshot_timer_sync(&chip->user_read_timer);
-	flush_scheduled_work();
 	ret_size = atomic_read(&chip->data_pending);
-	atomic_set(&chip->data_pending, 0);
 	if (ret_size > 0) {	/* relay data */
 		if (size < ret_size)
 			ret_size = size;
 
+		pos = atomic_read(&chip->data_position);
+
 		down(&chip->buffer_mutex);
-		if (copy_to_user(buf, chip->data_buffer, ret_size))
+		if (copy_to_user(buf, &chip->data_buffer[pos], ret_size)) {
 			ret_size = -EFAULT;
+		} else {
+			pending = atomic_read(&chip->data_pending) - ret_size;
+			if ( pending ) {
+				atomic_set( &chip->data_pending, pending );
+				atomic_set( &chip->data_position, pos+ret_size );
+			}
+		}
 		up(&chip->buffer_mutex);
 	}
+	
+	if ( ret_size <= 0 || pending == 0 ) {
+		atomic_set( &chip->data_pending, 0 );
+		del_singleshot_timer_sync(&chip->user_read_timer);
+		flush_scheduled_work();
+	}
 
 	return ret_size;
 }
@@ -544,6 +560,12 @@ int tpm_register_hardware(struct device 
 	chip->user_read_timer.data = (unsigned long) chip;
 
 	chip->vendor = entry;
+	
+	if (entry->buffersize < TPM_MIN_BUFSIZE) {
+		entry->buffersize = TPM_MIN_BUFSIZE;
+	} else if (entry->buffersize > TPM_MAX_BUFSIZE) {
+		entry->buffersize = TPM_MAX_BUFSIZE;
+	}
 
 	chip->dev_num = -1;
 
Index: head-2006-04-28/drivers/char/tpm/tpm.h
===================================================================
--- head-2006-04-28.orig/drivers/char/tpm/tpm.h	2006-04-28 09:18:11.000000000 +0200
+++ head-2006-04-28/drivers/char/tpm/tpm.h	2006-04-28 09:19:28.000000000 +0200
@@ -50,6 +50,7 @@ struct tpm_vendor_specific {
 	u8 req_complete_mask;
 	u8 req_complete_val;
 	u8 req_canceled;
+	u32 buffersize;
 	void __iomem *iobase;		/* ioremapped address */
 	unsigned long base;		/* TPM base address */
 
@@ -74,6 +75,7 @@ struct tpm_chip {
 	/* Data passed to and from the tpm via the read/write calls */
 	u8 *data_buffer;
 	atomic_t data_pending;
+	atomic_t data_position;
 	struct semaphore buffer_mutex;
 
 	struct timer_list user_read_timer;	/* user needs to claim result */
@@ -99,6 +101,11 @@ static inline void tpm_write_index(int b
 	outb(value & 0xFF, base+1);
 }
 
+static inline u32 get_chip_buffersize(struct tpm_chip *chip)
+{
+	return chip->vendor->buffersize;
+}
+
 extern int tpm_register_hardware(struct device *,
 				 struct tpm_vendor_specific *);
 extern int tpm_open(struct inode *, struct file *);
Index: head-2006-04-28/drivers/char/tpm/tpm_xen.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ head-2006-04-28/drivers/char/tpm/tpm_xen.c	2006-04-28 09:19:28.000000000 +0200
@@ -0,0 +1,536 @@
+/*
+ * Copyright (C) 2004 IBM Corporation
+ *
+ * Authors:
+ * Leendert van Doorn <leendert@watson.ibm.com>
+ * Dave Safford <safford@watson.ibm.com>
+ * Reiner Sailer <sailer@watson.ibm.com>
+ * Kylene Hall <kjhall@us.ibm.com>
+ * Stefan Berger <stefanb@us.ibm.com>
+ *
+ * Maintained by: <tpmdd_devel@lists.sourceforge.net>
+ *
+ * Device driver for TCG/TCPA TPM (trusted platform module) for XEN.
+ * Specifications at www.trustedcomputinggroup.org
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation, version 2 of the
+ * License.
+ *
+ */
+
+#include <asm/uaccess.h>
+#include <linux/list.h>
+#include <xen/tpmfe.h>
+#include <linux/device.h>
+#include <linux/interrupt.h>
+#include <linux/platform_device.h>
+#include "tpm.h"
+
+/* read status bits */
+enum {
+	STATUS_BUSY = 0x01,
+	STATUS_DATA_AVAIL = 0x02,
+	STATUS_READY = 0x04
+};
+
+#define MIN(x,y)  ((x) < (y)) ? (x) : (y)
+
+struct transmission {
+	struct list_head next;
+	unsigned char *request;
+	unsigned int request_len;
+	unsigned char *rcv_buffer;
+	unsigned int  buffersize;
+	unsigned int flags;
+};
+
+enum {
+	TRANSMISSION_FLAG_WAS_QUEUED = 0x1
+};
+
+struct data_exchange {
+	struct transmission *current_request;
+	spinlock_t           req_list_lock;
+	wait_queue_head_t    req_wait_queue;
+
+	struct list_head     queued_requests;
+
+	struct transmission *current_response;
+	spinlock_t           resp_list_lock;
+	wait_queue_head_t    resp_wait_queue;     // processes waiting for responses
+
+	struct transmission *req_cancelled;       // if a cancellation was encounterd
+
+	unsigned int         fe_status;
+	unsigned int         flags;
+};
+
+enum {
+	DATAEX_FLAG_QUEUED_ONLY = 0x1
+};
+
+static struct data_exchange dataex;
+
+static unsigned long disconnect_time;
+
+static struct tpmfe_device tpmfe;
+
+/* local function prototypes */
+static void __exit cleanup_xen(void);
+
+
+/* =============================================================
+ * Some utility functions
+ * =============================================================
+ */
+static inline struct transmission *
+transmission_alloc(void)
+{
+	return kzalloc(sizeof(struct transmission), GFP_KERNEL);
+}
+
+static inline unsigned char *
+transmission_set_buffer(struct transmission *t,
+                        unsigned char *buffer, unsigned int len)
+{
+	kfree(t->request);
+	t->request = kmalloc(len, GFP_KERNEL);
+	if (t->request) {
+		memcpy(t->request,
+		       buffer,
+		       len);
+		t->request_len = len;
+	}
+	return t->request;
+}
+
+static inline void
+transmission_free(struct transmission *t)
+{
+	kfree(t->request);
+	kfree(t->rcv_buffer);
+	kfree(t);
+}
+
+/* =============================================================
+ * Interface with the TPM shared memory driver for XEN
+ * =============================================================
+ */
+static int tpm_recv(const u8 *buffer, size_t count, const void *ptr)
+{
+	int ret_size = 0;
+	struct transmission *t;
+
+	/*
+	 * The list with requests must contain one request
+	 * only and the element there must be the one that
+	 * was passed to me from the front-end.
+	 */
+	if (dataex.current_request != ptr) {
+		printk("WARNING: The request pointer is different than the "
+		       "pointer the shared memory driver returned to me. "
+		       "%p != %p\n",
+		       dataex.current_request, ptr);
+	}
+
+	/*
+	 * If the request has been cancelled, just quit here
+	 */
+	if (dataex.req_cancelled == (struct transmission *)ptr) {
+		if (dataex.current_request == dataex.req_cancelled) {
+			dataex.current_request = NULL;
+		}
+		transmission_free(dataex.req_cancelled);
+		dataex.req_cancelled = NULL;
+		return 0;
+	}
+
+	if (NULL != (t = dataex.current_request)) {
+		transmission_free(t);
+		dataex.current_request = NULL;
+	}
+
+	t = transmission_alloc();
+	if (t) {
+		unsigned long flags;
+		t->rcv_buffer = kmalloc(count, GFP_KERNEL);
+		if (! t->rcv_buffer) {
+			transmission_free(t);
+			return -ENOMEM;
+		}
+		t->buffersize = count;
+		memcpy(t->rcv_buffer, buffer, count);
+		ret_size = count;
+
+		spin_lock_irqsave(&dataex.resp_list_lock ,flags);
+		dataex.current_response = t;
+		spin_unlock_irqrestore(&dataex.resp_list_lock, flags);
+		wake_up_interruptible(&dataex.resp_wait_queue);
+	}
+	return ret_size;
+}
+
+
+static void tpm_fe_status(unsigned int flags)
+{
+	dataex.fe_status = flags;
+	if ((dataex.fe_status & TPMFE_STATUS_CONNECTED) == 0) {
+		disconnect_time = jiffies;
+	}
+}
+
+/* =============================================================
+ * Interface with the generic TPM driver
+ * =============================================================
+ */
+static int tpm_xen_recv(struct tpm_chip *chip, u8 * buf, size_t count)
+{
+	unsigned long flags;
+	int rc = 0;
+
+	spin_lock_irqsave(&dataex.resp_list_lock, flags);
+	/*
+	 * Check if the previous operation only queued the command
+	 * In this case there won't be a response, so I just
+	 * return from here and reset that flag. In any other
+	 * case I should receive a response from the back-end.
+	 */
+	if ((dataex.flags & DATAEX_FLAG_QUEUED_ONLY) != 0) {
+		dataex.flags &= ~DATAEX_FLAG_QUEUED_ONLY;
+		spin_unlock_irqrestore(&dataex.resp_list_lock, flags);
+		/*
+		 * a little hack here. The first few measurements
+		 * are queued since there's no way to talk to the
+		 * TPM yet (due to slowness of the control channel)
+		 * So we just make IMA happy by giving it 30 NULL
+		 * bytes back where the most important part is
+		 * that the result code is '0'.
+		 */
+
+		count = MIN(count, 30);
+		memset(buf, 0x0, count);
+		return count;
+	}
+	/*
+	 * Check whether something is in the responselist and if
+	 * there's nothing in the list wait for something to appear.
+	 */
+
+	if (NULL == dataex.current_response) {
+		spin_unlock_irqrestore(&dataex.resp_list_lock, flags);
+		interruptible_sleep_on_timeout(&dataex.resp_wait_queue,
+		                               1000);
+		spin_lock_irqsave(&dataex.resp_list_lock ,flags);
+	}
+
+	if (NULL != dataex.current_response) {
+		struct transmission *t = dataex.current_response;
+		dataex.current_response = NULL;
+		rc = MIN(count, t->buffersize);
+		memcpy(buf, t->rcv_buffer, rc);
+		transmission_free(t);
+	}
+
+	spin_unlock_irqrestore(&dataex.resp_list_lock, flags);
+	return rc;
+}
+
+static int tpm_xen_send(struct tpm_chip *chip, u8 * buf, size_t count)
+{
+	/*
+	 * We simply pass the packet onto the XEN shared
+	 * memory driver.
+	 */
+	unsigned long flags;
+	int rc;
+	struct transmission *t = transmission_alloc();
+
+	spin_lock_irqsave(&dataex.req_list_lock, flags);
+	/*
+	 * If there's a current request, it must be the
+	 * previous request that has timed out.
+	 */
+	if (dataex.current_request != NULL) {
+		printk("WARNING: Sending although there is a request outstanding.\n"
+		       "         Previous request must have timed out.\n");
+		transmission_free(dataex.current_request);
+		dataex.current_request = NULL;
+	}
+
+	if (t != NULL) {
+		unsigned int error = 0;
+		/*
+		 * Queue the packet if the driver below is not
+		 * ready, yet, or there is any packet already
+		 * in the queue.
+		 * If the driver below is ready, unqueue all
+		 * packets first before sending our current
+		 * packet.
+		 * For each unqueued packet, except for the
+		 * last (=current) packet, call the function
+		 * tpm_xen_recv to wait for the response to come
+		 * back.
+		 */
+		if ((dataex.fe_status & TPMFE_STATUS_CONNECTED) == 0) {
+			if (time_after(jiffies, disconnect_time + HZ * 10)) {
+				rc = -ENOENT;
+			} else {
+				/*
+				 * copy the request into the buffer
+				 */
+				if (transmission_set_buffer(t, buf, count)
+				    == NULL) {
+					transmission_free(t);
+					rc = -ENOMEM;
+					goto exit;
+				}
+				dataex.flags |= DATAEX_FLAG_QUEUED_ONLY;
+				list_add_tail(&t->next, &dataex.queued_requests);
+				rc = 0;
+			}
+		} else {
+			/*
+			 * Check whether there are any packets in the queue
+			 */
+			while (!list_empty(&dataex.queued_requests)) {
+				/*
+				 * Need to dequeue them.
+				 * Read the result into a dummy buffer.
+				 */
+				unsigned char buffer[1];
+				struct transmission *qt = (struct transmission *) dataex.queued_requests.next;
+				list_del(&qt->next);
+				dataex.current_request = qt;
+				spin_unlock_irqrestore(&dataex.req_list_lock,
+				                       flags);
+
+				rc = tpm_fe_send(tpmfe.tpm_private,
+				                 qt->request,
+				                 qt->request_len,
+				                 qt);
+
+				if (rc < 0) {
+					spin_lock_irqsave(&dataex.req_list_lock, flags);
+					if ((qt = dataex.current_request) != NULL) {
+						/*
+						 * requeue it at the beginning
+						 * of the list
+						 */
+						list_add(&qt->next,
+						         &dataex.queued_requests);
+					}
+					dataex.current_request = NULL;
+					error = 1;
+					break;
+				}
+				/*
+				 * After this point qt is not valid anymore!
+				 * It is freed when the front-end is delivering the data
+				 * by calling tpm_recv
+				 */
+
+				/*
+				 * Try to receive the response now into the provided dummy
+				 * buffer (I don't really care about this response since
+				 * there is no receiver anymore for this response)
+				 */
+				rc = tpm_xen_recv(chip, buffer, sizeof(buffer));
+
+				spin_lock_irqsave(&dataex.req_list_lock, flags);
+			}
+
+			if (error == 0) {
+				/*
+				 * Finally, send the current request.
+				 */
+				dataex.current_request = t;
+				/*
+				 * Call the shared memory driver
+				 * Pass to it the buffer with the request, the
+				 * amount of bytes in the request and
+				 * a void * pointer (here: transmission structure)
+				 */
+				rc = tpm_fe_send(tpmfe.tpm_private,
+				                 buf, count, t);
+				/*
+				 * The generic TPM driver will call
+				 * the function to receive the response.
+				 */
+				if (rc < 0) {
+					dataex.current_request = NULL;
+					goto queue_it;
+				}
+			} else {
+queue_it:
+				if (transmission_set_buffer(t, buf, count) == NULL) {
+					transmission_free(t);
+					rc = -ENOMEM;
+					goto exit;
+				}
+				/*
+				 * An error occurred. Don't event try
+				 * to send the current request. Just
+				 * queue it.
+				 */
+				dataex.flags |= DATAEX_FLAG_QUEUED_ONLY;
+				list_add_tail(&t->next,
+				              &dataex.queued_requests);
+				rc = 0;
+			}
+		}
+	} else {
+		rc = -ENOMEM;
+	}
+
+exit:
+	spin_unlock_irqrestore(&dataex.req_list_lock, flags);
+	return rc;
+}
+
+static void tpm_xen_cancel(struct tpm_chip *chip)
+{
+	unsigned long flags;
+	spin_lock_irqsave(&dataex.resp_list_lock,flags);
+
+	dataex.req_cancelled = dataex.current_request;
+
+	spin_unlock_irqrestore(&dataex.resp_list_lock,flags);
+}
+
+static u8 tpm_xen_status(struct tpm_chip *chip)
+{
+	unsigned long flags;
+	u8 rc = 0;
+	spin_lock_irqsave(&dataex.resp_list_lock, flags);
+	/*
+	 * Data are available if:
+	 *  - there's a current response
+	 *  - the last packet was queued only (this is fake, but necessary to
+	 *      get the generic TPM layer to call the receive function.)
+	 */
+	if (NULL != dataex.current_response ||
+	    0 != (dataex.flags & DATAEX_FLAG_QUEUED_ONLY)) {
+		rc = STATUS_DATA_AVAIL;
+	}
+	spin_unlock_irqrestore(&dataex.resp_list_lock, flags);
+	return rc;
+}
+
+static struct file_operations tpm_xen_ops = {
+	.owner = THIS_MODULE,
+	.llseek = no_llseek,
+	.open = tpm_open,
+	.read = tpm_read,
+	.write = tpm_write,
+	.release = tpm_release,
+};
+
+static DEVICE_ATTR(pubek, S_IRUGO, tpm_show_pubek, NULL);
+static DEVICE_ATTR(pcrs, S_IRUGO, tpm_show_pcrs, NULL);
+static DEVICE_ATTR(caps, S_IRUGO, tpm_show_caps, NULL);
+static DEVICE_ATTR(cancel, S_IWUSR |S_IWGRP, NULL, tpm_store_cancel);
+
+static struct attribute* xen_attrs[] = {
+	&dev_attr_pubek.attr,
+	&dev_attr_pcrs.attr,
+	&dev_attr_caps.attr,
+	&dev_attr_cancel.attr,
+	NULL,
+};
+
+static struct attribute_group xen_attr_grp = { .attrs = xen_attrs };
+
+static struct tpm_vendor_specific tpm_xen = {
+	.recv = tpm_xen_recv,
+	.send = tpm_xen_send,
+	.cancel = tpm_xen_cancel,
+	.status = tpm_xen_status,
+	.req_complete_mask = STATUS_BUSY | STATUS_DATA_AVAIL,
+	.req_complete_val  = STATUS_DATA_AVAIL,
+	.req_canceled = STATUS_READY,
+	.base = 0,
+	.attr_group = &xen_attr_grp,
+	.miscdev.fops = &tpm_xen_ops,
+	.buffersize = 64 * 1024,
+};
+
+static struct platform_device *pdev;
+
+static struct tpmfe_device tpmfe = {
+	.receive = tpm_recv,
+	.status  = tpm_fe_status,
+};
+
+
+static int __init init_xen(void)
+{
+	int rc;
+
+	if ((xen_start_info->flags & SIF_INITDOMAIN)) {
+		return -EPERM;
+	}
+	/*
+	 * Register device with the low lever front-end
+	 * driver
+	 */
+	if ((rc = tpm_fe_register_receiver(&tpmfe)) < 0) {
+		goto err_exit;
+	}
+
+	/*
+	 * Register our device with the system.
+	 */
+	pdev = platform_device_register_simple("tpm_vtpm", -1, NULL, 0);
+	if (IS_ERR(pdev)) {
+		rc = PTR_ERR(pdev);
+		goto err_unreg_fe;
+	}
+
+	tpm_xen.buffersize = tpmfe.max_tx_size;
+
+	if ((rc = tpm_register_hardware(&pdev->dev, &tpm_xen)) < 0) {
+		goto err_unreg_pdev;
+	}
+
+	dataex.current_request = NULL;
+	spin_lock_init(&dataex.req_list_lock);
+	init_waitqueue_head(&dataex.req_wait_queue);
+	INIT_LIST_HEAD(&dataex.queued_requests);
+
+	dataex.current_response = NULL;
+	spin_lock_init(&dataex.resp_list_lock);
+	init_waitqueue_head(&dataex.resp_wait_queue);
+
+	disconnect_time = jiffies;
+
+	return 0;
+
+
+err_unreg_pdev:
+	platform_device_unregister(pdev);
+err_unreg_fe:
+	tpm_fe_unregister_receiver();
+
+err_exit:
+	return rc;
+}
+
+static void __exit cleanup_xen(void)
+{
+	struct tpm_chip *chip = dev_get_drvdata(&pdev->dev);
+	if (chip) {
+		tpm_remove_hardware(chip->dev);
+		platform_device_unregister(pdev);
+		tpm_fe_unregister_receiver();
+	}
+}
+
+module_init(init_xen);
+module_exit(cleanup_xen);
+
+MODULE_AUTHOR("Stefan Berger (stefanb@us.ibm.com)");
+MODULE_DESCRIPTION("TPM Driver for XEN (shared memory)");
+MODULE_VERSION("1.0");
+MODULE_LICENSE("GPL");
Index: head-2006-04-28/drivers/char/tty_io.c
===================================================================
--- head-2006-04-28.orig/drivers/char/tty_io.c	2006-04-28 09:18:11.000000000 +0200
+++ head-2006-04-28/drivers/char/tty_io.c	2006-04-28 09:19:28.000000000 +0200
@@ -132,6 +132,8 @@ LIST_HEAD(tty_drivers);			/* linked list
    vt.c for deeply disgusting hack reasons */
 DECLARE_MUTEX(tty_sem);
 
+int console_use_vt = 1;
+
 #ifdef CONFIG_UNIX98_PTYS
 extern struct tty_driver *ptm_driver;	/* Unix98 pty masters; for /dev/ptmx */
 extern int pty_limit;		/* Config limit on Unix98 ptys */
@@ -2054,7 +2056,7 @@ retry_open:
 		goto got_driver;
 	}
 #ifdef CONFIG_VT
-	if (device == MKDEV(TTY_MAJOR,0)) {
+	if (console_use_vt && (device == MKDEV(TTY_MAJOR,0))) {
 		extern struct tty_driver *console_driver;
 		driver = console_driver;
 		index = fg_console;
@@ -3256,6 +3258,8 @@ static int __init tty_init(void)
 #endif
 
 #ifdef CONFIG_VT
+	if (!console_use_vt)
+		goto out_vt;
 	cdev_init(&vc0_cdev, &console_fops);
 	if (cdev_add(&vc0_cdev, MKDEV(TTY_MAJOR, 0), 1) ||
 	    register_chrdev_region(MKDEV(TTY_MAJOR, 0), 1, "/dev/vc/0") < 0)
@@ -3264,6 +3268,7 @@ static int __init tty_init(void)
 	class_device_create(tty_class, NULL, MKDEV(TTY_MAJOR, 0), NULL, "tty0");
 
 	vty_init();
+ out_vt:
 #endif
 	return 0;
 }
Index: head-2006-04-28/include/linux/gfp.h
===================================================================
--- head-2006-04-28.orig/include/linux/gfp.h	2006-04-28 09:18:11.000000000 +0200
+++ head-2006-04-28/include/linux/gfp.h	2006-04-28 09:19:28.000000000 +0200
@@ -98,7 +98,11 @@ static inline int gfp_zone(gfp_t gfp)
  */
 
 #ifndef HAVE_ARCH_FREE_PAGE
-static inline void arch_free_page(struct page *page, int order) { }
+/*
+ * If arch_free_page returns non-zero then the generic free_page code can
+ * immediately bail: the arch-specific function has done all the work.
+ */
+static inline int arch_free_page(struct page *page, int order) { return 0; }
 #endif
 
 extern struct page *
Index: head-2006-04-28/include/linux/highmem.h
===================================================================
--- head-2006-04-28.orig/include/linux/highmem.h	2006-04-28 09:18:11.000000000 +0200
+++ head-2006-04-28/include/linux/highmem.h	2006-04-28 09:19:28.000000000 +0200
@@ -13,10 +13,16 @@
 
 /* declarations for linux/mm/highmem.c */
 unsigned int nr_free_highpages(void);
+#ifdef CONFIG_XEN
+void kmap_flush_unused(void);
+#endif
 
 #else /* CONFIG_HIGHMEM */
 
 static inline unsigned int nr_free_highpages(void) { return 0; }
+#ifdef CONFIG_XEN
+static inline void kmap_flush_unused(void) { }
+#endif
 
 static inline void *kmap(struct page *page)
 {
Index: head-2006-04-28/include/linux/mm.h
===================================================================
--- head-2006-04-28.orig/include/linux/mm.h	2006-04-28 09:18:11.000000000 +0200
+++ head-2006-04-28/include/linux/mm.h	2006-04-28 09:19:28.000000000 +0200
@@ -166,6 +166,9 @@ extern unsigned int kobjsize(const void 
 #define VM_NONLINEAR	0x00800000	/* Is non-linear (remap_file_pages) */
 #define VM_MAPPED_COPY	0x01000000	/* T if mapped copy of data (nommu mmap) */
 #define VM_INSERTPAGE	0x02000000	/* The vma has had "vm_insert_page()" done on it */
+#ifdef CONFIG_XEN
+#define VM_FOREIGN	0x04000000	/* Has pages belonging to another VM */
+#endif
 
 #ifndef VM_STACK_DEFAULT_FLAGS		/* arch can override this */
 #define VM_STACK_DEFAULT_FLAGS VM_DATA_DEFAULT_FLAGS
@@ -246,6 +249,9 @@ struct page {
 #if NR_CPUS >= CONFIG_SPLIT_PTLOCK_CPUS
 	    spinlock_t ptl;
 #endif
+#ifdef CONFIG_XEN
+	    struct list_head ballooned;
+#endif
 	};
 	pgoff_t index;			/* Our offset within mapping. */
 	struct list_head lru;		/* Pageout list, eg. active_list
@@ -1014,6 +1020,13 @@ struct page *follow_page(struct vm_area_
 #define FOLL_GET	0x04	/* do get_page on page */
 #define FOLL_ANON	0x08	/* give ZERO_PAGE if no pgtable */
 
+#ifdef CONFIG_XEN
+typedef int (*pte_fn_t)(pte_t *pte, struct page *pmd_page, unsigned long addr,
+			void *data);
+extern int apply_to_page_range(struct mm_struct *mm, unsigned long address,
+			       unsigned long size, pte_fn_t fn, void *data);
+#endif
+
 #ifdef CONFIG_PROC_FS
 void vm_stat_account(struct mm_struct *, unsigned long, struct file *, long);
 #else
Index: head-2006-04-28/include/linux/skbuff.h
===================================================================
--- head-2006-04-28.orig/include/linux/skbuff.h	2006-04-28 09:18:11.000000000 +0200
+++ head-2006-04-28/include/linux/skbuff.h	2006-04-28 09:19:28.000000000 +0200
@@ -190,6 +190,8 @@ enum {
  *	@local_df: allow local fragmentation
  *	@cloned: Head may be cloned (check refcnt to be sure)
  *	@nohdr: Payload reference only, must not modify header
+ *	@proto_data_valid: Protocol csum validated since arriving at localhost
+ *	@proto_csum_blank: Protocol csum must be added before leaving localhost
  *	@pkt_type: Packet class
  *	@fclone: skbuff clone status
  *	@ip_summed: Driver fed us an IP checksum
@@ -286,6 +288,10 @@ struct sk_buff {
 	__u16			tc_verd;	/* traffic control verdict */
 #endif
 #endif
+#ifdef CONFIG_XEN
+	__u8			proto_data_valid:1,
+				proto_csum_blank:1;
+#endif
 #ifdef CONFIG_NET_DMA
 	dma_cookie_t		dma_cookie;
 #endif
@@ -325,7 +331,8 @@ static inline struct sk_buff *alloc_skb_
 
 extern struct sk_buff *alloc_skb_from_cache(kmem_cache_t *cp,
 					    unsigned int size,
-					    gfp_t priority);
+					    gfp_t priority,
+					    int fclone);
 extern void	       kfree_skbmem(struct sk_buff *skb);
 extern struct sk_buff *skb_clone(struct sk_buff *skb,
 				 gfp_t priority);
@@ -1055,7 +1062,7 @@ static inline struct sk_buff *__dev_allo
 	return skb;
 }
 #else
-extern struct sk_buff *__dev_alloc_skb(unsigned int length, int gfp_mask);
+extern struct sk_buff *__dev_alloc_skb(unsigned int length, gfp_t gfp_mask);
 #endif
 
 /**
Index: head-2006-04-28/lib/Makefile
===================================================================
--- head-2006-04-28.orig/lib/Makefile	2006-04-28 09:18:11.000000000 +0200
+++ head-2006-04-28/lib/Makefile	2006-04-28 09:19:28.000000000 +0200
@@ -48,6 +48,9 @@ obj-$(CONFIG_SGRB)	 += sgrb.o
 obj-$(CONFIG_STATISTICS) += statistic.o
 
 obj-$(CONFIG_SWIOTLB) += swiotlb.o
+ifneq ($(CONFIG_IA64),y)
+swiotlb-$(CONFIG_XEN) := ../arch/i386/kernel/swiotlb.o
+endif
 
 hostprogs-y	:= gen_crc32table
 clean-files	:= crc32table.h
Index: head-2006-04-28/mm/highmem.c
===================================================================
--- head-2006-04-28.orig/mm/highmem.c	2006-04-28 09:18:11.000000000 +0200
+++ head-2006-04-28/mm/highmem.c	2006-04-28 09:19:28.000000000 +0200
@@ -152,6 +152,17 @@ start:
 	return vaddr;
 }
 
+#ifdef CONFIG_XEN
+void kmap_flush_unused(void)
+{
+	spin_lock(&kmap_lock);
+	flush_all_zero_pkmaps();
+	spin_unlock(&kmap_lock);
+}
+
+EXPORT_SYMBOL(kmap_flush_unused);
+#endif
+
 void fastcall *kmap_high(struct page *page)
 {
 	unsigned long vaddr;
Index: head-2006-04-28/mm/memory.c
===================================================================
--- head-2006-04-28.orig/mm/memory.c	2006-04-28 09:18:12.000000000 +0200
+++ head-2006-04-28/mm/memory.c	2006-04-28 09:19:28.000000000 +0200
@@ -405,7 +405,8 @@ struct page *vm_normal_page(struct vm_ar
 	 * Remove this test eventually!
 	 */
 	if (unlikely(!pfn_valid(pfn))) {
-		print_bad_pte(vma, pte, addr);
+		if (!vma->vm_flags & VM_RESERVED)
+			print_bad_pte(vma, pte, addr);
 		return NULL;
 	}
 
@@ -1021,6 +1022,23 @@ int get_user_pages(struct task_struct *t
 			continue;
 		}
 
+#ifdef CONFIG_XEN
+		if (vma && (vma->vm_flags & VM_FOREIGN)) {
+			struct page **map = vma->vm_private_data;
+			int offset = (start - vma->vm_start) >> PAGE_SHIFT;
+
+			if (map[offset] != NULL) {
+				if (pages)
+					pages[i] = map[offset];
+				if (vmas)
+					vmas[i] = vma;
+				i++;
+				start += PAGE_SIZE;
+				len--;
+				continue;
+			}
+		}
+#endif
 		if (!vma || (vma->vm_flags & (VM_IO | VM_PFNMAP))
 				|| !(vm_flags & vma->vm_flags))
 			return i ? : -EFAULT;
@@ -1360,6 +1378,102 @@ int remap_pfn_range(struct vm_area_struc
 }
 EXPORT_SYMBOL(remap_pfn_range);
 
+#ifdef CONFIG_XEN
+static inline int apply_to_pte_range(struct mm_struct *mm, pmd_t *pmd,
+				     unsigned long addr, unsigned long end,
+				     pte_fn_t fn, void *data)
+{
+	pte_t *pte;
+	int err;
+	struct page *pmd_page;
+	spinlock_t *ptl;
+
+	pte = (mm == &init_mm) ?
+		pte_alloc_kernel(pmd, addr) :
+		pte_alloc_map_lock(mm, pmd, addr, &ptl);
+	if (!pte)
+		return -ENOMEM;
+
+	BUG_ON(pmd_huge(*pmd));
+
+	pmd_page = pmd_page(*pmd);
+
+	do {
+		err = fn(pte, pmd_page, addr, data);
+		if (err)
+			break;
+	} while (pte++, addr += PAGE_SIZE, addr != end);
+
+	if (mm != &init_mm)
+		pte_unmap_unlock(pte-1, ptl);
+	return err;
+}
+
+static inline int apply_to_pmd_range(struct mm_struct *mm, pud_t *pud,
+				     unsigned long addr, unsigned long end,
+				     pte_fn_t fn, void *data)
+{
+	pmd_t *pmd;
+	unsigned long next;
+	int err;
+
+	pmd = pmd_alloc(mm, pud, addr);
+	if (!pmd)
+		return -ENOMEM;
+	do {
+		next = pmd_addr_end(addr, end);
+		err = apply_to_pte_range(mm, pmd, addr, next, fn, data);
+		if (err)
+			break;
+	} while (pmd++, addr = next, addr != end);
+	return err;
+}
+
+static inline int apply_to_pud_range(struct mm_struct *mm, pgd_t *pgd,
+				     unsigned long addr, unsigned long end,
+				     pte_fn_t fn, void *data)
+{
+	pud_t *pud;
+	unsigned long next;
+	int err;
+
+	pud = pud_alloc(mm, pgd, addr);
+	if (!pud)
+		return -ENOMEM;
+	do {
+		next = pud_addr_end(addr, end);
+		err = apply_to_pmd_range(mm, pud, addr, next, fn, data);
+		if (err)
+			break;
+	} while (pud++, addr = next, addr != end);
+	return err;
+}
+
+/*
+ * Scan a region of virtual memory, filling in page tables as necessary
+ * and calling a provided function on each leaf page table.
+ */
+int apply_to_page_range(struct mm_struct *mm, unsigned long addr,
+			unsigned long size, pte_fn_t fn, void *data)
+{
+	pgd_t *pgd;
+	unsigned long next;
+	unsigned long end = addr + size;
+	int err;
+
+	BUG_ON(addr >= end);
+	pgd = pgd_offset(mm, addr);
+	do {
+		next = pgd_addr_end(addr, end);
+		err = apply_to_pud_range(mm, pgd, addr, next, fn, data);
+		if (err)
+			break;
+	} while (pgd++, addr = next, addr != end);
+	return err;
+}
+EXPORT_SYMBOL_GPL(apply_to_page_range);
+#endif
+
 /*
  * handle_pte_fault chooses page fault handler according to an entry
  * which was read non-atomically.  Before making any commitment, on
Index: head-2006-04-28/mm/mmap.c
===================================================================
--- head-2006-04-28.orig/mm/mmap.c	2006-04-28 09:18:12.000000000 +0200
+++ head-2006-04-28/mm/mmap.c	2006-04-28 09:19:28.000000000 +0200
@@ -1937,6 +1937,10 @@ void exit_mmap(struct mm_struct *mm)
 	unsigned long nr_accounted = 0;
 	unsigned long end;
 
+#ifdef arch_exit_mmap
+	arch_exit_mmap(mm);
+#endif
+
 	lru_add_drain();
 	flush_cache_mm(mm);
 	tlb = tlb_gather_mmu(mm, 1);
Index: head-2006-04-28/mm/page_alloc.c
===================================================================
--- head-2006-04-28.orig/mm/page_alloc.c	2006-04-28 09:18:12.000000000 +0200
+++ head-2006-04-28/mm/page_alloc.c	2006-04-28 09:19:28.000000000 +0200
@@ -422,7 +422,8 @@ static void __free_pages_ok(struct page 
 	int i;
 	int reserved = 0;
 
-	arch_free_page(page, order);
+	if (arch_free_page(page, order))
+		return;
 	if (!PageHighMem(page))
 		mutex_debug_check_no_locks_freed(page_address(page),
 						 PAGE_SIZE<<order);
@@ -720,7 +721,8 @@ static void fastcall free_hot_cold_page(
 	struct per_cpu_pages *pcp;
 	unsigned long flags;
 
-	arch_free_page(page, 0);
+	if (arch_free_page(page, 0))
+		return;
 
 	if (PageAnon(page))
 		page->mapping = NULL;
Index: head-2006-04-28/net/core/dev.c
===================================================================
--- head-2006-04-28.orig/net/core/dev.c	2006-04-28 09:18:12.000000000 +0200
+++ head-2006-04-28/net/core/dev.c	2006-04-28 09:19:28.000000000 +0200
@@ -117,6 +117,12 @@
 #include <asm/current.h>
 #include <linux/dmaengine.h>
 
+#ifdef CONFIG_XEN
+#include <net/ip.h>
+#include <linux/tcp.h>
+#include <linux/udp.h>
+#endif
+
 /*
  *	The list of packet types we will receive (as opposed to discard)
  *	and the routines to invoke.
@@ -1225,6 +1231,43 @@ int __skb_linearize(struct sk_buff *skb,
 	}						\
 }
 
+#ifdef CONFIG_XEN
+inline int skb_checksum_setup(struct sk_buff *skb)
+{
+	if (skb->proto_csum_blank) {
+		if (skb->protocol != htons(ETH_P_IP))
+			goto out;
+		skb->h.raw = (unsigned char *)skb->nh.iph + 4*skb->nh.iph->ihl;
+		if (skb->h.raw >= skb->tail)
+			goto out;
+		switch (skb->nh.iph->protocol) {
+		case IPPROTO_TCP:
+			skb->csum = offsetof(struct tcphdr, check);
+			break;
+		case IPPROTO_UDP:
+			skb->csum = offsetof(struct udphdr, check);
+			break;
+		default:
+			if (net_ratelimit())
+				printk(KERN_ERR "Attempting to checksum a non-"
+				       "TCP/UDP packet, dropping a protocol"
+				       " %d packet", skb->nh.iph->protocol);
+			goto out;
+		}
+		if ((skb->h.raw + skb->csum + 2) > skb->tail)
+			goto out;
+		skb->ip_summed = CHECKSUM_HW;
+		skb->proto_csum_blank = 0;
+	}
+	return 0;
+out:
+	return -EPROTO;
+}
+#else
+inline int skb_checksum_setup(struct sk_buff *skb) {}
+#endif
+
+
 /**
  *	dev_queue_xmit - transmit a buffer
  *	@skb: buffer to transmit
@@ -1271,6 +1314,12 @@ int dev_queue_xmit(struct sk_buff *skb)
 	    __skb_linearize(skb, GFP_ATOMIC))
 		goto out_kfree_skb;
 
+ 	/* If a checksum-deferred packet is forwarded to a device that needs a
+ 	 * checksum, correct the pointers and force checksumming.
+ 	 */
+ 	if(skb_checksum_setup(skb))
+ 		goto out_kfree_skb;
+  
 	/* If packet is not checksummed and device does not support
 	 * checksumming for this protocol, complete checksumming here.
 	 */
@@ -1620,6 +1669,19 @@ int netif_receive_skb(struct sk_buff *sk
 	}
 #endif
 
+#ifdef CONFIG_XEN
+	switch (skb->ip_summed) {
+	case CHECKSUM_UNNECESSARY:
+		skb->proto_data_valid = 1;
+		break;
+	case CHECKSUM_HW:
+		/* XXX Implement me. */
+	default:
+		skb->proto_data_valid = 0;
+		break;
+	}
+#endif
+
 	list_for_each_entry_rcu(ptype, &ptype_all, list) {
 		if (!ptype->dev || ptype->dev == skb->dev) {
 			if (pt_prev) 
@@ -3408,6 +3470,7 @@ EXPORT_SYMBOL(unregister_netdevice_notif
 EXPORT_SYMBOL(net_enable_timestamp);
 EXPORT_SYMBOL(net_disable_timestamp);
 EXPORT_SYMBOL(dev_get_flags);
+EXPORT_SYMBOL(skb_checksum_setup);
 
 #if defined(CONFIG_BRIDGE) || defined(CONFIG_BRIDGE_MODULE)
 EXPORT_SYMBOL(br_handle_frame_hook);
Index: head-2006-04-28/net/core/skbuff.c
===================================================================
--- head-2006-04-28.orig/net/core/skbuff.c	2006-04-28 09:18:12.000000000 +0200
+++ head-2006-04-28/net/core/skbuff.c	2006-04-28 09:19:28.000000000 +0200
@@ -132,6 +132,7 @@ void skb_under_panic(struct sk_buff *skb
  *	Buffers may only be allocated from interrupts using a @gfp_mask of
  *	%GFP_ATOMIC.
  */
+#ifndef CONFIG_HAVE_ARCH_ALLOC_SKB
 struct sk_buff *__alloc_skb(unsigned int size, gfp_t gfp_mask,
 			    int fclone)
 {
@@ -186,6 +187,7 @@ nodata:
 	skb = NULL;
 	goto out;
 }
+#endif /* !CONFIG_HAVE_ARCH_ALLOC_SKB */
 
 /**
  *	alloc_skb_from_cache	-	allocate a network buffer
@@ -203,14 +205,18 @@ nodata:
  */
 struct sk_buff *alloc_skb_from_cache(kmem_cache_t *cp,
 				     unsigned int size,
-				     gfp_t gfp_mask)
+				     gfp_t gfp_mask,
+				     int fclone)
 {
+	kmem_cache_t *cache;
+	struct skb_shared_info *shinfo;
 	struct sk_buff *skb;
 	u8 *data;
 
+	cache = fclone ? skbuff_fclone_cache : skbuff_head_cache;
+
 	/* Get the HEAD */
-	skb = kmem_cache_alloc(skbuff_head_cache,
-			       gfp_mask & ~__GFP_DMA);
+	skb = kmem_cache_alloc(cache, gfp_mask & ~__GFP_DMA);
 	if (!skb)
 		goto out;
 
@@ -227,16 +233,29 @@ struct sk_buff *alloc_skb_from_cache(kme
 	skb->data = data;
 	skb->tail = data;
 	skb->end  = data + size;
+	/* make sure we initialize shinfo sequentially */
+	shinfo = skb_shinfo(skb);
+	atomic_set(&shinfo->dataref, 1);
+	shinfo->nr_frags  = 0;
+	shinfo->tso_size = 0;
+	shinfo->tso_segs = 0;
+	shinfo->ufo_size = 0;
+	shinfo->ip6_frag_id = 0;
+	shinfo->frag_list = NULL;
 
-	atomic_set(&(skb_shinfo(skb)->dataref), 1);
-	skb_shinfo(skb)->nr_frags  = 0;
-	skb_shinfo(skb)->tso_size = 0;
-	skb_shinfo(skb)->tso_segs = 0;
-	skb_shinfo(skb)->frag_list = NULL;
+	if (fclone) {
+		struct sk_buff *child = skb + 1;
+		atomic_t *fclone_ref = (atomic_t *) (child + 1);
+
+		skb->fclone = SKB_FCLONE_ORIG;
+		atomic_set(fclone_ref, 1);
+
+		child->fclone = SKB_FCLONE_UNAVAILABLE;
+	}
 out:
 	return skb;
 nodata:
-	kmem_cache_free(skbuff_head_cache, skb);
+	kmem_cache_free(cache, skb);
 	skb = NULL;
 	goto out;
 }
@@ -440,6 +459,10 @@ struct sk_buff *skb_clone(struct sk_buff
 #endif
 
 #endif
+#ifdef CONFIG_XEN
+	C(proto_data_valid);
+	C(proto_csum_blank);
+#endif
 	C(truesize);
 	atomic_set(&n->users, 1);
 	C(head);
