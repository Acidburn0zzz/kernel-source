From: kernel.org
Subject: 2.6.25-rc7

## Automatically generated incremental diff
## From:   linux-2.6.25-rc6
## To:     linux-2.6.25-rc7
## Robot:  $Id: make-incremental-diff,v 1.12 2004/01/06 07:19:36 hpa Exp $

Automatically created from "patches.kernel.org/patch-2.6.25-rc6-rc7" by xen-port-patches.py
Acked-by: jbeulich@novell.com

Index: head-2008-04-02/arch/x86/kernel/e820_32-xen.c
===================================================================
--- head-2008-04-02.orig/arch/x86/kernel/e820_32-xen.c	2008-04-14 11:00:03.000000000 +0200
+++ head-2008-04-02/arch/x86/kernel/e820_32-xen.c	2008-04-14 11:00:10.000000000 +0200
@@ -833,6 +833,33 @@ static int __init parse_memmap(char *arg
 early_param("memmap", parse_memmap);
 
 #ifndef CONFIG_XEN
+void __init update_memory_range(u64 start, u64 size, unsigned old_type,
+				unsigned new_type)
+{
+	int i;
+
+	BUG_ON(old_type == new_type);
+
+	for (i = 0; i < e820.nr_map; i++) {
+		struct e820entry *ei = &e820.map[i];
+		u64 final_start, final_end;
+		if (ei->type != old_type)
+			continue;
+		/* totally covered? */
+		if (ei->addr >= start && ei->size <= size) {
+			ei->type = new_type;
+			continue;
+		}
+		/* partially covered */
+		final_start = max(start, ei->addr);
+		final_end = min(start + size, ei->addr + ei->size);
+		if (final_start >= final_end)
+			continue;
+		add_memory_region(final_start, final_end - final_start,
+					 new_type);
+	}
+}
+
 void __init update_e820(void)
 {
 	u8 nr_map;
Index: head-2008-04-02/arch/x86/kernel/e820_64-xen.c
===================================================================
--- head-2008-04-02.orig/arch/x86/kernel/e820_64-xen.c	2008-04-14 11:00:03.000000000 +0200
+++ head-2008-04-02/arch/x86/kernel/e820_64-xen.c	2008-04-02 15:00:51.000000000 +0200
@@ -831,6 +831,33 @@ void __init finish_e820_parsing(void)
 }
 
 #ifndef CONFIG_XEN
+void __init update_memory_range(u64 start, u64 size, unsigned old_type,
+				unsigned new_type)
+{
+	int i;
+
+	BUG_ON(old_type == new_type);
+
+	for (i = 0; i < e820.nr_map; i++) {
+		struct e820entry *ei = &e820.map[i];
+		u64 final_start, final_end;
+		if (ei->type != old_type)
+			continue;
+		/* totally covered? */
+		if (ei->addr >= start && ei->size <= size) {
+			ei->type = new_type;
+			continue;
+		}
+		/* partially covered */
+		final_start = max(start, ei->addr);
+		final_end = min(start + size, ei->addr + ei->size);
+		if (final_start >= final_end)
+			continue;
+		add_memory_region(final_start, final_end - final_start,
+					 new_type);
+	}
+}
+
 void __init update_e820(void)
 {
 	u8 nr_map;
Index: head-2008-04-02/arch/x86/kernel/quirks-xen.c
===================================================================
--- head-2008-04-02.orig/arch/x86/kernel/quirks-xen.c	2008-04-14 11:00:03.000000000 +0200
+++ head-2008-04-02/arch/x86/kernel/quirks-xen.c	2008-04-02 14:58:58.000000000 +0200
@@ -361,6 +361,8 @@ DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_N
 			nvidia_force_enable_hpet);
 
 /* LPC bridges */
+DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_NVIDIA, 0x0260,
+			nvidia_force_enable_hpet);
 DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_NVIDIA, 0x0360,
 			nvidia_force_enable_hpet);
 DECLARE_PCI_FIXUP_HEADER(PCI_VENDOR_ID_NVIDIA, 0x0361,
Index: head-2008-04-02/arch/x86/kernel/setup64-xen.c
===================================================================
--- head-2008-04-02.orig/arch/x86/kernel/setup64-xen.c	2008-04-14 11:00:03.000000000 +0200
+++ head-2008-04-02/arch/x86/kernel/setup64-xen.c	2008-04-02 14:58:58.000000000 +0200
@@ -153,14 +153,16 @@ void __init setup_per_cpu_areas(void)
 	printk(KERN_INFO "PERCPU: Allocating %lu bytes of per cpu data\n", size);
 	for_each_cpu_mask (i, cpu_possible_map) {
 		char *ptr;
+#ifndef CONFIG_NEED_MULTIPLE_NODES
+		ptr = alloc_bootmem_pages(size);
+#else
+		int node = early_cpu_to_node(i);
 
-		if (!NODE_DATA(early_cpu_to_node(i))) {
-			printk("cpu with no node %d, num_online_nodes %d\n",
-			       i, num_online_nodes());
+		if (!node_online(node) || !NODE_DATA(node))
 			ptr = alloc_bootmem_pages(size);
-		} else { 
-			ptr = alloc_bootmem_pages_node(NODE_DATA(early_cpu_to_node(i)), size);
-		}
+		else
+			ptr = alloc_bootmem_pages_node(NODE_DATA(node), size);
+#endif
 		if (!ptr)
 			panic("Cannot allocate cpu data for CPU %d\n", i);
 		cpu_pda(i)->data_offset = ptr - __per_cpu_start;
Index: head-2008-04-02/arch/x86/mm/ioremap-xen.c
===================================================================
--- head-2008-04-02.orig/arch/x86/mm/ioremap-xen.c	2008-04-14 11:00:03.000000000 +0200
+++ head-2008-04-02/arch/x86/mm/ioremap-xen.c	2008-04-02 15:58:11.000000000 +0200
@@ -262,7 +262,7 @@ static int ioremap_change_attr(unsigned 
  * have to convert them into an offset in a page-aligned mapping, but the
  * caller shouldn't need to know that small detail.
  */
-static void __iomem *__ioremap(unsigned long phys_addr, unsigned long size,
+static void __iomem *__ioremap(resource_size_t phys_addr, unsigned long size,
 			       enum ioremap_mode mode)
 {
 	unsigned long mfn, offset, last_addr, vaddr;
@@ -359,13 +359,13 @@ static void __iomem *__ioremap(unsigned 
  *
  * Must be freed with iounmap.
  */
-void __iomem *ioremap_nocache(unsigned long phys_addr, unsigned long size)
+void __iomem *ioremap_nocache(resource_size_t phys_addr, unsigned long size)
 {
 	return __ioremap(phys_addr, size, IOR_MODE_UNCACHED);
 }
 EXPORT_SYMBOL(ioremap_nocache);
 
-void __iomem *ioremap_cache(unsigned long phys_addr, unsigned long size)
+void __iomem *ioremap_cache(resource_size_t phys_addr, unsigned long size)
 {
 	return __ioremap(phys_addr, size, IOR_MODE_CACHED);
 }
Index: head-2008-04-02/include/asm-x86/mach-xen/asm/e820_64.h
===================================================================
--- head-2008-04-02.orig/include/asm-x86/mach-xen/asm/e820_64.h	2008-04-14 11:00:03.000000000 +0200
+++ head-2008-04-02/include/asm-x86/mach-xen/asm/e820_64.h	2008-04-02 14:58:58.000000000 +0200
@@ -18,6 +18,8 @@ extern unsigned long find_e820_area(unsi
 				    unsigned size, unsigned long align);
 extern void add_memory_region(unsigned long start, unsigned long size, 
 			      int type);
+extern void update_memory_range(u64 start, u64 size, unsigned old_type,
+				unsigned new_type);
 extern void setup_memory_region(void);
 extern void contig_e820_setup(void); 
 extern unsigned long e820_end_of_ram(void);
Index: head-2008-04-02/include/asm-x86/mach-xen/asm/io_32.h
===================================================================
--- head-2008-04-02.orig/include/asm-x86/mach-xen/asm/io_32.h	2008-04-14 11:00:03.000000000 +0200
+++ head-2008-04-02/include/asm-x86/mach-xen/asm/io_32.h	2008-04-02 14:58:58.000000000 +0200
@@ -127,13 +127,13 @@ static inline void * phys_to_virt(unsign
  * If the area you are trying to map is a PCI BAR you should have a
  * look at pci_iomap().
  */
-extern void __iomem *ioremap_nocache(unsigned long offset, unsigned long size);
-extern void __iomem *ioremap_cache(unsigned long offset, unsigned long size);
+extern void __iomem *ioremap_nocache(resource_size_t offset, unsigned long size);
+extern void __iomem *ioremap_cache(resource_size_t offset, unsigned long size);
 
 /*
  * The default ioremap() behavior is non-cached:
  */
-static inline void __iomem *ioremap(unsigned long offset, unsigned long size)
+static inline void __iomem *ioremap(resource_size_t offset, unsigned long size)
 {
 	return ioremap_nocache(offset, size);
 }
Index: head-2008-04-02/include/asm-x86/mach-xen/asm/io_64.h
===================================================================
--- head-2008-04-02.orig/include/asm-x86/mach-xen/asm/io_64.h	2008-04-14 11:00:03.000000000 +0200
+++ head-2008-04-02/include/asm-x86/mach-xen/asm/io_64.h	2008-04-02 14:58:58.000000000 +0200
@@ -178,13 +178,13 @@ extern void early_iounmap(void *addr, un
  * it's useful if some control registers are in such an area and write combining
  * or read caching is not desirable:
  */
-extern void __iomem *ioremap_nocache(unsigned long offset, unsigned long size);
-extern void __iomem *ioremap_cache(unsigned long offset, unsigned long size);
+extern void __iomem *ioremap_nocache(resource_size_t offset, unsigned long size);
+extern void __iomem *ioremap_cache(resource_size_t offset, unsigned long size);
 
 /*
  * The default ioremap() behavior is non-cached:
  */
-static inline void __iomem *ioremap(unsigned long offset, unsigned long size)
+static inline void __iomem *ioremap(resource_size_t offset, unsigned long size)
 {
 	return ioremap_nocache(offset, size);
 }
Index: head-2008-04-02/include/asm-x86/mach-xen/asm/page.h
===================================================================
--- head-2008-04-02.orig/include/asm-x86/mach-xen/asm/page.h	2008-04-14 11:00:03.000000000 +0200
+++ head-2008-04-02/include/asm-x86/mach-xen/asm/page.h	2008-04-02 15:53:32.000000000 +0200
@@ -62,13 +62,13 @@ extern int page_is_ram(unsigned long pag
 
 struct page;
 
-static void inline clear_user_page(void *page, unsigned long vaddr,
+static inline void clear_user_page(void *page, unsigned long vaddr,
 				struct page *pg)
 {
 	clear_page(page);
 }
 
-static void inline copy_user_page(void *to, void *from, unsigned long vaddr,
+static inline void copy_user_page(void *to, void *from, unsigned long vaddr,
 				struct page *topage)
 {
 	copy_page(to, from);
