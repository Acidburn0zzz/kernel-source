From: Jiri Slaby <jslaby@suse.cz>
Subject: Linux 3.2.8
Patch-mainline: 3.2.8
Git-commit: 34ddc81a230b15c0e345b6b253049db731499f7e
Git-commit: f94edacf998516ac9d849f7bc6949a703977a7f3
Git-commit: 4903062b5485f0e2c286a23b44c9b59d9b017d53
Git-commit: b3b0870ef3ffed72b92415423da864f440f57ad6
Git-commit: 6d59d7a9f5b723a7ac1925c136e93ec83c0c3043
Git-commit: b6c66418dcad0fcf83cd1d0a39482db37bf4fc41
Git-commit: 15d8791cae75dca27bfda8ecfe87dca9379d6bb0
Git-commit: c38e23456278e967f094b08247ffc3711b1029b2
Git-commit: 5b1cbac37798805c1fee18c8cebe5c0a13975b17
Git-commit: be98c2cdb15ba26148cd2bd58a857d4f7759ed38

Signed-off-by: Jiri Slaby <jslaby@suse.cz>
Automatically created from "patches.kernel.org/patch-3.2.7-8" by xen-port-patches.py. And made to apply cleanly by js.

---
 arch/x86/include/mach-xen/asm/i387.h      |   55 ++++++++++++++++++++++++++++++
 arch/x86/include/mach-xen/asm/processor.h |    7 ++-
 arch/x86/include/mach-xen/asm/xor_64.h    |    6 ++-
 arch/x86/kernel/process_32-xen.c          |   43 ++---------------------
 arch/x86/kernel/process_64-xen.c          |   46 ++-----------------------
 arch/x86/kernel/traps-xen.c               |   30 +++++++---------
 6 files changed, 84 insertions(+), 103 deletions(-)

--- /dev/null
+++ b/arch/x86/include/mach-xen/asm/i387.h
@@ -0,0 +1,55 @@
+#ifndef _ASM_X86_I387_H
+#define switch_fpu_prepare native_switch_fpu_prepare
+#include_next <asm/i387.h>
+
+#ifndef __ASSEMBLY__
+static inline void xen_thread_fpu_begin(struct task_struct *tsk,
+					multicall_entry_t *mcl)
+{
+	if (mcl) {
+		mcl->op = __HYPERVISOR_fpu_taskswitch;
+		mcl->args[0] = 0;
+	}
+	__thread_set_has_fpu(tsk);
+}
+
+static inline fpu_switch_t xen_switch_fpu_prepare(struct task_struct *old,
+						  struct task_struct *new,
+						  int cpu,
+						  multicall_entry_t **mcl)
+{
+	fpu_switch_t fpu;
+
+	fpu.preload = tsk_used_math(new) && new->fpu_counter > 5;
+	if (__thread_has_fpu(old)) {
+		if (!__save_init_fpu(old))
+			cpu = ~0;
+		old->thread.fpu.last_cpu = cpu;
+		old->thread.fpu.has_fpu = 0;	/* But leave fpu_owner_task! */
+
+		/* Don't change CR0.TS if we just switch! */
+		if (fpu.preload) {
+			new->fpu_counter++;
+			__thread_set_has_fpu(new);
+			prefetch(new->thread.fpu.state);
+		} else {
+			(*mcl)->op = __HYPERVISOR_fpu_taskswitch;
+			(*mcl)++->args[0] = 1;
+		}
+	} else {
+		old->fpu_counter = 0;
+		old->thread.fpu.last_cpu = ~0;
+		if (fpu.preload) {
+			new->fpu_counter++;
+			if (fpu_lazy_restore(new))
+				fpu.preload = 0;
+			else
+				prefetch(new->thread.fpu.state);
+			xen_thread_fpu_begin(new, (*mcl)++);
+		}
+	}
+	return fpu;
+}
+#endif /* __ASSEMBLY__ */
+
+#endif /* _ASM_X86_I387_H */
--- a/arch/x86/include/mach-xen/asm/processor.h
+++ b/arch/x86/include/mach-xen/asm/processor.h
@@ -109,7 +109,7 @@ struct cpuinfo_x86 {
 	u16			initial_apicid;
 #endif
 	u16			x86_clflush_size;
-#ifdef CONFIG_X86_HT
+#ifndef CONFIG_XEN
 	/* number of cores as seen by the OS: */
 	u16			booted_cores;
 	/* Physical processor id: */
@@ -119,10 +119,8 @@ struct cpuinfo_x86 {
 	/* Compute unit id */
 	u8			compute_unit_id;
 #endif
-#ifdef CONFIG_SMP
 	/* Index into per_cpu list: */
 	u16			cpu_index;
-#endif
 #ifndef CONFIG_XEN
 	u32			microcode;
 #endif
@@ -394,6 +392,8 @@ union thread_xstate {
 };
 
 struct fpu {
+	unsigned int last_cpu;
+	unsigned int has_fpu;
 	union thread_xstate *state;
 };
 
@@ -475,6 +475,7 @@ struct thread_struct {
 	unsigned long		trap_no;
 	unsigned long		error_code;
 	/* floating point and extended processor state */
+	unsigned long		has_fpu;
 	struct fpu		fpu;
 #ifdef CONFIG_X86_32
 	/* Virtual 86 mode info */
--- a/arch/x86/include/mach-xen/asm/xor_64.h
+++ b/arch/x86/include/mach-xen/asm/xor_64.h
@@ -1,6 +1,8 @@
 #ifndef _ASM_X86_XOR_64_H
 #define _ASM_X86_XOR_64_H
 
+#include <asm/i387.h>
+
 /*
  * x86-64 changes / gcc fixes from Andi Kleen.
  * Copyright 2002 Andi Kleen, SuSE Labs.
@@ -18,7 +20,7 @@ typedef struct {
 #define XMMS_SAVE				\
 do {						\
 	preempt_disable();			\
-	if (!(current_thread_info()->status & TS_USEDFPU))	\
+	if (!__thread_has_fpu(current))		\
 		clts();				\
 	asm volatile(				\
 		"movups %%xmm0,(%1)	;\n\t"	\
@@ -41,7 +43,7 @@ do {						\
 		:				\
 		: "r" (cr0), "r" (xmm_save)	\
 		: "memory");			\
-	if (!(current_thread_info()->status & TS_USEDFPU))	\
+	if (!__thread_has_fpu(current))		\
 		stts();				\
 	preempt_enable();			\
 } while (0)
--- a/arch/x86/kernel/process_32-xen.c
+++ b/arch/x86/kernel/process_32-xen.c
@@ -305,7 +305,7 @@ __switch_to(struct task_struct *prev_p,
 #ifndef CONFIG_X86_NO_TSS
 	struct tss_struct *tss = &per_cpu(init_tss, cpu);
 #endif
-	bool preload_fpu;
+	fpu_switch_t fpu;
 #if CONFIG_XEN_COMPAT > 0x030002
 	struct physdev_set_iopl iopl_op;
 	struct physdev_set_iobitmap iobmp_op;
@@ -318,29 +318,7 @@ __switch_to(struct task_struct *prev_p,
 
 	/* XEN NOTE: FS/GS saved in switch_mm(), not here. */
 
-	/*
-	 * If the task has used fpu the last 5 timeslices, just do a full
-	 * restore of the math state immediately to avoid the trap; the
-	 * chances of needing FPU soon are obviously high now
-	 */
-	preload_fpu = tsk_used_math(next_p) && next_p->fpu_counter > 5;
-
-	/*
-	 * This is basically '__unlazy_fpu', except that we queue a
-	 * multicall to indicate FPU task switch, rather than
-	 * synchronously trapping to Xen.
-	 */
-	if (task_thread_info(prev_p)->status & TS_USEDFPU) {
-		__save_init_fpu(prev_p); /* _not_ save_init_fpu() */
-		if (!preload_fpu) {
-			mcl->op      = __HYPERVISOR_fpu_taskswitch;
-			mcl->args[0] = 1;
-			mcl++;
-		}
-	}
-#if 0 /* lazy fpu sanity check */
-	else BUG_ON(!(read_cr0() & 8));
-#endif
+	fpu = xen_switch_fpu_prepare(prev_p, next_p, cpu, &mcl);
 
 	/*
 	 * Reload sp0.
@@ -382,14 +360,6 @@ __switch_to(struct task_struct *prev_p,
 		mcl++;
 	}
 
-	/* If we're going to preload the fpu context, make sure clts
-	   is run while we're batching the cpu state updates. */
-	if (preload_fpu) {
-		mcl->op      = __HYPERVISOR_fpu_taskswitch;
-		mcl->args[0] = 0;
-		mcl++;
-	}
-
 	if (unlikely(prev->io_bitmap_ptr || next->io_bitmap_ptr)) {
 		set_xen_guest_handle(iobmp_op.bitmap,
 				     (char *)next->io_bitmap_ptr);
@@ -413,10 +383,6 @@ __switch_to(struct task_struct *prev_p,
 	if (unlikely(HYPERVISOR_multicall_check(_mcl, mcl - _mcl, NULL)))
 		BUG();
 
-	/* we're going to use this soon, after a few expensive things */
-	if (preload_fpu)
-		prefetch(next->fpu.state);
-
 	/*
 	 * Now maybe handle debug registers
 	 */
@@ -433,15 +399,14 @@ __switch_to(struct task_struct *prev_p,
 	 */
 	arch_end_context_switch(next_p);
 
-	if (preload_fpu)
-		__math_state_restore();
-
 	/*
 	 * Restore %gs if needed (which is common)
 	 */
 	if (prev->gs | next->gs)
 		lazy_load_gs(next->gs);
 
+	switch_fpu_finish(next_p, fpu);
+
 	percpu_write(current_task, next_p);
 
 	return prev_p;
--- a/arch/x86/kernel/process_64-xen.c
+++ b/arch/x86/kernel/process_64-xen.c
@@ -392,7 +392,7 @@ __switch_to(struct task_struct *prev_p,
 #ifndef CONFIG_X86_NO_TSS
 	struct tss_struct *tss = &per_cpu(init_tss, cpu);
 #endif
-	bool preload_fpu;
+	fpu_switch_t fpu;
 #if CONFIG_XEN_COMPAT > 0x030002
 	struct physdev_set_iopl iopl_op;
 	struct physdev_set_iobitmap iobmp_op;
@@ -403,40 +403,7 @@ __switch_to(struct task_struct *prev_p,
 #endif
 	multicall_entry_t _mcl[8], *mcl = _mcl;
 
-	/*
-	 * If the task has used fpu the last 5 timeslices, just do a full
-	 * restore of the math state immediately to avoid the trap; the
-	 * chances of needing FPU soon are obviously high now
-	 */
-	preload_fpu = tsk_used_math(next_p) && next_p->fpu_counter > 5;
-
-	/* we're going to use this soon, after a few expensive things */
-	if (preload_fpu)
-		prefetch(next->fpu.state);
-
-	/*
-	 * This is basically '__unlazy_fpu', except that we queue a
-	 * multicall to indicate FPU task switch, rather than
-	 * synchronously trapping to Xen.
-	 * The AMD workaround requires it to be after DS reload, or
-	 * after DS has been cleared, which we do in __prepare_arch_switch.
-	 */
-	if (task_thread_info(prev_p)->status & TS_USEDFPU) {
-		__save_init_fpu(prev_p); /* _not_ save_init_fpu() */
-		if (!preload_fpu) {
-			mcl->op      = __HYPERVISOR_fpu_taskswitch;
-			mcl->args[0] = 1;
-			mcl++;
-		}
-	} else
-		prev_p->fpu_counter = 0;
-
-	/* Make sure cpu is ready for new context */
-	if (preload_fpu) {
-		mcl->op      = __HYPERVISOR_fpu_taskswitch;
-		mcl->args[0] = 0;
-		mcl++;
-	}
+	fpu = xen_switch_fpu_prepare(prev_p, next_p, cpu, &mcl);
 
 	/*
 	 * Reload sp0.
@@ -539,6 +506,8 @@ __switch_to(struct task_struct *prev_p,
 	if (next->gs)
 		WARN_ON(HYPERVISOR_set_segment_base(SEGBASE_GS_USER, next->gs));
 
+	switch_fpu_finish(next_p, fpu);
+
 	/*
 	 * Switch the PDA context.
 	 */
@@ -555,13 +524,6 @@ __switch_to(struct task_struct *prev_p,
 		     task_thread_info(prev_p)->flags & _TIF_WORK_CTXSW_PREV))
 		__switch_to_xtra(prev_p, next_p);
 
-	/*
-	 * Preload the FPU context, now that we've determined that the
-	 * task is likely to be using it.
-	 */
-	if (preload_fpu)
-		__math_state_restore();
-
 	return prev_p;
 }
 
--- a/arch/x86/kernel/traps-xen.c
+++ b/arch/x86/kernel/traps-xen.c
@@ -568,25 +568,20 @@ asmlinkage void __attribute__((weak)) sm
 #endif /* CONFIG_XEN */
 
 /*
- * __math_state_restore assumes that cr0.TS is already clear and the
- * fpu state is all ready for use.  Used during context switch.
+ * This gets called with the process already owning the
+ * FPU state, and with CR0.TS cleared. It just needs to
+ * restore the FPU register state.
  */
-void __math_state_restore(void)
+void __math_state_restore(struct task_struct *tsk)
 {
-	struct thread_info *thread = current_thread_info();
-	struct task_struct *tsk = thread->task;
-
 	/*
 	 * Paranoid restore. send a SIGSEGV if we fail to restore the state.
 	 */
 	if (unlikely(restore_fpu_checking(tsk))) {
-		stts();
+		__thread_fpu_end(tsk);
 		force_sig(SIGSEGV, tsk);
 		return;
 	}
-
-	thread->status |= TS_USEDFPU;	/* So we fnsave on switch_to() */
-	tsk->fpu_counter++;
 }
 
 /*
@@ -596,13 +591,12 @@ void __math_state_restore(void)
  * Careful.. There are problems with IBM-designed IRQ13 behaviour.
  * Don't touch unless you *really* know how it works.
  *
- * Must be called with kernel preemption disabled (in this case,
- * local interrupts are disabled at the call-site in entry.S).
+ * Must be called with kernel preemption disabled (eg with local
+ * local interrupts as in the case of do_device_not_available).
  */
-asmlinkage void math_state_restore(void)
+void math_state_restore(void)
 {
-	struct thread_info *thread = current_thread_info();
-	struct task_struct *tsk = thread->task;
+	struct task_struct *tsk = current;
 
 	if (!tsk_used_math(tsk)) {
 		local_irq_enable();
@@ -619,8 +613,10 @@ asmlinkage void math_state_restore(void)
 		local_irq_disable();
 	}
 
-	/* NB. 'clts' is done for us by Xen during virtual trap. */
-	__math_state_restore();
+	xen_thread_fpu_begin(tsk, NULL);
+	__math_state_restore(tsk);
+
+	tsk->fpu_counter++;
 }
 EXPORT_SYMBOL_GPL(math_state_restore);
 
