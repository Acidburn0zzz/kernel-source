From: Greg Kroah-Hartman <gregkh@suse.de>
Subject: Linux 2.6.27.12

Upstream 2.6.27.12 release from kernel.org

Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

Automatically created from "patches.kernel.org/patch-2.6.27.11-12" by xen-port-patches.py

--- head-2009-01-21.orig/arch/x86/kernel/head64-xen.c	2008-12-15 11:28:15.000000000 +0100
+++ head-2009-01-21/arch/x86/kernel/head64-xen.c	2009-01-16 10:31:56.000000000 +0100
@@ -33,7 +33,7 @@
 #include <asm/bios_ebda.h>
 
 /* boot cpu pda */
-static struct x8664_pda _boot_cpu_pda __read_mostly;
+static struct x8664_pda _boot_cpu_pda;
 
 #ifdef CONFIG_SMP
 /*
--- head-2009-01-21.orig/arch/x86/mm/fault-xen.c	2008-12-15 11:28:15.000000000 +0100
+++ head-2009-01-21/arch/x86/mm/fault-xen.c	2009-01-21 11:11:51.000000000 +0100
@@ -547,9 +547,7 @@ static int vmalloc_fault(unsigned long a
 	   happen within a race in page table update. In the later
 	   case just flush. */
 
-	/* On Xen the line below does not always work. Needs investigating! */
-	/*pgd = pgd_offset(current->mm ?: &init_mm, address);*/
-	pgd = __va(read_cr3() & PHYSICAL_PAGE_MASK);
+	pgd = pgd_offset(current->active_mm, address);
 	pgd += pgd_index(address);
 	pgd_ref = pgd_offset_k(address);
 	if (pgd_none(*pgd_ref))
--- head-2009-01-21.orig/arch/x86/mm/init_32-xen.c	2008-12-15 11:28:15.000000000 +0100
+++ head-2009-01-21/arch/x86/mm/init_32-xen.c	2009-01-16 10:31:56.000000000 +0100
@@ -773,7 +773,7 @@ static unsigned long __init extend_init_
 	return start_pfn;
 }
 
-static void __init find_early_table_space(unsigned long end)
+static void __init find_early_table_space(unsigned long end, int use_pse)
 {
 	unsigned long puds, pmds, ptes, tables;
 
@@ -783,7 +783,7 @@ static void __init find_early_table_spac
 	pmds = (end + PMD_SIZE - 1) >> PMD_SHIFT;
 	tables += PAGE_ALIGN(pmds * sizeof(pmd_t));
 
-	if (cpu_has_pse) {
+	if (use_pse) {
 		unsigned long extra;
 
 		extra = end - ((end>>PMD_SHIFT) << PMD_SHIFT);
@@ -816,12 +816,22 @@ unsigned long __init_refok init_memory_m
 	pgd_t *pgd_base = swapper_pg_dir;
 	unsigned long start_pfn, end_pfn;
 	unsigned long big_page_start;
+#ifdef CONFIG_DEBUG_PAGEALLOC
+	/*
+	 * For CONFIG_DEBUG_PAGEALLOC, identity mapping will use small pages.
+	 * This will simplify cpa(), which otherwise needs to support splitting
+	 * large pages into small in interrupt context, etc.
+	 */
+	int use_pse = 0;
+#else
+	int use_pse = cpu_has_pse;
+#endif
 
 	/*
 	 * Find space for the kernel direct mapping tables.
 	 */
 	if (!after_init_bootmem)
-		find_early_table_space(end);
+		find_early_table_space(end, use_pse);
 
 #ifdef CONFIG_X86_PAE
 	set_nx();
@@ -867,7 +877,7 @@ unsigned long __init_refok init_memory_m
 	end_pfn = (end>>PMD_SHIFT) << (PMD_SHIFT - PAGE_SHIFT);
 	if (start_pfn < end_pfn)
 		kernel_physical_mapping_init(pgd_base, start_pfn, end_pfn,
-						cpu_has_pse);
+					     use_pse);
 
 	/* tail is not big page alignment ? */
 	start_pfn = end_pfn;
--- head-2009-01-21.orig/arch/x86/mm/init_64-xen.c	2009-01-07 10:53:11.000000000 +0100
+++ head-2009-01-21/arch/x86/mm/init_64-xen.c	2009-01-16 10:36:16.000000000 +0100
@@ -694,7 +694,8 @@ static void __init extend_init_mapping(u
 			      table_cur << PAGE_SHIFT, "INITMAP");
 }
 
-static void __init find_early_table_space(unsigned long end)
+static void __init find_early_table_space(unsigned long end, int use_pse,
+					  int use_gbpages)
 {
 	unsigned long puds, pmds, ptes, tables;
 
@@ -866,6 +867,7 @@ unsigned long __init_refok init_memory_m
 	bool first = !table_start;
 	struct map_range mr[NR_RANGE_MR];
 	int nr_range, i;
+	int use_pse, use_gbpages;
 
 	printk(KERN_INFO "init_memory_mapping\n");
 
@@ -879,9 +881,21 @@ unsigned long __init_refok init_memory_m
 	if (!after_bootmem)
 		init_gbpages();
 
-	if (direct_gbpages)
+#ifdef CONFIG_DEBUG_PAGEALLOC
+	/*
+	 * For CONFIG_DEBUG_PAGEALLOC, identity mapping will use small pages.
+	 * This will simplify cpa(), which otherwise needs to support splitting
+	 * large pages into small in interrupt context, etc.
+	 */
+	use_pse = use_gbpages = 0;
+#else
+	use_pse = cpu_has_pse;
+	use_gbpages = direct_gbpages;
+#endif
+
+	if (use_gbpages)
 		page_size_mask |= 1 << PG_LEVEL_1G;
-	if (cpu_has_pse)
+	if (use_pse)
 		page_size_mask |= 1 << PG_LEVEL_2M;
 
 	memset(mr, 0, sizeof(mr));
@@ -942,7 +956,7 @@ unsigned long __init_refok init_memory_m
 			 (mr[i].page_size_mask & (1<<PG_LEVEL_2M))?"2M":"4k"));
 
 	if (first)
-		find_early_table_space(end);
+		find_early_table_space(end, use_pse, use_gbpages);
 
 	for (i = 0; i < nr_range; i++)
 		last_map_addr = kernel_physical_mapping_init(
