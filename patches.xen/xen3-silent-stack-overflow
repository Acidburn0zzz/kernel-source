From: Andrea Arcangeli <andrea@suse.de>
Subject: avoid silent stack overflow over the heap
Patch-mainline: no
References: SUSE44807

x

Signed-off-by: Andrea Arcangeli <andrea@suse.de>

Automatically created from "patches.suse/silent-stack-overflow" by xen-port-patches.py
Further porting to arch/xen by jbeulich

Index: linux-2.6.12/arch/xen/i386/mm/fault.c
===================================================================
--- linux-2.6.12.orig/arch/xen/i386/mm/fault.c
+++ linux-2.6.12/arch/xen/i386/mm/fault.c
@@ -275,9 +275,9 @@ fastcall void do_page_fault(struct pt_re
 			      unsigned long address)
 {
 	struct task_struct *tsk;
 	struct mm_struct *mm;
-	struct vm_area_struct * vma;
+	struct vm_area_struct *vma, *prev_vma;
 	int write;
 	siginfo_t info;
 
 	/* Set the "privileged fault" bit to something sane. */
@@ -369,9 +369,15 @@ fastcall void do_page_fault(struct pt_re
 		 */
 		if (address + 32 < regs->esp)
 			goto bad_area;
 	}
-	if (expand_stack(vma, address))
+	/*
+	 * find_vma_prev is just a bit slower, because it cannot
+	 * use the mmap_cache, so we run it only in the growsdown
+	 * slow path and we leave find_vma in the fast path.
+	 */
+	find_vma_prev(current->mm, address, &prev_vma);
+	if (expand_stack(vma, address, prev_vma))
 		goto bad_area;
 /*
  * Ok, we have a good vm_area for this memory access, so
  * we can handle it..
Index: linux-2.6.12/arch/xen/x86_64/mm/fault.c
===================================================================
--- linux-2.6.12.orig/arch/xen/x86_64/mm/fault.c
+++ linux-2.6.12/arch/xen/x86_64/mm/fault.c
@@ -315,9 +315,9 @@ asmlinkage void do_page_fault(struct pt_
        unsigned long address)
 {
 	struct task_struct *tsk;
 	struct mm_struct *mm;
-	struct vm_area_struct * vma;
+	struct vm_area_struct *vma, *prev_vma;
 	const struct exception_table_entry *fixup;
 	int write;
 	siginfo_t info;
 
@@ -423,9 +423,15 @@ asmlinkage void do_page_fault(struct pt_
 		// XXX: align red zone size with ABI 
 		if (address + 128 < regs->rsp)
 			goto bad_area;
 	}
-	if (expand_stack(vma, address))
+	/*
+	 * find_vma_prev is just a bit slower, because it cannot
+	 * use the mmap_cache, so we run it only in the growsdown
+	 * slow path and we leave find_vma in the fast path.
+	 */
+	find_vma_prev(current->mm, address, &prev_vma);
+	if (expand_stack(vma, address, prev_vma))
 		goto bad_area;
 /*
  * Ok, we have a good vm_area for this memory access, so
  * we can handle it..
