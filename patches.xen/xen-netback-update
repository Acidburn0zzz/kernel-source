Subject: update network backend driver to xen 3.0.0 release level.
From: kraxel@suse.de

Index: linux-2.6.14/drivers/xen/netback/common.h
===================================================================
--- linux-2.6.14.orig/drivers/xen/netback/common.h	2005-12-05 11:49:01.000000000 +0100
+++ linux-2.6.14/drivers/xen/netback/common.h	2005-12-06 12:09:02.000000000 +0100
@@ -45,24 +45,20 @@ typedef struct netif_st {
 	u8               fe_dev_addr[6];
 
 	/* Physical parameters of the comms window. */
-	u16              tx_shmem_handle;
+	grant_handle_t   tx_shmem_handle;
 	grant_ref_t      tx_shmem_ref; 
-	u16              rx_shmem_handle;
+	grant_handle_t   rx_shmem_handle;
 	grant_ref_t      rx_shmem_ref; 
 	unsigned int     evtchn;
 	unsigned int     irq;
 
 	/* The shared rings and indexes. */
-	netif_tx_interface_t *tx;
-	netif_rx_interface_t *rx;
+	netif_tx_back_ring_t tx;
+	netif_rx_back_ring_t rx;
 	struct vm_struct *comms_area;
 
-	/* Private indexes into shared ring. */
-	NETIF_RING_IDX rx_req_cons;
-	NETIF_RING_IDX rx_resp_prod; /* private version of shared variable */
-	NETIF_RING_IDX rx_resp_prod_copy;
-	NETIF_RING_IDX tx_req_cons;
-	NETIF_RING_IDX tx_resp_prod; /* private version of shared variable */
+	/* Allow netif_be_start_xmit() to peek ahead in the rx request ring. */
+	RING_IDX rx_req_cons_peek;
 
 	/* Transmit shaping: allow 'credit_bytes' every 'credit_usec'. */
 	unsigned long   credit_bytes;
@@ -81,6 +77,9 @@ typedef struct netif_st {
 	struct work_struct free_work;
 } netif_t;
 
+#define NET_TX_RING_SIZE __RING_SIZE((netif_tx_sring_t *)0, PAGE_SIZE)
+#define NET_RX_RING_SIZE __RING_SIZE((netif_rx_sring_t *)0, PAGE_SIZE)
+
 void netif_creditlimit(netif_t *netif);
 int  netif_disconnect(netif_t *netif);
 
Index: linux-2.6.14/drivers/xen/netback/interface.c
===================================================================
--- linux-2.6.14.orig/drivers/xen/netback/interface.c	2005-12-05 11:49:01.000000000 +0100
+++ linux-2.6.14/drivers/xen/netback/interface.c	2005-12-06 12:09:02.000000000 +0100
@@ -127,9 +127,9 @@ static int map_frontend_pages(
 	unlock_vm_area(netif->comms_area);
 	BUG_ON(ret);
 
-	if (op.handle < 0) { 
+	if (op.status) { 
 		DPRINTK(" Gnttab failure mapping tx_ring_ref!\n");
-		return op.handle;
+		return op.status;
 	}
 
 	netif->tx_shmem_ref    = tx_ring_ref;
@@ -145,9 +145,9 @@ static int map_frontend_pages(
 	unlock_vm_area(netif->comms_area);
 	BUG_ON(ret);
 
-	if (op.handle < 0) { 
+	if (op.status) {
 		DPRINTK(" Gnttab failure mapping rx_ring_ref!\n");
-		return op.handle;
+		return op.status;
 	}
 
 	netif->rx_shmem_ref    = rx_ring_ref;
@@ -184,6 +184,8 @@ int netif_map(netif_t *netif, unsigned l
 	      unsigned long rx_ring_ref, unsigned int evtchn)
 {
 	int err;
+	netif_tx_sring_t *txs;
+	netif_rx_sring_t *rxs;
 	evtchn_op_t op = {
 		.cmd = EVTCHNOP_bind_interdomain,
 		.u.bind_interdomain.remote_dom = netif->domid,
@@ -216,10 +218,15 @@ int netif_map(netif_t *netif, unsigned l
 		netif->evtchn, netif_be_int, 0, netif->dev->name, netif);
 	disable_irq(netif->irq);
 
-	netif->tx = (netif_tx_interface_t *)netif->comms_area->addr;
-	netif->rx = (netif_rx_interface_t *)
+	txs = (netif_tx_sring_t *)netif->comms_area->addr;
+	BACK_RING_INIT(&netif->tx, txs, PAGE_SIZE);
+
+	rxs = (netif_rx_sring_t *)
 		((char *)netif->comms_area->addr + PAGE_SIZE);
-	netif->tx->resp_prod = netif->rx->resp_prod = 0;
+	BACK_RING_INIT(&netif->rx, rxs, PAGE_SIZE);
+
+	netif->rx_req_cons_peek = 0;
+
 	netif_get(netif);
 	wmb(); /* Other CPUs see new state before interface is started. */
 
@@ -246,7 +253,7 @@ static void free_netif_callback(void *ar
 
 	unregister_netdev(netif->dev);
 
-	if (netif->tx) {
+	if (netif->tx.sring) {
 		unmap_frontend_pages(netif);
 		free_vm_area(netif->comms_area);
 	}
Index: linux-2.6.14/drivers/xen/netback/netback.c
===================================================================
--- linux-2.6.14.orig/drivers/xen/netback/netback.c	2005-12-05 11:49:01.000000000 +0100
+++ linux-2.6.14/drivers/xen/netback/netback.c	2005-12-06 12:09:02.000000000 +0100
@@ -25,7 +25,7 @@ static int  make_rx_response(netif_t *ne
                              s8       st,
                              u16      offset,
                              u16      size,
-                             u16      csum_valid);
+                             u16      flags);
 
 static void net_tx_action(unsigned long unused);
 static DECLARE_TASKLET(net_tx_tasklet, net_tx_action, 0);
@@ -38,15 +38,12 @@ static struct timer_list net_timer;
 #define MAX_PENDING_REQS 256
 
 static struct sk_buff_head rx_queue;
-static multicall_entry_t rx_mcl[NETIF_RX_RING_SIZE*2+1];
-static mmu_update_t rx_mmu[NETIF_RX_RING_SIZE];
+static multicall_entry_t rx_mcl[NET_RX_RING_SIZE*2+1];
+static mmu_update_t rx_mmu[NET_RX_RING_SIZE];
 
 static gnttab_transfer_t grant_rx_op[MAX_PENDING_REQS];
 static unsigned char rx_notify[NR_IRQS];
 
-/* Don't currently gate addition of an interface to the tx scheduling list. */
-#define tx_work_exists(_if) (1)
-
 static unsigned long mmap_vstart;
 #define MMAP_VADDR(_req) (mmap_vstart + ((_req) * PAGE_SIZE))
 
@@ -68,7 +65,7 @@ static PEND_RING_IDX dealloc_prod, deall
 
 static struct sk_buff_head tx_queue;
 
-static u16 grant_tx_ref[MAX_PENDING_REQS];
+static grant_handle_t grant_tx_handle[MAX_PENDING_REQS];
 static gnttab_unmap_grant_ref_t tx_unmap_ops[MAX_PENDING_REQS];
 static gnttab_map_grant_ref_t tx_map_ops[MAX_PENDING_REQS];
 
@@ -126,8 +123,9 @@ int netif_be_start_xmit(struct sk_buff *
 
 	/* Drop the packet if the target domain has no receive buffers. */
 	if (!netif->active || 
-	    (netif->rx_req_cons == netif->rx->req_prod) ||
-	    ((netif->rx_req_cons-netif->rx_resp_prod) == NETIF_RX_RING_SIZE))
+	    (netif->rx_req_cons_peek == netif->rx.sring->req_prod) ||
+	    ((netif->rx_req_cons_peek - netif->rx.rsp_prod_pvt) ==
+	     NET_RX_RING_SIZE))
 		goto drop;
 
 	/*
@@ -154,7 +152,7 @@ int netif_be_start_xmit(struct sk_buff *
 		skb = nskb;
 	}
 
-	netif->rx_req_cons++;
+	netif->rx_req_cons_peek++;
 	netif_get(netif);
 
 	skb_queue_tail(&rx_queue, skb);
@@ -198,7 +196,7 @@ static void net_rx_action(unsigned long 
 	unsigned long vdata, old_mfn, new_mfn;
 	struct sk_buff_head rxq;
 	struct sk_buff *skb;
-	u16 notify_list[NETIF_RX_RING_SIZE];
+	u16 notify_list[NET_RX_RING_SIZE];
 	int notify_nr = 0;
 	int ret;
 
@@ -233,9 +231,9 @@ static void net_rx_action(unsigned long 
 
 		gop->mfn = old_mfn;
 		gop->domid = netif->domid;
-		gop->ref = netif->rx->ring[
-			MASK_NETIF_RX_IDX(netif->rx_resp_prod_copy)].req.gref;
-		netif->rx_resp_prod_copy++;
+		gop->ref = RING_GET_REQUEST(
+			&netif->rx, netif->rx.req_cons)->gref;
+		netif->rx.req_cons++;
 		gop++;
 
 		mmu->ptr = ((maddr_t)new_mfn << PAGE_SHIFT) |
@@ -300,11 +298,11 @@ static void net_rx_action(unsigned long 
 			status = NETIF_RSP_ERROR; 
 		}
 		irq = netif->irq;
-		id = netif->rx->ring[
-			MASK_NETIF_RX_IDX(netif->rx_resp_prod)].req.id;
+		id = RING_GET_REQUEST(&netif->rx, netif->rx.rsp_prod_pvt)->id;
 		if (make_rx_response(netif, id, status,
 				     (unsigned long)skb->data & ~PAGE_MASK,
-				     size, skb->proto_csum_valid) &&
+				     size, skb->proto_csum_valid ?
+				     NETRXF_csum_valid : 0) &&
 		    (rx_notify[irq] == 0)) {
 			rx_notify[irq] = 1;
 			notify_list[notify_nr++] = irq;
@@ -371,10 +369,25 @@ static void add_to_net_schedule_list_tai
 	spin_unlock_irq(&net_schedule_list_lock);
 }
 
+/*
+ * Note on CONFIG_XEN_NETDEV_PIPELINED_TRANSMITTER:
+ * If this driver is pipelining transmit requests then we can be very
+ * aggressive in avoiding new-packet notifications -- frontend only needs to
+ * send a notification if there are no outstanding unreceived responses.
+ * If we may be buffer transmit buffers for any reason then we must be rather
+ * more conservative and treat this as the final check for pending work.
+ */
 void netif_schedule_work(netif_t *netif)
 {
-	if ((netif->tx_req_cons != netif->tx->req_prod) &&
-	    ((netif->tx_req_cons-netif->tx_resp_prod) != NETIF_TX_RING_SIZE)) {
+	int more_to_do;
+
+#ifdef CONFIG_XEN_NETDEV_PIPELINED_TRANSMITTER
+	more_to_do = RING_HAS_UNCONSUMED_REQUESTS(&netif->tx);
+#else
+	RING_FINAL_CHECK_FOR_REQUESTS(&netif->tx, more_to_do);
+#endif
+
+	if (more_to_do) {
 		add_to_net_schedule_list_tail(netif);
 		maybe_schedule_tx_action();
 	}
@@ -412,7 +425,7 @@ inline static void net_tx_action_dealloc
 		pending_idx = dealloc_ring[MASK_PEND_IDX(dc++)];
 		gop->host_addr    = MMAP_VADDR(pending_idx);
 		gop->dev_bus_addr = 0;
-		gop->handle       = grant_tx_ref[pending_idx];
+		gop->handle       = grant_tx_handle[pending_idx];
 		gop++;
 	}
 	ret = HYPERVISOR_grant_table_op(
@@ -429,19 +442,6 @@ inline static void net_tx_action_dealloc
         
 		pending_ring[MASK_PEND_IDX(pending_prod++)] = pending_idx;
 
-		/*
-		 * Scheduling checks must happen after the above response is
-		 * posted. This avoids a possible race with a guest OS on
-		 * another CPU if that guest is testing against 'resp_prod'
-		 * when deciding whether to notify us when it queues additional
-                 * packets.
-		 */
-		mb();
-		if ((netif->tx_req_cons != netif->tx->req_prod) &&
-		    ((netif->tx_req_cons-netif->tx_resp_prod) !=
-		     NETIF_TX_RING_SIZE))
-			add_to_net_schedule_list_tail(netif);
-        
 		netif_put(netif);
 	}
 }
@@ -454,10 +454,10 @@ static void net_tx_action(unsigned long 
 	netif_t *netif;
 	netif_tx_request_t txreq;
 	u16 pending_idx;
-	NETIF_RING_IDX i;
+	RING_IDX i;
 	gnttab_map_grant_ref_t *mop;
 	unsigned int data_len;
-	int ret;
+	int ret, work_to_do;
 
 	if (dealloc_cons != dealloc_prod)
 		net_tx_action_dealloc();
@@ -471,17 +471,15 @@ static void net_tx_action(unsigned long 
 		netif_get(netif);
 		remove_from_net_schedule_list(netif);
 
-		/* Work to do? */
-		i = netif->tx_req_cons;
-		if ((i == netif->tx->req_prod) ||
-		    ((i-netif->tx_resp_prod) == NETIF_TX_RING_SIZE)) {
+		RING_FINAL_CHECK_FOR_REQUESTS(&netif->tx, work_to_do);
+		if (!work_to_do) {
 			netif_put(netif);
 			continue;
 		}
 
+		i = netif->tx.req_cons;
 		rmb(); /* Ensure that we see the request before we copy it. */
-		memcpy(&txreq, &netif->tx->ring[MASK_NETIF_TX_IDX(i)].req, 
-		       sizeof(txreq));
+		memcpy(&txreq, RING_GET_REQUEST(&netif->tx, i), sizeof(txreq));
 		/* Credit-based scheduling. */
 		if (txreq.size > netif->remaining_credit) {
 			unsigned long now = jiffies;
@@ -515,12 +513,7 @@ static void net_tx_action(unsigned long 
 		}
 		netif->remaining_credit -= txreq.size;
 
-		/*
-		 * Why the barrier? It ensures that the frontend sees updated
-		 * req_cons before we check for more work to schedule.
-		 */
-		netif->tx->req_cons = ++netif->tx_req_cons;
-		mb();
+		netif->tx.req_cons++;
 
 		netif_schedule_work(netif);
 
@@ -592,7 +585,7 @@ static void net_tx_action(unsigned long 
 		       sizeof(txreq));
 
 		/* Check the remap error code. */
-		if (unlikely(mop->handle < 0)) {
+		if (unlikely(mop->status)) {
 			printk(KERN_ALERT "#### netback grant fails\n");
 			make_tx_response(netif, txreq.id, NETIF_RSP_ERROR);
 			netif_put(netif);
@@ -605,7 +598,7 @@ static void net_tx_action(unsigned long 
 		set_phys_to_machine(
 			__pa(MMAP_VADDR(pending_idx)) >> PAGE_SHIFT,
 			FOREIGN_FRAME(mop->dev_bus_addr >> PAGE_SHIFT));
-		grant_tx_ref[pending_idx] = mop->handle;
+		grant_tx_handle[pending_idx] = mop->handle;
 
 		data_len = (txreq.size > PKT_PROT_LEN) ?
 			PKT_PROT_LEN : txreq.size;
@@ -640,7 +633,7 @@ static void net_tx_action(unsigned long 
                  */
 		skb->ip_summed        = CHECKSUM_UNNECESSARY;
 		skb->proto_csum_valid = 1;
-		skb->proto_csum_blank = txreq.csum_blank;
+		skb->proto_csum_blank = !!(txreq.flags & NETTXF_csum_blank);
 
 		netif->stats.rx_bytes += txreq.size;
 		netif->stats.rx_packets++;
@@ -677,10 +670,8 @@ static void netif_page_release(struct pa
 irqreturn_t netif_be_int(int irq, void *dev_id, struct pt_regs *regs)
 {
 	netif_t *netif = dev_id;
-	if (tx_work_exists(netif)) {
-		add_to_net_schedule_list_tail(netif);
-		maybe_schedule_tx_action();
-	}
+	add_to_net_schedule_list_tail(netif);
+	maybe_schedule_tx_action();
 	return IRQ_HANDLED;
 }
 
@@ -688,18 +679,27 @@ static void make_tx_response(netif_t *ne
                              u16      id,
                              s8       st)
 {
-	NETIF_RING_IDX i = netif->tx_resp_prod;
+	RING_IDX i = netif->tx.rsp_prod_pvt;
 	netif_tx_response_t *resp;
+	int notify;
 
-	resp = &netif->tx->ring[MASK_NETIF_TX_IDX(i)].resp;
+	resp = RING_GET_RESPONSE(&netif->tx, i);
 	resp->id     = id;
 	resp->status = st;
-	wmb();
-	netif->tx->resp_prod = netif->tx_resp_prod = ++i;
 
-	mb(); /* Update producer before checking event threshold. */
-	if (i == netif->tx->event)
+	netif->tx.rsp_prod_pvt = ++i;
+	RING_PUSH_RESPONSES_AND_CHECK_NOTIFY(&netif->tx, notify);
+	if (notify)
 		notify_remote_via_irq(netif->irq);
+
+#ifdef CONFIG_XEN_NETDEV_PIPELINED_TRANSMITTER
+	if (i == netif->tx.req_cons) {
+		int more_to_do;
+		RING_FINAL_CHECK_FOR_REQUESTS(&netif->tx, more_to_do);
+		if (more_to_do)
+			add_to_net_schedule_list_tail(netif);
+	}
+#endif
 }
 
 static int make_rx_response(netif_t *netif, 
@@ -707,23 +707,24 @@ static int make_rx_response(netif_t *net
                             s8       st,
                             u16      offset,
                             u16      size,
-                            u16      csum_valid)
+                            u16      flags)
 {
-	NETIF_RING_IDX i = netif->rx_resp_prod;
+	RING_IDX i = netif->rx.rsp_prod_pvt;
 	netif_rx_response_t *resp;
+	int notify;
 
-	resp = &netif->rx->ring[MASK_NETIF_RX_IDX(i)].resp;
+	resp = RING_GET_RESPONSE(&netif->rx, i);
 	resp->offset     = offset;
-	resp->csum_valid = csum_valid;
+	resp->flags      = flags;
 	resp->id         = id;
 	resp->status     = (s16)size;
 	if (st < 0)
 		resp->status = (s16)st;
-	wmb();
-	netif->rx->resp_prod = netif->rx_resp_prod = ++i;
 
-	mb(); /* Update producer before checking event threshold. */
-	return (i == netif->rx->event);
+	netif->rx.rsp_prod_pvt = ++i;
+	RING_PUSH_RESPONSES_AND_CHECK_NOTIFY(&netif->rx, notify);
+
+	return notify;
 }
 
 static irqreturn_t netif_be_dbg(int irq, void *dev_id, struct pt_regs *regs)
@@ -739,16 +740,16 @@ static irqreturn_t netif_be_dbg(int irq,
 		netif = list_entry(ent, netif_t, list);
 		printk(KERN_ALERT " %d: private(rx_req_cons=%08x "
 		       "rx_resp_prod=%08x\n",
-		       i, netif->rx_req_cons, netif->rx_resp_prod);
+		       i, netif->rx.req_cons, netif->rx.rsp_prod_pvt);
 		printk(KERN_ALERT "   tx_req_cons=%08x tx_resp_prod=%08x)\n",
-		       netif->tx_req_cons, netif->tx_resp_prod);
+		       netif->tx.req_cons, netif->tx.rsp_prod_pvt);
 		printk(KERN_ALERT "   shared(rx_req_prod=%08x "
 		       "rx_resp_prod=%08x\n",
-		       netif->rx->req_prod, netif->rx->resp_prod);
+		       netif->rx.sring->req_prod, netif->rx.sring->rsp_prod);
 		printk(KERN_ALERT "   rx_event=%08x tx_req_prod=%08x\n",
-		       netif->rx->event, netif->tx->req_prod);
+		       netif->rx.sring->rsp_event, netif->tx.sring->req_prod);
 		printk(KERN_ALERT "   tx_resp_prod=%08x, tx_event=%08x)\n",
-		       netif->tx->resp_prod, netif->tx->event);
+		       netif->tx.sring->rsp_prod, netif->tx.sring->rsp_event);
 		i++;
 	}
 
@@ -764,7 +765,7 @@ static int __init netback_init(void)
 	struct page *page;
 
 	/* We can increase reservation by this much in net_rx_action(). */
-	balloon_update_driver_allowance(NETIF_RX_RING_SIZE);
+	balloon_update_driver_allowance(NET_RX_RING_SIZE);
 
 	skb_queue_head_init(&rx_queue);
 	skb_queue_head_init(&tx_queue);
