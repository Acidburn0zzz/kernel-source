Date: Tue, 11 May 2004 17:03:49 +1000
From: Paul Mackerras <paulus@samba.org>
To: olh@suse.de
Cc: anton@samba.org, benh@kernel.crashing.org
Subject: new ppc64 io barrier patch

Hi Olaf,

I have had pointed out to me that we have a potential problem with our
I/O accessors and spinlocks in ppc64 in that I/O writes can slide out
of a spinlocked region.  (This was the case before my recent patch; my
patch didn't make things any worse in this respect, but did prod some
people to look at the code.)

In addition we have the problems in LTC7201 where I seem to have made
the problems with the e100 when using NAPI worse somehow.  I don't
understand why my changes would have made things worse there, and I
suspect there is some other bug in the driver.

Anyway, here is a proposed patch which give us all the ordering
guarantees we need.  It may cost a little bit in performance.
Basically, compared to what we had before my previous io barrier
patch:

* writex/outx have a sync instead of an eieio
* readx/inx is unchanged
* spin_unlock, read_unlock, write_unlock have an lwsync instead of eieio
* wmb is sync instead of eieio.

Since this just makes barriers stronger, and doesn't weaken or remove
any barriers, I think it should be safe.

What would be good is if Anton and the guys in Austin could test this
patch today (their time) and let you (Olaf) know how it goes.  I have
compile-tested this patch but I haven't tested it.  The patch is
against Anton's latest snapshot of your cvs.

Regards,
Paul.

diff -urN linux-2.6.5/include/asm-ppc64/io.h newxmon/include/asm-ppc64/io.h
--- linux-2.6.5/include/asm-ppc64/io.h	2004-05-11 09:56:09.000000000 +1000
+++ newxmon/include/asm-ppc64/io.h	2004-05-11 16:24:47.000000000 +1000
@@ -240,14 +240,14 @@
 {
 	int ret;
 
-	__asm__ __volatile__("eieio; lbz%U1%X1 %0,%1; twi 0,%0,0; isync"
+	__asm__ __volatile__("lbz%U1%X1 %0,%1; twi 0,%0,0; isync"
 			     : "=r" (ret) : "m" (*addr));
 	return ret;
 }
 
 static inline void out_8(volatile unsigned char *addr, int val)
 {
-	__asm__ __volatile__("sync; stb%U0%X0 %1,%0"
+	__asm__ __volatile__("stb%U0%X0 %1,%0; sync"
 			     : "=m" (*addr) : "r" (val));
 }
 
@@ -255,7 +255,7 @@
 {
 	int ret;
 
-	__asm__ __volatile__("eieio; lhbrx %0,0,%1; twi 0,%0,0; isync"
+	__asm__ __volatile__("lhbrx %0,0,%1; twi 0,%0,0; isync"
 			     : "=r" (ret) : "r" (addr), "m" (*addr));
 	return ret;
 }
@@ -264,20 +264,20 @@
 {
 	int ret;
 
-	__asm__ __volatile__("eieio; lhz%U1%X1 %0,%1; twi 0,%0,0; isync"
+	__asm__ __volatile__("lhz%U1%X1 %0,%1; twi 0,%0,0; isync"
 			     : "=r" (ret) : "m" (*addr));
 	return ret;
 }
 
 static inline void out_le16(volatile unsigned short *addr, int val)
 {
-	__asm__ __volatile__("sync; sthbrx %1,0,%2"
+	__asm__ __volatile__("sthbrx %1,0,%2; sync"
 			     : "=m" (*addr) : "r" (val), "r" (addr));
 }
 
 static inline void out_be16(volatile unsigned short *addr, int val)
 {
-	__asm__ __volatile__("sync; sth%U0%X0 %1,%0"
+	__asm__ __volatile__("sth%U0%X0 %1,%0; sync"
 			     : "=m" (*addr) : "r" (val));
 }
 
@@ -285,7 +285,7 @@
 {
 	unsigned ret;
 
-	__asm__ __volatile__("eieio; lwbrx %0,0,%1; twi 0,%0,0; isync"
+	__asm__ __volatile__("lwbrx %0,0,%1; twi 0,%0,0; isync"
 			     : "=r" (ret) : "r" (addr), "m" (*addr));
 	return ret;
 }
@@ -294,20 +294,20 @@
 {
 	unsigned ret;
 
-	__asm__ __volatile__("eieio; lwz%U1%X1 %0,%1; twi 0,%0,0; isync"
+	__asm__ __volatile__("lwz%U1%X1 %0,%1; twi 0,%0,0; isync"
 			     : "=r" (ret) : "m" (*addr));
 	return ret;
 }
 
 static inline void out_le32(volatile unsigned *addr, int val)
 {
-	__asm__ __volatile__("sync; stwbrx %1,0,%2" : "=m" (*addr)
+	__asm__ __volatile__("stwbrx %1,0,%2; sync" : "=m" (*addr)
 			     : "r" (val), "r" (addr));
 }
 
 static inline void out_be32(volatile unsigned *addr, int val)
 {
-	__asm__ __volatile__("sync; stw%U0%X0 %1,%0; eieio"
+	__asm__ __volatile__("stw%U0%X0 %1,%0; eieio"
 			     : "=m" (*addr) : "r" (val));
 }
 
@@ -316,7 +316,7 @@
 	unsigned long tmp, ret;
 
 	__asm__ __volatile__(
-			     "eieio; ld %1,0(%2)\n"
+			     "ld %1,0(%2)\n"
 			     "twi 0,%1,0\n"
 			     "isync\n"
 			     "rldimi %0,%1,5*8,1*8\n"
@@ -334,7 +334,7 @@
 {
 	unsigned long ret;
 
-	__asm__ __volatile__("eieio; ld %0,0(%1); twi 0,%0,0; isync"
+	__asm__ __volatile__("ld %0,0(%1); twi 0,%0,0; isync"
 			     : "=r" (ret) : "m" (*addr));
 	return ret;
 }
@@ -351,13 +351,14 @@
 			     "rldicl %1,%1,32,0\n"
 			     "rlwimi %0,%1,8,8,31\n"
 			     "rlwimi %0,%1,24,16,23\n"
-			     "sync; std %0,0(%2)\n"
+			     "std %0,0(%2)\n"
+			     "sync"
 			     : "=r" (tmp) : "r" (val), "b" (addr) , "m" (*addr));
 }
 
 static inline void out_be64(volatile unsigned long *addr, int val)
 {
-	__asm__ __volatile__("sync; std %1,0(%0)" : "=m" (*addr) : "r" (val));
+	__asm__ __volatile__("std %1,0(%0); sync" : "=m" (*addr) : "r" (val));
 }
 
 #ifndef CONFIG_PPC_ISERIES 
diff -urN linux-2.6.5/include/asm-ppc64/spinlock.h newxmon/include/asm-ppc64/spinlock.h
--- linux-2.6.5/include/asm-ppc64/spinlock.h	2004-05-11 09:56:12.000000000 +1000
+++ newxmon/include/asm-ppc64/spinlock.h	2004-05-11 16:27:51.703591936 +1000
@@ -84,7 +84,7 @@
 
 static __inline__ void _raw_spin_unlock(spinlock_t *lock)
 {
-	__asm__ __volatile__("eieio	# spin_unlock": : :"memory");
+	__asm__ __volatile__("lwsync	# spin_unlock": : :"memory");
 	lock->lock = 0;
 }
 
@@ -157,7 +157,7 @@
 	unsigned int tmp;
 
 	__asm__ __volatile__(
-	"eieio				# read_unlock\n\
+	"lwsync				# read_unlock\n\
 1:	lwarx		%0,0,%1\n\
 	addic		%0,%0,-1\n\
 	stwcx.		%0,0,%1\n\
@@ -209,7 +209,7 @@
 
 static __inline__ void _raw_write_unlock(rwlock_t *rw)
 {
-	__asm__ __volatile__("eieio		# write_unlock": : :"memory");
+	__asm__ __volatile__("lwsync		# write_unlock": : :"memory");
 	rw->lock = 0;
 }
 
diff -urN linux-2.6.5/include/asm-ppc64/system.h newxmon/include/asm-ppc64/system.h
--- linux-2.6.5/include/asm-ppc64/system.h	2004-05-11 09:56:38.000000000 +1000
+++ newxmon/include/asm-ppc64/system.h	2004-05-11 16:22:35.184649696 +1000
@@ -29,22 +29,26 @@
  * read_barrier_depends() prevents data-dependent loads being reordered
  *	across this point (nop on PPC).
  *
- * We can use the eieio instruction for wmb, but since it doesn't
- * give any ordering guarantees about loads, we have to use the
- * stronger but slower sync instruction for mb and rmb.
+ * We have to use the sync instructions for mb(), since lwsync doesn't
+ * order loads with respect to previous stores.  Lwsync is fine for
+ * rmb(), though.
+ * For wmb(), we use sync since wmb is used in drivers to order
+ * stores to system memory with respect to writes to the device.
+ * However, smp_wmb() can be a lighter-weight eieio barrier on
+ * SMP since it is only used to order updates to system memory.
  */
 #define mb()   __asm__ __volatile__ ("sync" : : : "memory")
 #define rmb()  __asm__ __volatile__ ("lwsync" : : : "memory")
-#define wmb()  __asm__ __volatile__ ("eieio" : : : "memory")
+#define wmb()  __asm__ __volatile__ ("sync" : : : "memory")
 #define read_barrier_depends()  do { } while(0)
 
-#define set_mb(var, value)	do { var = value; mb(); } while (0)
-#define set_wmb(var, value)	do { var = value; wmb(); } while (0)
+#define set_mb(var, value)	do { var = value; smp_mb(); } while (0)
+#define set_wmb(var, value)	do { var = value; smp_wmb(); } while (0)
 
 #ifdef CONFIG_SMP
 #define smp_mb()	mb()
 #define smp_rmb()	rmb()
-#define smp_wmb()	wmb()
+#define smp_wmb()	__asm__ __volatile__ ("eieio" : : : "memory")
 #define smp_read_barrier_depends()  read_barrier_depends()
 #else
 #define smp_mb()	__asm__ __volatile__("": : :"memory")

