Date: Thu, 29 Apr 2004 19:02:39 +1000
From: Anton Blanchard <anton@samba.org>
To: olh@suse.de
Subject: Re: # 7754

 
> We have been looking at optimising our SLB reload path, its being hit
> very heavily in some benchmarks. There are a few patches on that bug but
> there are only 2 things I feel comfortable merging at this point.

Here is the second patch. We bolt in the first vmalloc segment, it gets
hit very heavily especially when we have modules configured.

It reduced our SLB miss rate by a significant amount on an NFS
benchmark.

Anton

diff -puN arch/ppc64/kernel/head.S~slb_optimisation arch/ppc64/kernel/head.S
--- linux-2.6.5/arch/ppc64/kernel/head.S~slb_optimisation	2004-04-29 00:59:09.969258722 -0500
+++ linux-2.6.5-anton/arch/ppc64/kernel/head.S	2004-04-29 00:59:09.989255551 -0500
@@ -1126,7 +1126,7 @@ SLB_NUM_ENTRIES = 64
 	addi	r21,r22,1
 	cmpdi	r21,SLB_NUM_ENTRIES
 	blt+	2f
-	li	r21,1			/* dont touch bolted slot 0 */
+	li	r21,2			/* dont touch slot 0 or 1 */
 2:	std	r21,PACASTABRR(r20)
 
 	/* r20 = paca, r22 = entry */
diff -puN arch/ppc64/kernel/stab.c~slb_optimisation arch/ppc64/kernel/stab.c
--- linux-2.6.5/arch/ppc64/kernel/stab.c~slb_optimisation	2004-04-29 00:59:09.974257929 -0500
+++ linux-2.6.5-anton/arch/ppc64/kernel/stab.c	2004-04-29 00:59:44.232024551 -0500
@@ -51,6 +51,14 @@ void stab_initialize(unsigned long stab)
 		make_slbe(esid, vsid, seg0_largepages, 1);
 		asm volatile("isync":::"memory");
 #endif
+
+		/* 
+		 * Bolt in the first vmalloc segment. Since modules end
+		 * up there it gets hit very heavily.
+		 */
+		esid = GET_ESID(VMALLOCBASE);
+		vsid = get_kernel_vsid(VMALLOCBASE); 
+		make_slbe(esid, vsid, 0, 1);
 	} else {
 		asm volatile("isync; slbia; isync":::"memory");
 		make_ste(stab, esid, vsid);
@@ -335,8 +343,12 @@ static void make_slbe(unsigned long esid
 	do {
 		entry = castout_entry;
 		castout_entry++; 
+		/* 
+		 * We bolt in the first kernel segment and the first
+		 * vmalloc segment.
+		 */
 		if (castout_entry >= naca->slb_size)
-			castout_entry = 1; 
+			castout_entry = 2;
 		asm volatile("slbmfee  %0,%1" : "=r" (esid_data) : "r" (entry));
 	} while (esid_data.data.v &&
 		 esid_data.data.esid == GET_ESID(__get_SP()));
@@ -422,6 +434,8 @@ int slb_allocate(unsigned long ea)
 	}
 
 	esid = GET_ESID(ea);
+
+	BUG_ON((esid << SID_SHIFT) == VMALLOCBASE);
 	__slb_allocate(esid, vsid, context);
 
 	return 0;
@@ -494,7 +508,9 @@ void flush_slb(struct task_struct *tsk, 
 		unsigned long word0;
 		slb_dword0 data;
 	} esid_data;
+	unsigned long esid, vsid;
 
+	WARN_ON(!irqs_disabled());
 
 	if (offset <= NR_STAB_CACHE_ENTRIES) {
 		int i;
@@ -502,11 +518,23 @@ void flush_slb(struct task_struct *tsk, 
 		for (i = 0; i < offset; i++) {
 			esid_data.word0 = 0;
 			esid_data.data.esid = __get_cpu_var(stab_cache[i]);
+			BUG_ON(esid_data.data.esid == GET_ESID(VMALLOCBASE));
 			asm volatile("slbie %0" : : "r" (esid_data));
 		}
 		asm volatile("isync" : : : "memory");
 	} else {
 		asm volatile("isync; slbia; isync" : : : "memory");
+
+		/* 
+		 * Bolt in the first vmalloc segment. Since modules end
+		 * up there it gets hit very heavily. We must not touch
+		 * the vmalloc region between the slbia and here, thats
+		 * why we require interrupts off.
+		 */
+		esid = GET_ESID(VMALLOCBASE);
+		vsid = get_kernel_vsid(VMALLOCBASE); 
+		get_paca()->xStab_data.next_round_robin = 1;
+		make_slbe(esid, vsid, 0, 1);
 	}
 
 	/* Workaround POWER5 < DD2.1 issue */
diff -puN include/asm-ppc64/mmu_context.h~slb_optimisation include/asm-ppc64/mmu_context.h
--- linux-2.6.5/include/asm-ppc64/mmu_context.h~slb_optimisation	2004-04-29 00:59:09.980256978 -0500
+++ linux-2.6.5-anton/include/asm-ppc64/mmu_context.h	2004-04-29 00:59:09.994254758 -0500
@@ -172,8 +172,14 @@ static inline void switch_mm(struct mm_s
  * After we have set current->mm to a new value, this activates
  * the context for the new mm so we see the new mappings.
  */
-#define activate_mm(active_mm, mm) \
-	switch_mm(active_mm, mm, current);
+static inline void activate_mm(struct mm_struct *prev, struct mm_struct *next)
+{
+	unsigned long flags;
+
+	local_irq_save(flags);
+	switch_mm(prev, next, current);
+	local_irq_restore(flags);
+}
 
 #define VSID_RANDOMIZER 42470972311
 #define VSID_MASK	0xfffffffff

_

