diff -purN linux-2.5/MAINTAINERS linuxppc64-2.5/MAINTAINERS
--- linux-2.5/MAINTAINERS	2004-03-02 03:01:27.000000000 +0000
+++ linuxppc64-2.5/MAINTAINERS	2004-03-02 12:16:58.000000000 +0000
@@ -970,6 +970,11 @@ M:	langa2@kph.uni-mainz.de
 W:	http://www.uni-mainz.de/~langm000/linux.html
 S:	Maintained
 
+IBM Power Linux RAID adapter
+P:	Brian King
+M:	brking@us.ibm.com
+S:	Supported
+
 IBM ServeRAID RAID DRIVER
 P:	Jack Hammer
 P:	Dave Jeffery
diff -purN linux-2.5/arch/ppc64/Kconfig linuxppc64-2.5/arch/ppc64/Kconfig
--- linux-2.5/arch/ppc64/Kconfig	2004-02-27 07:24:31.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/Kconfig	2004-03-01 02:24:54.000000000 +0000
@@ -205,7 +205,7 @@ config SCANLOG
 	depends on PPC_RTAS
 
 config LPARCFG
-	bool "LPAR Configuration Data"
+	tristate "LPAR Configuration Data"
 	help
 	Provide system capacity information via human readable 
 	<key word>=<value> pairs through a /proc/ppc64/lparcfg interface.
@@ -369,17 +369,38 @@ config DEBUGGER
 	  Include in-kernel hooks for kernel debuggers. Unless you are
 	  intending to debug the kernel, say N here.
 
-config XMON
-	bool "Include xmon kernel debugger"
+choice
+	optional
 	depends on DEBUGGER
+	prompt "Kernel Debugger"
+
+config XMON
+	bool "XMON"
 	help
 	  Include in-kernel hooks for the xmon kernel monitor/debugger.
 	  Unless you are intending to debug the kernel, say N here.
 
+config KDB
+	bool "KDB"
+	help
+	  Include in-kernel hooks for the kdb kernel monitor/debugger.
+	  Unless you are intending to debug the kernel, say N here.
+
+endchoice
+
+
 config XMON_DEFAULT
 	bool "Enable xmon by default"
 	depends on XMON
 
+config KDB_OFF
+	bool "Turn KDB off as default."
+	depends on KDB
+
+	help
+ 	   KDB will remain built into the kernel, but will be turned off. 
+	   "cat 1 > /proc/sys/kernel/kdb" to turn it on. 
+
 config PPCDBG
 	bool "Include PPCDBG realtime debugging"
 	depends on DEBUG_KERNEL
@@ -392,6 +413,13 @@ config DEBUG_INFO
 	  debugging info resulting in a larger kernel image.
 	  Say Y here only if you plan to use gdb to debug the kernel.
 	  If you don't debug the kernel, you can say N.
+
+config DEBUG_SPINLOCK_SLEEP
+	bool "Sleep-inside-spinlock checking"
+	depends on DEBUG_KERNEL
+	help
+	  If you say Y here, various routines which may sleep will become very
+	  noisy if they are called with a spinlock held.
 	  
 endmenu
 
diff -purN linux-2.5/arch/ppc64/Makefile linuxppc64-2.5/arch/ppc64/Makefile
--- linux-2.5/arch/ppc64/Makefile	2004-02-25 02:29:31.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/Makefile	2004-02-26 05:41:41.000000000 +0000
@@ -43,6 +43,11 @@ libs-y				+= arch/ppc64/lib/
 core-y				+= arch/ppc64/kernel/
 core-y				+= arch/ppc64/mm/
 core-$(CONFIG_XMON)		+= arch/ppc64/xmon/
+ifeq ($(CONFIG_KDB),y)
+  # Use ifeq for now because kdb subdirs are not in bk yet
+  # Otherwise make mrproper will die because it also cleans core-n
+  core-y			+= arch/ppc64/kdb/
+endif
 drivers-$(CONFIG_OPROFILE)	+= arch/ppc64/oprofile/
 
 boot := arch/ppc64/boot
diff -purN linux-2.5/arch/ppc64/configs/iSeries_defconfig linuxppc64-2.5/arch/ppc64/configs/iSeries_defconfig
--- linux-2.5/arch/ppc64/configs/iSeries_defconfig	2004-02-27 13:23:20.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/configs/iSeries_defconfig	2004-03-02 01:49:29.000000000 +0000
@@ -56,6 +56,7 @@ CONFIG_PPC_ISERIES=y
 CONFIG_PPC=y
 CONFIG_PPC64=y
 # CONFIG_POWER4_ONLY is not set
+# CONFIG_IOMMU_VMERGE is not set
 CONFIG_SMP=y
 CONFIG_NR_CPUS=32
 CONFIG_MSCHUNKS=y
@@ -88,7 +89,7 @@ CONFIG_PCI_NAMES=y
 #
 # Generic Driver Options
 #
-CONFIG_FW_LOADER=m
+CONFIG_FW_LOADER=y
 
 #
 # Memory Technology Devices (MTD)
@@ -169,8 +170,12 @@ CONFIG_SCSI_CONSTANTS=y
 # CONFIG_SCSI_FUTURE_DOMAIN is not set
 # CONFIG_SCSI_GDTH is not set
 # CONFIG_SCSI_IPS is not set
+# CONFIG_SCSI_IBMVSCSI is not set
 # CONFIG_SCSI_INIA100 is not set
 # CONFIG_SCSI_SYM53C8XX_2 is not set
+CONFIG_SCSI_IPR=y
+# CONFIG_SCSI_IPR_TRACE is not set
+# CONFIG_SCSI_IPR_DUMP is not set
 # CONFIG_SCSI_QLOGIC_ISP is not set
 # CONFIG_SCSI_QLOGIC_FC is not set
 # CONFIG_SCSI_QLOGIC_1280 is not set
@@ -409,7 +414,7 @@ CONFIG_E1000=y
 #
 CONFIG_IXGB=m
 # CONFIG_IXGB_NAPI is not set
-# CONFIG_VETH is not set
+# CONFIG_ISERIES_VETH is not set
 # CONFIG_FDDI is not set
 # CONFIG_HIPPI is not set
 CONFIG_PPP=m
@@ -763,6 +768,7 @@ CONFIG_MAGIC_SYSRQ=y
 # CONFIG_DEBUGGER is not set
 # CONFIG_PPCDBG is not set
 # CONFIG_DEBUG_INFO is not set
+CONFIG_DEBUG_SPINLOCK_SLEEP=y
 
 #
 # Security options
diff -purN linux-2.5/arch/ppc64/configs/pSeries_defconfig linuxppc64-2.5/arch/ppc64/configs/pSeries_defconfig
--- linux-2.5/arch/ppc64/configs/pSeries_defconfig	2004-02-27 13:20:30.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/configs/pSeries_defconfig	2004-03-02 01:49:29.000000000 +0000
@@ -60,6 +60,7 @@ CONFIG_ALTIVEC=y
 # CONFIG_PPC_PMAC is not set
 # CONFIG_BOOTX_TEXT is not set
 # CONFIG_POWER4_ONLY is not set
+# CONFIG_IOMMU_VMERGE is not set
 CONFIG_SMP=y
 CONFIG_IRQ_ALL_CPUS=y
 CONFIG_NR_CPUS=32
@@ -105,7 +106,7 @@ CONFIG_PROC_DEVICETREE=y
 #
 # Generic Driver Options
 #
-CONFIG_FW_LOADER=m
+CONFIG_FW_LOADER=y
 
 #
 # Memory Technology Devices (MTD)
@@ -241,12 +242,17 @@ CONFIG_SCSI_CONSTANTS=y
 # CONFIG_SCSI_FUTURE_DOMAIN is not set
 # CONFIG_SCSI_GDTH is not set
 # CONFIG_SCSI_IPS is not set
+CONFIG_SCSI_IBMVSCSI=m
+CONFIG_SCSI_IBMVSCSIS=m
 # CONFIG_SCSI_INIA100 is not set
 CONFIG_SCSI_SYM53C8XX_2=y
 CONFIG_SCSI_SYM53C8XX_DMA_ADDRESSING_MODE=0
 CONFIG_SCSI_SYM53C8XX_DEFAULT_TAGS=16
 CONFIG_SCSI_SYM53C8XX_MAX_TAGS=64
 # CONFIG_SCSI_SYM53C8XX_IOMAPPED is not set
+CONFIG_SCSI_IPR=y
+# CONFIG_SCSI_IPR_TRACE is not set
+# CONFIG_SCSI_IPR_DUMP is not set
 # CONFIG_SCSI_QLOGIC_ISP is not set
 # CONFIG_SCSI_QLOGIC_FC is not set
 # CONFIG_SCSI_QLOGIC_1280 is not set
@@ -487,9 +493,9 @@ CONFIG_TIGON3=y
 #
 CONFIG_IXGB=m
 # CONFIG_IXGB_NAPI is not set
+CONFIG_IBMVETH=m
 # CONFIG_FDDI is not set
 # CONFIG_HIPPI is not set
-CONFIG_IBMVETH=m
 CONFIG_PPP=m
 # CONFIG_PPP_MULTILINK is not set
 # CONFIG_PPP_FILTER is not set
@@ -617,6 +623,7 @@ CONFIG_UNIX98_PTYS=y
 CONFIG_LEGACY_PTYS=y
 CONFIG_LEGACY_PTY_COUNT=256
 CONFIG_HVC_CONSOLE=y
+CONFIG_HVCS=m
 
 #
 # Mice
@@ -1049,9 +1056,11 @@ CONFIG_DEBUG_STACK_USAGE=y
 CONFIG_MAGIC_SYSRQ=y
 CONFIG_DEBUGGER=y
 CONFIG_XMON=y
+# CONFIG_KDB is not set
 CONFIG_XMON_DEFAULT=y
 # CONFIG_PPCDBG is not set
 # CONFIG_DEBUG_INFO is not set
+CONFIG_DEBUG_SPINLOCK_SLEEP=y
 
 #
 # Security options
diff -purN linux-2.5/arch/ppc64/defconfig linuxppc64-2.5/arch/ppc64/defconfig
--- linux-2.5/arch/ppc64/defconfig	2004-02-27 13:20:30.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/defconfig	2004-03-02 01:49:29.000000000 +0000
@@ -60,6 +60,7 @@ CONFIG_ALTIVEC=y
 # CONFIG_PPC_PMAC is not set
 # CONFIG_BOOTX_TEXT is not set
 # CONFIG_POWER4_ONLY is not set
+# CONFIG_IOMMU_VMERGE is not set
 CONFIG_SMP=y
 CONFIG_IRQ_ALL_CPUS=y
 CONFIG_NR_CPUS=32
@@ -105,7 +106,7 @@ CONFIG_PROC_DEVICETREE=y
 #
 # Generic Driver Options
 #
-CONFIG_FW_LOADER=m
+CONFIG_FW_LOADER=y
 
 #
 # Memory Technology Devices (MTD)
@@ -241,12 +242,17 @@ CONFIG_SCSI_CONSTANTS=y
 # CONFIG_SCSI_FUTURE_DOMAIN is not set
 # CONFIG_SCSI_GDTH is not set
 # CONFIG_SCSI_IPS is not set
+CONFIG_SCSI_IBMVSCSI=m
+CONFIG_SCSI_IBMVSCSIS=m
 # CONFIG_SCSI_INIA100 is not set
 CONFIG_SCSI_SYM53C8XX_2=y
 CONFIG_SCSI_SYM53C8XX_DMA_ADDRESSING_MODE=0
 CONFIG_SCSI_SYM53C8XX_DEFAULT_TAGS=16
 CONFIG_SCSI_SYM53C8XX_MAX_TAGS=64
 # CONFIG_SCSI_SYM53C8XX_IOMAPPED is not set
+CONFIG_SCSI_IPR=y
+# CONFIG_SCSI_IPR_TRACE is not set
+# CONFIG_SCSI_IPR_DUMP is not set
 # CONFIG_SCSI_QLOGIC_ISP is not set
 # CONFIG_SCSI_QLOGIC_FC is not set
 # CONFIG_SCSI_QLOGIC_1280 is not set
@@ -487,9 +493,9 @@ CONFIG_TIGON3=y
 #
 CONFIG_IXGB=m
 # CONFIG_IXGB_NAPI is not set
+CONFIG_IBMVETH=m
 # CONFIG_FDDI is not set
 # CONFIG_HIPPI is not set
-CONFIG_IBMVETH=m
 CONFIG_PPP=m
 # CONFIG_PPP_MULTILINK is not set
 # CONFIG_PPP_FILTER is not set
@@ -617,6 +623,7 @@ CONFIG_UNIX98_PTYS=y
 CONFIG_LEGACY_PTYS=y
 CONFIG_LEGACY_PTY_COUNT=256
 CONFIG_HVC_CONSOLE=y
+CONFIG_HVCS=m
 
 #
 # Mice
@@ -1049,9 +1056,11 @@ CONFIG_DEBUG_STACK_USAGE=y
 CONFIG_MAGIC_SYSRQ=y
 CONFIG_DEBUGGER=y
 CONFIG_XMON=y
+# CONFIG_KDB is not set
 CONFIG_XMON_DEFAULT=y
 # CONFIG_PPCDBG is not set
 # CONFIG_DEBUG_INFO is not set
+CONFIG_DEBUG_SPINLOCK_SLEEP=y
 
 #
 # Security options
diff -purN linux-2.5/arch/ppc64/kdb/Makefile linuxppc64-2.5/arch/ppc64/kdb/Makefile
--- linux-2.5/arch/ppc64/kdb/Makefile	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/Makefile	2003-10-13 16:16:56.000000000 +0000
@@ -0,0 +1,7 @@
+obj-y		:= kdba_bt.o kdba_bp.o kdba_id.o kdba_io.o ppc-dis.o ppc-opc.o kdbasupport.o 
+
+# Warning: running with a minimal-toc means that kdb_setjmp will break
+# due to saving the wrong r30. A solution would be to move it into setjmp.S
+EXTRA_CFLAGS = -mno-minimal-toc
+
+override CFLAGS := $(CFLAGS) -I. -Iarch/ppc64/kdb
diff -purN linux-2.5/arch/ppc64/kdb/ansidecl.h linuxppc64-2.5/arch/ppc64/kdb/ansidecl.h
--- linux-2.5/arch/ppc64/kdb/ansidecl.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/ansidecl.h	2003-10-13 16:16:57.000000000 +0000
@@ -0,0 +1,198 @@
+/* ANSI and traditional C compatability macros
+   Copyright 1991, 1992, 1996, 1999 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+
+This program is free software; you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation; either version 2 of the License, or
+(at your option) any later version.
+
+This program is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; if not, write to the Free Software
+Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  */
+
+/* ANSI and traditional C compatibility macros
+
+   ANSI C is assumed if __STDC__ is #defined.
+
+   Macro	ANSI C definition	Traditional C definition
+   -----	---- - ----------	----------- - ----------
+   PTR		`void *'		`char *'
+   LONG_DOUBLE	`long double'		`double'
+   VOLATILE	`volatile'		`'
+   SIGNED	`signed'		`'
+   PTRCONST	`void *const'		`char *'
+   ANSI_PROTOTYPES  1			not defined
+
+   CONST is also defined, but is obsolete.  Just use const.
+
+   obsolete --     DEFUN (name, arglist, args)
+
+	Defines function NAME.
+
+	ARGLIST lists the arguments, separated by commas and enclosed in
+	parentheses.  ARGLIST becomes the argument list in traditional C.
+
+	ARGS list the arguments with their types.  It becomes a prototype in
+	ANSI C, and the type declarations in traditional C.  Arguments should
+	be separated with `AND'.  For functions with a variable number of
+	arguments, the last thing listed should be `DOTS'.
+
+   obsolete --     DEFUN_VOID (name)
+
+	Defines a function NAME, which takes no arguments.
+
+   obsolete --     EXFUN (name, (prototype))	-- obsolete.
+
+	Replaced by PARAMS.  Do not use; will disappear someday soon.
+	Was used in external function declarations.
+	In ANSI C it is `NAME PROTOTYPE' (so PROTOTYPE should be enclosed in
+	parentheses).  In traditional C it is `NAME()'.
+	For a function that takes no arguments, PROTOTYPE should be `(void)'.
+
+   obsolete --     PROTO (type, name, (prototype)    -- obsolete.
+
+	This one has also been replaced by PARAMS.  Do not use.
+
+   PARAMS ((args))
+
+	We could use the EXFUN macro to handle prototype declarations, but
+	the name is misleading and the result is ugly.  So we just define a
+	simple macro to handle the parameter lists, as in:
+
+	      static int foo PARAMS ((int, char));
+
+	This produces:  `static int foo();' or `static int foo (int, char);'
+
+	EXFUN would have done it like this:
+
+	      static int EXFUN (foo, (int, char));
+
+	but the function is not external...and it's hard to visually parse
+	the function name out of the mess.   EXFUN should be considered
+	obsolete; new code should be written to use PARAMS.
+
+   DOTS is also obsolete.
+
+   Examples:
+
+	extern int printf PARAMS ((const char *format, ...));
+*/
+
+#ifndef	_ANSIDECL_H
+
+#define	_ANSIDECL_H	1
+
+
+/* Every source file includes this file,
+   so they will all get the switch for lint.  */
+/* LINTLIBRARY */
+
+
+#if defined (__STDC__) || defined (_AIX) || (defined (__mips) && defined (_SYSTYPE_SVR4)) || defined(_WIN32)
+/* All known AIX compilers implement these things (but don't always
+   define __STDC__).  The RISC/OS MIPS compiler defines these things
+   in SVR4 mode, but does not define __STDC__.  */
+
+#define	PTR		void *
+#define	PTRCONST	void *CONST
+#define	LONG_DOUBLE	long double
+
+#ifndef IN_GCC
+#define	AND		,
+#define	NOARGS		void
+#define	VOLATILE	volatile
+#define	SIGNED		signed
+#endif /* ! IN_GCC */
+
+#ifndef PARAMS
+#define PARAMS(paramlist)		paramlist
+#endif
+#define ANSI_PROTOTYPES			1
+
+#define VPARAMS(ARGS)			ARGS
+#define VA_START(va_list,var)		va_start(va_list,var)
+
+/* These are obsolete.  Do not use.  */
+#ifndef IN_GCC
+#define CONST				const
+#define DOTS				, ...
+#define PROTO(type, name, arglist)	type name arglist
+#define EXFUN(name, proto)		name proto
+#define DEFUN(name, arglist, args)	name(args)
+#define DEFUN_VOID(name)		name(void)
+#endif /* ! IN_GCC */
+
+#else	/* Not ANSI C.  */
+
+#define	PTR		char *
+#define	PTRCONST	PTR
+#define	LONG_DOUBLE	double
+
+#ifndef IN_GCC
+#define	AND		;
+#define	NOARGS
+#define	VOLATILE
+#define	SIGNED
+#endif /* !IN_GCC */
+
+#ifndef const /* some systems define it in header files for non-ansi mode */
+#define	const
+#endif
+
+#define PARAMS(paramlist)		()
+
+#define VPARAMS(ARGS)			(va_alist) va_dcl
+#define VA_START(va_list,var)		va_start(va_list)
+
+/* These are obsolete.  Do not use.  */
+#ifndef IN_GCC
+#define CONST
+#define DOTS
+#define PROTO(type, name, arglist)	type name ()
+#define EXFUN(name, proto)		name()
+#define DEFUN(name, arglist, args)	name arglist args;
+#define DEFUN_VOID(name)		name()
+#endif /* ! IN_GCC */
+
+#endif	/* ANSI C.  */
+
+/* Define macros for some gcc attributes.  This permits us to use the
+   macros freely, and know that they will come into play for the
+   version of gcc in which they are supported.  */
+
+#if __GNUC__ < 2 || (__GNUC__ == 2 && __GNUC_MINOR__ < 7)
+# define __attribute__(x)
+#endif
+
+#ifndef ATTRIBUTE_UNUSED_LABEL
+# if __GNUC__ < 2 || (__GNUC__ == 2 && __GNUC_MINOR__ < 93)
+#  define ATTRIBUTE_UNUSED_LABEL
+# else
+#  define ATTRIBUTE_UNUSED_LABEL ATTRIBUTE_UNUSED
+# endif /* GNUC < 2.93 */
+#endif /* ATTRIBUTE_UNUSED_LABEL */
+
+#ifndef ATTRIBUTE_UNUSED
+#define ATTRIBUTE_UNUSED __attribute__ ((__unused__))
+#endif /* ATTRIBUTE_UNUSED */
+
+#ifndef ATTRIBUTE_NORETURN
+#define ATTRIBUTE_NORETURN __attribute__ ((__noreturn__))
+#endif /* ATTRIBUTE_NORETURN */
+
+#ifndef ATTRIBUTE_PRINTF
+#define ATTRIBUTE_PRINTF(m, n) __attribute__ ((format (__printf__, m, n)))
+#define ATTRIBUTE_PRINTF_1 ATTRIBUTE_PRINTF(1, 2)
+#define ATTRIBUTE_PRINTF_2 ATTRIBUTE_PRINTF(2, 3)
+#define ATTRIBUTE_PRINTF_3 ATTRIBUTE_PRINTF(3, 4)
+#define ATTRIBUTE_PRINTF_4 ATTRIBUTE_PRINTF(4, 5)
+#define ATTRIBUTE_PRINTF_5 ATTRIBUTE_PRINTF(5, 6)
+#endif /* ATTRIBUTE_PRINTF */
+
+#endif	/* ansidecl.h	*/
diff -purN linux-2.5/arch/ppc64/kdb/bfd.h linuxppc64-2.5/arch/ppc64/kdb/bfd.h
--- linux-2.5/arch/ppc64/kdb/bfd.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/bfd.h	2003-10-13 16:16:57.000000000 +0000
@@ -0,0 +1,3706 @@
+/* DO NOT EDIT!  -*- buffer-read-only: t -*-  This file is automatically 
+   generated from "bfd-in.h", "init.c", "opncls.c", "libbfd.c", 
+   "section.c", "archures.c", "reloc.c", "syms.c", "bfd.c", "archive.c", 
+   "corefile.c", "targets.c" and "format.c".
+   Run "make headers" in your build bfd/ to regenerate.  */
+
+/* Main header file for the bfd library -- portable access to object files.
+   Copyright 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999,
+   2000, 2001
+   Free Software Foundation, Inc.
+   Contributed by Cygnus Support.
+
+This file is part of BFD, the Binary File Descriptor library.
+
+This program is free software; you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation; either version 2 of the License, or
+(at your option) any later version.
+
+This program is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; if not, write to the Free Software
+Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  */
+
+#ifndef __BFD_H_SEEN__
+#define __BFD_H_SEEN__
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#include "ansidecl.h"
+#include "symcat.h"
+#if defined (__STDC__) || defined (ALMOST_STDC) || defined (HAVE_STRINGIZE)
+#ifndef SABER
+/* This hack is to avoid a problem with some strict ANSI C preprocessors.
+   The problem is, "32_" is not a valid preprocessing token, and we don't
+   want extra underscores (e.g., "nlm_32_").  The XCONCAT2 macro will
+   cause the inner CONCAT2 macros to be evaluated first, producing
+   still-valid pp-tokens.  Then the final concatenation can be done.  */
+#undef CONCAT4
+#define CONCAT4(a,b,c,d) XCONCAT2(CONCAT2(a,b),CONCAT2(c,d))
+#endif
+#endif
+
+#define BFD_VERSION 211920007
+#define BFD_VERSION_DATE 20011016
+#define BFD_VERSION_STRING "2.11.92.0.7 20011016 Debian\/GNU Linux"
+
+/* The word size used by BFD on the host.  This may be 64 with a 32
+   bit target if the host is 64 bit, or if other 64 bit targets have
+   been selected with --enable-targets, or if --enable-64-bit-bfd.  */
+#define BFD_ARCH_SIZE 64
+
+/* The word size of the default bfd target.  */
+#define BFD_DEFAULT_TARGET_SIZE 32
+
+#define BFD_HOST_64BIT_LONG 1
+#define BFD_HOST_64_BIT long
+#define BFD_HOST_U_64_BIT unsigned long
+
+#if BFD_ARCH_SIZE >= 64
+#define BFD64
+#endif
+
+#ifndef INLINE
+#if __GNUC__ >= 2
+#define INLINE __inline__
+#else
+#define INLINE
+#endif
+#endif
+
+/* forward declaration */
+typedef struct _bfd bfd;
+
+/* To squelch erroneous compiler warnings ("illegal pointer
+   combination") from the SVR3 compiler, we would like to typedef
+   boolean to int (it doesn't like functions which return boolean.
+   Making sure they are never implicitly declared to return int
+   doesn't seem to help).  But this file is not configured based on
+   the host.  */
+/* General rules: functions which are boolean return true on success
+   and false on failure (unless they're a predicate).   -- bfd.doc */
+/* I'm sure this is going to break something and someone is going to
+   force me to change it.  */
+/* typedef enum boolean {false, true} boolean; */
+/* Yup, SVR4 has a "typedef enum boolean" in <sys/types.h>  -fnf */
+/* It gets worse if the host also defines a true/false enum... -sts */
+/* And even worse if your compiler has built-in boolean types... -law */
+#if defined (__GNUG__) && (__GNUC__ > 2 || (__GNUC__ == 2 && __GNUC_MINOR__ >= 6))
+#define TRUE_FALSE_ALREADY_DEFINED
+#endif
+#ifdef MPW
+/* Pre-emptive strike - get the file with the enum.  */
+#include <Types.h>
+#define TRUE_FALSE_ALREADY_DEFINED
+#endif /* MPW */
+#ifndef TRUE_FALSE_ALREADY_DEFINED
+typedef enum bfd_boolean {false, true} boolean;
+#define BFD_TRUE_FALSE
+#else
+/* Use enum names that will appear nowhere else.  */
+typedef enum bfd_boolean {bfd_fffalse, bfd_tttrue} boolean;
+#endif
+
+/* Support for different sizes of target format ints and addresses.
+   If the type `long' is at least 64 bits, BFD_HOST_64BIT_LONG will be
+   set to 1 above.  Otherwise, if gcc is being used, this code will
+   use gcc's "long long" type.  Otherwise, BFD_HOST_64_BIT must be
+   defined above.  */
+
+#ifndef BFD_HOST_64_BIT
+# if BFD_HOST_64BIT_LONG
+#  define BFD_HOST_64_BIT long
+#  define BFD_HOST_U_64_BIT unsigned long
+# else
+#  ifdef __GNUC__
+#   if __GNUC__ >= 2
+#    define BFD_HOST_64_BIT long long
+#    define BFD_HOST_U_64_BIT unsigned long long
+#   endif /* __GNUC__ >= 2 */
+#  endif /* ! defined (__GNUC__) */
+# endif /* ! BFD_HOST_64BIT_LONG */
+#endif /* ! defined (BFD_HOST_64_BIT) */
+
+#ifdef BFD64
+
+#ifndef BFD_HOST_64_BIT
+ #error No 64 bit integer type available
+#endif /* ! defined (BFD_HOST_64_BIT) */
+
+typedef BFD_HOST_U_64_BIT bfd_vma;
+typedef BFD_HOST_64_BIT bfd_signed_vma;
+typedef BFD_HOST_U_64_BIT bfd_size_type;
+typedef BFD_HOST_U_64_BIT symvalue;
+
+#ifndef fprintf_vma
+#if BFD_HOST_64BIT_LONG
+#define sprintf_vma(s,x) sprintf (s, "%016lx", x)
+#define fprintf_vma(f,x) fprintf (f, "%016lx", x)
+#else
+#define _bfd_int64_low(x) ((unsigned long) (((x) & 0xffffffff)))
+#define _bfd_int64_high(x) ((unsigned long) (((x) >> 32) & 0xffffffff))
+#define fprintf_vma(s,x) \
+  fprintf ((s), "%08lx%08lx", _bfd_int64_high (x), _bfd_int64_low (x))
+#define sprintf_vma(s,x) \
+  sprintf ((s), "%08lx%08lx", _bfd_int64_high (x), _bfd_int64_low (x))
+#endif
+#endif
+
+#else /* not BFD64  */
+
+/* Represent a target address.  Also used as a generic unsigned type
+   which is guaranteed to be big enough to hold any arithmetic types
+   we need to deal with.  */
+typedef unsigned long bfd_vma;
+
+/* A generic signed type which is guaranteed to be big enough to hold any
+   arithmetic types we need to deal with.  Can be assumed to be compatible
+   with bfd_vma in the same way that signed and unsigned ints are compatible
+   (as parameters, in assignment, etc).  */
+typedef long bfd_signed_vma;
+
+typedef unsigned long symvalue;
+typedef unsigned long bfd_size_type;
+
+/* Print a bfd_vma x on stream s.  */
+#define fprintf_vma(s,x) fprintf (s, "%08lx", x)
+#define sprintf_vma(s,x) sprintf (s, "%08lx", x)
+
+#endif /* not BFD64  */
+
+/* A pointer to a position in a file.  */
+/* FIXME:  This should be using off_t from <sys/types.h>.
+   For now, try to avoid breaking stuff by not including <sys/types.h> here.
+   This will break on systems with 64-bit file offsets (e.g. 4.4BSD).
+   Probably the best long-term answer is to avoid using file_ptr AND off_t
+   in this header file, and to handle this in the BFD implementation
+   rather than in its interface.  */
+/* typedef off_t	file_ptr; */
+typedef bfd_signed_vma file_ptr;
+typedef bfd_vma ufile_ptr;
+
+extern void bfd_sprintf_vma PARAMS ((bfd *, char *, bfd_vma));
+extern void bfd_fprintf_vma PARAMS ((bfd *, PTR, bfd_vma));
+
+#define printf_vma(x) fprintf_vma(stdout,x)
+#define bfd_printf_vma(abfd,x) bfd_fprintf_vma (abfd,stdout,x)
+
+typedef unsigned int flagword;	/* 32 bits of flags */
+typedef unsigned char bfd_byte;
+
+/** File formats */
+
+typedef enum bfd_format {
+	      bfd_unknown = 0,	/* file format is unknown */
+	      bfd_object,	/* linker/assember/compiler output */
+	      bfd_archive,	/* object archive file */
+	      bfd_core,		/* core dump */
+	      bfd_type_end}	/* marks the end; don't use it! */
+         bfd_format;
+
+/* Values that may appear in the flags field of a BFD.  These also
+   appear in the object_flags field of the bfd_target structure, where
+   they indicate the set of flags used by that backend (not all flags
+   are meaningful for all object file formats) (FIXME: at the moment,
+   the object_flags values have mostly just been copied from backend
+   to another, and are not necessarily correct).  */
+
+/* No flags.  */
+#define BFD_NO_FLAGS   	0x00
+
+/* BFD contains relocation entries.  */
+#define HAS_RELOC   	0x01
+
+/* BFD is directly executable.  */
+#define EXEC_P      	0x02
+
+/* BFD has line number information (basically used for F_LNNO in a
+   COFF header).  */
+#define HAS_LINENO  	0x04
+
+/* BFD has debugging information.  */
+#define HAS_DEBUG   	0x08
+
+/* BFD has symbols.  */
+#define HAS_SYMS    	0x10
+
+/* BFD has local symbols (basically used for F_LSYMS in a COFF
+   header).  */
+#define HAS_LOCALS  	0x20
+
+/* BFD is a dynamic object.  */
+#define DYNAMIC     	0x40
+
+/* Text section is write protected (if D_PAGED is not set, this is
+   like an a.out NMAGIC file) (the linker sets this by default, but
+   clears it for -r or -N).  */
+#define WP_TEXT     	0x80
+
+/* BFD is dynamically paged (this is like an a.out ZMAGIC file) (the
+   linker sets this by default, but clears it for -r or -n or -N).  */
+#define D_PAGED     	0x100
+
+/* BFD is relaxable (this means that bfd_relax_section may be able to
+   do something) (sometimes bfd_relax_section can do something even if
+   this is not set).  */
+#define BFD_IS_RELAXABLE 0x200
+
+/* This may be set before writing out a BFD to request using a
+   traditional format.  For example, this is used to request that when
+   writing out an a.out object the symbols not be hashed to eliminate
+   duplicates.  */
+#define BFD_TRADITIONAL_FORMAT 0x400
+
+/* This flag indicates that the BFD contents are actually cached in
+   memory.  If this is set, iostream points to a bfd_in_memory struct.  */
+#define BFD_IN_MEMORY 0x800
+
+/* symbols and relocation */
+
+/* A count of carsyms (canonical archive symbols).  */
+typedef unsigned long symindex;
+
+/* How to perform a relocation.  */
+typedef const struct reloc_howto_struct reloc_howto_type;
+
+#define BFD_NO_MORE_SYMBOLS ((symindex) ~0)
+
+/* General purpose part of a symbol X;
+   target specific parts are in libcoff.h, libaout.h, etc.  */
+
+#define bfd_get_section(x) ((x)->section)
+#define bfd_get_output_section(x) ((x)->section->output_section)
+#define bfd_set_section(x,y) ((x)->section) = (y)
+#define bfd_asymbol_base(x) ((x)->section->vma)
+#define bfd_asymbol_value(x) (bfd_asymbol_base(x) + (x)->value)
+#define bfd_asymbol_name(x) ((x)->name)
+/*Perhaps future: #define bfd_asymbol_bfd(x) ((x)->section->owner)*/
+#define bfd_asymbol_bfd(x) ((x)->the_bfd)
+#define bfd_asymbol_flavour(x) (bfd_asymbol_bfd(x)->xvec->flavour)
+
+/* A canonical archive symbol.  */
+/* This is a type pun with struct ranlib on purpose! */
+typedef struct carsym {
+  char *name;
+  file_ptr file_offset;		/* look here to find the file */
+} carsym;			/* to make these you call a carsymogen */
+
+/* Used in generating armaps (archive tables of contents).
+   Perhaps just a forward definition would do? */
+struct orl {			/* output ranlib */
+  char **name;			/* symbol name */
+  union {
+    file_ptr pos;
+    bfd *abfd;
+  } u;				/* bfd* or file position */
+  int namidx;			/* index into string table */
+};
+
+/* Linenumber stuff */
+typedef struct lineno_cache_entry {
+  unsigned int line_number;	/* Linenumber from start of function*/
+  union {
+    struct symbol_cache_entry *sym; /* Function name */
+    bfd_vma offset;	    /* Offset into section */
+  } u;
+} alent;
+
+/* object and core file sections */
+
+#define	align_power(addr, align)	\
+	( ((addr) + ((1<<(align))-1)) & (-1 << (align)))
+
+typedef struct sec *sec_ptr;
+
+#define bfd_get_section_name(bfd, ptr) ((ptr)->name + 0)
+#define bfd_get_section_vma(bfd, ptr) ((ptr)->vma + 0)
+#define bfd_get_section_alignment(bfd, ptr) ((ptr)->alignment_power + 0)
+#define bfd_section_name(bfd, ptr) ((ptr)->name)
+#define bfd_section_size(bfd, ptr) (bfd_get_section_size_before_reloc(ptr))
+#define bfd_section_vma(bfd, ptr) ((ptr)->vma)
+#define bfd_section_lma(bfd, ptr) ((ptr)->lma)
+#define bfd_section_alignment(bfd, ptr) ((ptr)->alignment_power)
+#define bfd_get_section_flags(bfd, ptr) ((ptr)->flags + 0)
+#define bfd_get_section_userdata(bfd, ptr) ((ptr)->userdata)
+
+#define bfd_is_com_section(ptr) (((ptr)->flags & SEC_IS_COMMON) != 0)
+
+#define bfd_set_section_vma(bfd, ptr, val) (((ptr)->vma = (ptr)->lma= (val)), ((ptr)->user_set_vma = (boolean)true), true)
+#define bfd_set_section_alignment(bfd, ptr, val) (((ptr)->alignment_power = (val)),true)
+#define bfd_set_section_userdata(bfd, ptr, val) (((ptr)->userdata = (val)),true)
+
+typedef struct stat stat_type;
+
+typedef enum bfd_print_symbol
+{
+  bfd_print_symbol_name,
+  bfd_print_symbol_more,
+  bfd_print_symbol_all
+} bfd_print_symbol_type;
+
+/* Information about a symbol that nm needs.  */
+
+typedef struct _symbol_info
+{
+  symvalue value;
+  char type;
+  const char *name;            /* Symbol name.  */
+  unsigned char stab_type;     /* Stab type.  */
+  char stab_other;             /* Stab other.  */
+  short stab_desc;             /* Stab desc.  */
+  const char *stab_name;       /* String for stab type.  */
+} symbol_info;
+
+/* Get the name of a stabs type code.  */
+
+extern const char *bfd_get_stab_name PARAMS ((int));
+
+/* Hash table routines.  There is no way to free up a hash table.  */
+
+/* An element in the hash table.  Most uses will actually use a larger
+   structure, and an instance of this will be the first field.  */
+
+struct bfd_hash_entry
+{
+  /* Next entry for this hash code.  */
+  struct bfd_hash_entry *next;
+  /* String being hashed.  */
+  const char *string;
+  /* Hash code.  This is the full hash code, not the index into the
+     table.  */
+  unsigned long hash;
+};
+
+/* A hash table.  */
+
+struct bfd_hash_table
+{
+  /* The hash array.  */
+  struct bfd_hash_entry **table;
+  /* The number of slots in the hash table.  */
+  unsigned int size;
+  /* A function used to create new elements in the hash table.  The
+     first entry is itself a pointer to an element.  When this
+     function is first invoked, this pointer will be NULL.  However,
+     having the pointer permits a hierarchy of method functions to be
+     built each of which calls the function in the superclass.  Thus
+     each function should be written to allocate a new block of memory
+     only if the argument is NULL.  */
+  struct bfd_hash_entry *(*newfunc) PARAMS ((struct bfd_hash_entry *,
+					     struct bfd_hash_table *,
+					     const char *));
+   /* An objalloc for this hash table.  This is a struct objalloc *,
+     but we use PTR to avoid requiring the inclusion of objalloc.h.  */
+  PTR memory;
+};
+
+/* Initialize a hash table.  */
+extern boolean bfd_hash_table_init
+  PARAMS ((struct bfd_hash_table *,
+	   struct bfd_hash_entry *(*) (struct bfd_hash_entry *,
+				       struct bfd_hash_table *,
+				       const char *)));
+
+/* Initialize a hash table specifying a size.  */
+extern boolean bfd_hash_table_init_n
+  PARAMS ((struct bfd_hash_table *,
+	   struct bfd_hash_entry *(*) (struct bfd_hash_entry *,
+				       struct bfd_hash_table *,
+				       const char *),
+	   unsigned int size));
+
+/* Free up a hash table.  */
+extern void bfd_hash_table_free PARAMS ((struct bfd_hash_table *));
+
+/* Look up a string in a hash table.  If CREATE is true, a new entry
+   will be created for this string if one does not already exist.  The
+   COPY argument must be true if this routine should copy the string
+   into newly allocated memory when adding an entry.  */
+extern struct bfd_hash_entry *bfd_hash_lookup
+  PARAMS ((struct bfd_hash_table *, const char *, boolean create,
+	   boolean copy));
+
+/* Replace an entry in a hash table.  */
+extern void bfd_hash_replace
+  PARAMS ((struct bfd_hash_table *, struct bfd_hash_entry *old,
+	   struct bfd_hash_entry *nw));
+
+/* Base method for creating a hash table entry.  */
+extern struct bfd_hash_entry *bfd_hash_newfunc
+  PARAMS ((struct bfd_hash_entry *, struct bfd_hash_table *,
+	   const char *));
+
+/* Grab some space for a hash table entry.  */
+extern PTR bfd_hash_allocate PARAMS ((struct bfd_hash_table *,
+				      unsigned int));
+
+/* Traverse a hash table in a random order, calling a function on each
+   element.  If the function returns false, the traversal stops.  The
+   INFO argument is passed to the function.  */
+extern void bfd_hash_traverse PARAMS ((struct bfd_hash_table *,
+				       boolean (*) (struct bfd_hash_entry *,
+						    PTR),
+				       PTR info));
+
+#define COFF_SWAP_TABLE (PTR) &bfd_coff_std_swap_table
+
+/* User program access to BFD facilities */
+
+/* Direct I/O routines, for programs which know more about the object
+   file than BFD does.  Use higher level routines if possible.  */
+
+extern bfd_size_type bfd_bread PARAMS ((PTR, bfd_size_type, bfd *));
+extern bfd_size_type bfd_bwrite PARAMS ((const PTR, bfd_size_type, bfd *));
+extern int bfd_seek PARAMS ((bfd *, file_ptr, int));
+extern ufile_ptr bfd_tell PARAMS ((bfd *));
+extern int bfd_flush PARAMS ((bfd *));
+extern int bfd_stat PARAMS ((bfd *, struct stat *));
+
+/* Deprecated old routines.  */
+#if __GNUC__
+#define bfd_read(BUF, ELTSIZE, NITEMS, ABFD)				\
+  (warn_deprecated ("bfd_read", __FILE__, __LINE__, __FUNCTION__),	\
+   bfd_bread ((BUF), (ELTSIZE) * (NITEMS), (ABFD)))
+#define bfd_write(BUF, ELTSIZE, NITEMS, ABFD)				\
+  (warn_deprecated ("bfd_write", __FILE__, __LINE__, __FUNCTION__),	\
+   bfd_bwrite ((BUF), (ELTSIZE) * (NITEMS), (ABFD)))
+#else
+#define bfd_read(BUF, ELTSIZE, NITEMS, ABFD)				\
+  (warn_deprecated ("bfd_read", (const char *) 0, 0, (const char *) 0), \
+   bfd_bread ((BUF), (ELTSIZE) * (NITEMS), (ABFD)))
+#define bfd_write(BUF, ELTSIZE, NITEMS, ABFD)				\
+  (warn_deprecated ("bfd_write", (const char *) 0, 0, (const char *) 0),\
+   bfd_bwrite ((BUF), (ELTSIZE) * (NITEMS), (ABFD)))
+#endif
+extern void warn_deprecated
+  PARAMS ((const char *, const char *, int, const char *));
+
+/* Cast from const char * to char * so that caller can assign to
+   a char * without a warning.  */
+#define bfd_get_filename(abfd) ((char *) (abfd)->filename)
+#define bfd_get_cacheable(abfd) ((abfd)->cacheable)
+#define bfd_get_format(abfd) ((abfd)->format)
+#define bfd_get_target(abfd) ((abfd)->xvec->name)
+#define bfd_get_flavour(abfd) ((abfd)->xvec->flavour)
+#define bfd_family_coff(abfd) \
+  (bfd_get_flavour (abfd) == bfd_target_coff_flavour || \
+   bfd_get_flavour (abfd) == bfd_target_xcoff_flavour)
+#define bfd_big_endian(abfd) ((abfd)->xvec->byteorder == BFD_ENDIAN_BIG)
+#define bfd_little_endian(abfd) ((abfd)->xvec->byteorder == BFD_ENDIAN_LITTLE)
+#define bfd_header_big_endian(abfd) \
+  ((abfd)->xvec->header_byteorder == BFD_ENDIAN_BIG)
+#define bfd_header_little_endian(abfd) \
+  ((abfd)->xvec->header_byteorder == BFD_ENDIAN_LITTLE)
+#define bfd_get_file_flags(abfd) ((abfd)->flags)
+#define bfd_applicable_file_flags(abfd) ((abfd)->xvec->object_flags)
+#define bfd_applicable_section_flags(abfd) ((abfd)->xvec->section_flags)
+#define bfd_my_archive(abfd) ((abfd)->my_archive)
+#define bfd_has_map(abfd) ((abfd)->has_armap)
+
+#define bfd_valid_reloc_types(abfd) ((abfd)->xvec->valid_reloc_types)
+#define bfd_usrdata(abfd) ((abfd)->usrdata)
+
+#define bfd_get_start_address(abfd) ((abfd)->start_address)
+#define bfd_get_symcount(abfd) ((abfd)->symcount)
+#define bfd_get_outsymbols(abfd) ((abfd)->outsymbols)
+#define bfd_count_sections(abfd) ((abfd)->section_count)
+
+#define bfd_get_symbol_leading_char(abfd) ((abfd)->xvec->symbol_leading_char)
+
+#define bfd_set_cacheable(abfd,bool) (((abfd)->cacheable = (boolean) (bool)), true)
+
+extern boolean bfd_cache_close PARAMS ((bfd *abfd));
+/* NB: This declaration should match the autogenerated one in libbfd.h.  */
+
+extern boolean bfd_record_phdr
+  PARAMS ((bfd *, unsigned long, boolean, flagword, boolean, bfd_vma,
+	   boolean, boolean, unsigned int, struct sec **));
+
+/* Byte swapping routines.  */
+
+bfd_vma		bfd_getb64	   PARAMS ((const unsigned char *));
+bfd_vma 	bfd_getl64	   PARAMS ((const unsigned char *));
+bfd_signed_vma	bfd_getb_signed_64 PARAMS ((const unsigned char *));
+bfd_signed_vma	bfd_getl_signed_64 PARAMS ((const unsigned char *));
+bfd_vma		bfd_getb32	   PARAMS ((const unsigned char *));
+bfd_vma		bfd_getl32	   PARAMS ((const unsigned char *));
+bfd_signed_vma	bfd_getb_signed_32 PARAMS ((const unsigned char *));
+bfd_signed_vma	bfd_getl_signed_32 PARAMS ((const unsigned char *));
+bfd_vma		bfd_getb16	   PARAMS ((const unsigned char *));
+bfd_vma		bfd_getl16	   PARAMS ((const unsigned char *));
+bfd_signed_vma	bfd_getb_signed_16 PARAMS ((const unsigned char *));
+bfd_signed_vma	bfd_getl_signed_16 PARAMS ((const unsigned char *));
+void		bfd_putb64	   PARAMS ((bfd_vma, unsigned char *));
+void		bfd_putl64	   PARAMS ((bfd_vma, unsigned char *));
+void		bfd_putb32	   PARAMS ((bfd_vma, unsigned char *));
+void		bfd_putl32	   PARAMS ((bfd_vma, unsigned char *));
+void		bfd_putb16	   PARAMS ((bfd_vma, unsigned char *));
+void		bfd_putl16	   PARAMS ((bfd_vma, unsigned char *));
+
+/* Byte swapping routines which take size and endiannes as arguments.  */
+
+bfd_vma         bfd_get_bits       PARAMS ((bfd_byte *, int, boolean));
+void            bfd_put_bits       PARAMS ((bfd_vma, bfd_byte *, int, boolean));
+
+/* Externally visible ECOFF routines.  */
+
+#if defined(__STDC__) || defined(ALMOST_STDC)
+struct ecoff_debug_info;
+struct ecoff_debug_swap;
+struct ecoff_extr;
+struct symbol_cache_entry;
+struct bfd_link_info;
+struct bfd_link_hash_entry;
+struct bfd_elf_version_tree;
+#endif
+extern bfd_vma bfd_ecoff_get_gp_value PARAMS ((bfd * abfd));
+extern boolean bfd_ecoff_set_gp_value PARAMS ((bfd *abfd, bfd_vma gp_value));
+extern boolean bfd_ecoff_set_regmasks
+  PARAMS ((bfd *abfd, unsigned long gprmask, unsigned long fprmask,
+	   unsigned long *cprmask));
+extern PTR bfd_ecoff_debug_init
+  PARAMS ((bfd *output_bfd, struct ecoff_debug_info *output_debug,
+	   const struct ecoff_debug_swap *output_swap,
+	   struct bfd_link_info *));
+extern void bfd_ecoff_debug_free
+  PARAMS ((PTR handle, bfd *output_bfd, struct ecoff_debug_info *output_debug,
+	   const struct ecoff_debug_swap *output_swap,
+	   struct bfd_link_info *));
+extern boolean bfd_ecoff_debug_accumulate
+  PARAMS ((PTR handle, bfd *output_bfd, struct ecoff_debug_info *output_debug,
+	   const struct ecoff_debug_swap *output_swap,
+	   bfd *input_bfd, struct ecoff_debug_info *input_debug,
+	   const struct ecoff_debug_swap *input_swap,
+	   struct bfd_link_info *));
+extern boolean bfd_ecoff_debug_accumulate_other
+  PARAMS ((PTR handle, bfd *output_bfd, struct ecoff_debug_info *output_debug,
+	   const struct ecoff_debug_swap *output_swap, bfd *input_bfd,
+	   struct bfd_link_info *));
+extern boolean bfd_ecoff_debug_externals
+  PARAMS ((bfd *abfd, struct ecoff_debug_info *debug,
+	   const struct ecoff_debug_swap *swap,
+	   boolean relocateable,
+	   boolean (*get_extr) (struct symbol_cache_entry *,
+				struct ecoff_extr *),
+	   void (*set_index) (struct symbol_cache_entry *,
+			      bfd_size_type)));
+extern boolean bfd_ecoff_debug_one_external
+  PARAMS ((bfd *abfd, struct ecoff_debug_info *debug,
+	   const struct ecoff_debug_swap *swap,
+	   const char *name, struct ecoff_extr *esym));
+extern bfd_size_type bfd_ecoff_debug_size
+  PARAMS ((bfd *abfd, struct ecoff_debug_info *debug,
+	   const struct ecoff_debug_swap *swap));
+extern boolean bfd_ecoff_write_debug
+  PARAMS ((bfd *abfd, struct ecoff_debug_info *debug,
+	   const struct ecoff_debug_swap *swap, file_ptr where));
+extern boolean bfd_ecoff_write_accumulated_debug
+  PARAMS ((PTR handle, bfd *abfd, struct ecoff_debug_info *debug,
+	   const struct ecoff_debug_swap *swap,
+	   struct bfd_link_info *info, file_ptr where));
+extern boolean bfd_mips_ecoff_create_embedded_relocs
+  PARAMS ((bfd *, struct bfd_link_info *, struct sec *, struct sec *,
+	   char **));
+
+/* Externally visible ELF routines.  */
+
+struct bfd_link_needed_list
+{
+  struct bfd_link_needed_list *next;
+  bfd *by;
+  const char *name;
+};
+
+extern boolean bfd_elf32_record_link_assignment
+  PARAMS ((bfd *, struct bfd_link_info *, const char *, boolean));
+extern boolean bfd_elf64_record_link_assignment
+  PARAMS ((bfd *, struct bfd_link_info *, const char *, boolean));
+extern struct bfd_link_needed_list *bfd_elf_get_needed_list
+  PARAMS ((bfd *, struct bfd_link_info *));
+extern boolean bfd_elf_get_bfd_needed_list
+  PARAMS ((bfd *, struct bfd_link_needed_list **));
+extern boolean bfd_elf32_size_dynamic_sections
+  PARAMS ((bfd *, const char *, const char *, const char *,
+	   const char * const *, struct bfd_link_info *, struct sec **,
+	   struct bfd_elf_version_tree *));
+extern boolean bfd_elf64_size_dynamic_sections
+  PARAMS ((bfd *, const char *, const char *, const char *,
+	   const char * const *, struct bfd_link_info *, struct sec **,
+	   struct bfd_elf_version_tree *));
+extern void bfd_elf_set_dt_needed_name PARAMS ((bfd *, const char *));
+extern void bfd_elf_set_dt_needed_soname PARAMS ((bfd *, const char *));
+extern const char *bfd_elf_get_dt_soname PARAMS ((bfd *));
+extern struct bfd_link_needed_list *bfd_elf_get_runpath_list
+  PARAMS ((bfd *, struct bfd_link_info *));
+
+/* Return an upper bound on the number of bytes required to store a
+   copy of ABFD's program header table entries.  Return -1 if an error
+   occurs; bfd_get_error will return an appropriate code.  */
+extern long bfd_get_elf_phdr_upper_bound PARAMS ((bfd *abfd));
+
+/* Copy ABFD's program header table entries to *PHDRS.  The entries
+   will be stored as an array of Elf_Internal_Phdr structures, as
+   defined in include/elf/internal.h.  To find out how large the
+   buffer needs to be, call bfd_get_elf_phdr_upper_bound.
+
+   Return the number of program header table entries read, or -1 if an
+   error occurs; bfd_get_error will return an appropriate code.  */
+extern int bfd_get_elf_phdrs PARAMS ((bfd *abfd, void *phdrs));
+
+/* Return the arch_size field of an elf bfd, or -1 if not elf.  */
+extern int bfd_get_arch_size PARAMS ((bfd *));
+
+/* Return true if address "naturally" sign extends, or -1 if not elf.  */
+extern int bfd_get_sign_extend_vma PARAMS ((bfd *));
+
+extern boolean bfd_m68k_elf32_create_embedded_relocs
+  PARAMS ((bfd *, struct bfd_link_info *, struct sec *, struct sec *,
+	   char **));
+
+/* SunOS shared library support routines for the linker.  */
+
+extern struct bfd_link_needed_list *bfd_sunos_get_needed_list
+  PARAMS ((bfd *, struct bfd_link_info *));
+extern boolean bfd_sunos_record_link_assignment
+  PARAMS ((bfd *, struct bfd_link_info *, const char *));
+extern boolean bfd_sunos_size_dynamic_sections
+  PARAMS ((bfd *, struct bfd_link_info *, struct sec **, struct sec **,
+	   struct sec **));
+
+/* Linux shared library support routines for the linker.  */
+
+extern boolean bfd_i386linux_size_dynamic_sections
+  PARAMS ((bfd *, struct bfd_link_info *));
+extern boolean bfd_m68klinux_size_dynamic_sections
+  PARAMS ((bfd *, struct bfd_link_info *));
+extern boolean bfd_sparclinux_size_dynamic_sections
+  PARAMS ((bfd *, struct bfd_link_info *));
+
+/* mmap hacks */
+
+struct _bfd_window_internal;
+typedef struct _bfd_window_internal bfd_window_internal;
+
+typedef struct _bfd_window {
+  /* What the user asked for.  */
+  PTR data;
+  bfd_size_type size;
+  /* The actual window used by BFD.  Small user-requested read-only
+     regions sharing a page may share a single window into the object
+     file.  Read-write versions shouldn't until I've fixed things to
+     keep track of which portions have been claimed by the
+     application; don't want to give the same region back when the
+     application wants two writable copies!  */
+  struct _bfd_window_internal *i;
+} bfd_window;
+
+extern void bfd_init_window PARAMS ((bfd_window *));
+extern void bfd_free_window PARAMS ((bfd_window *));
+extern boolean bfd_get_file_window
+  PARAMS ((bfd *, file_ptr, bfd_size_type, bfd_window *, boolean));
+
+/* XCOFF support routines for the linker.  */
+
+extern boolean bfd_xcoff_link_record_set
+  PARAMS ((bfd *, struct bfd_link_info *, struct bfd_link_hash_entry *,
+	   bfd_size_type));
+extern boolean bfd_xcoff_import_symbol
+  PARAMS ((bfd *, struct bfd_link_info *, struct bfd_link_hash_entry *,
+	   bfd_vma, const char *, const char *, const char *, unsigned int));
+extern boolean bfd_xcoff_export_symbol
+  PARAMS ((bfd *, struct bfd_link_info *, struct bfd_link_hash_entry *));
+extern boolean bfd_xcoff_link_count_reloc
+  PARAMS ((bfd *, struct bfd_link_info *, const char *));
+extern boolean bfd_xcoff_record_link_assignment
+  PARAMS ((bfd *, struct bfd_link_info *, const char *));
+extern boolean bfd_xcoff_size_dynamic_sections
+  PARAMS ((bfd *, struct bfd_link_info *, const char *, const char *,
+	   unsigned long, unsigned long, unsigned long, boolean,
+	   int, boolean, boolean, struct sec **));
+
+/* Externally visible COFF routines.  */
+
+#if defined(__STDC__) || defined(ALMOST_STDC)
+struct internal_syment;
+union internal_auxent;
+#endif
+
+extern boolean bfd_coff_get_syment
+  PARAMS ((bfd *, struct symbol_cache_entry *, struct internal_syment *));
+
+extern boolean bfd_coff_get_auxent
+  PARAMS ((bfd *, struct symbol_cache_entry *, int, union internal_auxent *));
+
+extern boolean bfd_coff_set_symbol_class
+  PARAMS ((bfd *, struct symbol_cache_entry *, unsigned int));
+
+extern boolean bfd_m68k_coff_create_embedded_relocs
+  PARAMS ((bfd *, struct bfd_link_info *, struct sec *, struct sec *,
+	   char **));
+
+/* ARM Interworking support.  Called from linker.  */
+extern boolean bfd_arm_allocate_interworking_sections
+  PARAMS ((struct bfd_link_info *));
+
+extern boolean bfd_arm_process_before_allocation
+  PARAMS ((bfd *, struct bfd_link_info *, int));
+
+extern boolean bfd_arm_get_bfd_for_interworking
+  PARAMS ((bfd *, struct bfd_link_info *));
+
+/* PE ARM Interworking support.  Called from linker.  */
+extern boolean bfd_arm_pe_allocate_interworking_sections
+  PARAMS ((struct bfd_link_info *));
+
+extern boolean bfd_arm_pe_process_before_allocation
+  PARAMS ((bfd *, struct bfd_link_info *, int));
+
+extern boolean bfd_arm_pe_get_bfd_for_interworking
+  PARAMS ((bfd *, struct bfd_link_info *));
+
+/* ELF ARM Interworking support.  Called from linker.  */
+extern boolean bfd_elf32_arm_allocate_interworking_sections
+  PARAMS ((struct bfd_link_info *));
+
+extern boolean bfd_elf32_arm_process_before_allocation
+  PARAMS ((bfd *, struct bfd_link_info *, int));
+
+extern boolean bfd_elf32_arm_get_bfd_for_interworking
+  PARAMS ((bfd *, struct bfd_link_info *));
+
+/* TI COFF load page support.  */
+extern void bfd_ticoff_set_section_load_page
+  PARAMS ((struct sec *, int));
+
+extern int bfd_ticoff_get_section_load_page
+  PARAMS ((struct sec *));
+
+/* And more from the source.  */
+void
+bfd_init PARAMS ((void));
+
+bfd *
+bfd_openr PARAMS ((const char *filename, const char *target));
+
+bfd *
+bfd_fdopenr PARAMS ((const char *filename, const char *target, int fd));
+
+bfd *
+bfd_openstreamr PARAMS ((const char *, const char *, PTR));
+
+bfd *
+bfd_openw PARAMS ((const char *filename, const char *target));
+
+boolean
+bfd_close PARAMS ((bfd *abfd));
+
+boolean
+bfd_close_all_done PARAMS ((bfd *));
+
+bfd *
+bfd_create PARAMS ((const char *filename, bfd *templ));
+
+boolean
+bfd_make_writable PARAMS ((bfd *abfd));
+
+boolean
+bfd_make_readable PARAMS ((bfd *abfd));
+
+
+/* Byte swapping macros for user section data.  */
+
+#define bfd_put_8(abfd, val, ptr) \
+                ((void) (*((unsigned char *) (ptr)) = (unsigned char) (val)))
+#define bfd_put_signed_8 \
+               bfd_put_8
+#define bfd_get_8(abfd, ptr) \
+                (*(unsigned char *) (ptr) & 0xff)
+#define bfd_get_signed_8(abfd, ptr) \
+               (((*(unsigned char *) (ptr) & 0xff) ^ 0x80) - 0x80)
+
+#define bfd_put_16(abfd, val, ptr) \
+                BFD_SEND(abfd, bfd_putx16, ((val),(ptr)))
+#define bfd_put_signed_16 \
+                bfd_put_16
+#define bfd_get_16(abfd, ptr) \
+                BFD_SEND(abfd, bfd_getx16, (ptr))
+#define bfd_get_signed_16(abfd, ptr) \
+                BFD_SEND (abfd, bfd_getx_signed_16, (ptr))
+
+#define bfd_put_32(abfd, val, ptr) \
+                BFD_SEND(abfd, bfd_putx32, ((val),(ptr)))
+#define bfd_put_signed_32 \
+                bfd_put_32
+#define bfd_get_32(abfd, ptr) \
+                BFD_SEND(abfd, bfd_getx32, (ptr))
+#define bfd_get_signed_32(abfd, ptr) \
+                BFD_SEND(abfd, bfd_getx_signed_32, (ptr))
+
+#define bfd_put_64(abfd, val, ptr) \
+                BFD_SEND(abfd, bfd_putx64, ((val), (ptr)))
+#define bfd_put_signed_64 \
+                bfd_put_64
+#define bfd_get_64(abfd, ptr) \
+                BFD_SEND(abfd, bfd_getx64, (ptr))
+#define bfd_get_signed_64(abfd, ptr) \
+                BFD_SEND(abfd, bfd_getx_signed_64, (ptr))
+
+#define bfd_get(bits, abfd, ptr)                               \
+                ( (bits) ==  8 ? (bfd_vma) bfd_get_8 (abfd, ptr)       \
+                : (bits) == 16 ? bfd_get_16 (abfd, ptr)        \
+                : (bits) == 32 ? bfd_get_32 (abfd, ptr)        \
+                : (bits) == 64 ? bfd_get_64 (abfd, ptr)        \
+                : (abort (), (bfd_vma) - 1))
+
+#define bfd_put(bits, abfd, val, ptr)                          \
+                ( (bits) ==  8 ? bfd_put_8  (abfd, val, ptr)   \
+                : (bits) == 16 ? bfd_put_16 (abfd, val, ptr)   \
+                : (bits) == 32 ? bfd_put_32 (abfd, val, ptr)   \
+                : (bits) == 64 ? bfd_put_64 (abfd, val, ptr)   \
+                : (abort (), (void) 0))
+
+
+/* Byte swapping macros for file header data.  */
+
+#define bfd_h_put_8(abfd, val, ptr) \
+  bfd_put_8 (abfd, val, ptr)
+#define bfd_h_put_signed_8(abfd, val, ptr) \
+  bfd_put_8 (abfd, val, ptr)
+#define bfd_h_get_8(abfd, ptr) \
+  bfd_get_8 (abfd, ptr)
+#define bfd_h_get_signed_8(abfd, ptr) \
+  bfd_get_signed_8 (abfd, ptr)
+
+#define bfd_h_put_16(abfd, val, ptr) \
+  BFD_SEND (abfd, bfd_h_putx16, (val, ptr))
+#define bfd_h_put_signed_16 \
+  bfd_h_put_16
+#define bfd_h_get_16(abfd, ptr) \
+  BFD_SEND (abfd, bfd_h_getx16, (ptr))
+#define bfd_h_get_signed_16(abfd, ptr) \
+  BFD_SEND (abfd, bfd_h_getx_signed_16, (ptr))
+
+#define bfd_h_put_32(abfd, val, ptr) \
+  BFD_SEND (abfd, bfd_h_putx32, (val, ptr))
+#define bfd_h_put_signed_32 \
+  bfd_h_put_32
+#define bfd_h_get_32(abfd, ptr) \
+  BFD_SEND (abfd, bfd_h_getx32, (ptr))
+#define bfd_h_get_signed_32(abfd, ptr) \
+  BFD_SEND (abfd, bfd_h_getx_signed_32, (ptr))
+
+#define bfd_h_put_64(abfd, val, ptr) \
+  BFD_SEND (abfd, bfd_h_putx64, (val, ptr))
+#define bfd_h_put_signed_64 \
+  bfd_h_put_64
+#define bfd_h_get_64(abfd, ptr) \
+  BFD_SEND (abfd, bfd_h_getx64, (ptr))
+#define bfd_h_get_signed_64(abfd, ptr) \
+  BFD_SEND (abfd, bfd_h_getx_signed_64, (ptr))
+
+/* Refinements on the above, which should eventually go away.  Save
+   cluttering the source with (bfd_vma) and (bfd_byte *) casts.  */
+
+#define H_PUT_64(abfd, val, where) \
+  bfd_h_put_64 ((abfd), (bfd_vma) (val), (bfd_byte *) (where))
+
+#define H_PUT_32(abfd, val, where) \
+  bfd_h_put_32 ((abfd), (bfd_vma) (val), (bfd_byte *) (where))
+
+#define H_PUT_16(abfd, val, where) \
+  bfd_h_put_16 ((abfd), (bfd_vma) (val), (bfd_byte *) (where))
+
+#define H_PUT_8 bfd_h_put_8
+
+#define H_PUT_S64(abfd, val, where) \
+  bfd_h_put_signed_64 ((abfd), (bfd_vma) (val), (bfd_byte *) (where))
+
+#define H_PUT_S32(abfd, val, where) \
+  bfd_h_put_signed_32 ((abfd), (bfd_vma) (val), (bfd_byte *) (where))
+
+#define H_PUT_S16(abfd, val, where) \
+  bfd_h_put_signed_16 ((abfd), (bfd_vma) (val), (bfd_byte *) (where))
+
+#define H_PUT_S8 bfd_h_put_signed_8
+
+#define H_GET_64(abfd, where) \
+  bfd_h_get_64 ((abfd), (bfd_byte *) (where))
+
+#define H_GET_32(abfd, where) \
+  bfd_h_get_32 ((abfd), (bfd_byte *) (where))
+
+#define H_GET_16(abfd, where) \
+  bfd_h_get_16 ((abfd), (bfd_byte *) (where))
+
+#define H_GET_8 bfd_h_get_8
+
+#define H_GET_S64(abfd, where) \
+  bfd_h_get_signed_64 ((abfd), (bfd_byte *) (where))
+
+#define H_GET_S32(abfd, where) \
+  bfd_h_get_signed_32 ((abfd), (bfd_byte *) (where))
+
+#define H_GET_S16(abfd, where) \
+  bfd_h_get_signed_16 ((abfd), (bfd_byte *) (where))
+
+#define H_GET_S8 bfd_h_get_signed_8
+
+
+/* This structure is used for a comdat section, as in PE.  A comdat
+   section is associated with a particular symbol.  When the linker
+   sees a comdat section, it keeps only one of the sections with a
+   given name and associated with a given symbol.  */
+
+struct bfd_comdat_info
+{
+  /* The name of the symbol associated with a comdat section.  */
+  const char *name;
+
+  /* The local symbol table index of the symbol associated with a
+     comdat section.  This is only meaningful to the object file format
+     specific code; it is not an index into the list returned by
+     bfd_canonicalize_symtab.  */
+  long symbol;
+};
+
+typedef struct sec
+{
+  /* The name of the section; the name isn't a copy, the pointer is
+     the same as that passed to bfd_make_section.  */
+
+  const char *name;
+
+  /* A unique sequence number.  */
+
+  int id;
+
+  /* Which section in the bfd; 0..n-1 as sections are created in a bfd.  */
+
+  int index;
+
+  /* The next section in the list belonging to the BFD, or NULL.  */
+
+  struct sec *next;
+
+  /* The field flags contains attributes of the section. Some
+     flags are read in from the object file, and some are
+     synthesized from other information.  */
+
+  flagword flags;
+
+#define SEC_NO_FLAGS   0x000
+
+  /* Tells the OS to allocate space for this section when loading.
+     This is clear for a section containing debug information only.  */
+#define SEC_ALLOC      0x001
+
+  /* Tells the OS to load the section from the file when loading.
+     This is clear for a .bss section.  */
+#define SEC_LOAD       0x002
+
+  /* The section contains data still to be relocated, so there is
+     some relocation information too.  */
+#define SEC_RELOC      0x004
+
+  /* ELF reserves 4 processor specific bits and 8 operating system
+     specific bits in sh_flags; at present we can get away with just
+     one in communicating between the assembler and BFD, but this
+     isn't a good long-term solution.  */
+#define SEC_ARCH_BIT_0 0x008
+
+  /* A signal to the OS that the section contains read only data.  */
+#define SEC_READONLY   0x010
+
+  /* The section contains code only.  */
+#define SEC_CODE       0x020
+
+  /* The section contains data only.  */
+#define SEC_DATA       0x040
+
+  /* The section will reside in ROM.  */
+#define SEC_ROM        0x080
+
+  /* The section contains constructor information. This section
+     type is used by the linker to create lists of constructors and
+     destructors used by <<g++>>. When a back end sees a symbol
+     which should be used in a constructor list, it creates a new
+     section for the type of name (e.g., <<__CTOR_LIST__>>), attaches
+     the symbol to it, and builds a relocation. To build the lists
+     of constructors, all the linker has to do is catenate all the
+     sections called <<__CTOR_LIST__>> and relocate the data
+     contained within - exactly the operations it would peform on
+     standard data.  */
+#define SEC_CONSTRUCTOR 0x100
+
+  /* The section is a constructor, and should be placed at the
+     end of the text, data, or bss section(?).  */
+#define SEC_CONSTRUCTOR_TEXT 0x1100
+#define SEC_CONSTRUCTOR_DATA 0x2100
+#define SEC_CONSTRUCTOR_BSS  0x3100
+
+  /* The section has contents - a data section could be
+     <<SEC_ALLOC>> | <<SEC_HAS_CONTENTS>>; a debug section could be
+     <<SEC_HAS_CONTENTS>>  */
+#define SEC_HAS_CONTENTS 0x200
+
+  /* An instruction to the linker to not output the section
+     even if it has information which would normally be written.  */
+#define SEC_NEVER_LOAD 0x400
+
+  /* The section is a COFF shared library section.  This flag is
+     only for the linker.  If this type of section appears in
+     the input file, the linker must copy it to the output file
+     without changing the vma or size.  FIXME: Although this
+     was originally intended to be general, it really is COFF
+     specific (and the flag was renamed to indicate this).  It
+     might be cleaner to have some more general mechanism to
+     allow the back end to control what the linker does with
+     sections.  */
+#define SEC_COFF_SHARED_LIBRARY 0x800
+
+  /* The section has GOT references.  This flag is only for the
+     linker, and is currently only used by the elf32-hppa back end.
+     It will be set if global offset table references were detected
+     in this section, which indicate to the linker that the section
+     contains PIC code, and must be handled specially when doing a
+     static link.  */
+#define SEC_HAS_GOT_REF 0x4000
+
+  /* The section contains common symbols (symbols may be defined
+     multiple times, the value of a symbol is the amount of
+     space it requires, and the largest symbol value is the one
+     used).  Most targets have exactly one of these (which we
+     translate to bfd_com_section_ptr), but ECOFF has two.  */
+#define SEC_IS_COMMON 0x8000
+
+  /* The section contains only debugging information.  For
+     example, this is set for ELF .debug and .stab sections.
+     strip tests this flag to see if a section can be
+     discarded.  */
+#define SEC_DEBUGGING 0x10000
+
+  /* The contents of this section are held in memory pointed to
+     by the contents field.  This is checked by bfd_get_section_contents,
+     and the data is retrieved from memory if appropriate.  */
+#define SEC_IN_MEMORY 0x20000
+
+  /* The contents of this section are to be excluded by the
+     linker for executable and shared objects unless those
+     objects are to be further relocated.  */
+#define SEC_EXCLUDE 0x40000
+
+  /* The contents of this section are to be sorted based on the sum of
+     the symbol and addend values specified by the associated relocation
+     entries.  Entries without associated relocation entries will be
+     appended to the end of the section in an unspecified order.  */
+#define SEC_SORT_ENTRIES 0x80000
+
+  /* When linking, duplicate sections of the same name should be
+     discarded, rather than being combined into a single section as
+     is usually done.  This is similar to how common symbols are
+     handled.  See SEC_LINK_DUPLICATES below.  */
+#define SEC_LINK_ONCE 0x100000
+
+  /* If SEC_LINK_ONCE is set, this bitfield describes how the linker
+     should handle duplicate sections.  */
+#define SEC_LINK_DUPLICATES 0x600000
+
+  /* This value for SEC_LINK_DUPLICATES means that duplicate
+     sections with the same name should simply be discarded.  */
+#define SEC_LINK_DUPLICATES_DISCARD 0x0
+
+  /* This value for SEC_LINK_DUPLICATES means that the linker
+     should warn if there are any duplicate sections, although
+     it should still only link one copy.  */
+#define SEC_LINK_DUPLICATES_ONE_ONLY 0x200000
+
+  /* This value for SEC_LINK_DUPLICATES means that the linker
+     should warn if any duplicate sections are a different size.  */
+#define SEC_LINK_DUPLICATES_SAME_SIZE 0x400000
+
+  /* This value for SEC_LINK_DUPLICATES means that the linker
+     should warn if any duplicate sections contain different
+     contents.  */
+#define SEC_LINK_DUPLICATES_SAME_CONTENTS 0x600000
+
+  /* This section was created by the linker as part of dynamic
+     relocation or other arcane processing.  It is skipped when
+     going through the first-pass output, trusting that someone
+     else up the line will take care of it later.  */
+#define SEC_LINKER_CREATED 0x800000
+
+  /* This section should not be subject to garbage collection.  */
+#define SEC_KEEP 0x1000000
+
+  /* This section contains "short" data, and should be placed
+     "near" the GP.  */
+#define SEC_SMALL_DATA 0x2000000
+
+  /* This section contains data which may be shared with other
+     executables or shared objects.  */
+#define SEC_SHARED 0x4000000
+
+  /* When a section with this flag is being linked, then if the size of
+     the input section is less than a page, it should not cross a page
+     boundary.  If the size of the input section is one page or more, it
+     should be aligned on a page boundary.  */
+#define SEC_BLOCK 0x8000000
+
+  /* Conditionally link this section; do not link if there are no
+     references found to any symbol in the section.  */
+#define SEC_CLINK 0x10000000
+
+  /* Attempt to merge identical entities in the section.
+     Entity size is given in the entsize field.  */
+#define SEC_MERGE 0x20000000
+
+  /* If given with SEC_MERGE, entities to merge are zero terminated
+     strings where entsize specifies character size instead of fixed
+     size entries.  */
+#define SEC_STRINGS 0x40000000
+
+  /* This section contains data about section groups.  */
+#define SEC_GROUP 0x80000000
+
+  /*  End of section flags.  */
+
+  /* Some internal packed boolean fields.  */
+
+  /* See the vma field.  */
+  unsigned int user_set_vma : 1;
+
+  /* Whether relocations have been processed.  */
+  unsigned int reloc_done : 1;
+
+  /* A mark flag used by some of the linker backends.  */
+  unsigned int linker_mark : 1;
+
+  /* Another mark flag used by some of the linker backends.  Set for
+     output sections that have an input section.  */
+  unsigned int linker_has_input : 1;
+
+  /* A mark flag used by some linker backends for garbage collection.  */
+  unsigned int gc_mark : 1;
+
+  /* Used by the ELF code to mark sections which have been allocated
+     to segments.  */
+  unsigned int segment_mark : 1;
+
+  /* End of internal packed boolean fields.  */
+
+  /*  The virtual memory address of the section - where it will be
+      at run time.  The symbols are relocated against this.  The
+      user_set_vma flag is maintained by bfd; if it's not set, the
+      backend can assign addresses (for example, in <<a.out>>, where
+      the default address for <<.data>> is dependent on the specific
+      target and various flags).  */
+
+  bfd_vma vma;
+
+  /*  The load address of the section - where it would be in a
+      rom image; really only used for writing section header
+      information. */
+
+  bfd_vma lma;
+
+  /* The size of the section in octets, as it will be output.
+     Contains a value even if the section has no contents (e.g., the
+     size of <<.bss>>).  This will be filled in after relocation.  */
+
+  bfd_size_type _cooked_size;
+
+  /* The original size on disk of the section, in octets.  Normally this
+     value is the same as the size, but if some relaxing has
+     been done, then this value will be bigger.  */
+
+  bfd_size_type _raw_size;
+
+  /* If this section is going to be output, then this value is the
+     offset in *bytes* into the output section of the first byte in the
+     input section (byte ==> smallest addressable unit on the
+     target).  In most cases, if this was going to start at the
+     100th octet (8-bit quantity) in the output section, this value
+     would be 100.  However, if the target byte size is 16 bits
+     (bfd_octets_per_byte is "2"), this value would be 50.  */
+
+  bfd_vma output_offset;
+
+  /* The output section through which to map on output.  */
+
+  struct sec *output_section;
+
+  /* The alignment requirement of the section, as an exponent of 2 -
+     e.g., 3 aligns to 2^3 (or 8).  */
+
+  unsigned int alignment_power;
+
+  /* If an input section, a pointer to a vector of relocation
+     records for the data in this section.  */
+
+  struct reloc_cache_entry *relocation;
+
+  /* If an output section, a pointer to a vector of pointers to
+     relocation records for the data in this section.  */
+
+  struct reloc_cache_entry **orelocation;
+
+  /* The number of relocation records in one of the above  */
+
+  unsigned reloc_count;
+
+  /* Information below is back end specific - and not always used
+     or updated.  */
+
+  /* File position of section data.  */
+
+  file_ptr filepos;
+
+  /* File position of relocation info.  */
+
+  file_ptr rel_filepos;
+
+  /* File position of line data.  */
+
+  file_ptr line_filepos;
+
+  /* Pointer to data for applications.  */
+
+  PTR userdata;
+
+  /* If the SEC_IN_MEMORY flag is set, this points to the actual
+     contents.  */
+  unsigned char *contents;
+
+  /* Attached line number information.  */
+
+  alent *lineno;
+
+  /* Number of line number records.  */
+
+  unsigned int lineno_count;
+
+  /* Entity size for merging purposes.  */
+
+  unsigned int entsize;
+
+  /* Optional information about a COMDAT entry; NULL if not COMDAT.  */
+
+  struct bfd_comdat_info *comdat;
+
+  /* When a section is being output, this value changes as more
+     linenumbers are written out.  */
+
+  file_ptr moving_line_filepos;
+
+  /* What the section number is in the target world.  */
+
+  int target_index;
+
+  PTR used_by_bfd;
+
+  /* If this is a constructor section then here is a list of the
+     relocations created to relocate items within it.  */
+
+  struct relent_chain *constructor_chain;
+
+  /* The BFD which owns the section.  */
+
+  bfd *owner;
+
+  /* A symbol which points at this section only */
+  struct symbol_cache_entry *symbol;
+  struct symbol_cache_entry **symbol_ptr_ptr;
+
+  struct bfd_link_order *link_order_head;
+  struct bfd_link_order *link_order_tail;
+} asection ;
+
+/* These sections are global, and are managed by BFD.  The application
+   and target back end are not permitted to change the values in
+   these sections.  New code should use the section_ptr macros rather
+   than referring directly to the const sections.  The const sections
+   may eventually vanish.  */
+#define BFD_ABS_SECTION_NAME "*ABS*"
+#define BFD_UND_SECTION_NAME "*UND*"
+#define BFD_COM_SECTION_NAME "*COM*"
+#define BFD_IND_SECTION_NAME "*IND*"
+
+/* the absolute section */
+extern const asection bfd_abs_section;
+#define bfd_abs_section_ptr ((asection *) &bfd_abs_section)
+#define bfd_is_abs_section(sec) ((sec) == bfd_abs_section_ptr)
+/* Pointer to the undefined section */
+extern const asection bfd_und_section;
+#define bfd_und_section_ptr ((asection *) &bfd_und_section)
+#define bfd_is_und_section(sec) ((sec) == bfd_und_section_ptr)
+/* Pointer to the common section */
+extern const asection bfd_com_section;
+#define bfd_com_section_ptr ((asection *) &bfd_com_section)
+/* Pointer to the indirect section */
+extern const asection bfd_ind_section;
+#define bfd_ind_section_ptr ((asection *) &bfd_ind_section)
+#define bfd_is_ind_section(sec) ((sec) == bfd_ind_section_ptr)
+
+extern const struct symbol_cache_entry * const bfd_abs_symbol;
+extern const struct symbol_cache_entry * const bfd_com_symbol;
+extern const struct symbol_cache_entry * const bfd_und_symbol;
+extern const struct symbol_cache_entry * const bfd_ind_symbol;
+#define bfd_get_section_size_before_reloc(section) \
+     ((section)->reloc_done ? (abort (), (bfd_size_type) 1) \
+                            : (section)->_raw_size)
+#define bfd_get_section_size_after_reloc(section) \
+     ((section)->reloc_done ? (section)->_cooked_size \
+                            : (abort (), (bfd_size_type) 1))
+asection *
+bfd_get_section_by_name PARAMS ((bfd *abfd, const char *name));
+
+char *
+bfd_get_unique_section_name PARAMS ((bfd *abfd,
+    const char *templat,
+    int *count));
+
+asection *
+bfd_make_section_old_way PARAMS ((bfd *abfd, const char *name));
+
+asection *
+bfd_make_section_anyway PARAMS ((bfd *abfd, const char *name));
+
+asection *
+bfd_make_section PARAMS ((bfd *, const char *name));
+
+boolean
+bfd_set_section_flags PARAMS ((bfd *abfd, asection *sec, flagword flags));
+
+void
+bfd_map_over_sections PARAMS ((bfd *abfd,
+    void (*func) (bfd *abfd,
+    asection *sect,
+    PTR obj),
+    PTR obj));
+
+boolean
+bfd_set_section_size PARAMS ((bfd *abfd, asection *sec, bfd_size_type val));
+
+boolean
+bfd_set_section_contents PARAMS ((bfd *abfd, asection *section,
+    PTR data, file_ptr offset,
+    bfd_size_type count));
+
+boolean
+bfd_get_section_contents PARAMS ((bfd *abfd, asection *section,
+    PTR location, file_ptr offset,
+    bfd_size_type count));
+
+boolean
+bfd_copy_private_section_data PARAMS ((bfd *ibfd, asection *isec,
+    bfd *obfd, asection *osec));
+
+#define bfd_copy_private_section_data(ibfd, isection, obfd, osection) \
+     BFD_SEND (obfd, _bfd_copy_private_section_data, \
+               (ibfd, isection, obfd, osection))
+void
+_bfd_strip_section_from_output PARAMS ((struct bfd_link_info *info, asection *section));
+
+enum bfd_architecture
+{
+  bfd_arch_unknown,   /* File arch not known */
+  bfd_arch_obscure,   /* Arch known, not one of these */
+  bfd_arch_m68k,      /* Motorola 68xxx */
+#define bfd_mach_m68000 1
+#define bfd_mach_m68008 2
+#define bfd_mach_m68010 3
+#define bfd_mach_m68020 4
+#define bfd_mach_m68030 5
+#define bfd_mach_m68040 6
+#define bfd_mach_m68060 7
+#define bfd_mach_cpu32  8
+#define bfd_mach_mcf5200  9
+#define bfd_mach_mcf5206e 10
+#define bfd_mach_mcf5307  11
+#define bfd_mach_mcf5407  12
+  bfd_arch_vax,       /* DEC Vax */
+  bfd_arch_i960,      /* Intel 960 */
+    /* The order of the following is important.
+       lower number indicates a machine type that
+       only accepts a subset of the instructions
+       available to machines with higher numbers.
+       The exception is the "ca", which is
+       incompatible with all other machines except
+       "core". */
+
+#define bfd_mach_i960_core      1
+#define bfd_mach_i960_ka_sa     2
+#define bfd_mach_i960_kb_sb     3
+#define bfd_mach_i960_mc        4
+#define bfd_mach_i960_xa        5
+#define bfd_mach_i960_ca        6
+#define bfd_mach_i960_jx        7
+#define bfd_mach_i960_hx        8
+
+  bfd_arch_a29k,      /* AMD 29000 */
+  bfd_arch_sparc,     /* SPARC */
+#define bfd_mach_sparc                 1
+/* The difference between v8plus and v9 is that v9 is a true 64 bit env.  */
+#define bfd_mach_sparc_sparclet        2
+#define bfd_mach_sparc_sparclite       3
+#define bfd_mach_sparc_v8plus          4
+#define bfd_mach_sparc_v8plusa         5 /* with ultrasparc add'ns */
+#define bfd_mach_sparc_sparclite_le    6
+#define bfd_mach_sparc_v9              7
+#define bfd_mach_sparc_v9a             8 /* with ultrasparc add'ns */
+#define bfd_mach_sparc_v8plusb         9 /* with cheetah add'ns */
+#define bfd_mach_sparc_v9b             10 /* with cheetah add'ns */
+/* Nonzero if MACH has the v9 instruction set.  */
+#define bfd_mach_sparc_v9_p(mach) \
+  ((mach) >= bfd_mach_sparc_v8plus && (mach) <= bfd_mach_sparc_v9b \
+   && (mach) != bfd_mach_sparc_sparclite_le)
+  bfd_arch_mips,      /* MIPS Rxxxx */
+#define bfd_mach_mips3000              3000
+#define bfd_mach_mips3900              3900
+#define bfd_mach_mips4000              4000
+#define bfd_mach_mips4010              4010
+#define bfd_mach_mips4100              4100
+#define bfd_mach_mips4111              4111
+#define bfd_mach_mips4300              4300
+#define bfd_mach_mips4400              4400
+#define bfd_mach_mips4600              4600
+#define bfd_mach_mips4650              4650
+#define bfd_mach_mips5000              5000
+#define bfd_mach_mips6000              6000
+#define bfd_mach_mips8000              8000
+#define bfd_mach_mips10000             10000
+#define bfd_mach_mips12000             12000
+#define bfd_mach_mips16                16
+#define bfd_mach_mips5                 5
+#define bfd_mach_mips_sb1              12310201 /* octal 'SB', 01 */
+#define bfd_mach_mipsisa32             32
+#define bfd_mach_mipsisa64             64
+  bfd_arch_i386,      /* Intel 386 */
+#define bfd_mach_i386_i386 0
+#define bfd_mach_i386_i8086 1
+#define bfd_mach_i386_i386_intel_syntax 2
+#define bfd_mach_x86_64 3
+#define bfd_mach_x86_64_intel_syntax 4
+  bfd_arch_we32k,     /* AT&T WE32xxx */
+  bfd_arch_tahoe,     /* CCI/Harris Tahoe */
+  bfd_arch_i860,      /* Intel 860 */
+  bfd_arch_i370,      /* IBM 360/370 Mainframes */
+  bfd_arch_romp,      /* IBM ROMP PC/RT */
+  bfd_arch_alliant,   /* Alliant */
+  bfd_arch_convex,    /* Convex */
+  bfd_arch_m88k,      /* Motorola 88xxx */
+  bfd_arch_pyramid,   /* Pyramid Technology */
+  bfd_arch_h8300,     /* Hitachi H8/300 */
+#define bfd_mach_h8300   1
+#define bfd_mach_h8300h  2
+#define bfd_mach_h8300s  3
+  bfd_arch_pdp11,     /* DEC PDP-11 */
+  bfd_arch_powerpc,   /* PowerPC */
+#define bfd_mach_ppc           0
+#define bfd_mach_ppc_403       403
+#define bfd_mach_ppc_403gc     4030
+#define bfd_mach_ppc_505       505
+#define bfd_mach_ppc_601       601
+#define bfd_mach_ppc_602       602
+#define bfd_mach_ppc_603       603
+#define bfd_mach_ppc_ec603e    6031
+#define bfd_mach_ppc_604       604
+#define bfd_mach_ppc_620       620
+#define bfd_mach_ppc_630       630
+#define bfd_mach_ppc_750       750
+#define bfd_mach_ppc_860       860
+#define bfd_mach_ppc_a35       35
+#define bfd_mach_ppc_rs64ii    642
+#define bfd_mach_ppc_rs64iii   643
+#define bfd_mach_ppc_7400      7400
+  bfd_arch_rs6000,    /* IBM RS/6000 */
+#define bfd_mach_rs6k          0
+#define bfd_mach_rs6k_rs1      6001
+#define bfd_mach_rs6k_rsc      6003
+#define bfd_mach_rs6k_rs2      6002
+  bfd_arch_hppa,      /* HP PA RISC */
+  bfd_arch_d10v,      /* Mitsubishi D10V */
+#define bfd_mach_d10v          0
+#define bfd_mach_d10v_ts2      2
+#define bfd_mach_d10v_ts3      3
+  bfd_arch_d30v,      /* Mitsubishi D30V */
+  bfd_arch_m68hc11,   /* Motorola 68HC11 */
+  bfd_arch_m68hc12,   /* Motorola 68HC12 */
+  bfd_arch_z8k,       /* Zilog Z8000 */
+#define bfd_mach_z8001         1
+#define bfd_mach_z8002         2
+  bfd_arch_h8500,     /* Hitachi H8/500 */
+  bfd_arch_sh,        /* Hitachi SH */
+#define bfd_mach_sh            0
+#define bfd_mach_sh2        0x20
+#define bfd_mach_sh_dsp     0x2d
+#define bfd_mach_sh3        0x30
+#define bfd_mach_sh3_dsp    0x3d
+#define bfd_mach_sh3e       0x3e
+#define bfd_mach_sh4        0x40
+  bfd_arch_alpha,     /* Dec Alpha */
+#define bfd_mach_alpha_ev4  0x10
+#define bfd_mach_alpha_ev5  0x20
+#define bfd_mach_alpha_ev6  0x30
+  bfd_arch_arm,       /* Advanced Risc Machines ARM */
+#define bfd_mach_arm_2         1
+#define bfd_mach_arm_2a        2
+#define bfd_mach_arm_3         3
+#define bfd_mach_arm_3M        4
+#define bfd_mach_arm_4         5
+#define bfd_mach_arm_4T        6
+#define bfd_mach_arm_5         7
+#define bfd_mach_arm_5T        8
+#define bfd_mach_arm_5TE       9
+#define bfd_mach_arm_XScale    10
+  bfd_arch_ns32k,     /* National Semiconductors ns32000 */
+  bfd_arch_w65,       /* WDC 65816 */
+  bfd_arch_tic30,     /* Texas Instruments TMS320C30 */
+  bfd_arch_tic54x,    /* Texas Instruments TMS320C54X */
+  bfd_arch_tic80,     /* TI TMS320c80 (MVP) */
+  bfd_arch_v850,      /* NEC V850 */
+#define bfd_mach_v850          0
+#define bfd_mach_v850e         'E'
+#define bfd_mach_v850ea        'A'
+  bfd_arch_arc,       /* ARC Cores */
+#define bfd_mach_arc_5         0
+#define bfd_mach_arc_6         1
+#define bfd_mach_arc_7         2
+#define bfd_mach_arc_8         3
+  bfd_arch_m32r,      /* Mitsubishi M32R/D */
+#define bfd_mach_m32r          0 /* backwards compatibility */
+#define bfd_mach_m32rx         'x'
+  bfd_arch_mn10200,   /* Matsushita MN10200 */
+  bfd_arch_mn10300,   /* Matsushita MN10300 */
+#define bfd_mach_mn10300               300
+#define bfd_mach_am33          330
+  bfd_arch_fr30,
+#define bfd_mach_fr30          0x46523330
+  bfd_arch_mcore,
+  bfd_arch_ia64,      /* HP/Intel ia64 */
+#define bfd_mach_ia64_elf64    0
+#define bfd_mach_ia64_elf32    1
+  bfd_arch_pj,
+  bfd_arch_avr,       /* Atmel AVR microcontrollers */
+#define bfd_mach_avr1          1
+#define bfd_mach_avr2          2
+#define bfd_mach_avr3          3
+#define bfd_mach_avr4          4
+#define bfd_mach_avr5          5
+  bfd_arch_cris,      /* Axis CRIS */
+  bfd_arch_s390,      /* IBM s390 */
+#define bfd_mach_s390_esa      0
+#define bfd_mach_s390_esame    1
+  bfd_arch_openrisc,  /* OpenRISC */
+  bfd_arch_last
+  };
+
+typedef struct bfd_arch_info
+{
+  int bits_per_word;
+  int bits_per_address;
+  int bits_per_byte;
+  enum bfd_architecture arch;
+  unsigned long mach;
+  const char *arch_name;
+  const char *printable_name;
+  unsigned int section_align_power;
+  /* True if this is the default machine for the architecture.  */
+  boolean the_default;
+  const struct bfd_arch_info * (*compatible)
+       PARAMS ((const struct bfd_arch_info *a,
+                const struct bfd_arch_info *b));
+
+  boolean (*scan) PARAMS ((const struct bfd_arch_info *, const char *));
+
+  const struct bfd_arch_info *next;
+} bfd_arch_info_type;
+const char *
+bfd_printable_name PARAMS ((bfd *abfd));
+
+const bfd_arch_info_type *
+bfd_scan_arch PARAMS ((const char *string));
+
+const char **
+bfd_arch_list PARAMS ((void));
+
+const bfd_arch_info_type *
+bfd_arch_get_compatible PARAMS ((
+    const bfd *abfd,
+    const bfd *bbfd));
+
+void
+bfd_set_arch_info PARAMS ((bfd *abfd, const bfd_arch_info_type *arg));
+
+enum bfd_architecture
+bfd_get_arch PARAMS ((bfd *abfd));
+
+unsigned long
+bfd_get_mach PARAMS ((bfd *abfd));
+
+unsigned int
+bfd_arch_bits_per_byte PARAMS ((bfd *abfd));
+
+unsigned int
+bfd_arch_bits_per_address PARAMS ((bfd *abfd));
+
+const bfd_arch_info_type *
+bfd_get_arch_info PARAMS ((bfd *abfd));
+
+const bfd_arch_info_type *
+bfd_lookup_arch PARAMS ((enum bfd_architecture
+    arch,
+    unsigned long machine));
+
+const char *
+bfd_printable_arch_mach PARAMS ((enum bfd_architecture arch, unsigned long machine));
+
+unsigned int
+bfd_octets_per_byte PARAMS ((bfd *abfd));
+
+unsigned int
+bfd_arch_mach_octets_per_byte PARAMS ((enum bfd_architecture arch,
+    unsigned long machine));
+
+typedef enum bfd_reloc_status
+{
+  /* No errors detected */
+  bfd_reloc_ok,
+
+  /* The relocation was performed, but there was an overflow. */
+  bfd_reloc_overflow,
+
+  /* The address to relocate was not within the section supplied. */
+  bfd_reloc_outofrange,
+
+  /* Used by special functions */
+  bfd_reloc_continue,
+
+  /* Unsupported relocation size requested. */
+  bfd_reloc_notsupported,
+
+  /* Unused */
+  bfd_reloc_other,
+
+  /* The symbol to relocate against was undefined. */
+  bfd_reloc_undefined,
+
+  /* The relocation was performed, but may not be ok - presently
+     generated only when linking i960 coff files with i960 b.out
+     symbols.  If this type is returned, the error_message argument
+     to bfd_perform_relocation will be set.  */
+  bfd_reloc_dangerous
+ }
+ bfd_reloc_status_type;
+
+
+typedef struct reloc_cache_entry
+{
+  /* A pointer into the canonical table of pointers  */
+  struct symbol_cache_entry **sym_ptr_ptr;
+
+  /* offset in section */
+  bfd_size_type address;
+
+  /* addend for relocation value */
+  bfd_vma addend;
+
+  /* Pointer to how to perform the required relocation */
+  reloc_howto_type *howto;
+
+} arelent;
+enum complain_overflow
+{
+  /* Do not complain on overflow. */
+  complain_overflow_dont,
+
+  /* Complain if the bitfield overflows, whether it is considered
+     as signed or unsigned. */
+  complain_overflow_bitfield,
+
+  /* Complain if the value overflows when considered as signed
+     number. */
+  complain_overflow_signed,
+
+  /* Complain if the value overflows when considered as an
+     unsigned number. */
+  complain_overflow_unsigned
+};
+
+struct reloc_howto_struct
+{
+  /*  The type field has mainly a documentary use - the back end can
+      do what it wants with it, though normally the back end's
+      external idea of what a reloc number is stored
+      in this field.  For example, a PC relative word relocation
+      in a coff environment has the type 023 - because that's
+      what the outside world calls a R_PCRWORD reloc.  */
+  unsigned int type;
+
+  /*  The value the final relocation is shifted right by.  This drops
+      unwanted data from the relocation.  */
+  unsigned int rightshift;
+
+  /*  The size of the item to be relocated.  This is *not* a
+      power-of-two measure.  To get the number of bytes operated
+      on by a type of relocation, use bfd_get_reloc_size.  */
+  int size;
+
+  /*  The number of bits in the item to be relocated.  This is used
+      when doing overflow checking.  */
+  unsigned int bitsize;
+
+  /*  Notes that the relocation is relative to the location in the
+      data section of the addend.  The relocation function will
+      subtract from the relocation value the address of the location
+      being relocated.  */
+  boolean pc_relative;
+
+  /*  The bit position of the reloc value in the destination.
+      The relocated value is left shifted by this amount.  */
+  unsigned int bitpos;
+
+  /* What type of overflow error should be checked for when
+     relocating.  */
+  enum complain_overflow complain_on_overflow;
+
+  /* If this field is non null, then the supplied function is
+     called rather than the normal function.  This allows really
+     strange relocation methods to be accomodated (e.g., i960 callj
+     instructions).  */
+  bfd_reloc_status_type (*special_function)
+    PARAMS ((bfd *, arelent *, struct symbol_cache_entry *, PTR, asection *,
+             bfd *, char **));
+
+  /* The textual name of the relocation type.  */
+  char *name;
+
+  /* Some formats record a relocation addend in the section contents
+     rather than with the relocation.  For ELF formats this is the
+     distinction between USE_REL and USE_RELA (though the code checks
+     for USE_REL == 1/0).  The value of this field is TRUE if the
+     addend is recorded with the section contents; when performing a
+     partial link (ld -r) the section contents (the data) will be
+     modified.  The value of this field is FALSE if addends are
+     recorded with the relocation (in arelent.addend); when performing
+     a partial link the relocation will be modified.
+     All relocations for all ELF USE_RELA targets should set this field
+     to FALSE (values of TRUE should be looked on with suspicion).
+     However, the converse is not true: not all relocations of all ELF
+     USE_REL targets set this field to TRUE.  Why this is so is peculiar
+     to each particular target.  For relocs that aren't used in partial
+     links (e.g. GOT stuff) it doesn't matter what this is set to.  */
+  boolean partial_inplace;
+
+  /* The src_mask selects which parts of the read in data
+     are to be used in the relocation sum.  E.g., if this was an 8 bit
+     byte of data which we read and relocated, this would be
+     0x000000ff.  When we have relocs which have an addend, such as
+     sun4 extended relocs, the value in the offset part of a
+     relocating field is garbage so we never use it.  In this case
+     the mask would be 0x00000000.  */
+  bfd_vma src_mask;
+
+  /* The dst_mask selects which parts of the instruction are replaced
+     into the instruction.  In most cases src_mask == dst_mask,
+     except in the above special case, where dst_mask would be
+     0x000000ff, and src_mask would be 0x00000000.  */
+  bfd_vma dst_mask;
+
+  /* When some formats create PC relative instructions, they leave
+     the value of the pc of the place being relocated in the offset
+     slot of the instruction, so that a PC relative relocation can
+     be made just by adding in an ordinary offset (e.g., sun3 a.out).
+     Some formats leave the displacement part of an instruction
+     empty (e.g., m88k bcs); this flag signals the fact.  */
+  boolean pcrel_offset;
+};
+#define HOWTO(C, R, S, B, P, BI, O, SF, NAME, INPLACE, MASKSRC, MASKDST, PC) \
+  { (unsigned) C, R, S, B, P, BI, O, SF, NAME, INPLACE, MASKSRC, MASKDST, PC }
+#define NEWHOWTO(FUNCTION, NAME, SIZE, REL, IN) \
+  HOWTO (0, 0, SIZE, 0, REL, 0, complain_overflow_dont, FUNCTION, \
+         NAME, false, 0, 0, IN)
+
+#define EMPTY_HOWTO(C) \
+  HOWTO ((C), 0, 0, 0, false, 0, complain_overflow_dont, NULL, \
+         NULL, false, 0, 0, false)
+
+#define HOWTO_PREPARE(relocation, symbol)               \
+  {                                                     \
+    if (symbol != (asymbol *) NULL)                     \
+      {                                                 \
+        if (bfd_is_com_section (symbol->section))       \
+          {                                             \
+            relocation = 0;                             \
+          }                                             \
+        else                                            \
+          {                                             \
+            relocation = symbol->value;                 \
+          }                                             \
+      }                                                 \
+  }
+unsigned int
+bfd_get_reloc_size PARAMS ((reloc_howto_type *));
+
+typedef struct relent_chain
+{
+  arelent relent;
+  struct relent_chain *next;
+} arelent_chain;
+bfd_reloc_status_type
+bfd_check_overflow PARAMS ((enum complain_overflow how,
+    unsigned int bitsize,
+    unsigned int rightshift,
+    unsigned int addrsize,
+    bfd_vma relocation));
+
+bfd_reloc_status_type
+bfd_perform_relocation PARAMS ((bfd *abfd,
+    arelent *reloc_entry,
+    PTR data,
+    asection *input_section,
+    bfd *output_bfd,
+    char **error_message));
+
+bfd_reloc_status_type
+bfd_install_relocation PARAMS ((bfd *abfd,
+    arelent *reloc_entry,
+    PTR data, bfd_vma data_start,
+    asection *input_section,
+    char **error_message));
+
+enum bfd_reloc_code_real {
+  _dummy_first_bfd_reloc_code_real,
+
+
+/* Basic absolute relocations of N bits. */
+  BFD_RELOC_64,
+  BFD_RELOC_32,
+  BFD_RELOC_26,
+  BFD_RELOC_24,
+  BFD_RELOC_16,
+  BFD_RELOC_14,
+  BFD_RELOC_8,
+
+/* PC-relative relocations.  Sometimes these are relative to the address
+of the relocation itself; sometimes they are relative to the start of
+the section containing the relocation.  It depends on the specific target.
+
+The 24-bit relocation is used in some Intel 960 configurations. */
+  BFD_RELOC_64_PCREL,
+  BFD_RELOC_32_PCREL,
+  BFD_RELOC_24_PCREL,
+  BFD_RELOC_16_PCREL,
+  BFD_RELOC_12_PCREL,
+  BFD_RELOC_8_PCREL,
+
+/* For ELF. */
+  BFD_RELOC_32_GOT_PCREL,
+  BFD_RELOC_16_GOT_PCREL,
+  BFD_RELOC_8_GOT_PCREL,
+  BFD_RELOC_32_GOTOFF,
+  BFD_RELOC_16_GOTOFF,
+  BFD_RELOC_LO16_GOTOFF,
+  BFD_RELOC_HI16_GOTOFF,
+  BFD_RELOC_HI16_S_GOTOFF,
+  BFD_RELOC_8_GOTOFF,
+  BFD_RELOC_64_PLT_PCREL,
+  BFD_RELOC_32_PLT_PCREL,
+  BFD_RELOC_24_PLT_PCREL,
+  BFD_RELOC_16_PLT_PCREL,
+  BFD_RELOC_8_PLT_PCREL,
+  BFD_RELOC_64_PLTOFF,
+  BFD_RELOC_32_PLTOFF,
+  BFD_RELOC_16_PLTOFF,
+  BFD_RELOC_LO16_PLTOFF,
+  BFD_RELOC_HI16_PLTOFF,
+  BFD_RELOC_HI16_S_PLTOFF,
+  BFD_RELOC_8_PLTOFF,
+
+/* Relocations used by 68K ELF. */
+  BFD_RELOC_68K_GLOB_DAT,
+  BFD_RELOC_68K_JMP_SLOT,
+  BFD_RELOC_68K_RELATIVE,
+
+/* Linkage-table relative. */
+  BFD_RELOC_32_BASEREL,
+  BFD_RELOC_16_BASEREL,
+  BFD_RELOC_LO16_BASEREL,
+  BFD_RELOC_HI16_BASEREL,
+  BFD_RELOC_HI16_S_BASEREL,
+  BFD_RELOC_8_BASEREL,
+  BFD_RELOC_RVA,
+
+/* Absolute 8-bit relocation, but used to form an address like 0xFFnn. */
+  BFD_RELOC_8_FFnn,
+
+/* These PC-relative relocations are stored as word displacements --
+i.e., byte displacements shifted right two bits.  The 30-bit word
+displacement (<<32_PCREL_S2>> -- 32 bits, shifted 2) is used on the
+SPARC.  (SPARC tools generally refer to this as <<WDISP30>>.)  The
+signed 16-bit displacement is used on the MIPS, and the 23-bit
+displacement is used on the Alpha. */
+  BFD_RELOC_32_PCREL_S2,
+  BFD_RELOC_16_PCREL_S2,
+  BFD_RELOC_23_PCREL_S2,
+
+/* High 22 bits and low 10 bits of 32-bit value, placed into lower bits of
+the target word.  These are used on the SPARC. */
+  BFD_RELOC_HI22,
+  BFD_RELOC_LO10,
+
+/* For systems that allocate a Global Pointer register, these are
+displacements off that register.  These relocation types are
+handled specially, because the value the register will have is
+decided relatively late. */
+  BFD_RELOC_GPREL16,
+  BFD_RELOC_GPREL32,
+
+/* Reloc types used for i960/b.out. */
+  BFD_RELOC_I960_CALLJ,
+
+/* SPARC ELF relocations.  There is probably some overlap with other
+relocation types already defined. */
+  BFD_RELOC_NONE,
+  BFD_RELOC_SPARC_WDISP22,
+  BFD_RELOC_SPARC22,
+  BFD_RELOC_SPARC13,
+  BFD_RELOC_SPARC_GOT10,
+  BFD_RELOC_SPARC_GOT13,
+  BFD_RELOC_SPARC_GOT22,
+  BFD_RELOC_SPARC_PC10,
+  BFD_RELOC_SPARC_PC22,
+  BFD_RELOC_SPARC_WPLT30,
+  BFD_RELOC_SPARC_COPY,
+  BFD_RELOC_SPARC_GLOB_DAT,
+  BFD_RELOC_SPARC_JMP_SLOT,
+  BFD_RELOC_SPARC_RELATIVE,
+  BFD_RELOC_SPARC_UA16,
+  BFD_RELOC_SPARC_UA32,
+  BFD_RELOC_SPARC_UA64,
+
+/* I think these are specific to SPARC a.out (e.g., Sun 4). */
+  BFD_RELOC_SPARC_BASE13,
+  BFD_RELOC_SPARC_BASE22,
+
+/* SPARC64 relocations */
+#define BFD_RELOC_SPARC_64 BFD_RELOC_64
+  BFD_RELOC_SPARC_10,
+  BFD_RELOC_SPARC_11,
+  BFD_RELOC_SPARC_OLO10,
+  BFD_RELOC_SPARC_HH22,
+  BFD_RELOC_SPARC_HM10,
+  BFD_RELOC_SPARC_LM22,
+  BFD_RELOC_SPARC_PC_HH22,
+  BFD_RELOC_SPARC_PC_HM10,
+  BFD_RELOC_SPARC_PC_LM22,
+  BFD_RELOC_SPARC_WDISP16,
+  BFD_RELOC_SPARC_WDISP19,
+  BFD_RELOC_SPARC_7,
+  BFD_RELOC_SPARC_6,
+  BFD_RELOC_SPARC_5,
+#define BFD_RELOC_SPARC_DISP64 BFD_RELOC_64_PCREL
+  BFD_RELOC_SPARC_PLT64,
+  BFD_RELOC_SPARC_HIX22,
+  BFD_RELOC_SPARC_LOX10,
+  BFD_RELOC_SPARC_H44,
+  BFD_RELOC_SPARC_M44,
+  BFD_RELOC_SPARC_L44,
+  BFD_RELOC_SPARC_REGISTER,
+
+/* SPARC little endian relocation */
+  BFD_RELOC_SPARC_REV32,
+
+/* Alpha ECOFF and ELF relocations.  Some of these treat the symbol or
+"addend" in some special way.
+For GPDISP_HI16 ("gpdisp") relocations, the symbol is ignored when
+writing; when reading, it will be the absolute section symbol.  The
+addend is the displacement in bytes of the "lda" instruction from
+the "ldah" instruction (which is at the address of this reloc). */
+  BFD_RELOC_ALPHA_GPDISP_HI16,
+
+/* For GPDISP_LO16 ("ignore") relocations, the symbol is handled as
+with GPDISP_HI16 relocs.  The addend is ignored when writing the
+relocations out, and is filled in with the file's GP value on
+reading, for convenience. */
+  BFD_RELOC_ALPHA_GPDISP_LO16,
+
+/* The ELF GPDISP relocation is exactly the same as the GPDISP_HI16
+relocation except that there is no accompanying GPDISP_LO16
+relocation. */
+  BFD_RELOC_ALPHA_GPDISP,
+
+/* The Alpha LITERAL/LITUSE relocs are produced by a symbol reference;
+the assembler turns it into a LDQ instruction to load the address of
+the symbol, and then fills in a register in the real instruction.
+
+The LITERAL reloc, at the LDQ instruction, refers to the .lita
+section symbol.  The addend is ignored when writing, but is filled
+in with the file's GP value on reading, for convenience, as with the
+GPDISP_LO16 reloc.
+
+The ELF_LITERAL reloc is somewhere between 16_GOTOFF and GPDISP_LO16.
+It should refer to the symbol to be referenced, as with 16_GOTOFF,
+but it generates output not based on the position within the .got
+section, but relative to the GP value chosen for the file during the
+final link stage.
+
+The LITUSE reloc, on the instruction using the loaded address, gives
+information to the linker that it might be able to use to optimize
+away some literal section references.  The symbol is ignored (read
+as the absolute section symbol), and the "addend" indicates the type
+of instruction using the register:
+1 - "memory" fmt insn
+2 - byte-manipulation (byte offset reg)
+3 - jsr (target of branch) */
+  BFD_RELOC_ALPHA_LITERAL,
+  BFD_RELOC_ALPHA_ELF_LITERAL,
+  BFD_RELOC_ALPHA_LITUSE,
+
+/* The HINT relocation indicates a value that should be filled into the
+"hint" field of a jmp/jsr/ret instruction, for possible branch-
+prediction logic which may be provided on some processors. */
+  BFD_RELOC_ALPHA_HINT,
+
+/* The LINKAGE relocation outputs a linkage pair in the object file,
+which is filled by the linker. */
+  BFD_RELOC_ALPHA_LINKAGE,
+
+/* The CODEADDR relocation outputs a STO_CA in the object file,
+which is filled by the linker. */
+  BFD_RELOC_ALPHA_CODEADDR,
+
+/* The GPREL_HI/LO relocations together form a 32-bit offset from the
+GP register. */
+  BFD_RELOC_ALPHA_GPREL_HI16,
+  BFD_RELOC_ALPHA_GPREL_LO16,
+
+/* Bits 27..2 of the relocation address shifted right 2 bits;
+simple reloc otherwise. */
+  BFD_RELOC_MIPS_JMP,
+
+/* The MIPS16 jump instruction. */
+  BFD_RELOC_MIPS16_JMP,
+
+/* MIPS16 GP relative reloc. */
+  BFD_RELOC_MIPS16_GPREL,
+
+/* High 16 bits of 32-bit value; simple reloc. */
+  BFD_RELOC_HI16,
+
+/* High 16 bits of 32-bit value but the low 16 bits will be sign
+extended and added to form the final result.  If the low 16
+bits form a negative number, we need to add one to the high value
+to compensate for the borrow when the low bits are added. */
+  BFD_RELOC_HI16_S,
+
+/* Low 16 bits. */
+  BFD_RELOC_LO16,
+
+/* Like BFD_RELOC_HI16_S, but PC relative. */
+  BFD_RELOC_PCREL_HI16_S,
+
+/* Like BFD_RELOC_LO16, but PC relative. */
+  BFD_RELOC_PCREL_LO16,
+
+/* Relocation relative to the global pointer. */
+#define BFD_RELOC_MIPS_GPREL BFD_RELOC_GPREL16
+
+/* Relocation against a MIPS literal section. */
+  BFD_RELOC_MIPS_LITERAL,
+
+/* MIPS ELF relocations. */
+  BFD_RELOC_MIPS_GOT16,
+  BFD_RELOC_MIPS_CALL16,
+#define BFD_RELOC_MIPS_GPREL32 BFD_RELOC_GPREL32
+  BFD_RELOC_MIPS_GOT_HI16,
+  BFD_RELOC_MIPS_GOT_LO16,
+  BFD_RELOC_MIPS_CALL_HI16,
+  BFD_RELOC_MIPS_CALL_LO16,
+  BFD_RELOC_MIPS_SUB,
+  BFD_RELOC_MIPS_GOT_PAGE,
+  BFD_RELOC_MIPS_GOT_OFST,
+  BFD_RELOC_MIPS_GOT_DISP,
+  BFD_RELOC_MIPS_SHIFT5,
+  BFD_RELOC_MIPS_SHIFT6,
+  BFD_RELOC_MIPS_INSERT_A,
+  BFD_RELOC_MIPS_INSERT_B,
+  BFD_RELOC_MIPS_DELETE,
+  BFD_RELOC_MIPS_HIGHEST,
+  BFD_RELOC_MIPS_HIGHER,
+  BFD_RELOC_MIPS_SCN_DISP,
+  BFD_RELOC_MIPS_REL16,
+  BFD_RELOC_MIPS_RELGOT,
+  BFD_RELOC_MIPS_JALR,
+
+
+/* i386/elf relocations */
+  BFD_RELOC_386_GOT32,
+  BFD_RELOC_386_PLT32,
+  BFD_RELOC_386_COPY,
+  BFD_RELOC_386_GLOB_DAT,
+  BFD_RELOC_386_JUMP_SLOT,
+  BFD_RELOC_386_RELATIVE,
+  BFD_RELOC_386_GOTOFF,
+  BFD_RELOC_386_GOTPC,
+
+/* x86-64/elf relocations */
+  BFD_RELOC_X86_64_GOT32,
+  BFD_RELOC_X86_64_PLT32,
+  BFD_RELOC_X86_64_COPY,
+  BFD_RELOC_X86_64_GLOB_DAT,
+  BFD_RELOC_X86_64_JUMP_SLOT,
+  BFD_RELOC_X86_64_RELATIVE,
+  BFD_RELOC_X86_64_GOTPCREL,
+  BFD_RELOC_X86_64_32S,
+
+/* ns32k relocations */
+  BFD_RELOC_NS32K_IMM_8,
+  BFD_RELOC_NS32K_IMM_16,
+  BFD_RELOC_NS32K_IMM_32,
+  BFD_RELOC_NS32K_IMM_8_PCREL,
+  BFD_RELOC_NS32K_IMM_16_PCREL,
+  BFD_RELOC_NS32K_IMM_32_PCREL,
+  BFD_RELOC_NS32K_DISP_8,
+  BFD_RELOC_NS32K_DISP_16,
+  BFD_RELOC_NS32K_DISP_32,
+  BFD_RELOC_NS32K_DISP_8_PCREL,
+  BFD_RELOC_NS32K_DISP_16_PCREL,
+  BFD_RELOC_NS32K_DISP_32_PCREL,
+
+/* PDP11 relocations */
+  BFD_RELOC_PDP11_DISP_8_PCREL,
+  BFD_RELOC_PDP11_DISP_6_PCREL,
+
+/* Picojava relocs.  Not all of these appear in object files. */
+  BFD_RELOC_PJ_CODE_HI16,
+  BFD_RELOC_PJ_CODE_LO16,
+  BFD_RELOC_PJ_CODE_DIR16,
+  BFD_RELOC_PJ_CODE_DIR32,
+  BFD_RELOC_PJ_CODE_REL16,
+  BFD_RELOC_PJ_CODE_REL32,
+
+/* Power(rs6000) and PowerPC relocations. */
+  BFD_RELOC_PPC_B26,
+  BFD_RELOC_PPC_BA26,
+  BFD_RELOC_PPC_TOC16,
+  BFD_RELOC_PPC_B16,
+  BFD_RELOC_PPC_B16_BRTAKEN,
+  BFD_RELOC_PPC_B16_BRNTAKEN,
+  BFD_RELOC_PPC_BA16,
+  BFD_RELOC_PPC_BA16_BRTAKEN,
+  BFD_RELOC_PPC_BA16_BRNTAKEN,
+  BFD_RELOC_PPC_COPY,
+  BFD_RELOC_PPC_GLOB_DAT,
+  BFD_RELOC_PPC_JMP_SLOT,
+  BFD_RELOC_PPC_RELATIVE,
+  BFD_RELOC_PPC_LOCAL24PC,
+  BFD_RELOC_PPC_EMB_NADDR32,
+  BFD_RELOC_PPC_EMB_NADDR16,
+  BFD_RELOC_PPC_EMB_NADDR16_LO,
+  BFD_RELOC_PPC_EMB_NADDR16_HI,
+  BFD_RELOC_PPC_EMB_NADDR16_HA,
+  BFD_RELOC_PPC_EMB_SDAI16,
+  BFD_RELOC_PPC_EMB_SDA2I16,
+  BFD_RELOC_PPC_EMB_SDA2REL,
+  BFD_RELOC_PPC_EMB_SDA21,
+  BFD_RELOC_PPC_EMB_MRKREF,
+  BFD_RELOC_PPC_EMB_RELSEC16,
+  BFD_RELOC_PPC_EMB_RELST_LO,
+  BFD_RELOC_PPC_EMB_RELST_HI,
+  BFD_RELOC_PPC_EMB_RELST_HA,
+  BFD_RELOC_PPC_EMB_BIT_FLD,
+  BFD_RELOC_PPC_EMB_RELSDA,
+  BFD_RELOC_PPC64_HIGHER,
+  BFD_RELOC_PPC64_HIGHER_S,
+  BFD_RELOC_PPC64_HIGHEST,
+  BFD_RELOC_PPC64_HIGHEST_S,
+  BFD_RELOC_PPC64_TOC16_LO,
+  BFD_RELOC_PPC64_TOC16_HI,
+  BFD_RELOC_PPC64_TOC16_HA,
+  BFD_RELOC_PPC64_TOC,
+  BFD_RELOC_PPC64_PLTGOT16,
+  BFD_RELOC_PPC64_PLTGOT16_LO,
+  BFD_RELOC_PPC64_PLTGOT16_HI,
+  BFD_RELOC_PPC64_PLTGOT16_HA,
+  BFD_RELOC_PPC64_ADDR16_DS,
+  BFD_RELOC_PPC64_ADDR16_LO_DS,
+  BFD_RELOC_PPC64_GOT16_DS,
+  BFD_RELOC_PPC64_GOT16_LO_DS,
+  BFD_RELOC_PPC64_PLT16_LO_DS,
+  BFD_RELOC_PPC64_SECTOFF_DS,
+  BFD_RELOC_PPC64_SECTOFF_LO_DS,
+  BFD_RELOC_PPC64_TOC16_DS,
+  BFD_RELOC_PPC64_TOC16_LO_DS,
+  BFD_RELOC_PPC64_PLTGOT16_DS,
+  BFD_RELOC_PPC64_PLTGOT16_LO_DS,
+
+/* IBM 370/390 relocations */
+  BFD_RELOC_I370_D12,
+
+/* The type of reloc used to build a contructor table - at the moment
+probably a 32 bit wide absolute relocation, but the target can choose.
+It generally does map to one of the other relocation types. */
+  BFD_RELOC_CTOR,
+
+/* ARM 26 bit pc-relative branch.  The lowest two bits must be zero and are
+not stored in the instruction. */
+  BFD_RELOC_ARM_PCREL_BRANCH,
+
+/* ARM 26 bit pc-relative branch.  The lowest bit must be zero and is
+not stored in the instruction.  The 2nd lowest bit comes from a 1 bit
+field in the instruction. */
+  BFD_RELOC_ARM_PCREL_BLX,
+
+/* Thumb 22 bit pc-relative branch.  The lowest bit must be zero and is
+not stored in the instruction.  The 2nd lowest bit comes from a 1 bit
+field in the instruction. */
+  BFD_RELOC_THUMB_PCREL_BLX,
+
+/* These relocs are only used within the ARM assembler.  They are not
+(at present) written to any object files. */
+  BFD_RELOC_ARM_IMMEDIATE,
+  BFD_RELOC_ARM_ADRL_IMMEDIATE,
+  BFD_RELOC_ARM_OFFSET_IMM,
+  BFD_RELOC_ARM_SHIFT_IMM,
+  BFD_RELOC_ARM_SWI,
+  BFD_RELOC_ARM_MULTI,
+  BFD_RELOC_ARM_CP_OFF_IMM,
+  BFD_RELOC_ARM_ADR_IMM,
+  BFD_RELOC_ARM_LDR_IMM,
+  BFD_RELOC_ARM_LITERAL,
+  BFD_RELOC_ARM_IN_POOL,
+  BFD_RELOC_ARM_OFFSET_IMM8,
+  BFD_RELOC_ARM_HWLITERAL,
+  BFD_RELOC_ARM_THUMB_ADD,
+  BFD_RELOC_ARM_THUMB_IMM,
+  BFD_RELOC_ARM_THUMB_SHIFT,
+  BFD_RELOC_ARM_THUMB_OFFSET,
+  BFD_RELOC_ARM_GOT12,
+  BFD_RELOC_ARM_GOT32,
+  BFD_RELOC_ARM_JUMP_SLOT,
+  BFD_RELOC_ARM_COPY,
+  BFD_RELOC_ARM_GLOB_DAT,
+  BFD_RELOC_ARM_PLT32,
+  BFD_RELOC_ARM_RELATIVE,
+  BFD_RELOC_ARM_GOTOFF,
+  BFD_RELOC_ARM_GOTPC,
+
+/* Hitachi SH relocs.  Not all of these appear in object files. */
+  BFD_RELOC_SH_PCDISP8BY2,
+  BFD_RELOC_SH_PCDISP12BY2,
+  BFD_RELOC_SH_IMM4,
+  BFD_RELOC_SH_IMM4BY2,
+  BFD_RELOC_SH_IMM4BY4,
+  BFD_RELOC_SH_IMM8,
+  BFD_RELOC_SH_IMM8BY2,
+  BFD_RELOC_SH_IMM8BY4,
+  BFD_RELOC_SH_PCRELIMM8BY2,
+  BFD_RELOC_SH_PCRELIMM8BY4,
+  BFD_RELOC_SH_SWITCH16,
+  BFD_RELOC_SH_SWITCH32,
+  BFD_RELOC_SH_USES,
+  BFD_RELOC_SH_COUNT,
+  BFD_RELOC_SH_ALIGN,
+  BFD_RELOC_SH_CODE,
+  BFD_RELOC_SH_DATA,
+  BFD_RELOC_SH_LABEL,
+  BFD_RELOC_SH_LOOP_START,
+  BFD_RELOC_SH_LOOP_END,
+  BFD_RELOC_SH_COPY,
+  BFD_RELOC_SH_GLOB_DAT,
+  BFD_RELOC_SH_JMP_SLOT,
+  BFD_RELOC_SH_RELATIVE,
+  BFD_RELOC_SH_GOTPC,
+
+/* Thumb 23-, 12- and 9-bit pc-relative branches.  The lowest bit must
+be zero and is not stored in the instruction. */
+  BFD_RELOC_THUMB_PCREL_BRANCH9,
+  BFD_RELOC_THUMB_PCREL_BRANCH12,
+  BFD_RELOC_THUMB_PCREL_BRANCH23,
+
+/* ARC Cores relocs.
+ARC 22 bit pc-relative branch.  The lowest two bits must be zero and are
+not stored in the instruction.  The high 20 bits are installed in bits 26
+through 7 of the instruction. */
+  BFD_RELOC_ARC_B22_PCREL,
+
+/* ARC 26 bit absolute branch.  The lowest two bits must be zero and are not
+stored in the instruction.  The high 24 bits are installed in bits 23
+through 0. */
+  BFD_RELOC_ARC_B26,
+
+/* Mitsubishi D10V relocs.
+This is a 10-bit reloc with the right 2 bits
+assumed to be 0. */
+  BFD_RELOC_D10V_10_PCREL_R,
+
+/* Mitsubishi D10V relocs.
+This is a 10-bit reloc with the right 2 bits
+assumed to be 0.  This is the same as the previous reloc
+except it is in the left container, i.e.,
+shifted left 15 bits. */
+  BFD_RELOC_D10V_10_PCREL_L,
+
+/* This is an 18-bit reloc with the right 2 bits
+assumed to be 0. */
+  BFD_RELOC_D10V_18,
+
+/* This is an 18-bit reloc with the right 2 bits
+assumed to be 0. */
+  BFD_RELOC_D10V_18_PCREL,
+
+/* Mitsubishi D30V relocs.
+This is a 6-bit absolute reloc. */
+  BFD_RELOC_D30V_6,
+
+/* This is a 6-bit pc-relative reloc with
+the right 3 bits assumed to be 0. */
+  BFD_RELOC_D30V_9_PCREL,
+
+/* This is a 6-bit pc-relative reloc with
+the right 3 bits assumed to be 0. Same
+as the previous reloc but on the right side
+of the container. */
+  BFD_RELOC_D30V_9_PCREL_R,
+
+/* This is a 12-bit absolute reloc with the
+right 3 bitsassumed to be 0. */
+  BFD_RELOC_D30V_15,
+
+/* This is a 12-bit pc-relative reloc with
+the right 3 bits assumed to be 0. */
+  BFD_RELOC_D30V_15_PCREL,
+
+/* This is a 12-bit pc-relative reloc with
+the right 3 bits assumed to be 0. Same
+as the previous reloc but on the right side
+of the container. */
+  BFD_RELOC_D30V_15_PCREL_R,
+
+/* This is an 18-bit absolute reloc with
+the right 3 bits assumed to be 0. */
+  BFD_RELOC_D30V_21,
+
+/* This is an 18-bit pc-relative reloc with
+the right 3 bits assumed to be 0. */
+  BFD_RELOC_D30V_21_PCREL,
+
+/* This is an 18-bit pc-relative reloc with
+the right 3 bits assumed to be 0. Same
+as the previous reloc but on the right side
+of the container. */
+  BFD_RELOC_D30V_21_PCREL_R,
+
+/* This is a 32-bit absolute reloc. */
+  BFD_RELOC_D30V_32,
+
+/* This is a 32-bit pc-relative reloc. */
+  BFD_RELOC_D30V_32_PCREL,
+
+/* Mitsubishi M32R relocs.
+This is a 24 bit absolute address. */
+  BFD_RELOC_M32R_24,
+
+/* This is a 10-bit pc-relative reloc with the right 2 bits assumed to be 0. */
+  BFD_RELOC_M32R_10_PCREL,
+
+/* This is an 18-bit reloc with the right 2 bits assumed to be 0. */
+  BFD_RELOC_M32R_18_PCREL,
+
+/* This is a 26-bit reloc with the right 2 bits assumed to be 0. */
+  BFD_RELOC_M32R_26_PCREL,
+
+/* This is a 16-bit reloc containing the high 16 bits of an address
+used when the lower 16 bits are treated as unsigned. */
+  BFD_RELOC_M32R_HI16_ULO,
+
+/* This is a 16-bit reloc containing the high 16 bits of an address
+used when the lower 16 bits are treated as signed. */
+  BFD_RELOC_M32R_HI16_SLO,
+
+/* This is a 16-bit reloc containing the lower 16 bits of an address. */
+  BFD_RELOC_M32R_LO16,
+
+/* This is a 16-bit reloc containing the small data area offset for use in
+add3, load, and store instructions. */
+  BFD_RELOC_M32R_SDA16,
+
+/* This is a 9-bit reloc */
+  BFD_RELOC_V850_9_PCREL,
+
+/* This is a 22-bit reloc */
+  BFD_RELOC_V850_22_PCREL,
+
+/* This is a 16 bit offset from the short data area pointer. */
+  BFD_RELOC_V850_SDA_16_16_OFFSET,
+
+/* This is a 16 bit offset (of which only 15 bits are used) from the
+short data area pointer. */
+  BFD_RELOC_V850_SDA_15_16_OFFSET,
+
+/* This is a 16 bit offset from the zero data area pointer. */
+  BFD_RELOC_V850_ZDA_16_16_OFFSET,
+
+/* This is a 16 bit offset (of which only 15 bits are used) from the
+zero data area pointer. */
+  BFD_RELOC_V850_ZDA_15_16_OFFSET,
+
+/* This is an 8 bit offset (of which only 6 bits are used) from the
+tiny data area pointer. */
+  BFD_RELOC_V850_TDA_6_8_OFFSET,
+
+/* This is an 8bit offset (of which only 7 bits are used) from the tiny
+data area pointer. */
+  BFD_RELOC_V850_TDA_7_8_OFFSET,
+
+/* This is a 7 bit offset from the tiny data area pointer. */
+  BFD_RELOC_V850_TDA_7_7_OFFSET,
+
+/* This is a 16 bit offset from the tiny data area pointer. */
+  BFD_RELOC_V850_TDA_16_16_OFFSET,
+
+/* This is a 5 bit offset (of which only 4 bits are used) from the tiny
+data area pointer. */
+  BFD_RELOC_V850_TDA_4_5_OFFSET,
+
+/* This is a 4 bit offset from the tiny data area pointer. */
+  BFD_RELOC_V850_TDA_4_4_OFFSET,
+
+/* This is a 16 bit offset from the short data area pointer, with the
+bits placed non-contigously in the instruction. */
+  BFD_RELOC_V850_SDA_16_16_SPLIT_OFFSET,
+
+/* This is a 16 bit offset from the zero data area pointer, with the
+bits placed non-contigously in the instruction. */
+  BFD_RELOC_V850_ZDA_16_16_SPLIT_OFFSET,
+
+/* This is a 6 bit offset from the call table base pointer. */
+  BFD_RELOC_V850_CALLT_6_7_OFFSET,
+
+/* This is a 16 bit offset from the call table base pointer. */
+  BFD_RELOC_V850_CALLT_16_16_OFFSET,
+
+
+/* This is a 32bit pcrel reloc for the mn10300, offset by two bytes in the
+instruction. */
+  BFD_RELOC_MN10300_32_PCREL,
+
+/* This is a 16bit pcrel reloc for the mn10300, offset by two bytes in the
+instruction. */
+  BFD_RELOC_MN10300_16_PCREL,
+
+/* This is a 8bit DP reloc for the tms320c30, where the most
+significant 8 bits of a 24 bit word are placed into the least
+significant 8 bits of the opcode. */
+  BFD_RELOC_TIC30_LDP,
+
+/* This is a 7bit reloc for the tms320c54x, where the least
+significant 7 bits of a 16 bit word are placed into the least
+significant 7 bits of the opcode. */
+  BFD_RELOC_TIC54X_PARTLS7,
+
+/* This is a 9bit DP reloc for the tms320c54x, where the most
+significant 9 bits of a 16 bit word are placed into the least
+significant 9 bits of the opcode. */
+  BFD_RELOC_TIC54X_PARTMS9,
+
+/* This is an extended address 23-bit reloc for the tms320c54x. */
+  BFD_RELOC_TIC54X_23,
+
+/* This is a 16-bit reloc for the tms320c54x, where the least
+significant 16 bits of a 23-bit extended address are placed into
+the opcode. */
+  BFD_RELOC_TIC54X_16_OF_23,
+
+/* This is a reloc for the tms320c54x, where the most
+significant 7 bits of a 23-bit extended address are placed into
+the opcode. */
+  BFD_RELOC_TIC54X_MS7_OF_23,
+
+/* This is a 48 bit reloc for the FR30 that stores 32 bits. */
+  BFD_RELOC_FR30_48,
+
+/* This is a 32 bit reloc for the FR30 that stores 20 bits split up into
+two sections. */
+  BFD_RELOC_FR30_20,
+
+/* This is a 16 bit reloc for the FR30 that stores a 6 bit word offset in
+4 bits. */
+  BFD_RELOC_FR30_6_IN_4,
+
+/* This is a 16 bit reloc for the FR30 that stores an 8 bit byte offset
+into 8 bits. */
+  BFD_RELOC_FR30_8_IN_8,
+
+/* This is a 16 bit reloc for the FR30 that stores a 9 bit short offset
+into 8 bits. */
+  BFD_RELOC_FR30_9_IN_8,
+
+/* This is a 16 bit reloc for the FR30 that stores a 10 bit word offset
+into 8 bits. */
+  BFD_RELOC_FR30_10_IN_8,
+
+/* This is a 16 bit reloc for the FR30 that stores a 9 bit pc relative
+short offset into 8 bits. */
+  BFD_RELOC_FR30_9_PCREL,
+
+/* This is a 16 bit reloc for the FR30 that stores a 12 bit pc relative
+short offset into 11 bits. */
+  BFD_RELOC_FR30_12_PCREL,
+
+/* Motorola Mcore relocations. */
+  BFD_RELOC_MCORE_PCREL_IMM8BY4,
+  BFD_RELOC_MCORE_PCREL_IMM11BY2,
+  BFD_RELOC_MCORE_PCREL_IMM4BY2,
+  BFD_RELOC_MCORE_PCREL_32,
+  BFD_RELOC_MCORE_PCREL_JSR_IMM11BY2,
+  BFD_RELOC_MCORE_RVA,
+
+/* This is a 16 bit reloc for the AVR that stores 8 bit pc relative
+short offset into 7 bits. */
+  BFD_RELOC_AVR_7_PCREL,
+
+/* This is a 16 bit reloc for the AVR that stores 13 bit pc relative
+short offset into 12 bits. */
+  BFD_RELOC_AVR_13_PCREL,
+
+/* This is a 16 bit reloc for the AVR that stores 17 bit value (usually
+program memory address) into 16 bits. */
+  BFD_RELOC_AVR_16_PM,
+
+/* This is a 16 bit reloc for the AVR that stores 8 bit value (usually
+data memory address) into 8 bit immediate value of LDI insn. */
+  BFD_RELOC_AVR_LO8_LDI,
+
+/* This is a 16 bit reloc for the AVR that stores 8 bit value (high 8 bit
+of data memory address) into 8 bit immediate value of LDI insn. */
+  BFD_RELOC_AVR_HI8_LDI,
+
+/* This is a 16 bit reloc for the AVR that stores 8 bit value (most high 8 bit
+of program memory address) into 8 bit immediate value of LDI insn. */
+  BFD_RELOC_AVR_HH8_LDI,
+
+/* This is a 16 bit reloc for the AVR that stores negated 8 bit value
+(usually data memory address) into 8 bit immediate value of SUBI insn. */
+  BFD_RELOC_AVR_LO8_LDI_NEG,
+
+/* This is a 16 bit reloc for the AVR that stores negated 8 bit value
+(high 8 bit of data memory address) into 8 bit immediate value of
+SUBI insn. */
+  BFD_RELOC_AVR_HI8_LDI_NEG,
+
+/* This is a 16 bit reloc for the AVR that stores negated 8 bit value
+(most high 8 bit of program memory address) into 8 bit immediate value
+of LDI or SUBI insn. */
+  BFD_RELOC_AVR_HH8_LDI_NEG,
+
+/* This is a 16 bit reloc for the AVR that stores 8 bit value (usually
+command address) into 8 bit immediate value of LDI insn. */
+  BFD_RELOC_AVR_LO8_LDI_PM,
+
+/* This is a 16 bit reloc for the AVR that stores 8 bit value (high 8 bit
+of command address) into 8 bit immediate value of LDI insn. */
+  BFD_RELOC_AVR_HI8_LDI_PM,
+
+/* This is a 16 bit reloc for the AVR that stores 8 bit value (most high 8 bit
+of command address) into 8 bit immediate value of LDI insn. */
+  BFD_RELOC_AVR_HH8_LDI_PM,
+
+/* This is a 16 bit reloc for the AVR that stores negated 8 bit value
+(usually command address) into 8 bit immediate value of SUBI insn. */
+  BFD_RELOC_AVR_LO8_LDI_PM_NEG,
+
+/* This is a 16 bit reloc for the AVR that stores negated 8 bit value
+(high 8 bit of 16 bit command address) into 8 bit immediate value
+of SUBI insn. */
+  BFD_RELOC_AVR_HI8_LDI_PM_NEG,
+
+/* This is a 16 bit reloc for the AVR that stores negated 8 bit value
+(high 6 bit of 22 bit command address) into 8 bit immediate
+value of SUBI insn. */
+  BFD_RELOC_AVR_HH8_LDI_PM_NEG,
+
+/* This is a 32 bit reloc for the AVR that stores 23 bit value
+into 22 bits. */
+  BFD_RELOC_AVR_CALL,
+
+/* Direct 12 bit. */
+  BFD_RELOC_390_12,
+
+/* 12 bit GOT offset. */
+  BFD_RELOC_390_GOT12,
+
+/* 32 bit PC relative PLT address. */
+  BFD_RELOC_390_PLT32,
+
+/* Copy symbol at runtime. */
+  BFD_RELOC_390_COPY,
+
+/* Create GOT entry. */
+  BFD_RELOC_390_GLOB_DAT,
+
+/* Create PLT entry. */
+  BFD_RELOC_390_JMP_SLOT,
+
+/* Adjust by program base. */
+  BFD_RELOC_390_RELATIVE,
+
+/* 32 bit PC relative offset to GOT. */
+  BFD_RELOC_390_GOTPC,
+
+/* 16 bit GOT offset. */
+  BFD_RELOC_390_GOT16,
+
+/* PC relative 16 bit shifted by 1. */
+  BFD_RELOC_390_PC16DBL,
+
+/* 16 bit PC rel. PLT shifted by 1. */
+  BFD_RELOC_390_PLT16DBL,
+
+/* PC relative 32 bit shifted by 1. */
+  BFD_RELOC_390_PC32DBL,
+
+/* 32 bit PC rel. PLT shifted by 1. */
+  BFD_RELOC_390_PLT32DBL,
+
+/* 32 bit PC rel. GOT shifted by 1. */
+  BFD_RELOC_390_GOTPCDBL,
+
+/* 64 bit GOT offset. */
+  BFD_RELOC_390_GOT64,
+
+/* 64 bit PC relative PLT address. */
+  BFD_RELOC_390_PLT64,
+
+/* 32 bit rel. offset to GOT entry. */
+  BFD_RELOC_390_GOTENT,
+
+/* These two relocations are used by the linker to determine which of
+the entries in a C++ virtual function table are actually used.  When
+the --gc-sections option is given, the linker will zero out the entries
+that are not used, so that the code for those functions need not be
+included in the output.
+
+VTABLE_INHERIT is a zero-space relocation used to describe to the
+linker the inheritence tree of a C++ virtual function table.  The
+relocation's symbol should be the parent class' vtable, and the
+relocation should be located at the child vtable.
+
+VTABLE_ENTRY is a zero-space relocation that describes the use of a
+virtual function table entry.  The reloc's symbol should refer to the
+table of the class mentioned in the code.  Off of that base, an offset
+describes the entry that is being used.  For Rela hosts, this offset
+is stored in the reloc's addend.  For Rel hosts, we are forced to put
+this offset in the reloc's section offset. */
+  BFD_RELOC_VTABLE_INHERIT,
+  BFD_RELOC_VTABLE_ENTRY,
+
+/* Intel IA64 Relocations. */
+  BFD_RELOC_IA64_IMM14,
+  BFD_RELOC_IA64_IMM22,
+  BFD_RELOC_IA64_IMM64,
+  BFD_RELOC_IA64_DIR32MSB,
+  BFD_RELOC_IA64_DIR32LSB,
+  BFD_RELOC_IA64_DIR64MSB,
+  BFD_RELOC_IA64_DIR64LSB,
+  BFD_RELOC_IA64_GPREL22,
+  BFD_RELOC_IA64_GPREL64I,
+  BFD_RELOC_IA64_GPREL32MSB,
+  BFD_RELOC_IA64_GPREL32LSB,
+  BFD_RELOC_IA64_GPREL64MSB,
+  BFD_RELOC_IA64_GPREL64LSB,
+  BFD_RELOC_IA64_LTOFF22,
+  BFD_RELOC_IA64_LTOFF64I,
+  BFD_RELOC_IA64_PLTOFF22,
+  BFD_RELOC_IA64_PLTOFF64I,
+  BFD_RELOC_IA64_PLTOFF64MSB,
+  BFD_RELOC_IA64_PLTOFF64LSB,
+  BFD_RELOC_IA64_FPTR64I,
+  BFD_RELOC_IA64_FPTR32MSB,
+  BFD_RELOC_IA64_FPTR32LSB,
+  BFD_RELOC_IA64_FPTR64MSB,
+  BFD_RELOC_IA64_FPTR64LSB,
+  BFD_RELOC_IA64_PCREL21B,
+  BFD_RELOC_IA64_PCREL21BI,
+  BFD_RELOC_IA64_PCREL21M,
+  BFD_RELOC_IA64_PCREL21F,
+  BFD_RELOC_IA64_PCREL22,
+  BFD_RELOC_IA64_PCREL60B,
+  BFD_RELOC_IA64_PCREL64I,
+  BFD_RELOC_IA64_PCREL32MSB,
+  BFD_RELOC_IA64_PCREL32LSB,
+  BFD_RELOC_IA64_PCREL64MSB,
+  BFD_RELOC_IA64_PCREL64LSB,
+  BFD_RELOC_IA64_LTOFF_FPTR22,
+  BFD_RELOC_IA64_LTOFF_FPTR64I,
+  BFD_RELOC_IA64_LTOFF_FPTR32MSB,
+  BFD_RELOC_IA64_LTOFF_FPTR32LSB,
+  BFD_RELOC_IA64_LTOFF_FPTR64MSB,
+  BFD_RELOC_IA64_LTOFF_FPTR64LSB,
+  BFD_RELOC_IA64_SEGREL32MSB,
+  BFD_RELOC_IA64_SEGREL32LSB,
+  BFD_RELOC_IA64_SEGREL64MSB,
+  BFD_RELOC_IA64_SEGREL64LSB,
+  BFD_RELOC_IA64_SECREL32MSB,
+  BFD_RELOC_IA64_SECREL32LSB,
+  BFD_RELOC_IA64_SECREL64MSB,
+  BFD_RELOC_IA64_SECREL64LSB,
+  BFD_RELOC_IA64_REL32MSB,
+  BFD_RELOC_IA64_REL32LSB,
+  BFD_RELOC_IA64_REL64MSB,
+  BFD_RELOC_IA64_REL64LSB,
+  BFD_RELOC_IA64_LTV32MSB,
+  BFD_RELOC_IA64_LTV32LSB,
+  BFD_RELOC_IA64_LTV64MSB,
+  BFD_RELOC_IA64_LTV64LSB,
+  BFD_RELOC_IA64_IPLTMSB,
+  BFD_RELOC_IA64_IPLTLSB,
+  BFD_RELOC_IA64_COPY,
+  BFD_RELOC_IA64_TPREL22,
+  BFD_RELOC_IA64_TPREL64MSB,
+  BFD_RELOC_IA64_TPREL64LSB,
+  BFD_RELOC_IA64_LTOFF_TP22,
+  BFD_RELOC_IA64_LTOFF22X,
+  BFD_RELOC_IA64_LDXMOV,
+
+/* Motorola 68HC11 reloc.
+This is the 8 bits high part of an absolute address. */
+  BFD_RELOC_M68HC11_HI8,
+
+/* Motorola 68HC11 reloc.
+This is the 8 bits low part of an absolute address. */
+  BFD_RELOC_M68HC11_LO8,
+
+/* Motorola 68HC11 reloc.
+This is the 3 bits of a value. */
+  BFD_RELOC_M68HC11_3B,
+
+/* These relocs are only used within the CRIS assembler.  They are not
+(at present) written to any object files. */
+  BFD_RELOC_CRIS_BDISP8,
+  BFD_RELOC_CRIS_UNSIGNED_5,
+  BFD_RELOC_CRIS_SIGNED_6,
+  BFD_RELOC_CRIS_UNSIGNED_6,
+  BFD_RELOC_CRIS_UNSIGNED_4,
+
+/* Relocs used in ELF shared libraries for CRIS. */
+  BFD_RELOC_CRIS_COPY,
+  BFD_RELOC_CRIS_GLOB_DAT,
+  BFD_RELOC_CRIS_JUMP_SLOT,
+  BFD_RELOC_CRIS_RELATIVE,
+
+/* 32-bit offset to symbol-entry within GOT. */
+  BFD_RELOC_CRIS_32_GOT,
+
+/* 16-bit offset to symbol-entry within GOT. */
+  BFD_RELOC_CRIS_16_GOT,
+
+/* 32-bit offset to symbol-entry within GOT, with PLT handling. */
+  BFD_RELOC_CRIS_32_GOTPLT,
+
+/* 16-bit offset to symbol-entry within GOT, with PLT handling. */
+  BFD_RELOC_CRIS_16_GOTPLT,
+
+/* 32-bit offset to symbol, relative to GOT. */
+  BFD_RELOC_CRIS_32_GOTREL,
+
+/* 32-bit offset to symbol with PLT entry, relative to GOT. */
+  BFD_RELOC_CRIS_32_PLT_GOTREL,
+
+/* 32-bit offset to symbol with PLT entry, relative to this relocation. */
+  BFD_RELOC_CRIS_32_PLT_PCREL,
+
+/* Intel i860 Relocations. */
+  BFD_RELOC_860_COPY,
+  BFD_RELOC_860_GLOB_DAT,
+  BFD_RELOC_860_JUMP_SLOT,
+  BFD_RELOC_860_RELATIVE,
+  BFD_RELOC_860_PC26,
+  BFD_RELOC_860_PLT26,
+  BFD_RELOC_860_PC16,
+  BFD_RELOC_860_LOW0,
+  BFD_RELOC_860_SPLIT0,
+  BFD_RELOC_860_LOW1,
+  BFD_RELOC_860_SPLIT1,
+  BFD_RELOC_860_LOW2,
+  BFD_RELOC_860_SPLIT2,
+  BFD_RELOC_860_LOW3,
+  BFD_RELOC_860_LOGOT0,
+  BFD_RELOC_860_SPGOT0,
+  BFD_RELOC_860_LOGOT1,
+  BFD_RELOC_860_SPGOT1,
+  BFD_RELOC_860_LOGOTOFF0,
+  BFD_RELOC_860_SPGOTOFF0,
+  BFD_RELOC_860_LOGOTOFF1,
+  BFD_RELOC_860_SPGOTOFF1,
+  BFD_RELOC_860_LOGOTOFF2,
+  BFD_RELOC_860_LOGOTOFF3,
+  BFD_RELOC_860_LOPC,
+  BFD_RELOC_860_HIGHADJ,
+  BFD_RELOC_860_HAGOT,
+  BFD_RELOC_860_HAGOTOFF,
+  BFD_RELOC_860_HAPC,
+  BFD_RELOC_860_HIGH,
+  BFD_RELOC_860_HIGOT,
+  BFD_RELOC_860_HIGOTOFF,
+
+/* OpenRISC Relocations. */
+  BFD_RELOC_OPENRISC_ABS_26,
+  BFD_RELOC_OPENRISC_REL_26,
+
+/* H8 elf Relocations. */
+  BFD_RELOC_H8_DIR16A8,
+  BFD_RELOC_H8_DIR16R8,
+  BFD_RELOC_H8_DIR24A8,
+  BFD_RELOC_H8_DIR24R8,
+  BFD_RELOC_H8_DIR32A16,
+  BFD_RELOC_UNUSED };
+typedef enum bfd_reloc_code_real bfd_reloc_code_real_type;
+reloc_howto_type *
+bfd_reloc_type_lookup PARAMS ((bfd *abfd, bfd_reloc_code_real_type code));
+
+const char *
+bfd_get_reloc_code_name PARAMS ((bfd_reloc_code_real_type code));
+
+
+typedef struct symbol_cache_entry
+{
+       /* A pointer to the BFD which owns the symbol. This information
+          is necessary so that a back end can work out what additional
+          information (invisible to the application writer) is carried
+          with the symbol.
+
+          This field is *almost* redundant, since you can use section->owner
+          instead, except that some symbols point to the global sections
+          bfd_{abs,com,und}_section.  This could be fixed by making
+          these globals be per-bfd (or per-target-flavor).  FIXME. */
+
+  struct _bfd *the_bfd; /* Use bfd_asymbol_bfd(sym) to access this field. */
+
+       /* The text of the symbol. The name is left alone, and not copied; the
+          application may not alter it. */
+  const char *name;
+
+       /* The value of the symbol.  This really should be a union of a
+          numeric value with a pointer, since some flags indicate that
+          a pointer to another symbol is stored here.  */
+  symvalue value;
+
+       /* Attributes of a symbol: */
+
+#define BSF_NO_FLAGS    0x00
+
+       /* The symbol has local scope; <<static>> in <<C>>. The value
+          is the offset into the section of the data. */
+#define BSF_LOCAL      0x01
+
+       /* The symbol has global scope; initialized data in <<C>>. The
+          value is the offset into the section of the data. */
+#define BSF_GLOBAL     0x02
+
+       /* The symbol has global scope and is exported. The value is
+          the offset into the section of the data. */
+#define BSF_EXPORT     BSF_GLOBAL /* no real difference */
+
+       /* A normal C symbol would be one of:
+          <<BSF_LOCAL>>, <<BSF_FORT_COMM>>,  <<BSF_UNDEFINED>> or
+          <<BSF_GLOBAL>> */
+
+       /* The symbol is a debugging record. The value has an arbitary
+          meaning, unless BSF_DEBUGGING_RELOC is also set.  */
+#define BSF_DEBUGGING  0x08
+
+       /* The symbol denotes a function entry point.  Used in ELF,
+          perhaps others someday.  */
+#define BSF_FUNCTION    0x10
+
+       /* Used by the linker. */
+#define BSF_KEEP        0x20
+#define BSF_KEEP_G      0x40
+
+       /* A weak global symbol, overridable without warnings by
+          a regular global symbol of the same name.  */
+#define BSF_WEAK        0x80
+
+       /* This symbol was created to point to a section, e.g. ELF's
+          STT_SECTION symbols.  */
+#define BSF_SECTION_SYM 0x100
+
+       /* The symbol used to be a common symbol, but now it is
+          allocated. */
+#define BSF_OLD_COMMON  0x200
+
+       /* The default value for common data. */
+#define BFD_FORT_COMM_DEFAULT_VALUE 0
+
+       /* In some files the type of a symbol sometimes alters its
+          location in an output file - ie in coff a <<ISFCN>> symbol
+          which is also <<C_EXT>> symbol appears where it was
+          declared and not at the end of a section.  This bit is set
+          by the target BFD part to convey this information. */
+
+#define BSF_NOT_AT_END    0x400
+
+       /* Signal that the symbol is the label of constructor section. */
+#define BSF_CONSTRUCTOR   0x800
+
+       /* Signal that the symbol is a warning symbol.  The name is a
+          warning.  The name of the next symbol is the one to warn about;
+          if a reference is made to a symbol with the same name as the next
+          symbol, a warning is issued by the linker. */
+#define BSF_WARNING       0x1000
+
+       /* Signal that the symbol is indirect.  This symbol is an indirect
+          pointer to the symbol with the same name as the next symbol. */
+#define BSF_INDIRECT      0x2000
+
+       /* BSF_FILE marks symbols that contain a file name.  This is used
+          for ELF STT_FILE symbols.  */
+#define BSF_FILE          0x4000
+
+       /* Symbol is from dynamic linking information.  */
+#define BSF_DYNAMIC       0x8000
+
+       /* The symbol denotes a data object.  Used in ELF, and perhaps
+          others someday.  */
+#define BSF_OBJECT        0x10000
+
+       /* This symbol is a debugging symbol.  The value is the offset
+          into the section of the data.  BSF_DEBUGGING should be set
+          as well.  */
+#define BSF_DEBUGGING_RELOC 0x20000
+
+  flagword flags;
+
+       /* A pointer to the section to which this symbol is
+          relative.  This will always be non NULL, there are special
+          sections for undefined and absolute symbols.  */
+  struct sec *section;
+
+       /* Back end special data.  */
+  union
+    {
+      PTR p;
+      bfd_vma i;
+    } udata;
+
+} asymbol;
+#define bfd_get_symtab_upper_bound(abfd) \
+     BFD_SEND (abfd, _bfd_get_symtab_upper_bound, (abfd))
+boolean
+bfd_is_local_label PARAMS ((bfd *abfd, asymbol *sym));
+
+boolean
+bfd_is_local_label_name PARAMS ((bfd *abfd, const char *name));
+
+#define bfd_is_local_label_name(abfd, name) \
+     BFD_SEND (abfd, _bfd_is_local_label_name, (abfd, name))
+#define bfd_canonicalize_symtab(abfd, location) \
+     BFD_SEND (abfd, _bfd_canonicalize_symtab,\
+                  (abfd, location))
+boolean
+bfd_set_symtab PARAMS ((bfd *abfd, asymbol **location, unsigned int count));
+
+void
+bfd_print_symbol_vandf PARAMS ((bfd *abfd, PTR file, asymbol *symbol));
+
+#define bfd_make_empty_symbol(abfd) \
+     BFD_SEND (abfd, _bfd_make_empty_symbol, (abfd))
+#define bfd_make_debug_symbol(abfd,ptr,size) \
+        BFD_SEND (abfd, _bfd_make_debug_symbol, (abfd, ptr, size))
+int
+bfd_decode_symclass PARAMS ((asymbol *symbol));
+
+boolean
+bfd_is_undefined_symclass PARAMS ((int symclass));
+
+void
+bfd_symbol_info PARAMS ((asymbol *symbol, symbol_info *ret));
+
+boolean
+bfd_copy_private_symbol_data PARAMS ((bfd *ibfd, asymbol *isym, bfd *obfd, asymbol *osym));
+
+#define bfd_copy_private_symbol_data(ibfd, isymbol, obfd, osymbol) \
+     BFD_SEND (obfd, _bfd_copy_private_symbol_data, \
+               (ibfd, isymbol, obfd, osymbol))
+struct _bfd
+{
+    /* The filename the application opened the BFD with.  */
+    const char *filename;
+
+    /* A pointer to the target jump table.             */
+    const struct bfd_target *xvec;
+
+    /* To avoid dragging too many header files into every file that
+       includes `<<bfd.h>>', IOSTREAM has been declared as a "char
+       *", and MTIME as a "long".  Their correct types, to which they
+       are cast when used, are "FILE *" and "time_t".    The iostream
+       is the result of an fopen on the filename.  However, if the
+       BFD_IN_MEMORY flag is set, then iostream is actually a pointer
+       to a bfd_in_memory struct.  */
+    PTR iostream;
+
+    /* Is the file descriptor being cached?  That is, can it be closed as
+       needed, and re-opened when accessed later?  */
+
+    boolean cacheable;
+
+    /* Marks whether there was a default target specified when the
+       BFD was opened. This is used to select which matching algorithm
+       to use to choose the back end. */
+
+    boolean target_defaulted;
+
+    /* The caching routines use these to maintain a
+       least-recently-used list of BFDs */
+
+    struct _bfd *lru_prev, *lru_next;
+
+    /* When a file is closed by the caching routines, BFD retains
+       state information on the file here: */
+
+    ufile_ptr where;
+
+    /* and here: (``once'' means at least once) */
+
+    boolean opened_once;
+
+    /* Set if we have a locally maintained mtime value, rather than
+       getting it from the file each time: */
+
+    boolean mtime_set;
+
+    /* File modified time, if mtime_set is true: */
+
+    long mtime;
+
+    /* Reserved for an unimplemented file locking extension.*/
+
+    int ifd;
+
+    /* The format which belongs to the BFD. (object, core, etc.) */
+
+    bfd_format format;
+
+    /* The direction the BFD was opened with*/
+
+    enum bfd_direction {no_direction = 0,
+                        read_direction = 1,
+                        write_direction = 2,
+                        both_direction = 3} direction;
+
+    /* Format_specific flags*/
+
+    flagword flags;
+
+    /* Currently my_archive is tested before adding origin to
+       anything. I believe that this can become always an add of
+       origin, with origin set to 0 for non archive files.   */
+
+    ufile_ptr origin;
+
+    /* Remember when output has begun, to stop strange things
+       from happening. */
+    boolean output_has_begun;
+
+    /* Pointer to linked list of sections*/
+    struct sec  *sections;
+
+    /* The number of sections */
+    unsigned int section_count;
+
+    /* Stuff only useful for object files:
+       The start address. */
+    bfd_vma start_address;
+
+    /* Used for input and output*/
+    unsigned int symcount;
+
+    /* Symbol table for output BFD (with symcount entries) */
+    struct symbol_cache_entry  **outsymbols;
+
+    /* Pointer to structure which contains architecture information*/
+    const struct bfd_arch_info *arch_info;
+
+    /* Stuff only useful for archives:*/
+    PTR arelt_data;
+    struct _bfd *my_archive;     /* The containing archive BFD.  */
+    struct _bfd *next;           /* The next BFD in the archive.  */
+    struct _bfd *archive_head;   /* The first BFD in the archive.  */
+    boolean has_armap;
+
+    /* A chain of BFD structures involved in a link.  */
+    struct _bfd *link_next;
+
+    /* A field used by _bfd_generic_link_add_archive_symbols.  This will
+       be used only for archive elements.  */
+    int archive_pass;
+
+    /* Used by the back end to hold private data. */
+
+    union
+      {
+      struct aout_data_struct *aout_data;
+      struct artdata *aout_ar_data;
+      struct _oasys_data *oasys_obj_data;
+      struct _oasys_ar_data *oasys_ar_data;
+      struct coff_tdata *coff_obj_data;
+      struct pe_tdata *pe_obj_data;
+      struct xcoff_tdata *xcoff_obj_data;
+      struct ecoff_tdata *ecoff_obj_data;
+      struct ieee_data_struct *ieee_data;
+      struct ieee_ar_data_struct *ieee_ar_data;
+      struct srec_data_struct *srec_data;
+      struct ihex_data_struct *ihex_data;
+      struct tekhex_data_struct *tekhex_data;
+      struct elf_obj_tdata *elf_obj_data;
+      struct nlm_obj_tdata *nlm_obj_data;
+      struct bout_data_struct *bout_data;
+      struct sun_core_struct *sun_core_data;
+      struct sco5_core_struct *sco5_core_data;
+      struct trad_core_struct *trad_core_data;
+      struct som_data_struct *som_data;
+      struct hpux_core_struct *hpux_core_data;
+      struct hppabsd_core_struct *hppabsd_core_data;
+      struct sgi_core_struct *sgi_core_data;
+      struct lynx_core_struct *lynx_core_data;
+      struct osf_core_struct *osf_core_data;
+      struct cisco_core_struct *cisco_core_data;
+      struct versados_data_struct *versados_data;
+      struct netbsd_core_struct *netbsd_core_data;
+      PTR any;
+      } tdata;
+
+    /* Used by the application to hold private data*/
+    PTR usrdata;
+
+  /* Where all the allocated stuff under this BFD goes.  This is a
+     struct objalloc *, but we use PTR to avoid requiring the inclusion of
+     objalloc.h.  */
+    PTR memory;
+};
+
+typedef enum bfd_error
+{
+  bfd_error_no_error = 0,
+  bfd_error_system_call,
+  bfd_error_invalid_target,
+  bfd_error_wrong_format,
+  bfd_error_wrong_object_format,
+  bfd_error_invalid_operation,
+  bfd_error_no_memory,
+  bfd_error_no_symbols,
+  bfd_error_no_armap,
+  bfd_error_no_more_archived_files,
+  bfd_error_malformed_archive,
+  bfd_error_file_not_recognized,
+  bfd_error_file_ambiguously_recognized,
+  bfd_error_no_contents,
+  bfd_error_nonrepresentable_section,
+  bfd_error_no_debug_section,
+  bfd_error_bad_value,
+  bfd_error_file_truncated,
+  bfd_error_file_too_big,
+  bfd_error_invalid_error_code
+} bfd_error_type;
+
+bfd_error_type
+bfd_get_error PARAMS ((void));
+
+void
+bfd_set_error PARAMS ((bfd_error_type error_tag));
+
+const char *
+bfd_errmsg PARAMS ((bfd_error_type error_tag));
+
+void
+bfd_perror PARAMS ((const char *message));
+
+typedef void (*bfd_error_handler_type) PARAMS ((const char *, ...));
+
+bfd_error_handler_type
+bfd_set_error_handler PARAMS ((bfd_error_handler_type));
+
+void
+bfd_set_error_program_name PARAMS ((const char *));
+
+bfd_error_handler_type
+bfd_get_error_handler PARAMS ((void));
+
+const char *
+bfd_archive_filename PARAMS ((bfd *));
+
+long
+bfd_get_reloc_upper_bound PARAMS ((bfd *abfd, asection *sect));
+
+long
+bfd_canonicalize_reloc PARAMS ((bfd *abfd,
+    asection *sec,
+    arelent **loc,
+    asymbol **syms));
+
+void
+bfd_set_reloc PARAMS ((bfd *abfd, asection *sec, arelent **rel, unsigned int count)
+    
+    );
+
+boolean
+bfd_set_file_flags PARAMS ((bfd *abfd, flagword flags));
+
+int
+bfd_get_arch_size PARAMS ((bfd *abfd));
+
+int
+bfd_get_sign_extend_vma PARAMS ((bfd *abfd));
+
+boolean
+bfd_set_start_address PARAMS ((bfd *abfd, bfd_vma vma));
+
+long
+bfd_get_mtime PARAMS ((bfd *abfd));
+
+long
+bfd_get_size PARAMS ((bfd *abfd));
+
+unsigned int
+bfd_get_gp_size PARAMS ((bfd *abfd));
+
+void
+bfd_set_gp_size PARAMS ((bfd *abfd, unsigned int i));
+
+bfd_vma
+bfd_scan_vma PARAMS ((const char *string, const char **end, int base));
+
+boolean
+bfd_copy_private_bfd_data PARAMS ((bfd *ibfd, bfd *obfd));
+
+#define bfd_copy_private_bfd_data(ibfd, obfd) \
+     BFD_SEND (obfd, _bfd_copy_private_bfd_data, \
+               (ibfd, obfd))
+boolean
+bfd_merge_private_bfd_data PARAMS ((bfd *ibfd, bfd *obfd));
+
+#define bfd_merge_private_bfd_data(ibfd, obfd) \
+     BFD_SEND (obfd, _bfd_merge_private_bfd_data, \
+               (ibfd, obfd))
+boolean
+bfd_set_private_flags PARAMS ((bfd *abfd, flagword flags));
+
+#define bfd_set_private_flags(abfd, flags) \
+     BFD_SEND (abfd, _bfd_set_private_flags, \
+               (abfd, flags))
+#define bfd_sizeof_headers(abfd, reloc) \
+     BFD_SEND (abfd, _bfd_sizeof_headers, (abfd, reloc))
+
+#define bfd_find_nearest_line(abfd, sec, syms, off, file, func, line) \
+     BFD_SEND (abfd, _bfd_find_nearest_line,  (abfd, sec, syms, off, file, func, line))
+
+       /* Do these three do anything useful at all, for any back end?  */
+#define bfd_debug_info_start(abfd) \
+        BFD_SEND (abfd, _bfd_debug_info_start, (abfd))
+
+#define bfd_debug_info_end(abfd) \
+        BFD_SEND (abfd, _bfd_debug_info_end, (abfd))
+
+#define bfd_debug_info_accumulate(abfd, section) \
+        BFD_SEND (abfd, _bfd_debug_info_accumulate, (abfd, section))
+
+
+#define bfd_stat_arch_elt(abfd, stat) \
+        BFD_SEND (abfd, _bfd_stat_arch_elt,(abfd, stat))
+
+#define bfd_update_armap_timestamp(abfd) \
+        BFD_SEND (abfd, _bfd_update_armap_timestamp, (abfd))
+
+#define bfd_set_arch_mach(abfd, arch, mach)\
+        BFD_SEND ( abfd, _bfd_set_arch_mach, (abfd, arch, mach))
+
+#define bfd_relax_section(abfd, section, link_info, again) \
+       BFD_SEND (abfd, _bfd_relax_section, (abfd, section, link_info, again))
+
+#define bfd_gc_sections(abfd, link_info) \
+       BFD_SEND (abfd, _bfd_gc_sections, (abfd, link_info))
+
+#define bfd_merge_sections(abfd, link_info) \
+       BFD_SEND (abfd, _bfd_merge_sections, (abfd, link_info))
+
+#define bfd_link_hash_table_create(abfd) \
+       BFD_SEND (abfd, _bfd_link_hash_table_create, (abfd))
+
+#define bfd_link_add_symbols(abfd, info) \
+       BFD_SEND (abfd, _bfd_link_add_symbols, (abfd, info))
+
+#define bfd_final_link(abfd, info) \
+       BFD_SEND (abfd, _bfd_final_link, (abfd, info))
+
+#define bfd_free_cached_info(abfd) \
+       BFD_SEND (abfd, _bfd_free_cached_info, (abfd))
+
+#define bfd_get_dynamic_symtab_upper_bound(abfd) \
+       BFD_SEND (abfd, _bfd_get_dynamic_symtab_upper_bound, (abfd))
+
+#define bfd_print_private_bfd_data(abfd, file)\
+       BFD_SEND (abfd, _bfd_print_private_bfd_data, (abfd, file))
+
+#define bfd_canonicalize_dynamic_symtab(abfd, asymbols) \
+       BFD_SEND (abfd, _bfd_canonicalize_dynamic_symtab, (abfd, asymbols))
+
+#define bfd_get_dynamic_reloc_upper_bound(abfd) \
+       BFD_SEND (abfd, _bfd_get_dynamic_reloc_upper_bound, (abfd))
+
+#define bfd_canonicalize_dynamic_reloc(abfd, arels, asyms) \
+       BFD_SEND (abfd, _bfd_canonicalize_dynamic_reloc, (abfd, arels, asyms))
+
+extern bfd_byte *bfd_get_relocated_section_contents
+       PARAMS ((bfd *, struct bfd_link_info *,
+                 struct bfd_link_order *, bfd_byte *,
+                 boolean, asymbol **));
+
+boolean
+bfd_alt_mach_code PARAMS ((bfd *abfd, int index));
+
+symindex
+bfd_get_next_mapent PARAMS ((bfd *abfd, symindex previous, carsym **sym));
+
+boolean
+bfd_set_archive_head PARAMS ((bfd *output, bfd *new_head));
+
+bfd *
+bfd_openr_next_archived_file PARAMS ((bfd *archive, bfd *previous));
+
+const char *
+bfd_core_file_failing_command PARAMS ((bfd *abfd));
+
+int
+bfd_core_file_failing_signal PARAMS ((bfd *abfd));
+
+boolean
+core_file_matches_executable_p PARAMS ((bfd *core_bfd, bfd *exec_bfd));
+
+#define BFD_SEND(bfd, message, arglist) \
+               ((*((bfd)->xvec->message)) arglist)
+
+#ifdef DEBUG_BFD_SEND
+#undef BFD_SEND
+#define BFD_SEND(bfd, message, arglist) \
+  (((bfd) && (bfd)->xvec && (bfd)->xvec->message) ? \
+    ((*((bfd)->xvec->message)) arglist) : \
+    (bfd_assert (__FILE__,__LINE__), NULL))
+#endif
+#define BFD_SEND_FMT(bfd, message, arglist) \
+            (((bfd)->xvec->message[(int) ((bfd)->format)]) arglist)
+
+#ifdef DEBUG_BFD_SEND
+#undef BFD_SEND_FMT
+#define BFD_SEND_FMT(bfd, message, arglist) \
+  (((bfd) && (bfd)->xvec && (bfd)->xvec->message) ? \
+   (((bfd)->xvec->message[(int) ((bfd)->format)]) arglist) : \
+   (bfd_assert (__FILE__,__LINE__), NULL))
+#endif
+enum bfd_flavour {
+  bfd_target_unknown_flavour,
+  bfd_target_aout_flavour,
+  bfd_target_coff_flavour,
+  bfd_target_ecoff_flavour,
+  bfd_target_xcoff_flavour,
+  bfd_target_elf_flavour,
+  bfd_target_ieee_flavour,
+  bfd_target_nlm_flavour,
+  bfd_target_oasys_flavour,
+  bfd_target_tekhex_flavour,
+  bfd_target_srec_flavour,
+  bfd_target_ihex_flavour,
+  bfd_target_som_flavour,
+  bfd_target_os9k_flavour,
+  bfd_target_versados_flavour,
+  bfd_target_msdos_flavour,
+  bfd_target_ovax_flavour,
+  bfd_target_evax_flavour
+};
+
+enum bfd_endian { BFD_ENDIAN_BIG, BFD_ENDIAN_LITTLE, BFD_ENDIAN_UNKNOWN };
+
+/* Forward declaration.  */
+typedef struct bfd_link_info _bfd_link_info;
+
+typedef struct bfd_target
+{
+  char *name;
+  enum bfd_flavour flavour;
+  enum bfd_endian byteorder;
+  enum bfd_endian header_byteorder;
+  flagword object_flags;
+  flagword section_flags;
+  char symbol_leading_char;
+  char ar_pad_char;
+  unsigned short ar_max_namelen;
+  bfd_vma        (*bfd_getx64) PARAMS ((const bfd_byte *));
+  bfd_signed_vma (*bfd_getx_signed_64) PARAMS ((const bfd_byte *));
+  void           (*bfd_putx64) PARAMS ((bfd_vma, bfd_byte *));
+  bfd_vma        (*bfd_getx32) PARAMS ((const bfd_byte *));
+  bfd_signed_vma (*bfd_getx_signed_32) PARAMS ((const bfd_byte *));
+  void           (*bfd_putx32) PARAMS ((bfd_vma, bfd_byte *));
+  bfd_vma        (*bfd_getx16) PARAMS ((const bfd_byte *));
+  bfd_signed_vma (*bfd_getx_signed_16) PARAMS ((const bfd_byte *));
+  void           (*bfd_putx16) PARAMS ((bfd_vma, bfd_byte *));
+  bfd_vma        (*bfd_h_getx64) PARAMS ((const bfd_byte *));
+  bfd_signed_vma (*bfd_h_getx_signed_64) PARAMS ((const bfd_byte *));
+  void           (*bfd_h_putx64) PARAMS ((bfd_vma, bfd_byte *));
+  bfd_vma        (*bfd_h_getx32) PARAMS ((const bfd_byte *));
+  bfd_signed_vma (*bfd_h_getx_signed_32) PARAMS ((const bfd_byte *));
+  void           (*bfd_h_putx32) PARAMS ((bfd_vma, bfd_byte *));
+  bfd_vma        (*bfd_h_getx16) PARAMS ((const bfd_byte *));
+  bfd_signed_vma (*bfd_h_getx_signed_16) PARAMS ((const bfd_byte *));
+  void           (*bfd_h_putx16) PARAMS ((bfd_vma, bfd_byte *));
+  const struct bfd_target *(*_bfd_check_format[bfd_type_end]) PARAMS ((bfd *));
+  boolean  (*_bfd_set_format[bfd_type_end]) PARAMS ((bfd *));
+  boolean  (*_bfd_write_contents[bfd_type_end]) PARAMS ((bfd *));
+
+  /* Generic entry points.  */
+#define BFD_JUMP_TABLE_GENERIC(NAME) \
+CONCAT2 (NAME,_close_and_cleanup), \
+CONCAT2 (NAME,_bfd_free_cached_info), \
+CONCAT2 (NAME,_new_section_hook), \
+CONCAT2 (NAME,_get_section_contents), \
+CONCAT2 (NAME,_get_section_contents_in_window)
+
+  /* Called when the BFD is being closed to do any necessary cleanup.  */
+  boolean  (*_close_and_cleanup) PARAMS ((bfd *));
+  /* Ask the BFD to free all cached information.  */
+  boolean  (*_bfd_free_cached_info) PARAMS ((bfd *));
+  /* Called when a new section is created.  */
+  boolean  (*_new_section_hook) PARAMS ((bfd *, sec_ptr));
+  /* Read the contents of a section.  */
+  boolean  (*_bfd_get_section_contents) PARAMS ((bfd *, sec_ptr, PTR,
+                                                 file_ptr, bfd_size_type));
+  boolean  (*_bfd_get_section_contents_in_window)
+    PARAMS ((bfd *, sec_ptr, bfd_window *, file_ptr, bfd_size_type));
+
+  /* Entry points to copy private data.  */
+#define BFD_JUMP_TABLE_COPY(NAME) \
+CONCAT2 (NAME,_bfd_copy_private_bfd_data), \
+CONCAT2 (NAME,_bfd_merge_private_bfd_data), \
+CONCAT2 (NAME,_bfd_copy_private_section_data), \
+CONCAT2 (NAME,_bfd_copy_private_symbol_data), \
+CONCAT2 (NAME,_bfd_set_private_flags), \
+CONCAT2 (NAME,_bfd_print_private_bfd_data) \
+  /* Called to copy BFD general private data from one object file
+     to another.  */
+  boolean  (*_bfd_copy_private_bfd_data) PARAMS ((bfd *, bfd *));
+  /* Called to merge BFD general private data from one object file
+     to a common output file when linking.  */
+  boolean  (*_bfd_merge_private_bfd_data) PARAMS ((bfd *, bfd *));
+  /* Called to copy BFD private section data from one object file
+     to another.  */
+  boolean  (*_bfd_copy_private_section_data) PARAMS ((bfd *, sec_ptr,
+                                                      bfd *, sec_ptr));
+  /* Called to copy BFD private symbol data from one symbol
+     to another.  */
+  boolean  (*_bfd_copy_private_symbol_data) PARAMS ((bfd *, asymbol *,
+                                                     bfd *, asymbol *));
+  /* Called to set private backend flags */
+  boolean  (*_bfd_set_private_flags) PARAMS ((bfd *, flagword));
+
+  /* Called to print private BFD data */
+  boolean  (*_bfd_print_private_bfd_data) PARAMS ((bfd *, PTR));
+
+  /* Core file entry points.  */
+#define BFD_JUMP_TABLE_CORE(NAME) \
+CONCAT2 (NAME,_core_file_failing_command), \
+CONCAT2 (NAME,_core_file_failing_signal), \
+CONCAT2 (NAME,_core_file_matches_executable_p)
+  char *   (*_core_file_failing_command) PARAMS ((bfd *));
+  int      (*_core_file_failing_signal) PARAMS ((bfd *));
+  boolean  (*_core_file_matches_executable_p) PARAMS ((bfd *, bfd *));
+
+  /* Archive entry points.  */
+#define BFD_JUMP_TABLE_ARCHIVE(NAME) \
+CONCAT2 (NAME,_slurp_armap), \
+CONCAT2 (NAME,_slurp_extended_name_table), \
+CONCAT2 (NAME,_construct_extended_name_table), \
+CONCAT2 (NAME,_truncate_arname), \
+CONCAT2 (NAME,_write_armap), \
+CONCAT2 (NAME,_read_ar_hdr), \
+CONCAT2 (NAME,_openr_next_archived_file), \
+CONCAT2 (NAME,_get_elt_at_index), \
+CONCAT2 (NAME,_generic_stat_arch_elt), \
+CONCAT2 (NAME,_update_armap_timestamp)
+  boolean  (*_bfd_slurp_armap) PARAMS ((bfd *));
+  boolean  (*_bfd_slurp_extended_name_table) PARAMS ((bfd *));
+  boolean  (*_bfd_construct_extended_name_table)
+    PARAMS ((bfd *, char **, bfd_size_type *, const char **));
+  void     (*_bfd_truncate_arname) PARAMS ((bfd *, const char *, char *));
+  boolean  (*write_armap)
+    PARAMS ((bfd *, unsigned int, struct orl *, unsigned int, int));
+  PTR      (*_bfd_read_ar_hdr_fn) PARAMS ((bfd *));
+  bfd *    (*openr_next_archived_file) PARAMS ((bfd *, bfd *));
+#define bfd_get_elt_at_index(b,i) BFD_SEND(b, _bfd_get_elt_at_index, (b,i))
+  bfd *    (*_bfd_get_elt_at_index) PARAMS ((bfd *, symindex));
+  int      (*_bfd_stat_arch_elt) PARAMS ((bfd *, struct stat *));
+  boolean  (*_bfd_update_armap_timestamp) PARAMS ((bfd *));
+
+  /* Entry points used for symbols.  */
+#define BFD_JUMP_TABLE_SYMBOLS(NAME) \
+CONCAT2 (NAME,_get_symtab_upper_bound), \
+CONCAT2 (NAME,_get_symtab), \
+CONCAT2 (NAME,_make_empty_symbol), \
+CONCAT2 (NAME,_print_symbol), \
+CONCAT2 (NAME,_get_symbol_info), \
+CONCAT2 (NAME,_bfd_is_local_label_name), \
+CONCAT2 (NAME,_get_lineno), \
+CONCAT2 (NAME,_find_nearest_line), \
+CONCAT2 (NAME,_bfd_make_debug_symbol), \
+CONCAT2 (NAME,_read_minisymbols), \
+CONCAT2 (NAME,_minisymbol_to_symbol)
+  long     (*_bfd_get_symtab_upper_bound) PARAMS ((bfd *));
+  long     (*_bfd_canonicalize_symtab) PARAMS ((bfd *,
+                                                struct symbol_cache_entry **));
+  struct symbol_cache_entry *
+           (*_bfd_make_empty_symbol) PARAMS ((bfd *));
+  void     (*_bfd_print_symbol) PARAMS ((bfd *, PTR,
+                                         struct symbol_cache_entry *,
+                                         bfd_print_symbol_type));
+#define bfd_print_symbol(b,p,s,e) BFD_SEND(b, _bfd_print_symbol, (b,p,s,e))
+  void     (*_bfd_get_symbol_info) PARAMS ((bfd *,
+                                            struct symbol_cache_entry *,
+                                            symbol_info *));
+#define bfd_get_symbol_info(b,p,e) BFD_SEND(b, _bfd_get_symbol_info, (b,p,e))
+  boolean  (*_bfd_is_local_label_name) PARAMS ((bfd *, const char *));
+
+  alent *  (*_get_lineno) PARAMS ((bfd *, struct symbol_cache_entry *));
+  boolean  (*_bfd_find_nearest_line)
+    PARAMS ((bfd *, struct sec *, struct symbol_cache_entry **, bfd_vma,
+             const char **, const char **, unsigned int *));
+ /* Back-door to allow format-aware applications to create debug symbols
+    while using BFD for everything else.  Currently used by the assembler
+    when creating COFF files.  */
+  asymbol *(*_bfd_make_debug_symbol) PARAMS ((bfd *, void *,
+                                              unsigned long size));
+#define bfd_read_minisymbols(b, d, m, s) \
+  BFD_SEND (b, _read_minisymbols, (b, d, m, s))
+  long     (*_read_minisymbols) PARAMS ((bfd *, boolean, PTR *,
+                                         unsigned int *));
+#define bfd_minisymbol_to_symbol(b, d, m, f) \
+  BFD_SEND (b, _minisymbol_to_symbol, (b, d, m, f))
+  asymbol *(*_minisymbol_to_symbol) PARAMS ((bfd *, boolean, const PTR,
+                                             asymbol *));
+
+  /* Routines for relocs.  */
+#define BFD_JUMP_TABLE_RELOCS(NAME) \
+CONCAT2 (NAME,_get_reloc_upper_bound), \
+CONCAT2 (NAME,_canonicalize_reloc), \
+CONCAT2 (NAME,_bfd_reloc_type_lookup)
+  long     (*_get_reloc_upper_bound) PARAMS ((bfd *, sec_ptr));
+  long     (*_bfd_canonicalize_reloc) PARAMS ((bfd *, sec_ptr, arelent **,
+                                               struct symbol_cache_entry **));
+  /* See documentation on reloc types.  */
+  reloc_howto_type *
+           (*reloc_type_lookup) PARAMS ((bfd *, bfd_reloc_code_real_type));
+
+  /* Routines used when writing an object file.  */
+#define BFD_JUMP_TABLE_WRITE(NAME) \
+CONCAT2 (NAME,_set_arch_mach), \
+CONCAT2 (NAME,_set_section_contents)
+  boolean  (*_bfd_set_arch_mach) PARAMS ((bfd *, enum bfd_architecture,
+                                          unsigned long));
+  boolean  (*_bfd_set_section_contents) PARAMS ((bfd *, sec_ptr, PTR,
+                                                 file_ptr, bfd_size_type));
+
+  /* Routines used by the linker.  */
+#define BFD_JUMP_TABLE_LINK(NAME) \
+CONCAT2 (NAME,_sizeof_headers), \
+CONCAT2 (NAME,_bfd_get_relocated_section_contents), \
+CONCAT2 (NAME,_bfd_relax_section), \
+CONCAT2 (NAME,_bfd_link_hash_table_create), \
+CONCAT2 (NAME,_bfd_link_add_symbols), \
+CONCAT2 (NAME,_bfd_final_link), \
+CONCAT2 (NAME,_bfd_link_split_section), \
+CONCAT2 (NAME,_bfd_gc_sections), \
+CONCAT2 (NAME,_bfd_merge_sections)
+  int      (*_bfd_sizeof_headers) PARAMS ((bfd *, boolean));
+  bfd_byte *(*_bfd_get_relocated_section_contents)
+    PARAMS ((bfd *, struct bfd_link_info *, struct bfd_link_order *,
+             bfd_byte *, boolean, struct symbol_cache_entry **));
+
+  boolean  (*_bfd_relax_section)
+    PARAMS ((bfd *, struct sec *, struct bfd_link_info *, boolean *));
+
+  /* Create a hash table for the linker.  Different backends store
+     different information in this table.  */
+  struct bfd_link_hash_table *(*_bfd_link_hash_table_create) PARAMS ((bfd *));
+
+  /* Add symbols from this object file into the hash table.  */
+  boolean  (*_bfd_link_add_symbols) PARAMS ((bfd *, struct bfd_link_info *));
+
+  /* Do a link based on the link_order structures attached to each
+     section of the BFD.  */
+  boolean  (*_bfd_final_link) PARAMS ((bfd *, struct bfd_link_info *));
+
+  /* Should this section be split up into smaller pieces during linking.  */
+  boolean  (*_bfd_link_split_section) PARAMS ((bfd *, struct sec *));
+
+  /* Remove sections that are not referenced from the output.  */
+  boolean  (*_bfd_gc_sections) PARAMS ((bfd *, struct bfd_link_info *));
+
+  /* Attempt to merge SEC_MERGE sections.  */
+  boolean  (*_bfd_merge_sections) PARAMS ((bfd *, struct bfd_link_info *));
+
+  /* Routines to handle dynamic symbols and relocs.  */
+#define BFD_JUMP_TABLE_DYNAMIC(NAME) \
+CONCAT2 (NAME,_get_dynamic_symtab_upper_bound), \
+CONCAT2 (NAME,_canonicalize_dynamic_symtab), \
+CONCAT2 (NAME,_get_dynamic_reloc_upper_bound), \
+CONCAT2 (NAME,_canonicalize_dynamic_reloc)
+  /* Get the amount of memory required to hold the dynamic symbols. */
+  long     (*_bfd_get_dynamic_symtab_upper_bound) PARAMS ((bfd *));
+  /* Read in the dynamic symbols.  */
+  long     (*_bfd_canonicalize_dynamic_symtab)
+    PARAMS ((bfd *, struct symbol_cache_entry **));
+  /* Get the amount of memory required to hold the dynamic relocs.  */
+  long     (*_bfd_get_dynamic_reloc_upper_bound) PARAMS ((bfd *));
+  /* Read in the dynamic relocs.  */
+  long     (*_bfd_canonicalize_dynamic_reloc)
+    PARAMS ((bfd *, arelent **, struct symbol_cache_entry **));
+
+ /* Opposite endian version of this target.  */
+ const struct bfd_target * alternative_target;
+
+ PTR backend_data;
+
+} bfd_target;
+boolean
+bfd_set_default_target PARAMS ((const char *name));
+
+const bfd_target *
+bfd_find_target PARAMS ((const char *target_name, bfd *abfd));
+
+const char **
+bfd_target_list PARAMS ((void));
+
+const bfd_target *
+bfd_search_for_target PARAMS ((int (* search_func) (const bfd_target *, void *), void *));
+
+boolean
+bfd_check_format PARAMS ((bfd *abfd, bfd_format format));
+
+boolean
+bfd_check_format_matches PARAMS ((bfd *abfd, bfd_format format, char ***matching));
+
+boolean
+bfd_set_format PARAMS ((bfd *abfd, bfd_format format));
+
+const char *
+bfd_format_string PARAMS ((bfd_format format));
+
+#ifdef __cplusplus
+}
+#endif
+#endif
diff -purN linux-2.5/arch/ppc64/kdb/kdba_bp.c linuxppc64-2.5/arch/ppc64/kdb/kdba_bp.c
--- linux-2.5/arch/ppc64/kdb/kdba_bp.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/kdba_bp.c	2003-10-13 16:16:57.000000000 +0000
@@ -0,0 +1,789 @@
+/*
+ * Kernel Debugger Architecture Dependent Breakpoint Handling
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *	Scott Lurndal			1999/12/12
+ *		v1.0 restructuring.
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ */
+
+#include <linux/string.h>
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/smp.h>
+#include <linux/ptrace.h>
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+#include "privinst.h"
+
+static char *kdba_rwtypes[] = { "Instruction(Register)", "Data Write",
+			"I/O", "Data Access"};
+
+extern void set_all_DABR(unsigned long val);
+
+/*
+ * Table describing processor architecture hardware
+ * breakpoint registers.
+ */
+
+kdbhard_bp_t	kdb_hardbreaks[KDB_MAXHARDBPT];
+
+/*
+ * kdba_db_trap
+ *
+ * 	Perform breakpoint processing upon entry to the
+ *	processor debugger fault.   Determine and print
+ *	the active breakpoint.
+ *
+ * Parameters:
+ *	ef	Exception frame containing machine register state
+ *	error	Error number passed to kdb.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	KDB_DB_BPT	Standard instruction or data breakpoint encountered
+ *	KDB_DB_SS	Single Step fault ('ss' command or end of 'ssb' command)
+ *	KDB_DB_SSB	Single Step fault, caller should continue ('ssb' command)
+ *	KDB_DB_SSBPT	Single step over breakpoint
+ *	KDB_DB_NOBPT	No existing kdb breakpoint matches this debug exception
+ * Locking:
+ *	None.
+ * Remarks:
+ *	Yup, there be goto's here.
+ *
+ *	If multiple processors receive debug exceptions simultaneously,
+ *	one may be waiting at the kdb fence in kdb() while the user
+ *	issues a 'bc' command to clear the breakpoint the processor
+ *	which is waiting has already encountered.  If this is the case,
+ *	the debug registers will no longer match any entry in the
+ *	breakpoint table, and we'll return the value KDB_DB_NOBPT.
+ *	This can cause a panic in die_if_kernel().  It is safer to
+ *	disable the breakpoint (bd), go until all processors are past
+ *	the breakpoint then clear the breakpoint (bc).  This code
+ *	recognises a breakpoint even when disabled but not when it has
+ *	been cleared.
+ *
+ *	WARNING: This routine clears the debug state.  It should be called
+ *		 once per debug and the result cached.
+ */
+
+kdb_dbtrap_t
+kdba_db_trap(kdb_eframe_t ef, int error_unused)
+{
+	kdb_machreg_t  msr,trap;
+	int rw, reg;
+	int i;
+	kdb_dbtrap_t rv = KDB_DB_BPT;
+	kdb_bp_t *bp;
+	unsigned long primary;
+	unsigned long extended;
+
+	msr = get_msr();
+	trap = ef->trap;
+	if (KDB_DEBUG(BP))
+		kdb_printf("kdb: msr 0x%lx trap 0x%lx\n", msr,trap);
+	if (msr & MSR_SE || ((trap & 0x700) || (trap & 0xd00))) 
+	{
+		if (KDB_STATE(SSBPT)) {
+			if (KDB_DEBUG(BP))
+				kdb_printf("ssbpt\n");
+			KDB_STATE_CLEAR(SSBPT);
+			for(i=0,bp=kdb_breakpoints;
+			    i < KDB_MAXBPT;
+			    i++, bp++) {
+				if (KDB_DEBUG(BP))
+					kdb_printf("bp 0x%p enabled %d delayed %d global %d cpu %d\n",
+						   bp, bp->bp_enabled, bp->bp_delayed, bp->bp_global, bp->bp_cpu);
+				if (!bp->bp_enabled)
+					continue;
+				if (!bp->bp_global && bp->bp_cpu != smp_processor_id())
+					continue;
+				if (KDB_DEBUG(BP))
+					kdb_printf("bp for this cpu\n");
+				if (bp->bp_delayed) {
+					bp->bp_delayed = 0;
+					if (KDB_DEBUG(BP))
+						kdb_printf("kdba_installbp\n");
+					kdba_installbp(ef, bp);
+					if (!KDB_STATE(DOING_SS)) {
+						set_msr(get_msr() & ~MSR_SE);
+						return(KDB_DB_SSBPT);
+					}
+					break;
+				}
+			}
+			if (i == KDB_MAXBPT) {
+				kdb_printf("kdb: Unable to find delayed breakpoint\n");
+			}
+			if (!KDB_STATE(DOING_SS)) {
+				set_msr(get_msr() & ~MSR_SE);
+				return(KDB_DB_NOBPT);
+			}
+			/* FALLTHROUGH */
+		}
+
+		/*
+		 * KDB_STATE_DOING_SS is set when the kernel debugger is using
+		 * the processor trap flag to single-step a processor.  If a
+		 * single step trap occurs and this flag is clear, the SS trap
+		 * will be ignored by KDB and the kernel will be allowed to deal
+		 * with it as necessary (e.g. for ptrace).
+		 */
+		if (!KDB_STATE(DOING_SS))
+			goto unknown;
+
+		/* single step */
+		rv = KDB_DB_SS;		/* Indicate single step */
+		if (KDB_STATE(DOING_SSB)) {
+		    unsigned long instruction;
+
+			kdb_id1(ef->nip);
+			kdb_getarea(instruction,ef->nip);
+			primary=instruction & 0xfc000000;
+			extended=instruction & 0x000007fe;
+			if (kdb_getarea(instruction, ef->nip) ||   /* read failure */
+/* branch conditional */
+			    (primary==16 )||
+/* branch */
+			    (primary==18 )||    
+/* branch conditional to LR, or branch conditional to CR */
+			    (primary==19 && (extended==16 || extended == 528) 
+			     )) {
+				/*
+				 * End the ssb command here.
+				 */
+			    KDB_STATE_CLEAR(DOING_SSB);
+			    KDB_STATE_CLEAR(DOING_SS);
+			    }
+			rv = KDB_DB_SSB; /* Indicate ssb - dismiss immediately */
+		} else {
+			/*
+			 * Print current insn
+			 */
+			kdb_printf("SS trap at ");
+			kdb_symbol_print(ef->nip, NULL, KDB_SP_DEFAULT|KDB_SP_NEWLINE);
+			kdb_printf(" "); /* wms */
+			kdb_id1(ef->nip);
+			KDB_STATE_CLEAR(DOING_SS);
+		}
+
+		if (rv != KDB_DB_SSB)
+			set_msr(get_msr() & ~MSR_SE);
+	}
+	if (rv > 0)
+		goto handled;
+	
+	goto handle;
+
+
+handle:
+
+	/*
+	 * Determine which breakpoint was encountered.
+	 */
+	for(i=0, bp=kdb_breakpoints; i<KDB_MAXBPT; i++, bp++) {
+		if (!(bp->bp_free)
+		 && (bp->bp_global || bp->bp_cpu == smp_processor_id())
+		 && (bp->bp_hard)
+		 && (bp->bp_hard->bph_reg == reg)) {
+			/*
+			 * Hit this breakpoint.
+			 */
+			kdb_printf("%s breakpoint #%d at " kdb_bfd_vma_fmt "\n", 
+				  kdba_rwtypes[rw],
+				  i, (long )bp->bp_addr);
+
+			/*
+			 * For an instruction breakpoint, disassemble
+			 * the current instruction.
+			 */
+			if (rw == 0) {
+				kdb_id1(ef->nip);
+			}
+
+			goto handled;
+		}
+	}
+
+unknown:
+	rv = KDB_DB_NOBPT;	/* Cause kdb() to return */
+
+handled:
+
+
+	return rv;
+}
+
+/*
+ * kdba_bp_trap
+ *
+ * 	Perform breakpoint processing upon entry to the
+ *	processor breakpoint instruction fault.   Determine and print
+ *	the active breakpoint.
+ *
+ * Parameters:
+ *	ef	Exception frame containing machine register state
+ *	error	Error number passed to kdb.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	0	Standard instruction or data breakpoint encountered
+ *	1	Single Step fault ('ss' command)
+ *	2	Single Step fault, caller should continue ('ssb' command)
+ *	3	No existing kdb breakpoint matches this debug exception
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ * 	If multiple processors receive debug exceptions simultaneously,
+ *	one may be waiting at the kdb fence in kdb() while the user
+ *	issues a 'bc' command to clear the breakpoint the processor which
+ * 	is waiting has already encountered.   If this is the case, the
+ *	debug registers will no longer match any entry in the breakpoint
+ *	table, and we'll return the value '3'.  This can cause a panic
+ *	in die_if_kernel().  It is safer to disable the breakpoint (bd),
+ *	'go' until all processors are past the breakpoint then clear the
+ *	breakpoint (bc).  This code recognises a breakpoint even when
+ *	disabled but not when it has been cleared.
+ *
+ *	WARNING: This routine resets the eip.  It should be called
+ *		 once per breakpoint and the result cached.
+ */
+
+kdb_dbtrap_t
+kdba_bp_trap(kdb_eframe_t ef, int error_unused)
+{
+	int i;
+	kdb_dbtrap_t rv;
+	kdb_bp_t *bp;
+
+	/*
+	 * Determine which breakpoint was encountered.
+	 */
+	if (KDB_DEBUG(BP))
+		kdb_printf("kdba_bp_trap: eip=0x%lx (not adjusted) "
+			   "msr=0x%lx trap=0x%lx ef=0x%p esp=0x%lx\n",
+			   ef->nip, ef->msr, ef->trap, ef, ef->gpr[1]);
+
+	rv = KDB_DB_NOBPT;	/* Cause kdb() to return */
+
+	for(i=0, bp=kdb_breakpoints; i<KDB_MAXBPT; i++, bp++) {
+		if (bp->bp_free)
+			continue;
+		if (!bp->bp_global && bp->bp_cpu != smp_processor_id())
+			continue;
+		 if (bp->bp_addr == (ef->nip - bp->bp_adjust)) {
+			/* Hit this breakpoint.  */
+			ef->nip -= bp->bp_adjust;
+			kdb_printf("Instruction(i) breakpoint #%d at 0x%lx (adjusted)\n",
+				  i, ef->nip);
+			kdb_id1(ef->nip);
+			rv = KDB_DB_BPT;
+			bp->bp_delay = 1;
+			break;
+		}
+	}
+
+	return rv;
+}
+
+/*
+ * kdba_handle_bp
+ *
+ *	Handle an instruction-breakpoint trap.  Called when re-installing
+ *	an enabled breakpoint which has has the bp_delay bit set.
+ *
+ * Parameters:
+ * Returns:
+ * Locking:
+ * Remarks:
+ *
+ * Ok, we really need to:
+ *	1) Restore the original instruction byte
+ *	2) Single Step
+ *	3) Restore breakpoint instruction
+ *	4) Continue.
+ *
+ *
+ */
+
+static void
+kdba_handle_bp(kdb_eframe_t ef, kdb_bp_t *bp)
+{
+	if (!ef) {
+		kdb_printf("kdba_handle_bp: ef == NULL\n");
+		return;
+	}
+
+	if (KDB_DEBUG(BP))
+		kdb_printf("ef->eip = 0x%lx\n", ef->nip);
+
+	/*
+	 * Setup single step
+	 */
+	kdba_setsinglestep(ef);
+
+	/* KDB_STATE_SSBPT is set when the kernel debugger must single step
+	 * a task in order to re-establish an instruction breakpoint which
+	 * uses the instruction replacement mechanism. 
+	 */
+	KDB_STATE_SET(SSBPT);
+
+	/*
+	 * Reset delay attribute
+	 */
+	bp->bp_delay = 0;
+	bp->bp_delayed = 1;
+}
+
+
+/*
+ * kdba_bptype
+ *
+ *	Return a string describing type of breakpoint.
+ *
+ * Parameters:
+ *	bph	Pointer to hardware breakpoint description
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Character string.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+char *
+kdba_bptype(kdbhard_bp_t *bph)
+{
+	char *mode;
+
+	mode = kdba_rwtypes[bph->bph_mode];
+
+	return mode;
+}
+
+/*
+ * kdba_printbpreg
+ *
+ *	Print register name assigned to breakpoint
+ *
+ * Parameters:
+ *	bph	Pointer hardware breakpoint structure
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+void
+kdba_printbpreg(kdbhard_bp_t *bph)
+{
+	kdb_printf(" in dr%ld", bph->bph_reg);
+}
+
+/*
+ * kdba_printbp
+ *
+ *	Print string describing hardware breakpoint.
+ *
+ * Parameters:
+ *	bph	Pointer to hardware breakpoint description
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+void
+kdba_printbp(kdb_bp_t *bp)
+{
+	kdb_printf("\n    is enabled");
+	if (bp->bp_hardtype) {
+		kdba_printbpreg(bp->bp_hard);
+		if (bp->bp_hard->bph_mode != 0) {
+			kdb_printf(" for %d bytes",
+				   bp->bp_hard->bph_length+1);
+		}
+	}
+}
+
+/*
+ * kdba_parsebp
+ *
+ *	Parse architecture dependent portion of the
+ *	breakpoint command.
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Zero for success, a kdb diagnostic for failure
+ * Locking:
+ *	None.
+ * Remarks:
+ *	for Ia32 architure, data access, data write and
+ *	I/O breakpoints are supported in addition to instruction
+ * 	breakpoints.
+ *
+ *	{datar|dataw|io|inst} [length]
+ */
+
+int
+kdba_parsebp(int argc, const char **argv, int *nextargp, kdb_bp_t *bp)
+{
+	int		nextarg = *nextargp;
+	int		diag;
+	kdbhard_bp_t 	*bph = &bp->bp_template;
+
+	bph->bph_mode = 0;		/* Default to instruction breakpoint */
+	bph->bph_length = 0;		/* Length must be zero for insn bp */
+	if ((argc + 1) != nextarg) {
+		if (strnicmp(argv[nextarg], "datar", sizeof("datar")) == 0) {
+			bph->bph_mode = 3;
+		} else if (strnicmp(argv[nextarg], "dataw", sizeof("dataw")) == 0) {
+			bph->bph_mode = 1;
+		} else if (strnicmp(argv[nextarg], "io", sizeof("io")) == 0) {
+			bph->bph_mode = 2;
+		} else if (strnicmp(argv[nextarg], "inst", sizeof("inst")) == 0) {
+			bph->bph_mode = 0;
+		} else {
+			return KDB_ARGCOUNT;
+		}
+
+		bph->bph_length = 3;	/* Default to 4 byte */
+
+		nextarg++;
+
+		if ((argc + 1) != nextarg) {
+			unsigned long len;
+
+			diag = kdbgetularg((char *)argv[nextarg],
+					   &len);
+			if (diag)
+				return diag;
+
+
+			if ((len > 4) || (len == 3))
+				return KDB_BADLENGTH;
+
+			bph->bph_length = len;
+			bph->bph_length--; /* Normalize for debug register */
+			nextarg++;
+		}
+
+		if ((argc + 1) != nextarg)
+			return KDB_ARGCOUNT;
+
+		/*
+		 * Indicate to architecture independent level that
+		 * a hardware register assignment is required to enable
+		 * this breakpoint.
+		 */
+
+		bph->bph_free = 0;
+	} else {
+		if (KDB_DEBUG(BP))
+			kdb_printf("kdba_bp: no args, forcehw is %d\n", bp->bp_forcehw);
+		if (bp->bp_forcehw) {
+			/*
+			 * We are forced to use a hardware register for this
+			 * breakpoint because either the bph or bpha
+			 * commands were used to establish this breakpoint.
+			 */
+			bph->bph_free = 0;
+		} else {
+			/*
+			 * Indicate to architecture dependent level that
+			 * the instruction replacement breakpoint technique
+			 * should be used for this breakpoint.
+			 */
+			bph->bph_free = 1;
+			bp->bp_adjust = PPC64_ADJUST_OFFSET;
+		}
+	}
+
+	*nextargp = nextarg;
+	return 0;
+}
+
+/*
+ * kdba_allocbp
+ *
+ *	Associate a hardware register with a breakpoint.
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	A pointer to the allocated register kdbhard_bp_t structure for
+ *	success, Null and a non-zero diagnostic for failure.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+kdbhard_bp_t *
+kdba_allocbp(kdbhard_bp_t *bph, int *diagp)
+{
+	int i;
+	kdbhard_bp_t *newbph;
+
+	for(i=0,newbph=kdb_hardbreaks; i < KDB_MAXHARDBPT; i++, newbph++) {
+		if (newbph->bph_free) {
+			break;
+		}
+	}
+
+	if (i == KDB_MAXHARDBPT) {
+		*diagp = KDB_TOOMANYDBREGS;
+		return NULL;
+	}
+
+	*diagp = 0;
+
+	/*
+	 * Copy data from template.  Can't just copy the entire template
+	 * here because the register number in kdb_hardbreaks must be
+	 * preserved.
+	 */
+	newbph->bph_data = bph->bph_data;
+	newbph->bph_write = bph->bph_write;
+	newbph->bph_mode = bph->bph_mode;
+	newbph->bph_length = bph->bph_length;
+
+	/*
+	 * Mark entry allocated.
+	 */
+	newbph->bph_free = 0;
+
+	return newbph;
+}
+
+/*
+ * kdba_freebp
+ *
+ *	Deallocate a hardware breakpoint
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Zero for success, a kdb diagnostic for failure
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+void
+kdba_freebp(kdbhard_bp_t *bph)
+{
+	bph->bph_free = 1;
+}
+
+/*
+ * kdba_initbp
+ *
+ *	Initialize the breakpoint table for the hardware breakpoint
+ *	register.
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Zero for success, a kdb diagnostic for failure
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ *	There is one entry per register.  On the ia32 architecture
+ *	all the registers are interchangeable, so no special allocation
+ *	criteria are required.
+ */
+
+void
+kdba_initbp(void)
+{
+	int i;
+	kdbhard_bp_t *bph;
+
+	/*
+	 * Clear the hardware breakpoint table
+	 */
+
+	memset(kdb_hardbreaks, '\0', sizeof(kdb_hardbreaks));
+
+	for(i=0,bph=kdb_hardbreaks; i<KDB_MAXHARDBPT; i++, bph++) {
+		bph->bph_reg = i;
+		bph->bph_free = 1;
+	}
+}
+
+/*
+ * kdba_installbp
+ *
+ *	Install a breakpoint
+ *
+ * Parameters:
+ *	ef	Exception frame
+ *	bp	Breakpoint structure for the breakpoint to be installed
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	For hardware breakpoints, a debug register is allocated
+ *	and assigned to the breakpoint.  If no debug register is
+ *	available, a warning message is printed and the breakpoint
+ *	is disabled.
+ *
+ *	For instruction replacement breakpoints, we must single-step
+ *	over the replaced instruction at this point so we can re-install
+ *	the breakpoint instruction after the single-step.
+ */
+
+int
+kdba_installbp(kdb_eframe_t ef, kdb_bp_t *bp)
+{
+    int rc;
+	/*
+	 * Install the breakpoint, if it is not already installed.
+	 */
+
+	if (KDB_DEBUG(BP)) {
+		kdb_printf("kdba_installbp bp_installed %d\n", bp->bp_installed);
+	}
+	if (!bp->bp_installed) {
+		if (bp->bp_hardtype) {
+			kdba_installdbreg(bp); 
+			bp->bp_installed = 1;
+			if (KDB_DEBUG(BP)) {
+				kdb_printf("kdba_installbp hardware reg %ld at " kdb_bfd_vma_fmt "\n",
+					   (long unsigned int) bp->bp_hard->bph_reg, (long unsigned int) bp->bp_addr);
+			}
+		} else if (bp->bp_delay) {
+			if (KDB_DEBUG(BP))
+				kdb_printf("kdba_installbp delayed bp\n");
+			kdba_handle_bp(ef, bp);
+		} else {
+		    if (KDB_DEBUG(BP))
+			kdb_printf("0x%lx 0x%lx 0x%lx\n",bp->bp_inst,bp->bp_addr,sizeof(bp->bp_addr));
+		    rc = kdb_getword(&bp->bp_inst, bp->bp_addr,sizeof(bp->bp_addr));
+		    kdb_putword(bp->bp_addr, PPC64_BREAKPOINT_INSTRUCTION,sizeof(PPC64_BREAKPOINT_INSTRUCTION));
+		    if (KDB_DEBUG(BP))
+			kdb_printf("kdba_installbp instruction 0x%x at " kdb_bfd_vma_fmt "\n",
+				   PPC64_BREAKPOINT_INSTRUCTION, bp->bp_addr);
+		    bp->bp_installed = 1;
+		}
+	}
+return 0;
+}
+
+/*
+ * kdba_removebp
+ *
+ *	Make a breakpoint ineffective.
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+int
+kdba_removebp(kdb_bp_t *bp)
+{
+	/*
+	 * For hardware breakpoints, remove it from the active register,
+	 * for software breakpoints, restore the instruction stream.
+	 */
+	if (KDB_DEBUG(BP)) {
+		kdb_printf("kdba_removebp bp_installed %d\n", bp->bp_installed);
+	}
+	if (bp->bp_installed) {
+		if (bp->bp_hardtype) {
+			if (KDB_DEBUG(BP)) {
+				kdb_printf("kdb: removing hardware reg %ld at " kdb_bfd_vma_fmt "\n",
+					   bp->bp_hard->bph_reg, bp->bp_addr);
+			}
+			kdba_removedbreg(bp);
+		} else
+		{
+			if (KDB_DEBUG(BP))
+				kdb_printf("kdb: restoring instruction 0x%lx at " kdb_bfd_vma_fmt "\n",
+					   bp->bp_inst, bp->bp_addr);
+			kdb_putword(bp->bp_addr, bp->bp_inst,sizeof(bp->bp_inst));
+		}
+		bp->bp_installed = 0;
+	}
+return 0;
+}
+
+#if 0
+#define systemcfg naca
+#endif
+
+/* install data breakpoint */
+void
+kdba_installdbreg(kdb_bp_t *bp) {
+    if (systemcfg->platform==PLATFORM_PSERIES)
+    {
+    /* set_dabr is the kdb form, using mtspr instructions */
+	set_dabr(bp->bp_addr); 
+    } else if (systemcfg->platform==PLATFORM_PSERIES_LPAR ) {
+	/*set_all_DABR(bp->bp_addr); missing from 2.5? */
+#if 0
+	HvCall_setDABR(bp->bp_addr);
+#endif
+    } else if (systemcfg->platform==PLATFORM_ISERIES_LPAR ) {
+	/* different hcall interface needed here. */
+    }
+}
+
+/* remove data breakpoint-- set it to zero. */
+void
+kdba_removedbreg(kdb_bp_t *bp) {
+    if (systemcfg->platform==PLATFORM_PSERIES)
+    {
+    /* set_dabr is the kdb form, using mtspr instructions */
+	set_dabr(0x0); 
+    } else if (systemcfg->platform==PLATFORM_PSERIES_LPAR ) {
+	/*set_all_DABR(bp->bp_addr); missing from 2.5? */
+#if 0
+	HvCall_setDABR(0x0);
+#endif
+    } else if (systemcfg->platform==PLATFORM_ISERIES_LPAR ) {
+	/* different hcall interface needed here. */
+    }
+}
diff -purN linux-2.5/arch/ppc64/kdb/kdba_bt.c linuxppc64-2.5/arch/ppc64/kdb/kdba_bt.c
--- linux-2.5/arch/ppc64/kdb/kdba_bt.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/kdba_bt.c	2003-11-26 17:14:34.000000000 +0000
@@ -0,0 +1,276 @@
+/*
+ * Minimalist Kernel Debugger - Architecture Dependent Stack Traceback
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *      Srinivasa Thirumalachar
+ *              RSE support for ia64
+ *	Masahiro Adegawa                1999/12/01
+ *		'sr' command, active flag in 'ps'
+ *	Scott Lurndal			1999/12/12
+ *		Significantly restructure for linux2.3
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ *
+ */
+
+#include <linux/ctype.h>
+#include <linux/string.h>
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/kallsyms.h>
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+#include <linux/ptrace.h>	/* for STACK_FRAME_OVERHEAD */
+#include <asm/system.h>
+#include "privinst.h"
+
+void systemreset(struct pt_regs *regs)
+{
+	udbg_printf("Oh no!\n");
+	kdb_printf("Oh no!\n");
+	kdb(KDB_REASON_OOPS, 0, (kdb_eframe_t) regs);
+	for (;;);
+}
+
+/* human name vector lookup. */
+static
+const char *getvecname(unsigned long vec)
+{
+	char *ret;
+	switch (vec) {
+	case 0x100:	ret = "(System Reset)"; break; 
+	case 0x200:	ret = "(Machine Check)"; break; 
+	case 0x300:	ret = "(Data Access)"; break; 
+	case 0x400:	ret = "(Instruction Access)"; break; 
+	case 0x500:	ret = "(Hardware Interrupt)"; break; 
+	case 0x600:	ret = "(Alignment)"; break; 
+	case 0x700:	ret = "(Program Check)"; break; 
+	case 0x800:	ret = "(FPU Unavailable)"; break; 
+	case 0x900:	ret = "(Decrementer)"; break; 
+	case 0xc00:	ret = "(System Call)"; break; 
+	case 0xd00:	ret = "(Single Step)"; break; 
+	case 0xf00:	ret = "(Performance Monitor)"; break; 
+	default: ret = "";
+	}
+	return ret;
+}
+
+
+extern unsigned long kdba_getword(unsigned long addr, size_t width);
+
+/* Copy a block of memory using kdba_getword().
+ * This is not efficient.
+ */
+static void kdba_getmem(unsigned long addr, void *p, int size)
+{
+	unsigned char *dst = (unsigned char *)p;
+	while (size > 0) {
+		*dst++ = kdba_getword(addr++, 1);
+		size--;
+	}
+}
+
+
+/*
+ * kdba_bt_stack_ppc
+ *
+ *	kdba_bt_stack with ppc specific parameters.
+ *	Specification as kdba_bt_stack plus :-
+ *
+ * Inputs:
+ *	As kba_bt_stack plus
+ *	regs_esp If 1 get esp from the registers (exception frame), if 0
+ *		 get esp from kdba_getregcontents.
+ */
+
+static int
+kdba_bt_stack_ppc(struct pt_regs *regs, kdb_machreg_t *addr, int argcount,
+		   struct task_struct *p, int regs_esp)
+{
+
+	kdb_machreg_t	esp,eip,ebp,old_esp;
+/*	kdb_symtab_t	symtab, *sym; */
+	kdbtbtable_t	tbtab;
+	/* declare these as raw ptrs so we don't get func descriptors */
+	extern void *ret_from_except, *ret_from_syscall_1;
+/*	int do_bottom_half_ret=0; */
+
+	const char *name;
+	char namebuf[128];
+	unsigned long symsize,symoffset;
+	char *symmodname;
+
+	if (!regs && !addr)
+	{
+		kdb_printf(" invalid regs pointer \n");
+		return 0;
+	}
+
+	/*
+	 * The caller may have supplied an address at which the
+	 * stack traceback operation should begin.  This address
+	 * is assumed by this code to point to a return-address
+	 * on the stack to be traced back.
+	 *
+	 * The end result of this will make it appear as if a function
+	 * entitled '<unknown>' was called from the function which
+	 * contains return-address.
+	 */
+	if (addr) {
+		eip = 0;
+		esp = *addr;
+		ebp=0;
+	} else {
+		ebp=regs->link;
+		eip = regs->nip;
+		if (regs_esp)
+			esp = regs->gpr[1];
+		else
+			kdba_getregcontents("esp", regs, &esp);
+	}
+
+	kdb_printf("          SP(esp)            PC(eip)      Function(args)\n");
+
+	/* (Ref: 64-bit PowerPC ELF ABI Spplement; Ian Lance Taylor, Zembu Labs).
+	 A PPC stack frame looks like this:
+
+	 High Address
+	 Back Chain
+	 FP reg save area
+	 GP reg save area
+	 Local var space
+	 Parameter save area		(SP+48)
+	 TOC save area		(SP+40)
+	 link editor doubleword	(SP+32)
+	 compiler doubleword		(SP+24)
+	 LR save			(SP+16)
+	 CR save			(SP+8)
+	 Back Chain			(SP+0)
+
+	 Note that the LR (ret addr) may not be saved in the *current* frame if
+	 no functions have been called from the current function.
+	 */
+
+	/*
+	 * Run through the activation records and print them.
+	 */
+	while (1) {
+		kdb_printf("0x%016lx  0x%016lx  ", esp, eip);
+		/*		kdbnearsym(eip, &symtab); */
+		kdba_find_tb_table(eip, &tbtab); 
+
+		/*		sym = symtab.sym_name ? &symtab : &tbtab.symtab; *//* use fake symtab if necessary */
+		name = NULL;
+		if (esp >= PAGE_OFFSET) { 
+			/*if ((sym) )*/ 
+			/* if this fails, eip is outside of kernel space, dont trust it. */
+			if (eip > PAGE_OFFSET) {
+				name = kallsyms_lookup(eip, &symsize, &symoffset, &symmodname,
+						       namebuf);
+			}
+			if (name) { 
+				kdb_printf("%s", name);
+			} else {
+				kdb_printf("NO_SYMBOL or Userspace"); 
+			}
+		}
+
+		/* if this fails, eip is outside of kernel space, dont trust data. */
+		if (eip > PAGE_OFFSET) { 
+			if (eip - symoffset > 0) {
+				kdb_printf(" +0x%lx", /*eip -*/ symoffset);
+			}
+		}
+		kdb_printf("\n");
+
+		/* ret_from_except=0xa5e0 ret_from_syscall_1=a378 do_bottom_half_ret=a5e0 */
+		if (esp < PAGE_OFFSET) { /* below kernelspace..   */
+			kdb_printf("<Stack contents outside of kernel space.  %.16lx>\n", esp );
+			break;
+		} else {
+			if (eip == (kdb_machreg_t)ret_from_except ||
+			    eip == (kdb_machreg_t)ret_from_syscall_1 /* ||
+								      eip == (kdb_machreg_t)do_bottom_half_ret */) {
+				/* pull exception regs from the stack */
+				struct pt_regs eregs;
+				kdba_getmem(esp+STACK_FRAME_OVERHEAD, &eregs, sizeof(eregs));
+				kdb_printf("  [exception: %lx:%s regs 0x%lx] nip:[0x%lx] gpr[1]:[0x%x]\n", eregs.trap,getvecname(eregs.trap), esp+STACK_FRAME_OVERHEAD,(unsigned int)eregs.nip,(unsigned int)eregs.gpr[1]);
+				old_esp = esp;
+				esp = kdba_getword(esp, 8);
+				if (!esp)
+					break;
+				eip = kdba_getword(esp+16, 8);	/* saved lr */
+				if (esp < PAGE_OFFSET) {  /* userspace... */
+					if (old_esp > PAGE_OFFSET) {
+						kdb_printf("<Stack drops into userspace here %.16lx>\n",esp);
+						break;
+					}
+				}
+				/* we want to follow exception registers, not into user stack.  ...   */
+				esp = eregs.gpr[1];
+				eip = eregs.nip;
+			} else {
+				esp = kdba_getword(esp, 8);
+				if (!esp)
+					break;
+				eip = kdba_getword(esp+16, 8);	/* saved lr */
+			}
+		}
+	}
+	return 0;
+}
+
+
+/*
+ * kdba_bt_stack
+ *
+ *	This function implements the 'bt' command.  Print a stack
+ *	traceback.
+ *
+ *	bt [<address-expression>]   (addr-exp is for alternate stacks)
+ *	btp <pid>		     (Kernel stack for <pid>)
+ *
+ * 	address expression refers to a return address on the stack.  It
+ *	may be preceeded by a frame pointer.
+ *
+ * Inputs:
+ *	regs	registers at time kdb was entered.
+ *	addr	Pointer to Address provided to 'bt' command, if any.
+ *	argcount
+ *	p	Pointer to task for 'btp' command.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	zero for success, a kdb diagnostic if error
+ * Locking:
+ *	none.
+ * Remarks:
+ *	mds comes in handy when examining the stack to do a manual
+ *	traceback.
+ */
+
+int
+kdba_bt_stack(struct pt_regs *regs, kdb_machreg_t *addr, int argcount,
+	      struct task_struct *p)
+{
+	return(kdba_bt_stack_ppc(regs, addr, argcount, p, 0));
+}
+
+int
+kdba_bt_process(struct task_struct *p, int argcount)
+{
+	return (kdba_bt_stack_ppc(p->thread.regs, (kdb_machreg_t *) p->thread.ksp, argcount, p, 0));
+}
diff -purN linux-2.5/arch/ppc64/kdb/kdba_id.c linuxppc64-2.5/arch/ppc64/kdb/kdba_id.c
--- linux-2.5/arch/ppc64/kdb/kdba_id.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/kdba_id.c	2003-10-13 16:16:57.000000000 +0000
@@ -0,0 +1,278 @@
+/*
+ * Minimalist Kernel Debugger - Architecture Dependent Instruction Disassembly
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *      Srinivasa Thirumalachar
+ *              RSE support for ia64
+ *	Masahiro Adegawa                1999/12/01
+ *		'sr' command, active flag in 'ps'
+ *	Scott Lurndal			1999/12/12
+ *		Significantly restructure for linux2.3
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ *
+ */
+
+#include <stdarg.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/ctype.h>
+#include <linux/string.h>
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+
+/*
+ * kdba_dis_getsym
+ *
+ *	Get a symbol for the disassembler.
+ *
+ * Parameters:
+ *	addr	Address for which to get symbol
+ *	dip	Pointer to disassemble_info
+ * Returns:
+ *	0
+ * Locking:
+ * Remarks:
+ *	Not used for kdb.
+ */
+
+/* ARGSUSED */
+static int
+kdba_dis_getsym(bfd_vma addr, disassemble_info *dip)
+{
+
+	return 0;
+}
+
+/*
+ * kdba_printaddress
+ *
+ *	Print (symbolically) an address.
+ *
+ * Parameters:
+ *	addr	Address for which to get symbol
+ *	dip	Pointer to disassemble_info
+ *	flag	True if a ":<tab>" sequence should follow the address
+ * Returns:
+ *	number of chars printed
+ * Locking:
+ * Remarks:
+ *
+ */
+
+/* ARGSUSED */
+void
+kdba_printaddress(kdb_machreg_t addr, disassemble_info *dip, int flag)
+{
+	kdb_symtab_t symtab;
+
+	/*
+	 * Print a symbol name or address as necessary.
+	 */
+	kdbnearsym(addr, &symtab);
+	if (symtab.sym_name) {
+		/* Do not use kdb_symbol_print here, it always does
+		 * kdb_printf but we want dip->fprintf_func.
+		 */
+		dip->fprintf_func(dip->stream,
+			"0x%0*lx %s",
+			2*sizeof(addr), addr, symtab.sym_name);
+		/* Add offset if needed.  Pad output with blanks to get
+		 * consistent size symbols for disassembly listings.
+		 */
+		if (addr == symtab.sym_start) {
+			if (!flag)
+				dip->fprintf_func(dip->stream, "         ");
+		} else {
+			int len, i;
+			char buf[20];
+			sprintf(buf, "%lx", addr - symtab.sym_start);
+			dip->fprintf_func(dip->stream, "+0x%s", buf);
+			if (!flag) {
+				len = strlen(buf);
+				for (i = len; i < 6; i++)
+					dip->fprintf_func(dip->stream, " ");
+			}
+		}
+
+	} else {
+		dip->fprintf_func(dip->stream, "0x%0*lx", 2*sizeof(addr), addr);
+	}
+
+	if (flag)
+		dip->fprintf_func(dip->stream, ":   ");
+}
+
+/*
+ * kdba_dis_printaddr
+ *
+ *	Print (symbolically) an address.  Called by GNU disassembly
+ *	code via disassemble_info structure.
+ *
+ * Parameters:
+ *	addr	Address for which to get symbol
+ *	dip	Pointer to disassemble_info
+ * Returns:
+ *	number of chars printed.
+ * Locking:
+ * Remarks:
+ *	This function will never append ":<tab>" to the printed
+ *	symbolic address.
+ */
+
+static void
+kdba_dis_printaddr(bfd_vma addr, disassemble_info *dip)
+{
+	return kdba_printaddress(addr, dip, 0);
+}
+
+/*
+ * kdba_dis_getmem
+ *
+ *	Fetch 'length' bytes from 'addr' into 'buf'.
+ *
+ * Parameters:
+ *	addr	Address for which to get symbol
+ *	buf	Address of buffer to fill with bytes from 'addr'
+ *	length	Number of bytes to fetch
+ *	dip	Pointer to disassemble_info
+ * Returns:
+ *	0
+ * Locking:
+ * Remarks:
+ *
+ */
+extern int kdba_getword(unsigned long addr, size_t width);
+
+
+/* ARGSUSED */
+static int
+kdba_dis_getmem(bfd_vma addr, bfd_byte *buf, unsigned int length, disassemble_info *dip)
+{
+	bfd_byte	*bp = buf;
+	int		i;
+
+	/*
+	 * Fill the provided buffer with bytes from
+	 * memory, starting at address 'addr' for 'length bytes.
+	 *
+	 */
+
+	for(i=0; i<length; i++ ){
+		*bp++ = (bfd_byte)kdba_getword(addr++, sizeof(bfd_byte));
+	}
+
+	return 0;
+}
+
+/*
+ * kdba_id_parsemode
+ *
+ * 	Parse IDMODE environment variable string and
+ *	set appropriate value into "disassemble_info" structure.
+ *
+ * Parameters:
+ *	mode	Mode string
+ *	dip	Disassemble_info structure pointer
+ * Returns:
+ * Locking:
+ * Remarks:
+ *	We handle the values 'x86' and '8086' to enable either
+ *	32-bit instruction set or 16-bit legacy instruction set.
+ */
+
+int
+kdba_id_parsemode(const char *mode, disassemble_info *dip)
+{
+
+
+	return 0;
+}
+
+/*
+ * kdba_check_pc
+ *
+ * 	Check that the pc is satisfactory.
+ *
+ * Parameters:
+ *	pc	Program Counter Value.
+ * Returns:
+ *	None
+ * Locking:
+ *	None.
+ * Remarks:
+ *	Can change pc.
+ */
+
+void
+kdba_check_pc(kdb_machreg_t *pc)
+{
+	/* No action */
+}
+
+/*
+ * kdba_id_printinsn
+ *
+ * 	Format and print a single instruction at 'pc'. Return the
+ *	length of the instruction.
+ *
+ * Parameters:
+ *	pc	Program Counter Value.
+ *	dip	Disassemble_info structure pointer
+ * Returns:
+ *	Length of instruction, -1 for error.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	Depends on 'IDMODE' environment variable.
+ */
+
+int
+kdba_id_printinsn(kdb_machreg_t pc, disassemble_info *dip)
+{
+	kdba_dis_printaddr(pc, dip);
+	return print_insn_big_powerpc(pc, dip);
+}
+
+/*
+ * kdba_id_init
+ *
+ * 	Initialize the architecture dependent elements of
+ *	the disassembly information structure
+ *	for the GNU disassembler.
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+void __init
+kdba_id_init(disassemble_info *dip)
+{
+	dip->read_memory_func       = kdba_dis_getmem;
+	dip->print_address_func     = kdba_dis_printaddr;
+	dip->symbol_at_address_func = kdba_dis_getsym;
+
+	dip->flavour                = bfd_target_elf_flavour;
+	dip->arch		    = bfd_arch_powerpc;
+	dip->mach		    = bfd_mach_ppc_750;
+	dip->endian	    	    = BFD_ENDIAN_BIG;
+
+	dip->display_endian         = BFD_ENDIAN_BIG;
+}
diff -purN linux-2.5/arch/ppc64/kdb/kdba_io.c linuxppc64-2.5/arch/ppc64/kdb/kdba_io.c
--- linux-2.5/arch/ppc64/kdb/kdba_io.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/kdba_io.c	2003-11-26 17:14:34.000000000 +0000
@@ -0,0 +1,101 @@
+/*
+ * Kernel Debugger Console I/O handler
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *	Chuck Fleckenstein		1999/07/20
+ *		Move kdb_info struct declaration to this file
+ *		for cases where serial support is not compiled into
+ *		the kernel.
+ *
+ *	Masahiro Adegawa		1999/07/20
+ *		Handle some peculiarities of japanese 86/106
+ *		keyboards.
+ *
+ *	marc@mucom.co.il		1999/07/20
+ *		Catch buffer overflow for serial input.
+ *
+ *      Scott Foehner
+ *              Port to ia64
+ *
+ *	Scott Lurndal			2000/01/03
+ *		Restructure for v1.0
+ *
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ *
+ *	Andi Kleen			2000/03/19
+ *		Support simultaneous input from serial line and keyboard.
+ */
+
+#include <linux/kernel.h>
+#include <asm/io.h>
+#include <linux/wait.h>
+#include <linux/delay.h>
+#include <linux/console.h>
+#include <linux/ctype.h>
+#include <linux/keyboard.h>
+#include <linux/serial_reg.h>
+
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+#include <asm/machdep.h>
+#undef FILE
+
+int kdb_port;
+
+struct kdb_serial kdb_serial;
+/*{
+	int io_type;
+	unsigned long iobase;
+	unsigned long ioreg_shift;
+} kdb_serial_t;
+*/
+
+int inchar(void);
+
+
+char *
+kdba_read(char *buffer, size_t bufsize)
+{
+	char	*cp = buffer;
+	char	*bufend = buffer+bufsize-2;	/* Reserve space for newline and null byte */
+
+	for (;;) {
+	    unsigned char key = ppc_md.udbg_getc();
+		/* Echo is done in the low level functions */
+		switch (key) {
+		case '\b': /* backspace */
+		case '\x7f': /* delete */
+			if (cp > buffer) {
+				udbg_puts("\b \b");
+				--cp;
+			}
+			break;
+		case '\n': /* enter */
+		case '\r': /* - the other enter... */
+			ppc_md.udbg_putc('\n');
+			*cp++ = '\n';
+			*cp++ = '\0';
+			return buffer;
+		case '\x00': /* check for NULL from udbg_getc */
+		        break;
+		default:
+			if (cp < bufend)
+			ppc_md.udbg_putc(key);
+				*cp++ = key;
+			break;
+		}
+	}
+}
+
+
+
diff -purN linux-2.5/arch/ppc64/kdb/kdbasupport.c linuxppc64-2.5/arch/ppc64/kdb/kdbasupport.c
--- linux-2.5/arch/ppc64/kdb/kdbasupport.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/kdbasupport.c	2004-01-22 06:11:58.000000000 +0000
@@ -0,0 +1,2053 @@
+/*
+ * Kernel Debugger Architecture Independent Support Functions
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *	Scott Lurndal			1999/12/12
+ *		v1.0 restructuring.
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ */
+
+#include <linux/config.h>
+#include <linux/string.h>
+#include <linux/stddef.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/ptrace.h>
+#include <linux/mm.h>
+#include <linux/sched.h>
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+
+#include <asm/processor.h>
+#include "privinst.h"
+#include <asm/uaccess.h>
+#include <asm/machdep.h>
+
+extern const char *kdb_diemsg;
+unsigned long cpus_in_kdb=0;
+volatile unsigned long kdb_do_reboot=0;
+
+/* prototypes */
+int valid_ppc64_kernel_address(unsigned long addr, unsigned long size);
+int kdba_excprint(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+int kdba_super_regs(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+int kdba_dissect_msr(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+int kdba_halt(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+int kdba_dump_tce_table(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+int kdba_kernelversion(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+int kdba_dmesg(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+int kdba_dump_pci_info(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+int kdba_rd(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+int kdba_bt(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+unsigned long kdba_getword(unsigned long addr, size_t width);
+
+
+extern int kdb_dmesg(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+extern int kdb_ps(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+
+extern int kdb_parse(const char *cmdstr, struct pt_regs *regs);
+
+/* 60secs * 1000*1000 usecs/sec.  HMC interface requires larger amount of time,.. */
+#define KDB_RESET_TIMEOUT 60*1000*1000
+
+/* kdb will use UDBG */
+#define USE_UDBG
+
+#ifdef USE_UDBG
+#include <asm/udbg.h>
+#endif
+
+#include <linux/kbd_kern.h>
+#include <linux/sysrq.h>
+#include <linux/interrupt.h>
+
+#ifdef CONFIG_MAGIC_SYSRQ
+static void
+sysrq_handle_kdb(int key, struct pt_regs *pt_regs, struct kbd_struct *kbd, struct tty_struct *tty) 
+{
+  kdb(KDB_REASON_KEYBOARD,0,pt_regs);
+}
+
+static struct sysrq_key_op sysrq_kdb_op = 
+{
+	handler:	(void*)sysrq_handle_kdb,
+	help_msg:	"(x)kdb",
+	action_msg:	"Entering kdb\n",
+};
+
+void
+kdb_map_scc(void)
+{
+	/* register sysrq 'x' */
+	__sysrq_put_key_op('x', &sysrq_kdb_op);
+}
+#endif
+
+
+/*
+ * kdba_prologue
+ *
+ *	This function analyzes a gcc-generated function prototype
+ *	with or without frame pointers to determine the amount of
+ *	automatic storage and register save storage is used on the
+ *	stack of the target function.  It only counts instructions
+ *	that have been executed up to but excluding the current nip.
+ * Inputs:
+ *	code	Start address of function code to analyze
+ *	pc	Current program counter within function
+ *	sp	Current stack pointer for function
+ *	fp	Current frame pointer for function, may not be valid
+ *	ss	Start of stack for current process.
+ *	caller	1 if looking for data on the caller frame, 0 for callee.
+ * Outputs:
+ *	ar	Activation record, all fields may be set.  fp and oldfp
+ *		are 0 if they cannot be extracted.  return is 0 if the
+ *		code cannot find a valid return address.  args and arg0
+ *		are 0 if the number of arguments cannot be safely
+ *		calculated.
+ * Returns:
+ *	1 if prologue is valid, 0 otherwise.  If pc is 0 treat it as a
+ *	valid prologue to allow bt on wild branches.
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ */
+int
+kdba_prologue(const kdb_symtab_t *symtab, kdb_machreg_t pc, kdb_machreg_t sp,
+	      kdb_machreg_t fp, kdb_machreg_t ss, int caller, kdb_ar_t *ar)
+{
+	/* We don't currently use kdb's generic activation record scanning
+	 * code to handle backtrace.
+	 */
+	return 0;
+}
+
+
+
+/*
+ * kdba_getregcontents
+ *
+ *	Return the contents of the register specified by the
+ *	input string argument.   Return an error if the string
+ *	does not match a machine register.
+ *
+ *	The following pseudo register names are supported:
+ *	   &regs	 - Prints address of exception frame
+ *	   kesp		 - Prints kernel stack pointer at time of fault
+ *	   cesp		 - Prints current kernel stack pointer, inside kdb
+ *	   ceflags	 - Prints current flags, inside kdb
+ *	   %<regname>	 - Uses the value of the registers at the
+ *			   last time the user process entered kernel
+ *			   mode, instead of the registers at the time
+ *			   kdb was entered.
+ *
+ * Parameters:
+ *	regname		Pointer to string naming register
+ *	regs		Pointer to structure containing registers.
+ * Outputs:
+ *	*contents	Pointer to unsigned long to recieve register contents
+ * Returns:
+ *	0		Success
+ *	KDB_BADREG	Invalid register name
+ * Locking:
+ * 	None.
+ * Remarks:
+ * 	If kdb was entered via an interrupt from the kernel itself then
+ *	ss and esp are *not* on the stack.
+ */
+
+static struct kdbregs {
+	char   *reg_name;
+	size_t	reg_offset;
+} kdbreglist[] = {
+	{ "gpr0",	offsetof(struct pt_regs, gpr[0]) },
+	{ "gpr1",	offsetof(struct pt_regs, gpr[1]) },
+	{ "gpr2",	offsetof(struct pt_regs, gpr[2]) },
+	{ "gpr3",	offsetof(struct pt_regs, gpr[3]) },
+	{ "gpr4",	offsetof(struct pt_regs, gpr[4]) },
+	{ "gpr5",	offsetof(struct pt_regs, gpr[5]) },
+	{ "gpr6",	offsetof(struct pt_regs, gpr[6]) },
+	{ "gpr7",	offsetof(struct pt_regs, gpr[7]) },
+	{ "gpr8",	offsetof(struct pt_regs, gpr[8]) },
+	{ "gpr9",	offsetof(struct pt_regs, gpr[9]) },
+	{ "gpr10",	offsetof(struct pt_regs, gpr[10]) },
+	{ "gpr11",	offsetof(struct pt_regs, gpr[11]) },
+	{ "gpr12",	offsetof(struct pt_regs, gpr[12]) },
+	{ "gpr13",	offsetof(struct pt_regs, gpr[13]) },
+	{ "gpr14",	offsetof(struct pt_regs, gpr[14]) },
+	{ "gpr15",	offsetof(struct pt_regs, gpr[15]) },
+	{ "gpr16",	offsetof(struct pt_regs, gpr[16]) },
+	{ "gpr17",	offsetof(struct pt_regs, gpr[17]) },
+	{ "gpr18",	offsetof(struct pt_regs, gpr[18]) },
+	{ "gpr19",	offsetof(struct pt_regs, gpr[19]) },
+	{ "gpr20",	offsetof(struct pt_regs, gpr[20]) },
+	{ "gpr21",	offsetof(struct pt_regs, gpr[21]) },
+	{ "gpr22",	offsetof(struct pt_regs, gpr[22]) },
+	{ "gpr23",	offsetof(struct pt_regs, gpr[23]) },
+	{ "gpr24",	offsetof(struct pt_regs, gpr[24]) },
+	{ "gpr25",	offsetof(struct pt_regs, gpr[25]) },
+	{ "gpr26",	offsetof(struct pt_regs, gpr[26]) },
+	{ "gpr27",	offsetof(struct pt_regs, gpr[27]) },
+	{ "gpr28",	offsetof(struct pt_regs, gpr[28]) },
+	{ "gpr29",	offsetof(struct pt_regs, gpr[29]) },
+	{ "gpr30",	offsetof(struct pt_regs, gpr[30]) },
+	{ "gpr31",	offsetof(struct pt_regs, gpr[31]) },
+	{ "nip",	offsetof(struct pt_regs, nip) },
+	{ "msr",	offsetof(struct pt_regs, msr) },
+	{ "esp",	offsetof(struct pt_regs, gpr[1]) },
+  	{ "orig_gpr3",  offsetof(struct pt_regs, orig_gpr3) },
+	{ "ctr", 	offsetof(struct pt_regs, ctr) },
+	{ "link",	offsetof(struct pt_regs, link) },
+	{ "xer", 	offsetof(struct pt_regs, xer) },
+	{ "ccr",	offsetof(struct pt_regs, ccr) },
+	{ "mq",		offsetof(struct pt_regs, softe) /* mq */ },
+	{ "trap",	offsetof(struct pt_regs, trap) },
+	{ "dar",	offsetof(struct pt_regs, dar)  },
+	{ "dsisr",	offsetof(struct pt_regs, dsisr) },
+	{ "result",	offsetof(struct pt_regs, result) },
+};
+
+static const int nkdbreglist = sizeof(kdbreglist) / sizeof(struct kdbregs);
+
+unsigned long
+getsp(void)
+{
+	unsigned long x;
+	asm("mr %0,1" : "=r" (x):);
+	return x;
+}
+
+int
+kdba_getregcontents(const char *regname,
+		    struct pt_regs *regs,
+		    kdb_machreg_t *contents)
+{
+	int i;
+
+	if (strcmp(regname, "&regs") == 0) {
+		*contents = (unsigned long)regs;
+		return 0;
+	}
+
+	if (strcmp(regname, "kesp") == 0) {
+		*contents = (unsigned long) current->thread.ksp;
+		return 0;
+	}
+
+	if (strcmp(regname, "cesp") == 0) {
+		*contents = getsp();
+		return 0;
+	}
+
+	if (strcmp(regname, "ceflags") == 0) {
+		long flags;
+		local_save_flags(flags);
+		*contents = flags;
+		return 0;
+	}
+
+	if (regname[0] == '%') {
+		/* User registers:  %%e[a-c]x, etc */
+		regname++;
+		regs = (struct pt_regs *)
+			(current->thread.ksp - sizeof(struct pt_regs));
+	}
+
+	for (i=0; i<nkdbreglist; i++) {
+		if (strnicmp(kdbreglist[i].reg_name,
+			     regname,
+			     strlen(regname)) == 0)
+			break;
+	}
+
+	if ((i < nkdbreglist)
+	 && (strlen(kdbreglist[i].reg_name) == strlen(regname))) {
+		*contents = *(unsigned long *)((unsigned long)regs +
+				kdbreglist[i].reg_offset);
+		return(0);
+	}
+
+	return KDB_BADREG;
+}
+
+/*
+ * kdba_setregcontents
+ *
+ *	Set the contents of the register specified by the
+ *	input string argument.   Return an error if the string
+ *	does not match a machine register.
+ *
+ *	Supports modification of user-mode registers via
+ *	%<register-name>
+ *
+ * Parameters:
+ *	regname		Pointer to string naming register
+ *	regs		Pointer to structure containing registers.
+ *	contents	Unsigned long containing new register contents
+ * Outputs:
+ * Returns:
+ *	0		Success
+ *	KDB_BADREG	Invalid register name
+ * Locking:
+ * 	None.
+ * Remarks:
+ */
+
+int
+kdba_setregcontents(const char *regname,
+		  struct pt_regs *regs,
+		  unsigned long contents)
+{
+	int i;
+
+	if (regname[0] == '%') {
+		regname++;
+		regs = (struct pt_regs *)
+			(current->thread.ksp - sizeof(struct pt_regs));
+	}
+
+	for (i=0; i<nkdbreglist; i++) {
+		if (strnicmp(kdbreglist[i].reg_name,
+			     regname,
+			     strlen(regname)) == 0)
+			break;
+	}
+
+	if ((i < nkdbreglist)
+	 && (strlen(kdbreglist[i].reg_name) == strlen(regname))) {
+		*(unsigned long *)((unsigned long)regs
+				   + kdbreglist[i].reg_offset) = contents;
+		return 0;
+	}
+
+	return KDB_BADREG;
+}
+
+/*
+ * kdba_dumpregs
+ *
+ *	Dump the specified register set to the display.
+ *
+ * Parameters:
+ *	regs		Pointer to structure containing registers.
+ *	type		Character string identifying register set to dump
+ *	extra		string further identifying register (optional)
+ * Outputs:
+ * Returns:
+ *	0		Success
+ * Locking:
+ * 	None.
+ * Remarks:
+ *	This function will dump the general register set if the type
+ *	argument is NULL (struct pt_regs).   The alternate register
+ *	set types supported by this function:
+ *
+ *	d 		Debug registers
+ *	c		Control registers
+ *	u		User registers at most recent entry to kernel
+ * Following not yet implemented:
+ *	m		Model Specific Registers (extra defines register #)
+ *	r		Memory Type Range Registers (extra defines register)
+ */
+
+int
+kdba_dumpregs(struct pt_regs *regs,
+	    const char *type,
+	    const char *extra)
+{
+	int i;
+	int count = 0;
+
+	if (type
+	 && (type[0] == 'u')) {
+		type = NULL;
+		regs = (struct pt_regs *)
+			(current->thread.ksp - sizeof(struct pt_regs));
+	}
+
+	if (type == NULL) {
+		struct kdbregs *rlp;
+		kdb_machreg_t contents;
+
+		for (i=0, rlp=kdbreglist; i<nkdbreglist; i++,rlp++) {
+			kdba_getregcontents(rlp->reg_name, regs, &contents);
+			kdb_printf("%-5s = 0x%p%c", rlp->reg_name, (void *)contents, (++count % 2) ? ' ' : '\n');
+		}
+
+		kdb_printf("&regs = 0x%p\n", regs);
+		return 0;
+ 	} else {  /* dump a specific register */
+ 	    kdb_machreg_t contents;
+ 	    if (KDB_BADREG==kdba_getregcontents(type, regs, &contents)) 
+ 		kdb_printf("register %-5s not found \n",type);
+ 	    else
+ 		kdb_printf("%-5s = 0x%p%c", type, (void *)contents, '\n');
+ 	    return 0;
+	}
+
+	switch (type[0]) {
+	case 'm':
+		break;
+	case 'r':
+		break;
+	default:
+		return KDB_BADREG;
+	}
+
+	/* NOTREACHED */
+	return 0;
+}
+
+kdb_machreg_t
+kdba_getpc(kdb_eframe_t ef)
+{
+    return ef ? ef->nip : 0;
+}
+
+int
+kdba_setpc(kdb_eframe_t ef, kdb_machreg_t newpc)
+{
+/* for ppc64, newpc passed in is actually a function descriptor for kdb. */
+    ef->nip =     kdba_getword(newpc+8, 8);
+    KDB_STATE_SET(IP_ADJUSTED);
+    return 0;
+}
+
+/*
+ * kdba_main_loop
+ *
+ *	Do any architecture specific set up before entering the main kdb loop.
+ *	The primary function of this routine is to make all processes look the
+ *	same to kdb, kdb must be able to list a process without worrying if the
+ *	process is running or blocked, so make all process look as though they
+ *	are blocked.
+ *
+ * Inputs:
+ *	reason		The reason KDB was invoked
+ *	error		The hardware-defined error code
+ *	error2		kdb's current reason code.  Initially error but can change
+ *			acording to kdb state.
+ *	db_result	Result from break or debug point.
+ *	ef		The exception frame at time of fault/breakpoint.  If reason
+ *			is KDB_REASON_SILENT then ef is NULL, otherwise it should
+ *			always be valid.
+ * Returns:
+ *	0	KDB was invoked for an event which it wasn't responsible
+ *	1	KDB handled the event for which it was invoked.
+ * Outputs:
+ *	Sets nip and esp in current->thread.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	none.
+ */
+
+int
+kdba_main_loop(kdb_reason_t reason, kdb_reason_t reason2, int error,
+	       kdb_dbtrap_t db_result, kdb_eframe_t ef)
+{
+	int rv;
+	kdb_do_reboot=0;
+
+	/* case where incoming registers are missing */
+	if (ef == NULL)
+	{
+		struct pt_regs regs;
+		asm volatile ("std	0,0(%0)\n\
+                               std	1,8(%0)\n\
+                               std	2,16(%0)\n\
+                               std	3,24(%0)\n\
+                               std	4,32(%0)\n\
+                               std	5,40(%0)\n\
+                               std	6,48(%0)\n\
+                               std	7,56(%0)\n\
+                               std	8,64(%0)\n\
+                               std	9,72(%0)\n\
+                               std	10,80(%0)\n\
+                               std	11,88(%0)\n\
+                               std	12,96(%0)\n\
+                               std	13,104(%0)\n\
+                               std	14,112(%0)\n\
+                               std	15,120(%0)\n\
+                               std	16,128(%0)\n\
+                               std	17,136(%0)\n\
+                               std	18,144(%0)\n\
+                               std	19,152(%0)\n\
+                               std	20,160(%0)\n\
+                               std	21,168(%0)\n\
+                               std	22,176(%0)\n\
+                               std	23,184(%0)\n\
+                               std	24,192(%0)\n\
+                               std	25,200(%0)\n\
+                               std	26,208(%0)\n\
+                               std	27,216(%0)\n\
+                               std	28,224(%0)\n\
+                               std	29,232(%0)\n\
+                               std	30,240(%0)\n\
+                               std	31,248(%0)" : : "b" (&regs));
+                /* one extra step back..  this frame disappears */
+		regs.gpr[1] = kdba_getword(regs.gpr[1], 8);
+		/* Fetch the link reg for this stack frame.
+		 NOTE: the prev kdb_printf fills in the lr. */
+		regs.nip = regs.link = ((unsigned long *)regs.gpr[1])[2];
+		regs.msr = get_msr();
+		regs.ctr = get_ctr();
+		regs.xer = get_xer();
+		regs.ccr = get_cr();
+		regs.trap = 0;
+		/*current->thread.regs = &regs; */
+		ef = &regs;
+	}
+	cpus_in_kdb++;
+	rv = kdb_main_loop(reason, reason2, error, db_result, ef);
+	cpus_in_kdb--;
+	return rv;
+}
+
+void
+kdba_disableint(kdb_intstate_t *state)
+{
+	unsigned long *fp = (unsigned long *)state;
+	unsigned long flags;
+	local_irq_save(flags);
+	*fp = flags;
+}
+
+void
+kdba_restoreint(kdb_intstate_t *state)
+{
+	unsigned long flags = *(unsigned long *)state;
+	local_irq_restore(flags);
+}
+
+void
+kdba_setsinglestep(struct pt_regs *regs)
+{
+	regs->msr |= MSR_SE;
+}
+
+void
+kdba_clearsinglestep(struct pt_regs *regs)
+{
+	
+	regs->msr &= ~MSR_SE;
+}
+
+int
+kdba_getcurrentframe(struct pt_regs *regs)
+{
+	regs->gpr[1] = getsp();
+	/* this stack pointer becomes invalid after we return, so take another step back.  */
+	regs->gpr[1] = kdba_getword(regs->gpr[1], 8);
+	return 0;
+}
+
+#ifdef KDB_HAVE_LONGJMP
+int
+kdba_setjmp(kdb_jmp_buf *buf)
+{
+    asm volatile (
+	"mflr 0; std 0,0(%0)\n\
+	 std	1,8(%0)\n\
+	 std	2,16(%0)\n\
+	 mfcr 0; std 0,24(%0)\n\
+	 std	13,32(%0)\n\
+	 std	14,40(%0)\n\
+	 std	15,48(%0)\n\
+	 std	16,56(%0)\n\
+	 std	17,64(%0)\n\
+	 std	18,72(%0)\n\
+	 std	19,80(%0)\n\
+	 std	20,88(%0)\n\
+	 std	21,96(%0)\n\
+	 std	22,104(%0)\n\
+	 std	23,112(%0)\n\
+	 std	24,120(%0)\n\
+	 std	25,128(%0)\n\
+	 std	26,136(%0)\n\
+	 std	27,144(%0)\n\
+	 std	28,152(%0)\n\
+	 std	29,160(%0)\n\
+	 std	30,168(%0)\n\
+	 std	31,176(%0)\n\
+	 " : : "r" (buf));
+    KDB_STATE_SET(LONGJMP);
+    return 0;
+}
+
+void
+kdba_longjmp(kdb_jmp_buf *buf, int val)
+{
+    if (val == 0)
+	val = 1;
+    asm volatile (
+	"ld	13,32(%0)\n\
+	 ld	14,40(%0)\n\
+	 ld	15,48(%0)\n\
+	 ld	16,56(%0)\n\
+	 ld	17,64(%0)\n\
+	 ld	18,72(%0)\n\
+	 ld	19,80(%0)\n\
+	 ld	20,88(%0)\n\
+	 ld	21,96(%0)\n\
+	 ld	22,104(%0)\n\
+	 ld	23,112(%0)\n\
+	 ld	24,120(%0)\n\
+	 ld	25,128(%0)\n\
+	 ld	26,136(%0)\n\
+	 ld	27,144(%0)\n\
+	 ld	28,152(%0)\n\
+	 ld	29,160(%0)\n\
+	 ld	30,168(%0)\n\
+	 ld	31,176(%0)\n\
+	 ld	0,24(%0)\n\
+	 mtcrf	0x38,0\n\
+	 ld	0,0(%0)\n\
+	 ld	1,8(%0)\n\
+	 ld	2,16(%0)\n\
+	 mtlr	0\n\
+	 mr	3,%1\n\
+	 " : : "r" (buf), "r" (val));
+}
+#endif
+
+/*
+ * kdba_enable_mce
+ *
+ *	This function is called once on each CPU to enable machine
+ *	check exception handling.
+ *
+ * Inputs:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ */
+
+void
+kdba_enable_mce(void)
+{
+}
+
+/*
+ * kdba_enable_lbr
+ *
+ *	Enable last branch recording.
+ *
+ * Parameters:
+ *	None.
+ * Returns:
+ *	None
+ * Locking:
+ *	None
+ * Remarks:
+ *	None.
+ */
+
+void
+kdba_enable_lbr(void)
+{
+}
+
+/*
+ * kdba_disable_lbr
+ *
+ *	disable last branch recording.
+ *
+ * Parameters:
+ *	None.
+ * Returns:
+ *	None
+ * Locking:
+ *	None
+ * Remarks:
+ *	None.
+ */
+
+void
+kdba_disable_lbr(void)
+{
+}
+
+/*
+ * kdba_print_lbr
+ *
+ *	Print last branch and last exception addresses
+ *
+ * Parameters:
+ *	None.
+ * Returns:
+ *	None
+ * Locking:
+ *	None
+ * Remarks:
+ *	None.
+ */
+
+void
+kdba_print_lbr(void)
+{
+}
+
+/*
+ * kdba_getword
+ *
+ * 	Architecture specific function to access kernel virtual
+ *	address space.
+ *
+ * Parameters:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	None.
+ */
+
+/* 	if (access_ok(VERIFY_READ,__gu_addr,size))			\ */
+ 
+extern inline void sync(void)
+{
+	asm volatile("sync; isync");
+}
+
+extern void (*debugger_fault_handler)(struct pt_regs *);
+extern void longjmp(u_int *, int);
+
+unsigned long
+kdba_getword(unsigned long addr, size_t width)
+{
+	/*
+	 * This function checks the address for validity.  Any address
+	 * in the range PAGE_OFFSET to high_memory is legal, any address
+	 * which maps to a vmalloc region is legal, and any address which
+	 * is a user address, we use get_user() to verify validity.
+	 */
+
+    if (!valid_ppc64_kernel_address(addr, width)) {
+		        /*
+			 * Would appear to be an illegal kernel address;
+			 * Print a message once, and don't print again until
+			 * a legal address is used.
+			 */
+			if (!KDB_STATE(SUPPRESS)) {
+				kdb_printf("    kdb: Not a kernel-space address 0x%lx \n",addr);
+				KDB_STATE_SET(SUPPRESS);
+			}
+			return 0L;
+	}
+
+
+	/*
+	 * A good address.  Reset error flag.
+	 */
+	KDB_STATE_CLEAR(SUPPRESS);
+
+	switch (width) {
+	case 8:
+	{	unsigned long *lp;
+
+		lp = (unsigned long *)(addr);
+		return *lp;
+	}
+	case 4:
+	{	unsigned int *ip;
+
+		ip = (unsigned int *)(addr);
+		return *ip;
+	}
+	case 2:
+	{	unsigned short *sp;
+
+		sp = (unsigned short *)(addr);
+		return *sp;
+	}
+	case 1:
+	{	unsigned char *cp;
+
+		cp = (unsigned char *)(addr);
+		return *cp;
+	}
+	}
+
+	kdb_printf("kdbgetword: Bad width\n");
+	return 0L;
+}
+
+
+
+/*
+ * kdba_putword
+ *
+ * 	Architecture specific function to access kernel virtual
+ *	address space.
+ *
+ * Parameters:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	None.
+ */
+
+unsigned long
+kdba_putword(unsigned long addr, size_t size, unsigned long contents)
+{
+	/*
+	 * This function checks the address for validity.  Any address
+	 * in the range PAGE_OFFSET to high_memory is legal, any address
+	 * which maps to a vmalloc region is legal, and any address which
+	 * is a user address, we use get_user() to verify validity.
+	 */
+
+	if (addr < PAGE_OFFSET) {
+		/*
+		 * Usermode address.
+		 */
+		unsigned long diag;
+
+		switch (size) {
+		case 4:
+		{	unsigned long *lp;
+
+			lp = (unsigned long *) addr;
+			diag = put_user(contents, lp);
+			break;
+		}
+		case 2:
+		{	unsigned short *sp;
+
+			sp = (unsigned short *) addr;
+			diag = put_user(contents, sp);
+			break;
+		}
+		case 1:
+		{	unsigned char *cp;
+
+			cp = (unsigned char *) addr;
+			diag = put_user(contents, cp);
+			break;
+		}
+		default:
+			kdb_printf("kdba_putword: Bad width\n");
+			return 0;
+		}
+
+		if (diag) {
+			if (!KDB_STATE(SUPPRESS)) {
+				kdb_printf("kdb: Bad user address 0x%lx\n", addr);
+				KDB_STATE_SET(SUPPRESS);
+			}
+			return 0;
+		}
+		KDB_STATE_CLEAR(SUPPRESS);
+		return 0;
+	}
+
+#if 0
+	if (addr > (unsigned long)high_memory) {
+		if (!kdb_vmlist_check(addr, addr+size)) {
+			/*
+			 * Would appear to be an illegal kernel address;
+			 * Print a message once, and don't print again until
+			 * a legal address is used.
+			 */
+			if (!KDB_STATE(SUPPRESS)) {
+				kdb_printf("kdb: xx Bad kernel address 0x%lx\n", addr);
+				KDB_STATE_SET(SUPPRESS);
+			}
+			return 0L;
+		}
+	}
+#endif
+
+	/*
+	 * A good address.  Reset error flag.
+	 */
+	KDB_STATE_CLEAR(SUPPRESS);
+
+	switch (size) {
+	case 4:
+	{	unsigned long *lp;
+
+		lp = (unsigned long *)(addr);
+		*lp = contents;
+		return 0;
+	}
+	case 2:
+	{	unsigned short *sp;
+
+		sp = (unsigned short *)(addr);
+		*sp = (unsigned short) contents;
+		return 0;
+	}
+	case 1:
+	{	unsigned char *cp;
+
+		cp = (unsigned char *)(addr);
+		*cp = (unsigned char) contents;
+		return 0;
+	}
+	}
+
+	kdb_printf("kdba_putword: Bad width 0x%lx\n",size);
+	return 0;
+}
+
+/*
+ * kdba_callback_die
+ *
+ *	Callback function for kernel 'die' function.
+ *
+ * Parameters:
+ *	regs	Register contents at time of trap
+ *	error_code  Trap-specific error code value
+ *	trapno	Trap number
+ *	vp	Pointer to die message
+ * Returns:
+ *	Returns 1 if fault handled by kdb.
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ */
+int
+kdba_callback_die(struct pt_regs *regs, int error_code, long trapno, void *vp)
+{
+	/*
+	 * Save a pointer to the message provided to 'die()'.
+	 */
+	kdb_diemsg = (char *)vp;
+
+	return kdb(KDB_REASON_OOPS, error_code, (kdb_eframe_t) regs);
+}
+
+/*
+ * kdba_callback_bp
+ *
+ *	Callback function for kernel breakpoint trap.
+ *
+ * Parameters:
+ *	regs	Register contents at time of trap
+ *	error_code  Trap-specific error code value
+ *	trapno	Trap number
+ *	vp	Not Used.
+ * Returns:
+ *	Returns 1 if fault handled by kdb.
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ */
+
+int
+kdba_callback_bp(struct pt_regs *regs, int error_code, long trapno, void *vp)
+{
+	int diag;
+
+	if (KDB_DEBUG(BP))
+		kdb_printf("cb_bp: e_c = %d  tn = %ld regs = 0x%p\n", error_code,
+			   trapno, regs);
+
+	diag = kdb(KDB_REASON_BREAK, error_code, (kdb_eframe_t) regs);
+
+	if (KDB_DEBUG(BP))
+		kdb_printf("cb_bp: e_c = %d  tn = %ld regs = 0x%p diag = %d\n", error_code,
+			   trapno, regs, diag);
+	return diag;
+}
+
+/*
+ * kdba_callback_debug
+ *
+ *	Callback function for kernel debug register trap.
+ *
+ * Parameters:
+ *	regs	Register contents at time of trap
+ *	error_code  Trap-specific error code value
+ *	trapno	Trap number
+ *	vp	Not used.
+ * Returns:
+ *	Returns 1 if fault handled by kdb.
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ */
+
+int
+kdba_callback_debug(struct pt_regs *regs, int error_code, long trapno, void *vp)
+{
+	return kdb(KDB_REASON_DEBUG, error_code, (kdb_eframe_t) regs);
+}
+
+
+
+
+/*
+ * kdba_adjust_ip
+ *
+ * 	Architecture specific adjustment of instruction pointer before leaving
+ *	kdb.
+ *
+ * Parameters:
+ *	reason		The reason KDB was invoked
+ *	error		The hardware-defined error code
+ *	ef		The exception frame at time of fault/breakpoint.  If reason
+ *			is KDB_REASON_SILENT then ef is NULL, otherwise it should
+ *			always be valid.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	noop on ix86.
+ */
+
+void
+kdba_adjust_ip(kdb_reason_t reason, int error, kdb_eframe_t ef)
+{
+	return;
+}
+
+
+
+/*
+ * kdba_find_tb_table
+ *
+ * 	Find the traceback table (defined by the ELF64 ABI) located at
+ *	the end of the function containing pc.
+ *
+ * Parameters:
+ *	nip	starting instruction addr.  does not need to be at the start of the func.
+ *	tab	table to populate if successful
+ * Returns:
+ *	non-zero if successful.  unsuccessful means that a valid tb table was not found
+ * Locking:
+ *	None.
+ * Remarks:
+ *	None.
+ */
+int kdba_find_tb_table(kdb_machreg_t nip, kdbtbtable_t *tab)
+{
+	kdb_machreg_t codeaddr = nip;
+	kdb_machreg_t codeaddr_max;
+	kdb_machreg_t tbtab_start;
+	int instr;
+	int num_parms;
+
+	if (tab == NULL)
+		return 0;
+	memset(tab, 0, sizeof(tab));
+
+	if (nip < PAGE_OFFSET) {  /* this is gonna fail for userspace, at least for now.. */
+	    return 0;
+	}
+
+	/* Scan instructions starting at codeaddr for 128k max */
+	for (codeaddr_max = codeaddr + 128*1024*4;
+	     codeaddr < codeaddr_max;
+	     codeaddr += 4) {
+		instr = kdba_getword(codeaddr, 4);
+		if (instr == 0) {
+			/* table should follow. */
+			int version;
+			unsigned long flags;
+			tbtab_start = codeaddr;	/* save it to compute func start addr */
+			codeaddr += 4;
+			flags = kdba_getword(codeaddr, 8);
+			tab->flags = flags;
+			version = (flags >> 56) & 0xff;
+			if (version != 0)
+				continue;	/* No tb table here. */
+			/* Now, like the version, some of the flags are values
+			 that are more conveniently extracted... */
+			tab->fp_saved = (flags >> 24) & 0x3f;
+			tab->gpr_saved = (flags >> 16) & 0x3f;
+			tab->fixedparms = (flags >> 8) & 0xff;
+			tab->floatparms = (flags >> 1) & 0x7f;
+			codeaddr += 8;
+			num_parms = tab->fixedparms + tab->floatparms;
+			if (num_parms) {
+				unsigned int parminfo;
+				int parm;
+				if (num_parms > 32)
+					return 1;	/* incomplete */
+				parminfo = kdba_getword(codeaddr, 4);
+				/* decode parminfo...32 bits.
+				 A zero means fixed.  A one means float and the
+				 following bit determines single (0) or double (1).
+				 */
+				for (parm = 0; parm < num_parms; parm++) {
+					if (parminfo & 0x80000000) {
+						parminfo <<= 1;
+						if (parminfo & 0x80000000)
+							tab->parminfo[parm] = KDBTBTAB_PARMDFLOAT;
+						else
+							tab->parminfo[parm] = KDBTBTAB_PARMSFLOAT;
+					} else {
+						tab->parminfo[parm] = KDBTBTAB_PARMFIXED;
+					}
+					parminfo <<= 1;
+				}
+				codeaddr += 4;
+			}
+			if (flags & KDBTBTAB_FLAGSHASTBOFF) {
+				tab->tb_offset = kdba_getword(codeaddr, 4);
+				if (tab->tb_offset > 0) {
+					tab->funcstart = tbtab_start - tab->tb_offset;
+				}
+				codeaddr += 4;
+			}
+			/* hand_mask appears to be always be omitted. */
+			if (flags & KDBTBTAB_FLAGSHASCTL) {
+				/* Assume this will never happen for C or asm */
+				return 1;	/* incomplete */
+			}
+			if (flags & KDBTBTAB_FLAGSNAMEPRESENT) {
+				int i;
+				short namlen = kdba_getword(codeaddr, 2);
+				if (namlen >= sizeof(tab->name))
+					namlen = sizeof(tab->name)-1;
+				codeaddr += 2;
+				for (i = 0; i < namlen; i++) {
+					tab->name[i] = kdba_getword(codeaddr++, 1);
+				}
+				tab->name[namlen] = '\0';
+			}
+			/* Fake up a symtab entry in case the caller finds it useful */
+			tab->symtab.value = tab->symtab.sym_start = tab->funcstart;
+			tab->symtab.sym_name = tab->name;
+			tab->symtab.sym_end = tbtab_start;
+			return 1;
+		}
+	}
+	return 0;	/* hit max...sorry. */
+}
+
+int
+kdba_putarea_size(unsigned long to_xxx, void *from, size_t size)
+{
+    char c;
+    c = *((volatile char *)from);
+    c = *((volatile char *)from+size-1);
+    return __copy_to_user((void *)to_xxx,from,size);
+}
+
+
+
+/*
+ * valid_ppc64_kernel_address() returns '1' if the address passed in is
+ * within a valid range.  Function returns 0 if address is outside valid ranges.
+ */
+
+/*
+
+    KERNELBASE    c000000000000000
+        (good range)
+    high_memory   c0000000 20000000
+
+    VMALLOC_START d000000000000000
+        (good range)
+    VMALLOC_END   VMALLOC_START + VALID_EA_BITS  
+
+    IMALLOC_START e000000000000000
+        (good range)
+    IMALLOC_END   IMALLOC_START + VALID_EA_BITS
+
+*/
+
+int
+valid_ppc64_kernel_address(unsigned long addr, unsigned long size)
+{
+	unsigned long i;
+	unsigned long end = (addr + size - 1);	
+
+	int userspace_enabled=0;
+
+/* set USERSPACE=1 to enable userspace memory lookups*/
+	kdbgetintenv("USERSPACE", &userspace_enabled);	
+
+	for (i = addr; i <= end; i = i ++ ) {
+	    if (
+		(!userspace_enabled &&
+		 ((unsigned long)i < (unsigned long)KERNELBASE     ))  || 		
+		(((unsigned long)i > (unsigned long)high_memory) &&
+		 ((unsigned long)i < (unsigned long)VMALLOC_START) )  ||
+		(((unsigned long)i > (unsigned long)VMALLOC_END) &&
+		 ((unsigned long)i < (unsigned long)IMALLOC_START) )  ||
+		( (unsigned long)i > (unsigned long)IMALLOC_END    )       ) {
+		return 0;
+	    }
+	}
+	return 1;
+}
+
+
+int
+kdba_getarea_size(void *to, unsigned long from_xxx, size_t size)
+{
+	int is_valid_kern_addr = valid_ppc64_kernel_address(from_xxx, size);
+	int diag = 0;
+
+	*((volatile char *)to) = '\0';
+	*((volatile char *)to + size - 1) = '\0';
+
+	if (is_valid_kern_addr) {
+		memcpy(to, (void *)from_xxx, size);
+	} else {
+            /*  user space address, just return.  */
+	    diag = -1;
+	}
+
+	return diag;
+}
+
+
+
+/*
+ *  kdba_readarea_size, reads size-lump of memory into to* passed in, returns size.
+ * Making it feel a bit more like mread.. when i'm clearer on kdba end, probally will
+ * remove one of these.
+ */
+int
+kdba_readarea_size(unsigned long from_xxx,void *to, size_t size)
+{
+    int is_valid_kern_addr = valid_ppc64_kernel_address(from_xxx, size);
+
+    *((volatile char *)to) = '\0';
+    *((volatile char *)to + size - 1) = '\0';
+
+    if (is_valid_kern_addr) {
+	memcpy(to, (void *)from_xxx, size);
+	return size;
+    } else {
+	/*  user-space, just return...    */
+	return 0;
+    }
+    /* wont get here */
+    return 0;
+}
+
+
+/* utilities migrated from Xmon or other kernel debug tools. */
+
+/*
+Notes for migrating functions from xmon...
+Add functions to this file.  parmlist for functions must match
+   (int argc, const char **argv, const char **envp, struct pt_regs *fp)
+add function prototype to kdbasupport.c
+add function hook to kdba_init() within kdbasupport.c
+
+Common bits...
+mread() function calls need to be changed to kdba_readarea_size calls.  straightforward change.
+This:
+	nr = mread(codeaddr, &namlen, 2); 
+becomes this:
+	nr = kdba_readarea_size(codeaddr,&namlen,2);
+*/
+
+#define EOF	(-1)
+
+/* for traverse_all_pci_devices */
+#include "../kernel/pci.h"
+/* for NUM_TCE_LEVELS */
+#include <asm/pci_dma.h>
+
+
+/* prototypes */
+int scanhex(unsigned long *);
+int hexdigit(int c);
+/* int kdba_readarea_size(unsigned long from_xxx,void *to,  size_t size); */
+void machine_halt(void); 
+
+
+
+/*
+ A traceback table typically follows each function.
+ The find_tb_table() func will fill in this struct.  Note that the struct
+ is not an exact match with the encoded table defined by the ABI.  It is
+ defined here more for programming convenience.
+ */
+struct tbtable {
+	unsigned long	flags;		/* flags: */
+#define TBTAB_FLAGSGLOBALLINK	(1L<<47)
+#define TBTAB_FLAGSISEPROL	(1L<<46)
+#define TBTAB_FLAGSHASTBOFF	(1L<<45)
+#define TBTAB_FLAGSINTPROC	(1L<<44)
+#define TBTAB_FLAGSHASCTL	(1L<<43)
+#define TBTAB_FLAGSTOCLESS	(1L<<42)
+#define TBTAB_FLAGSFPPRESENT	(1L<<41)
+#define TBTAB_FLAGSNAMEPRESENT	(1L<<38)
+#define TBTAB_FLAGSUSESALLOCA	(1L<<37)
+#define TBTAB_FLAGSSAVESCR	(1L<<33)
+#define TBTAB_FLAGSSAVESLR	(1L<<32)
+#define TBTAB_FLAGSSTORESBC	(1L<<31)
+#define TBTAB_FLAGSFIXUP	(1L<<30)
+#define TBTAB_FLAGSPARMSONSTK	(1L<<0)
+	unsigned char	fp_saved;	/* num fp regs saved f(32-n)..f31 */
+	unsigned char	gpr_saved;	/* num gpr's saved */
+	unsigned char	fixedparms;	/* num fixed point parms */
+	unsigned char	floatparms;	/* num float parms */
+	unsigned char	parminfo[32];	/* types of args.  null terminated */
+#define TBTAB_PARMFIXED 1
+#define TBTAB_PARMSFLOAT 2
+#define TBTAB_PARMDFLOAT 3
+	unsigned int	tb_offset;	/* offset from start of func */
+	unsigned long	funcstart;	/* addr of start of function */
+	char		name[64];	/* name of function (null terminated)*/
+};
+
+
+static int find_tb_table(unsigned long codeaddr, struct tbtable *tab);
+
+
+/* Very cheap human name for vector lookup. */
+static
+const char *getvecname(unsigned long vec)
+{
+	char *ret;
+	switch (vec) {
+	case 0x100:	ret = "(System Reset)"; break; 
+	case 0x200:	ret = "(Machine Check)"; break; 
+	case 0x300:	ret = "(Data Access)"; break; 
+	case 0x400:	ret = "(Instruction Access)"; break; 
+	case 0x500:	ret = "(Hardware Interrupt)"; break; 
+	case 0x600:	ret = "(Alignment)"; break; 
+	case 0x700:	ret = "(Program Check)"; break; 
+	case 0x800:	ret = "(FPU Unavailable)"; break; 
+	case 0x900:	ret = "(Decrementer)"; break; 
+	case 0xc00:	ret = "(System Call)"; break; 
+	case 0xd00:	ret = "(Single Step)"; break; 
+	case 0xf00:	ret = "(Performance Monitor)"; break; 
+	default: ret = "";
+	}
+	return ret;
+}
+
+int
+kdba_halt(int argc, const char **argv, const char **envp, struct pt_regs *fp)
+{
+    kdb_printf("halting machine. ");
+    machine_halt();
+return 0;
+}
+
+
+int
+kdba_excprint(int argc, const char **argv, const char **envp, struct pt_regs *fp)
+{
+	struct task_struct *c;
+	struct tbtable tab;
+
+#ifdef CONFIG_SMP
+	kdb_printf("cpu %d: ", smp_processor_id());
+#endif /* CONFIG_SMP */
+
+	kdb_printf("Vector: %lx %s at  [%p]\n", fp->trap, getvecname(fp->trap), fp);
+	kdb_printf("    pc: %lx", fp->nip);
+	if (find_tb_table(fp->nip, &tab) && tab.name[0]) {
+		/* Got a nice name for it */
+		int delta = fp->nip - tab.funcstart;
+		kdb_printf(" (%s+0x%x)", tab.name, delta);
+	}
+	kdb_printf("\n");
+	kdb_printf("    lr: %lx", fp->link);
+	if (find_tb_table(fp->link, &tab) && tab.name[0]) {
+		/* Got a nice name for it */
+		int delta = fp->link - tab.funcstart;
+		kdb_printf(" (%s+0x%x)", tab.name, delta);
+	}
+	kdb_printf("\n");
+	kdb_printf("    sp: %lx\n", fp->gpr[1]);
+	kdb_printf("   msr: %lx\n", fp->msr);
+
+	if (fp->trap == 0x300 || fp->trap == 0x380 || fp->trap == 0x600) {
+		kdb_printf("   dar: %lx\n", fp->dar);
+		kdb_printf(" dsisr: %lx\n", fp->dsisr);
+	}
+
+	/* XXX: need to copy current or we die.  Why? */
+	c = current;
+	kdb_printf("  current = 0x%p\n", c);
+	kdb_printf("  paca    = 0x%p\n", get_paca());
+	if (c) {
+		kdb_printf("  current = %p, pid = %ld, comm = %s\n",
+		       c, (unsigned long)c->pid, (char *)c->comm);
+	}
+return 0;
+}
+
+
+/* Starting at codeaddr scan forward for a tbtable and fill in the
+ given table.  Return non-zero if successful at doing something.
+ */
+static int
+find_tb_table(unsigned long codeaddr, struct tbtable *tab)
+{
+	unsigned long codeaddr_max;
+	unsigned long tbtab_start;
+	int nr;
+	int instr;
+	int num_parms;
+
+	if (tab == NULL)
+		return 0;
+	memset(tab, 0, sizeof(tab));
+
+	/* Scan instructions starting at codeaddr for 128k max */
+	for (codeaddr_max = codeaddr + 128*1024*4;
+	     codeaddr < codeaddr_max;
+	     codeaddr += 4) {
+	    nr=kdba_readarea_size(codeaddr,&instr,4);
+		if (nr != 4)
+			return 0;	/* Bad read.  Give up promptly. */
+		if (instr == 0) {
+			/* table should follow. */
+			int version;
+			unsigned long flags;
+			tbtab_start = codeaddr;	/* save it to compute func start addr */
+			codeaddr += 4;
+			nr = kdba_readarea_size(codeaddr,&flags,8);
+			if (nr != 8)
+				return 0;	/* Bad read or no tb table. */
+			tab->flags = flags;
+			version = (flags >> 56) & 0xff;
+			if (version != 0)
+				continue;	/* No tb table here. */
+			/* Now, like the version, some of the flags are values
+			 that are more conveniently extracted... */
+			tab->fp_saved = (flags >> 24) & 0x3f;
+			tab->gpr_saved = (flags >> 16) & 0x3f;
+			tab->fixedparms = (flags >> 8) & 0xff;
+			tab->floatparms = (flags >> 1) & 0x7f;
+			codeaddr += 8;
+			num_parms = tab->fixedparms + tab->floatparms;
+			if (num_parms) {
+				unsigned int parminfo;
+				int parm;
+				if (num_parms > 32)
+					return 1;	/* incomplete */
+				nr = kdba_readarea_size(codeaddr,&parminfo,4);
+				if (nr != 4)
+					return 1;	/* incomplete */
+				/* decode parminfo...32 bits.
+				 A zero means fixed.  A one means float and the
+				 following bit determines single (0) or double (1).
+				 */
+				for (parm = 0; parm < num_parms; parm++) {
+					if (parminfo & 0x80000000) {
+						parminfo <<= 1;
+						if (parminfo & 0x80000000)
+							tab->parminfo[parm] = TBTAB_PARMDFLOAT;
+						else
+							tab->parminfo[parm] = TBTAB_PARMSFLOAT;
+					} else {
+						tab->parminfo[parm] = TBTAB_PARMFIXED;
+					}
+					parminfo <<= 1;
+				}
+				codeaddr += 4;
+			}
+			if (flags & TBTAB_FLAGSHASTBOFF) {
+			    nr = kdba_readarea_size(codeaddr,&tab->tb_offset,4);
+				if (nr != 4)
+					return 1;	/* incomplete */
+				if (tab->tb_offset > 0) {
+					tab->funcstart = tbtab_start - tab->tb_offset;
+				}
+				codeaddr += 4;
+			}
+			/* hand_mask appears to be always be omitted. */
+			if (flags & TBTAB_FLAGSHASCTL) {
+				/* Assume this will never happen for C or asm */
+				return 1;	/* incomplete */
+			}
+			if (flags & TBTAB_FLAGSNAMEPRESENT) {
+				short namlen;
+				nr = kdba_readarea_size(codeaddr,&namlen,2);
+				if (nr != 2)
+					return 1;	/* incomplete */
+				if (namlen >= sizeof(tab->name))
+					namlen = sizeof(tab->name)-1;
+				codeaddr += 2;
+				nr = kdba_readarea_size(codeaddr,tab->name,namlen);
+				tab->name[namlen] = '\0';
+				codeaddr += namlen;
+			}
+			return 1;
+		}
+	}
+	return 0;	/* hit max...sorry. */
+}
+
+
+int
+kdba_dissect_msr(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+   long int msr;
+
+   if (argc==0)
+       msr = regs->msr;
+/*       msr = get_msr(); */
+    else 
+	kdbgetularg(argv[1], &msr);
+
+   kdb_printf("msr: %lx (",msr);
+   {
+       if (msr & MSR_SF)   kdb_printf("SF ");
+       if (msr & MSR_ISF)  kdb_printf("ISF ");
+       if (msr & MSR_HV)   kdb_printf("HV ");
+       if (msr & MSR_VEC)  kdb_printf("VEC ");
+       if (msr & MSR_POW)  kdb_printf("POW/");  /* pow/we share */
+       if (msr & MSR_WE)   kdb_printf("WE ");
+       if (msr & MSR_TGPR) kdb_printf("TGPR/"); /* tgpr/ce share */
+       if (msr & MSR_CE)   kdb_printf("CE ");
+       if (msr & MSR_ILE)  kdb_printf("ILE ");
+       if (msr & MSR_EE)   kdb_printf("EE ");
+       if (msr & MSR_PR)   kdb_printf("PR ");
+       if (msr & MSR_FP)   kdb_printf("FP ");
+       if (msr & MSR_ME)   kdb_printf("ME ");
+       if (msr & MSR_FE0)  kdb_printf("FE0 ");
+       if (msr & MSR_SE)   kdb_printf("SE ");
+       if (msr & MSR_BE)   kdb_printf("BE/");   /* be/de share */
+       if (msr & MSR_DE)   kdb_printf("DE ");
+       if (msr & MSR_FE1)  kdb_printf("FE1 ");
+       if (msr & MSR_IP)   kdb_printf("IP ");
+       if (msr & MSR_IR)   kdb_printf("IR ");
+       if (msr & MSR_DR)   kdb_printf("DR ");
+       if (msr & MSR_PE)   kdb_printf("PE ");
+       if (msr & MSR_PX)   kdb_printf("PX ");
+       if (msr & MSR_RI)   kdb_printf("RI ");
+       if (msr & MSR_LE)   kdb_printf("LE ");
+   }
+   kdb_printf(")\n");
+
+   if (msr & MSR_SF)   kdb_printf(" 64 bit mode enabled \n");
+   if (msr & MSR_ISF)  kdb_printf(" Interrupt 64b mode valid on 630 \n");
+   if (msr & MSR_HV)   kdb_printf(" Hypervisor State \n");
+   if (msr & MSR_VEC)  kdb_printf(" Enable Altivec \n");
+   if (msr & MSR_POW)  kdb_printf(" Enable Power Management  \n");
+   if (msr & MSR_WE)   kdb_printf(" Wait State Enable   \n");
+   if (msr & MSR_TGPR) kdb_printf(" TLB Update registers in use   \n");
+   if (msr & MSR_CE)   kdb_printf(" Critical Interrupt Enable   \n");
+   if (msr & MSR_ILE)  kdb_printf(" Interrupt Little Endian   \n");
+   if (msr & MSR_EE)   kdb_printf(" External Interrupt Enable   \n");
+   if (msr & MSR_PR)   kdb_printf(" Problem State / Privilege Level  \n"); 
+   if (msr & MSR_FP)   kdb_printf(" Floating Point enable   \n");
+   if (msr & MSR_ME)   kdb_printf(" Machine Check Enable   \n");
+   if (msr & MSR_FE0)  kdb_printf(" Floating Exception mode 0  \n"); 
+   if (msr & MSR_SE)   kdb_printf(" Single Step   \n");
+   if (msr & MSR_BE)   kdb_printf(" Branch Trace   \n");
+   if (msr & MSR_DE)   kdb_printf(" Debug Exception Enable   \n");
+   if (msr & MSR_FE1)  kdb_printf(" Floating Exception mode 1   \n");
+   if (msr & MSR_IP)   kdb_printf(" Exception prefix 0x000/0xFFF   \n");
+   if (msr & MSR_IR)   kdb_printf(" Instruction Relocate   \n");
+   if (msr & MSR_DR)   kdb_printf(" Data Relocate   \n");
+   if (msr & MSR_PE)   kdb_printf(" Protection Enable   \n");
+   if (msr & MSR_PX)   kdb_printf(" Protection Exclusive Mode   \n");
+   if (msr & MSR_RI)   kdb_printf(" Recoverable Exception   \n");
+   if (msr & MSR_LE)   kdb_printf(" Little Endian   \n");
+   kdb_printf(".\n");
+
+return 0;
+}
+
+int
+kdba_super_regs(int argc, const char **argv, const char **envp, struct pt_regs *regs){
+	int i;
+	struct paca_struct*  ptrPaca = NULL;
+	struct ItLpPaca*  ptrLpPaca = NULL;
+	struct ItLpRegSave*  ptrLpRegSave = NULL;
+
+	{
+	        unsigned long sp, toc;
+		kdb_printf("sr::");
+		asm("mr %0,1" : "=r" (sp) :);
+		asm("mr %0,2" : "=r" (toc) :);
+
+		kdb_printf("msr  = %.16lx  sprg0= %.16lx\n", get_msr(), get_sprg0());
+		kdb_printf("pvr  = %.16lx  sprg1= %.16lx\n", get_pvr(), get_sprg1()); 
+		kdb_printf("dec  = %.16lx  sprg2= %.16lx\n", get_dec(), get_sprg2());
+		kdb_printf("sp   = %.16lx  sprg3= %.16lx\n", sp, get_sprg3());
+		kdb_printf("toc  = %.16lx  dar  = %.16lx\n", toc, get_dar());
+		kdb_printf("srr0 = %.16lx  srr1 = %.16lx\n", get_srr0(), get_srr1());
+		kdb_printf("asr  = %.16lx\n", mfasr());
+		for (i = 0; i < 8; ++i)
+			kdb_printf("sr%.2ld = %.16lx  sr%.2ld = %.16lx\n", (long int)i, (unsigned long)get_sr(i), (long int)(i+8), (long unsigned int) get_sr(i+8));
+
+		// Dump out relevant Paca data areas.
+		kdb_printf("Paca: \n");
+		ptrPaca = (struct paca_struct*)get_sprg3();
+    
+		kdb_printf("  Local Processor Control Area (LpPaca): \n");
+		ptrLpPaca = ptrPaca->xLpPacaPtr;
+		kdb_printf("    Saved Srr0=%.16lx  Saved Srr1=%.16lx \n", ptrLpPaca->xSavedSrr0, ptrLpPaca->xSavedSrr1);
+		kdb_printf("    Saved Gpr3=%.16lx  Saved Gpr4=%.16lx \n", ptrLpPaca->xSavedGpr3, ptrLpPaca->xSavedGpr4);
+		kdb_printf("    Saved Gpr5=%.16lx \n", ptrLpPaca->xSavedGpr5);
+    
+		kdb_printf("  Local Processor Register Save Area (LpRegSave): \n");
+		ptrLpRegSave = ptrPaca->xLpRegSavePtr;
+		kdb_printf("    Saved Sprg0=%.16lx  Saved Sprg1=%.16lx \n", ptrLpRegSave->xSPRG0, ptrLpRegSave->xSPRG0);
+		kdb_printf("    Saved Sprg2=%.16lx  Saved Sprg3=%.16lx \n", ptrLpRegSave->xSPRG2, ptrLpRegSave->xSPRG3);
+		kdb_printf("    Saved Msr  =%.16lx  Saved Nia  =%.16lx \n", ptrLpRegSave->xMSR, ptrLpRegSave->xNIA);
+    
+		return 0;
+	} 
+}
+
+
+
+	
+int
+kdba_dump_tce_table(int argc, const char **argv, const char **envp, struct pt_regs *regs){
+    struct TceTable kt; 
+    long tce_table_address;
+    int nr;
+    int i,j,k;
+    int full,empty;
+    int fulldump=0;
+    u64 mapentry;
+    int totalpages;
+    int levelpages;
+
+    if (argc == 0) {
+	kdb_printf("need address\n");
+	return 0;
+    }
+    else 
+	kdbgetularg(argv[1], &tce_table_address);
+
+    if (argc==2)
+	if (strcmp(argv[2], "full") == 0) 
+	    fulldump=1;
+
+    /* with address, read contents of memory and dump tce table. */
+    /* possibly making some assumptions on the depth and size of table..*/
+
+    nr = kdba_readarea_size(tce_table_address+0 ,&kt.busNumber,8);
+    nr = kdba_readarea_size(tce_table_address+8 ,&kt.size,8);
+    nr = kdba_readarea_size(tce_table_address+16,&kt.startOffset,8);
+    nr = kdba_readarea_size(tce_table_address+24,&kt.base,8);
+    nr = kdba_readarea_size(tce_table_address+32,&kt.index,8);
+    nr = kdba_readarea_size(tce_table_address+40,&kt.tceType,8);
+    nr = kdba_readarea_size(tce_table_address+48,&kt.lock,8);
+
+    kdb_printf("\n");
+    kdb_printf("TceTable at address %s:\n",argv[1]);
+    kdb_printf("BusNumber:   0x%x \n",(uint)kt.busNumber);
+    kdb_printf("size:        0x%x \n",(uint)kt.size);
+    kdb_printf("startOffset: 0x%x \n",(uint)kt.startOffset);
+    kdb_printf("base:        0x%x \n",(uint)kt.base);
+    kdb_printf("index:       0x%x \n",(uint)kt.index);
+    kdb_printf("tceType:     0x%x \n",(uint)kt.tceType);
+#ifdef CONFIG_SMP
+    kdb_printf("lock:        0x%x \n",(uint)kt.lock.lock);
+#endif
+
+    nr = kdba_readarea_size(tce_table_address+56,&kt.mlbm.maxLevel,8);
+    kdb_printf(" maxLevel:        0x%x \n",(uint)kt.mlbm.maxLevel);
+    totalpages=0;
+    for (i=0;i<NUM_TCE_LEVELS;i++) {
+	nr = kdba_readarea_size(tce_table_address+64+i*24,&kt.mlbm.level[i].numBits,8);
+	nr = kdba_readarea_size(tce_table_address+72+i*24,&kt.mlbm.level[i].numBytes,8);
+	nr = kdba_readarea_size(tce_table_address+80+i*24,&kt.mlbm.level[i].map,8);
+	kdb_printf("   level[%d]\n",i);
+	kdb_printf("   numBits:   0x%x\n",(uint)kt.mlbm.level[i].numBits);
+	kdb_printf("   numBytes:  0x%x\n",(uint)kt.mlbm.level[i].numBytes);
+	kdb_printf("   map*:      %p\n",kt.mlbm.level[i].map);
+
+	 /* if these dont match, this might not be a valid tce table, so
+	    dont try to iterate the map entries. */
+	if (kt.mlbm.level[i].numBits == 8*kt.mlbm.level[i].numBytes) {
+	    full=0;empty=0;levelpages=0;
+	    for (j=0;j<kt.mlbm.level[i].numBytes; j++) {
+		mapentry=0;
+		nr = kdba_readarea_size((long int)(kt.mlbm.level[i].map+j),&mapentry,1);
+		if (mapentry)
+		    full++;
+		else
+		    empty++;
+		if (mapentry && fulldump) {
+		    kdb_printf("0x%lx\n",mapentry);
+		}
+		for (k=0;(k<=64) && ((0x1UL<<k) <= mapentry);k++) {
+		    if ((0x1UL<<k) & mapentry) levelpages++;
+		}
+	    }
+	    kdb_printf("      full:0x%x empty:0x%x pages:0x%x\n",full,empty,levelpages);
+	} else {
+	    kdb_printf("      numBits/numBytes mismatch..? \n");
+	}
+	totalpages+=levelpages;
+    }
+    kdb_printf("      Total pages:0x%x\n",totalpages);
+    kdb_printf("\n");
+    return 0;
+}
+
+int
+kdba_kernelversion(int argc, const char **argv, const char **envp, struct pt_regs *regs){
+    extern char *linux_banner;
+
+    kdb_printf("%s\n",linux_banner);
+
+    return 0;
+}
+
+
+static void * 
+kdba_dump_pci(struct device_node *dn, void *data)
+{
+    struct pci_controller *phb;
+    char *device_type;
+    char *status;
+
+    phb = (struct pci_controller *)data;
+    device_type = get_property(dn, "device_type", 0);
+    status = get_property(dn, "status", 0);
+
+    dn->phb = phb;
+    kdb_printf("dn:   %p \n",dn);
+    kdb_printf("    phb      : %p\n",dn->phb);
+    kdb_printf("    name     : %s\n",dn->name);
+    kdb_printf("    full_name: %s\n",dn->full_name);
+    kdb_printf("    busno    : 0x%x\n",dn->busno);
+    kdb_printf("    devfn    : 0x%x\n",dn->devfn);
+    kdb_printf("    tce_table: %p\n",dn->tce_table);
+    return NULL;
+}
+
+int
+kdba_dump_pci_info(int argc, const char **argv, const char **envp, struct pt_regs *regs){
+
+    kdb_printf("kdba_dump_pci_info\n");
+
+/* call this traverse function with my function pointer.. it takes care of traversing, my func just needs to parse the device info.. */
+    traverse_all_pci_devices(kdba_dump_pci);
+    return 0;
+}
+
+
+char *kdb_dumpall_cmds[] = {
+    "excp\n",
+    "bt\n",
+    "rd\n",
+    "dmesg\n",
+    "msr\n",
+    "superreg\n",
+    "pci_info\n",
+    "ps\n",
+    "cpu\n",
+    "set BTAPROMPT=none\n",
+    "bta\n",
+    0
+};
+
+char *kdb_dumpbasic_cmds[] = {
+    "excp\n",
+    "bt\n",
+    "rd\n",
+    "dmesg 25\n",
+    "msr\n",
+    "superreg\n",
+    "ps\n",
+    "cpu\n",
+    0
+};
+
+
+/* dump with "all" parm will dump all.  all other variations dump basic.  See the dump*_cmds defined above */
+int
+kdba_dump(int argc, const char **argv, const char **envp, struct pt_regs *fp)
+{
+    int i, diag;
+    kdb_printf("dump-all\n");
+    if ((argc==1)&& (strcmp(argv[1], "all")==0))	{
+	for (i = 0; kdb_dumpall_cmds[i]; ++i) {
+	    kdb_printf("kdb_cmd[%d]%s: %s",
+		       i, " ", kdb_dumpall_cmds[i]);
+	    diag = kdb_parse(kdb_dumpall_cmds[i], fp);
+	    if (diag)
+		kdb_printf("command failed, kdb diag %d\n", diag);
+	}
+    } else {
+	kdb_printf("dump-basic\n");
+	for (i = 0; kdb_dumpbasic_cmds[i]; ++i) {
+	    kdb_printf("kdb_cmd[%d]%s: %s",
+		       i, " ", kdb_dumpbasic_cmds[i]);
+	    diag = kdb_parse(kdb_dumpbasic_cmds[i], fp);
+	    if (diag)
+		kdb_printf("command failed, kdb diag %d\n", diag);
+	}
+    }
+    return 0;
+}
+
+
+/* Toggle the ppcdbg options.   kdb_parse tokenizes the parms, so need to account for that here.  */
+int
+kdba_ppcdbg(int argc, const char **argv, const char **envp, struct pt_regs *fp) {
+    extern char *trace_names[PPCDBG_NUM_FLAGS];
+
+    int i,j;
+    unsigned long mask;
+    int onoff;
+    if (argc==0)
+	goto ppcdbg_exit;
+
+    for (i=1;i<=argc;i++) {
+	onoff = 1;	/* default */
+	if (argv[i][0] == '+' || argv[i][0] == '-') {
+			/* explicit on or off */
+	    onoff = (argv[i][0] == '+');
+	    argv[i]++;
+	}
+
+	for (j=0;j<PPCDBG_NUM_FLAGS;j++) {
+	    if (trace_names[j] && strcmp(trace_names[j],argv[i])==0) {
+		/* have a match */
+		mask = (1 << j);
+		/* check special case */
+		if (strcmp(argv[i],"all")==0) {
+		    mask = PPCDBG_ALL;
+		}
+		if (mask) {
+		    if (onoff)
+			naca->debug_switch |= mask;
+		    else
+			naca->debug_switch &= ~mask;
+		}
+	    } 
+	}
+    }
+    ppcdbg_exit:
+      kdb_printf("naca->debug_switch 0x%lx\n",naca->debug_switch);
+    return 0;
+}
+
+/* enable or disable surveillance.. based on rtasd.c function.
+  no arguments - display current timeout value.
+  one argument - 'off' or '0' turn off surveillance.
+               - '1-255' set surveillance timeout to argument. */
+int
+kdba_surveillance(int argc, const char **argv, const char **envp, struct pt_regs *fp)
+{
+    unsigned long timeout;
+    int ibm_indicator_token = 9000;
+    int error;
+    unsigned long ret;
+
+    if (argc==0) {
+	goto surveillance_status;
+    } else if (((argc==1)&& (strcmp(argv[1], "off")==0))) {
+	timeout=0;
+    } else {
+	kdbgetularg(argv[1], &timeout);
+    }
+
+    error = rtas_call(rtas_token("set-indicator"), 3, 1, &ret,
+		      ibm_indicator_token, 0, timeout);
+    /*    kdb_printf("Surveillance set-indicator returned value: 0x%x\n",ret); */
+
+    if (error) 
+	kdb_printf("surveillance rtas_call failure 0x%x \n",error);
+
+    surveillance_status:
+      rtas_call(rtas_token("get-sensor-state"), 2, 2, &ret, 
+		ibm_indicator_token, 
+		0/* instance */);
+    kdb_printf("Current surveillance timeout is %ld minutes%s",ret,
+	       ret==0?" (disabled).\n":".\n");
+    return 0;
+}
+
+/* generic debugger() hooks into kdb.  These eliminate the need to add
+  ifdef CONFIG_KDB goop to traps.c and fault.c */
+
+void
+kdb_reset_debugger(struct pt_regs *regs) {
+    int cpu=smp_processor_id();
+    static int reset_cpu = -1;
+    static spinlock_t reset_lock = SPIN_LOCK_UNLOCKED;
+    spin_lock(&reset_lock);
+    if (reset_cpu == -1 || reset_cpu == cpu) {
+	reset_cpu = cpu;
+	spin_unlock(&reset_lock);
+	if (kdb_on) {
+	    ppc64_attention_msg(0x3200+cpu,"KDB Call        ");
+	    kdb(KDB_REASON_ENTER, regs->trap, (kdb_eframe_t) regs);
+	    ppc64_attention_msg(0x3300+cpu,"KDB Done        ");
+	} else {
+	    kdb_on=1;
+	    kdb_do_reboot=1;
+	    ppc64_attention_msg(0x3600+cpu,"KDB Enabled     ");
+	    udelay(KDB_RESET_TIMEOUT);
+	    kdb_on=0;
+	    if (kdb_do_reboot) {
+		ppc64_attention_msg(0x3900+cpu,"Rebooting       ");
+		ppc_md.restart("rebooting...");
+		return;	/* not reached */
+	    } else {
+		ppc64_attention_msg(0x3800+cpu,"KDB skip reboot ");
+		return;
+	    }
+	}
+    } else {
+	spin_unlock(&reset_lock);
+	return;
+    }
+}
+
+void
+kdb_debugger(struct pt_regs *regs) {
+    if (regs)
+	if (regs->trap==0x100) {
+	    kdb_reset_debugger(regs);
+	} else
+	    kdb(KDB_REASON_ENTER,regs->trap,regs);   /* ok */
+    else  /* regs invalid */
+	kdb(KDB_REASON_SILENT,0,regs);
+}
+
+int
+kdb_debugger_bpt(struct pt_regs *regs) {
+    if (regs)
+	return kdb(KDB_REASON_BREAK,regs->trap,regs);
+    else  /* regs invalid */
+	return kdb(KDB_REASON_SILENT,0,regs);
+}
+
+int
+kdb_debugger_sstep(struct pt_regs *regs) {
+    if (regs)
+	return kdb(KDB_REASON_DEBUG,regs->trap,regs); /* ok */
+    else  /* regs invalid */
+	return kdb(KDB_REASON_SILENT,0,regs);
+}
+
+int
+kdb_debugger_iabr_match(struct pt_regs *regs) {
+    if (regs)
+	return kdb(KDB_REASON_BREAK,regs->trap,regs);
+    else  /* regs invalid */
+	return kdb(KDB_REASON_SILENT,0,regs);
+}
+
+int
+kdb_debugger_dabr_match(struct pt_regs *regs) {
+    if (regs)
+	return kdb(KDB_REASON_BREAK,regs->trap,regs);
+    else  /* regs invalid */
+	return kdb(KDB_REASON_SILENT,0,regs);
+}
+
+void
+kdb_debugger_fault_handler(struct pt_regs *regs) {
+    if (regs)
+	kdb(KDB_REASON_FAULT,regs->trap,regs);
+    else  /* regs invalid */
+	kdb(KDB_REASON_SILENT,0,regs);
+    return;
+}
+
+
+
+int
+kdba_state(int argc, const char **argv, const char **envp, struct pt_regs *fp)
+{
+    int i;
+    for (i=0;i<NR_CPUS;i++) {
+	if ( kdb_state[i] != 0 ) {
+	    kdb_printf("kdb_state[%d] = %x" ,i,kdb_state[i]);
+	    kdb_printf(" [");
+	    if KDB_STATE_CPU(KDB,i) kdb_printf("KDB,");
+	    if KDB_STATE_CPU(LEAVING,i) kdb_printf("LEAVING,");
+	    if KDB_STATE_CPU(CMD,i) kdb_printf("CMD,");
+	    if KDB_STATE_CPU(KDB_CONTROL,i) kdb_printf("KDB_CONTROL,");
+	    if KDB_STATE_CPU(HOLD_CPU,i) kdb_printf("HOLD_CPU,");
+	    if KDB_STATE_CPU(DOING_SS,i) kdb_printf("DOING_SS,");
+	    if KDB_STATE_CPU(DOING_SSB,i) kdb_printf("DOING_SSB,");
+	    if KDB_STATE_CPU(SSBPT,i) kdb_printf("SSBPT,");
+	    if KDB_STATE_CPU(REENTRY,i) kdb_printf("REENTRY,");
+	    if KDB_STATE_CPU(SUPPRESS,i) kdb_printf("SUPPRESS,");
+	    if KDB_STATE_CPU(LONGJMP,i) kdb_printf("LONGJMP,");
+	    if KDB_STATE_CPU(PRINTF_LOCK,i) kdb_printf("PRINTF_LOCK,");
+	    if KDB_STATE_CPU(WAIT_IPI,i) kdb_printf("WAIT_IPI,");
+	    if KDB_STATE_CPU(RECURSE,i) kdb_printf("RECURSE,");
+	    if KDB_STATE_CPU(IP_ADJUSTED,i) kdb_printf("IP_ADJUSTED,");
+	    if KDB_STATE_CPU(NO_BP_DELAY,i) kdb_printf("NO_BP_DELAY");
+	    kdb_printf("]\n");
+	}
+    }
+return 0;
+}
+
+
+/*
+ * kdba_init
+ * 	Architecture specific initialization.
+ */
+/*
+kdb_register("commandname",              # name of command user will use to invoke function  
+             function_name,              # name of function within the code 
+             "function example usage",   # sample usage 
+             "function description",     # brief description. 
+             0                           # if i hit enter again, will command repeat itself ?
+Note: functions must take parameters as such:
+functionname(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+*/
+
+void __init
+kdba_init(void)
+{
+#ifdef CONFIG_MAGIC_SYSRQ
+	kdb_map_scc();		/* map sysrq key */
+#endif
+
+	debugger = kdb_debugger;
+	debugger_bpt = kdb_debugger_bpt;
+	debugger_sstep = kdb_debugger_sstep;
+	debugger_iabr_match = kdb_debugger_iabr_match;
+	debugger_dabr_match = kdb_debugger_dabr_match;
+	debugger_fault_handler = NULL; /* this guy is normally off. */
+				    /* = kdb_debugger_fault_handler; */
+
+	kdba_enable_lbr();
+	kdb_register("excp", kdba_excprint, "excp", "print exception info", 0);
+	kdb_register("superreg", kdba_super_regs, "superreg", "display super_regs", 0);
+	kdb_register("msr", kdba_dissect_msr, "msr", "dissect msr", 0);
+	kdb_register("halt", kdba_halt, "halt", "halt machine", 0);
+	kdb_register("tce_table", kdba_dump_tce_table, "tce_table <addr> [full]", "dump the tce table located at <addr>", 0);
+	kdb_register("kernel", kdba_kernelversion, "version", "display running kernel version", 0);
+	kdb_register("pci_info", kdba_dump_pci_info, "dump_pci_info", "dump pci device info", 0);
+	kdb_register("dump", kdba_dump, "dump (all|basic)", "dump all info", 0); 
+	kdb_register("state", kdba_state, "state ", "dump state of all processors", 0); 
+	kdb_register("surv", kdba_surveillance, "surv [off|1-255] ", "disable/change surveillance timeout", 0); 
+	kdb_register("ppcdbg", kdba_ppcdbg, "ppcdbg (a,+b,-c)","toggle PPCDBG options",0);
+	if (!ppc_md.udbg_getc_poll)
+		kdb_on = 0;
+}
diff -purN linux-2.5/arch/ppc64/kdb/opintl.h linuxppc64-2.5/arch/ppc64/kdb/opintl.h
--- linux-2.5/arch/ppc64/kdb/opintl.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/opintl.h	2003-10-13 16:16:57.000000000 +0000
@@ -0,0 +1,42 @@
+/* opintl.h - opcodes specific header for gettext code.
+   Copyright (C) 1998, 1999 Free Software Foundation, Inc.
+
+   Written by Tom Tromey <tromey@cygnus.com>
+
+   This file is part of the opcodes library used by GAS and the GNU binutils.
+
+   You should have received a copy of the GNU General Public License
+   along with GAS; see the file COPYING.  If not, write to the Free
+   Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+   02111-1307, USA. */
+
+#ifdef ENABLE_NLS
+# include <libintl.h>
+/* Note the use of dgetext() and PACKAGE here, rather than gettext().
+   
+   This is because the code in this directory is used to build a library which
+   will be linked with code in other directories to form programs.  We want to
+   maintain a seperate translation file for this directory however, rather
+   than being forced to merge it with that of any program linked to
+   libopcodes.  This is a library, so it cannot depend on the catalog
+   currently loaded.
+
+   In order to do this, we have to make sure that when we extract messages we
+   use the OPCODES domain rather than the domain of the program that included
+   the opcodes library, (eg OBJDUMP).  Hence we use dgettext (PACKAGE, String)
+   and define PACKAGE to be 'opcodes'.  (See the code in configure).  */
+# define _(String) dgettext (PACKAGE, String)
+# ifdef gettext_noop
+#  define N_(String) gettext_noop (String)
+# else
+#  define N_(String) (String)
+# endif
+#else
+# define gettext(Msgid) (Msgid)
+# define dgettext(Domainname, Msgid) (Msgid)
+# define dcgettext(Domainname, Msgid, Category) (Msgid)
+# define textdomain(Domainname) while (0) /* nothing */
+# define bindtextdomain(Domainname, Dirname) while (0) /* nothing */
+# define _(String) (String)
+# define N_(String) (String)
+#endif
diff -purN linux-2.5/arch/ppc64/kdb/ppc-dis.c linuxppc64-2.5/arch/ppc64/kdb/ppc-dis.c
--- linux-2.5/arch/ppc64/kdb/ppc-dis.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/ppc-dis.c	2003-10-13 16:16:57.000000000 +0000
@@ -0,0 +1,281 @@
+/* ppc-dis.c -- Disassemble PowerPC instructions
+   Copyright 1994 Free Software Foundation, Inc.
+   Written by Ian Lance Taylor, Cygnus Support
+
+This file is part of GDB, GAS, and the GNU binutils.
+
+GDB, GAS, and the GNU binutils are free software; you can redistribute
+them and/or modify them under the terms of the GNU General Public
+License as published by the Free Software Foundation; either version
+2, or (at your option) any later version.
+
+GDB, GAS, and the GNU binutils are distributed in the hope that they
+will be useful, but WITHOUT ANY WARRANTY; without even the implied
+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+the GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this file; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  */
+#ifdef __KERNEL__
+#include <linux/kernel.h>
+#include <linux/string.h>
+#include <linux/dis-asm.h>
+#include <linux/kdb.h>
+#include "ppc.h"
+
+#if 0
+#include <setjmp.h>
+#endif
+
+#else
+#include <stdio.h>
+#include "sysdep.h"
+#include "dis-asm.h"
+#include "opcode/ppc.h"
+#endif
+bfd_vma
+bfd_getb32 (addr)
+     register const bfd_byte *addr;
+{
+  unsigned long v;
+
+  v = (unsigned long) addr[0] << 24;
+  v |= (unsigned long) addr[1] << 16;
+  v |= (unsigned long) addr[2] << 8;
+  v |= (unsigned long) addr[3];
+  return (bfd_vma) v;
+}
+
+bfd_vma
+bfd_getl32 (addr)
+     register const bfd_byte *addr;
+{
+  unsigned long v;
+
+  v = (unsigned long) addr[0];
+  v |= (unsigned long) addr[1] << 8;
+  v |= (unsigned long) addr[2] << 16;
+  v |= (unsigned long) addr[3] << 24;
+  return (bfd_vma) v;
+}
+/* This file provides several disassembler functions, all of which use
+   the disassembler interface defined in dis-asm.h.  Several functions
+   are provided because this file handles disassembly for the PowerPC
+   in both big and little endian mode and also for the POWER (RS/6000)
+   chip.  */
+
+static int print_insn_powerpc PARAMS ((bfd_vma, struct disassemble_info *,
+				       int bigendian, int dialect));
+
+/* Print a big endian PowerPC instruction.  For convenience, also
+   disassemble instructions supported by the Motorola PowerPC 601
+   and the Altivec vector unit.  */
+
+int
+print_insn_big_powerpc (memaddr, info)
+     bfd_vma memaddr;
+     struct disassemble_info *info;
+{
+  return print_insn_powerpc (memaddr, info, 1,
+			     PPC_OPCODE_PPC | PPC_OPCODE_601 |
+			     PPC_OPCODE_ALTIVEC);
+}
+
+/* Print a little endian PowerPC instruction.  For convenience, also
+   disassemble instructions supported by the Motorola PowerPC 601
+   and the Altivec vector unit.  */
+
+int
+print_insn_little_powerpc (memaddr, info)
+     bfd_vma memaddr;
+     struct disassemble_info *info;
+{
+  return print_insn_powerpc (memaddr, info, 0,
+			     PPC_OPCODE_PPC | PPC_OPCODE_601 |
+			     PPC_OPCODE_ALTIVEC);
+}
+
+/* Print a POWER (RS/6000) instruction.  */
+
+int
+print_insn_rs6000 (memaddr, info)
+     bfd_vma memaddr;
+     struct disassemble_info *info;
+{
+  return print_insn_powerpc (memaddr, info, 1, PPC_OPCODE_POWER);
+}
+
+/* Print a PowerPC or POWER instruction.  */
+
+static int
+print_insn_powerpc (memaddr, info, bigendian, dialect)
+     bfd_vma memaddr;
+     struct disassemble_info *info;
+     int bigendian;
+     int dialect;
+{
+  bfd_byte buffer[4];
+  int status;
+  unsigned long insn;
+  const struct powerpc_opcode *opcode;
+  const struct powerpc_opcode *opcode_end;
+  unsigned long op;
+
+  (*info->fprintf_func) (info->stream, "  ");
+
+  status = (*info->read_memory_func) (memaddr, buffer, 4, info);
+  if (status != 0)
+    {
+      (*info->memory_error_func) (status, memaddr, info);
+      return -1;
+    }
+
+  if (bigendian)
+    insn = bfd_getb32 (buffer);
+  else
+    insn = bfd_getl32 (buffer);
+
+  /* Get the major opcode of the instruction.  */
+  op = PPC_OP (insn);
+
+  /* Find the first match in the opcode table.  We could speed this up
+     a bit by doing a binary search on the major opcode.  */
+  opcode_end = powerpc_opcodes + powerpc_num_opcodes;
+  for (opcode = powerpc_opcodes; opcode < opcode_end; opcode++)
+    {
+      unsigned long table_op;
+      const unsigned char *opindex;
+      const struct powerpc_operand *operand;
+      int invalid;
+      int need_comma;
+      int need_paren;
+
+      table_op = PPC_OP (opcode->opcode);
+      if (op < table_op)
+	break;
+      if (op > table_op)
+	continue;
+
+      if ((insn & opcode->mask) != opcode->opcode
+	  || (opcode->flags & dialect) == 0)
+	continue;
+
+      /* Make two passes over the operands.  First see if any of them
+	 have extraction functions, and, if they do, make sure the
+	 instruction is valid.  */
+      invalid = 0;
+      for (opindex = opcode->operands; *opindex != 0; opindex++)
+	{
+	  operand = powerpc_operands + *opindex;
+	  if (operand->extract)
+	    (*operand->extract) (insn, &invalid);
+	}
+      if (invalid)
+	continue;
+
+      /* The instruction is valid.  */
+      (*info->fprintf_func) (info->stream, "%-6s", opcode->name);
+      if (opcode->operands[0] != 0)
+	(*info->fprintf_func) (info->stream, "\t");
+
+      /* Now extract and print the operands.  */
+      need_comma = 0;
+      need_paren = 0;
+      for (opindex = opcode->operands; *opindex != 0; opindex++)
+	{
+	  long value;
+
+	  operand = powerpc_operands + *opindex;
+
+	  /* Operands that are marked FAKE are simply ignored.  We
+	     already made sure that the extract function considered
+	     the instruction to be valid.  */
+	  if ((operand->flags & PPC_OPERAND_FAKE) != 0)
+	    continue;
+
+	  /* Extract the value from the instruction.  */
+	  if (operand->extract)
+	    value = (*operand->extract) (insn, (int *) NULL);
+	  else
+	    {
+	      value = (insn >> operand->shift) & ((1 << operand->bits) - 1);
+	      if ((operand->flags & PPC_OPERAND_SIGNED) != 0
+		  && (value & (1 << (operand->bits - 1))) != 0)
+		value -= 1 << operand->bits;
+	    }
+
+	  /* If the operand is optional, and the value is zero, don't
+	     print anything.  */
+	  if ((operand->flags & PPC_OPERAND_OPTIONAL) != 0
+	      && (operand->flags & PPC_OPERAND_NEXT) == 0
+	      && value == 0)
+	    continue;
+
+	  if (need_comma)
+	    {
+	      (*info->fprintf_func) (info->stream, ",");
+	      need_comma = 0;
+	    }
+
+	  /* Print the operand as directed by the flags.  */
+	  if ((operand->flags & PPC_OPERAND_GPR) != 0)
+	    (*info->fprintf_func) (info->stream, "r%ld", value);
+	  else if ((operand->flags & PPC_OPERAND_FPR) != 0)
+	    (*info->fprintf_func) (info->stream, "f%ld", value);
+	  else if ((operand->flags & PPC_OPERAND_VR) != 0)
+	    (*info->fprintf_func) (info->stream, "v%ld", value);
+	  else if ((operand->flags & PPC_OPERAND_RELATIVE) != 0)
+	    (*info->print_address_func) (memaddr + value, info);
+	  else if ((operand->flags & PPC_OPERAND_ABSOLUTE) != 0)
+	    (*info->print_address_func) ((bfd_vma) value & 0xffffffff, info);
+	  else if ((operand->flags & PPC_OPERAND_CR) == 0
+		   || (dialect & PPC_OPCODE_PPC) == 0)
+	    (*info->fprintf_func) (info->stream, "%ld", value);
+	  else
+	    {
+	      if (operand->bits == 3)
+		(*info->fprintf_func) (info->stream, "cr%d", value);
+	      else
+		{
+		  static const char *cbnames[4] = { "lt", "gt", "eq", "so" };
+		  int cr;
+		  int cc;
+
+		  cr = value >> 2;
+		  if (cr != 0)
+		    (*info->fprintf_func) (info->stream, "4*cr%d", cr);
+		  cc = value & 3;
+		  if (cc != 0)
+		    {
+		      if (cr != 0)
+			(*info->fprintf_func) (info->stream, "+");
+		      (*info->fprintf_func) (info->stream, "%s", cbnames[cc]);
+		    }
+		}
+	    }
+
+	  if (need_paren)
+	    {
+	      (*info->fprintf_func) (info->stream, ")");
+	      need_paren = 0;
+	    }
+
+	  if ((operand->flags & PPC_OPERAND_PARENS) == 0)
+	    need_comma = 1;
+	  else
+	    {
+	      (*info->fprintf_func) (info->stream, "(");
+	      need_paren = 1;
+	    }
+	}
+
+      /* We have found and printed an instruction; return.  */
+      return 4;
+    }
+
+  /* We could not find a match.  */
+  (*info->fprintf_func) (info->stream, ".long 0x%lx", insn);
+
+  return 4;
+}
diff -purN linux-2.5/arch/ppc64/kdb/ppc-opc.c linuxppc64-2.5/arch/ppc64/kdb/ppc-opc.c
--- linux-2.5/arch/ppc64/kdb/ppc-opc.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/ppc-opc.c	2003-10-13 16:16:57.000000000 +0000
@@ -0,0 +1,3491 @@
+/* ppc-opc.c -- PowerPC opcode list
+   Copyright (c) 1994, 95, 96, 97, 98, 99, 2000 Free Software Foundation, Inc.
+   Written by Ian Lance Taylor, Cygnus Support
+
+This file is part of GDB, GAS, and the GNU binutils.
+
+GDB, GAS, and the GNU binutils are free software; you can redistribute
+them and/or modify them under the terms of the GNU General Public
+License as published by the Free Software Foundation; either version
+2, or (at your option) any later version.
+
+GDB, GAS, and the GNU binutils are distributed in the hope that they
+will be useful, but WITHOUT ANY WARRANTY; without even the implied
+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+the GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this file; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+02111-1307, USA.  */
+#ifndef __KERNEL__
+#include <stdio.h>
+#include "sysdep.h"
+#include "opcode/ppc.h"
+#include "opintl.h"
+#else
+#include <linux/kernel.h>
+#include <linux/string.h>
+#include <linux/dis-asm.h>
+#include <linux/kdb.h>
+#include "ppc.h"
+#include "opintl.h"
+#endif
+/* This file holds the PowerPC opcode table.  The opcode table
+   includes almost all of the extended instruction mnemonics.  This
+   permits the disassembler to use them, and simplifies the assembler
+   logic, at the cost of increasing the table size.  The table is
+   strictly constant data, so the compiler should be able to put it in
+   the .text section.
+
+   This file also holds the operand table.  All knowledge about
+   inserting operands into instructions and vice-versa is kept in this
+   file.  */
+
+/* Local insertion and extraction functions.  */
+
+static unsigned long insert_bat PARAMS ((unsigned long, long, const char **));
+static long extract_bat PARAMS ((unsigned long, int *));
+static unsigned long insert_bba PARAMS ((unsigned long, long, const char **));
+static long extract_bba PARAMS ((unsigned long, int *));
+static unsigned long insert_bd PARAMS ((unsigned long, long, const char **));
+static long extract_bd PARAMS ((unsigned long, int *));
+static unsigned long insert_bdm PARAMS ((unsigned long, long, const char **));
+static long extract_bdm PARAMS ((unsigned long, int *));
+static unsigned long insert_bdp PARAMS ((unsigned long, long, const char **));
+static long extract_bdp PARAMS ((unsigned long, int *));
+static int valid_bo PARAMS ((long));
+static unsigned long insert_bo PARAMS ((unsigned long, long, const char **));
+static long extract_bo PARAMS ((unsigned long, int *));
+static unsigned long insert_boe PARAMS ((unsigned long, long, const char **));
+static long extract_boe PARAMS ((unsigned long, int *));
+static unsigned long insert_ds PARAMS ((unsigned long, long, const char **));
+static long extract_ds PARAMS ((unsigned long, int *));
+static unsigned long insert_li PARAMS ((unsigned long, long, const char **));
+static long extract_li PARAMS ((unsigned long, int *));
+static unsigned long insert_mbe PARAMS ((unsigned long, long, const char **));
+static long extract_mbe PARAMS ((unsigned long, int *));
+static unsigned long insert_mb6 PARAMS ((unsigned long, long, const char **));
+static long extract_mb6 PARAMS ((unsigned long, int *));
+static unsigned long insert_nb PARAMS ((unsigned long, long, const char **));
+static long extract_nb PARAMS ((unsigned long, int *));
+static unsigned long insert_nsi PARAMS ((unsigned long, long, const char **));
+static long extract_nsi PARAMS ((unsigned long, int *));
+static unsigned long insert_ral PARAMS ((unsigned long, long, const char **));
+static unsigned long insert_ram PARAMS ((unsigned long, long, const char **));
+static unsigned long insert_ras PARAMS ((unsigned long, long, const char **));
+static unsigned long insert_rbs PARAMS ((unsigned long, long, const char **));
+static long extract_rbs PARAMS ((unsigned long, int *));
+static unsigned long insert_sh6 PARAMS ((unsigned long, long, const char **));
+static long extract_sh6 PARAMS ((unsigned long, int *));
+static unsigned long insert_spr PARAMS ((unsigned long, long, const char **));
+static long extract_spr PARAMS ((unsigned long, int *));
+static unsigned long insert_tbr PARAMS ((unsigned long, long, const char **));
+static long extract_tbr PARAMS ((unsigned long, int *));
+
+/* The operands table.
+
+   The fields are bits, shift, insert, extract, flags.
+
+   We used to put parens around the various additions, like the one
+   for BA just below.  However, that caused trouble with feeble
+   compilers with a limit on depth of a parenthesized expression, like
+   (reportedly) the compiler in Microsoft Developer Studio 5.  So we
+   omit the parens, since the macros are never used in a context where
+   the addition will be ambiguous.  */
+
+const struct powerpc_operand powerpc_operands[] =
+{
+  /* The zero index is used to indicate the end of the list of
+     operands.  */
+#define UNUSED 0
+  { 0, 0, 0, 0, 0 },
+
+  /* The BA field in an XL form instruction.  */
+#define BA UNUSED + 1
+#define BA_MASK (0x1f << 16)
+  { 5, 16, 0, 0, PPC_OPERAND_CR },
+
+  /* The BA field in an XL form instruction when it must be the same
+     as the BT field in the same instruction.  */
+#define BAT BA + 1
+  { 5, 16, insert_bat, extract_bat, PPC_OPERAND_FAKE },
+
+  /* The BB field in an XL form instruction.  */
+#define BB BAT + 1
+#define BB_MASK (0x1f << 11)
+  { 5, 11, 0, 0, PPC_OPERAND_CR },
+
+  /* The BB field in an XL form instruction when it must be the same
+     as the BA field in the same instruction.  */
+#define BBA BB + 1
+  { 5, 11, insert_bba, extract_bba, PPC_OPERAND_FAKE },
+
+  /* The BD field in a B form instruction.  The lower two bits are
+     forced to zero.  */
+#define BD BBA + 1
+  { 16, 0, insert_bd, extract_bd, PPC_OPERAND_RELATIVE | PPC_OPERAND_SIGNED },
+
+  /* The BD field in a B form instruction when absolute addressing is
+     used.  */
+#define BDA BD + 1
+  { 16, 0, insert_bd, extract_bd, PPC_OPERAND_ABSOLUTE | PPC_OPERAND_SIGNED },
+
+  /* The BD field in a B form instruction when the - modifier is used.
+     This sets the y bit of the BO field appropriately.  */
+#define BDM BDA + 1
+  { 16, 0, insert_bdm, extract_bdm,
+      PPC_OPERAND_RELATIVE | PPC_OPERAND_SIGNED },
+
+  /* The BD field in a B form instruction when the - modifier is used
+     and absolute address is used.  */
+#define BDMA BDM + 1
+  { 16, 0, insert_bdm, extract_bdm,
+      PPC_OPERAND_ABSOLUTE | PPC_OPERAND_SIGNED },
+
+  /* The BD field in a B form instruction when the + modifier is used.
+     This sets the y bit of the BO field appropriately.  */
+#define BDP BDMA + 1
+  { 16, 0, insert_bdp, extract_bdp,
+      PPC_OPERAND_RELATIVE | PPC_OPERAND_SIGNED },
+
+  /* The BD field in a B form instruction when the + modifier is used
+     and absolute addressing is used.  */
+#define BDPA BDP + 1
+  { 16, 0, insert_bdp, extract_bdp,
+      PPC_OPERAND_ABSOLUTE | PPC_OPERAND_SIGNED },
+
+  /* The BF field in an X or XL form instruction.  */
+#define BF BDPA + 1
+  { 3, 23, 0, 0, PPC_OPERAND_CR },
+
+  /* An optional BF field.  This is used for comparison instructions,
+     in which an omitted BF field is taken as zero.  */
+#define OBF BF + 1
+  { 3, 23, 0, 0, PPC_OPERAND_CR | PPC_OPERAND_OPTIONAL },
+
+  /* The BFA field in an X or XL form instruction.  */
+#define BFA OBF + 1
+  { 3, 18, 0, 0, PPC_OPERAND_CR },
+
+  /* The BI field in a B form or XL form instruction.  */
+#define BI BFA + 1
+#define BI_MASK (0x1f << 16)
+  { 5, 16, 0, 0, PPC_OPERAND_CR },
+
+  /* The BO field in a B form instruction.  Certain values are
+     illegal.  */
+#define BO BI + 1
+#define BO_MASK (0x1f << 21)
+  { 5, 21, insert_bo, extract_bo, 0 },
+
+  /* The BO field in a B form instruction when the + or - modifier is
+     used.  This is like the BO field, but it must be even.  */
+#define BOE BO + 1
+  { 5, 21, insert_boe, extract_boe, 0 },
+
+  /* The BT field in an X or XL form instruction.  */
+#define BT BOE + 1
+  { 5, 21, 0, 0, PPC_OPERAND_CR },
+
+  /* The condition register number portion of the BI field in a B form
+     or XL form instruction.  This is used for the extended
+     conditional branch mnemonics, which set the lower two bits of the
+     BI field.  This field is optional.  */
+#define CR BT + 1
+  { 3, 18, 0, 0, PPC_OPERAND_CR | PPC_OPERAND_OPTIONAL },
+
+  /* The D field in a D form instruction.  This is a displacement off
+     a register, and implies that the next operand is a register in
+     parentheses.  */
+#define D CR + 1
+  { 16, 0, 0, 0, PPC_OPERAND_PARENS | PPC_OPERAND_SIGNED },
+
+  /* The DS field in a DS form instruction.  This is like D, but the
+     lower two bits are forced to zero.  */
+#define DS D + 1
+  { 16, 0, insert_ds, extract_ds, PPC_OPERAND_PARENS | PPC_OPERAND_SIGNED },
+
+  /* The E field in a wrteei instruction.  */
+#define E DS + 1
+  { 1, 15, 0, 0, 0 },
+
+  /* The FL1 field in a POWER SC form instruction.  */
+#define FL1 E + 1
+  { 4, 12, 0, 0, 0 },
+
+  /* The FL2 field in a POWER SC form instruction.  */
+#define FL2 FL1 + 1
+  { 3, 2, 0, 0, 0 },
+
+  /* The FLM field in an XFL form instruction.  */
+#define FLM FL2 + 1
+  { 8, 17, 0, 0, 0 },
+
+  /* The FRA field in an X or A form instruction.  */
+#define FRA FLM + 1
+#define FRA_MASK (0x1f << 16)
+  { 5, 16, 0, 0, PPC_OPERAND_FPR },
+
+  /* The FRB field in an X or A form instruction.  */
+#define FRB FRA + 1
+#define FRB_MASK (0x1f << 11)
+  { 5, 11, 0, 0, PPC_OPERAND_FPR },
+
+  /* The FRC field in an A form instruction.  */
+#define FRC FRB + 1
+#define FRC_MASK (0x1f << 6)
+  { 5, 6, 0, 0, PPC_OPERAND_FPR },
+
+  /* The FRS field in an X form instruction or the FRT field in a D, X
+     or A form instruction.  */
+#define FRS FRC + 1
+#define FRT FRS
+  { 5, 21, 0, 0, PPC_OPERAND_FPR },
+
+  /* The FXM field in an XFX instruction.  */
+#define FXM FRS + 1
+#define FXM_MASK (0xff << 12)
+  { 8, 12, 0, 0, 0 },
+
+  /* The L field in a D or X form instruction.  */
+#define L FXM + 1
+  { 1, 21, 0, 0, PPC_OPERAND_OPTIONAL },
+
+  /* The LEV field in a POWER SC form instruction.  */
+#define LEV L + 1
+  { 7, 5, 0, 0, 0 },
+
+  /* The LI field in an I form instruction.  The lower two bits are
+     forced to zero.  */
+#define LI LEV + 1
+  { 26, 0, insert_li, extract_li, PPC_OPERAND_RELATIVE | PPC_OPERAND_SIGNED },
+
+  /* The LI field in an I form instruction when used as an absolute
+     address.  */
+#define LIA LI + 1
+  { 26, 0, insert_li, extract_li, PPC_OPERAND_ABSOLUTE | PPC_OPERAND_SIGNED },
+
+  /* The MB field in an M form instruction.  */
+#define MB LIA + 1
+#define MB_MASK (0x1f << 6)
+  { 5, 6, 0, 0, 0 },
+
+  /* The ME field in an M form instruction.  */
+#define ME MB + 1
+#define ME_MASK (0x1f << 1)
+  { 5, 1, 0, 0, 0 },
+
+  /* The MB and ME fields in an M form instruction expressed a single
+     operand which is a bitmask indicating which bits to select.  This
+     is a two operand form using PPC_OPERAND_NEXT.  See the
+     description in opcode/ppc.h for what this means.  */
+#define MBE ME + 1
+  { 5, 6, 0, 0, PPC_OPERAND_OPTIONAL | PPC_OPERAND_NEXT },
+  { 32, 0, insert_mbe, extract_mbe, 0 },
+
+  /* The MB or ME field in an MD or MDS form instruction.  The high
+     bit is wrapped to the low end.  */
+#define MB6 MBE + 2
+#define ME6 MB6
+#define MB6_MASK (0x3f << 5)
+  { 6, 5, insert_mb6, extract_mb6, 0 },
+
+  /* The NB field in an X form instruction.  The value 32 is stored as
+     0.  */
+#define NB MB6 + 1
+  { 6, 11, insert_nb, extract_nb, 0 },
+
+  /* The NSI field in a D form instruction.  This is the same as the
+     SI field, only negated.  */
+#define NSI NB + 1
+  { 16, 0, insert_nsi, extract_nsi,
+      PPC_OPERAND_NEGATIVE | PPC_OPERAND_SIGNED },
+
+  /* The RA field in an D, DS, X, XO, M, or MDS form instruction.  */
+#define RA NSI + 1
+#define RA_MASK (0x1f << 16)
+  { 5, 16, 0, 0, PPC_OPERAND_GPR },
+
+  /* The RA field in a D or X form instruction which is an updating
+     load, which means that the RA field may not be zero and may not
+     equal the RT field.  */
+#define RAL RA + 1
+  { 5, 16, insert_ral, 0, PPC_OPERAND_GPR },
+
+  /* The RA field in an lmw instruction, which has special value
+     restrictions.  */
+#define RAM RAL + 1
+  { 5, 16, insert_ram, 0, PPC_OPERAND_GPR },
+
+  /* The RA field in a D or X form instruction which is an updating
+     store or an updating floating point load, which means that the RA
+     field may not be zero.  */
+#define RAS RAM + 1
+  { 5, 16, insert_ras, 0, PPC_OPERAND_GPR },
+
+  /* The RB field in an X, XO, M, or MDS form instruction.  */
+#define RB RAS + 1
+#define RB_MASK (0x1f << 11)
+  { 5, 11, 0, 0, PPC_OPERAND_GPR },
+
+  /* The RB field in an X form instruction when it must be the same as
+     the RS field in the instruction.  This is used for extended
+     mnemonics like mr.  */
+#define RBS RB + 1
+  { 5, 1, insert_rbs, extract_rbs, PPC_OPERAND_FAKE },
+
+  /* The RS field in a D, DS, X, XFX, XS, M, MD or MDS form
+     instruction or the RT field in a D, DS, X, XFX or XO form
+     instruction.  */
+#define RS RBS + 1
+#define RT RS
+#define RT_MASK (0x1f << 21)
+  { 5, 21, 0, 0, PPC_OPERAND_GPR },
+
+  /* The SH field in an X or M form instruction.  */
+#define SH RS + 1
+#define SH_MASK (0x1f << 11)
+  { 5, 11, 0, 0, 0 },
+
+  /* The SH field in an MD form instruction.  This is split.  */
+#define SH6 SH + 1
+#define SH6_MASK ((0x1f << 11) | (1 << 1))
+  { 6, 1, insert_sh6, extract_sh6, 0 },
+
+  /* The SI field in a D form instruction.  */
+#define SI SH6 + 1
+  { 16, 0, 0, 0, PPC_OPERAND_SIGNED },
+
+  /* The SI field in a D form instruction when we accept a wide range
+     of positive values.  */
+#define SISIGNOPT SI + 1
+  { 16, 0, 0, 0, PPC_OPERAND_SIGNED | PPC_OPERAND_SIGNOPT },
+
+  /* The SPR field in an XFX form instruction.  This is flipped--the
+     lower 5 bits are stored in the upper 5 and vice- versa.  */
+#define SPR SISIGNOPT + 1
+#define SPR_MASK (0x3ff << 11)
+  { 10, 11, insert_spr, extract_spr, 0 },
+
+  /* The BAT index number in an XFX form m[ft]ibat[lu] instruction.  */
+#define SPRBAT SPR + 1
+#define SPRBAT_MASK (0x3 << 17)
+  { 2, 17, 0, 0, 0 },
+
+  /* The SPRG register number in an XFX form m[ft]sprg instruction.  */
+#define SPRG SPRBAT + 1
+#define SPRG_MASK (0x3 << 16)
+  { 2, 16, 0, 0, 0 },
+
+  /* The SR field in an X form instruction.  */
+#define SR SPRG + 1
+  { 4, 16, 0, 0, 0 },
+
+  /* The SV field in a POWER SC form instruction.  */
+#define SV SR + 1
+  { 14, 2, 0, 0, 0 },
+
+  /* The TBR field in an XFX form instruction.  This is like the SPR
+     field, but it is optional.  */
+#define TBR SV + 1
+  { 10, 11, insert_tbr, extract_tbr, PPC_OPERAND_OPTIONAL },
+
+  /* The TO field in a D or X form instruction.  */
+#define TO TBR + 1
+#define TO_MASK (0x1f << 21)
+  { 5, 21, 0, 0, 0 },
+
+  /* The U field in an X form instruction.  */
+#define U TO + 1
+  { 4, 12, 0, 0, 0 },
+
+  /* The UI field in a D form instruction.  */
+#define UI U + 1
+  { 16, 0, 0, 0, 0 },
+
+  /* The VA field in a VA, VX or VXR form instruction. */
+#define VA UI + 1
+#define VA_MASK	(0x1f << 16)
+  {5, 16, 0, 0, PPC_OPERAND_VR},
+
+  /* The VB field in a VA, VX or VXR form instruction. */
+#define VB VA + 1
+#define VB_MASK (0x1f << 11)
+  {5, 11, 0, 0, PPC_OPERAND_VR}, 
+
+  /* The VC field in a VA form instruction. */
+#define VC VB + 1
+#define VC_MASK (0x1f << 6)
+  {5, 6, 0, 0, PPC_OPERAND_VR},
+
+  /* The VD or VS field in a VA, VX, VXR or X form instruction. */
+#define VD VC + 1
+#define VS VD
+#define VD_MASK (0x1f << 21)
+  {5, 21, 0, 0, PPC_OPERAND_VR},
+
+  /* The SIMM field in a VX form instruction. */
+#define SIMM VD + 1
+  { 5, 16, 0, 0, PPC_OPERAND_SIGNED},
+
+  /* The UIMM field in a VX form instruction. */
+#define UIMM SIMM + 1
+  { 5, 16, 0, 0, 0 },
+
+  /* The SHB field in a VA form instruction. */
+#define SHB UIMM + 1
+  { 4, 6, 0, 0, 0 },
+};
+
+/* The functions used to insert and extract complicated operands.  */
+
+/* The BA field in an XL form instruction when it must be the same as
+   the BT field in the same instruction.  This operand is marked FAKE.
+   The insertion function just copies the BT field into the BA field,
+   and the extraction function just checks that the fields are the
+   same.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_bat (insn, value, errmsg)
+     unsigned long insn;
+     long value ATTRIBUTE_UNUSED;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | (((insn >> 21) & 0x1f) << 16);
+}
+
+static long
+extract_bat (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  if (invalid != (int *) NULL
+      && ((insn >> 21) & 0x1f) != ((insn >> 16) & 0x1f))
+    *invalid = 1;
+  return 0;
+}
+
+/* The BB field in an XL form instruction when it must be the same as
+   the BA field in the same instruction.  This operand is marked FAKE.
+   The insertion function just copies the BA field into the BB field,
+   and the extraction function just checks that the fields are the
+   same.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_bba (insn, value, errmsg)
+     unsigned long insn;
+     long value ATTRIBUTE_UNUSED;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | (((insn >> 16) & 0x1f) << 11);
+}
+
+static long
+extract_bba (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  if (invalid != (int *) NULL
+      && ((insn >> 16) & 0x1f) != ((insn >> 11) & 0x1f))
+    *invalid = 1;
+  return 0;
+}
+
+/* The BD field in a B form instruction.  The lower two bits are
+   forced to zero.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_bd (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | (value & 0xfffc);
+}
+
+/*ARGSUSED*/
+static long
+extract_bd (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  if ((insn & 0x8000) != 0)
+    return (insn & 0xfffc) - 0x10000;
+  else
+    return insn & 0xfffc;
+}
+
+/* The BD field in a B form instruction when the - modifier is used.
+   This modifier means that the branch is not expected to be taken.
+   We must set the y bit of the BO field to 1 if the offset is
+   negative.  When extracting, we require that the y bit be 1 and that
+   the offset be positive, since if the y bit is 0 we just want to
+   print the normal form of the instruction.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_bdm (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  if ((value & 0x8000) != 0)
+    insn |= 1 << 21;
+  return insn | (value & 0xfffc);
+}
+
+static long
+extract_bdm (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  if (invalid != (int *) NULL
+      && ((insn & (1 << 21)) == 0
+	  || (insn & (1 << 15)) == 0))
+    *invalid = 1;
+  if ((insn & 0x8000) != 0)
+    return (insn & 0xfffc) - 0x10000;
+  else
+    return insn & 0xfffc;
+}
+
+/* The BD field in a B form instruction when the + modifier is used.
+   This is like BDM, above, except that the branch is expected to be
+   taken.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_bdp (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  if ((value & 0x8000) == 0)
+    insn |= 1 << 21;
+  return insn | (value & 0xfffc);
+}
+
+static long
+extract_bdp (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  if (invalid != (int *) NULL
+      && ((insn & (1 << 21)) == 0
+	  || (insn & (1 << 15)) != 0))
+    *invalid = 1;
+  if ((insn & 0x8000) != 0)
+    return (insn & 0xfffc) - 0x10000;
+  else
+    return insn & 0xfffc;
+}
+
+/* Check for legal values of a BO field.  */
+
+static int
+valid_bo (value)
+     long value;
+{
+  /* Certain encodings have bits that are required to be zero.  These
+     are (z must be zero, y may be anything):
+         001zy
+	 011zy
+	 1z00y
+	 1z01y
+	 1z1zz
+     */
+  switch (value & 0x14)
+    {
+    default:
+    case 0:
+      return 1;
+    case 0x4:
+      return (value & 0x2) == 0;
+    case 0x10:
+      return (value & 0x8) == 0;
+    case 0x14:
+      return value == 0x14;
+    }
+}
+
+/* The BO field in a B form instruction.  Warn about attempts to set
+   the field to an illegal value.  */
+
+static unsigned long
+insert_bo (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  if (errmsg != (const char **) NULL
+      && ! valid_bo (value))
+    *errmsg = _("invalid conditional option");
+  return insn | ((value & 0x1f) << 21);
+}
+
+static long
+extract_bo (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  long value;
+
+  value = (insn >> 21) & 0x1f;
+  if (invalid != (int *) NULL
+      && ! valid_bo (value))
+    *invalid = 1;
+  return value;
+}
+
+/* The BO field in a B form instruction when the + or - modifier is
+   used.  This is like the BO field, but it must be even.  When
+   extracting it, we force it to be even.  */
+
+static unsigned long
+insert_boe (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  if (errmsg != (const char **) NULL)
+    {
+      if (! valid_bo (value))
+	*errmsg = _("invalid conditional option");
+      else if ((value & 1) != 0)
+	*errmsg = _("attempt to set y bit when using + or - modifier");
+    }
+  return insn | ((value & 0x1f) << 21);
+}
+
+static long
+extract_boe (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  long value;
+
+  value = (insn >> 21) & 0x1f;
+  if (invalid != (int *) NULL
+      && ! valid_bo (value))
+    *invalid = 1;
+  return value & 0x1e;
+}
+
+/* The DS field in a DS form instruction.  This is like D, but the
+   lower two bits are forced to zero.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_ds (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | (value & 0xfffc);
+}
+
+/*ARGSUSED*/
+static long
+extract_ds (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  if ((insn & 0x8000) != 0)
+    return (insn & 0xfffc) - 0x10000;
+  else
+    return insn & 0xfffc;
+}
+
+/* The LI field in an I form instruction.  The lower two bits are
+   forced to zero.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_li (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  if ((value & 3) != 0 && errmsg != (const char **) NULL)
+    *errmsg = _("ignoring least significant bits in branch offset");
+  return insn | (value & 0x3fffffc);
+}
+
+/*ARGSUSED*/
+static long
+extract_li (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  if ((insn & 0x2000000) != 0)
+    return (insn & 0x3fffffc) - 0x4000000;
+  else
+    return insn & 0x3fffffc;
+}
+
+/* The MB and ME fields in an M form instruction expressed as a single
+   operand which is itself a bitmask.  The extraction function always
+   marks it as invalid, since we never want to recognize an
+   instruction which uses a field of this type.  */
+
+static unsigned long
+insert_mbe (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  unsigned long uval, mask;
+  int mb, me, mx, count, last;
+
+  uval = value;
+
+  if (uval == 0)
+    {
+      if (errmsg != (const char **) NULL)
+	*errmsg = _("illegal bitmask");
+      return insn;
+    }
+
+  mb = 0;
+  me = 32;
+  if ((uval & 1) != 0)
+    last = 1;
+  else
+    last = 0;
+  count = 0;
+
+  /* mb: location of last 0->1 transition */
+  /* me: location of last 1->0 transition */
+  /* count: # transitions */
+
+  for (mx = 0, mask = 1 << 31; mx < 32; ++mx, mask >>= 1)
+    {
+      if ((uval & mask) && !last)
+	{
+	  ++count;
+	  mb = mx;
+	  last = 1;
+	}
+      else if (!(uval & mask) && last)
+	{
+	  ++count;
+	  me = mx;
+	  last = 0;
+	}
+    }
+  if (me == 0)
+    me = 32;
+
+  if (count != 2 && (count != 0 || ! last))
+    {
+      if (errmsg != (const char **) NULL)
+	*errmsg = _("illegal bitmask");
+    }
+
+  return insn | (mb << 6) | ((me - 1) << 1);
+}
+
+static long
+extract_mbe (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  long ret;
+  int mb, me;
+  int i;
+
+  if (invalid != (int *) NULL)
+    *invalid = 1;
+
+  mb = (insn >> 6) & 0x1f;
+  me = (insn >> 1) & 0x1f;
+  if (mb < me + 1)
+    {
+      ret = 0;
+      for (i = mb; i <= me; i++)
+	ret |= (long) 1 << (31 - i);
+    }
+  else if (mb == me + 1)
+    ret = ~0;
+  else /* (mb > me + 1) */
+    {
+      ret = ~ (long) 0;
+      for (i = me + 1; i < mb; i++)
+	ret &= ~ ((long) 1 << (31 - i));
+    }
+  return ret;
+}
+
+/* The MB or ME field in an MD or MDS form instruction.  The high bit
+   is wrapped to the low end.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_mb6 (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | ((value & 0x1f) << 6) | (value & 0x20);
+}
+
+/*ARGSUSED*/
+static long
+extract_mb6 (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  return ((insn >> 6) & 0x1f) | (insn & 0x20);
+}
+
+/* The NB field in an X form instruction.  The value 32 is stored as
+   0.  */
+
+static unsigned long
+insert_nb (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  if (value < 0 || value > 32)
+    *errmsg = _("value out of range");
+  if (value == 32)
+    value = 0;
+  return insn | ((value & 0x1f) << 11);
+}
+
+/*ARGSUSED*/
+static long
+extract_nb (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  long ret;
+
+  ret = (insn >> 11) & 0x1f;
+  if (ret == 0)
+    ret = 32;
+  return ret;
+}
+
+/* The NSI field in a D form instruction.  This is the same as the SI
+   field, only negated.  The extraction function always marks it as
+   invalid, since we never want to recognize an instruction which uses
+   a field of this type.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_nsi (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | ((- value) & 0xffff);
+}
+
+static long
+extract_nsi (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  if (invalid != (int *) NULL)
+    *invalid = 1;
+  if ((insn & 0x8000) != 0)
+    return - ((long)(insn & 0xffff) - 0x10000);
+  else
+    return - (long)(insn & 0xffff);
+}
+
+/* The RA field in a D or X form instruction which is an updating
+   load, which means that the RA field may not be zero and may not
+   equal the RT field.  */
+
+static unsigned long
+insert_ral (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  if (value == 0
+      || (unsigned long) value == ((insn >> 21) & 0x1f))
+    *errmsg = "invalid register operand when updating";
+  return insn | ((value & 0x1f) << 16);
+}
+
+/* The RA field in an lmw instruction, which has special value
+   restrictions.  */
+
+static unsigned long
+insert_ram (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  if ((unsigned long) value >= ((insn >> 21) & 0x1f))
+    *errmsg = _("index register in load range");
+  return insn | ((value & 0x1f) << 16);
+}
+
+/* The RA field in a D or X form instruction which is an updating
+   store or an updating floating point load, which means that the RA
+   field may not be zero.  */
+
+static unsigned long
+insert_ras (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  if (value == 0)
+    *errmsg = _("invalid register operand when updating");
+  return insn | ((value & 0x1f) << 16);
+}
+
+/* The RB field in an X form instruction when it must be the same as
+   the RS field in the instruction.  This is used for extended
+   mnemonics like mr.  This operand is marked FAKE.  The insertion
+   function just copies the BT field into the BA field, and the
+   extraction function just checks that the fields are the same.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_rbs (insn, value, errmsg)
+     unsigned long insn;
+     long value ATTRIBUTE_UNUSED;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | (((insn >> 21) & 0x1f) << 11);
+}
+
+static long
+extract_rbs (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  if (invalid != (int *) NULL
+      && ((insn >> 21) & 0x1f) != ((insn >> 11) & 0x1f))
+    *invalid = 1;
+  return 0;
+}
+
+/* The SH field in an MD form instruction.  This is split.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_sh6 (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | ((value & 0x1f) << 11) | ((value & 0x20) >> 4);
+}
+
+/*ARGSUSED*/
+static long
+extract_sh6 (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  return ((insn >> 11) & 0x1f) | ((insn << 4) & 0x20);
+}
+
+/* The SPR field in an XFX form instruction.  This is flipped--the
+   lower 5 bits are stored in the upper 5 and vice- versa.  */
+
+static unsigned long
+insert_spr (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | ((value & 0x1f) << 16) | ((value & 0x3e0) << 6);
+}
+
+static long
+extract_spr (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  return ((insn >> 16) & 0x1f) | ((insn >> 6) & 0x3e0);
+}
+
+/* The TBR field in an XFX instruction.  This is just like SPR, but it
+   is optional.  When TBR is omitted, it must be inserted as 268 (the
+   magic number of the TB register).  These functions treat 0
+   (indicating an omitted optional operand) as 268.  This means that
+   ``mftb 4,0'' is not handled correctly.  This does not matter very
+   much, since the architecture manual does not define mftb as
+   accepting any values other than 268 or 269.  */
+
+#define TB (268)
+
+static unsigned long
+insert_tbr (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  if (value == 0)
+    value = TB;
+  return insn | ((value & 0x1f) << 16) | ((value & 0x3e0) << 6);
+}
+
+static long
+extract_tbr (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  long ret;
+
+  ret = ((insn >> 16) & 0x1f) | ((insn >> 6) & 0x3e0);
+  if (ret == TB)
+    ret = 0;
+  return ret;
+}
+
+/* Macros used to form opcodes.  */
+
+/* The main opcode.  */
+#define OP(x) ((((unsigned long)(x)) & 0x3f) << 26)
+#define OP_MASK OP (0x3f)
+
+/* The main opcode combined with a trap code in the TO field of a D
+   form instruction.  Used for extended mnemonics for the trap
+   instructions.  */
+#define OPTO(x,to) (OP (x) | ((((unsigned long)(to)) & 0x1f) << 21))
+#define OPTO_MASK (OP_MASK | TO_MASK)
+
+/* The main opcode combined with a comparison size bit in the L field
+   of a D form or X form instruction.  Used for extended mnemonics for
+   the comparison instructions.  */
+#define OPL(x,l) (OP (x) | ((((unsigned long)(l)) & 1) << 21))
+#define OPL_MASK OPL (0x3f,1)
+
+/* An A form instruction.  */
+#define A(op, xop, rc) (OP (op) | ((((unsigned long)(xop)) & 0x1f) << 1) | (((unsigned long)(rc)) & 1))
+#define A_MASK A (0x3f, 0x1f, 1)
+
+/* An A_MASK with the FRB field fixed.  */
+#define AFRB_MASK (A_MASK | FRB_MASK)
+
+/* An A_MASK with the FRC field fixed.  */
+#define AFRC_MASK (A_MASK | FRC_MASK)
+
+/* An A_MASK with the FRA and FRC fields fixed.  */
+#define AFRAFRC_MASK (A_MASK | FRA_MASK | FRC_MASK)
+
+/* A B form instruction.  */
+#define B(op, aa, lk) (OP (op) | ((((unsigned long)(aa)) & 1) << 1) | ((lk) & 1))
+#define B_MASK B (0x3f, 1, 1)
+
+/* A B form instruction setting the BO field.  */
+#define BBO(op, bo, aa, lk) (B ((op), (aa), (lk)) | ((((unsigned long)(bo)) & 0x1f) << 21))
+#define BBO_MASK BBO (0x3f, 0x1f, 1, 1)
+
+/* A BBO_MASK with the y bit of the BO field removed.  This permits
+   matching a conditional branch regardless of the setting of the y
+   bit.  */
+#define Y_MASK (((unsigned long)1) << 21)
+#define BBOY_MASK (BBO_MASK &~ Y_MASK)
+
+/* A B form instruction setting the BO field and the condition bits of
+   the BI field.  */
+#define BBOCB(op, bo, cb, aa, lk) \
+  (BBO ((op), (bo), (aa), (lk)) | ((((unsigned long)(cb)) & 0x3) << 16))
+#define BBOCB_MASK BBOCB (0x3f, 0x1f, 0x3, 1, 1)
+
+/* A BBOCB_MASK with the y bit of the BO field removed.  */
+#define BBOYCB_MASK (BBOCB_MASK &~ Y_MASK)
+
+/* A BBOYCB_MASK in which the BI field is fixed.  */
+#define BBOYBI_MASK (BBOYCB_MASK | BI_MASK)
+
+/* The main opcode mask with the RA field clear.  */
+#define DRA_MASK (OP_MASK | RA_MASK)
+
+/* A DS form instruction.  */
+#define DSO(op, xop) (OP (op) | ((xop) & 0x3))
+#define DS_MASK DSO (0x3f, 3)
+
+/* An M form instruction.  */
+#define M(op, rc) (OP (op) | ((rc) & 1))
+#define M_MASK M (0x3f, 1)
+
+/* An M form instruction with the ME field specified.  */
+#define MME(op, me, rc) (M ((op), (rc)) | ((((unsigned long)(me)) & 0x1f) << 1))
+
+/* An M_MASK with the MB and ME fields fixed.  */
+#define MMBME_MASK (M_MASK | MB_MASK | ME_MASK)
+
+/* An M_MASK with the SH and ME fields fixed.  */
+#define MSHME_MASK (M_MASK | SH_MASK | ME_MASK)
+
+/* An MD form instruction.  */
+#define MD(op, xop, rc) (OP (op) | ((((unsigned long)(xop)) & 0x7) << 2) | ((rc) & 1))
+#define MD_MASK MD (0x3f, 0x7, 1)
+
+/* An MD_MASK with the MB field fixed.  */
+#define MDMB_MASK (MD_MASK | MB6_MASK)
+
+/* An MD_MASK with the SH field fixed.  */
+#define MDSH_MASK (MD_MASK | SH6_MASK)
+
+/* An MDS form instruction.  */
+#define MDS(op, xop, rc) (OP (op) | ((((unsigned long)(xop)) & 0xf) << 1) | ((rc) & 1))
+#define MDS_MASK MDS (0x3f, 0xf, 1)
+
+/* An MDS_MASK with the MB field fixed.  */
+#define MDSMB_MASK (MDS_MASK | MB6_MASK)
+
+/* An SC form instruction.  */
+#define SC(op, sa, lk) (OP (op) | ((((unsigned long)(sa)) & 1) << 1) | ((lk) & 1))
+#define SC_MASK (OP_MASK | (((unsigned long)0x3ff) << 16) | (((unsigned long)1) << 1) | 1)
+
+/* An VX form instruction. */
+#define VX(op, xop) (OP (op) | (((unsigned long)(xop)) & 0x7ff))
+
+/* The mask for an VX form instruction. */
+#define VX_MASK	VX(0x3f, 0x7ff)
+
+/* An VA form instruction. */
+#define VXA(op, xop) (OP (op) | (((unsigned long)(xop)) & 0x07f))
+
+/* The mask for an VA form instruction. */
+#define VXA_MASK VXA(0x3f, 0x7f)
+
+/* An VXR form instruction. */
+#define VXR(op, xop, rc) (OP (op) | (((rc) & 1) << 10) | (((unsigned long)(xop)) & 0x3ff))
+
+/* The mask for a VXR form instruction. */
+#define VXR_MASK VXR(0x3f, 0x3ff, 1)
+
+/* An X form instruction.  */
+#define X(op, xop) (OP (op) | ((((unsigned long)(xop)) & 0x3ff) << 1))
+
+/* An X form instruction with the RC bit specified.  */
+#define XRC(op, xop, rc) (X ((op), (xop)) | ((rc) & 1))
+
+/* The mask for an X form instruction.  */
+#define X_MASK XRC (0x3f, 0x3ff, 1)
+
+/* An X_MASK with the RA field fixed.  */
+#define XRA_MASK (X_MASK | RA_MASK)
+
+/* An X_MASK with the RB field fixed.  */
+#define XRB_MASK (X_MASK | RB_MASK)
+
+/* An X_MASK with the RT field fixed.  */
+#define XRT_MASK (X_MASK | RT_MASK)
+
+/* An X_MASK with the RA and RB fields fixed.  */
+#define XRARB_MASK (X_MASK | RA_MASK | RB_MASK)
+
+/* An X_MASK with the RT and RA fields fixed.  */
+#define XRTRA_MASK (X_MASK | RT_MASK | RA_MASK)
+
+/* An X form comparison instruction.  */
+#define XCMPL(op, xop, l) (X ((op), (xop)) | ((((unsigned long)(l)) & 1) << 21))
+
+/* The mask for an X form comparison instruction.  */
+#define XCMP_MASK (X_MASK | (((unsigned long)1) << 22))
+
+/* The mask for an X form comparison instruction with the L field
+   fixed.  */
+#define XCMPL_MASK (XCMP_MASK | (((unsigned long)1) << 21))
+
+/* An X form trap instruction with the TO field specified.  */
+#define XTO(op, xop, to) (X ((op), (xop)) | ((((unsigned long)(to)) & 0x1f) << 21))
+#define XTO_MASK (X_MASK | TO_MASK)
+
+/* An X form tlb instruction with the SH field specified.  */
+#define XTLB(op, xop, sh) (X ((op), (xop)) | ((((unsigned long)(sh)) & 0x1f) << 11))
+#define XTLB_MASK (X_MASK | SH_MASK)
+
+/* An XFL form instruction.  */
+#define XFL(op, xop, rc) (OP (op) | ((((unsigned long)(xop)) & 0x3ff) << 1) | (((unsigned long)(rc)) & 1))
+#define XFL_MASK (XFL (0x3f, 0x3ff, 1) | (((unsigned long)1) << 25) | (((unsigned long)1) << 16))
+
+/* An XL form instruction with the LK field set to 0.  */
+#define XL(op, xop) (OP (op) | ((((unsigned long)(xop)) & 0x3ff) << 1))
+
+/* An XL form instruction which uses the LK field.  */
+#define XLLK(op, xop, lk) (XL ((op), (xop)) | ((lk) & 1))
+
+/* The mask for an XL form instruction.  */
+#define XL_MASK XLLK (0x3f, 0x3ff, 1)
+
+/* An XL form instruction which explicitly sets the BO field.  */
+#define XLO(op, bo, xop, lk) \
+  (XLLK ((op), (xop), (lk)) | ((((unsigned long)(bo)) & 0x1f) << 21))
+#define XLO_MASK (XL_MASK | BO_MASK)
+
+/* An XL form instruction which explicitly sets the y bit of the BO
+   field.  */
+#define XLYLK(op, xop, y, lk) (XLLK ((op), (xop), (lk)) | ((((unsigned long)(y)) & 1) << 21))
+#define XLYLK_MASK (XL_MASK | Y_MASK)
+
+/* An XL form instruction which sets the BO field and the condition
+   bits of the BI field.  */
+#define XLOCB(op, bo, cb, xop, lk) \
+  (XLO ((op), (bo), (xop), (lk)) | ((((unsigned long)(cb)) & 3) << 16))
+#define XLOCB_MASK XLOCB (0x3f, 0x1f, 0x3, 0x3ff, 1)
+
+/* An XL_MASK or XLYLK_MASK or XLOCB_MASK with the BB field fixed.  */
+#define XLBB_MASK (XL_MASK | BB_MASK)
+#define XLYBB_MASK (XLYLK_MASK | BB_MASK)
+#define XLBOCBBB_MASK (XLOCB_MASK | BB_MASK)
+
+/* An XL_MASK with the BO and BB fields fixed.  */
+#define XLBOBB_MASK (XL_MASK | BO_MASK | BB_MASK)
+
+/* An XL_MASK with the BO, BI and BB fields fixed.  */
+#define XLBOBIBB_MASK (XL_MASK | BO_MASK | BI_MASK | BB_MASK)
+
+/* An XO form instruction.  */
+#define XO(op, xop, oe, rc) \
+  (OP (op) | ((((unsigned long)(xop)) & 0x1ff) << 1) | ((((unsigned long)(oe)) & 1) << 10) | (((unsigned long)(rc)) & 1))
+#define XO_MASK XO (0x3f, 0x1ff, 1, 1)
+
+/* An XO_MASK with the RB field fixed.  */
+#define XORB_MASK (XO_MASK | RB_MASK)
+
+/* An XS form instruction.  */
+#define XS(op, xop, rc) (OP (op) | ((((unsigned long)(xop)) & 0x1ff) << 2) | (((unsigned long)(rc)) & 1))
+#define XS_MASK XS (0x3f, 0x1ff, 1)
+
+/* A mask for the FXM version of an XFX form instruction.  */
+#define XFXFXM_MASK (X_MASK | (((unsigned long)1) << 20) | (((unsigned long)1) << 11))
+
+/* An XFX form instruction with the FXM field filled in.  */
+#define XFXM(op, xop, fxm) \
+  (X ((op), (xop)) | ((((unsigned long)(fxm)) & 0xff) << 12))
+
+/* An XFX form instruction with the SPR field filled in.  */
+#define XSPR(op, xop, spr) \
+  (X ((op), (xop)) | ((((unsigned long)(spr)) & 0x1f) << 16) | ((((unsigned long)(spr)) & 0x3e0) << 6))
+#define XSPR_MASK (X_MASK | SPR_MASK)
+
+/* An XFX form instruction with the SPR field filled in except for the
+   SPRBAT field.  */
+#define XSPRBAT_MASK (XSPR_MASK &~ SPRBAT_MASK)
+
+/* An XFX form instruction with the SPR field filled in except for the
+   SPRG field.  */
+#define XSPRG_MASK (XSPR_MASK &~ SPRG_MASK)
+
+/* An X form instruction with everything filled in except the E field.  */
+#define XE_MASK (0xffff7fff)
+
+/* The BO encodings used in extended conditional branch mnemonics.  */
+#define BODNZF	(0x0)
+#define BODNZFP	(0x1)
+#define BODZF	(0x2)
+#define BODZFP	(0x3)
+#define BOF	(0x4)
+#define BOFP	(0x5)
+#define BODNZT	(0x8)
+#define BODNZTP	(0x9)
+#define BODZT	(0xa)
+#define BODZTP	(0xb)
+#define BOT	(0xc)
+#define BOTP	(0xd)
+#define BODNZ	(0x10)
+#define BODNZP	(0x11)
+#define BODZ	(0x12)
+#define BODZP	(0x13)
+#define BOU	(0x14)
+
+/* The BI condition bit encodings used in extended conditional branch
+   mnemonics.  */
+#define CBLT	(0)
+#define CBGT	(1)
+#define CBEQ	(2)
+#define CBSO	(3)
+
+/* The TO encodings used in extended trap mnemonics.  */
+#define TOLGT	(0x1)
+#define TOLLT	(0x2)
+#define TOEQ	(0x4)
+#define TOLGE	(0x5)
+#define TOLNL	(0x5)
+#define TOLLE	(0x6)
+#define TOLNG	(0x6)
+#define TOGT	(0x8)
+#define TOGE	(0xc)
+#define TONL	(0xc)
+#define TOLT	(0x10)
+#define TOLE	(0x14)
+#define TONG	(0x14)
+#define TONE	(0x18)
+#define TOU	(0x1f)
+
+/* Smaller names for the flags so each entry in the opcodes table will
+   fit on a single line.  */
+#undef	PPC
+#define PPC     PPC_OPCODE_PPC | PPC_OPCODE_ANY
+#define PPCCOM	PPC_OPCODE_PPC | PPC_OPCODE_COMMON | PPC_OPCODE_ANY
+#define PPC32   PPC_OPCODE_PPC | PPC_OPCODE_32 | PPC_OPCODE_ANY
+#undef PPC64
+#define PPC64   PPC_OPCODE_PPC | PPC_OPCODE_64 | PPC_OPCODE_ANY
+#define PPCONLY	PPC_OPCODE_PPC
+#define PPC403	PPC
+#define PPC405	PPC403
+#define PPC750	PPC
+#define PPC860	PPC
+#define PPCVEC	PPC_OPCODE_ALTIVEC | PPC_OPCODE_ANY
+#define	POWER   PPC_OPCODE_POWER | PPC_OPCODE_ANY
+#define	POWER2	PPC_OPCODE_POWER | PPC_OPCODE_POWER2 | PPC_OPCODE_ANY
+#define PPCPWR2	PPC_OPCODE_PPC | PPC_OPCODE_POWER | PPC_OPCODE_POWER2 | PPC_OPCODE_ANY
+#define	POWER32	PPC_OPCODE_POWER | PPC_OPCODE_ANY | PPC_OPCODE_32
+#define	COM     PPC_OPCODE_POWER | PPC_OPCODE_PPC | PPC_OPCODE_COMMON | PPC_OPCODE_ANY
+#define	COM32   PPC_OPCODE_POWER | PPC_OPCODE_PPC | PPC_OPCODE_COMMON | PPC_OPCODE_ANY | PPC_OPCODE_32
+#define	M601    PPC_OPCODE_POWER | PPC_OPCODE_601 | PPC_OPCODE_ANY
+#define PWRCOM	PPC_OPCODE_POWER | PPC_OPCODE_601 | PPC_OPCODE_COMMON | PPC_OPCODE_ANY
+#define	MFDEC1	PPC_OPCODE_POWER
+#define	MFDEC2	PPC_OPCODE_PPC | PPC_OPCODE_601
+
+/* The opcode table.
+
+   The format of the opcode table is:
+
+   NAME	     OPCODE	MASK		FLAGS		{ OPERANDS }
+
+   NAME is the name of the instruction.
+   OPCODE is the instruction opcode.
+   MASK is the opcode mask; this is used to tell the disassembler
+     which bits in the actual opcode must match OPCODE.
+   FLAGS are flags indicated what processors support the instruction.
+   OPERANDS is the list of operands.
+
+   The disassembler reads the table in order and prints the first
+   instruction which matches, so this table is sorted to put more
+   specific instructions before more general instructions.  It is also
+   sorted by major opcode.  */
+
+const struct powerpc_opcode powerpc_opcodes[] = {
+{ "tdlgti",  OPTO(2,TOLGT), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdllti",  OPTO(2,TOLLT), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdeqi",   OPTO(2,TOEQ), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdlgei",  OPTO(2,TOLGE), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdlnli",  OPTO(2,TOLNL), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdllei",  OPTO(2,TOLLE), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdlngi",  OPTO(2,TOLNG), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdgti",   OPTO(2,TOGT), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdgei",   OPTO(2,TOGE), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdnli",   OPTO(2,TONL), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdlti",   OPTO(2,TOLT), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdlei",   OPTO(2,TOLE), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdngi",   OPTO(2,TONG), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdnei",   OPTO(2,TONE), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdi",     OP(2),	OP_MASK,	PPC64,		{ TO, RA, SI } },
+
+{ "twlgti",  OPTO(3,TOLGT), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tlgti",   OPTO(3,TOLGT), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twllti",  OPTO(3,TOLLT), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tllti",   OPTO(3,TOLLT), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "tweqi",   OPTO(3,TOEQ), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "teqi",    OPTO(3,TOEQ), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twlgei",  OPTO(3,TOLGE), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tlgei",   OPTO(3,TOLGE), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twlnli",  OPTO(3,TOLNL), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tlnli",   OPTO(3,TOLNL), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twllei",  OPTO(3,TOLLE), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tllei",   OPTO(3,TOLLE), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twlngi",  OPTO(3,TOLNG), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tlngi",   OPTO(3,TOLNG), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twgti",   OPTO(3,TOGT), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tgti",    OPTO(3,TOGT), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twgei",   OPTO(3,TOGE), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tgei",    OPTO(3,TOGE), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twnli",   OPTO(3,TONL), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tnli",    OPTO(3,TONL), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twlti",   OPTO(3,TOLT), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tlti",    OPTO(3,TOLT), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twlei",   OPTO(3,TOLE), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tlei",    OPTO(3,TOLE), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twngi",   OPTO(3,TONG), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tngi",    OPTO(3,TONG), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twnei",   OPTO(3,TONE), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tnei",    OPTO(3,TONE), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twi",     OP(3),	OP_MASK,	PPCCOM,		{ TO, RA, SI } },
+{ "ti",      OP(3),	OP_MASK,	PWRCOM,		{ TO, RA, SI } },
+
+{ "macchw",	XO(4,172,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchw.",	XO(4,172,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwo",	XO(4,172,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwo.",	XO(4,172,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchws",	XO(4,236,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchws.",	XO(4,236,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwso",	XO(4,236,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwso.",	XO(4,236,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwsu",	XO(4,204,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwsu.",	XO(4,204,0,1), XO_MASK, PPC405,		{ RT, RA, RB } },
+{ "macchwsuo",	XO(4,204,1,0), XO_MASK, PPC405,		{ RT, RA, RB } },
+{ "macchwsuo.",	XO(4,204,1,1), XO_MASK, PPC405,		{ RT, RA, RB } },
+{ "macchwu",	XO(4,140,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwu.",	XO(4,140,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwuo",	XO(4,140,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwuo.",	XO(4,140,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhw",	XO(4,44,0,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhw.",	XO(4,44,0,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwo",	XO(4,44,1,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwo.",	XO(4,44,1,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhws",	XO(4,108,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhws.",	XO(4,108,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwso",	XO(4,108,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwso.",	XO(4,108,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwsu",	XO(4,76,0,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwsu.",	XO(4,76,0,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwsuo",	XO(4,76,1,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwsuo.",	XO(4,76,1,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwu",	XO(4,12,0,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwu.",	XO(4,12,0,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwuo",	XO(4,12,1,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwuo.",	XO(4,12,1,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhw",	XO(4,428,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhw.",	XO(4,428,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwo",	XO(4,428,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwo.",	XO(4,428,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhws",	XO(4,492,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhws.",	XO(4,492,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwso",	XO(4,492,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwso.",	XO(4,492,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwsu",	XO(4,460,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwsu.",	XO(4,460,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwsuo",	XO(4,460,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwsuo.",	XO(4,460,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwu",	XO(4,396,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwu.",	XO(4,396,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwuo",	XO(4,396,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwuo.",	XO(4,396,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulchw",	XRC(4,168,0),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulchw.",	XRC(4,168,1),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulchwu",	XRC(4,136,0),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulchwu.",	XRC(4,136,1),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulhhw",	XRC(4,40,0),   X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulhhw.",	XRC(4,40,1),   X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulhhwu",	XRC(4,8,0),    X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulhhwu.",	XRC(4,8,1),    X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mullhw",	XRC(4,424,0),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mullhw.",	XRC(4,424,1),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mullhwu",	XRC(4,392,0),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mullhwu.",	XRC(4,392,1),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchw",	XO(4,174,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchw.",	XO(4,174,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchwo",	XO(4,174,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchwo.",	XO(4,174,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchws",	XO(4,238,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchws.",	XO(4,238,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchwso",	XO(4,238,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchwso.",	XO(4,238,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhw",	XO(4,46,0,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhw.",	XO(4,46,0,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhwo",	XO(4,46,1,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhwo.",	XO(4,46,1,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhws",	XO(4,110,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhws.",	XO(4,110,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhwso",	XO(4,110,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhwso.",	XO(4,110,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhw",	XO(4,430,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhw.",	XO(4,430,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhwo",	XO(4,430,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhwo.",	XO(4,430,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhws",	XO(4,494,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhws.",	XO(4,494,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhwso",	XO(4,494,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhwso.",	XO(4,494,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mfvscr",  VX(4, 1540), VX_MASK,	PPCVEC,		{ VD } },
+{ "mtvscr",  VX(4, 1604), VX_MASK,	PPCVEC,		{ VD } },
+{ "vaddcuw", VX(4,  384), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vaddfp",  VX(4,   10), VX_MASK, 	PPCVEC,		{ VD, VA, VB } },
+{ "vaddsbs", VX(4,  768), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vaddshs", VX(4,  832), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vaddsws", VX(4,  896), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vaddubm", VX(4,    0), VX_MASK, 	PPCVEC,		{ VD, VA, VB } },
+{ "vaddubs", VX(4,  512), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vadduhm", VX(4,   64), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vadduhs", VX(4,  576), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vadduwm", VX(4,  128), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vadduws", VX(4,  640), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vand",    VX(4, 1028), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vandc",   VX(4, 1092), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vavgsb",  VX(4, 1282), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vavgsh",  VX(4, 1346), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vavgsw",  VX(4, 1410), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vavgub",  VX(4, 1026), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vavguh",  VX(4, 1090), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vavguw",  VX(4, 1154), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vcfsx",   VX(4,  842), VX_MASK,	PPCVEC,		{ VD, VB, UIMM } },
+{ "vcfux",   VX(4,  778), VX_MASK,	PPCVEC,		{ VD, VB, UIMM } },
+{ "vcmpbfp",   VXR(4, 966, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpbfp.",  VXR(4, 966, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpeqfp",  VXR(4, 198, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpeqfp.", VXR(4, 198, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpequb",  VXR(4,   6, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpequb.", VXR(4,   6, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpequh",  VXR(4,  70, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpequh.", VXR(4,  70, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpequw",  VXR(4, 134, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpequw.", VXR(4, 134, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgefp",  VXR(4, 454, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgefp.", VXR(4, 454, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtfp",  VXR(4, 710, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtfp.", VXR(4, 710, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtsb",  VXR(4, 774, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtsb.", VXR(4, 774, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtsh",  VXR(4, 838, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtsh.", VXR(4, 838, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtsw",  VXR(4, 902, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtsw.", VXR(4, 902, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtub",  VXR(4, 518, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtub.", VXR(4, 518, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtuh",  VXR(4, 582, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtuh.", VXR(4, 582, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtuw",  VXR(4, 646, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtuw.", VXR(4, 646, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vctsxs",    VX(4,  970), VX_MASK,	PPCVEC,		{ VD, VB, UIMM } },
+{ "vctuxs",    VX(4,  906), VX_MASK,	PPCVEC,		{ VD, VB, UIMM } },
+{ "vexptefp",  VX(4,  394), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vlogefp",   VX(4,  458), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vmaddfp",   VXA(4,  46), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmaxfp",    VX(4, 1034), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmaxsb",    VX(4,  258), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmaxsh",    VX(4,  322), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmaxsw",    VX(4,  386), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmaxub",    VX(4,    2), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmaxuh",    VX(4,   66), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmaxuw",    VX(4,  130), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmhaddshs", VXA(4,  32), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmhraddshs", VXA(4, 33), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vminfp",    VX(4, 1098), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vminsb",    VX(4,  770), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vminsh",    VX(4,  834), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vminsw",    VX(4,  898), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vminub",    VX(4,  514), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vminuh",    VX(4,  578), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vminuw",    VX(4,  642), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmladduhm", VXA(4,  34), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmrghb",    VX(4,   12), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmrghh",    VX(4,   76), VX_MASK,    PPCVEC,		{ VD, VA, VB } },
+{ "vmrghw",    VX(4,  140), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmrglb",    VX(4,  268), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmrglh",    VX(4,  332), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmrglw",    VX(4,  396), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmsummbm",  VXA(4,  37), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmsumshm",  VXA(4,  40), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmsumshs",  VXA(4,  41), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmsumubm",  VXA(4,  36), VXA_MASK,   PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmsumuhm",  VXA(4,  38), VXA_MASK,   PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmsumuhs",  VXA(4,  39), VXA_MASK,   PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmulesb",   VX(4,  776), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmulesh",   VX(4,  840), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmuleub",   VX(4,  520), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmuleuh",   VX(4,  584), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmulosb",   VX(4,  264), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmulosh",   VX(4,  328), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmuloub",   VX(4,    8), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmulouh",   VX(4,   72), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vnmsubfp",  VXA(4,  47), VXA_MASK,	PPCVEC,		{ VD, VA, VC, VB } },
+{ "vnor",      VX(4, 1284), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vor",       VX(4, 1156), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vperm",     VXA(4,  43), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vpkpx",     VX(4,  782), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkshss",   VX(4,  398), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkshus",   VX(4,  270), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkswss",   VX(4,  462), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkswus",   VX(4,  334), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkuhum",   VX(4,   14), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkuhus",   VX(4,  142), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkuwum",   VX(4,   78), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkuwus",   VX(4,  206), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vrefp",     VX(4,  266), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vrfim",     VX(4,  714), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vrfin",     VX(4,  522), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vrfip",     VX(4,  650), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vrfiz",     VX(4,  586), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vrlb",      VX(4,    4), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vrlh",      VX(4,   68), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vrlw",      VX(4,  132), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vrsqrtefp", VX(4,  330), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vsel",      VXA(4,  42), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vsl",       VX(4,  452), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vslb",      VX(4,  260), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsldoi",    VXA(4,  44), VXA_MASK,	PPCVEC,		{ VD, VA, VB, SHB } },
+{ "vslh",      VX(4,  324), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vslo",      VX(4, 1036), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vslw",      VX(4,  388), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vspltb",    VX(4,  524), VX_MASK,	PPCVEC,		{ VD, VB, UIMM } },
+{ "vsplth",    VX(4,  588), VX_MASK,	PPCVEC,		{ VD, VB, UIMM } },
+{ "vspltisb",  VX(4,  780), VX_MASK,	PPCVEC,		{ VD, SIMM } },
+{ "vspltish",  VX(4,  844), VX_MASK,	PPCVEC,		{ VD, SIMM } },
+{ "vspltisw",  VX(4,  908), VX_MASK,	PPCVEC,		{ VD, SIMM } },
+{ "vspltw",    VX(4,  652), VX_MASK,	PPCVEC,		{ VD, VB, UIMM } },
+{ "vsr",       VX(4,  708), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsrab",     VX(4,  772), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsrah",     VX(4,  836), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsraw",     VX(4,  900), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsrb",      VX(4,  516), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsrh",      VX(4,  580), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsro",      VX(4, 1100), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsrw",      VX(4,  644), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubcuw",   VX(4, 1408), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubfp",    VX(4,   74), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubsbs",   VX(4, 1792), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubshs",   VX(4, 1856), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubsws",   VX(4, 1920), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsububm",   VX(4, 1024), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsububs",   VX(4, 1536), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubuhm",   VX(4, 1088), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubuhs",   VX(4, 1600), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubuwm",   VX(4, 1152), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubuws",   VX(4, 1664), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsumsws",   VX(4, 1928), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsum2sws",  VX(4, 1672), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsum4sbs",  VX(4, 1800), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsum4shs",  VX(4, 1608), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsum4ubs",  VX(4, 1544), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vupkhpx",   VX(4,  846), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vupkhsb",   VX(4,  526), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vupkhsh",   VX(4,  590), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vupklpx",   VX(4,  974), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vupklsb",   VX(4,  654), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vupklsh",   VX(4,  718), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vxor",      VX(4, 1220), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+
+{ "mulli",   OP(7),	OP_MASK,	PPCCOM,		{ RT, RA, SI } },
+{ "muli",    OP(7),	OP_MASK,	PWRCOM,		{ RT, RA, SI } },
+
+{ "subfic",  OP(8),	OP_MASK,	PPCCOM,		{ RT, RA, SI } },
+{ "sfi",     OP(8),	OP_MASK,	PWRCOM,		{ RT, RA, SI } },
+
+{ "dozi",    OP(9),	OP_MASK,	M601,		{ RT, RA, SI } },
+
+{ "cmplwi",  OPL(10,0),	OPL_MASK,	PPCCOM,		{ OBF, RA, UI } },
+{ "cmpldi",  OPL(10,1), OPL_MASK,	PPC64,		{ OBF, RA, UI } },
+{ "cmpli",   OP(10),	OP_MASK,	PPCONLY,	{ BF, L, RA, UI } },
+{ "cmpli",   OP(10),	OP_MASK,	PWRCOM,		{ BF, RA, UI } },
+
+{ "cmpwi",   OPL(11,0),	OPL_MASK,	PPCCOM,		{ OBF, RA, SI } },
+{ "cmpdi",   OPL(11,1),	OPL_MASK,	PPC64,		{ OBF, RA, SI } },
+{ "cmpi",    OP(11),	OP_MASK,	PPCONLY,	{ BF, L, RA, SI } },
+{ "cmpi",    OP(11),	OP_MASK,	PWRCOM,		{ BF, RA, SI } },
+
+{ "addic",   OP(12),	OP_MASK,	PPCCOM,		{ RT, RA, SI } },
+{ "ai",	     OP(12),	OP_MASK,	PWRCOM,		{ RT, RA, SI } },
+{ "subic",   OP(12),	OP_MASK,	PPCCOM,		{ RT, RA, NSI } },
+
+{ "addic.",  OP(13),	OP_MASK,	PPCCOM,		{ RT, RA, SI } },
+{ "ai.",     OP(13),	OP_MASK,	PWRCOM,		{ RT, RA, SI } },
+{ "subic.",  OP(13),	OP_MASK,	PPCCOM,		{ RT, RA, NSI } },
+
+{ "li",	     OP(14),	DRA_MASK,	PPCCOM,		{ RT, SI } },
+{ "lil",     OP(14),	DRA_MASK,	PWRCOM,		{ RT, SI } },
+{ "addi",    OP(14),	OP_MASK,	PPCCOM,		{ RT, RA, SI } },
+{ "cal",     OP(14),	OP_MASK,	PWRCOM,		{ RT, D, RA } },
+{ "subi",    OP(14),	OP_MASK,	PPCCOM,		{ RT, RA, NSI } },
+{ "la",	     OP(14),	OP_MASK,	PPCCOM,		{ RT, D, RA } },
+
+{ "lis",     OP(15),	DRA_MASK,	PPCCOM,		{ RT, SISIGNOPT } },
+{ "liu",     OP(15),	DRA_MASK,	PWRCOM,		{ RT, SISIGNOPT } },
+{ "addis",   OP(15),	OP_MASK,	PPCCOM,		{ RT,RA,SISIGNOPT } },
+{ "cau",     OP(15),	OP_MASK,	PWRCOM,		{ RT,RA,SISIGNOPT } },
+{ "subis",   OP(15),	OP_MASK,	PPCCOM,		{ RT, RA, NSI } },
+
+{ "bdnz-",   BBO(16,BODNZ,0,0), BBOYBI_MASK, PPCCOM,	{ BDM } },
+{ "bdnz+",   BBO(16,BODNZ,0,0), BBOYBI_MASK, PPCCOM,	{ BDP } },
+{ "bdnz",    BBO(16,BODNZ,0,0), BBOYBI_MASK, PPCCOM,	{ BD } },
+{ "bdn",     BBO(16,BODNZ,0,0), BBOYBI_MASK, PWRCOM,	{ BD } },
+{ "bdnzl-",  BBO(16,BODNZ,0,1), BBOYBI_MASK, PPCCOM,	{ BDM } },
+{ "bdnzl+",  BBO(16,BODNZ,0,1), BBOYBI_MASK, PPCCOM,	{ BDP } },
+{ "bdnzl",   BBO(16,BODNZ,0,1), BBOYBI_MASK, PPCCOM,	{ BD } },
+{ "bdnl",    BBO(16,BODNZ,0,1), BBOYBI_MASK, PWRCOM,	{ BD } },
+{ "bdnza-",  BBO(16,BODNZ,1,0), BBOYBI_MASK, PPCCOM,	{ BDMA } },
+{ "bdnza+",  BBO(16,BODNZ,1,0), BBOYBI_MASK, PPCCOM,	{ BDPA } },
+{ "bdnza",   BBO(16,BODNZ,1,0), BBOYBI_MASK, PPCCOM,	{ BDA } },
+{ "bdna",    BBO(16,BODNZ,1,0), BBOYBI_MASK, PWRCOM,	{ BDA } },
+{ "bdnzla-", BBO(16,BODNZ,1,1), BBOYBI_MASK, PPCCOM,	{ BDMA } },
+{ "bdnzla+", BBO(16,BODNZ,1,1), BBOYBI_MASK, PPCCOM,	{ BDPA } },
+{ "bdnzla",  BBO(16,BODNZ,1,1), BBOYBI_MASK, PPCCOM,	{ BDA } },
+{ "bdnla",   BBO(16,BODNZ,1,1), BBOYBI_MASK, PWRCOM,	{ BDA } },
+{ "bdz-",    BBO(16,BODZ,0,0),  BBOYBI_MASK, PPCCOM,	{ BDM } },
+{ "bdz+",    BBO(16,BODZ,0,0),  BBOYBI_MASK, PPCCOM,	{ BDP } },
+{ "bdz",     BBO(16,BODZ,0,0),  BBOYBI_MASK, COM,	{ BD } },
+{ "bdzl-",   BBO(16,BODZ,0,1),  BBOYBI_MASK, PPCCOM,	{ BDM } },
+{ "bdzl+",   BBO(16,BODZ,0,1),  BBOYBI_MASK, PPCCOM,	{ BDP } },
+{ "bdzl",    BBO(16,BODZ,0,1),  BBOYBI_MASK, COM,	{ BD } },
+{ "bdza-",   BBO(16,BODZ,1,0),  BBOYBI_MASK, PPCCOM,	{ BDMA } },
+{ "bdza+",   BBO(16,BODZ,1,0),  BBOYBI_MASK, PPCCOM,	{ BDPA } },
+{ "bdza",    BBO(16,BODZ,1,0),  BBOYBI_MASK, COM,	{ BDA } },
+{ "bdzla-",  BBO(16,BODZ,1,1),  BBOYBI_MASK, PPCCOM,	{ BDMA } },
+{ "bdzla+",  BBO(16,BODZ,1,1),  BBOYBI_MASK, PPCCOM,	{ BDPA } },
+{ "bdzla",   BBO(16,BODZ,1,1),  BBOYBI_MASK, COM,	{ BDA } },
+{ "blt-",    BBOCB(16,BOT,CBLT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "blt+",    BBOCB(16,BOT,CBLT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "blt",     BBOCB(16,BOT,CBLT,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bltl-",   BBOCB(16,BOT,CBLT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bltl+",   BBOCB(16,BOT,CBLT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bltl",    BBOCB(16,BOT,CBLT,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "blta-",   BBOCB(16,BOT,CBLT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "blta+",   BBOCB(16,BOT,CBLT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "blta",    BBOCB(16,BOT,CBLT,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bltla-",  BBOCB(16,BOT,CBLT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bltla+",  BBOCB(16,BOT,CBLT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bltla",   BBOCB(16,BOT,CBLT,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bgt-",    BBOCB(16,BOT,CBGT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bgt+",    BBOCB(16,BOT,CBGT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bgt",     BBOCB(16,BOT,CBGT,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bgtl-",   BBOCB(16,BOT,CBGT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bgtl+",   BBOCB(16,BOT,CBGT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bgtl",    BBOCB(16,BOT,CBGT,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bgta-",   BBOCB(16,BOT,CBGT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bgta+",   BBOCB(16,BOT,CBGT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bgta",    BBOCB(16,BOT,CBGT,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bgtla-",  BBOCB(16,BOT,CBGT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bgtla+",  BBOCB(16,BOT,CBGT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bgtla",   BBOCB(16,BOT,CBGT,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "beq-",    BBOCB(16,BOT,CBEQ,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "beq+",    BBOCB(16,BOT,CBEQ,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "beq",     BBOCB(16,BOT,CBEQ,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "beql-",   BBOCB(16,BOT,CBEQ,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "beql+",   BBOCB(16,BOT,CBEQ,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "beql",    BBOCB(16,BOT,CBEQ,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "beqa-",   BBOCB(16,BOT,CBEQ,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "beqa+",   BBOCB(16,BOT,CBEQ,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "beqa",    BBOCB(16,BOT,CBEQ,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "beqla-",  BBOCB(16,BOT,CBEQ,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "beqla+",  BBOCB(16,BOT,CBEQ,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "beqla",   BBOCB(16,BOT,CBEQ,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bso-",    BBOCB(16,BOT,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bso+",    BBOCB(16,BOT,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bso",     BBOCB(16,BOT,CBSO,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bsol-",   BBOCB(16,BOT,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bsol+",   BBOCB(16,BOT,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bsol",    BBOCB(16,BOT,CBSO,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bsoa-",   BBOCB(16,BOT,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bsoa+",   BBOCB(16,BOT,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bsoa",    BBOCB(16,BOT,CBSO,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bsola-",  BBOCB(16,BOT,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bsola+",  BBOCB(16,BOT,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bsola",   BBOCB(16,BOT,CBSO,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bun-",    BBOCB(16,BOT,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bun+",    BBOCB(16,BOT,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bun",     BBOCB(16,BOT,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BD } },
+{ "bunl-",   BBOCB(16,BOT,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bunl+",   BBOCB(16,BOT,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bunl",    BBOCB(16,BOT,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BD } },
+{ "buna-",   BBOCB(16,BOT,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "buna+",   BBOCB(16,BOT,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "buna",    BBOCB(16,BOT,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDA } },
+{ "bunla-",  BBOCB(16,BOT,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bunla+",  BBOCB(16,BOT,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bunla",   BBOCB(16,BOT,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDA } },
+{ "bge-",    BBOCB(16,BOF,CBLT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bge+",    BBOCB(16,BOF,CBLT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bge",     BBOCB(16,BOF,CBLT,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bgel-",   BBOCB(16,BOF,CBLT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bgel+",   BBOCB(16,BOF,CBLT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bgel",    BBOCB(16,BOF,CBLT,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bgea-",   BBOCB(16,BOF,CBLT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bgea+",   BBOCB(16,BOF,CBLT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bgea",    BBOCB(16,BOF,CBLT,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bgela-",  BBOCB(16,BOF,CBLT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bgela+",  BBOCB(16,BOF,CBLT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bgela",   BBOCB(16,BOF,CBLT,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bnl-",    BBOCB(16,BOF,CBLT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bnl+",    BBOCB(16,BOF,CBLT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bnl",     BBOCB(16,BOF,CBLT,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bnll-",   BBOCB(16,BOF,CBLT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bnll+",   BBOCB(16,BOF,CBLT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bnll",    BBOCB(16,BOF,CBLT,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bnla-",   BBOCB(16,BOF,CBLT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnla+",   BBOCB(16,BOF,CBLT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnla",    BBOCB(16,BOF,CBLT,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bnlla-",  BBOCB(16,BOF,CBLT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnlla+",  BBOCB(16,BOF,CBLT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnlla",   BBOCB(16,BOF,CBLT,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "ble-",    BBOCB(16,BOF,CBGT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "ble+",    BBOCB(16,BOF,CBGT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "ble",     BBOCB(16,BOF,CBGT,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "blel-",   BBOCB(16,BOF,CBGT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "blel+",   BBOCB(16,BOF,CBGT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "blel",    BBOCB(16,BOF,CBGT,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "blea-",   BBOCB(16,BOF,CBGT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "blea+",   BBOCB(16,BOF,CBGT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "blea",    BBOCB(16,BOF,CBGT,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "blela-",  BBOCB(16,BOF,CBGT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "blela+",  BBOCB(16,BOF,CBGT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "blela",   BBOCB(16,BOF,CBGT,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bng-",    BBOCB(16,BOF,CBGT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bng+",    BBOCB(16,BOF,CBGT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bng",     BBOCB(16,BOF,CBGT,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bngl-",   BBOCB(16,BOF,CBGT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bngl+",   BBOCB(16,BOF,CBGT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bngl",    BBOCB(16,BOF,CBGT,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bnga-",   BBOCB(16,BOF,CBGT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnga+",   BBOCB(16,BOF,CBGT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnga",    BBOCB(16,BOF,CBGT,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bngla-",  BBOCB(16,BOF,CBGT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bngla+",  BBOCB(16,BOF,CBGT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bngla",   BBOCB(16,BOF,CBGT,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bne-",    BBOCB(16,BOF,CBEQ,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bne+",    BBOCB(16,BOF,CBEQ,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bne",     BBOCB(16,BOF,CBEQ,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bnel-",   BBOCB(16,BOF,CBEQ,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bnel+",   BBOCB(16,BOF,CBEQ,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bnel",    BBOCB(16,BOF,CBEQ,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bnea-",   BBOCB(16,BOF,CBEQ,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnea+",   BBOCB(16,BOF,CBEQ,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnea",    BBOCB(16,BOF,CBEQ,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bnela-",  BBOCB(16,BOF,CBEQ,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnela+",  BBOCB(16,BOF,CBEQ,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnela",   BBOCB(16,BOF,CBEQ,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bns-",    BBOCB(16,BOF,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bns+",    BBOCB(16,BOF,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bns",     BBOCB(16,BOF,CBSO,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bnsl-",   BBOCB(16,BOF,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bnsl+",   BBOCB(16,BOF,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bnsl",    BBOCB(16,BOF,CBSO,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bnsa-",   BBOCB(16,BOF,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnsa+",   BBOCB(16,BOF,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnsa",    BBOCB(16,BOF,CBSO,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bnsla-",  BBOCB(16,BOF,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnsla+",  BBOCB(16,BOF,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnsla",   BBOCB(16,BOF,CBSO,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bnu-",    BBOCB(16,BOF,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bnu+",    BBOCB(16,BOF,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bnu",     BBOCB(16,BOF,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BD } },
+{ "bnul-",   BBOCB(16,BOF,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bnul+",   BBOCB(16,BOF,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bnul",    BBOCB(16,BOF,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BD } },
+{ "bnua-",   BBOCB(16,BOF,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnua+",   BBOCB(16,BOF,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnua",    BBOCB(16,BOF,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDA } },
+{ "bnula-",  BBOCB(16,BOF,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnula+",  BBOCB(16,BOF,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnula",   BBOCB(16,BOF,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDA } },
+{ "bdnzt-",  BBO(16,BODNZT,0,0), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdnzt+",  BBO(16,BODNZT,0,0), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdnzt",   BBO(16,BODNZT,0,0), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdnztl-", BBO(16,BODNZT,0,1), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdnztl+", BBO(16,BODNZT,0,1), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdnztl",  BBO(16,BODNZT,0,1), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdnzta-", BBO(16,BODNZT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdnzta+", BBO(16,BODNZT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdnzta",  BBO(16,BODNZT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bdnztla-",BBO(16,BODNZT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdnztla+",BBO(16,BODNZT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdnztla", BBO(16,BODNZT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bdnzf-",  BBO(16,BODNZF,0,0), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdnzf+",  BBO(16,BODNZF,0,0), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdnzf",   BBO(16,BODNZF,0,0), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdnzfl-", BBO(16,BODNZF,0,1), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdnzfl+", BBO(16,BODNZF,0,1), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdnzfl",  BBO(16,BODNZF,0,1), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdnzfa-", BBO(16,BODNZF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdnzfa+", BBO(16,BODNZF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdnzfa",  BBO(16,BODNZF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bdnzfla-",BBO(16,BODNZF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdnzfla+",BBO(16,BODNZF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdnzfla", BBO(16,BODNZF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bt-",     BBO(16,BOT,0,0), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bt+",     BBO(16,BOT,0,0), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bt",	     BBO(16,BOT,0,0), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bbt",     BBO(16,BOT,0,0), BBOY_MASK, PWRCOM,	{ BI, BD } },
+{ "btl-",    BBO(16,BOT,0,1), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "btl+",    BBO(16,BOT,0,1), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "btl",     BBO(16,BOT,0,1), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bbtl",    BBO(16,BOT,0,1), BBOY_MASK, PWRCOM,	{ BI, BD } },
+{ "bta-",    BBO(16,BOT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bta+",    BBO(16,BOT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bta",     BBO(16,BOT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bbta",    BBO(16,BOT,1,0), BBOY_MASK, PWRCOM,	{ BI, BDA } },
+{ "btla-",   BBO(16,BOT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "btla+",   BBO(16,BOT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "btla",    BBO(16,BOT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bbtla",   BBO(16,BOT,1,1), BBOY_MASK, PWRCOM,	{ BI, BDA } },
+{ "bf-",     BBO(16,BOF,0,0), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bf+",     BBO(16,BOF,0,0), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bf",	     BBO(16,BOF,0,0), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bbf",     BBO(16,BOF,0,0), BBOY_MASK, PWRCOM,	{ BI, BD } },
+{ "bfl-",    BBO(16,BOF,0,1), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bfl+",    BBO(16,BOF,0,1), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bfl",     BBO(16,BOF,0,1), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bbfl",    BBO(16,BOF,0,1), BBOY_MASK, PWRCOM,	{ BI, BD } },
+{ "bfa-",    BBO(16,BOF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bfa+",    BBO(16,BOF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bfa",     BBO(16,BOF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bbfa",    BBO(16,BOF,1,0), BBOY_MASK, PWRCOM,	{ BI, BDA } },
+{ "bfla-",   BBO(16,BOF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bfla+",   BBO(16,BOF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bfla",    BBO(16,BOF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bbfla",   BBO(16,BOF,1,1), BBOY_MASK, PWRCOM,	{ BI, BDA } },
+{ "bdzt-",   BBO(16,BODZT,0,0), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdzt+",   BBO(16,BODZT,0,0), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdzt",    BBO(16,BODZT,0,0), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdztl-",  BBO(16,BODZT,0,1), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdztl+",  BBO(16,BODZT,0,1), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdztl",   BBO(16,BODZT,0,1), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdzta-",  BBO(16,BODZT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdzta+",  BBO(16,BODZT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdzta",   BBO(16,BODZT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bdztla-", BBO(16,BODZT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdztla+", BBO(16,BODZT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdztla",  BBO(16,BODZT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bdzf-",   BBO(16,BODZF,0,0), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdzf+",   BBO(16,BODZF,0,0), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdzf",    BBO(16,BODZF,0,0), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdzfl-",  BBO(16,BODZF,0,1), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdzfl+",  BBO(16,BODZF,0,1), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdzfl",   BBO(16,BODZF,0,1), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdzfa-",  BBO(16,BODZF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdzfa+",  BBO(16,BODZF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdzfa",   BBO(16,BODZF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bdzfla-", BBO(16,BODZF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdzfla+", BBO(16,BODZF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdzfla",  BBO(16,BODZF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bc-",     B(16,0,0),	B_MASK,		PPCCOM,		{ BOE, BI, BDM } },
+{ "bc+",     B(16,0,0),	B_MASK,		PPCCOM,		{ BOE, BI, BDP } },
+{ "bc",	     B(16,0,0),	B_MASK,		COM,		{ BO, BI, BD } },
+{ "bcl-",    B(16,0,1),	B_MASK,		PPCCOM,		{ BOE, BI, BDM } },
+{ "bcl+",    B(16,0,1),	B_MASK,		PPCCOM,		{ BOE, BI, BDP } },
+{ "bcl",     B(16,0,1),	B_MASK,		COM,		{ BO, BI, BD } },
+{ "bca-",    B(16,1,0),	B_MASK,		PPCCOM,		{ BOE, BI, BDMA } },
+{ "bca+",    B(16,1,0),	B_MASK,		PPCCOM,		{ BOE, BI, BDPA } },
+{ "bca",     B(16,1,0),	B_MASK,		COM,		{ BO, BI, BDA } },
+{ "bcla-",   B(16,1,1),	B_MASK,		PPCCOM,		{ BOE, BI, BDMA } },
+{ "bcla+",   B(16,1,1),	B_MASK,		PPCCOM,		{ BOE, BI, BDPA } },
+{ "bcla",    B(16,1,1),	B_MASK,		COM,		{ BO, BI, BDA } },
+
+{ "sc",      SC(17,1,0), 0xffffffff,	PPC,		{ 0 } },
+{ "svc",     SC(17,0,0), SC_MASK,	POWER,		{ LEV, FL1, FL2 } },
+{ "svcl",    SC(17,0,1), SC_MASK,	POWER,		{ LEV, FL1, FL2 } },
+{ "svca",    SC(17,1,0), SC_MASK,	PWRCOM,		{ SV } },
+{ "svcla",   SC(17,1,1), SC_MASK,	POWER,		{ SV } },
+
+{ "b",	     B(18,0,0),	B_MASK,		COM,	{ LI } },
+{ "bl",      B(18,0,1),	B_MASK,		COM,	{ LI } },
+{ "ba",      B(18,1,0),	B_MASK,		COM,	{ LIA } },
+{ "bla",     B(18,1,1),	B_MASK,		COM,	{ LIA } },
+
+{ "mcrf",    XL(19,0),	XLBB_MASK|(3<<21)|(3<<16), COM,	{ BF, BFA } },
+
+{ "blr",     XLO(19,BOU,16,0), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "br",      XLO(19,BOU,16,0), XLBOBIBB_MASK, PWRCOM,	{ 0 } },
+{ "blrl",    XLO(19,BOU,16,1), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "brl",     XLO(19,BOU,16,1), XLBOBIBB_MASK, PWRCOM,	{ 0 } },
+{ "bdnzlr",  XLO(19,BODNZ,16,0), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdnzlr-", XLO(19,BODNZ,16,0), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdnzlr+", XLO(19,BODNZP,16,0), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdnzlrl", XLO(19,BODNZ,16,1), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdnzlrl-",XLO(19,BODNZ,16,1), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdnzlrl+",XLO(19,BODNZP,16,1), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdzlr",   XLO(19,BODZ,16,0), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdzlr-",  XLO(19,BODZ,16,0), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdzlr+",  XLO(19,BODZP,16,0), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdzlrl",  XLO(19,BODZ,16,1), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdzlrl-", XLO(19,BODZ,16,1), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdzlrl+", XLO(19,BODZP,16,1), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bltlr",   XLOCB(19,BOT,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bltlr-",  XLOCB(19,BOT,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bltlr+",  XLOCB(19,BOTP,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bltr",    XLOCB(19,BOT,CBLT,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bltlrl",  XLOCB(19,BOT,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bltlrl-", XLOCB(19,BOT,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bltlrl+", XLOCB(19,BOTP,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bltrl",   XLOCB(19,BOT,CBLT,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bgtlr",   XLOCB(19,BOT,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgtlr-",  XLOCB(19,BOT,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgtlr+",  XLOCB(19,BOTP,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgtr",    XLOCB(19,BOT,CBGT,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bgtlrl",  XLOCB(19,BOT,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgtlrl-", XLOCB(19,BOT,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgtlrl+", XLOCB(19,BOTP,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgtrl",   XLOCB(19,BOT,CBGT,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "beqlr",   XLOCB(19,BOT,CBEQ,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "beqlr-",  XLOCB(19,BOT,CBEQ,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "beqlr+",  XLOCB(19,BOTP,CBEQ,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "beqr",    XLOCB(19,BOT,CBEQ,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "beqlrl",  XLOCB(19,BOT,CBEQ,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "beqlrl-", XLOCB(19,BOT,CBEQ,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "beqlrl+", XLOCB(19,BOTP,CBEQ,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "beqrl",   XLOCB(19,BOT,CBEQ,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bsolr",   XLOCB(19,BOT,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bsolr-",  XLOCB(19,BOT,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bsolr+",  XLOCB(19,BOTP,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bsor",    XLOCB(19,BOT,CBSO,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bsolrl",  XLOCB(19,BOT,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bsolrl-", XLOCB(19,BOT,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bsolrl+", XLOCB(19,BOTP,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bsorl",   XLOCB(19,BOT,CBSO,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bunlr",   XLOCB(19,BOT,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bunlr-",  XLOCB(19,BOT,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bunlr+",  XLOCB(19,BOTP,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bunlrl",  XLOCB(19,BOT,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bunlrl-", XLOCB(19,BOT,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bunlrl+", XLOCB(19,BOTP,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgelr",   XLOCB(19,BOF,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgelr-",  XLOCB(19,BOF,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgelr+",  XLOCB(19,BOFP,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bger",    XLOCB(19,BOF,CBLT,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bgelrl",  XLOCB(19,BOF,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgelrl-", XLOCB(19,BOF,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgelrl+", XLOCB(19,BOFP,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgerl",   XLOCB(19,BOF,CBLT,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnllr",   XLOCB(19,BOF,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnllr-",  XLOCB(19,BOF,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnllr+",  XLOCB(19,BOFP,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnlr",    XLOCB(19,BOF,CBLT,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnllrl",  XLOCB(19,BOF,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnllrl-", XLOCB(19,BOF,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnllrl+", XLOCB(19,BOFP,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnlrl",   XLOCB(19,BOF,CBLT,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "blelr",   XLOCB(19,BOF,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "blelr-",  XLOCB(19,BOF,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "blelr+",  XLOCB(19,BOFP,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bler",    XLOCB(19,BOF,CBGT,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "blelrl",  XLOCB(19,BOF,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "blelrl-", XLOCB(19,BOF,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "blelrl+", XLOCB(19,BOFP,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "blerl",   XLOCB(19,BOF,CBGT,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnglr",   XLOCB(19,BOF,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnglr-",  XLOCB(19,BOF,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnglr+",  XLOCB(19,BOFP,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bngr",    XLOCB(19,BOF,CBGT,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnglrl",  XLOCB(19,BOF,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnglrl-", XLOCB(19,BOF,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnglrl+", XLOCB(19,BOFP,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bngrl",   XLOCB(19,BOF,CBGT,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnelr",   XLOCB(19,BOF,CBEQ,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnelr-",  XLOCB(19,BOF,CBEQ,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnelr+",  XLOCB(19,BOFP,CBEQ,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bner",    XLOCB(19,BOF,CBEQ,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnelrl",  XLOCB(19,BOF,CBEQ,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnelrl-", XLOCB(19,BOF,CBEQ,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnelrl+", XLOCB(19,BOFP,CBEQ,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnerl",   XLOCB(19,BOF,CBEQ,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnslr",   XLOCB(19,BOF,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnslr-",  XLOCB(19,BOF,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnslr+",  XLOCB(19,BOFP,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnsr",    XLOCB(19,BOF,CBSO,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnslrl",  XLOCB(19,BOF,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnslrl-", XLOCB(19,BOF,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnslrl+", XLOCB(19,BOFP,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnsrl",   XLOCB(19,BOF,CBSO,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnulr",   XLOCB(19,BOF,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnulr-",  XLOCB(19,BOF,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnulr+",  XLOCB(19,BOFP,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnulrl",  XLOCB(19,BOF,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnulrl-", XLOCB(19,BOF,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnulrl+", XLOCB(19,BOFP,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "btlr",    XLO(19,BOT,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btlr-",   XLO(19,BOT,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btlr+",   XLO(19,BOTP,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bbtr",    XLO(19,BOT,16,0), XLBOBB_MASK, PWRCOM,	{ BI } },
+{ "btlrl",   XLO(19,BOT,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btlrl-",  XLO(19,BOT,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btlrl+",  XLO(19,BOTP,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bbtrl",   XLO(19,BOT,16,1), XLBOBB_MASK, PWRCOM,	{ BI } },
+{ "bflr",    XLO(19,BOF,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bflr-",   XLO(19,BOF,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bflr+",   XLO(19,BOFP,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bbfr",    XLO(19,BOF,16,0), XLBOBB_MASK, PWRCOM,	{ BI } },
+{ "bflrl",   XLO(19,BOF,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bflrl-",  XLO(19,BOF,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bflrl+",  XLO(19,BOFP,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bbfrl",   XLO(19,BOF,16,1), XLBOBB_MASK, PWRCOM,	{ BI } },
+{ "bdnztlr", XLO(19,BODNZT,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnztlr-",XLO(19,BODNZT,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnztlr+",XLO(19,BODNZTP,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnztlrl",XLO(19,BODNZT,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnztlrl-",XLO(19,BODNZT,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnztlrl+",XLO(19,BODNZTP,16,1), XLBOBB_MASK, PPCCOM,{ BI } },
+{ "bdnzflr", XLO(19,BODNZF,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnzflr-",XLO(19,BODNZF,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnzflr+",XLO(19,BODNZFP,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnzflrl",XLO(19,BODNZF,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnzflrl-",XLO(19,BODNZF,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnzflrl+",XLO(19,BODNZFP,16,1), XLBOBB_MASK, PPCCOM,{ BI } },
+{ "bdztlr",  XLO(19,BODZT,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdztlr-", XLO(19,BODZT,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdztlr+", XLO(19,BODZTP,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdztlrl", XLO(19,BODZT,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdztlrl-",XLO(19,BODZT,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdztlrl+",XLO(19,BODZTP,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdzflr",  XLO(19,BODZF,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdzflr-", XLO(19,BODZF,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdzflr+", XLO(19,BODZFP,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdzflrl", XLO(19,BODZF,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdzflrl-",XLO(19,BODZF,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdzflrl+",XLO(19,BODZFP,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bclr",    XLLK(19,16,0), XLYBB_MASK,	PPCCOM,		{ BO, BI } },
+{ "bclrl",   XLLK(19,16,1), XLYBB_MASK,	PPCCOM,		{ BO, BI } },
+{ "bclr+",   XLYLK(19,16,1,0), XLYBB_MASK, PPCCOM,	{ BOE, BI } },
+{ "bclrl+",  XLYLK(19,16,1,1), XLYBB_MASK, PPCCOM,	{ BOE, BI } },
+{ "bclr-",   XLYLK(19,16,0,0), XLYBB_MASK, PPCCOM,	{ BOE, BI } },
+{ "bclrl-",  XLYLK(19,16,0,1), XLYBB_MASK, PPCCOM,	{ BOE, BI } },
+{ "bcr",     XLLK(19,16,0), XLBB_MASK,	PWRCOM,		{ BO, BI } },
+{ "bcrl",    XLLK(19,16,1), XLBB_MASK,	PWRCOM,		{ BO, BI } },
+
+{ "rfid",    XL(19,18),	0xffffffff,	PPC64,		{ 0 } },
+
+{ "crnot",   XL(19,33), XL_MASK,	PPCCOM,		{ BT, BA, BBA } },
+{ "crnor",   XL(19,33),	XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "rfi",     XL(19,50),	0xffffffff,	COM,		{ 0 } },
+{ "rfci",    XL(19,51),	0xffffffff,	PPC403,		{ 0 } },
+
+{ "rfsvc",   XL(19,82),	0xffffffff,	POWER,		{ 0 } },
+
+{ "crandc",  XL(19,129), XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "isync",   XL(19,150), 0xffffffff,	PPCCOM,		{ 0 } },
+{ "ics",     XL(19,150), 0xffffffff,	PWRCOM,		{ 0 } },
+
+{ "crclr",   XL(19,193), XL_MASK,	PPCCOM,		{ BT, BAT, BBA } },
+{ "crxor",   XL(19,193), XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "crnand",  XL(19,225), XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "crand",   XL(19,257), XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "crset",   XL(19,289), XL_MASK,	PPCCOM,		{ BT, BAT, BBA } },
+{ "creqv",   XL(19,289), XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "crorc",   XL(19,417), XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "crmove",  XL(19,449), XL_MASK,	PPCCOM,		{ BT, BA, BBA } },
+{ "cror",    XL(19,449), XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "bctr",    XLO(19,BOU,528,0), XLBOBIBB_MASK, COM,	{ 0 } },
+{ "bctrl",   XLO(19,BOU,528,1), XLBOBIBB_MASK, COM,	{ 0 } },
+{ "bltctr",  XLOCB(19,BOT,CBLT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bltctr-", XLOCB(19,BOT,CBLT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bltctr+", XLOCB(19,BOTP,CBLT,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bltctrl", XLOCB(19,BOT,CBLT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bltctrl-",XLOCB(19,BOT,CBLT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bltctrl+",XLOCB(19,BOTP,CBLT,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgtctr",  XLOCB(19,BOT,CBGT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgtctr-", XLOCB(19,BOT,CBGT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgtctr+", XLOCB(19,BOTP,CBGT,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgtctrl", XLOCB(19,BOT,CBGT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgtctrl-",XLOCB(19,BOT,CBGT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgtctrl+",XLOCB(19,BOTP,CBGT,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "beqctr",  XLOCB(19,BOT,CBEQ,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "beqctr-", XLOCB(19,BOT,CBEQ,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "beqctr+", XLOCB(19,BOTP,CBEQ,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "beqctrl", XLOCB(19,BOT,CBEQ,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "beqctrl-",XLOCB(19,BOT,CBEQ,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "beqctrl+",XLOCB(19,BOTP,CBEQ,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bsoctr",  XLOCB(19,BOT,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bsoctr-", XLOCB(19,BOT,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bsoctr+", XLOCB(19,BOTP,CBSO,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bsoctrl", XLOCB(19,BOT,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bsoctrl-",XLOCB(19,BOT,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bsoctrl+",XLOCB(19,BOTP,CBSO,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bunctr",  XLOCB(19,BOT,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bunctr-", XLOCB(19,BOT,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bunctr+", XLOCB(19,BOTP,CBSO,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bunctrl", XLOCB(19,BOT,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bunctrl-",XLOCB(19,BOT,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bunctrl+",XLOCB(19,BOTP,CBSO,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgectr",  XLOCB(19,BOF,CBLT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgectr-", XLOCB(19,BOF,CBLT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgectr+", XLOCB(19,BOFP,CBLT,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgectrl", XLOCB(19,BOF,CBLT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgectrl-",XLOCB(19,BOF,CBLT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgectrl+",XLOCB(19,BOFP,CBLT,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnlctr",  XLOCB(19,BOF,CBLT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnlctr-", XLOCB(19,BOF,CBLT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnlctr+", XLOCB(19,BOFP,CBLT,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnlctrl", XLOCB(19,BOF,CBLT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnlctrl-",XLOCB(19,BOF,CBLT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnlctrl+",XLOCB(19,BOFP,CBLT,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "blectr",  XLOCB(19,BOF,CBGT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "blectr-", XLOCB(19,BOF,CBGT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "blectr+", XLOCB(19,BOFP,CBGT,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "blectrl", XLOCB(19,BOF,CBGT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "blectrl-",XLOCB(19,BOF,CBGT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "blectrl+",XLOCB(19,BOFP,CBGT,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bngctr",  XLOCB(19,BOF,CBGT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bngctr-", XLOCB(19,BOF,CBGT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bngctr+", XLOCB(19,BOFP,CBGT,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bngctrl", XLOCB(19,BOF,CBGT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bngctrl-",XLOCB(19,BOF,CBGT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bngctrl+",XLOCB(19,BOFP,CBGT,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnectr",  XLOCB(19,BOF,CBEQ,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnectr-", XLOCB(19,BOF,CBEQ,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnectr+", XLOCB(19,BOFP,CBEQ,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnectrl", XLOCB(19,BOF,CBEQ,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnectrl-",XLOCB(19,BOF,CBEQ,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnectrl+",XLOCB(19,BOFP,CBEQ,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnsctr",  XLOCB(19,BOF,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnsctr-", XLOCB(19,BOF,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnsctr+", XLOCB(19,BOFP,CBSO,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnsctrl", XLOCB(19,BOF,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnsctrl-",XLOCB(19,BOF,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnsctrl+",XLOCB(19,BOFP,CBSO,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnuctr",  XLOCB(19,BOF,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnuctr-", XLOCB(19,BOF,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnuctr+", XLOCB(19,BOFP,CBSO,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnuctrl", XLOCB(19,BOF,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnuctrl-",XLOCB(19,BOF,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnuctrl+",XLOCB(19,BOFP,CBSO,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "btctr",   XLO(19,BOT,528,0),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btctr-",  XLO(19,BOT,528,0),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btctr+",  XLO(19,BOTP,528,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btctrl",  XLO(19,BOT,528,1),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btctrl-", XLO(19,BOT,528,1),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btctrl+", XLO(19,BOTP,528,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bfctr",   XLO(19,BOF,528,0),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bfctr-",  XLO(19,BOF,528,0),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bfctr+",  XLO(19,BOFP,528,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bfctrl",  XLO(19,BOF,528,1),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bfctrl-", XLO(19,BOF,528,1),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bfctrl+", XLO(19,BOFP,528,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bcctr",   XLLK(19,528,0),     XLYBB_MASK,  PPCCOM,	{ BO, BI } },
+{ "bcctr-",  XLYLK(19,528,0,0),  XLYBB_MASK,  PPCCOM,	{ BOE, BI } },
+{ "bcctr+",  XLYLK(19,528,1,0),  XLYBB_MASK,  PPCCOM,	{ BOE, BI } },
+{ "bcctrl",  XLLK(19,528,1),     XLYBB_MASK,  PPCCOM,	{ BO, BI } },
+{ "bcctrl-", XLYLK(19,528,0,1),  XLYBB_MASK,  PPCCOM,	{ BOE, BI } },
+{ "bcctrl+", XLYLK(19,528,1,1),  XLYBB_MASK,  PPCCOM,	{ BOE, BI } },
+{ "bcc",     XLLK(19,528,0),     XLBB_MASK,   PWRCOM,	{ BO, BI } },
+{ "bccl",    XLLK(19,528,1),     XLBB_MASK,   PWRCOM,	{ BO, BI } },
+
+{ "rlwimi",  M(20,0),	M_MASK,		PPCCOM,		{ RA,RS,SH,MBE,ME } },
+{ "rlimi",   M(20,0),	M_MASK,		PWRCOM,		{ RA,RS,SH,MBE,ME } },
+
+{ "rlwimi.", M(20,1),	M_MASK,		PPCCOM,		{ RA,RS,SH,MBE,ME } },
+{ "rlimi.",  M(20,1),	M_MASK,		PWRCOM,		{ RA,RS,SH,MBE,ME } },
+
+{ "rotlwi",  MME(21,31,0), MMBME_MASK,	PPCCOM,		{ RA, RS, SH } },
+{ "clrlwi",  MME(21,31,0), MSHME_MASK,	PPCCOM,		{ RA, RS, MB } },
+{ "rlwinm",  M(21,0),	M_MASK,		PPCCOM,		{ RA,RS,SH,MBE,ME } },
+{ "rlinm",   M(21,0),	M_MASK,		PWRCOM,		{ RA,RS,SH,MBE,ME } },
+{ "rotlwi.", MME(21,31,1), MMBME_MASK,	PPCCOM,		{ RA,RS,SH } },
+{ "clrlwi.", MME(21,31,1), MSHME_MASK,	PPCCOM,		{ RA, RS, MB } },
+{ "rlwinm.", M(21,1),	M_MASK,		PPCCOM,		{ RA,RS,SH,MBE,ME } },
+{ "rlinm.",  M(21,1),	M_MASK,		PWRCOM,		{ RA,RS,SH,MBE,ME } },
+
+{ "rlmi",    M(22,0),	M_MASK,		M601,		{ RA,RS,RB,MBE,ME } },
+{ "rlmi.",   M(22,1),	M_MASK,		M601,		{ RA,RS,RB,MBE,ME } },
+
+{ "rotlw",   MME(23,31,0), MMBME_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "rlwnm",   M(23,0),	M_MASK,		PPCCOM,		{ RA,RS,RB,MBE,ME } },
+{ "rlnm",    M(23,0),	M_MASK,		PWRCOM,		{ RA,RS,RB,MBE,ME } },
+{ "rotlw.",  MME(23,31,1), MMBME_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "rlwnm.",  M(23,1),	M_MASK,		PPCCOM,		{ RA,RS,RB,MBE,ME } },
+{ "rlnm.",   M(23,1),	M_MASK,		PWRCOM,		{ RA,RS,RB,MBE,ME } },
+
+{ "nop",     OP(24),	0xffffffff,	PPCCOM,		{ 0 } },
+{ "ori",     OP(24),	OP_MASK,	PPCCOM,		{ RA, RS, UI } },
+{ "oril",    OP(24),	OP_MASK,	PWRCOM,		{ RA, RS, UI } },
+
+{ "oris",    OP(25),	OP_MASK,	PPCCOM,		{ RA, RS, UI } },
+{ "oriu",    OP(25),	OP_MASK,	PWRCOM,		{ RA, RS, UI } },
+
+{ "xori",    OP(26),	OP_MASK,	PPCCOM,		{ RA, RS, UI } },
+{ "xoril",   OP(26),	OP_MASK,	PWRCOM,		{ RA, RS, UI } },
+
+{ "xoris",   OP(27),	OP_MASK,	PPCCOM,		{ RA, RS, UI } },
+{ "xoriu",   OP(27),	OP_MASK,	PWRCOM,		{ RA, RS, UI } },
+
+{ "andi.",   OP(28),	OP_MASK,	PPCCOM,		{ RA, RS, UI } },
+{ "andil.",  OP(28),	OP_MASK,	PWRCOM,		{ RA, RS, UI } },
+
+{ "andis.",  OP(29),	OP_MASK,	PPCCOM,		{ RA, RS, UI } },
+{ "andiu.",  OP(29),	OP_MASK,	PWRCOM,		{ RA, RS, UI } },
+
+{ "rotldi",  MD(30,0,0), MDMB_MASK,	PPC64,		{ RA, RS, SH6 } },
+{ "clrldi",  MD(30,0,0), MDSH_MASK,	PPC64,		{ RA, RS, MB6 } },
+{ "rldicl",  MD(30,0,0), MD_MASK,	PPC64,		{ RA, RS, SH6, MB6 } },
+{ "rotldi.", MD(30,0,1), MDMB_MASK,	PPC64,		{ RA, RS, SH6 } },
+{ "clrldi.", MD(30,0,1), MDSH_MASK,	PPC64,		{ RA, RS, MB6 } },
+{ "rldicl.", MD(30,0,1), MD_MASK,	PPC64,		{ RA, RS, SH6, MB6 } },
+
+{ "rldicr",  MD(30,1,0), MD_MASK,	PPC64,		{ RA, RS, SH6, ME6 } },
+{ "rldicr.", MD(30,1,1), MD_MASK,	PPC64,		{ RA, RS, SH6, ME6 } },
+
+{ "rldic",   MD(30,2,0), MD_MASK,	PPC64,		{ RA, RS, SH6, MB6 } },
+{ "rldic.",  MD(30,2,1), MD_MASK,	PPC64,		{ RA, RS, SH6, MB6 } },
+
+{ "rldimi",  MD(30,3,0), MD_MASK,	PPC64,		{ RA, RS, SH6, MB6 } },
+{ "rldimi.", MD(30,3,1), MD_MASK,	PPC64,		{ RA, RS, SH6, MB6 } },
+
+{ "rotld",   MDS(30,8,0), MDSMB_MASK,	PPC64,		{ RA, RS, RB } },
+{ "rldcl",   MDS(30,8,0), MDS_MASK,	PPC64,		{ RA, RS, RB, MB6 } },
+{ "rotld.",  MDS(30,8,1), MDSMB_MASK,	PPC64,		{ RA, RS, RB } },
+{ "rldcl.",  MDS(30,8,1), MDS_MASK,	PPC64,		{ RA, RS, RB, MB6 } },
+
+{ "rldcr",   MDS(30,9,0), MDS_MASK,	PPC64,		{ RA, RS, RB, ME6 } },
+{ "rldcr.",  MDS(30,9,1), MDS_MASK,	PPC64,		{ RA, RS, RB, ME6 } },
+
+{ "cmpw",    XCMPL(31,0,0), XCMPL_MASK, PPCCOM,		{ OBF, RA, RB } },
+{ "cmpd",    XCMPL(31,0,1), XCMPL_MASK, PPC64,		{ OBF, RA, RB } },
+{ "cmp",     X(31,0),	XCMP_MASK,	PPCONLY,	{ BF, L, RA, RB } },
+{ "cmp",     X(31,0),	XCMPL_MASK,	PWRCOM,		{ BF, RA, RB } },
+
+{ "twlgt",   XTO(31,4,TOLGT), XTO_MASK, PPCCOM,		{ RA, RB } },
+{ "tlgt",    XTO(31,4,TOLGT), XTO_MASK, PWRCOM,		{ RA, RB } },
+{ "twllt",   XTO(31,4,TOLLT), XTO_MASK, PPCCOM,		{ RA, RB } },
+{ "tllt",    XTO(31,4,TOLLT), XTO_MASK, PWRCOM,		{ RA, RB } },
+{ "tweq",    XTO(31,4,TOEQ), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "teq",     XTO(31,4,TOEQ), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "twlge",   XTO(31,4,TOLGE), XTO_MASK, PPCCOM,		{ RA, RB } },
+{ "tlge",    XTO(31,4,TOLGE), XTO_MASK, PWRCOM,		{ RA, RB } },
+{ "twlnl",   XTO(31,4,TOLNL), XTO_MASK, PPCCOM,		{ RA, RB } },
+{ "tlnl",    XTO(31,4,TOLNL), XTO_MASK, PWRCOM,		{ RA, RB } },
+{ "twlle",   XTO(31,4,TOLLE), XTO_MASK, PPCCOM,		{ RA, RB } },
+{ "tlle",    XTO(31,4,TOLLE), XTO_MASK, PWRCOM,		{ RA, RB } },
+{ "twlng",   XTO(31,4,TOLNG), XTO_MASK, PPCCOM,		{ RA, RB } },
+{ "tlng",    XTO(31,4,TOLNG), XTO_MASK, PWRCOM,		{ RA, RB } },
+{ "twgt",    XTO(31,4,TOGT), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "tgt",     XTO(31,4,TOGT), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "twge",    XTO(31,4,TOGE), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "tge",     XTO(31,4,TOGE), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "twnl",    XTO(31,4,TONL), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "tnl",     XTO(31,4,TONL), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "twlt",    XTO(31,4,TOLT), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "tlt",     XTO(31,4,TOLT), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "twle",    XTO(31,4,TOLE), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "tle",     XTO(31,4,TOLE), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "twng",    XTO(31,4,TONG), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "tng",     XTO(31,4,TONG), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "twne",    XTO(31,4,TONE), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "tne",     XTO(31,4,TONE), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "trap",    XTO(31,4,TOU), 0xffffffff,	PPCCOM,		{ 0 } },
+{ "tw",      X(31,4),	X_MASK,		PPCCOM,		{ TO, RA, RB } },
+{ "t",       X(31,4),	X_MASK,		PWRCOM,		{ TO, RA, RB } },
+
+{ "subfc",   XO(31,8,0,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sf",      XO(31,8,0,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "subc",    XO(31,8,0,0), XO_MASK,	PPC,		{ RT, RB, RA } },
+{ "subfc.",  XO(31,8,0,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sf.",     XO(31,8,0,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "subc.",   XO(31,8,0,1), XO_MASK,	PPCCOM,		{ RT, RB, RA } },
+{ "subfco",  XO(31,8,1,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sfo",     XO(31,8,1,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "subco",   XO(31,8,1,0), XO_MASK,	PPC,		{ RT, RB, RA } },
+{ "subfco.", XO(31,8,1,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sfo.",    XO(31,8,1,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "subco.",  XO(31,8,1,1), XO_MASK,	PPC,		{ RT, RB, RA } },
+
+{ "mulhdu",  XO(31,9,0,0), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "mulhdu.", XO(31,9,0,1), XO_MASK,	PPC64,		{ RT, RA, RB } },
+
+{ "addc",    XO(31,10,0,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "a",       XO(31,10,0,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "addc.",   XO(31,10,0,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "a.",      XO(31,10,0,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "addco",   XO(31,10,1,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "ao",      XO(31,10,1,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "addco.",  XO(31,10,1,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "ao.",     XO(31,10,1,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+
+{ "mulhwu",  XO(31,11,0,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "mulhwu.", XO(31,11,0,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+
+{ "mfcr",    X(31,19),	XRARB_MASK,	COM,		{ RT } },
+
+{ "lwarx",   X(31,20),	X_MASK,		PPC,		{ RT, RA, RB } },
+
+{ "ldx",     X(31,21),	X_MASK,		PPC64,		{ RT, RA, RB } },
+
+{ "lwzx",    X(31,23),	X_MASK,		PPCCOM,		{ RT, RA, RB } },
+{ "lx",      X(31,23),	X_MASK,		PWRCOM,		{ RT, RA, RB } },
+
+{ "slw",     XRC(31,24,0), X_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "sl",      XRC(31,24,0), X_MASK,	PWRCOM,		{ RA, RS, RB } },
+{ "slw.",    XRC(31,24,1), X_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "sl.",     XRC(31,24,1), X_MASK,	PWRCOM,		{ RA, RS, RB } },
+
+{ "cntlzw",  XRC(31,26,0), XRB_MASK,	PPCCOM,		{ RA, RS } },
+{ "cntlz",   XRC(31,26,0), XRB_MASK,	PWRCOM,		{ RA, RS } },
+{ "cntlzw.", XRC(31,26,1), XRB_MASK,	PPCCOM,		{ RA, RS } },
+{ "cntlz.",  XRC(31,26,1), XRB_MASK, 	PWRCOM,		{ RA, RS } },
+
+{ "sld",     XRC(31,27,0), X_MASK,	PPC64,		{ RA, RS, RB } },
+{ "sld.",    XRC(31,27,1), X_MASK,	PPC64,		{ RA, RS, RB } },
+
+{ "and",     XRC(31,28,0), X_MASK,	COM,		{ RA, RS, RB } },
+{ "and.",    XRC(31,28,1), X_MASK,	COM,		{ RA, RS, RB } },
+
+{ "maskg",   XRC(31,29,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "maskg.",  XRC(31,29,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "cmplw",   XCMPL(31,32,0), XCMPL_MASK, PPCCOM,	{ OBF, RA, RB } },
+{ "cmpld",   XCMPL(31,32,1), XCMPL_MASK, PPC64,		{ OBF, RA, RB } },
+{ "cmpl",    X(31,32),	XCMP_MASK,	 PPCONLY,	{ BF, L, RA, RB } },
+{ "cmpl",    X(31,32),	XCMPL_MASK,	 PWRCOM,	{ BF, RA, RB } },
+
+{ "subf",    XO(31,40,0,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "sub",     XO(31,40,0,0), XO_MASK,	PPC,		{ RT, RB, RA } },
+{ "subf.",   XO(31,40,0,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "sub.",    XO(31,40,0,1), XO_MASK,	PPC,		{ RT, RB, RA } },
+{ "subfo",   XO(31,40,1,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "subo",    XO(31,40,1,0), XO_MASK,	PPC,		{ RT, RB, RA } },
+{ "subfo.",  XO(31,40,1,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "subo.",   XO(31,40,1,1), XO_MASK,	PPC,		{ RT, RB, RA } },
+
+{ "ldux",    X(31,53),	X_MASK,		PPC64,		{ RT, RAL, RB } },
+
+{ "dcbst",   X(31,54),	XRT_MASK,	PPC,		{ RA, RB } },
+
+{ "lwzux",   X(31,55),	X_MASK,		PPCCOM,		{ RT, RAL, RB } },
+{ "lux",     X(31,55),	X_MASK,		PWRCOM,		{ RT, RA, RB } },
+
+{ "cntlzd",  XRC(31,58,0), XRB_MASK,	PPC64,		{ RA, RS } },
+{ "cntlzd.", XRC(31,58,1), XRB_MASK,	PPC64,		{ RA, RS } },
+
+{ "andc",    XRC(31,60,0), X_MASK,	COM,	{ RA, RS, RB } },
+{ "andc.",   XRC(31,60,1), X_MASK,	COM,	{ RA, RS, RB } },
+
+{ "tdlgt",   XTO(31,68,TOLGT), XTO_MASK, PPC64,		{ RA, RB } },
+{ "tdllt",   XTO(31,68,TOLLT), XTO_MASK, PPC64,		{ RA, RB } },
+{ "tdeq",    XTO(31,68,TOEQ), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "tdlge",   XTO(31,68,TOLGE), XTO_MASK, PPC64,		{ RA, RB } },
+{ "tdlnl",   XTO(31,68,TOLNL), XTO_MASK, PPC64,		{ RA, RB } },
+{ "tdlle",   XTO(31,68,TOLLE), XTO_MASK, PPC64,		{ RA, RB } },
+{ "tdlng",   XTO(31,68,TOLNG), XTO_MASK, PPC64,		{ RA, RB } },
+{ "tdgt",    XTO(31,68,TOGT), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "tdge",    XTO(31,68,TOGE), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "tdnl",    XTO(31,68,TONL), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "tdlt",    XTO(31,68,TOLT), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "tdle",    XTO(31,68,TOLE), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "tdng",    XTO(31,68,TONG), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "tdne",    XTO(31,68,TONE), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "td",	     X(31,68),	X_MASK,		 PPC64,		{ TO, RA, RB } },
+
+{ "mulhd",   XO(31,73,0,0), XO_MASK,	 PPC64,		{ RT, RA, RB } },
+{ "mulhd.",  XO(31,73,0,1), XO_MASK,	 PPC64,		{ RT, RA, RB } },
+
+{ "mulhw",   XO(31,75,0,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "mulhw.",  XO(31,75,0,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+
+{ "mtsrd",   X(31,82),	XRB_MASK|(1<<20), PPC64,	{ SR, RS } },
+
+{ "mfmsr",   X(31,83),	XRARB_MASK,	COM,		{ RT } },
+
+{ "ldarx",   X(31,84),	X_MASK,		PPC64,		{ RT, RA, RB } },
+
+{ "dcbf",    X(31,86),	XRT_MASK,	PPC,		{ RA, RB } },
+
+{ "lbzx",    X(31,87),	X_MASK,		COM,		{ RT, RA, RB } },
+
+{ "neg",     XO(31,104,0,0), XORB_MASK,	COM,		{ RT, RA } },
+{ "neg.",    XO(31,104,0,1), XORB_MASK,	COM,		{ RT, RA } },
+{ "nego",    XO(31,104,1,0), XORB_MASK,	COM,		{ RT, RA } },
+{ "nego.",   XO(31,104,1,1), XORB_MASK,	COM,		{ RT, RA } },
+
+{ "mul",     XO(31,107,0,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "mul.",    XO(31,107,0,1), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "mulo",    XO(31,107,1,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "mulo.",   XO(31,107,1,1), XO_MASK,	M601,		{ RT, RA, RB } },
+
+{ "mtsrdin", X(31,114),	XRA_MASK,	PPC64,		{ RS, RB } },
+
+{ "clf",     X(31,118), XRB_MASK,	POWER,		{ RT, RA } },
+
+{ "lbzux",   X(31,119),	X_MASK,		COM,		{ RT, RAL, RB } },
+
+{ "not",     XRC(31,124,0), X_MASK,	COM,		{ RA, RS, RBS } },
+{ "nor",     XRC(31,124,0), X_MASK,	COM,		{ RA, RS, RB } },
+{ "not.",    XRC(31,124,1), X_MASK,	COM,		{ RA, RS, RBS } },
+{ "nor.",    XRC(31,124,1), X_MASK,	COM,		{ RA, RS, RB } },
+
+{ "wrtee",   X(31,131),	XRARB_MASK,	PPC403,		{ RS } },
+
+{ "subfe",   XO(31,136,0,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sfe",     XO(31,136,0,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "subfe.",  XO(31,136,0,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sfe.",    XO(31,136,0,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "subfeo",  XO(31,136,1,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sfeo",    XO(31,136,1,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "subfeo.", XO(31,136,1,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sfeo.",   XO(31,136,1,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+
+{ "adde",    XO(31,138,0,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "ae",      XO(31,138,0,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "adde.",   XO(31,138,0,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "ae.",     XO(31,138,0,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "addeo",   XO(31,138,1,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "aeo",     XO(31,138,1,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "addeo.",  XO(31,138,1,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "aeo.",    XO(31,138,1,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+
+{ "mtcr",    XFXM(31,144,0xff), XFXFXM_MASK|FXM_MASK, COM,	{ RS }},
+{ "mtcrf",   X(31,144),	XFXFXM_MASK,	COM,		{ FXM, RS } },
+
+{ "mtmsr",   X(31,146),	XRARB_MASK,	COM,		{ RS } },
+
+{ "stdx",    X(31,149), X_MASK,		PPC64,		{ RS, RA, RB } },
+
+{ "stwcx.",  XRC(31,150,1), X_MASK,	PPC,		{ RS, RA, RB } },
+
+{ "stwx",    X(31,151), X_MASK,		PPCCOM,		{ RS, RA, RB } },
+{ "stx",     X(31,151), X_MASK,		PWRCOM,		{ RS, RA, RB } },
+
+{ "slq",     XRC(31,152,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "slq.",    XRC(31,152,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "sle",     XRC(31,153,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "sle.",    XRC(31,153,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "wrteei",  X(31,163),	XE_MASK,	PPC403,		{ E } },
+
+{ "mtmsrd",  X(31,178),	XRARB_MASK,	PPC64,		{ RS } },
+
+{ "stdux",   X(31,181),	X_MASK,		PPC64,		{ RS, RAS, RB } },
+
+{ "stwux",   X(31,183),	X_MASK,		PPCCOM,		{ RS, RAS, RB } },
+{ "stux",    X(31,183),	X_MASK,		PWRCOM,		{ RS, RA, RB } },
+
+{ "sliq",    XRC(31,184,0), X_MASK,	M601,		{ RA, RS, SH } },
+{ "sliq.",   XRC(31,184,1), X_MASK,	M601,		{ RA, RS, SH } },
+
+{ "subfze",  XO(31,200,0,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfze",    XO(31,200,0,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "subfze.", XO(31,200,0,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfze.",   XO(31,200,0,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "subfzeo", XO(31,200,1,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfzeo",   XO(31,200,1,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "subfzeo.",XO(31,200,1,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfzeo.",  XO(31,200,1,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+
+{ "addze",   XO(31,202,0,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "aze",     XO(31,202,0,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "addze.",  XO(31,202,0,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "aze.",    XO(31,202,0,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "addzeo",  XO(31,202,1,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "azeo",    XO(31,202,1,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "addzeo.", XO(31,202,1,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "azeo.",   XO(31,202,1,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+
+{ "mtsr",    X(31,210),	XRB_MASK|(1<<20), COM32,	{ SR, RS } },
+
+{ "stdcx.",  XRC(31,214,1), X_MASK,	PPC64,		{ RS, RA, RB } },
+
+{ "stbx",    X(31,215),	X_MASK,		COM,	{ RS, RA, RB } },
+
+{ "sllq",    XRC(31,216,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "sllq.",   XRC(31,216,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "sleq",    XRC(31,217,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "sleq.",   XRC(31,217,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "subfme",  XO(31,232,0,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfme",    XO(31,232,0,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "subfme.", XO(31,232,0,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfme.",   XO(31,232,0,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "subfmeo", XO(31,232,1,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfmeo",   XO(31,232,1,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "subfmeo.",XO(31,232,1,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfmeo.",  XO(31,232,1,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+
+{ "mulld",   XO(31,233,0,0), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "mulld.",  XO(31,233,0,1), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "mulldo",  XO(31,233,1,0), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "mulldo.", XO(31,233,1,1), XO_MASK,	PPC64,		{ RT, RA, RB } },
+
+{ "addme",   XO(31,234,0,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "ame",     XO(31,234,0,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "addme.",  XO(31,234,0,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "ame.",    XO(31,234,0,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "addmeo",  XO(31,234,1,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "ameo",    XO(31,234,1,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "addmeo.", XO(31,234,1,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "ameo.",   XO(31,234,1,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+
+{ "mullw",   XO(31,235,0,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "muls",    XO(31,235,0,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "mullw.",  XO(31,235,0,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "muls.",   XO(31,235,0,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "mullwo",  XO(31,235,1,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "mulso",   XO(31,235,1,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "mullwo.", XO(31,235,1,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "mulso.",  XO(31,235,1,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+
+{ "mtsrin",  X(31,242),	XRA_MASK,	PPC32,		{ RS, RB } },
+{ "mtsri",   X(31,242),	XRA_MASK,	POWER32,	{ RS, RB } },
+
+{ "dcbtst",  X(31,246),	XRT_MASK,	PPC,		{ RA, RB } },
+
+{ "stbux",   X(31,247),	X_MASK,		COM,		{ RS, RAS, RB } },
+
+{ "slliq",   XRC(31,248,0), X_MASK,	M601,		{ RA, RS, SH } },
+{ "slliq.",  XRC(31,248,1), X_MASK,	M601,		{ RA, RS, SH } },
+
+{ "doz",     XO(31,264,0,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "doz.",    XO(31,264,0,1), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "dozo",    XO(31,264,1,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "dozo.",   XO(31,264,1,1), XO_MASK,	M601,		{ RT, RA, RB } },
+
+{ "add",     XO(31,266,0,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "cax",     XO(31,266,0,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "add.",    XO(31,266,0,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "cax.",    XO(31,266,0,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "addo",    XO(31,266,1,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "caxo",    XO(31,266,1,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "addo.",   XO(31,266,1,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "caxo.",   XO(31,266,1,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+
+{ "lscbx",   XRC(31,277,0), X_MASK,	M601,		{ RT, RA, RB } },
+{ "lscbx.",  XRC(31,277,1), X_MASK,	M601,		{ RT, RA, RB } },
+
+{ "dcbt",    X(31,278),	XRT_MASK,	PPC,		{ RA, RB } },
+
+{ "lhzx",    X(31,279),	X_MASK,		COM,		{ RT, RA, RB } },
+
+{ "icbt",    X(31,262),	XRT_MASK,	PPC403,		{ RA, RB } },
+
+{ "eqv",     XRC(31,284,0), X_MASK,	COM,		{ RA, RS, RB } },
+{ "eqv.",    XRC(31,284,1), X_MASK,	COM,		{ RA, RS, RB } },
+
+{ "tlbie",   X(31,306),	XRTRA_MASK,	PPC,		{ RB } },
+{ "tlbi",    X(31,306),	XRT_MASK,	POWER,		{ RA, RB } },
+
+{ "eciwx",   X(31,310), X_MASK,		PPC,		{ RT, RA, RB } },
+
+{ "lhzux",   X(31,311),	X_MASK,		COM,		{ RT, RAL, RB } },
+
+{ "xor",     XRC(31,316,0), X_MASK,	COM,		{ RA, RS, RB } },
+{ "xor.",    XRC(31,316,1), X_MASK,	COM,		{ RA, RS, RB } },
+
+{ "mfexisr", XSPR(31,323,64), XSPR_MASK, PPC403,	{ RT } },
+{ "mfexier", XSPR(31,323,66), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr0",   XSPR(31,323,128), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr1",   XSPR(31,323,129), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr2",   XSPR(31,323,130), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr3",   XSPR(31,323,131), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr4",   XSPR(31,323,132), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr5",   XSPR(31,323,133), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr6",   XSPR(31,323,134), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr7",   XSPR(31,323,135), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbear",  XSPR(31,323,144), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbesr",  XSPR(31,323,145), XSPR_MASK, PPC403,	{ RT } },
+{ "mfiocr",  XSPR(31,323,160), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacr0", XSPR(31,323,192), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmact0", XSPR(31,323,193), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmada0", XSPR(31,323,194), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmasa0", XSPR(31,323,195), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacc0", XSPR(31,323,196), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacr1", XSPR(31,323,200), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmact1", XSPR(31,323,201), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmada1", XSPR(31,323,202), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmasa1", XSPR(31,323,203), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacc1", XSPR(31,323,204), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacr2", XSPR(31,323,208), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmact2", XSPR(31,323,209), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmada2", XSPR(31,323,210), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmasa2", XSPR(31,323,211), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacc2", XSPR(31,323,212), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacr3", XSPR(31,323,216), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmact3", XSPR(31,323,217), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmada3", XSPR(31,323,218), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmasa3", XSPR(31,323,219), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacc3", XSPR(31,323,220), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmasr", XSPR(31,323,224), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdcr",   X(31,323),	X_MASK,		PPC403,		{ RT, SPR } },
+
+{ "div",     XO(31,331,0,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "div.",    XO(31,331,0,1), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "divo",    XO(31,331,1,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "divo.",   XO(31,331,1,1), XO_MASK,	M601,		{ RT, RA, RB } },
+
+{ "mfmq",     XSPR(31,339,0),   XSPR_MASK, M601,	{ RT } },
+{ "mfxer",    XSPR(31,339,1),   XSPR_MASK, COM,		{ RT } },
+{ "mfrtcu",   XSPR(31,339,4),   XSPR_MASK, COM,		{ RT } },
+{ "mfrtcl",   XSPR(31,339,5),   XSPR_MASK, COM,		{ RT } },
+{ "mfdec",    XSPR(31,339,6),   XSPR_MASK, MFDEC1,	{ RT } },
+{ "mflr",     XSPR(31,339,8),   XSPR_MASK, COM,		{ RT } },
+{ "mfctr",    XSPR(31,339,9),   XSPR_MASK, COM,		{ RT } },
+{ "mftid",    XSPR(31,339,17),  XSPR_MASK, POWER,	{ RT } },
+{ "mfdsisr",  XSPR(31,339,18),  XSPR_MASK, COM,		{ RT } },
+{ "mfdar",    XSPR(31,339,19),  XSPR_MASK, COM,		{ RT } },
+{ "mfdec",    XSPR(31,339,22),  XSPR_MASK, MFDEC2,	{ RT } },
+{ "mfsdr0",   XSPR(31,339,24),  XSPR_MASK, POWER,	{ RT } },
+{ "mfsdr1",   XSPR(31,339,25),  XSPR_MASK, COM,		{ RT } },
+{ "mfsrr0",   XSPR(31,339,26),  XSPR_MASK, COM,		{ RT } },
+{ "mfsrr1",   XSPR(31,339,27),  XSPR_MASK, COM,		{ RT } },
+{ "mfcmpa",   XSPR(31,339,144), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcmpb",   XSPR(31,339,145), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcmpc",   XSPR(31,339,146), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcmpd",   XSPR(31,339,147), XSPR_MASK, PPC860,	{ RT } },
+{ "mficr",    XSPR(31,339,148), XSPR_MASK, PPC860,	{ RT } },
+{ "mfder",    XSPR(31,339,149), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcounta", XSPR(31,339,150), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcountb", XSPR(31,339,151), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcmpe",   XSPR(31,339,152), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcmpf",   XSPR(31,339,153), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcmpg",   XSPR(31,339,154), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcmph",   XSPR(31,339,155), XSPR_MASK, PPC860,	{ RT } },
+{ "mflctrl1", XSPR(31,339,156), XSPR_MASK, PPC860,	{ RT } },
+{ "mflctrl2", XSPR(31,339,157), XSPR_MASK, PPC860,	{ RT } },
+{ "mfictrl",  XSPR(31,339,158), XSPR_MASK, PPC860,	{ RT } },
+{ "mfbar",    XSPR(31,339,159), XSPR_MASK, PPC860,	{ RT } },
+{ "mfsprg4",  XSPR(31,339,260), XSPR_MASK, PPC405,	{ RT } },
+{ "mfsprg5",  XSPR(31,339,261), XSPR_MASK, PPC405,	{ RT } },
+{ "mfsprg6",  XSPR(31,339,262), XSPR_MASK, PPC405,	{ RT } },
+{ "mfsprg7",  XSPR(31,339,263), XSPR_MASK, PPC405,	{ RT } },
+{ "mfsprg",   XSPR(31,339,272), XSPRG_MASK, PPC,	{ RT, SPRG } },
+{ "mfsprg0",  XSPR(31,339,272), XSPR_MASK, PPC,		{ RT } },
+{ "mfsprg1",  XSPR(31,339,273), XSPR_MASK, PPC,		{ RT } },
+{ "mfsprg2",  XSPR(31,339,274), XSPR_MASK, PPC,		{ RT } },
+{ "mfsprg3",  XSPR(31,339,275), XSPR_MASK, PPC,		{ RT } },
+{ "mfasr",    XSPR(31,339,280), XSPR_MASK, PPC64,	{ RT } },
+{ "mfear",    XSPR(31,339,282), XSPR_MASK, PPC,		{ RT } },
+{ "mfpvr",    XSPR(31,339,287), XSPR_MASK, PPC,		{ RT } },
+{ "mfibatu",  XSPR(31,339,528), XSPRBAT_MASK, PPC,	{ RT, SPRBAT } },
+{ "mfibatl",  XSPR(31,339,529), XSPRBAT_MASK, PPC,	{ RT, SPRBAT } },
+{ "mfdbatu",  XSPR(31,339,536), XSPRBAT_MASK, PPC,	{ RT, SPRBAT } },
+{ "mfdbatl",  XSPR(31,339,537), XSPRBAT_MASK, PPC,	{ RT, SPRBAT } },
+{ "mfic_cst", XSPR(31,339,560), XSPR_MASK, PPC860,	{ RT } },
+{ "mfic_adr", XSPR(31,339,561), XSPR_MASK, PPC860,	{ RT } },
+{ "mfic_dat", XSPR(31,339,562), XSPR_MASK, PPC860,	{ RT } },
+{ "mfdc_cst", XSPR(31,339,568), XSPR_MASK, PPC860,	{ RT } },
+{ "mfdc_adr", XSPR(31,339,569), XSPR_MASK, PPC860,	{ RT } },
+{ "mfdc_dat", XSPR(31,339,570), XSPR_MASK, PPC860,	{ RT } },
+{ "mfdpdr",   XSPR(31,339,630), XSPR_MASK, PPC860,	{ RT } },
+{ "mfdpir",   XSPR(31,339,631), XSPR_MASK, PPC860,	{ RT } },
+{ "mfimmr",   XSPR(31,339,638), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_ctr", XSPR(31,339,784), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_ap",  XSPR(31,339,786), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_epn", XSPR(31,339,787), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_twc", XSPR(31,339,789), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_rpn", XSPR(31,339,790), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_ctr", XSPR(31,339,792), XSPR_MASK, PPC860,	{ RT } },
+{ "mfm_casid",XSPR(31,339,793), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_ap",  XSPR(31,339,794), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_epn", XSPR(31,339,795), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_twb", XSPR(31,339,796), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_twc", XSPR(31,339,797), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_rpn", XSPR(31,339,798), XSPR_MASK, PPC860,	{ RT } },
+{ "mfm_tw",   XSPR(31,339,799), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_dbcam",XSPR(31,339,816), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_dbram0",XSPR(31,339,817), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_dbram1",XSPR(31,339,818), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_dbcam", XSPR(31,339,824), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_dbram0",XSPR(31,339,825), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_dbram1",XSPR(31,339,826), XSPR_MASK, PPC860,	{ RT } },
+{ "mfzpr",   	XSPR(31,339,944), XSPR_MASK, PPC403,	{ RT } },
+{ "mfpid",   	XSPR(31,339,945), XSPR_MASK, PPC403,	{ RT } },
+{ "mfccr0",  	XSPR(31,339,947), XSPR_MASK, PPC405,	{ RT } },
+{ "mficdbdr",	XSPR(31,339,979), XSPR_MASK, PPC403,	{ RT } },
+{ "mfummcr0",	XSPR(31,339,936),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfupmc1",	XSPR(31,339,937),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfupmc2",	XSPR(31,339,938),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfusia",	XSPR(31,339,939),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfummcr1",	XSPR(31,339,940),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfupmc3",	XSPR(31,339,941),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfupmc4",	XSPR(31,339,942),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfiac3",     XSPR(31,339,948),  XSPR_MASK, PPC405,	{ RT } },
+{ "mfiac4",     XSPR(31,339,949),  XSPR_MASK, PPC405,	{ RT } },
+{ "mfdvc1",     XSPR(31,339,950),  XSPR_MASK, PPC405,	{ RT } },
+{ "mfdvc2",     XSPR(31,339,951),  XSPR_MASK, PPC405,	{ RT } },
+{ "mfmmcr0",	XSPR(31,339,952),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfpmc1",	XSPR(31,339,953),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfsgr",	XSPR(31,339,953),  XSPR_MASK, PPC403,	{ RT } },
+{ "mfpmc2",	XSPR(31,339,954),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfdcwr", 	XSPR(31,339,954),  XSPR_MASK, PPC403,	{ RT } },
+{ "mfsia",	XSPR(31,339,955),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfsler",	XSPR(31,339,955),  XSPR_MASK, PPC405,	{ RT } },
+{ "mfmmcr1",	XSPR(31,339,956),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfsu0r",	XSPR(31,339,956),  XSPR_MASK, PPC405,	{ RT } },
+{ "mfpmc3",	XSPR(31,339,957),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfdbcr1", 	XSPR(31,339,957),  XSPR_MASK, PPC405,	{ RT } },
+{ "mfpmc4",	XSPR(31,339,958),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfesr",   XSPR(31,339,980), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdear",  XSPR(31,339,981), XSPR_MASK, PPC403,	{ RT } },
+{ "mfevpr",  XSPR(31,339,982), XSPR_MASK, PPC403,	{ RT } },
+{ "mfcdbcr", XSPR(31,339,983), XSPR_MASK, PPC403,	{ RT } },
+{ "mftsr",   XSPR(31,339,984), XSPR_MASK, PPC403,	{ RT } },
+{ "mftcr",   XSPR(31,339,986), XSPR_MASK, PPC403,	{ RT } },
+{ "mfpit",   XSPR(31,339,987), XSPR_MASK, PPC403,	{ RT } },
+{ "mftbhi",  XSPR(31,339,988), XSPR_MASK, PPC403,	{ RT } },
+{ "mftblo",  XSPR(31,339,989), XSPR_MASK, PPC403,	{ RT } },
+{ "mfsrr2",  XSPR(31,339,990), XSPR_MASK, PPC403,	{ RT } },
+{ "mfsrr3",  XSPR(31,339,991), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdbsr",  XSPR(31,339,1008), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdbcr0", XSPR(31,339,1010), XSPR_MASK, PPC405,	{ RT } },
+{ "mfiac1",  XSPR(31,339,1012), XSPR_MASK, PPC403,	{ RT } },
+{ "mfiac2",  XSPR(31,339,1013), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdac1",  XSPR(31,339,1014), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdac2",  XSPR(31,339,1015), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdccr",  XSPR(31,339,1018), XSPR_MASK, PPC403,	{ RT } },
+{ "mficcr",  XSPR(31,339,1019), XSPR_MASK, PPC403,	{ RT } },
+{ "mfpbl1",  XSPR(31,339,1020), XSPR_MASK, PPC403,	{ RT } },
+{ "mfpbu1",  XSPR(31,339,1021), XSPR_MASK, PPC403,	{ RT } },
+{ "mfpbl2",  XSPR(31,339,1022), XSPR_MASK, PPC403,	{ RT } },
+{ "mfpbu2",  XSPR(31,339,1023), XSPR_MASK, PPC403,	{ RT } },
+{ "mfl2cr",	XSPR(31,339,1017), XSPR_MASK, PPC750,	{ RT } },
+{ "mfictc",	XSPR(31,339,1019), XSPR_MASK, PPC750,	{ RT } },
+{ "mfthrm1",	XSPR(31,339,1020), XSPR_MASK, PPC750,	{ RT } },
+{ "mfthrm2",	XSPR(31,339,1021), XSPR_MASK, PPC750,	{ RT } },
+{ "mfthrm3",	XSPR(31,339,1022), XSPR_MASK, PPC750,	{ RT } },
+{ "mfspr",   X(31,339),	X_MASK,		COM,		{ RT, SPR } },
+
+{ "lwax",    X(31,341),	X_MASK,		PPC64,		{ RT, RA, RB } },
+
+{ "lhax",    X(31,343),	X_MASK,		COM,		{ RT, RA, RB } },
+
+{ "dccci",   X(31,454),	XRT_MASK,	PPC403,		{ RA, RB } },
+
+{ "abs",     XO(31,360,0,0), XORB_MASK, M601,		{ RT, RA } },
+{ "abs.",    XO(31,360,0,1), XORB_MASK, M601,		{ RT, RA } },
+{ "abso",    XO(31,360,1,0), XORB_MASK, M601,		{ RT, RA } },
+{ "abso.",   XO(31,360,1,1), XORB_MASK, M601,		{ RT, RA } },
+
+{ "divs",    XO(31,363,0,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "divs.",   XO(31,363,0,1), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "divso",   XO(31,363,1,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "divso.",  XO(31,363,1,1), XO_MASK,	M601,		{ RT, RA, RB } },
+
+{ "tlbia",   X(31,370),	0xffffffff,	PPC,		{ 0 } },
+
+{ "mftbl",   XSPR(31,371,268), XSPR_MASK, PPC,		{ RT } },
+{ "mftbu",   XSPR(31,371,269), XSPR_MASK, PPC,		{ RT } },
+{ "mftb",    X(31,371),	X_MASK,		PPC,		{ RT, TBR } },
+
+{ "lwaux",   X(31,373),	X_MASK,		PPC64,		{ RT, RAL, RB } },
+
+{ "lhaux",   X(31,375),	X_MASK,		COM,		{ RT, RAL, RB } },
+
+{ "sthx",    X(31,407),	X_MASK,		COM,		{ RS, RA, RB } },
+
+{ "lfqx",    X(31,791),	X_MASK,		POWER2,		{ FRT, RA, RB } },
+
+{ "lfqux",   X(31,823),	X_MASK,		POWER2,		{ FRT, RA, RB } },
+
+{ "stfqx",   X(31,919),	X_MASK,		POWER2,		{ FRS, RA, RB } },
+
+{ "stfqux",  X(31,951),	X_MASK,		POWER2,		{ FRS, RA, RB } },
+
+{ "orc",     XRC(31,412,0), X_MASK,	COM,		{ RA, RS, RB } },
+{ "orc.",    XRC(31,412,1), X_MASK,	COM,		{ RA, RS, RB } },
+
+{ "sradi",   XS(31,413,0), XS_MASK,	PPC64,		{ RA, RS, SH6 } },
+{ "sradi.",  XS(31,413,1), XS_MASK,	PPC64,		{ RA, RS, SH6 } },
+
+{ "slbie",   X(31,434),	XRTRA_MASK,	PPC64,		{ RB } },
+
+{ "ecowx",   X(31,438),	X_MASK,		PPC,		{ RT, RA, RB } },
+
+{ "sthux",   X(31,439),	X_MASK,		COM,		{ RS, RAS, RB } },
+
+{ "mr",	     XRC(31,444,0), X_MASK,	COM,		{ RA, RS, RBS } },
+{ "or",      XRC(31,444,0), X_MASK,	COM,		{ RA, RS, RB } },
+{ "mr.",     XRC(31,444,1), X_MASK,	COM,		{ RA, RS, RBS } },
+{ "or.",     XRC(31,444,1), X_MASK,	COM,		{ RA, RS, RB } },
+
+{ "mtexisr", XSPR(31,451,64), XSPR_MASK, PPC403,	{ RT } },
+{ "mtexier", XSPR(31,451,66), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr0",   XSPR(31,451,128), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr1",   XSPR(31,451,129), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr2",   XSPR(31,451,130), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr3",   XSPR(31,451,131), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr4",   XSPR(31,451,132), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr5",   XSPR(31,451,133), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr6",   XSPR(31,451,134), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr7",   XSPR(31,451,135), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbear",  XSPR(31,451,144), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbesr",  XSPR(31,451,145), XSPR_MASK, PPC403,	{ RT } },
+{ "mtiocr",  XSPR(31,451,160), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacr0", XSPR(31,451,192), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmact0", XSPR(31,451,193), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmada0", XSPR(31,451,194), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmasa0", XSPR(31,451,195), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacc0", XSPR(31,451,196), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacr1", XSPR(31,451,200), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmact1", XSPR(31,451,201), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmada1", XSPR(31,451,202), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmasa1", XSPR(31,451,203), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacc1", XSPR(31,451,204), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacr2", XSPR(31,451,208), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmact2", XSPR(31,451,209), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmada2", XSPR(31,451,210), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmasa2", XSPR(31,451,211), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacc2", XSPR(31,451,212), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacr3", XSPR(31,451,216), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmact3", XSPR(31,451,217), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmada3", XSPR(31,451,218), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmasa3", XSPR(31,451,219), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacc3", XSPR(31,451,220), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmasr", XSPR(31,451,224), XSPR_MASK, PPC403,	{ RT } },
+{ "mtummcr0",	XSPR(31,451,936),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtupmc1",	XSPR(31,451,937),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtupmc2",	XSPR(31,451,938),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtusia",	XSPR(31,451,939),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtummcr1",	XSPR(31,451,940),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtupmc3",	XSPR(31,451,941),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtupmc4",	XSPR(31,451,942),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtmmcr0",	XSPR(31,451,952),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtpmc1",	XSPR(31,451,953),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtpmc2",	XSPR(31,451,954),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtsia",	XSPR(31,451,955),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtmmcr1",	XSPR(31,451,956),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtpmc3",	XSPR(31,451,957),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtpmc4",	XSPR(31,451,958),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtl2cr",	XSPR(31,451,1017), XSPR_MASK, PPC750,	{ RT } },
+{ "mtictc",	XSPR(31,451,1019), XSPR_MASK, PPC750,	{ RT } },
+{ "mtthrm1",	XSPR(31,451,1020), XSPR_MASK, PPC750,	{ RT } },
+{ "mtthrm2",	XSPR(31,451,1021), XSPR_MASK, PPC750,	{ RT } },
+{ "mtthrm3",	XSPR(31,451,1022), XSPR_MASK, PPC750,	{ RT } },
+{ "mtdcr",   X(31,451),	X_MASK,		PPC403,		{ SPR, RS } },
+
+{ "divdu",   XO(31,457,0,0), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "divdu.",  XO(31,457,0,1), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "divduo",  XO(31,457,1,0), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "divduo.", XO(31,457,1,1), XO_MASK,	PPC64,		{ RT, RA, RB } },
+
+{ "divwu",   XO(31,459,0,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "divwu.",  XO(31,459,0,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "divwuo",  XO(31,459,1,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "divwuo.", XO(31,459,1,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+
+{ "mtmq",    XSPR(31,467,0),   XSPR_MASK,    M601,	{ RS } },
+{ "mtxer",   XSPR(31,467,1),   XSPR_MASK,    COM,	{ RS } },
+{ "mtlr",    XSPR(31,467,8),   XSPR_MASK,    COM,	{ RS } },
+{ "mtctr",   XSPR(31,467,9),   XSPR_MASK,    COM,	{ RS } },
+{ "mttid",   XSPR(31,467,17),  XSPR_MASK,    POWER,	{ RS } },
+{ "mtdsisr", XSPR(31,467,18),  XSPR_MASK,    COM,	{ RS } },
+{ "mtdar",   XSPR(31,467,19),  XSPR_MASK,    COM,	{ RS } },
+{ "mtrtcu",  XSPR(31,467,20),  XSPR_MASK,    COM,	{ RS } },
+{ "mtrtcl",  XSPR(31,467,21),  XSPR_MASK,    COM,	{ RS } },
+{ "mtdec",   XSPR(31,467,22),  XSPR_MASK,    COM,	{ RS } },
+{ "mtsdr0",  XSPR(31,467,24),  XSPR_MASK,    POWER,	{ RS } },
+{ "mtsdr1",  XSPR(31,467,25),  XSPR_MASK,    COM,	{ RS } },
+{ "mtsrr0",  XSPR(31,467,26),  XSPR_MASK,    COM,	{ RS } },
+{ "mtsrr1",  XSPR(31,467,27),  XSPR_MASK,    COM,	{ RS } },
+{ "mtcmpa",   XSPR(31,467,144), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcmpb",   XSPR(31,467,145), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcmpc",   XSPR(31,467,146), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcmpd",   XSPR(31,467,147), XSPR_MASK, PPC860,	{ RT } },
+{ "mticr",    XSPR(31,467,148), XSPR_MASK, PPC860,	{ RT } },
+{ "mtder",    XSPR(31,467,149), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcounta", XSPR(31,467,150), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcountb", XSPR(31,467,151), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcmpe",   XSPR(31,467,152), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcmpf",   XSPR(31,467,153), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcmpg",   XSPR(31,467,154), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcmph",   XSPR(31,467,155), XSPR_MASK, PPC860,	{ RT } },
+{ "mtlctrl1", XSPR(31,467,156), XSPR_MASK, PPC860,	{ RT } },
+{ "mtlctrl2", XSPR(31,467,157), XSPR_MASK, PPC860,	{ RT } },
+{ "mtictrl",  XSPR(31,467,158), XSPR_MASK, PPC860,	{ RT } },
+{ "mtbar",    XSPR(31,467,159), XSPR_MASK, PPC860,	{ RT } },
+{ "mtsprg",  XSPR(31,467,272), XSPRG_MASK,   PPC,	{ SPRG, RS } },
+{ "mtsprg0", XSPR(31,467,272), XSPR_MASK,    PPC,	{ RT } },
+{ "mtsprg1", XSPR(31,467,273), XSPR_MASK,    PPC,	{ RT } },
+{ "mtsprg2", XSPR(31,467,274), XSPR_MASK,    PPC,	{ RT } },
+{ "mtsprg3", XSPR(31,467,275), XSPR_MASK,    PPC,	{ RT } },
+{ "mtsprg4", XSPR(31,467,276), XSPR_MASK,    PPC405,	{ RT } },
+{ "mtsprg5", XSPR(31,467,277), XSPR_MASK,    PPC405,	{ RT } },
+{ "mtsprg6", XSPR(31,467,278), XSPR_MASK,    PPC405,	{ RT } },
+{ "mtsprg7", XSPR(31,467,279), XSPR_MASK,    PPC405,	{ RT } },
+{ "mtasr",   XSPR(31,467,280), XSPR_MASK,    PPC64,	{ RS } },
+{ "mtear",   XSPR(31,467,282), XSPR_MASK,    PPC,	{ RS } },
+{ "mttbl",   XSPR(31,467,284), XSPR_MASK,    PPC,	{ RS } },
+{ "mttbu",   XSPR(31,467,285), XSPR_MASK,    PPC,	{ RS } },
+{ "mtibatu", XSPR(31,467,528), XSPRBAT_MASK, PPC,	{ SPRBAT, RS } },
+{ "mtibatl", XSPR(31,467,529), XSPRBAT_MASK, PPC,	{ SPRBAT, RS } },
+{ "mtdbatu", XSPR(31,467,536), XSPRBAT_MASK, PPC,	{ SPRBAT, RS } },
+{ "mtdbatl", XSPR(31,467,537), XSPRBAT_MASK, PPC,	{ SPRBAT, RS } },
+{ "mtzpr",   XSPR(31,467,944), XSPR_MASK, PPC403,	{ RT } },
+{ "mtpid",   XSPR(31,467,945), XSPR_MASK, PPC403,	{ RT } },
+{ "mtccr0",  XSPR(31,467,947), XSPR_MASK, PPC405,	{ RT } },
+{ "mtiac3",  XSPR(31,467,948), XSPR_MASK, PPC405,	{ RT } },
+{ "mtiac4",  XSPR(31,467,949), XSPR_MASK, PPC405,	{ RT } },
+{ "mtdvc1",  XSPR(31,467,950), XSPR_MASK, PPC405,	{ RT } },
+{ "mtdvc2",  XSPR(31,467,951), XSPR_MASK, PPC405,	{ RT } },
+{ "mtsgr",   XSPR(31,467,953), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdcwr",  XSPR(31,467,954), XSPR_MASK, PPC403,	{ RT } },
+{ "mtsler",  XSPR(31,467,955), XSPR_MASK, PPC405,	{ RT } },
+{ "mtsu0r",  XSPR(31,467,956), XSPR_MASK, PPC405,	{ RT } },
+{ "mtdbcr1", XSPR(31,467,957), XSPR_MASK, PPC405,	{ RT } },
+{ "mticdbdr",XSPR(31,467,979), XSPR_MASK, PPC403,	{ RT } },
+{ "mtesr",   XSPR(31,467,980), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdear",  XSPR(31,467,981), XSPR_MASK, PPC403,	{ RT } },
+{ "mtevpr",  XSPR(31,467,982), XSPR_MASK, PPC403,	{ RT } },
+{ "mtcdbcr", XSPR(31,467,983), XSPR_MASK, PPC403,	{ RT } },
+{ "mttsr",   XSPR(31,467,984), XSPR_MASK, PPC403,	{ RT } },
+{ "mttcr",   XSPR(31,467,986), XSPR_MASK, PPC403,	{ RT } },
+{ "mtpit",   XSPR(31,467,987), XSPR_MASK, PPC403,	{ RT } },
+{ "mttbhi",  XSPR(31,467,988), XSPR_MASK, PPC403,	{ RT } },
+{ "mttblo",  XSPR(31,467,989), XSPR_MASK, PPC403,	{ RT } },
+{ "mtsrr2",  XSPR(31,467,990), XSPR_MASK, PPC403,	{ RT } },
+{ "mtsrr3",  XSPR(31,467,991), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdbsr",  XSPR(31,467,1008), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdbcr0", XSPR(31,467,1010), XSPR_MASK, PPC405,	{ RT } },
+{ "mtiac1",  XSPR(31,467,1012), XSPR_MASK, PPC403,	{ RT } },
+{ "mtiac2",  XSPR(31,467,1013), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdac1",  XSPR(31,467,1014), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdac2",  XSPR(31,467,1015), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdccr",  XSPR(31,467,1018), XSPR_MASK, PPC403,	{ RT } },
+{ "mticcr",  XSPR(31,467,1019), XSPR_MASK, PPC403,	{ RT } },
+{ "mtpbl1",  XSPR(31,467,1020), XSPR_MASK, PPC403,	{ RT } },
+{ "mtpbu1",  XSPR(31,467,1021), XSPR_MASK, PPC403,	{ RT } },
+{ "mtpbl2",  XSPR(31,467,1022), XSPR_MASK, PPC403,	{ RT } },
+{ "mtpbu2",  XSPR(31,467,1023), XSPR_MASK, PPC403,	{ RT } },
+{ "mtspr",   X(31,467),	       X_MASK,	     COM,	{ SPR, RS } },
+
+{ "dcbi",    X(31,470),	XRT_MASK,	PPC,		{ RA, RB } },
+
+{ "nand",    XRC(31,476,0), X_MASK,	COM,		{ RA, RS, RB } },
+{ "nand.",   XRC(31,476,1), X_MASK,	COM,		{ RA, RS, RB } },
+
+{ "dcread",  X(31,486),	X_MASK,		PPC403,		{ RT, RA, RB }},
+
+{ "nabs",    XO(31,488,0,0), XORB_MASK, M601,		{ RT, RA } },
+{ "nabs.",   XO(31,488,0,1), XORB_MASK, M601,		{ RT, RA } },
+{ "nabso",   XO(31,488,1,0), XORB_MASK, M601,		{ RT, RA } },
+{ "nabso.",  XO(31,488,1,1), XORB_MASK, M601,		{ RT, RA } },
+
+{ "divd",    XO(31,489,0,0), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "divd.",   XO(31,489,0,1), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "divdo",   XO(31,489,1,0), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "divdo.",  XO(31,489,1,1), XO_MASK,	PPC64,		{ RT, RA, RB } },
+
+{ "divw",    XO(31,491,0,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "divw.",   XO(31,491,0,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "divwo",   XO(31,491,1,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "divwo.",  XO(31,491,1,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+
+{ "slbia",   X(31,498),	0xffffffff,	PPC64,		{ 0 } },
+
+{ "cli",     X(31,502), XRB_MASK,	POWER,		{ RT, RA } },
+
+{ "mcrxr",   X(31,512),	XRARB_MASK|(3<<21), COM,	{ BF } },
+
+{ "clcs",    X(31,531), XRB_MASK,	M601,		{ RT, RA } },
+
+{ "lswx",    X(31,533),	X_MASK,		PPCCOM,		{ RT, RA, RB } },
+{ "lsx",     X(31,533),	X_MASK,		PWRCOM,		{ RT, RA, RB } },
+
+{ "lwbrx",   X(31,534),	X_MASK,		PPCCOM,		{ RT, RA, RB } },
+{ "lbrx",    X(31,534),	X_MASK,		PWRCOM,		{ RT, RA, RB } },
+
+{ "lfsx",    X(31,535),	X_MASK,		COM,		{ FRT, RA, RB } },
+
+{ "srw",     XRC(31,536,0), X_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "sr",      XRC(31,536,0), X_MASK,	PWRCOM,		{ RA, RS, RB } },
+{ "srw.",    XRC(31,536,1), X_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "sr.",     XRC(31,536,1), X_MASK,	PWRCOM,		{ RA, RS, RB } },
+
+{ "rrib",    XRC(31,537,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "rrib.",   XRC(31,537,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "srd",     XRC(31,539,0), X_MASK,	PPC64,		{ RA, RS, RB } },
+{ "srd.",    XRC(31,539,1), X_MASK,	PPC64,		{ RA, RS, RB } },
+
+{ "maskir",  XRC(31,541,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "maskir.", XRC(31,541,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "tlbsync", X(31,566),	0xffffffff,	PPC,		{ 0 } },
+
+{ "lfsux",   X(31,567),	X_MASK,		COM,		{ FRT, RAS, RB } },
+
+{ "mfsr",    X(31,595),	XRB_MASK|(1<<20), COM32,	{ RT, SR } },
+
+{ "lswi",    X(31,597),	X_MASK,		PPCCOM,		{ RT, RA, NB } },
+{ "lsi",     X(31,597),	X_MASK,		PWRCOM,		{ RT, RA, NB } },
+
+{ "sync",    X(31,598), 0xffffffff,	PPCCOM,		{ 0 } },
+{ "dcs",     X(31,598), 0xffffffff,	PWRCOM,		{ 0 } },
+
+{ "lfdx",    X(31,599), X_MASK,		COM,		{ FRT, RA, RB } },
+
+{ "mfsri",   X(31,627), X_MASK,		PWRCOM,		{ RT, RA, RB } },
+
+{ "dclst",   X(31,630), XRB_MASK,	PWRCOM,		{ RS, RA } },
+
+{ "lfdux",   X(31,631), X_MASK,		COM,		{ FRT, RAS, RB } },
+
+{ "mfsrin",  X(31,659), XRA_MASK,	PPC32,		{ RT, RB } },
+
+{ "stswx",   X(31,661), X_MASK,		PPCCOM,		{ RS, RA, RB } },
+{ "stsx",    X(31,661), X_MASK,		PWRCOM,		{ RS, RA, RB } },
+
+{ "stwbrx",  X(31,662), X_MASK,		PPCCOM,		{ RS, RA, RB } },
+{ "stbrx",   X(31,662), X_MASK,		PWRCOM,		{ RS, RA, RB } },
+
+{ "stfsx",   X(31,663), X_MASK,		COM,		{ FRS, RA, RB } },
+
+{ "srq",     XRC(31,664,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "srq.",    XRC(31,664,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "sre",     XRC(31,665,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "sre.",    XRC(31,665,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "stfsux",  X(31,695),	X_MASK,		COM,		{ FRS, RAS, RB } },
+
+{ "sriq",    XRC(31,696,0), X_MASK,	M601,		{ RA, RS, SH } },
+{ "sriq.",   XRC(31,696,1), X_MASK,	M601,		{ RA, RS, SH } },
+
+{ "stswi",   X(31,725),	X_MASK,		PPCCOM,		{ RS, RA, NB } },
+{ "stsi",    X(31,725),	X_MASK,		PWRCOM,		{ RS, RA, NB } },
+
+{ "stfdx",   X(31,727),	X_MASK,		COM,		{ FRS, RA, RB } },
+
+{ "srlq",    XRC(31,728,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "srlq.",   XRC(31,728,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "sreq",    XRC(31,729,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "sreq.",   XRC(31,729,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "dcba",    X(31,758),	XRT_MASK,	PPC405,		{ RA, RB } },
+
+{ "stfdux",  X(31,759),	X_MASK,		COM,		{ FRS, RAS, RB } },
+
+{ "srliq",   XRC(31,760,0), X_MASK,	M601,		{ RA, RS, SH } },
+{ "srliq.",  XRC(31,760,1), X_MASK,	M601,		{ RA, RS, SH } },
+
+{ "lhbrx",   X(31,790),	X_MASK,		COM,		{ RT, RA, RB } },
+
+{ "sraw",    XRC(31,792,0), X_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "sra",     XRC(31,792,0), X_MASK,	PWRCOM,		{ RA, RS, RB } },
+{ "sraw.",   XRC(31,792,1), X_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "sra.",    XRC(31,792,1), X_MASK,	PWRCOM,		{ RA, RS, RB } },
+
+{ "srad",    XRC(31,794,0), X_MASK,	PPC64,		{ RA, RS, RB } },
+{ "srad.",   XRC(31,794,1), X_MASK,	PPC64,		{ RA, RS, RB } },
+
+{ "rac",     X(31,818),	X_MASK,		PWRCOM,		{ RT, RA, RB } },
+
+{ "srawi",   XRC(31,824,0), X_MASK,	PPCCOM,		{ RA, RS, SH } },
+{ "srai",    XRC(31,824,0), X_MASK,	PWRCOM,		{ RA, RS, SH } },
+{ "srawi.",  XRC(31,824,1), X_MASK,	PPCCOM,		{ RA, RS, SH } },
+{ "srai.",   XRC(31,824,1), X_MASK,	PWRCOM,		{ RA, RS, SH } },
+
+{ "eieio",   X(31,854),	0xffffffff,	PPC,		{ 0 } },
+
+{ "tlbsx",   XRC(31,914,0), X_MASK, PPC403,	{ RT, RA, RB } },
+{ "tlbsx.",  XRC(31,914,1), X_MASK, PPC403,	{ RT, RA, RB } },
+
+{ "sthbrx",  X(31,918),	X_MASK,		COM,		{ RS, RA, RB } },
+
+{ "sraq",    XRC(31,920,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "sraq.",   XRC(31,920,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "srea",    XRC(31,921,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "srea.",   XRC(31,921,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "extsh",   XRC(31,922,0), XRB_MASK,	PPCCOM,		{ RA, RS } },
+{ "exts",    XRC(31,922,0), XRB_MASK,	PWRCOM,		{ RA, RS } },
+{ "extsh.",  XRC(31,922,1), XRB_MASK,	PPCCOM,		{ RA, RS } },
+{ "exts.",   XRC(31,922,1), XRB_MASK,	PWRCOM,		{ RA, RS } },
+
+{ "tlbrehi", XTLB(31,946,0), XTLB_MASK,	PPC403,		{ RT, RA } },
+{ "tlbrelo", XTLB(31,946,1), XTLB_MASK,	PPC403,		{ RT, RA } },
+{ "tlbre",   X(31,946),	X_MASK,		PPC403,		{ RT, RA, SH } },
+
+{ "sraiq",   XRC(31,952,0), X_MASK,	M601,		{ RA, RS, SH } },
+{ "sraiq.",  XRC(31,952,1), X_MASK,	M601,		{ RA, RS, SH } },
+
+{ "extsb",   XRC(31,954,0), XRB_MASK,	PPC,		{ RA, RS} },
+{ "extsb.",  XRC(31,954,1), XRB_MASK,	PPC,		{ RA, RS} },
+
+{ "iccci",   X(31,966),	XRT_MASK,	PPC403,		{ RA, RB } },
+
+{ "tlbld",   X(31,978),	XRTRA_MASK,	PPC,		{ RB } },
+
+{ "tlbwehi", XTLB(31,978,0), XTLB_MASK,	PPC403,		{ RT, RA } },
+{ "tlbwelo", XTLB(31,978,1), XTLB_MASK,	PPC403,		{ RT, RA } },
+{ "tlbwe",   X(31,978),	X_MASK,		PPC403,		{ RS, RA, SH } },
+
+{ "icbi",    X(31,982),	XRT_MASK,	PPC,		{ RA, RB } },
+
+{ "stfiwx",  X(31,983),	X_MASK,		PPC,		{ FRS, RA, RB } },
+
+{ "extsw",   XRC(31,986,0), XRB_MASK,	PPC,		{ RA, RS } },
+{ "extsw.",  XRC(31,986,1), XRB_MASK,	PPC,		{ RA, RS } },
+
+{ "icread",  X(31,998),	XRT_MASK,	PPC403,		{ RA, RB } },
+
+{ "tlbli",   X(31,1010), XRTRA_MASK,	PPC,		{ RB } },
+
+{ "dcbz",    X(31,1014), XRT_MASK,	PPC,		{ RA, RB } },
+{ "dclz",    X(31,1014), XRT_MASK,	PPC,		{ RA, RB } },
+
+{ "lvebx",   X(31,   7), X_MASK,	PPCVEC,		{ VD, RA, RB } },
+{ "lvehx",   X(31,  39), X_MASK,	PPCVEC,		{ VD, RA, RB } },
+{ "lvewx",   X(31,  71), X_MASK,	PPCVEC,		{ VD, RA, RB } },
+{ "lvsl",    X(31,   6), X_MASK,	PPCVEC,		{ VD, RA, RB } },
+{ "lvsr",    X(31,  38), X_MASK,	PPCVEC,		{ VD, RA, RB } },
+{ "lvx",     X(31, 103), X_MASK,	PPCVEC,		{ VD, RA, RB } },
+{ "lvxl",    X(31, 359), X_MASK,	PPCVEC,		{ VD, RA, RB } },
+{ "stvebx",  X(31, 135), X_MASK,	PPCVEC,		{ VS, RA, RB } },
+{ "stvehx",  X(31, 167), X_MASK,	PPCVEC,		{ VS, RA, RB } },
+{ "stvewx",  X(31, 199), X_MASK,	PPCVEC,		{ VS, RA, RB } },
+{ "stvx",    X(31, 231), X_MASK,	PPCVEC,		{ VS, RA, RB } },
+{ "stvxl",   X(31, 487), X_MASK,	PPCVEC,		{ VS, RA, RB } },
+
+{ "lwz",     OP(32),	OP_MASK,	PPCCOM,		{ RT, D, RA } },
+{ "l",	     OP(32),	OP_MASK,	PWRCOM,		{ RT, D, RA } },
+
+{ "lwzu",    OP(33),	OP_MASK,	PPCCOM,		{ RT, D, RAL } },
+{ "lu",      OP(33),	OP_MASK,	PWRCOM,		{ RT, D, RA } },
+
+{ "lbz",     OP(34),	OP_MASK,	COM,		{ RT, D, RA } },
+
+{ "lbzu",    OP(35),	OP_MASK,	COM,		{ RT, D, RAL } },
+
+{ "stw",     OP(36),	OP_MASK,	PPCCOM,		{ RS, D, RA } },
+{ "st",      OP(36),	OP_MASK,	PWRCOM,		{ RS, D, RA } },
+
+{ "stwu",    OP(37),	OP_MASK,	PPCCOM,		{ RS, D, RAS } },
+{ "stu",     OP(37),	OP_MASK,	PWRCOM,		{ RS, D, RA } },
+
+{ "stb",     OP(38),	OP_MASK,	COM,		{ RS, D, RA } },
+
+{ "stbu",    OP(39),	OP_MASK,	COM,		{ RS, D, RAS } },
+
+{ "lhz",     OP(40),	OP_MASK,	COM,		{ RT, D, RA } },
+
+{ "lhzu",    OP(41),	OP_MASK,	COM,		{ RT, D, RAL } },
+
+{ "lha",     OP(42),	OP_MASK,	COM,		{ RT, D, RA } },
+
+{ "lhau",    OP(43),	OP_MASK,	COM,		{ RT, D, RAL } },
+
+{ "sth",     OP(44),	OP_MASK,	COM,		{ RS, D, RA } },
+
+{ "sthu",    OP(45),	OP_MASK,	COM,		{ RS, D, RAS } },
+
+{ "lmw",     OP(46),	OP_MASK,	PPCCOM,		{ RT, D, RAM } },
+{ "lm",      OP(46),	OP_MASK,	PWRCOM,		{ RT, D, RA } },
+
+{ "stmw",    OP(47),	OP_MASK,	PPCCOM,		{ RS, D, RA } },
+{ "stm",     OP(47),	OP_MASK,	PWRCOM,		{ RS, D, RA } },
+
+{ "lfs",     OP(48),	OP_MASK,	COM,		{ FRT, D, RA } },
+
+{ "lfsu",    OP(49),	OP_MASK,	COM,		{ FRT, D, RAS } },
+
+{ "lfd",     OP(50),	OP_MASK,	COM,		{ FRT, D, RA } },
+
+{ "lfdu",    OP(51),	OP_MASK,	COM,		{ FRT, D, RAS } },
+
+{ "stfs",    OP(52),	OP_MASK,	COM,		{ FRS, D, RA } },
+
+{ "stfsu",   OP(53),	OP_MASK,	COM,		{ FRS, D, RAS } },
+
+{ "stfd",    OP(54),	OP_MASK,	COM,		{ FRS, D, RA } },
+
+{ "stfdu",   OP(55),	OP_MASK,	COM,		{ FRS, D, RAS } },
+
+{ "lfq",     OP(56),	OP_MASK,	POWER2,		{ FRT, D, RA } },
+
+{ "lfqu",    OP(57),	OP_MASK,	POWER2,		{ FRT, D, RA } },
+
+{ "ld",      DSO(58,0),	DS_MASK,	PPC64,		{ RT, DS, RA } },
+
+{ "ldu",     DSO(58,1), DS_MASK,	PPC64,		{ RT, DS, RAL } },
+
+{ "lwa",     DSO(58,2), DS_MASK,	PPC64,		{ RT, DS, RA } },
+
+{ "fdivs",   A(59,18,0), AFRC_MASK,	PPC,		{ FRT, FRA, FRB } },
+{ "fdivs.",  A(59,18,1), AFRC_MASK,	PPC,		{ FRT, FRA, FRB } },
+
+{ "fsubs",   A(59,20,0), AFRC_MASK,	PPC,		{ FRT, FRA, FRB } },
+{ "fsubs.",  A(59,20,1), AFRC_MASK,	PPC,		{ FRT, FRA, FRB } },
+
+{ "fadds",   A(59,21,0), AFRC_MASK,	PPC,		{ FRT, FRA, FRB } },
+{ "fadds.",  A(59,21,1), AFRC_MASK,	PPC,		{ FRT, FRA, FRB } },
+
+{ "fsqrts",  A(59,22,0), AFRAFRC_MASK,	PPC,		{ FRT, FRB } },
+{ "fsqrts.", A(59,22,1), AFRAFRC_MASK,	PPC,		{ FRT, FRB } },
+
+{ "fres",    A(59,24,0), AFRAFRC_MASK,	PPC,		{ FRT, FRB } },
+{ "fres.",   A(59,24,1), AFRAFRC_MASK,	PPC,		{ FRT, FRB } },
+
+{ "fmuls",   A(59,25,0), AFRB_MASK,	PPC,		{ FRT, FRA, FRC } },
+{ "fmuls.",  A(59,25,1), AFRB_MASK,	PPC,		{ FRT, FRA, FRC } },
+
+{ "fmsubs",  A(59,28,0), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+{ "fmsubs.", A(59,28,1), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+
+{ "fmadds",  A(59,29,0), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+{ "fmadds.", A(59,29,1), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+
+{ "fnmsubs", A(59,30,0), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+{ "fnmsubs.",A(59,30,1), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+
+{ "fnmadds", A(59,31,0), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+{ "fnmadds.",A(59,31,1), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+
+{ "stfq",    OP(60),	OP_MASK,	POWER2,		{ FRS, D, RA } },
+
+{ "stfqu",   OP(61),	OP_MASK,	POWER2,		{ FRS, D, RA } },
+
+{ "std",     DSO(62,0),	DS_MASK,	PPC64,		{ RS, DS, RA } },
+
+{ "stdu",    DSO(62,1),	DS_MASK,	PPC64,		{ RS, DS, RAS } },
+
+{ "fcmpu",   X(63,0),	X_MASK|(3<<21),	COM,		{ BF, FRA, FRB } },
+
+{ "frsp",    XRC(63,12,0), XRA_MASK,	COM,		{ FRT, FRB } },
+{ "frsp.",   XRC(63,12,1), XRA_MASK,	COM,		{ FRT, FRB } },
+
+{ "fctiw",   XRC(63,14,0), XRA_MASK,	PPCCOM,		{ FRT, FRB } },
+{ "fcir",    XRC(63,14,0), XRA_MASK,	POWER2,		{ FRT, FRB } },
+{ "fctiw.",  XRC(63,14,1), XRA_MASK,	PPCCOM,		{ FRT, FRB } },
+{ "fcir.",   XRC(63,14,1), XRA_MASK,	POWER2,		{ FRT, FRB } },
+
+{ "fctiwz",  XRC(63,15,0), XRA_MASK,	PPCCOM,		{ FRT, FRB } },
+{ "fcirz",   XRC(63,15,0), XRA_MASK,	POWER2,		{ FRT, FRB } },
+{ "fctiwz.", XRC(63,15,1), XRA_MASK,	PPCCOM,		{ FRT, FRB } },
+{ "fcirz.",  XRC(63,15,1), XRA_MASK,	POWER2,		{ FRT, FRB } },
+
+{ "fdiv",    A(63,18,0), AFRC_MASK,	PPCCOM,		{ FRT, FRA, FRB } },
+{ "fd",      A(63,18,0), AFRC_MASK,	PWRCOM,		{ FRT, FRA, FRB } },
+{ "fdiv.",   A(63,18,1), AFRC_MASK,	PPCCOM,		{ FRT, FRA, FRB } },
+{ "fd.",     A(63,18,1), AFRC_MASK,	PWRCOM,		{ FRT, FRA, FRB } },
+
+{ "fsub",    A(63,20,0), AFRC_MASK,	PPCCOM,		{ FRT, FRA, FRB } },
+{ "fs",      A(63,20,0), AFRC_MASK,	PWRCOM,		{ FRT, FRA, FRB } },
+{ "fsub.",   A(63,20,1), AFRC_MASK,	PPCCOM,		{ FRT, FRA, FRB } },
+{ "fs.",     A(63,20,1), AFRC_MASK,	PWRCOM,		{ FRT, FRA, FRB } },
+
+{ "fadd",    A(63,21,0), AFRC_MASK,	PPCCOM,		{ FRT, FRA, FRB } },
+{ "fa",      A(63,21,0), AFRC_MASK,	PWRCOM,		{ FRT, FRA, FRB } },
+{ "fadd.",   A(63,21,1), AFRC_MASK,	PPCCOM,		{ FRT, FRA, FRB } },
+{ "fa.",     A(63,21,1), AFRC_MASK,	PWRCOM,		{ FRT, FRA, FRB } },
+
+{ "fsqrt",   A(63,22,0), AFRAFRC_MASK,	PPCPWR2,	{ FRT, FRB } },
+{ "fsqrt.",  A(63,22,1), AFRAFRC_MASK,	PPCPWR2,	{ FRT, FRB } },
+
+{ "fsel",    A(63,23,0), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+{ "fsel.",   A(63,23,1), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+
+{ "fmul",    A(63,25,0), AFRB_MASK,	PPCCOM,		{ FRT, FRA, FRC } },
+{ "fm",      A(63,25,0), AFRB_MASK,	PWRCOM,		{ FRT, FRA, FRC } },
+{ "fmul.",   A(63,25,1), AFRB_MASK,	PPCCOM,		{ FRT, FRA, FRC } },
+{ "fm.",     A(63,25,1), AFRB_MASK,	PWRCOM,		{ FRT, FRA, FRC } },
+
+{ "frsqrte", A(63,26,0), AFRAFRC_MASK,	PPC,		{ FRT, FRB } },
+{ "frsqrte.",A(63,26,1), AFRAFRC_MASK,	PPC,		{ FRT, FRB } },
+
+{ "fmsub",   A(63,28,0), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fms",     A(63,28,0), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fmsub.",  A(63,28,1), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fms.",    A(63,28,1), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+
+{ "fmadd",   A(63,29,0), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fma",     A(63,29,0), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fmadd.",  A(63,29,1), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fma.",    A(63,29,1), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+
+{ "fnmsub",  A(63,30,0), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fnms",    A(63,30,0), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fnmsub.", A(63,30,1), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fnms.",   A(63,30,1), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+
+{ "fnmadd",  A(63,31,0), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fnma",    A(63,31,0), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fnmadd.", A(63,31,1), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fnma.",   A(63,31,1), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+
+{ "fcmpo",   X(63,32),	X_MASK|(3<<21),	COM,		{ BF, FRA, FRB } },
+
+{ "mtfsb1",  XRC(63,38,0), XRARB_MASK,	COM,		{ BT } },
+{ "mtfsb1.", XRC(63,38,1), XRARB_MASK,	COM,		{ BT } },
+
+{ "fneg",    XRC(63,40,0), XRA_MASK,	COM,		{ FRT, FRB } },
+{ "fneg.",   XRC(63,40,1), XRA_MASK,	COM,		{ FRT, FRB } },
+
+{ "mcrfs",   X(63,64),	XRB_MASK|(3<<21)|(3<<16), COM,	{ BF, BFA } },
+
+{ "mtfsb0",  XRC(63,70,0), XRARB_MASK,	COM,		{ BT } },
+{ "mtfsb0.", XRC(63,70,1), XRARB_MASK,	COM,		{ BT } },
+
+{ "fmr",     XRC(63,72,0), XRA_MASK,	COM,		{ FRT, FRB } },
+{ "fmr.",    XRC(63,72,1), XRA_MASK,	COM,		{ FRT, FRB } },
+
+{ "mtfsfi",  XRC(63,134,0), XRA_MASK|(3<<21)|(1<<11), COM, { BF, U } },
+{ "mtfsfi.", XRC(63,134,1), XRA_MASK|(3<<21)|(1<<11), COM, { BF, U } },
+
+{ "fnabs",   XRC(63,136,0), XRA_MASK,	COM,		{ FRT, FRB } },
+{ "fnabs.",  XRC(63,136,1), XRA_MASK,	COM,		{ FRT, FRB } },
+
+{ "fabs",    XRC(63,264,0), XRA_MASK,	COM,		{ FRT, FRB } },
+{ "fabs.",   XRC(63,264,1), XRA_MASK,	COM,		{ FRT, FRB } },
+
+{ "mffs",    XRC(63,583,0), XRARB_MASK,	COM,		{ FRT } },
+{ "mffs.",   XRC(63,583,1), XRARB_MASK,	COM,		{ FRT } },
+
+{ "mtfsf",   XFL(63,711,0), XFL_MASK,	COM,		{ FLM, FRB } },
+{ "mtfsf.",  XFL(63,711,1), XFL_MASK,	COM,		{ FLM, FRB } },
+
+{ "fctid",   XRC(63,814,0), XRA_MASK,	PPC64,		{ FRT, FRB } },
+{ "fctid.",  XRC(63,814,1), XRA_MASK,	PPC64,		{ FRT, FRB } },
+
+{ "fctidz",  XRC(63,815,0), XRA_MASK,	PPC64,		{ FRT, FRB } },
+{ "fctidz.", XRC(63,815,1), XRA_MASK,	PPC64,		{ FRT, FRB } },
+
+{ "fcfid",   XRC(63,846,0), XRA_MASK,	PPC64,		{ FRT, FRB } },
+{ "fcfid.",  XRC(63,846,1), XRA_MASK,	PPC64,		{ FRT, FRB } },
+
+};
+
+const int powerpc_num_opcodes =
+  sizeof (powerpc_opcodes) / sizeof (powerpc_opcodes[0]);
+
+/* The macro table.  This is only used by the assembler.  */
+
+/* The expressions of the form (-x ! 31) & (x | 31) have the value 0
+   when x=0; 32-x when x is between 1 and 31; are negative if x is
+   negative; and are 32 or more otherwise.  This is what you want
+   when, for instance, you are emulating a right shift by a
+   rotate-left-and-mask, because the underlying instructions support
+   shifts of size 0 but not shifts of size 32.  By comparison, when
+   extracting x bits from some word you want to use just 32-x, because
+   the underlying instructions don't support extracting 0 bits but do
+   support extracting the whole word (32 bits in this case).  */
+
+const struct powerpc_macro powerpc_macros[] = {
+{ "extldi",  4,   PPC64,	"rldicr %0,%1,%3,(%2)-1" },
+{ "extldi.", 4,   PPC64,	"rldicr. %0,%1,%3,(%2)-1" },
+{ "extrdi",  4,   PPC64,	"rldicl %0,%1,(%2)+(%3),64-(%2)" },
+{ "extrdi.", 4,   PPC64,	"rldicl. %0,%1,(%2)+(%3),64-(%2)" },
+{ "insrdi",  4,   PPC64,	"rldimi %0,%1,64-((%2)+(%3)),%3" },
+{ "insrdi.", 4,   PPC64,	"rldimi. %0,%1,64-((%2)+(%3)),%3" },
+{ "rotrdi",  3,   PPC64,	"rldicl %0,%1,(-(%2)!63)&((%2)|63),0" },
+{ "rotrdi.", 3,   PPC64,	"rldicl. %0,%1,(-(%2)!63)&((%2)|63),0" },
+{ "sldi",    3,   PPC64,	"rldicr %0,%1,%2,63-(%2)" },
+{ "sldi.",   3,   PPC64,	"rldicr. %0,%1,%2,63-(%2)" },
+{ "srdi",    3,   PPC64,	"rldicl %0,%1,(-(%2)!63)&((%2)|63),%2" },
+{ "srdi.",   3,   PPC64,	"rldicl. %0,%1,(-(%2)!63)&((%2)|63),%2" },
+{ "clrrdi",  3,   PPC64,	"rldicr %0,%1,0,63-(%2)" },
+{ "clrrdi.", 3,   PPC64,	"rldicr. %0,%1,0,63-(%2)" },
+{ "clrlsldi",4,   PPC64,	"rldic %0,%1,%3,(%2)-(%3)" },
+{ "clrlsldi.",4,  PPC64,	"rldic. %0,%1,%3,(%2)-(%3)" },
+
+{ "extlwi",  4,   PPCCOM,	"rlwinm %0,%1,%3,0,(%2)-1" },
+{ "extlwi.", 4,   PPCCOM,	"rlwinm. %0,%1,%3,0,(%2)-1" },
+{ "extrwi",  4,   PPCCOM,	"rlwinm %0,%1,(%2)+(%3),32-(%2),31" },
+{ "extrwi.", 4,   PPCCOM,	"rlwinm. %0,%1,(%2)+(%3),32-(%2),31" },
+{ "inslwi",  4,   PPCCOM,	"rlwimi %0,%1,(-(%3)!31)&((%3)|31),%3,(%2)+(%3)-1" },
+{ "inslwi.", 4,   PPCCOM,	"rlwimi. %0,%1,(-(%3)!31)&((%3)|31),%3,(%2)+(%3)-1"},
+{ "insrwi",  4,   PPCCOM,	"rlwimi %0,%1,32-((%2)+(%3)),%3,(%2)+(%3)-1" },
+{ "insrwi.", 4,   PPCCOM,	"rlwimi. %0,%1,32-((%2)+(%3)),%3,(%2)+(%3)-1"},
+{ "rotrwi",  3,   PPCCOM,	"rlwinm %0,%1,(-(%2)!31)&((%2)|31),0,31" },
+{ "rotrwi.", 3,   PPCCOM,	"rlwinm. %0,%1,(-(%2)!31)&((%2)|31),0,31" },
+{ "slwi",    3,   PPCCOM,	"rlwinm %0,%1,%2,0,31-(%2)" },
+{ "sli",     3,   PWRCOM,	"rlinm %0,%1,%2,0,31-(%2)" },
+{ "slwi.",   3,   PPCCOM,	"rlwinm. %0,%1,%2,0,31-(%2)" },
+{ "sli.",    3,   PWRCOM,	"rlinm. %0,%1,%2,0,31-(%2)" },
+{ "srwi",    3,   PPCCOM,	"rlwinm %0,%1,(-(%2)!31)&((%2)|31),%2,31" },
+{ "sri",     3,   PWRCOM,	"rlinm %0,%1,(-(%2)!31)&((%2)|31),%2,31" },
+{ "srwi.",   3,   PPCCOM,	"rlwinm. %0,%1,(-(%2)!31)&((%2)|31),%2,31" },
+{ "sri.",    3,   PWRCOM,	"rlinm. %0,%1,(-(%2)!31)&((%2)|31),%2,31" },
+{ "clrrwi",  3,   PPCCOM,	"rlwinm %0,%1,0,0,31-(%2)" },
+{ "clrrwi.", 3,   PPCCOM,	"rlwinm. %0,%1,0,0,31-(%2)" },
+{ "clrlslwi",4,   PPCCOM,	"rlwinm %0,%1,%3,(%2)-(%3),31-(%3)" },
+{ "clrlslwi.",4,  PPCCOM,	"rlwinm. %0,%1,%3,(%2)-(%3),31-(%3)" },
+
+};
+
+const int powerpc_num_macros =
+  sizeof (powerpc_macros) / sizeof (powerpc_macros[0]);
diff -purN linux-2.5/arch/ppc64/kdb/ppc.h linuxppc64-2.5/arch/ppc64/kdb/ppc.h
--- linux-2.5/arch/ppc64/kdb/ppc.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/ppc.h	2003-10-13 16:16:57.000000000 +0000
@@ -0,0 +1,259 @@
+/* ppc.h -- Header file for PowerPC opcode table
+   Copyright 1994, 1995 Free Software Foundation, Inc.
+   Written by Ian Lance Taylor, Cygnus Support
+
+This file is part of GDB, GAS, and the GNU binutils.
+
+GDB, GAS, and the GNU binutils are free software; you can redistribute
+them and/or modify them under the terms of the GNU General Public
+License as published by the Free Software Foundation; either version
+1, or (at your option) any later version.
+
+GDB, GAS, and the GNU binutils are distributed in the hope that they
+will be useful, but WITHOUT ANY WARRANTY; without even the implied
+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+the GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this file; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  */
+
+#ifndef PPC_H
+#define PPC_H
+
+/* The opcode table is an array of struct powerpc_opcode.  */
+
+struct powerpc_opcode
+{
+  /* The opcode name.  */
+  const char *name;
+
+  /* The opcode itself.  Those bits which will be filled in with
+     operands are zeroes.  */
+  unsigned long opcode;
+
+  /* The opcode mask.  This is used by the disassembler.  This is a
+     mask containing ones indicating those bits which must match the
+     opcode field, and zeroes indicating those bits which need not
+     match (and are presumably filled in by operands).  */
+  unsigned long mask;
+
+  /* One bit flags for the opcode.  These are used to indicate which
+     specific processors support the instructions.  The defined values
+     are listed below.  */
+  unsigned long flags;
+
+  /* An array of operand codes.  Each code is an index into the
+     operand table.  They appear in the order which the operands must
+     appear in assembly code, and are terminated by a zero.  */
+  unsigned char operands[8];
+};
+
+/* The table itself is sorted by major opcode number, and is otherwise
+   in the order in which the disassembler should consider
+   instructions.  */
+extern const struct powerpc_opcode powerpc_opcodes[];
+extern const int powerpc_num_opcodes;
+
+/* Values defined for the flags field of a struct powerpc_opcode.  */
+
+/* Opcode is defined for the PowerPC architecture.  */
+#define PPC_OPCODE_PPC (01)
+
+/* Opcode is defined for the POWER (RS/6000) architecture.  */
+#define PPC_OPCODE_POWER (02)
+
+/* Opcode is defined for the POWER2 (Rios 2) architecture.  */
+#define PPC_OPCODE_POWER2 (04)
+
+/* Opcode is only defined on 32 bit architectures.  */
+#define PPC_OPCODE_32 (010)
+
+/* Opcode is only defined on 64 bit architectures.  */
+#define PPC_OPCODE_64 (020)
+
+/* Opcode is supported by the Motorola PowerPC 601 processor.  The 601
+   is assumed to support all PowerPC (PPC_OPCODE_PPC) instructions,
+   but it also supports many additional POWER instructions.  */
+#define PPC_OPCODE_601 (040)
+
+/* Opcode is supported in both the Power and PowerPC architectures
+   (ie, compiler's -mcpu=common or assembler's -mcom).  */
+#define PPC_OPCODE_COMMON (0100)
+
+/* Opcode is supported for any Power or PowerPC platform (this is
+   for the assembler's -many option, and it eliminates duplicates).  */
+#define PPC_OPCODE_ANY (0200)
+
+/* Opcode is supported as part of the 64-bit bridge.  */
+#define PPC_OPCODE_64_BRIDGE (0400)
+
+/* Opcode is supported by Altivec Vector Unit */
+#define PPC_OPCODE_ALTIVEC   (01000)
+
+/* A macro to extract the major opcode from an instruction.  */
+#define PPC_OP(i) (((i) >> 26) & 0x3f)
+
+/* The operands table is an array of struct powerpc_operand.  */
+
+struct powerpc_operand
+{
+  /* The number of bits in the operand.  */
+  int bits;
+
+  /* How far the operand is left shifted in the instruction.  */
+  int shift;
+
+  /* Insertion function.  This is used by the assembler.  To insert an
+     operand value into an instruction, check this field.
+
+     If it is NULL, execute
+         i |= (op & ((1 << o->bits) - 1)) << o->shift;
+     (i is the instruction which we are filling in, o is a pointer to
+     this structure, and op is the opcode value; this assumes twos
+     complement arithmetic).
+
+     If this field is not NULL, then simply call it with the
+     instruction and the operand value.  It will return the new value
+     of the instruction.  If the ERRMSG argument is not NULL, then if
+     the operand value is illegal, *ERRMSG will be set to a warning
+     string (the operand will be inserted in any case).  If the
+     operand value is legal, *ERRMSG will be unchanged (most operands
+     can accept any value).  */
+  unsigned long (*insert) PARAMS ((unsigned long instruction, long op,
+				   const char **errmsg));
+
+  /* Extraction function.  This is used by the disassembler.  To
+     extract this operand type from an instruction, check this field.
+
+     If it is NULL, compute
+         op = ((i) >> o->shift) & ((1 << o->bits) - 1);
+	 if ((o->flags & PPC_OPERAND_SIGNED) != 0
+	     && (op & (1 << (o->bits - 1))) != 0)
+	   op -= 1 << o->bits;
+     (i is the instruction, o is a pointer to this structure, and op
+     is the result; this assumes twos complement arithmetic).
+
+     If this field is not NULL, then simply call it with the
+     instruction value.  It will return the value of the operand.  If
+     the INVALID argument is not NULL, *INVALID will be set to
+     non-zero if this operand type can not actually be extracted from
+     this operand (i.e., the instruction does not match).  If the
+     operand is valid, *INVALID will not be changed.  */
+  long (*extract) PARAMS ((unsigned long instruction, int *invalid));
+
+  /* One bit syntax flags.  */
+  unsigned long flags;
+};
+
+/* Elements in the table are retrieved by indexing with values from
+   the operands field of the powerpc_opcodes table.  */
+
+extern const struct powerpc_operand powerpc_operands[];
+
+/* Values defined for the flags field of a struct powerpc_operand.  */
+
+/* This operand takes signed values.  */
+#define PPC_OPERAND_SIGNED (01)
+
+/* This operand takes signed values, but also accepts a full positive
+   range of values when running in 32 bit mode.  That is, if bits is
+   16, it takes any value from -0x8000 to 0xffff.  In 64 bit mode,
+   this flag is ignored.  */
+#define PPC_OPERAND_SIGNOPT (02)
+
+/* This operand does not actually exist in the assembler input.  This
+   is used to support extended mnemonics such as mr, for which two
+   operands fields are identical.  The assembler should call the
+   insert function with any op value.  The disassembler should call
+   the extract function, ignore the return value, and check the value
+   placed in the valid argument.  */
+#define PPC_OPERAND_FAKE (04)
+
+/* The next operand should be wrapped in parentheses rather than
+   separated from this one by a comma.  This is used for the load and
+   store instructions which want their operands to look like
+       reg,displacement(reg)
+   */
+#define PPC_OPERAND_PARENS (010)
+
+/* This operand may use the symbolic names for the CR fields, which
+   are
+       lt  0	gt  1	eq  2	so  3	un  3
+       cr0 0	cr1 1	cr2 2	cr3 3
+       cr4 4	cr5 5	cr6 6	cr7 7
+   These may be combined arithmetically, as in cr2*4+gt.  These are
+   only supported on the PowerPC, not the POWER.  */
+#define PPC_OPERAND_CR (020)
+
+/* This operand names a register.  The disassembler uses this to print
+   register names with a leading 'r'.  */
+#define PPC_OPERAND_GPR (040)
+
+/* This operand names a floating point register.  The disassembler
+   prints these with a leading 'f'.  */
+#define PPC_OPERAND_FPR (0100)
+
+/* This operand is a relative branch displacement.  The disassembler
+   prints these symbolically if possible.  */
+#define PPC_OPERAND_RELATIVE (0200)
+
+/* This operand is an absolute branch address.  The disassembler
+   prints these symbolically if possible.  */
+#define PPC_OPERAND_ABSOLUTE (0400)
+
+/* This operand is optional, and is zero if omitted.  This is used for
+   the optional BF and L fields in the comparison instructions.  The
+   assembler must count the number of operands remaining on the line,
+   and the number of operands remaining for the opcode, and decide
+   whether this operand is present or not.  The disassembler should
+   print this operand out only if it is not zero.  */
+#define PPC_OPERAND_OPTIONAL (01000)
+
+/* This flag is only used with PPC_OPERAND_OPTIONAL.  If this operand
+   is omitted, then for the next operand use this operand value plus
+   1, ignoring the next operand field for the opcode.  This wretched
+   hack is needed because the Power rotate instructions can take
+   either 4 or 5 operands.  The disassembler should print this operand
+   out regardless of the PPC_OPERAND_OPTIONAL field.  */
+#define PPC_OPERAND_NEXT (02000)
+
+/* This operand should be regarded as a negative number for the
+   purposes of overflow checking (i.e., the normal most negative
+   number is disallowed and one more than the normal most positive
+   number is allowed).  This flag will only be set for a signed
+   operand.  */
+#define PPC_OPERAND_NEGATIVE (04000)
+
+/* This operand names a vector unit register.  The disassembler
+   prints these with a leading 'v'.  */
+#define PPC_OPERAND_VR (010000)
+
+
+/* The POWER and PowerPC assemblers use a few macros.  We keep them
+   with the operands table for simplicity.  The macro table is an
+   array of struct powerpc_macro.  */
+
+struct powerpc_macro
+{
+  /* The macro name.  */
+  const char *name;
+
+  /* The number of operands the macro takes.  */
+  unsigned int operands;
+
+  /* One bit flags for the opcode.  These are used to indicate which
+     specific processors support the instructions.  The values are the
+     same as those for the struct powerpc_opcode flags field.  */
+  unsigned long flags;
+
+  /* A format string to turn the macro into a normal instruction.
+     Each %N in the string is replaced with operand number N (zero
+     based).  */
+  const char *format;
+};
+
+extern const struct powerpc_macro powerpc_macros[];
+extern const int powerpc_num_macros;
+
+#endif /* PPC_H */
diff -purN linux-2.5/arch/ppc64/kdb/privinst.h linuxppc64-2.5/arch/ppc64/kdb/privinst.h
--- linux-2.5/arch/ppc64/kdb/privinst.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/privinst.h	2004-01-22 06:11:58.000000000 +0000
@@ -0,0 +1,94 @@
+/*
+ * Copyright (C) 1996 Paul Mackerras.
+ *
+ *      This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+#define GETREG(reg)		\
+    static inline unsigned long get_ ## reg (void)	\
+	{ unsigned long ret; asm volatile ("mf" #reg " %0" : "=r" (ret) :); return ret; }
+
+#define SETREG(reg)		\
+    static inline void set_ ## reg (unsigned long val)	\
+	{ asm volatile ("mt" #reg " %0" : : "r" (val)); }
+
+GETREG(msr)
+SETREG(msr)
+SETREG(msrd)
+GETREG(cr)
+
+#define GSETSPR(n, name)	\
+    static inline long get_ ## name (void) \
+	{ long ret; asm volatile ("mfspr %0," #n : "=r" (ret) : ); return ret; } \
+    static inline void set_ ## name (long val) \
+	{ asm volatile ("mtspr " #n ",%0" : : "r" (val)); }
+
+GSETSPR(0, mq)
+GSETSPR(1, xer)
+GSETSPR(4, rtcu)
+GSETSPR(5, rtcl)
+GSETSPR(8, lr)
+GSETSPR(9, ctr)
+GSETSPR(18, dsisr)
+GSETSPR(19, dar)
+GSETSPR(22, dec)
+GSETSPR(25, sdr1)
+GSETSPR(26, srr0)
+GSETSPR(27, srr1)
+GSETSPR(272, sprg0)
+GSETSPR(273, sprg1)
+GSETSPR(274, sprg2)
+GSETSPR(275, sprg3)
+GSETSPR(282, ear)
+GSETSPR(287, pvr)
+GSETSPR(528, bat0u)
+GSETSPR(529, bat0l)
+GSETSPR(530, bat1u)
+GSETSPR(531, bat1l)
+GSETSPR(532, bat2u)
+GSETSPR(533, bat2l)
+GSETSPR(534, bat3u)
+GSETSPR(535, bat3l)
+GSETSPR(1008, hid0)
+GSETSPR(1009, hid1)
+GSETSPR(1010, iabr)
+GSETSPR(1013, dabr)
+GSETSPR(1023, pir)
+
+static inline int get_sr(int n)
+{
+    int ret;
+
+#if 0
+// DRENG does not assemble 
+    asm (" mfsrin %0,%1" : "=r" (ret) : "r" (n << 28));
+#endif
+    return ret;
+}
+
+static inline void set_sr(int n, int val)
+{
+#if 0
+// DRENG does not assemble 
+    asm ("mtsrin %0,%1" : : "r" (val), "r" (n << 28));
+#endif
+}
+
+static inline void store_inst(void *p)
+{
+    asm volatile ("dcbst 0,%0; sync; icbi 0,%0; isync" : : "r" (p));
+}
+
+static inline void cflush(void *p)
+{
+    asm volatile ("dcbf 0,%0; icbi 0,%0" : : "r" (p));
+}
+
+static inline void cinval(void *p)
+{
+    asm volatile ("dcbi 0,%0; icbi 0,%0" : : "r" (p));
+}
+
diff -purN linux-2.5/arch/ppc64/kdb/symcat.h linuxppc64-2.5/arch/ppc64/kdb/symcat.h
--- linux-2.5/arch/ppc64/kdb/symcat.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/symcat.h	2003-10-13 16:16:57.000000000 +0000
@@ -0,0 +1,49 @@
+/* Symbol concatenation utilities.
+
+   Copyright (C) 1998, 2000 Free Software Foundation, Inc.
+
+   This program is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 2 of the License, or
+   (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+ 
+   You should have received a copy of the GNU General Public License along
+   with this program; if not, write to the Free Software Foundation, Inc.,
+   59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  */
+
+#ifndef SYM_CAT_H
+#define SYM_CAT_H
+
+#if defined (__STDC__) || defined (ALMOST_STDC) || defined (HAVE_STRINGIZE)
+#define CONCAT2(a,b)	 a##b
+#define CONCAT3(a,b,c)	 a##b##c
+#define CONCAT4(a,b,c,d) a##b##c##d
+#define STRINGX(s) #s
+#else
+/* Note one should never pass extra whitespace to the CONCATn macros,
+   e.g. CONCAT2(foo, bar) because traditonal C will keep the space between
+   the two labels instead of concatenating them.  Instead, make sure to
+   write CONCAT2(foo,bar).  */
+#define CONCAT2(a,b)	 a/**/b
+#define CONCAT3(a,b,c)	 a/**/b/**/c
+#define CONCAT4(a,b,c,d) a/**/b/**/c/**/d
+#define STRINGX(s) "s"
+#endif
+
+#define XCONCAT2(a,b)     CONCAT2(a,b)
+#define XCONCAT3(a,b,c)   CONCAT3(a,b,c)
+#define XCONCAT4(a,b,c,d) CONCAT4(a,b,c,d)
+
+/* Note the layer of indirection here is typically used to allow
+   stringification of the expansion of macros.  I.e. "#define foo
+   bar", "XSTRING(foo)", to yield "bar".  Be aware that this only
+   works for __STDC__, not for traditional C which will still resolve
+   to "foo".  */
+#define XSTRING(s) STRINGX(s) 
+
+#endif /* SYM_CAT_H */
diff -purN linux-2.5/arch/ppc64/kernel/Makefile linuxppc64-2.5/arch/ppc64/kernel/Makefile
--- linux-2.5/arch/ppc64/kernel/Makefile	2004-02-27 22:44:57.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/Makefile	2004-03-01 02:24:55.000000000 +0000
@@ -5,7 +5,7 @@
 EXTRA_CFLAGS	+= -mno-minimal-toc
 extra-y		:= head.o vmlinux.lds.s
 
-obj-y               :=	setup.o entry.o traps.o irq.o idle.o \
+obj-y               :=	setup.o entry.o traps.o irq.o idle.o dma.o \
 			time.o process.o signal.o syscalls.o misc.o ptrace.o \
 			align.o semaphore.o bitops.o stab.o pacaData.o \
 			udbg.o binfmt_elf32.o sys_ppc32.o ioctl32.o \
diff -purN linux-2.5/arch/ppc64/kernel/chrp_setup.c linuxppc64-2.5/arch/ppc64/kernel/chrp_setup.c
--- linux-2.5/arch/ppc64/kernel/chrp_setup.c	2004-02-27 23:02:35.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/chrp_setup.c	2004-03-01 13:40:28.000000000 +0000
@@ -142,10 +142,6 @@ chrp_setup_arch(void)
 #ifdef CONFIG_BLK_DEV_INITRD
 	/* this is fine for chrp */
 	initrd_below_start_ok = 1;
-	
-	if (initrd_start)
-		ROOT_DEV = Root_RAM0;
-	else
 #endif
 	ROOT_DEV = Root_SDA2;
 
@@ -255,7 +251,7 @@ chrp_init(unsigned long r3, unsigned lon
 	ppc_md.setup_arch     = chrp_setup_arch;
 	ppc_md.setup_residual = NULL;
 	ppc_md.get_cpuinfo    = chrp_get_cpuinfo;
-	if(naca->interrupt_controller == IC_OPEN_PIC) {
+	if (naca->interrupt_controller == IC_OPEN_PIC) {
 		ppc_md.init_IRQ       = pSeries_init_openpic; 
 		ppc_md.get_irq        = openpic_get_irq;
 	} else {
@@ -270,6 +266,7 @@ chrp_init(unsigned long r3, unsigned lon
 	ppc_md.restart        = rtas_restart;
 	ppc_md.power_off      = rtas_power_off;
 	ppc_md.halt           = rtas_halt;
+	ppc_md.panic          = rtas_os_term;
 
 	ppc_md.get_boot_time  = pSeries_get_boot_time;
 	ppc_md.get_rtc_time   = pSeries_get_rtc_time;
diff -purN linux-2.5/arch/ppc64/kernel/dma.c linuxppc64-2.5/arch/ppc64/kernel/dma.c
--- linux-2.5/arch/ppc64/kernel/dma.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/dma.c	2004-02-26 04:19:10.000000000 +0000
@@ -0,0 +1,184 @@
+/*
+ * Copyright (C) 2004 IBM Corporation
+ *
+ * Implements the generic device dma API for ppc64. Handles
+ * the pci and vio busses
+ */
+
+#include <linux/config.h>
+#include <linux/device.h>
+#include <linux/dma-mapping.h>
+/* Include the busses we support */
+#include <linux/pci.h>
+#ifdef CONFIG_PPC_PSERIES
+#include <asm/vio.h>
+#endif
+#include <asm/scatterlist.h>
+#include <asm/bug.h>
+
+int dma_supported(struct device *dev, u64 mask)
+{
+	if (dev->bus == &pci_bus_type)
+		return pci_dma_supported(to_pci_dev(dev), mask);
+#ifdef CONFIG_PPC_PSERIES
+	if (dev->bus == &vio_bus_type)
+		return vio_dma_supported(to_vio_dev(dev), mask);
+#endif
+	BUG();
+	return 0;
+}
+EXPORT_SYMBOL(dma_supported);
+
+int dma_set_mask(struct device *dev, u64 dma_mask)
+{
+	if (dev->bus == &pci_bus_type)
+		return pci_set_dma_mask(to_pci_dev(dev), dma_mask);
+#ifdef CONFIG_PPC_PSERIES
+	if (dev->bus == &vio_bus_type)
+		return vio_set_dma_mask(to_vio_dev(dev), dma_mask);
+#endif
+	BUG();
+	return 0;
+}
+EXPORT_SYMBOL(dma_set_mask);
+
+void *dma_alloc_coherent(struct device *dev, size_t size,
+		dma_addr_t *dma_handle, int flag)
+{
+	if (dev->bus == &pci_bus_type)
+		return pci_alloc_consistent(to_pci_dev(dev), size, dma_handle);
+#ifdef CONFIG_PPC_PSERIES
+	if (dev->bus == &vio_bus_type)
+		return vio_alloc_consistent(to_vio_dev(dev), size, dma_handle);
+#endif
+	BUG();
+	return 0;
+}
+EXPORT_SYMBOL(dma_alloc_coherent);
+
+void dma_free_coherent(struct device *dev, size_t size, void *cpu_addr,
+		dma_addr_t dma_handle)
+{
+	if (dev->bus == &pci_bus_type)
+		pci_free_consistent(to_pci_dev(dev), size, cpu_addr, dma_handle);
+#ifdef CONFIG_PPC_PSERIES
+	else if (dev->bus == &vio_bus_type)
+		vio_free_consistent(to_vio_dev(dev), size, cpu_addr, dma_handle);
+#endif
+	else
+		BUG();
+}
+EXPORT_SYMBOL(dma_free_coherent);
+
+dma_addr_t dma_map_single(struct device *dev, void *cpu_addr, size_t size,
+		enum dma_data_direction direction)
+{
+	if (dev->bus == &pci_bus_type)
+		return pci_map_single(to_pci_dev(dev), cpu_addr, size, (int)direction);
+#ifdef CONFIG_PPC_PSERIES
+	if (dev->bus == &vio_bus_type)
+		return vio_map_single(to_vio_dev(dev), cpu_addr, size, (int)direction);
+#endif
+	BUG();
+	return (dma_addr_t)0;
+}
+EXPORT_SYMBOL(dma_map_single);
+
+void dma_unmap_single(struct device *dev, dma_addr_t dma_addr, size_t size,
+		enum dma_data_direction direction)
+{
+	if (dev->bus == &pci_bus_type)
+		pci_unmap_single(to_pci_dev(dev), dma_addr, size, (int)direction);
+#ifdef CONFIG_PPC_PSERIES
+	else if (dev->bus == &vio_bus_type)
+		vio_unmap_single(to_vio_dev(dev), dma_addr, size, (int)direction);
+#endif
+	else
+		BUG();
+}
+EXPORT_SYMBOL(dma_unmap_single);
+
+dma_addr_t dma_map_page(struct device *dev, struct page *page,
+		unsigned long offset, size_t size,
+		enum dma_data_direction direction)
+{
+	if (dev->bus == &pci_bus_type)
+		return pci_map_page(to_pci_dev(dev), page, offset, size, (int)direction);
+#ifdef CONFIG_PPC_PSERIES
+	if (dev->bus == &vio_bus_type)
+		return vio_map_page(to_vio_dev(dev), page, offset, size, (int)direction);
+#endif
+	BUG();
+	return (dma_addr_t)0;
+}
+EXPORT_SYMBOL(dma_map_page);
+
+void dma_unmap_page(struct device *dev, dma_addr_t dma_address, size_t size,
+		enum dma_data_direction direction)
+{
+	if (dev->bus == &pci_bus_type)
+		pci_unmap_page(to_pci_dev(dev), dma_address, size, (int)direction);
+#ifdef CONFIG_PPC_PSERIES
+	else if (dev->bus == &vio_bus_type)
+		vio_unmap_page(to_vio_dev(dev), dma_address, size, (int)direction);
+#endif
+	else
+		BUG();
+}
+EXPORT_SYMBOL(dma_unmap_page);
+
+int dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
+		enum dma_data_direction direction)
+{
+	if (dev->bus == &pci_bus_type)
+		return pci_map_sg(to_pci_dev(dev), sg, nents, (int)direction);
+#ifdef CONFIG_PPC_PSERIES
+	if (dev->bus == &vio_bus_type)
+		return vio_map_sg(to_vio_dev(dev), sg, nents, (int)direction);
+#endif
+	BUG();
+	return 0;
+}
+EXPORT_SYMBOL(dma_map_sg);
+
+void dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nhwentries,
+		enum dma_data_direction direction)
+{
+	if (dev->bus == &pci_bus_type)
+		pci_unmap_sg(to_pci_dev(dev), sg, nhwentries, (int)direction);
+#ifdef CONFIG_PPC_PSERIES
+	else if (dev->bus == &vio_bus_type)
+		vio_unmap_sg(to_vio_dev(dev), sg, nhwentries, (int)direction);
+#endif
+	else
+		BUG();
+}
+EXPORT_SYMBOL(dma_unmap_sg);
+
+void dma_sync_single(struct device *dev, dma_addr_t dma_handle, size_t size,
+		enum dma_data_direction direction)
+{
+	if (dev->bus == &pci_bus_type)
+		pci_dma_sync_single(to_pci_dev(dev), dma_handle, size, (int)direction);
+#ifdef CONFIG_PPC_PSERIES
+	else if (dev->bus == &vio_bus_type)
+		vio_dma_sync_single(to_vio_dev(dev), dma_handle, size, (int)direction);
+#endif
+	else
+		BUG();
+}
+EXPORT_SYMBOL(dma_sync_single);
+
+void dma_sync_sg(struct device *dev, struct scatterlist *sg, int nelems,
+		enum dma_data_direction direction)
+{
+	if (dev->bus == &pci_bus_type)
+		pci_dma_sync_sg(to_pci_dev(dev), sg, nelems, (int)direction);
+#ifdef CONFIG_PPC_PSERIES
+	else if (dev->bus == &vio_bus_type)
+		vio_dma_sync_sg(to_vio_dev(dev), sg, nelems, (int)direction);
+#endif
+	else
+		BUG();
+}
+EXPORT_SYMBOL(dma_sync_sg);
diff -purN linux-2.5/arch/ppc64/kernel/eeh.c linuxppc64-2.5/arch/ppc64/kernel/eeh.c
--- linux-2.5/arch/ppc64/kernel/eeh.c	2004-02-23 16:39:09.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/eeh.c	2004-03-02 14:44:36.000000000 +0000
@@ -17,63 +17,348 @@
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
 
-/* Change Activity:
- * 2001/10/27 : engebret : Created.
- * End Change Activity 
- */
-
 #include <linux/init.h>
 #include <linux/pci.h>
 #include <linux/proc_fs.h>
 #include <linux/bootmem.h>
 #include <linux/mm.h>
+#include <linux/rbtree.h>
+#include <linux/spinlock.h>
+#include <linux/seq_file.h>
 #include <asm/paca.h>
 #include <asm/processor.h>
 #include <asm/naca.h>
 #include <asm/io.h>
 #include <asm/machdep.h>
+#include <asm/pgtable.h>
 #include "pci.h"
 
+#undef DEBUG
+
 #define BUID_HI(buid) ((buid) >> 32)
 #define BUID_LO(buid) ((buid) & 0xffffffff)
-#define CONFIG_ADDR(busno, devfn) (((((busno) & 0xff) << 8) | ((devfn) & 0xf8)) << 8)
+#define CONFIG_ADDR(busno, devfn) \
+		(((((busno) & 0xff) << 8) | ((devfn) & 0xf8)) << 8)
 
-unsigned long eeh_total_mmio_ffs;
-unsigned long eeh_false_positives;
 /* RTAS tokens */
 static int ibm_set_eeh_option;
 static int ibm_set_slot_reset;
 static int ibm_read_slot_reset_state;
 
-static int eeh_implemented;
+static int eeh_subsystem_enabled;
 #define EEH_MAX_OPTS 4096
 static char *eeh_opts;
 static int eeh_opts_last;
 
-unsigned char	slot_err_buf[RTAS_ERROR_LOG_MAX];
+/* System monitoring statistics */
+static DEFINE_PER_CPU(unsigned long, total_mmio_ffs);
+static DEFINE_PER_CPU(unsigned long, false_positives);
+static DEFINE_PER_CPU(unsigned long, ignored_failures);
 
-pte_t *find_linux_pte(pgd_t *pgdir, unsigned long va);	/* from htab.c */
-static int eeh_check_opts_config(struct device_node *dn,
-				 int class_code, int vendor_id, int device_id,
+static int eeh_check_opts_config(struct device_node *dn, int class_code,
+				 int vendor_id, int device_id,
 				 int default_state);
 
-unsigned long eeh_token_to_phys(unsigned long token)
+/**
+ * The pci address cache subsystem.  This subsystem places
+ * PCI device address resources into a red-black tree, sorted
+ * according to the address range, so that given only an i/o
+ * address, the corresponding PCI device can be **quickly**
+ * found.
+ *
+ * Currently, the only customer of this code is the EEH subsystem;
+ * thus, this code has been somewhat tailored to suit EEH better.
+ * In particular, the cache does *not* hold the addresses of devices
+ * for which EEH is not enabled.
+ *
+ * (Implementation Note: The RB tree seems to be better/faster
+ * than any hash algo I could think of for this problem, even
+ * with the penalty of slow pointer chases for d-cache misses).
+ */
+struct pci_io_addr_range
+{
+	struct rb_node rb_node;
+	unsigned long addr_lo;
+	unsigned long addr_hi;
+	struct pci_dev *pcidev;
+	unsigned int flags;
+};
+
+static struct pci_io_addr_cache
+{
+	struct rb_root rb_root;
+	spinlock_t piar_lock;
+} pci_io_addr_cache_root;
+
+static inline struct pci_dev *__pci_get_device_by_addr(unsigned long addr)
+{
+	struct rb_node *n = pci_io_addr_cache_root.rb_root.rb_node;
+
+	while (n) {
+		struct pci_io_addr_range *piar;
+		piar = rb_entry(n, struct pci_io_addr_range, rb_node);
+
+		if (addr < piar->addr_lo) {
+			n = n->rb_left;
+		} else {
+			if (addr > piar->addr_hi) {
+				n = n->rb_right;
+			} else {
+				pci_dev_get(piar->pcidev);
+				return piar->pcidev;
+			}
+		}
+	}
+
+	return NULL;
+}
+
+/**
+ * pci_get_device_by_addr - Get device, given only address
+ * @addr: mmio (PIO) phys address or i/o port number
+ *
+ * Given an mmio phys address, or a port number, find a pci device
+ * that implements this address.  Be sure to pci_dev_put the device
+ * when finished.  I/O port numbers are assumed to be offset
+ * from zero (that is, they do *not* have pci_io_addr added in).
+ * It is safe to call this function within an interrupt.
+ */
+static struct pci_dev *pci_get_device_by_addr(unsigned long addr)
+{
+	struct pci_dev *dev;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pci_io_addr_cache_root.piar_lock, flags);
+	dev = __pci_get_device_by_addr(addr);
+	spin_unlock_irqrestore(&pci_io_addr_cache_root.piar_lock, flags);
+	return dev;
+}
+
+#ifdef DEBUG
+/*
+ * Handy-dandy debug print routine, does nothing more
+ * than print out the contents of our addr cache.
+ */
+static void pci_addr_cache_print(struct pci_io_addr_cache *cache)
+{
+	struct rb_node *n;
+	int cnt = 0;
+
+	n = rb_first(&cache->rb_root);
+	while (n) {
+		struct pci_io_addr_range *piar;
+		piar = rb_entry(n, struct pci_io_addr_range, rb_node);
+		printk(KERN_DEBUG "PCI: %s addr range %d [%lx-%lx]: %s %s\n",
+		       (piar->flags & IORESOURCE_IO) ? "i/o" : "mem", cnt,
+		       piar->addr_lo, piar->addr_hi, pci_name(piar->pcidev),
+		       pci_pretty_name(piar->pcidev));
+		cnt++;
+		n = rb_next(n);
+	}
+}
+#endif
+
+/* Insert address range into the rb tree. */
+static struct pci_io_addr_range *
+pci_addr_cache_insert(struct pci_dev *dev, unsigned long alo,
+		      unsigned long ahi, unsigned int flags)
+{
+	struct rb_node **p = &pci_io_addr_cache_root.rb_root.rb_node;
+	struct rb_node *parent = NULL;
+	struct pci_io_addr_range *piar;
+
+	/* Walk tree, find a place to insert into tree */
+	while (*p) {
+		parent = *p;
+		piar = rb_entry(parent, struct pci_io_addr_range, rb_node);
+		if (alo < piar->addr_lo) {
+			p = &parent->rb_left;
+		} else if (ahi > piar->addr_hi) {
+			p = &parent->rb_right;
+		} else {
+			if (dev != piar->pcidev ||
+			    alo != piar->addr_lo || ahi != piar->addr_hi) {
+				printk(KERN_WARNING "PIAR: overlapping address range\n");
+			}
+			return piar;
+		}
+	}
+	piar = (struct pci_io_addr_range *)kmalloc(sizeof(struct pci_io_addr_range), GFP_ATOMIC);
+	if (!piar)
+		return NULL;
+
+	piar->addr_lo = alo;
+	piar->addr_hi = ahi;
+	piar->pcidev = dev;
+	piar->flags = flags;
+
+	rb_link_node(&piar->rb_node, parent, p);
+	rb_insert_color(&piar->rb_node, &pci_io_addr_cache_root.rb_root);
+
+	return piar;
+}
+
+static void __pci_addr_cache_insert_device(struct pci_dev *dev)
+{
+	struct device_node *dn;
+	int i;
+
+	dn = pci_device_to_OF_node(dev);
+	if (!dn) {
+		printk(KERN_WARNING "PCI: no pci dn found for dev=%s %s\n",
+			pci_name(dev), pci_pretty_name(dev));
+		pci_dev_put(dev);
+		return;
+	}
+
+	/* Skip any devices for which EEH is not enabled. */
+	if (!(dn->eeh_mode & EEH_MODE_SUPPORTED) ||
+	    dn->eeh_mode & EEH_MODE_NOCHECK) {
+#ifdef DEBUG
+		printk(KERN_INFO "PCI: skip building address cache for=%s %s\n",
+		       pci_name(dev), pci_pretty_name(dev));
+#endif
+		pci_dev_put(dev);
+		return;
+	}
+
+	/* Walk resources on this device, poke them into the tree */
+	for (i = 0; i < DEVICE_COUNT_RESOURCE; i++) {
+		unsigned long start = pci_resource_start(dev,i);
+		unsigned long end = pci_resource_end(dev,i);
+		unsigned int flags = pci_resource_flags(dev,i);
+
+		/* We are interested only bus addresses, not dma or other stuff */
+		if (0 == (flags & (IORESOURCE_IO | IORESOURCE_MEM)))
+			continue;
+		if (start == 0 || ~start == 0 || end == 0 || ~end == 0)
+			 continue;
+		pci_addr_cache_insert(dev, start, end, flags);
+	}
+}
+
+/**
+ * pci_addr_cache_insert_device - Add a device to the address cache
+ * @dev: PCI device whose I/O addresses we are interested in.
+ *
+ * In order to support the fast lookup of devices based on addresses,
+ * we maintain a cache of devices that can be quickly searched.
+ * This routine adds a device to that cache.
+ */
+void pci_addr_cache_insert_device(struct pci_dev *dev)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&pci_io_addr_cache_root.piar_lock, flags);
+	__pci_addr_cache_insert_device(dev);
+	spin_unlock_irqrestore(&pci_io_addr_cache_root.piar_lock, flags);
+}
+
+static inline void __pci_addr_cache_remove_device(struct pci_dev *dev)
+{
+	struct rb_node *n;
+
+restart:
+	n = rb_first(&pci_io_addr_cache_root.rb_root);
+	while (n) {
+		struct pci_io_addr_range *piar;
+		piar = rb_entry(n, struct pci_io_addr_range, rb_node);
+
+		if (piar->pcidev == dev) {
+			rb_erase(n, &pci_io_addr_cache_root.rb_root);
+			kfree(piar);
+			goto restart;
+		}
+		n = rb_next(n);
+	}
+	pci_dev_put(dev);
+}
+
+/**
+ * pci_addr_cache_remove_device - remove pci device from addr cache
+ * @dev: device to remove
+ *
+ * Remove a device from the addr-cache tree.
+ * This is potentially expensive, since it will walk
+ * the tree multiple times (once per resource).
+ * But so what; device removal doesn't need to be that fast.
+ */
+void pci_addr_cache_remove_device(struct pci_dev *dev)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&pci_io_addr_cache_root.piar_lock, flags);
+	__pci_addr_cache_remove_device(dev);
+	spin_unlock_irqrestore(&pci_io_addr_cache_root.piar_lock, flags);
+}
+
+/**
+ * pci_addr_cache_build - Build a cache of I/O addresses
+ *
+ * Build a cache of pci i/o addresses.  This cache will be used to
+ * find the pci device that corresponds to a given address.
+ * This routine scans all pci busses to build the cache.
+ * Must be run late in boot process, after the pci controllers
+ * have been scaned for devices (after all device resources are known).
+ */
+void __init pci_addr_cache_build(void)
+{
+	struct pci_dev *dev = NULL;
+
+	spin_lock_init(&pci_io_addr_cache_root.piar_lock);
+
+	while ((dev = pci_get_device(PCI_ANY_ID, PCI_ANY_ID, dev)) != NULL) {
+		/* Ignore PCI bridges ( XXX why ??) */
+		if ((dev->class >> 16) == PCI_BASE_CLASS_BRIDGE) {
+			pci_dev_put(dev);
+			continue;
+		}
+		pci_addr_cache_insert_device(dev);
+	}
+
+#ifdef DEBUG
+	/* Verify tree built up above, echo back the list of addrs. */
+	pci_addr_cache_print(&pci_io_addr_cache_root);
+#endif
+}
+
+/**
+ * eeh_token_to_phys - convert EEH address token to phys address
+ * @token i/o token, should be address in the form 0xA....
+ *
+ * Converts EEH address tokens into physical addresses.  Note that
+ * ths routine does *not* convert I/O BAR addresses (which start
+ * with 0xE...) to phys addresses!
+ */
+static unsigned long eeh_token_to_phys(unsigned long token)
 {
-	if (REGION_ID(token) == EEH_REGION_ID) {
-		unsigned long vaddr = IO_TOKEN_TO_ADDR(token);
-		pte_t *ptep = find_linux_pte(ioremap_mm.pgd, vaddr);
-		unsigned long pa = pte_pfn(*ptep) << PAGE_SHIFT;
-		return pa | (vaddr & (PAGE_SIZE-1));
-	} else
+	pte_t *ptep;
+	unsigned long pa, vaddr;
+
+	if (REGION_ID(token) == EEH_REGION_ID)
+		vaddr = IO_TOKEN_TO_ADDR(token);
+	else
 		return token;
+
+	ptep = find_linux_pte(ioremap_mm.pgd, vaddr);
+	pa = pte_pfn(*ptep) << PAGE_SHIFT;
+
+	return pa | (vaddr & (PAGE_SIZE-1));
 }
 
-/* Check for an eeh failure at the given token address.
+/**
+ * eeh_check_failure - check if all 1's data is due to EEH slot freeze
+ * @token i/o token, should be address in the form 0xA....
+ * @val value, should be all 1's (XXX why do we need this arg??)
+ *
+ * Check for an eeh failure at the given token address.
  * The given value has been read and it should be 1's (0xff, 0xffff or
  * 0xffffffff).
  *
  * Probe to determine if an error actually occurred.  If not return val.
  * Otherwise panic.
+ *
+ * Note this routine might be called in an interrupt context ...
  */
 unsigned long eeh_check_failure(void *token, unsigned long val)
 {
@@ -81,81 +366,97 @@ unsigned long eeh_check_failure(void *to
 	struct pci_dev *dev;
 	struct device_node *dn;
 	unsigned long ret, rets[2];
+	static spinlock_t lock = SPIN_LOCK_UNLOCKED;
+	/* dont want this on the stack */
+	static unsigned char slot_err_buf[RTAS_ERROR_LOG_MAX];
+	unsigned long flags;
 
-	/* IO BAR access could get us here...or if we manually force EEH
-	 * operation on even if the hardware won't support it.
-	 */
-	if (!eeh_implemented || ibm_read_slot_reset_state == RTAS_UNKNOWN_SERVICE)
+	__get_cpu_var(total_mmio_ffs)++;
+
+	if (!eeh_subsystem_enabled)
 		return val;
 
-	/* Finding the phys addr + pci device is quite expensive.
-	 * However, the RTAS call is MUCH slower.... :(
-	 */
+	/* Finding the phys addr + pci device; this is pretty quick. */
 	addr = eeh_token_to_phys((unsigned long)token);
-	dev = pci_find_dev_by_addr(addr);
-	if (!dev) {
-		printk("EEH: no pci dev found for addr=0x%lx\n", addr);
+	dev = pci_get_device_by_addr(addr);
+	if (!dev)
 		return val;
-	}
+
 	dn = pci_device_to_OF_node(dev);
 	if (!dn) {
-		printk("EEH: no pci dn found for addr=0x%lx\n", addr);
+		pci_dev_put(dev);
 		return val;
 	}
 
 	/* Access to IO BARs might get this far and still not want checking. */
-	if (!(dn->eeh_mode & EEH_MODE_SUPPORTED) || dn->eeh_mode & EEH_MODE_NOCHECK)
+	if (!(dn->eeh_mode & EEH_MODE_SUPPORTED) ||
+	    dn->eeh_mode & EEH_MODE_NOCHECK) {
+		pci_dev_put(dev);
 		return val;
+	}
 
+	if (!dn->eeh_config_addr) {
+		pci_dev_put(dev);
+		return val;
+	}
 
-	/* Now test for an EEH failure.  This is VERY expensive.
+	/*
+	 * Now test for an EEH failure.  This is VERY expensive.
 	 * Note that the eeh_config_addr may be a parent device
 	 * in the case of a device behind a bridge, or it may be
 	 * function zero of a multi-function device.
 	 * In any case they must share a common PHB.
 	 */
-	if (dn->eeh_config_addr) {
-		ret = rtas_call(ibm_read_slot_reset_state, 3, 3, rets,
-				dn->eeh_config_addr, BUID_HI(dn->phb->buid),
-				BUID_LO(dn->phb->buid));
-		if (ret == 0 && rets[1] == 1 && rets[0] >= 2) {
-			unsigned long	slot_err_ret;
-
-			memset(slot_err_buf, 0, RTAS_ERROR_LOG_MAX);
-			slot_err_ret = rtas_call(rtas_token("ibm,slot-error-detail"),
-						 8, 1, NULL, dn->eeh_config_addr,
-						 BUID_HI(dn->phb->buid),
-						 BUID_LO(dn->phb->buid), NULL, 0,
-						 __pa(slot_err_buf), RTAS_ERROR_LOG_MAX,
-						 2 /* Permanent Error */);
-
-			if (slot_err_ret == 0)
-				log_error(slot_err_buf, ERR_TYPE_RTAS_LOG, 1 /* Fatal */);
-
-			/*
-			 * XXX We should create a separate sysctl for this.
-			 *
-			 * Since the panic_on_oops sysctl is used to halt
-			 * the system in light of potential corruption, we
-			 * can use it here.
-			 */
-			if (panic_on_oops)
-				panic("EEH: MMIO failure (%ld) on device:\n%s\n",
-				      rets[0], pci_name(dev));
-			else
-				printk("EEH: MMIO failure (%ld) on device:\n%s\n",
-				       rets[0], pci_name(dev));
+	ret = rtas_call(ibm_read_slot_reset_state, 3, 3, rets,
+			dn->eeh_config_addr, BUID_HI(dn->phb->buid),
+			BUID_LO(dn->phb->buid));
+
+	if (ret == 0 && rets[1] == 1 && rets[0] >= 2) {
+		unsigned long slot_err_ret;
+
+		spin_lock_irqsave(&lock, flags);
+		memset(slot_err_buf, 0, RTAS_ERROR_LOG_MAX);
+		slot_err_ret = rtas_call(rtas_token("ibm,slot-error-detail"),
+					 8, 1, NULL, dn->eeh_config_addr,
+					 BUID_HI(dn->phb->buid),
+					 BUID_LO(dn->phb->buid), NULL, 0,
+					 __pa(slot_err_buf),
+					 RTAS_ERROR_LOG_MAX,
+					 2 /* Permanent Error */);
+
+		if (slot_err_ret == 0)
+			log_error(slot_err_buf, ERR_TYPE_RTAS_LOG,
+				  1 /* Fatal */);
+
+		spin_unlock_irqrestore(&lock, flags);
+
+		/*
+		 * XXX We should create a separate sysctl for this.
+		 *
+		 * Since the panic_on_oops sysctl is used to halt
+		 * the system in light of potential corruption, we
+		 * can use it here.
+		 */
+		if (panic_on_oops) {
+			panic("EEH: MMIO failure (%ld) on device:%s %s\n",
+			      rets[0], pci_name(dev), pci_pretty_name(dev));
+		} else {
+			__get_cpu_var(ignored_failures)++;
+			printk(KERN_INFO "EEH: MMIO failure (%ld) on device:%s %s\n",
+			       rets[0], pci_name(dev), pci_pretty_name(dev));
 		}
+	} else {
+		__get_cpu_var(false_positives)++;
 	}
-	eeh_false_positives++;
-	return val;	/* good case */
 
+	pci_dev_put(dev);
+	return val;
 }
+EXPORT_SYMBOL(eeh_check_failure);
 
 struct eeh_early_enable_info {
 	unsigned int buid_hi;
 	unsigned int buid_lo;
-	int adapters_enabled;
 };
 
 /* Enable eeh for the given device node. */
@@ -165,7 +466,7 @@ static void *early_enable_eeh(struct dev
 	long ret;
 	char *status = get_property(dn, "status", 0);
 	u32 *class_code = (u32 *)get_property(dn, "class-code", 0);
-	u32 *vendor_id =(u32 *) get_property(dn, "vendor-id", 0);
+	u32 *vendor_id = (u32 *)get_property(dn, "vendor-id", 0);
 	u32 *device_id = (u32 *)get_property(dn, "device-id", 0);
 	u32 *regs;
 	int enable;
@@ -183,7 +484,8 @@ static void *early_enable_eeh(struct dev
 	     *device_id == 0x0188 || *device_id == 0x0302))
 		return NULL;
 
-	/* Now decide if we are going to "Disable" EEH checking
+	/*
+	 * Now decide if we are going to "Disable" EEH checking
 	 * for this device.  We still run with the EEH hardware active,
 	 * but we won't be checking for ff's.  This means a driver
 	 * could return bad data (very bad!), an interrupt handler could
@@ -194,15 +496,19 @@ static void *early_enable_eeh(struct dev
 	if ((*class_code >> 16) == PCI_BASE_CLASS_DISPLAY)
 		enable = 0;
 
-	if (!eeh_check_opts_config(dn, *class_code, *vendor_id, *device_id, enable)) {
+	if (!eeh_check_opts_config(dn, *class_code, *vendor_id, *device_id,
+				   enable)) {
 		if (enable) {
-			printk(KERN_INFO "EEH: %s user requested to run without EEH.\n", dn->full_name);
+			printk(KERN_WARNING "EEH: %s user requested to run "
+			       "without EEH.\n", dn->full_name);
 			enable = 0;
 		}
 	}
 
-	if (!enable)
+	if (!enable) {
 		dn->eeh_mode = EEH_MODE_NOCHECK;
+		return NULL;
+	}
 
 	/* This device may already have an EEH parent. */
 	if (dn->parent && (dn->parent->eeh_mode & EEH_MODE_SUPPORTED)) {
@@ -212,7 +518,7 @@ static void *early_enable_eeh(struct dev
 		return NULL;
 	}
 
-	/* Ok..see if this device supports EEH. */
+	/* Ok... see if this device supports EEH. */
 	regs = (u32 *)get_property(dn, "reg", 0);
 	if (regs) {
 		/* First register entry is addr (00BBSS00)  */
@@ -221,16 +527,27 @@ static void *early_enable_eeh(struct dev
 				regs[0], info->buid_hi, info->buid_lo,
 				EEH_ENABLE);
 		if (ret == 0) {
-			info->adapters_enabled++;
+			eeh_subsystem_enabled = 1;
 			dn->eeh_mode |= EEH_MODE_SUPPORTED;
 			dn->eeh_config_addr = regs[0];
+#ifdef DEBUG
+			printk(KERN_DEBUG "EEH: %s: eeh enabled\n",
+			       dn->full_name);
+#endif
+		} else {
+			printk(KERN_WARNING "EEH: %s: rtas_call failed.\n",
+			       dn->full_name);
 		}
+	} else {
+		printk(KERN_WARNING "EEH: %s: unable to get reg property.\n",
+		       dn->full_name);
 	}
+
 	return NULL; 
 }
 
 /*
- * Initialize eeh by trying to enable it for all of the adapters in the system.
+ * Initialize EEH by trying to enable it for all of the adapters in the system.
  * As a side effect we can determine here if eeh is supported at all.
  * Note that we leave EEH on so failed config cycles won't cause a machine
  * check.  If a user turns off EEH for a particular adapter they are really
@@ -240,43 +557,35 @@ static void *early_enable_eeh(struct dev
  * but for now disabling EEH for adapters is mostly to work around drivers that
  * directly access mmio space (without using the macros).
  *
- * The eeh-force-off/on option does literally what it says, so if Linux must
+ * The eeh-force-off option does literally what it says, so if Linux must
  * avoid enabling EEH this must be done.
  */
-void eeh_init(void)
+void __init eeh_init(void)
 {
 	struct device_node *phb;
 	struct eeh_early_enable_info info;
 	char *eeh_force_off = strstr(saved_command_line, "eeh-force-off");
-	char *eeh_force_on = strstr(saved_command_line, "eeh-force-on");
 
 	ibm_set_eeh_option = rtas_token("ibm,set-eeh-option");
 	ibm_set_slot_reset = rtas_token("ibm,set-slot-reset");
 	ibm_read_slot_reset_state = rtas_token("ibm,read-slot-reset-state");
 
-	/* Allow user to force eeh mode on or off -- even if the hardware
-	 * doesn't exist.  This allows driver writers to at least test use
-	 * of I/O macros even if we can't actually test for EEH failure.
-	 */
-	if (eeh_force_on > eeh_force_off)
-		eeh_implemented = 1;
-	else if (ibm_set_eeh_option == RTAS_UNKNOWN_SERVICE)
+	if (ibm_set_eeh_option == RTAS_UNKNOWN_SERVICE)
 		return;
 
-	if (eeh_force_off > eeh_force_on) {
-		/* User is forcing EEH off.  Be noisy if it is implemented. */
-		if (eeh_implemented)
-			printk(KERN_WARNING "EEH: WARNING: PCI Enhanced I/O Error Handling is user disabled\n");
-		eeh_implemented = 0;
+	if (eeh_force_off) {
+		printk(KERN_WARNING "EEH: WARNING: PCI Enhanced I/O Error "
+		       "Handling is user disabled\n");
 		return;
 	}
 
-
 	/* Enable EEH for all adapters.  Note that eeh requires buid's */
-	info.adapters_enabled = 0;
-	for (phb = of_find_node_by_name(NULL, "pci"); phb; phb = of_find_node_by_name(phb, "pci")) {
+	for (phb = of_find_node_by_name(NULL, "pci"); phb;
+	     phb = of_find_node_by_name(phb, "pci")) {
 		int len;
-		int *buid_vals = (int *) get_property(phb, "ibm,fw-phb-id", &len);
+		int *buid_vals;
+
+		buid_vals = (int *)get_property(phb, "ibm,fw-phb-id", &len);
 		if (!buid_vals)
 			continue;
 		if (len == sizeof(int)) {
@@ -286,35 +595,80 @@ void eeh_init(void)
 			info.buid_hi = buid_vals[0];
 			info.buid_lo = buid_vals[1];
 		} else {
-			printk("EEH: odd ibm,fw-phb-id len returned: %d\n", len);
+			printk(KERN_INFO "EEH: odd ibm,fw-phb-id len returned: %d\n", len);
 			continue;
 		}
 		traverse_pci_devices(phb, early_enable_eeh, NULL, &info);
 	}
-	if (info.adapters_enabled) {
+
+	if (eeh_subsystem_enabled)
 		printk(KERN_INFO "EEH: PCI Enhanced I/O Error Handling Enabled\n");
-		eeh_implemented = 1;
-	}
 }
 
-
-int eeh_set_option(struct pci_dev *dev, int option)
+/**
+ * eeh_add_device - perform EEH initialization for the indicated pci device
+ * @dev: pci device for which to set up EEH
+ *
+ * This routine can be used to perform EEH initialization for PCI
+ * devices that were added after system boot (e.g. hotplug, dlpar).
+ * Whether this actually enables EEH or not for this device depends
+ * on the type of the device, on earlier boot command-line
+ * arguments & etc.
+ */
+void eeh_add_device(struct pci_dev *dev)
 {
-	struct device_node *dn = pci_device_to_OF_node(dev);
-	struct pci_controller *phb = PCI_GET_PHB_PTR(dev);
+	struct device_node *dn;
+	struct pci_controller *phb;
+	struct eeh_early_enable_info info;
 
-	if (dn == NULL || phb == NULL || phb->buid == 0 || !eeh_implemented)
-		return -2;
+	if (!dev || !eeh_subsystem_enabled)
+		return;
 
-	return rtas_call(ibm_set_eeh_option, 4, 1, NULL,
-			 CONFIG_ADDR(dn->busno, dn->devfn),
-			 BUID_HI(phb->buid), BUID_LO(phb->buid), option);
+#ifdef DEBUG
+	printk(KERN_DEBUG "EEH: adding device %s %s\n", pci_name(dev),
+	       pci_pretty_name(dev));
+#endif
+	dn = pci_device_to_OF_node(dev);
+	if (NULL == dn)
+		return;
+
+	phb = PCI_GET_PHB_PTR(dev);
+	if (NULL == phb || 0 == phb->buid) {
+		printk(KERN_WARNING "EEH: Expected buid but found none\n");
+		return;
+	}
+
+	info.buid_hi = BUID_HI(phb->buid);
+	info.buid_lo = BUID_LO(phb->buid);
+
+	early_enable_eeh(dn, &info);
+	pci_addr_cache_insert_device (dev);
 }
 
+/**
+ * eeh_remove_device - undo EEH setup for the indicated pci device
+ * @dev: pci device to be removed
+ *
+ * This routine should be when a device is removed from a running
+ * system (e.g. by hotplug or dlpar).
+ */
+void eeh_remove_device(struct pci_dev *dev)
+{
+	if (!dev || !eeh_subsystem_enabled)
+		return;
+
+	/* Unregister the device with the EEH/PCI address search system */
+#ifdef DEBUG
+	printk(KERN_DEBUG "EEH: remove device %s %s\n", pci_name(dev),
+	       pci_pretty_name(dev));
+#endif
+	pci_addr_cache_remove_device(dev);
+}
 
-/* If EEH is implemented, find the PCI device using given phys addr
+/*
+ * If EEH is implemented, find the PCI device using given phys addr
  * and check to see if eeh failure checking is disabled.
- * Remap the addr (trivially) to the EEH region if not.
+ * Remap the addr (trivially) to the EEH region if EEH checking enabled.
  * For addresses not known to PCI the vaddr is simply returned unchanged.
  */
 void *eeh_ioremap(unsigned long addr, void *vaddr)
@@ -322,43 +676,78 @@ void *eeh_ioremap(unsigned long addr, vo
 	struct pci_dev *dev;
 	struct device_node *dn;
 
-	if (!eeh_implemented)
+	if (!eeh_subsystem_enabled)
 		return vaddr;
-	dev = pci_find_dev_by_addr(addr);
+
+	dev = pci_get_device_by_addr(addr);
 	if (!dev)
 		return vaddr;
+
 	dn = pci_device_to_OF_node(dev);
-	if (!dn)
+	if (!dn) {
+		pci_dev_put(dev);
 		return vaddr;
-	if (dn->eeh_mode & EEH_MODE_NOCHECK)
+	}
+
+	if (dn->eeh_mode & EEH_MODE_NOCHECK) {
+		pci_dev_put(dev);
 		return vaddr;
+	}
 
+	pci_dev_put(dev);
 	return (void *)IO_ADDR_TO_TOKEN(vaddr);
 }
 
-static int eeh_proc_falsepositive_read(char *page, char **start, off_t off,
-			 int count, int *eof, void *data)
+static int proc_eeh_show(struct seq_file *m, void *v)
 {
-	int len;
-	len = sprintf(page, "eeh_false_positives=%ld\n"
-		      "eeh_total_mmio_ffs=%ld\n",
-		      eeh_false_positives, eeh_total_mmio_ffs);
-	return len;
+	unsigned int cpu;
+	unsigned long ffs = 0, positives = 0, failures = 0;
+
+	for_each_cpu(cpu) {
+		ffs += per_cpu(total_mmio_ffs, cpu);
+		positives += per_cpu(false_positives, cpu);
+		failures += per_cpu(ignored_failures, cpu);
+	}
+
+	if (0 == eeh_subsystem_enabled) {
+		seq_printf(m, "EEH Subsystem is globally disabled\n");
+		seq_printf(m, "eeh_total_mmio_ffs=%ld\n", ffs);
+	} else {
+		seq_printf(m, "EEH Subsystem is enabled\n");
+		seq_printf(m, "eeh_total_mmio_ffs=%ld\n"
+			   "eeh_false_positives=%ld\n"
+			   "eeh_ignored_failures=%ld\n",
+			   ffs, positives, failures);
+	}
+
+	return 0;
 }
 
-/* Implementation of /proc/ppc64/eeh
- * For now it is one file showing false positives.
- */
+static int proc_eeh_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, proc_eeh_show, NULL);
+}
+
+static struct file_operations proc_eeh_operations = {
+	.open		= proc_eeh_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
 static int __init eeh_init_proc(void)
 {
-	struct proc_dir_entry *ent = create_proc_entry("ppc64/eeh", S_IRUGO, 0);
-	if (ent) {
-		ent->nlink = 1;
-		ent->data = NULL;
-		ent->read_proc = (void *)eeh_proc_falsepositive_read;
+	struct proc_dir_entry *e;
+
+	if (systemcfg->platform & PLATFORM_PSERIES) {
+		e = create_proc_entry("ppc64/eeh", 0, NULL);
+		if (e)
+			e->proc_fops = &proc_eeh_operations;
 	}
-	return 0;
+
+        return 0;
 }
+__initcall(eeh_init_proc);
 
 /*
  * Test if "dev" should be configured on or off.
@@ -386,10 +775,12 @@ static int eeh_check_opts_config(struct 
 	strs[nstrs++] = classname;
 	strs[nstrs++] = "";	/* yes, this matches the empty string */
 
-	/* Now see if any string matches the eeh_opts list.
+	/*
+	 * Now see if any string matches the eeh_opts list.
 	 * The eeh_opts list entries start with + or -.
 	 */
-	for (s = eeh_opts; s && (s < (eeh_opts + eeh_opts_last)); s += strlen(s)+1) {
+	for (s = eeh_opts; s && (s < (eeh_opts + eeh_opts_last));
+	     s += strlen(s)+1) {
 		for (i = 0; i < nstrs; i++) {
 			if (strcasecmp(strs[i], s+1) == 0) {
 				ret = (strs[i][0] == '+') ? 1 : 0;
@@ -399,7 +790,8 @@ static int eeh_check_opts_config(struct 
 	return ret;
 }
 
-/* Handle kernel eeh-on & eeh-off cmd line options for eeh.
+/*
+ * Handle kernel eeh-on & eeh-off cmd line options for eeh.
  *
  * We support:
  *	eeh-off=loc1,loc2,loc3...
@@ -420,7 +812,8 @@ static int eeh_check_opts_config(struct 
  * so eeh-off means eeh by default is off.
  */
 
-/* This is implemented as a null separated list of strings.
+/*
+ * This is implemented as a null separated list of strings.
  * Each string looks like this:  "+X" or "-X"
  * where X is a loc code, vendor:device, class (as shown above)
  * or empty which is used to indicate all.
@@ -428,10 +821,10 @@ static int eeh_check_opts_config(struct 
  * We interpret this option string list so that it will literally
  * behave left-to-right even if some combinations don't make sense.
  */
-
 static int __init eeh_parm(char *str, int state)
 {
 	char *s, *cur, *curend;
+
 	if (!eeh_opts) {
 		eeh_opts = alloc_bootmem(EEH_MAX_OPTS);
 		eeh_opts[eeh_opts_last++] = '+'; /* default */
@@ -446,15 +839,17 @@ static int __init eeh_parm(char *str, in
 		str++;
 	for (s = str; s && *s != '\0'; s = curend) {
 		cur = s;
+		/* ignore empties.  Don't treat as "all-on" or "all-off" */
 		while (*cur == ',')
-			cur++;	/* ignore empties.  Don't treat as "all-on" or "all-off" */
+			cur++;
 		curend = strchr(cur, ',');
 		if (!curend)
 			curend = cur + strlen(cur);
 		if (*cur) {
 			int curlen = curend-cur;
 			if (eeh_opts_last + curlen > EEH_MAX_OPTS-2) {
-				printk(KERN_INFO "EEH: sorry...too many eeh cmd line options\n");
+				printk(KERN_WARNING "EEH: sorry...too many "
+				       "eeh cmd line options\n");
 				return 1;
 			}
 			eeh_opts[eeh_opts_last++] = state ? '+' : '-';
@@ -463,6 +858,7 @@ static int __init eeh_parm(char *str, in
 			eeh_opts[eeh_opts_last++] = '\0';
 		}
 	}
+
 	return 1;
 }
 
@@ -476,6 +872,5 @@ static int __init eehon_parm(char *str)
 	return eeh_parm(str, 1);
 }
 
-__initcall(eeh_init_proc);
 __setup("eeh-off", eehoff_parm);
 __setup("eeh-on", eehon_parm);
diff -purN linux-2.5/arch/ppc64/kernel/hvconsole.c linuxppc64-2.5/arch/ppc64/kernel/hvconsole.c
--- linux-2.5/arch/ppc64/kernel/hvconsole.c	2004-02-05 21:10:52.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/hvconsole.c	2004-02-27 21:16:22.000000000 +0000
@@ -20,15 +20,139 @@
  */
 
 #include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/list.h>
 #include <asm/hvcall.h>
 #include <asm/prom.h>
 #include <asm/hvconsole.h>
+#include <asm/termios.h>
+#include <asm/uaccess.h>
+#include <asm/io.h>
 
-int hvc_get_chars(int index, char *buf, int count)
+#define __ALIGNED__	__attribute__((__aligned__(8)))
+
+#define HVCS_LONG_INVALID	0xFFFFFFFFFFFFFFFF
+
+struct vtty_struct {
+	uint32_t vtermno;
+	int (*get_chars)(struct vtty_struct *vtty, char *buf, int count);
+	int (*put_chars)(struct vtty_struct *vtty, const char *buf, int count);
+	int (*ioctl)(struct vtty_struct *vtty, unsigned int cmd, unsigned long val);
+	uint16_t seqno; /* HVSI packet sequence number */
+};
+static struct vtty_struct vttys[MAX_NR_HVC_CONSOLES];
+
+#define HVSI_VERSION 1
+
+#define VS_DATA_PACKET_HEADER           0xff
+#define VS_CONTROL_PACKET_HEADER        0xfe
+#define VS_QUERY_PACKET_HEADER          0xfd
+#define VS_QUERY_RESPONSE_PACKET_HEADER 0xfc
+
+/* control verbs */
+#define VSV_SET_MODEM_CTL    1 /* to service processor only */
+#define VSV_MODEM_CTL_UPDATE 2 /* from service processor only */
+#define VSV_CLOSE_PROTOCOL   3
+
+/* query verbs */
+#define VSV_SEND_VERSION_NUMBER 1
+
+/* yes, these masks are not consecutive. */
+#define TSDTR 0x1
+#define TSCD 0x20
+
+struct hvsi1_header {
+	uint8_t  type;
+	uint8_t  len;
+	uint16_t seqno;
+};
+
+struct hvsi1_control {
+	uint8_t  type;
+	uint8_t  len;
+	uint16_t seqno;
+	uint8_t  version;
+	uint8_t  verb;
+	/* optional depending on verb: */
+	uint32_t word;
+	uint32_t mask;
+};
+
+struct hvsi1_query {
+	uint8_t  type;
+	uint8_t  len;
+	uint16_t seqno;
+	uint8_t  version;
+	uint8_t  verb;
+};
+
+struct hvsi1_query_resp {
+	uint8_t  type;
+	uint8_t  len;
+	uint16_t seqno;
+	uint8_t  version;
+	uint8_t  verb;
+	uint16_t query_seqno;
+	/* optional, depending on query type */
+	union {
+		uint8_t version;
+	} response;
+};
+
+/* ring buffer stuff: */
+struct packet_desc {
+	struct hvsi1_header *pkt;
+	int remaining;
+};
+#define N_PACKETS 4
+static struct packet_desc ring[N_PACKETS];
+static struct packet_desc *write=ring; /* next packet to write to */
+static struct packet_desc *read=ring;  /* next packet to read from */
+
+static struct packet_desc *next_desc(struct packet_desc *cur)
+{
+	if ((cur+1) > ring + (N_PACKETS-1))
+		return ring;
+	return (cur+1);
+}
+
+static inline int hdrlen(const struct hvsi1_header *pkt)
+{
+	const int lengths[] = { 4, 6, 6, 8, };
+	int index = VS_DATA_PACKET_HEADER - pkt->type;
+
+	if (index > sizeof(lengths)/sizeof(lengths[0]))
+		panic("%s: unknown packet type\n", __FUNCTION__);
+
+	return lengths[index];
+}
+
+static inline uint8_t *pktdata(const struct hvsi1_header *pkt)
+{
+	return (uint8_t *)pkt + hdrlen(pkt);
+}
+
+static inline int pktlen(const struct hvsi1_header *pkt)
+{
+	return (int)pkt->len;
+}
+
+static inline int datalen(const struct hvsi1_header *pkt)
+{
+	return pktlen(pkt) - hdrlen(pkt);
+}
+
+static void print_hdr(struct hvsi1_header *pkt)
+{
+	printk("type 0x%x, len %i, seqno %i\n", pkt->type, pkt->len, pkt->seqno);
+}
+
+/* normal hypervisor virtual console code */
+int hvterm_get_chars(uint32_t vtermno, char *buf, int count)
 {
 	unsigned long got;
 
-	if (plpar_hcall(H_GET_TERM_CHAR, index, 0, 0, 0, &got,
+	if (plpar_hcall(H_GET_TERM_CHAR, vtermno, 0, 0, 0, &got,
 		(unsigned long *)buf, (unsigned long *)buf+1) == H_Success) {
 		/*
 		 * Work around a HV bug where it gives us a null
@@ -49,13 +173,21 @@ int hvc_get_chars(int index, char *buf, 
 	}
 	return 0;
 }
+EXPORT_SYMBOL(hvterm_get_chars);
 
-int hvc_put_chars(int index, const char *buf, int count)
+/* wrapper exists just so that hvterm_get_chars() is callable by outside
+ * drivers without a vtty_struct */
+int hvc_hvterm_get_chars(struct vtty_struct *vtty, char *buf, int count)
+{
+	return hvterm_get_chars(vtty->vtermno, buf, count);
+}
+
+int hvterm_put_chars(uint32_t vtermno, const char *buf, int count)
 {
 	unsigned long *lbuf = (unsigned long *) buf;
 	long ret;
 
-	ret = plpar_hcall_norets(H_PUT_TERM_CHAR, index, count, lbuf[0],
+	ret = plpar_hcall_norets(H_PUT_TERM_CHAR, vtermno, count, lbuf[0],
 				 lbuf[1]);
 	if (ret == H_Success)
 		return count;
@@ -63,26 +195,569 @@ int hvc_put_chars(int index, const char 
 		return 0;
 	return -1;
 }
+EXPORT_SYMBOL(hvterm_put_chars);
+
+/* wrapper exists just so that hvterm_put_chars() is callable by outside
+ * drivers without a vtty_struct */
+int hvc_hvterm_put_chars(struct vtty_struct *vtty, const char *buf, int count)
+{
+	return hvterm_put_chars(vtty->vtermno, buf, count);
+}
+
+/* Host Virtual Serial Interface (HVSI) code */
+
+static int hvsi1_read(struct vtty_struct *vtty, char *buf, int count)
+{
+	unsigned long got;
+
+	if (plpar_hcall(H_GET_TERM_CHAR, vtty->vtermno, 0, 0, 0, &got,
+		(unsigned long *)buf, (unsigned long *)buf+1) == H_Success) {
+		return got;
+	}
+	return 0;
+}
+
+/* load up ring buffers */
+static int hvsi1_load_chunk(struct vtty_struct *vtty)
+{
+	uint8_t localbuf[16];
+	uint8_t *chunk = localbuf;
+	struct hvsi1_header *pkt;
+	int chunklen;
+
+	chunklen = hvsi1_read(vtty, chunk, 16);
+	if (!chunklen)
+		return 0;
+
+	printk("new chunk, len %i\n", chunklen);
+
+	/* fill in unfinished packet */
+	if (write->remaining) {
+		int size = min(write->remaining, chunklen);
+
+		printk("completing partial packet with %i bytes\n", size);
+		memcpy((uint8_t *)write->pkt + pktlen(write->pkt) - write->remaining, chunk, size);
+		write->remaining -= size;
+		if (write->remaining == 0)
+			write = next_desc(write);
+		chunklen -= size;
+		chunk += size;
+	}
+
+	/* new packet(s) */
+	while (chunklen > 0) {
+		int size;
+		pkt = (struct hvsi1_header *)chunk;
+
+		size = min(pktlen(pkt), chunklen);
+		printk("%i bytes of new packet\n", size);
+
+		/* XXX handle 1-byte read */
+		if (size == 1) {
+			panic("%s: can't handle 1-byte reads!\n", __FUNCTION__);
+		}
+
+		memcpy(write->pkt, chunk, size);
+		write->remaining = pktlen(pkt) - size;
+
+		if (write->remaining == 0)
+			write = next_desc(write);
+		chunklen -= size;
+		chunk += size;
+	}
+	return 1;
+}
+
+static void hvsi1_load_buffers(struct vtty_struct *vtty)
+{
+	while (hvsi1_load_chunk(vtty))
+		; /* keep reading from hypervisor until there's no more */
+}
+
+static int hvsi1_recv_control(struct vtty_struct *vtty,
+	struct hvsi1_control *pkt)
+{
+	switch (pkt->verb) {
+		case VSV_MODEM_CTL_UPDATE:
+			if ((pkt->word & TSCD) == 0) {
+				/* CD went away; no more connection */
+				// XXX tty_hangup(hvc->tty);
+				return -EPIPE;
+			}
+			break;
+	}
+	return 0;
+}
+
+/* transfer from ring buffers to caller's buffer */
+static int hvsi1_deliver(struct vtty_struct *vtty, uint8_t *buf, int buflen)
+{
+	int written = 0;
+
+	for (; (read != write) && buflen; read = next_desc(read)) {
+		struct hvsi1_header *pkt = read->pkt;
+		int size;
+
+		switch (pkt->type) {
+			case VS_DATA_PACKET_HEADER:
+				size = min(datalen(pkt), buflen);
+				printk("delivering %i-sized data packet\n", size);
+				memcpy(buf, pktdata(pkt), size);
+				buf += size;
+				buflen -= size;
+				written += size;
+				break;
+			case VS_CONTROL_PACKET_HEADER:
+				hvsi1_recv_control(vtty, (struct hvsi1_control *)pkt);
+				break;
+			default:
+				printk("%s: ignoring HVSI packet ", __FUNCTION__);
+				print_hdr(pkt);
+				break;
+		}
+	}
+
+	return written;
+}
+
+static int hvsi1_get_chars(struct vtty_struct *vtty, char *databuf, int count)
+{
+	hvsi1_load_buffers(vtty); /* get pending data */
+	return hvsi1_deliver(vtty, databuf, count); /* hand it up */
+}
+
+/* Handshaking step 3:
+ * 
+ * We're waiting for the service processor to query our version, at which point
+ * we immediately respond and then we have an open HVSI connection. */
+static int hvsi1_handshake3(struct vtty_struct *vtty, char *databuf, int count)
+{
+	struct hvsi1_query_resp response __ALIGNED__ = {
+		.type = VS_QUERY_RESPONSE_PACKET_HEADER,
+		.len = 9,
+		.version = HVSI_VERSION,
+		.verb = VSV_SEND_VERSION_NUMBER,
+		.response.version = HVSI_VERSION,
+	};
+	int done;
+
+	/* bring in queued packets */
+	hvsi1_load_buffers(vtty);
+
+	/* look for the version query packet */
+	for (done = 0; (!done) && (read != write); read = next_desc(read)) {
+		struct hvsi1_header *pkt = read->pkt;
+		struct hvsi1_query *query;
+		int wrote;
+
+		switch (pkt->type) {
+			case VS_QUERY_PACKET_HEADER:
+				query = (struct hvsi1_query *)pkt;
+
+				/* send query response */
+				response.seqno = ++vtty->seqno;
+				response.query_seqno = query->seqno+1,
+				wrote = hvc_hvterm_put_chars(vtty, (char *)&response,
+					response.len);
+				if (wrote != response.len) {
+					/* uh oh, command didn't go through? */
+					printk(KERN_ERR "%s: couldn't send query response!\n",
+						__FUNCTION__);
+					return -EIO;
+				}
+
+				/* we're open for business */
+				vtty->get_chars = hvsi1_get_chars;
+				done = 1;
+				break;
+			default:
+				printk("%s: ignoring HVSI packet ", __FUNCTION__);
+				print_hdr(pkt);
+				break;
+		}
+	}
+
+	return 0; /* nothing written to databuf */
+}
 
-/* return the number of client vterms present */
-/* XXX this requires an interface change to handle multiple discontiguous
- * vterms */
-int hvc_count(int *start_termno)
+/* Handshaking step 2:
+ *
+ * We've sent a version query; now we're waiting for the service processor to
+ * respond. Since we haven't established a connection yet, we won't be writing
+ * anything into databuf. */
+static int hvsi1_handshake2(struct vtty_struct *vtty, char *databuf, int count)
+{
+	int done;
+
+	/* bring in queued packets */
+	hvsi1_load_buffers(vtty);
+
+	/* look for the version query response packet */
+	for (done = 0; (!done) && (read != write); read = next_desc(read)) {
+		struct hvsi1_header *pkt = read->pkt;
+
+		switch (pkt->type) {
+			case VS_QUERY_RESPONSE_PACKET_HEADER:
+				/* XXX check response */
+				vtty->get_chars = hvsi1_handshake3;
+				done = 1;
+				break;
+			default:
+				printk("%s: ignoring HVSI packet ", __FUNCTION__);
+				print_hdr(pkt);
+				break;
+		}
+	}
+
+	return 0; /* nothing written to databuf */
+}
+
+/* Handshaking step 1:
+ *
+ * Send a version query to SP. */
+static int hvsi1_handshake1(uint32_t vtermno)
+{
+	struct hvsi1_query packet __ALIGNED__ = {
+		.type = VS_QUERY_PACKET_HEADER,
+		.len = sizeof(struct hvsi1_query),
+		.seqno = 0,
+		.version = HVSI_VERSION,
+		.verb = VSV_SEND_VERSION_NUMBER,
+	};
+	uint64_t *lbuf = (uint64_t *)&packet;
+	uint64_t dummy;
+
+	plpar_hcall(H_PUT_TERM_CHAR, vtermno, sizeof(struct hvsi1_query),
+	    lbuf[0], lbuf[1], &dummy, &dummy, &dummy);
+
+	return 0;
+}
+
+static int hvsi1_put_chars(struct vtty_struct *vtty, const char *buf, int count)
+{
+	char packet[16] __ALIGNED__;
+	uint64_t dummy;
+	uint64_t *lbuf = (uint64_t *)packet;
+	struct hvsi1_header *hdr = (struct hvsi1_header *)packet;
+	long ret;
+
+	hdr->type = VS_DATA_PACKET_HEADER;
+	hdr->seqno = ++vtty->seqno;
+
+	if (count > 12)
+		count = 12; /* we'll leave some chars behind in buf */
+	hdr->len = count + sizeof(struct hvsi1_header);
+	memcpy(packet + sizeof(struct hvsi1_header), buf, count);
+
+	ret = plpar_hcall(H_PUT_TERM_CHAR, vtty->vtermno, count, lbuf[0], lbuf[1],
+			  &dummy, &dummy, &dummy);
+	if (ret == H_Success)
+		return count;
+	if (ret == H_Busy)
+		return 0;
+	return -1;
+}
+
+#if 0
+static int hvsi1_send_dtr(struct vtty_struct *vtty, int set)
+{
+	struct hvsi1_control command __ALIGNED__ = {
+		.type = VS_CONTROL_PACKET_HEADER,
+		.len = 16,
+		.version = HVSI_VERSION,
+		.verb = VSV_SET_MODEM_CTL,
+		.mask = TSDTR,
+	};
+	int wrote;
+
+	command.seqno = ++vtty->seqno;
+
+	if (set)
+		command.word = TSDTR;
+	else
+		command.word = 0;
+
+	wrote = hvc_hvterm_put_chars(vtty, (char *)&command, command.len);
+	if (wrote != command.len) {
+		/* uh oh, command didn't go through? */
+		printk(KERN_ERR "%s: couldn't set DTR!\n", __FUNCTION__);
+		return -1;
+	}
+
+	return 0;
+}
+
+/* XXX 2.6 tty layer turned TIO* into separate tty_struct functions, not
+ * passed to tty_struct->ioctl() */
+static int hvsi1_ioctl(struct vtty_struct *vtty, unsigned int cmd,
+	unsigned long arg)
+{
+	unsigned long *ptr = (unsigned long *)arg;
+	unsigned long val;
+	int newdtr = -1;
+
+	switch (cmd) {
+		case TIOCMBIS:
+			if (get_user(val, ptr))
+				return -EFAULT;
+			if (val & TIOCM_DTR) {
+				newdtr = 1;
+			}
+			break;
+		case TIOCMBIC:
+			if (get_user(val, ptr))
+				return -EFAULT;
+			if (val & TIOCM_DTR) {
+				newdtr = 0;
+			}
+			break;
+		case TIOCMSET:
+			if (get_user(val, ptr))
+				return -EFAULT;
+			newdtr = val & TIOCM_DTR;
+			break;
+		default:
+			return -ENOIOCTLCMD;
+	}
+
+	if (newdtr != -1) {
+		if (0 > hvsi1_send_dtr(vtty, newdtr))
+			return -EIO;
+	}
+
+	return 0;
+}
+#endif
+
+/* external (hvc_console.c) interface: */
+
+int hvc_get_chars(int index, char *buf, int count)
+{
+	struct vtty_struct *vtty = &vttys[index];
+
+	if (index >= MAX_NR_HVC_CONSOLES)
+		return -1;
+
+	return vtty->get_chars(vtty, buf, count);
+}
+
+int hvc_put_chars(int index, const char *buf, int count)
+{
+	struct vtty_struct *vtty = &vttys[index];
+
+	if (index >= MAX_NR_HVC_CONSOLES)
+		return -1;
+
+	return vtty->put_chars(vtty, buf, count);
+}
+
+int hvc_find_vterms(void)
 {
 	struct device_node *vty;
-	int num_found = 0;
+	int count = 0;
+
+	for (vty = of_find_node_by_name(NULL, "vty"); vty != NULL;
+			vty = of_find_node_by_name(vty, "vty")) {
+		struct vtty_struct *vtty;
+		uint32_t *vtermno;
+
+		vtermno = (uint32_t *)get_property(vty, "reg", NULL);
+		if (!vtermno)
+			continue;
+
+		if (count >= MAX_NR_HVC_CONSOLES)
+			break;
+
+		vtty = &vttys[count];
+		if (device_is_compatible(vty, "hvterm1")) {
+			vtty->vtermno = *vtermno;
+			vtty->get_chars = hvc_hvterm_get_chars;
+			vtty->put_chars = hvc_hvterm_put_chars;
+			vtty->ioctl = NULL;
+			hvc_instantiate();
+			count++;
+		} else if (device_is_compatible(vty, "hvterm-protocol")) {
+			vtty->vtermno = *vtermno;
+			vtty->seqno = 0;
+			vtty->get_chars = hvsi1_handshake2;
+			vtty->put_chars = hvsi1_put_chars;
+			//vtty->ioctl = hvsi1_ioctl;
+			hvsi1_handshake1(vtty->vtermno);
+			hvc_instantiate();
+			count++;
+		}
+	}
+
+	return count;
+}
+
+/* Convert arch specific return codes into relevant errnos.  The hvcs
+ * functions aren't performance sensitive, so this conversion isn't an
+ * issue. */
+int hvcs_convert(long to_convert)
+{
+	switch (to_convert) {
+		case H_Success:
+			return 0;
+		case H_Parameter:
+			return -EINVAL;
+		case H_Hardware:
+			return -EIO;
+		case H_Busy:
+			return -EBUSY;
+		case H_Function: /* fall through */
+		default:
+			return -EPERM;
+	}
+}
 
-	/* consider only the first vty node.
-	 * we should _always_ be able to find one. */
-	vty = of_find_node_by_name(NULL, "vty");
-	if (vty && device_is_compatible(vty, "hvterm1")) {
-		u32 *termno = (u32 *)get_property(vty, "reg", 0);
+int hvcs_free_partner_info(struct list_head *head)
+{
+	struct hvcs_partner_info *pi;
+	struct list_head *element;
 
-		if (termno && start_termno)
-			*start_termno = *termno;
-		num_found = 1;
-		of_node_put(vty);
+	if(!head) {
+		return -EINVAL;
 	}
 
-	return num_found;
+	while (!list_empty(head)) {
+		element = head->next;
+		pi = list_entry(element,struct hvcs_partner_info,node);
+		list_del(element);
+		kfree(pi);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(hvcs_free_partner_info);
+
+/* Helper function for hvcs_get_partner_info */
+int hvcs_next_partner(unsigned int unit_address, unsigned long last_p_partition_ID, unsigned long last_p_unit_address, unsigned long *pi_buff)
+{
+	long retval;
+	retval = plpar_hcall_norets(H_VTERM_PARTNER_INFO, unit_address,
+			last_p_partition_ID,
+				last_p_unit_address, virt_to_phys(pi_buff));
+	return hvcs_convert(retval);
+}
+
+/* The unit_address parameter is the unit address of the vty-server@ vdevice
+ * in whose partner information the caller is interested.  This function
+ * uses a pointer to a list_head instance in which to store the partner info.
+ * This function returns Non-Zero on success, or if there is no partner info.
+ *
+ * Invocation of this function should always be followed by an invocation of
+ * hvcs_free_partner_info() using a pointer to the SAME list head instance
+ * that was used to store the partner_info list.
+ */
+int hvcs_get_partner_info(unsigned int unit_address, struct list_head *head)
+{
+	/* This is a page sized buffer to be passed to hvcall per invocation.
+	 * NOTE: the first long returned is unit_address.  The second long
+	 * returned is the partition ID and starting with pi_buff[2] are
+	 * HVCS_CLC_LENGTH characters, which are diff size than the unsigned
+	 * long, hence the casting mumbojumbo you see later. */
+	unsigned long	*pi_buff;
+	unsigned long	last_p_partition_ID;
+	unsigned long	last_p_unit_address;
+	struct hvcs_partner_info *next_partner_info = NULL;
+	int more = 1;
+	int retval;
+
+	/* invalid parameters */
+	if (!head)
+		return -EINVAL;
+
+	last_p_partition_ID = last_p_unit_address = HVCS_LONG_INVALID;
+	INIT_LIST_HEAD(head);
+
+	pi_buff = kmalloc(PAGE_SIZE, GFP_KERNEL);
+
+	if(!pi_buff)
+		return -ENOMEM;
+
+	do {
+		retval = hvcs_next_partner(unit_address, last_p_partition_ID,
+				last_p_unit_address, pi_buff);
+		if(retval) {
+			kfree(pi_buff);
+			pi_buff = 0;
+			/* don't indicate that we've failed if we have
+			 * any list elements. */
+			if(!list_empty(head))
+				return 0;
+			return retval;
+		}
+
+		last_p_partition_ID = pi_buff[0];
+		last_p_unit_address = pi_buff[1];
+
+		/* This indicates that there are no further partners */
+		if (last_p_partition_ID == HVCS_LONG_INVALID
+				&& last_p_unit_address == HVCS_LONG_INVALID)
+			break;
+
+		next_partner_info = kmalloc(sizeof(struct hvcs_partner_info),
+				GFP_KERNEL);
+
+		if (!next_partner_info) {
+			printk(KERN_WARNING "HVCONSOLE: kmalloc() failed to"
+				" allocate partner info struct.\n");
+			hvcs_free_partner_info(head);
+			kfree(pi_buff);
+			pi_buff = 0;
+			return -ENOMEM;
+		}
+
+		next_partner_info->unit_address
+			= (unsigned int)last_p_unit_address;
+		next_partner_info->partition_ID
+			= (unsigned int)last_p_partition_ID;
+
+		/* copy the Null-term char too */
+		strncpy(&next_partner_info->location_code[0],
+			(char *)&pi_buff[2],
+			strlen((char *)&pi_buff[2]) + 1);
+
+		list_add_tail(&(next_partner_info->node), head);
+		next_partner_info = NULL;
+
+	} while (more);
+
+	kfree(pi_buff);
+	pi_buff = 0;
+
+	return 0;
+}
+EXPORT_SYMBOL(hvcs_get_partner_info);
+
+/* If this function is called once and -EINVAL is returned it may
+ * indicate that the partner info needs to be refreshed for the
+ * target unit address at which point the caller must invoke
+ * hvcs_get_partner_info() and then call this function again.  If,
+ * for a second time, -EINVAL is returned then it indicates that
+ * there is probably already a partner connection registered to a
+ * different vty-server@ vdevice.  It is also possible that a second
+ * -EINVAL may indicate that one of the parms is not valid, for
+ * instance if the link was removed between the vty-server@ vdevice
+ * and the vty@ vdevice that you are trying to open.  Don't shoot the
+ * messenger.  Firmware implemented it this way.
+ */
+int hvcs_register_connection( unsigned int unit_address, unsigned int p_partition_ID, unsigned int p_unit_address)
+{
+	long retval;
+	retval = plpar_hcall_norets(H_REGISTER_VTERM, unit_address,
+				p_partition_ID, p_unit_address);
+	return hvcs_convert(retval);
+}
+EXPORT_SYMBOL(hvcs_register_connection);
+
+/* If -EBUSY is returned continue to call this function
+ * until 0 is returned */
+int hvcs_free_connection(unsigned int unit_address)
+{
+	long retval;
+	retval = plpar_hcall_norets(H_FREE_VTERM, unit_address);
+	return hvcs_convert(retval);
 }
+EXPORT_SYMBOL(hvcs_free_connection);
diff -purN linux-2.5/arch/ppc64/kernel/i8259.c linuxppc64-2.5/arch/ppc64/kernel/i8259.c
--- linux-2.5/arch/ppc64/kernel/i8259.c	2003-02-11 11:51:44.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/i8259.c	2003-09-12 19:50:39.000000000 +0000
@@ -124,8 +124,8 @@ static void i8259_unmask_irq(unsigned in
 
 static void i8259_end_irq(unsigned int irq)
 {
-	if (!(irq_desc[irq].status & (IRQ_DISABLED|IRQ_INPROGRESS)) &&
-	    irq_desc[irq].action)
+	if (!(get_irq_desc(irq)->status & (IRQ_DISABLED|IRQ_INPROGRESS)) &&
+	    get_irq_desc(irq)->action)
 		i8259_unmask_irq(irq);
 }
 
diff -purN linux-2.5/arch/ppc64/kernel/iSeries_iommu.c linuxppc64-2.5/arch/ppc64/kernel/iSeries_iommu.c
--- linux-2.5/arch/ppc64/kernel/iSeries_iommu.c	2004-02-27 18:28:03.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/iSeries_iommu.c	2004-03-02 03:00:22.000000000 +0000
@@ -43,6 +43,7 @@
 #include <asm/iommu.h>
 #include <asm/pci-bridge.h>
 #include <asm/iSeries/iSeries_pci.h>
+#include <asm/iSeries/vio.h>
 
 #include <asm/machdep.h>
 
@@ -58,11 +59,6 @@ static struct iSeries_Device_Node vio_de
 static struct pci_dev _veth_dev = { .sysdata = &veth_dev_node };
 static struct pci_dev _vio_dev  = { .sysdata = &vio_dev_node, .dev.bus = &pci_bus_type  };
 
-/*
- * I wonder what the deal is with these.  Nobody uses them.  Why do they
- * exist? Why do we export them to modules? Why is this comment here, and
- * why didn't I just delete them?
- */
 struct pci_dev *iSeries_veth_dev = &_veth_dev;
 struct device *iSeries_vio_dev = &_vio_dev.dev;
 
diff -purN linux-2.5/arch/ppc64/kernel/iSeries_irq.c linuxppc64-2.5/arch/ppc64/kernel/iSeries_irq.c
--- linux-2.5/arch/ppc64/kernel/iSeries_irq.c	2004-02-25 02:54:12.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/iSeries_irq.c	2004-02-26 06:29:56.000000000 +0000
@@ -122,8 +122,8 @@ void __init iSeries_activate_IRQs()
 	int irq;
 	unsigned long flags;
 
-	for (irq = 0; irq < NR_IRQS; irq++) {
-		irq_desc_t *desc = &irq_desc[irq];
+	for_each_irq (irq) {
+		irq_desc_t *desc = get_irq_desc(irq);
 
 		if (desc && desc->handler && desc->handler->startup) {
 			spin_lock_irqsave(&desc->lock, flags);
diff -purN linux-2.5/arch/ppc64/kernel/iSeries_pci.c linuxppc64-2.5/arch/ppc64/kernel/iSeries_pci.c
--- linux-2.5/arch/ppc64/kernel/iSeries_pci.c	2004-02-27 23:02:35.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/iSeries_pci.c	2004-03-02 02:44:16.000000000 +0000
@@ -45,6 +45,7 @@
 #include <asm/iSeries/iSeries_irq.h>
 #include <asm/iSeries/iSeries_pci.h>
 #include <asm/iSeries/mf.h>
+#include <asm/iSeries/vio.h>
 
 #include "iSeries_IoMmTable.h"
 #include "pci.h"
@@ -282,6 +283,11 @@ void __init iSeries_pci_final_fixup(void
 	iSeries_IoMmTable_Status();
 	iSeries_activate_IRQs();
 	mf_displaySrc(0xC9000200);
+
+	/* Now set up virtual bus device information */
+	if (device_register(iSeries_vio_dev)) {
+	    printk("pcibios error registering iSeries_vio_dev\n");
+	}
 }
 
 void pcibios_fixup_bus(struct pci_bus *PciBus)
diff -purN linux-2.5/arch/ppc64/kernel/iSeries_setup.c linuxppc64-2.5/arch/ppc64/kernel/iSeries_setup.c
--- linux-2.5/arch/ppc64/kernel/iSeries_setup.c	2004-02-19 03:42:25.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/iSeries_setup.c	2004-02-26 10:56:03.000000000 +0000
@@ -299,7 +299,6 @@ void __init iSeries_init_early(void)
 		initrd_start = (unsigned long)__va(naca->xRamDisk);
 		initrd_end = initrd_start + naca->xRamDiskSize * PAGE_SIZE;
 		initrd_below_start_ok = 1;	// ramdisk in kernel space
-		ROOT_DEV = Root_RAM0;
 		if (((rd_size * 1024) / PAGE_SIZE) < naca->xRamDiskSize)
 			rd_size = (naca->xRamDiskSize * PAGE_SIZE) / 1024;
 	} else
diff -purN linux-2.5/arch/ppc64/kernel/iSeries_setup.h linuxppc64-2.5/arch/ppc64/kernel/iSeries_setup.h
--- linux-2.5/arch/ppc64/kernel/iSeries_setup.h	2004-01-21 01:50:56.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/iSeries_setup.h	2004-02-26 10:56:03.000000000 +0000
@@ -29,7 +29,6 @@ extern void iSeries_setup_arch(void);
 extern void iSeries_setup_residual(struct seq_file *m, int cpu_id);
 extern void iSeries_get_cpuinfo(struct seq_file *m);
 extern void iSeries_init_IRQ(void);
-extern void iSeries_init_irq_desc(irq_desc_t *);
 extern int iSeries_get_irq(struct pt_regs *regs);
 extern void iSeries_restart(char *cmd);
 extern void iSeries_power_off(void);
diff -purN linux-2.5/arch/ppc64/kernel/irq.c linuxppc64-2.5/arch/ppc64/kernel/irq.c
--- linux-2.5/arch/ppc64/kernel/irq.c	2004-02-25 02:54:12.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/irq.c	2004-02-27 04:21:44.000000000 +0000
@@ -66,7 +66,8 @@ irq_desc_t irq_desc[NR_IRQS] __cacheline
 		.lock = SPIN_LOCK_UNLOCKED
 	}
 };
-	
+
+int __irq_offset_value;
 int ppc_spurious_interrupts = 0;
 unsigned long lpEvent_count = 0;
 
@@ -76,7 +77,7 @@ setup_irq(unsigned int irq, struct irqac
 	int shared = 0;
 	unsigned long flags;
 	struct irqaction *old, **p;
-	irq_desc_t *desc = irq_desc + irq;
+	irq_desc_t *desc = get_irq_desc(irq);
 
 	/*
 	 * Some drivers like serial.c use request_irq() heavily,
@@ -134,7 +135,7 @@ setup_irq(unsigned int irq, struct irqac
 
 inline void synchronize_irq(unsigned int irq)
 {
-	while (irq_desc[irq].status & IRQ_INPROGRESS)
+	while (get_irq_desc(irq)->status & IRQ_INPROGRESS)
 		cpu_relax();
 }
 
@@ -148,11 +149,10 @@ EXPORT_SYMBOL(synchronize_irq);
 static int
 do_free_irq(int irq, void* dev_id)
 {
-	irq_desc_t *desc;
+	irq_desc_t *desc = get_irq_desc(irq);
 	struct irqaction **p;
 	unsigned long flags;
 
-	desc = irq_desc + irq;
 	spin_lock_irqsave(&desc->lock,flags);
 	p = &desc->action;
 	for (;;) {
@@ -183,6 +183,7 @@ do_free_irq(int irq, void* dev_id)
 	return -ENOENT;
 }
 
+
 int request_irq(unsigned int irq,
 	irqreturn_t (*handler)(int, void *, struct pt_regs *),
 	unsigned long irqflags, const char * devname, void *dev_id)
@@ -195,25 +196,25 @@ int request_irq(unsigned int irq,
 	if (!handler)
 		/* We could implement really free_irq() instead of that... */
 		return do_free_irq(irq, dev_id);
-	
+
 	action = (struct irqaction *)
 		kmalloc(sizeof(struct irqaction), GFP_KERNEL);
 	if (!action) {
 		printk(KERN_ERR "kmalloc() failed for irq %d !\n", irq);
 		return -ENOMEM;
 	}
-	
+
 	action->handler = handler;
-	action->flags = irqflags;					
+	action->flags = irqflags;
 	action->mask = 0;
 	action->name = devname;
 	action->dev_id = dev_id;
 	action->next = NULL;
-	
+
 	retval = setup_irq(irq, action);
 	if (retval)
 		kfree(action);
-		
+
 	return 0;
 }
 
@@ -246,7 +247,7 @@ EXPORT_SYMBOL(free_irq);
  
 inline void disable_irq_nosync(unsigned int irq)
 {
-	irq_desc_t *desc = irq_desc + irq;
+	irq_desc_t *desc = get_irq_desc(irq);
 	unsigned long flags;
 
 	spin_lock_irqsave(&desc->lock, flags);
@@ -275,7 +276,7 @@ EXPORT_SYMBOL(disable_irq_nosync);
  
 void disable_irq(unsigned int irq)
 {
-	irq_desc_t *desc = irq_desc + irq;
+	irq_desc_t *desc = get_irq_desc(irq);
 	disable_irq_nosync(irq);
 	if (desc->action)
 		synchronize_irq(irq);
@@ -295,7 +296,7 @@ EXPORT_SYMBOL(disable_irq);
  
 void enable_irq(unsigned int irq)
 {
-	irq_desc_t *desc = irq_desc + irq;
+	irq_desc_t *desc = get_irq_desc(irq);
 	unsigned long flags;
 
 	spin_lock_irqsave(&desc->lock, flags);
@@ -326,6 +327,7 @@ int show_interrupts(struct seq_file *p, 
 {
 	int i = *(loff_t *) v, j;
 	struct irqaction * action;
+	irq_desc_t *desc;
 	unsigned long flags;
 
 	if (i == 0) {
@@ -338,30 +340,31 @@ int show_interrupts(struct seq_file *p, 
 	}
 
 	if (i < NR_IRQS) {
-		spin_lock_irqsave(&irq_desc[i].lock, flags);
-		action = irq_desc[i].action;
+		desc = get_irq_desc(i);
+		spin_lock_irqsave(&desc->lock, flags);
+		action = desc->action;
 		if (!action || !action->handler)
 			goto skip;
-		seq_printf(p, "%3d: ", i);		
+		seq_printf(p, "%3d: ", i);
 #ifdef CONFIG_SMP
 		for (j = 0; j < NR_CPUS; j++) {
 			if (cpu_online(j))
 				seq_printf(p, "%10u ", kstat_cpu(j).irqs[i]);
 		}
-#else		
+#else
 		seq_printf(p, "%10u ", kstat_irqs(i));
 #endif /* CONFIG_SMP */
-		if (irq_desc[i].handler)		
-			seq_printf(p, " %s ", irq_desc[i].handler->typename );
+		if (desc->handler)
+			seq_printf(p, " %s ", desc->handler->typename );
 		else
 			seq_printf(p, "  None      ");
-		seq_printf(p, "%s", (irq_desc[i].status & IRQ_LEVEL) ? "Level " : "Edge  ");
+		seq_printf(p, "%s", (desc->status & IRQ_LEVEL) ? "Level " : "Edge  ");
 		seq_printf(p, "    %s",action->name);
 		for (action=action->next; action; action = action->next)
 			seq_printf(p, ", %s", action->name);
 		seq_putc(p, '\n');
 skip:
-		spin_unlock_irqrestore(&irq_desc[i].lock, flags);
+		spin_unlock_irqrestore(&desc->lock, flags);
 	} else if (i == NR_IRQS)
 		seq_printf(p, "BAD: %10u\n", ppc_spurious_interrupts);
 	return 0;
@@ -477,7 +480,7 @@ void ppc_irq_dispatch_handler(struct pt_
 	int status;
 	struct irqaction *action;
 	int cpu = smp_processor_id();
-	irq_desc_t *desc = irq_desc + irq;
+	irq_desc_t *desc = get_irq_desc(irq);
 	irqreturn_t action_ret;
 
 	kstat_cpu(cpu).irqs[irq]++;
@@ -559,11 +562,11 @@ out:
 	 * The ->end() handler has to deal with interrupts which got
 	 * disabled while the handler was running.
 	 */
-	if (irq_desc[irq].handler) {
-		if (irq_desc[irq].handler->end)
-			irq_desc[irq].handler->end(irq);
-		else if (irq_desc[irq].handler->enable)
-			irq_desc[irq].handler->enable(irq);
+	if (desc->handler) {
+		if (desc->handler->end)
+			desc->handler->end(irq);
+		else if (desc->handler->enable)
+			desc->handler->enable(irq);
 	}
 	spin_unlock(&desc->lock);
 }
@@ -670,7 +673,7 @@ void __init init_IRQ(void)
 		return;
 
 	once++;
-	
+
 	ppc_md.init_IRQ();
 }
 
@@ -697,10 +700,13 @@ static int irq_affinity_read_proc (char 
 static int irq_affinity_write_proc (struct file *file, const char *buffer,
 					unsigned long count, void *data)
 {
-	int irq = (long)data, full_count = count, err;
+	unsigned int irq = (long)data;
+	irq_desc_t *desc = get_irq_desc(irq);
+	int full_count = count, err;
 	cpumask_t new_value, tmp;
+	cpumask_t allcpus = CPU_MASK_ALL;
 
-	if (!irq_desc[irq].handler->set_affinity)
+	if (!desc->handler->set_affinity)
 		return -EIO;
 
 	err = cpumask_parse(buffer, count, new_value);
@@ -708,6 +714,14 @@ static int irq_affinity_write_proc (stru
 		return err;
 
 	/*
+	 * We check for CPU_MASK_ALL in xics to send irqs to all cpus.
+	 * In some cases CPU_MASK_ALL is smaller than the cpumask (eg
+	 * NR_CPUS == 32 and cpumask is a long), so we mask it here to
+	 * be consistent.
+	 */
+	cpus_and(new_value, new_value, allcpus);
+
+	/*
 	 * Do not allow disabling IRQs completely - it's a too easy
 	 * way to make the system unusable accidentally :-) At least
 	 * one online CPU still has to be targeted.
@@ -717,8 +731,7 @@ static int irq_affinity_write_proc (stru
 		return -EINVAL;
 
 	irq_affinity[irq] = new_value;
-	irq_desc[irq].handler->set_affinity(irq, new_value);
-
+	desc->handler->set_affinity(irq, new_value);
 	return full_count;
 }
 
@@ -814,8 +827,8 @@ void init_irq_proc (void)
 	/*
 	 * Create entries for all existing IRQs.
 	 */
-	for (i = 0; i < NR_IRQS; i++) {
-		if (irq_desc[i].handler == NULL)
+	for_each_irq(i) {
+		if (get_irq_desc(i)->handler == NULL)
 			continue;
 		register_irq_proc(i);
 	}
@@ -843,7 +856,7 @@ unsigned int virt_irq_to_real_map[NR_IRQ
  * we don't end up with an interrupt number >= NR_IRQS.
  */
 #define MIN_VIRT_IRQ	3
-#define MAX_VIRT_IRQ	(NR_IRQS - NUM_8259_INTERRUPTS - 1)
+#define MAX_VIRT_IRQ	(NR_IRQS - NUM_ISA_INTERRUPTS - 1)
 #define NR_VIRT_IRQS	(MAX_VIRT_IRQ - MIN_VIRT_IRQ + 1)
 
 void
diff -purN linux-2.5/arch/ppc64/kernel/lparcfg.c linuxppc64-2.5/arch/ppc64/kernel/lparcfg.c
--- linux-2.5/arch/ppc64/kernel/lparcfg.c	2004-02-27 05:33:08.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/lparcfg.c	2004-02-27 22:08:01.000000000 +0000
@@ -29,9 +29,29 @@
 #include <asm/hvcall.h>
 #include <asm/cputable.h>
 
-#define MODULE_VERS "1.0"
+#define MODULE_VERS "1.1"
 #define MODULE_NAME "lparcfg"
 
+/* #define LPARCFG_DEBUG */
+
+/* find a better place for this function... */
+void log_plpar_hcall_return(unsigned long rc,char * tag)
+{
+	if (rc==0) /* success, return */
+		return;
+/* check for null tag ? */
+	if (rc == H_Hardware)
+		printk("plpar-hcall (%s) failed with hardware fault\n",tag);
+	else if (rc == H_Function)
+		printk("plpar-hcall (%s) failed; function not allowed\n",tag);
+	else if (rc == H_Authority)
+		printk("plpar-hcall (%s) failed; not authorized to this function\n",tag);
+	else
+		printk("plpar-hcall (%s) failed with unexpected rc(0x%lx)\n",tag,rc);
+
+}
+
+
 static struct proc_dir_entry *proc_ppc64_lparcfg;
 #define LPARCFG_BUFF_SIZE 4096
 
@@ -217,7 +237,7 @@ static int lparcfg_data(unsigned char *b
  *          XXXX - reserved (0)
  *              XXXX - Group Number
  *                  XXXX - Pool Number.
- *  R7 (PPOONNMMLLKKJJII)
+ *  R7 (IIJJKKLLMMNNOOPP).
  *      XX - reserved. (0)
  *        XX - bit 0-6 reserved (0).   bit 7 is Capped indicator.
  *          XX - variable processor Capacity Weight
@@ -229,72 +249,132 @@ unsigned int h_get_ppp(unsigned long *en
 {
 	unsigned long rc;
 	rc = plpar_hcall_4out(H_GET_PPP,0,0,0,0,entitled,unallocated,aggregation,resource);
+
+	log_plpar_hcall_return(rc,"H_GET_PPP");
+
+	return 0;
+}
+
+unsigned int h_pic(unsigned long *pool_idle_time,unsigned long *num_procs)
+{
+	unsigned long rc;
+	unsigned long dummy;
+	rc = plpar_hcall(H_PIC,0,0,0,0,pool_idle_time,num_procs,&dummy);
+
+	log_plpar_hcall_return(rc,"H_PIC");
+
+	return 0;
+}
+
+/* prototyping h_purr functionality.  this may need to be moved into decrementer code. */
+unsigned int h_purr(unsigned long *purr)
+{
+	unsigned long rc;
+
+	unsigned long dummy;
+	rc = plpar_hcall(H_PURR, 0, 0, 0, 0,purr, &dummy, &dummy);
+
+	log_plpar_hcall_return(rc,"H_PURR");
 	return 0;
 }
 
-/*
- * get_splpar_potential_characteristics().
- * Retrieve the potential_processors and max_entitled_capacity values
- * through the get-system-parameter rtas call.
- */
 #define SPLPAR_CHARACTERISTICS_TOKEN 20
 #define SPLPAR_MAXLENGTH 1026*(sizeof(char))
-unsigned int get_splpar_potential_characteristics(void)
+
+/*
+ * parse_system_parameter_string()
+ * Retrieve the potential_processors, max_entitled_capacity and friends 
+ * through the get-system-parameter rtas call.  Replace keyword strings as
+ * necessary, and add the contents to 'buf'.
+ */
+unsigned long parse_system_parameter_string(unsigned long n, char * buf)
 {
-	/* return 0 for now.  Underlying rtas functionality is not yet complete. 12/01/2003*/
-	return 0; 
-#if 0 
 	long call_status;
 	unsigned long ret[2];
 
-	char * buffer = kmalloc(SPLPAR_MAXLENGTH, GFP_KERNEL);
-
-	printk("token for ibm,get-system-parameter (0x%x)\n",rtas_token("ibm,get-system-parameter"));
+	char * local_buffer = kmalloc(SPLPAR_MAXLENGTH, GFP_KERNEL);
 
-	call_status = rtas_call(rtas_token("ibm,get-system-parameter"), 3, 1,
-				NULL,
-				SPLPAR_CHARACTERISTICS_TOKEN,
-				&buffer,
-				SPLPAR_MAXLENGTH,
-				(void *)&ret);
+	{
+		spin_lock(&rtas_data_buf_lock);
+		memset(rtas_data_buf, 0, SPLPAR_MAXLENGTH); 
+		call_status = rtas_call(rtas_token("ibm,get-system-parameter"), 3, 1,
+					NULL,
+					SPLPAR_CHARACTERISTICS_TOKEN,
+					__pa(rtas_data_buf),
+					SPLPAR_MAXLENGTH,
+					(void *)&ret);
+		memcpy(local_buffer,rtas_data_buf, SPLPAR_MAXLENGTH);
+		spin_unlock(&rtas_data_buf_lock);
+	}
 
 	if (call_status!=0) {
-		printk("Error calling get-system-parameter (0x%lx)\n",call_status);
-		kfree(buffer);
-		return -1;
+		printk("%s %s Error calling get-system-parameter (0x%lx)\n",__FILE__,__FUNCTION__,call_status);
 	} else {
-		printk("get-system-parameter (%s)\n",buffer);
-		kfree(buffer);
-		/* TODO: Add code here to parse out value for system_potential_processors and partition_max_entitled_capacity */
-		return 1;
-	}
+		int splpar_strlen;
+#ifdef LPARCFG_DEBUG
+		printk("success calling get-system-parameter \n");
 #endif
+		splpar_strlen=local_buffer[0]*16+local_buffer[1];
+		local_buffer+=2; /* step over strlen value */
+
+		char * workbuffer = kmalloc(SPLPAR_MAXLENGTH, GFP_KERNEL);
+		int idx,w_idx;
+		memset(workbuffer, 0, SPLPAR_MAXLENGTH);
+		w_idx=0; idx=0;
+		while ((*local_buffer) && (idx<splpar_strlen)) {
+			workbuffer[w_idx++]=local_buffer[idx++];
+			if ((local_buffer[idx]==',')||(local_buffer[idx]=='\0')) {
+				workbuffer[w_idx]='\0';
+				if (w_idx) /* avoid the empty string */
+				{
+					n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+						      "%s\n",workbuffer);
+				}
+				memset(workbuffer, 0, SPLPAR_MAXLENGTH);
+				idx++; /* skip the comma */
+				w_idx=0;
+			} else if (local_buffer[idx]=='=') {
+				/* code here to replace workbuffer contents
+				 with different keyword strings */
+				if (0==strcmp(workbuffer,"MaxEntCap")) {
+					strcpy(workbuffer,"partition_max_entitled_capacity\0");
+					w_idx=strlen(workbuffer);
+				}
+				if (0==strcmp(workbuffer,"MaxPlatProcs")) {
+					strcpy(workbuffer,"system_potential_processors\0");
+					w_idx=strlen(workbuffer);
+				}
+			}
+		}
+		kfree(workbuffer);
+		local_buffer-=2; /* back up over strlen value */
+	}
+	kfree(local_buffer);
+	return n;
 }
 
 static int lparcfg_data(unsigned char *buf, unsigned long size)
 {
-	unsigned long n = 0;
-	int shared, max_entitled_capacity;
-	int processors, system_active_processors, system_potential_processors;
-	struct device_node *root;
+	unsigned long n = 0;		/* scnprintf index */
+	int system_active_processors;
+	struct device_node *rootdn;
 	const char *model = "";
 	const char *system_id = "";
 	unsigned int *lp_index_ptr, lp_index = 0;
 	struct device_node *rtas_node;
-	int *ip;
-	unsigned long h_entitled,h_unallocated,h_aggregation,h_resource;
+	int *lrdrp;
 
 	if((buf == NULL) || (size > LPARCFG_BUFF_SIZE)) {
 		return -EFAULT;
 	}
 	memset(buf, 0, size); 
 
-	root = find_path_device("/");
-	if (root) {
-		model = get_property(root, "model", NULL);
-		system_id = get_property(root, "system-id", NULL);
-		lp_index_ptr = (unsigned int *)get_property(root, "ibm,partition-no", NULL);
-		if(lp_index_ptr) lp_index = *lp_index_ptr;
+	rootdn = find_path_device("/");
+	if (rootdn) {
+		model = get_property(rootdn, "model", NULL);
+		system_id = get_property(rootdn, "system-id", NULL);
+		lp_index_ptr = (unsigned int *)get_property(rootdn, "ibm,partition-no", NULL);
+		if (lp_index_ptr) lp_index = *lp_index_ptr;
 	}
 
 	n  = scnprintf(buf, LPARCFG_BUFF_SIZE - n,
@@ -307,16 +387,20 @@ static int lparcfg_data(unsigned char *b
 		      "partition_id=%d\n", (int)lp_index); 
 
 	rtas_node = find_path_device("/rtas");
-	ip = (int *)get_property(rtas_node, "ibm,lrdr-capacity", NULL);
-	if (ip == NULL) {
+	lrdrp = (int *)get_property(rtas_node, "ibm,lrdr-capacity", NULL);
+
+	if (lrdrp == NULL) {
 		system_active_processors = systemcfg->processorCount; 
 	} else {
-		system_active_processors = *(ip + 4);
-	}
+		system_active_processors = *(lrdrp + 4);
+	} 
 
 	if (cur_cpu_spec->firmware_features & FW_FEATURE_SPLPAR) {
+		unsigned long h_entitled,h_unallocated,h_aggregation,h_resource;
+
 		h_get_ppp(&h_entitled,&h_unallocated,&h_aggregation,&h_resource);
-#ifdef DEBUG
+
+#ifdef LPARCFG_DEBUG
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
 			      "R4=0x%lx\n", h_entitled);
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
@@ -325,68 +409,93 @@ static int lparcfg_data(unsigned char *b
 			      "R6=0x%lx\n", h_aggregation);
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
 			      "R7=0x%lx\n", h_resource);
-#endif /* DEBUG */
-	}
+#endif /* LPARCFG_DEBUG */
+
+		/* this call handles the ibm,get-system-parameter contents */
+		n = parse_system_parameter_string(n,buf);
+
+		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
+			      "partition_entitled_capacity=%ld\n",
+			      h_entitled);
+
+		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
+			      "pool=%ld\n",
+			      (h_aggregation >> 0*8) & 0xffff);
+
+		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
+			      "group=%ld\n",
+			      (h_aggregation >> 2*8) & 0xffff);
 
-	if (cur_cpu_spec->firmware_features & FW_FEATURE_SPLPAR) {
-		system_potential_processors =  get_splpar_potential_characteristics();
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
 			      "system_active_processors=%ld\n", 
 			      (h_resource >> 2*8) & 0xffff);
+
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-			      "system_potential_processors=%d\n", 
-			      system_potential_processors);
-	} else {
-		system_potential_processors = system_active_processors;
-		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-			      "system_active_processors=%d\n", 
-			      system_active_processors);
+			      "pool_capacity=%ld\n",
+			      (h_resource >> 3*8) & 0xffff);
+
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-			      "system_potential_processors=%d\n", 
-			      system_potential_processors);
-	}
+			      "capacity_weight=%ld\n",
+			      (h_resource>>5*8) & 0xFF);
 
-	processors = systemcfg->processorCount;
-	n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-		      "partition_active_processors=%d\n", processors);  
-	n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-		      "partition_potential_processors=%d\n",
-		      system_active_processors);
+		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
+			      "capped=%ld\n",
+			      (h_resource >> 6*8) & 0x40);
 
-	/* max_entitled_capacity will come out of get_splpar_potential_characteristics() when that function is complete */
-	max_entitled_capacity = system_active_processors * 100; 
-	if (cur_cpu_spec->firmware_features & FW_FEATURE_SPLPAR) {
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-			      "partition_entitled_capacity=%ld\n", h_entitled);
-	} else {
+			      "unallocated_variable_weight=%ld\n",
+			      (h_resource>>7*8) & 0xFF);
+
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-			      "partition_entitled_capacity=%d\n", system_active_processors*100);
-	}
+			      "unallocated_capacity=%ld\n",
+			      h_unallocated);
 
-	n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-		      "partition_max_entitled_capacity=%d\n", 
-		      max_entitled_capacity);
+		unsigned long pool_idle_time,pool_procs;
+		h_pic(&pool_idle_time,&pool_procs);
+		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "pool_idle_time=%ld\n",
+			      pool_idle_time);
 
-	shared = 0;
-	n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-		      "shared_processor_mode=%d\n", shared);
+		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "pool_num_procs=%ld\n",
+			      pool_procs);
 
-	if (cur_cpu_spec->firmware_features & FW_FEATURE_SPLPAR) {
-		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-			      "pool=%ld\n", (h_aggregation >> 0*8)&0xffff);
+		unsigned long purr;
+		h_purr(&purr);
+		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "purr=%ld\n",
+			      purr);
 
-		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-			      "pool_capacity=%ld\n", (h_resource >> 3*8) &0xffff);
+	} else /* non SPLPAR case */ {
+		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "system_active_processors=%d\n",
+			      system_active_processors);
 
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-			      "group=%ld\n", (h_aggregation >> 2*8)&0xffff);
+			      "system_potential_processors=%d\n",
+			      system_active_processors);
 
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-			      "capped=%ld\n", (h_resource >> 6*8)&0x40);
+			      "partition_max_entitled_capacity=%d\n",
+			      100*system_active_processors);
 
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-			      "capacity_weight=%d\n", (int)(h_resource>>5*8)&0xFF);
+			      "partition_entitled_capacity=%d\n",
+			      system_active_processors*100);
 	}
+
+	n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
+		      "partition_active_processors=%d\n",
+		      (int) systemcfg->processorCount);
+
+	n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
+		      "partition_potential_processors=%d\n",
+		      system_active_processors);
+
+	n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
+		      "shared_processor_mode=%d\n",
+		      paca[0].xLpPaca.xSharedProc);
+
 	return 0;
 }
 
diff -purN linux-2.5/arch/ppc64/kernel/mf.c linuxppc64-2.5/arch/ppc64/kernel/mf.c
--- linux-2.5/arch/ppc64/kernel/mf.c	2004-01-19 06:28:27.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/mf.c	2004-03-02 02:44:16.000000000 +0000
@@ -38,10 +38,9 @@
 #include <asm/iSeries/ItSpCommArea.h>
 #include <asm/iSeries/iSeries_proc.h>
 #include <asm/uaccess.h>
-#include <linux/pci.h>
+#include <linux/dma-mapping.h>
 #include <linux/bcd.h>
-
-extern struct pci_dev *iSeries_vio_dev;
+#include <asm/iSeries/vio.h>
 
 /*
  * This is the structure layout for the Machine Facilites LPAR event
@@ -791,7 +790,8 @@ void mf_setCmdLine(const char *cmdline, 
 {
 	struct VspCmdData myVspCmd;
 	dma_addr_t dma_addr = 0;
-	char *page = pci_alloc_consistent(iSeries_vio_dev, size, &dma_addr);
+	char *page = dma_alloc_coherent(iSeries_vio_dev, size, &dma_addr,
+			GFP_ATOMIC);
 
 	if (page == NULL) {
 		printk(KERN_ERR "mf.c: couldn't allocate memory to set command line\n");
@@ -809,7 +809,7 @@ void mf_setCmdLine(const char *cmdline, 
 	mb();
 	(void)signal_vsp_instruction(&myVspCmd);
 
-	pci_free_consistent(iSeries_vio_dev, size, page, dma_addr);
+	dma_free_coherent(iSeries_vio_dev, size, page, dma_addr);
 }
 
 int mf_getCmdLine(char *cmdline, int *size, u64 side)
@@ -819,8 +819,8 @@ int mf_getCmdLine(char *cmdline, int *si
 	int len = *size;
 	dma_addr_t dma_addr;
 
-	dma_addr = pci_map_single(iSeries_vio_dev, cmdline, len,
-			PCI_DMA_FROMDEVICE);
+	dma_addr = dma_map_single(iSeries_vio_dev, cmdline, len,
+			DMA_FROM_DEVICE);
 	memset(cmdline, 0, len);
 	memset(&myVspCmd, 0, sizeof(myVspCmd));
 	myVspCmd.cmd = 33;
@@ -840,7 +840,7 @@ int mf_getCmdLine(char *cmdline, int *si
 #endif
 	}
 
-	pci_unmap_single(iSeries_vio_dev, dma_addr, *size, PCI_DMA_FROMDEVICE);
+	dma_unmap_single(iSeries_vio_dev, dma_addr, *size, DMA_FROM_DEVICE);
 
 	return len;
 }
@@ -851,7 +851,8 @@ int mf_setVmlinuxChunk(const char *buffe
 	struct VspCmdData myVspCmd;
 	int rc;
 	dma_addr_t dma_addr = 0;
-	char *page = pci_alloc_consistent(iSeries_vio_dev, size, &dma_addr);
+	char *page = dma_alloc_coherent(iSeries_vio_dev, size, &dma_addr,
+			GFP_ATOMIC);
 
 	if (page == NULL) {
 		printk(KERN_ERR "mf.c: couldn't allocate memory to set vmlinux chunk\n");
@@ -876,7 +877,7 @@ int mf_setVmlinuxChunk(const char *buffe
 			rc = -ENOMEM;
 	}
 
-	pci_free_consistent(iSeries_vio_dev, size, page, dma_addr);
+	dma_free_coherent(iSeries_vio_dev, size, page, dma_addr);
 
 	return rc;
 }
@@ -888,8 +889,8 @@ int mf_getVmlinuxChunk(char *buffer, int
 	int len = *size;
 	dma_addr_t dma_addr;
 
-	dma_addr = pci_map_single(iSeries_vio_dev, buffer, len,
-			PCI_DMA_FROMDEVICE);
+	dma_addr = dma_map_single(iSeries_vio_dev, buffer, len,
+			DMA_FROM_DEVICE);
 	memset(buffer, 0, len);
 	memset(&myVspCmd, 0, sizeof(myVspCmd));
 	myVspCmd.cmd = 32;
@@ -907,7 +908,7 @@ int mf_getVmlinuxChunk(char *buffer, int
 			rc = -ENOMEM;
 	}
 
-	pci_unmap_single(iSeries_vio_dev, dma_addr, len, PCI_DMA_FROMDEVICE);
+	dma_unmap_single(iSeries_vio_dev, dma_addr, len, DMA_FROM_DEVICE);
 
 	return rc;
 }
diff -purN linux-2.5/arch/ppc64/kernel/open_pic.c linuxppc64-2.5/arch/ppc64/kernel/open_pic.c
--- linux-2.5/arch/ppc64/kernel/open_pic.c	2004-02-23 16:39:14.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/open_pic.c	2004-02-26 10:56:03.000000000 +0000
@@ -67,7 +67,6 @@ static void openpic_disable_irq(u_int ir
 static void openpic_initirq(u_int irq, u_int pri, u_int vector, int polarity,
 			    int is_level);
 static void openpic_mapirq(u_int irq, u_int cpumask);
-static void openpic_set_sense(u_int irq, int sense);
 
 static void find_ISUs(void);
 
@@ -170,7 +169,7 @@ void __init pSeries_init_openpic(void)
         int i;
         unsigned int *addrp;
         unsigned char* chrp_int_ack_special = 0;
-        unsigned char init_senses[NR_IRQS - NUM_8259_INTERRUPTS];
+        unsigned char init_senses[NR_IRQS - NUM_ISA_INTERRUPTS];
         int nmi_irq = -1;
 #if defined(CONFIG_VT) && defined(CONFIG_ADB_KEYBOARD) && defined(XMON)
         struct device_node *kbd;
@@ -185,12 +184,12 @@ void __init pSeries_init_openpic(void)
 			__ioremap(addrp[prom_n_addr_cells(np)-1], 1, _PAGE_NO_CACHE);
         /* hydra still sets OpenPIC_InitSenses to a static set of values */
         if (OpenPIC_InitSenses == NULL) {
-                prom_get_irq_senses(init_senses, NUM_8259_INTERRUPTS, NR_IRQS);
+                prom_get_irq_senses(init_senses, NUM_ISA_INTERRUPTS, NR_IRQS);
                 OpenPIC_InitSenses = init_senses;
-                OpenPIC_NumInitSenses = NR_IRQS - NUM_8259_INTERRUPTS;
+                OpenPIC_NumInitSenses = NR_IRQS - NUM_ISA_INTERRUPTS;
         }
-        openpic_init(1, NUM_8259_INTERRUPTS, chrp_int_ack_special, nmi_irq);
-        for ( i = 0 ; i < NUM_8259_INTERRUPTS  ; i++ )
+        openpic_init(1, NUM_ISA_INTERRUPTS, chrp_int_ack_special, nmi_irq);
+        for (i = 0; i < NUM_ISA_INTERRUPTS; i++)
                 irq_desc[i].handler = &i8259_pic;
 	of_node_put(np);
 }
@@ -441,7 +440,7 @@ static int __init openpic_setup_i8259(vo
 
 	if (naca->interrupt_controller == IC_OPEN_PIC) {
 		/* Initialize the cascade */
-		if (request_irq(NUM_8259_INTERRUPTS, no_action, SA_INTERRUPT,
+		if (request_irq(NUM_ISA_INTERRUPTS, no_action, SA_INTERRUPT,
 				"82c59 cascade", NULL))
 			printk(KERN_ERR "Unable to get OpenPIC IRQ 0 for cascade\n");
 		i8259_init();
@@ -820,13 +819,21 @@ static void openpic_mapirq(u_int irq, u_
  *
  *  sense: 1 for level, 0 for edge
  */
-static inline void openpic_set_sense(u_int irq, int sense)
+#if 0	/* not used */
+static void openpic_set_sense(u_int irq, int sense)
 {
 	openpic_safe_writefield(&GET_ISU(irq).Vector_Priority,
 				OPENPIC_SENSE_LEVEL,
 				(sense ? OPENPIC_SENSE_LEVEL : 0));
 }
 
+static int openpic_get_sense(u_int irq)
+{
+	return openpic_readfield(&GET_ISU(irq).Vector_Priority,
+				 OPENPIC_SENSE_LEVEL) != 0;
+}
+#endif
+
 static void openpic_end_irq(unsigned int irq_nr)
 {
 	openpic_eoi();
diff -purN linux-2.5/arch/ppc64/kernel/open_pic.h linuxppc64-2.5/arch/ppc64/kernel/open_pic.h
--- linux-2.5/arch/ppc64/kernel/open_pic.h	2004-02-12 04:30:16.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/open_pic.h	2004-02-26 10:56:03.000000000 +0000
@@ -14,6 +14,7 @@
 
 #include <linux/config.h>
 #include <linux/cpumask.h>
+#include <linux/irq.h>
 
 #define OPENPIC_SIZE	0x40000
 
@@ -38,11 +39,4 @@ extern void openpic_init_processor(u_int
 extern void openpic_setup_ISU(int isu_num, unsigned long addr);
 extern void openpic_cause_IPI(u_int ipi, u_int cpumask);
 
-extern inline int openpic_to_irq(int irq)
-{
-	if (systemcfg->platform == PLATFORM_POWERMAC)
-		return irq;
-	return irq += NUM_8259_INTERRUPTS;
-}
-/*extern int open_pic_irq_offset;*/
 #endif /* _PPC64_KERNEL_OPEN_PIC_H */
diff -purN linux-2.5/arch/ppc64/kernel/open_pic_u3.c linuxppc64-2.5/arch/ppc64/kernel/open_pic_u3.c
--- linux-2.5/arch/ppc64/kernel/open_pic_u3.c	2004-02-12 03:47:52.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/open_pic_u3.c	2004-02-26 10:56:03.000000000 +0000
@@ -251,6 +251,14 @@ static inline void openpic2_set_sense(u_
 				 (sense ? OPENPIC_SENSE_LEVEL : 0));
 }
 
+#if 0	/* not used */
+static int openpic2_get_sense(u_int irq)
+{
+	return openpic2_readfield(&GET_ISU(irq).Vector_Priority,
+				  OPENPIC_SENSE_LEVEL) != 0;
+}
+#endif
+
 static void openpic2_end_irq(unsigned int irq_nr)
 {
 	openpic2_eoi();
diff -purN linux-2.5/arch/ppc64/kernel/pSeries_lpar.c linuxppc64-2.5/arch/ppc64/kernel/pSeries_lpar.c
--- linux-2.5/arch/ppc64/kernel/pSeries_lpar.c	2004-03-02 03:01:21.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/pSeries_lpar.c	2004-03-02 12:17:06.000000000 +0000
@@ -37,6 +37,12 @@
 #include <asm/hvcall.h>
 #include <asm/prom.h>
 
+/* in pSeries_hvCall.S */
+EXPORT_SYMBOL(plpar_hcall);
+EXPORT_SYMBOL(plpar_hcall_4out);
+EXPORT_SYMBOL(plpar_hcall_norets);
+EXPORT_SYMBOL(plpar_hcall_8arg_2ret);
+
 long poll_pending(void)
 {
 	unsigned long dummy;
diff -purN linux-2.5/arch/ppc64/kernel/pci.c linuxppc64-2.5/arch/ppc64/kernel/pci.c
--- linux-2.5/arch/ppc64/kernel/pci.c	2004-02-27 23:02:35.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/pci.c	2004-03-02 11:34:18.000000000 +0000
@@ -119,43 +119,6 @@ static void fixup_windbond_82c105(struct
 	}
 }
 
-/* Given an mmio phys address, find a pci device that implements
- * this address.  This is of course expensive, but only used
- * for device initialization or error paths.
- * For io BARs it is assumed the pci_io_base has already been added
- * into addr.
- *
- * Bridges are ignored although they could be used to optimize the search.
- */
-struct pci_dev *pci_find_dev_by_addr(unsigned long addr)
-{
-	struct pci_dev *dev = NULL;
-	int i;
-	unsigned long ioaddr;
-
-	ioaddr = (addr > isa_io_base) ? addr - isa_io_base : 0;
-
-	while ((dev = pci_find_device(PCI_ANY_ID, PCI_ANY_ID, dev)) != NULL) {
-		if ((dev->class >> 16) == PCI_BASE_CLASS_BRIDGE)
-			continue;
-		for (i = 0; i < DEVICE_COUNT_RESOURCE; i++) {
-			unsigned long start = pci_resource_start(dev,i);
-			unsigned long end = pci_resource_end(dev,i);
-			unsigned int flags = pci_resource_flags(dev,i);
-			if (start == 0 || ~start == 0 ||
-			    end == 0 || ~end == 0)
-				continue;
-			if ((flags & IORESOURCE_IO) &&
-			    (ioaddr >= start && ioaddr <= end))
-				return dev;
-			else if ((flags & IORESOURCE_MEM) &&
-				 (addr >= start && addr <= end))
-				return dev;
-		}
-	}
-	return NULL;
-}
-
 void 
 pcibios_resource_to_bus(struct pci_dev *dev, struct pci_bus_region *region,
 			struct resource *res)
@@ -359,6 +322,10 @@ static int __init pcibios_init(void)
 	printk("PCI: Probing PCI hardware done\n");
 	//ppc64_boot_msg(0x41, "PCI Done");
 
+#ifdef CONFIG_PPC_PSERIES
+	pci_addr_cache_build();
+#endif
+
 	return 0;
 }
 
diff -purN linux-2.5/arch/ppc64/kernel/pci.h linuxppc64-2.5/arch/ppc64/kernel/pci.h
--- linux-2.5/arch/ppc64/kernel/pci.h	2003-06-24 02:05:22.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/pci.h	2004-03-02 11:34:18.000000000 +0000
@@ -37,11 +37,14 @@ typedef void *(*traverse_func)(struct de
 void *traverse_pci_devices(struct device_node *start, traverse_func pre, traverse_func post, void *data);
 void *traverse_all_pci_devices(traverse_func pre);
 
-struct pci_dev *pci_find_dev_by_addr(unsigned long addr);
 void pci_devs_phb_init(void);
 void pci_fix_bus_sysdata(void);
 struct device_node *fetch_dev_dn(struct pci_dev *dev);
 
 #define PCI_GET_PHB_PTR(dev)    (((struct device_node *)(dev)->sysdata)->phb)
 
+/* PCI address cache management routines */
+void pci_addr_cache_insert_device(struct pci_dev *dev);
+void pci_addr_cache_remove_device(struct pci_dev *dev);
+
 #endif /* __PPC_KERNEL_PCI_H__ */
diff -purN linux-2.5/arch/ppc64/kernel/ppc_ksyms.c linuxppc64-2.5/arch/ppc64/kernel/ppc_ksyms.c
--- linux-2.5/arch/ppc64/kernel/ppc_ksyms.c	2004-02-27 18:28:03.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/ppc_ksyms.c	2004-03-02 11:34:18.000000000 +0000
@@ -148,10 +148,6 @@ EXPORT_SYMBOL(iSeries_Write_Byte);
 EXPORT_SYMBOL(iSeries_Write_Word);
 EXPORT_SYMBOL(iSeries_Write_Long);
 #endif /* CONFIG_PPC_ISERIES */
-#ifndef CONFIG_PPC_ISERIES
-EXPORT_SYMBOL(eeh_check_failure);
-EXPORT_SYMBOL(eeh_total_mmio_ffs);
-#endif /* CONFIG_PPC_ISERIES */
 #endif /* CONFIG_PCI */
 
 EXPORT_SYMBOL(start_thread);
@@ -204,3 +200,5 @@ EXPORT_SYMBOL(console_drivers);
 EXPORT_SYMBOL(tb_ticks_per_usec);
 EXPORT_SYMBOL(paca);
 EXPORT_SYMBOL(proc_ppc64);
+EXPORT_SYMBOL(cur_cpu_spec);
+EXPORT_SYMBOL(systemcfg);
diff -purN linux-2.5/arch/ppc64/kernel/proc_ppc64.c linuxppc64-2.5/arch/ppc64/kernel/proc_ppc64.c
--- linux-2.5/arch/ppc64/kernel/proc_ppc64.c	2004-02-05 21:10:53.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/proc_ppc64.c	2004-02-07 08:58:46.000000000 +0000
@@ -45,6 +45,7 @@
 struct proc_ppc64_t proc_ppc64;
 
 void proc_ppc64_create_paca(int num);
+void proc_ppc64_create_smt(void);
 
 static loff_t  page_map_seek( struct file *file, loff_t off, int whence);
 static ssize_t page_map_read( struct file *file, char *buf, size_t nbytes, loff_t *ppos);
@@ -119,6 +120,8 @@ int __init proc_ppc64_init(void)
 	if (proc_ppc64.rtas)
 		proc_symlink("rtas", 0, "ppc64/rtas");
 
+	proc_ppc64_create_smt();
+
 	proc_ppc64_create_ofdt(proc_ppc64.root);
 #endif
 
@@ -424,4 +427,86 @@ static void release_prop_list(const stru
 }
 #endif	/* defined(CONFIG_PPC_PSERIES) */
 
+static int proc_ppc64_smt_snooze_read(char *page, char **start, off_t off,
+				      int count, int *eof, void *data)
+{
+	if (naca->smt_snooze_delay)
+		return sprintf(page, "%lu\n", naca->smt_snooze_delay);
+	else 
+		return sprintf(page, "disabled\n");
+}
+ 
+static int proc_ppc64_smt_snooze_write(struct file* file, const char *buffer,
+				       unsigned long count, void *data)
+{
+	unsigned long val;
+	char val_string[22];
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EACCES;
+
+	if (count > sizeof(val_string) - 1)
+		return -EINVAL;
+
+	if (copy_from_user(val_string, buffer, count))
+		return -EFAULT;
+
+	val_string[count] = '\0';
+
+	if (val_string[0] == '0' && (val_string[1] == '\n' || val_string[1] == '\0')) {
+		naca->smt_snooze_delay = 0;
+		return count;
+	}
+ 
+	val = simple_strtoul(val_string, NULL, 10);
+	if (val != 0) 
+		naca->smt_snooze_delay = val;
+	else
+		return -EINVAL;
+
+	return count;
+}
+ 
+static int proc_ppc64_smt_state_read(char *page, char **start, off_t off,
+				      int count, int *eof, void *data)
+{
+	switch(naca->smt_state) {
+	case SMT_OFF:
+		return sprintf(page, "off\n");
+		break;
+	case SMT_ON:
+		return sprintf(page, "on\n");
+		break;
+	case SMT_DYNAMIC:
+		return sprintf(page, "dynamic\n");
+		break;
+	default:
+		return sprintf(page, "unknown\n");
+		break;
+	}
+}
+ 
+void proc_ppc64_create_smt(void)
+{
+	struct proc_dir_entry *ent_snooze = 
+		create_proc_entry("smt-snooze-delay", S_IRUGO | S_IWUSR, 
+				  proc_ppc64.root);
+	struct proc_dir_entry *ent_enabled = 
+		create_proc_entry("smt-enabled", S_IRUGO | S_IWUSR, 
+				  proc_ppc64.root);
+	if (ent_snooze) {
+		ent_snooze->nlink = 1;
+		ent_snooze->data = NULL;
+		ent_snooze->read_proc = (void *)proc_ppc64_smt_snooze_read;
+		ent_snooze->write_proc = (void *)proc_ppc64_smt_snooze_write;
+	}
+
+	if (ent_enabled) {
+		ent_enabled->nlink = 1;
+		ent_enabled->data = NULL;
+		ent_enabled->read_proc = (void *)proc_ppc64_smt_state_read;
+		ent_enabled->write_proc = NULL;
+	}
+}
+
 fs_initcall(proc_ppc64_init);
diff -purN linux-2.5/arch/ppc64/kernel/prom.c linuxppc64-2.5/arch/ppc64/kernel/prom.c
--- linux-2.5/arch/ppc64/kernel/prom.c	2004-02-28 08:27:02.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/prom.c	2004-03-01 02:24:58.000000000 +0000
@@ -851,15 +851,13 @@ prom_initialize_tce_table(void)
 		 * size to 4 MB.  This is enough to map 2GB of PCI DMA space.
 		 * By doing this, we avoid the pitfalls of trying to DMA to
 		 * MMIO space and the DMA alias hole.
-		 */
-		/* 
+		 *
 		 * On POWER4, firmware sets the TCE region by assuming
 		 * each TCE table is 8MB. Using this memory for anything
 		 * else will impact performance, so we always allocate 8MB.
 		 * Anton
-		 *
-		 * XXX FIXME use a cpu feature here
 		 */
+
 		minsize = 8UL << 20;
 
 		/* Align to the greater of the align or size */
@@ -1309,7 +1307,7 @@ smt_setup(void)
 		}
 
 		if (!found) {
-			my_smt_snooze_delay = 30000; /* default value */
+			my_smt_snooze_delay = 0; /* default value */
 		}
 	} else {
 		my_smt_snooze_delay = 0; /* default value */
@@ -2122,7 +2120,7 @@ finish_node_interrupts(struct device_nod
 			printk(KERN_CRIT "Could not allocate interrupt "
 			       "number for %s\n", np->full_name);
 		} else
-			np->intrs[i].line = openpic_to_irq(virq);
+			np->intrs[i].line = irq_offset_up(virq);
 
 		/* We offset irq numbers for the u3 MPIC by 128 in PowerMac */
 		if (systemcfg->platform == PLATFORM_POWERMAC && ic && ic->parent) {
@@ -2836,40 +2834,12 @@ int of_remove_node(struct device_node *n
 	struct device_node *parent, *child;
 
 	parent = of_get_parent(np);
-
 	if (!parent)
 		return -EINVAL;
 
-	/* Make sure we are not recursively removing
-	 * more than one level of nodes.  We need to
-	 * allow this so we can remove a slot containing
-	 * an IOA.
-	 */
-	for (child = of_get_next_child(np, NULL);
-	     child != NULL;
-	     child = of_get_next_child(np, child)) {
-		struct device_node *grandchild;
-
-		if ((grandchild = of_get_next_child(child, NULL))) {
-			/* Too deep */
-			of_node_put(grandchild);
-			of_node_put(child);
-			return -EBUSY;
-		}
-	}
-
-	/* Now that we're reasonably sure that we won't
-	 * overflow our stack, remove any children of np.
-	 */
-	for (child = of_get_next_child(np, NULL);
-	     child != NULL;
-	     child = of_get_next_child(np, child)) {
-		int rc;
-
-		if ((rc = of_remove_node(child))) {
-			of_node_put(child);
-			return rc;
-		}
+	if ((child = of_get_next_child(np, NULL))) {
+		of_node_put(child);
+		return -EBUSY;
 	}
 
 	write_lock(&devtree_lock);
@@ -2946,6 +2916,53 @@ static void remove_node_proc_entries(str
 }
 #endif /* CONFIG_PROC_DEVICETREE */
 
+/* 
+ * Fix up n_intrs and intrs fields in a new device node
+ *
+ */
+static int of_finish_dynamic_node_interrupts(struct device_node *node)
+{
+	int intrcells, intlen, i;
+	unsigned *irq, *ints, virq;
+	struct device_node *ic;
+
+	ints = (unsigned int *)get_property(node, "interrupts", &intlen);
+	intrcells = prom_n_intr_cells(node);
+	intlen /= intrcells * sizeof(unsigned int);
+	node->n_intrs = intlen;
+	node->intrs = kmalloc(sizeof(struct interrupt_info) * intlen,
+			      GFP_KERNEL);
+	if (!node->intrs) 
+		return -ENOMEM;
+
+	for (i = 0; i < intlen; ++i) {
+		int n, j;
+		node->intrs[i].line = 0;
+		node->intrs[i].sense = 1;
+		n = map_interrupt(&irq, &ic, node, ints, intrcells);
+		if (n <= 0)
+			continue;
+		virq = virt_irq_create_mapping(irq[0]);
+		if (virq == NO_IRQ) {
+			printk(KERN_CRIT "Could not allocate interrupt "
+			       "number for %s\n", node->full_name);
+			return -ENOMEM;
+		}
+		node->intrs[i].line = irq_offset_up(virq);
+		if (n > 1)
+			node->intrs[i].sense = irq[1];
+		if (n > 2) {
+			printk(KERN_DEBUG "hmmm, got %d intr cells for %s:", n,
+			       node->full_name);
+			for (j = 0; j < n; ++j)
+				printk(" %d", irq[j]);
+			printk("\n");
+		}
+		ints += intrcells;
+	}
+	return 0;
+}
+
 /*
  * Fix up the uninitialized fields in a new device node:
  * name, type, n_addrs, addrs, n_intrs, intrs, and pci-specific fields
@@ -2961,11 +2978,8 @@ static int of_finish_dynamic_node(struct
 {
 	struct device_node *parent = of_get_parent(node);
 	u32 *regs;
-	unsigned int *ints;
-	int intlen, intrcells;
-	int i, j, n, err = 0;
-	unsigned int *irq, virq;
-	struct device_node *ic;
+	int err = 0;
+	phandle *ibm_phandle;
  
 	node->name = get_property(node, "name", 0);
 	node->type = get_property(node, "device_type", 0);
@@ -2981,6 +2995,10 @@ static int of_finish_dynamic_node(struct
 	if (systemcfg->platform == PLATFORM_POWERMAC)
 		return -ENODEV;
 
+	/* fix up new node's linux_phandle field */
+	if ((ibm_phandle = (unsigned int *)get_property(node, "ibm,phandle", NULL)))
+		node->linux_phandle = *ibm_phandle;
+
 	/* do the work of interpret_pci_props */
 	if (parent->type && !strcmp(parent->type, "pci")) {
 		struct address_range *adr;
@@ -3010,45 +3028,9 @@ static int of_finish_dynamic_node(struct
 	}
 
 	/* now do the work of finish_node_interrupts */
-
-	ints = (unsigned int *) get_property(node, "interrupts", &intlen);
-	if (!ints) {
-		err = -ENODEV;
-		goto out;
-	}
-
-	intrcells = prom_n_intr_cells(node);
-	intlen /= intrcells * sizeof(unsigned int);
-	node->n_intrs = intlen;
-	node->intrs = kmalloc(sizeof(struct interrupt_info) * intlen,
-			      GFP_KERNEL);
-	if (!node->intrs) {
-		err = -ENOMEM;
-		goto out;
-	}
-
-	for (i = 0; i < intlen; ++i) {
-		node->intrs[i].line = 0;
-		node->intrs[i].sense = 1;
-		n = map_interrupt(&irq, &ic, node, ints, intrcells);
-		if (n <= 0)
-			continue;
-		virq = virt_irq_create_mapping(irq[0]);
-		if (virq == NO_IRQ) {
-			printk(KERN_CRIT "Could not allocate interrupt "
-			       "number for %s\n", node->full_name);
-		} else
-			node->intrs[i].line = openpic_to_irq(virq);
-		if (n > 1)
-			node->intrs[i].sense = irq[1];
-		if (n > 2) {
-			printk(KERN_DEBUG "hmmm, got %d intr cells for %s:", n,
-			       node->full_name);
-			for (j = 0; j < n; ++j)
-				printk(" %d", irq[j]);
-			printk("\n");
-		}
-		ints += intrcells;
+	if (get_property(node, "interrupts", 0)) {
+		err = of_finish_dynamic_node_interrupts(node);
+		if (err) goto out;
 	}
 
        /* now do the rough equivalent of update_dn_pci_info, this
diff -purN linux-2.5/arch/ppc64/kernel/ras.c linuxppc64-2.5/arch/ppc64/kernel/ras.c
--- linux-2.5/arch/ppc64/kernel/ras.c	2004-02-25 02:54:12.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/ras.c	2004-02-26 06:29:56.000000000 +0000
@@ -1,4 +1,3 @@
-
 /*
  * ras.c
  * Copyright (C) 2001 Dave Engebretsen IBM Corporation
@@ -81,7 +80,7 @@ static int __init init_ras_IRQ(void)
 				       "number for %s\n", np->full_name);
 				break;
 			}
-			request_irq(virq + NUM_8259_INTERRUPTS, 
+			request_irq(irq_offset_up(virq),
 				    ras_error_interrupt, 0, 
 				    "RAS_ERROR", NULL);
 			ireg++;
@@ -99,7 +98,7 @@ static int __init init_ras_IRQ(void)
 				       " number for %s\n", np->full_name);
 				break;
 			}
-			request_irq(virq + NUM_8259_INTERRUPTS, 
+			request_irq(irq_offset_up(virq), 
 				    ras_epow_interrupt, 0, 
 				    "RAS_EPOW", NULL);
 			ireg++;
diff -purN linux-2.5/arch/ppc64/kernel/rtas.c linuxppc64-2.5/arch/ppc64/kernel/rtas.c
--- linux-2.5/arch/ppc64/kernel/rtas.c	2004-02-23 16:39:07.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/rtas.c	2004-02-27 20:53:50.000000000 +0000
@@ -420,12 +420,25 @@ rtas_halt(void)
         rtas_power_off();
 }
 
+void
+rtas_os_term(void)
+{
+	long status;
+	char *str = "OS panic";
+
+	status = rtas_call(rtas_token("ibm,os-term"), 1, 1, NULL, __pa(str));
+	if (status != 0)
+		printk(KERN_EMERG "ibm,os-term call failed %ld\n", status);
+}
+
+
 unsigned long rtas_rmo_buf = 0;
 
 asmlinkage int ppc_rtas(struct rtas_args __user *uargs)
 {
 	struct rtas_args args;
 	unsigned long flags;
+	int nargs;
 
 	if (!capable(CAP_SYS_ADMIN))
 		return -EPERM;
@@ -433,14 +446,15 @@ asmlinkage int ppc_rtas(struct rtas_args
 	if (copy_from_user(&args, uargs, 3 * sizeof(u32)) != 0)
 		return -EFAULT;
 
-	if (args.nargs > ARRAY_SIZE(args.args)
+	nargs = args.nargs;
+	if (nargs > ARRAY_SIZE(args.args)
 	    || args.nret > ARRAY_SIZE(args.args)
-	    || args.nargs + args.nret > ARRAY_SIZE(args.args))
+	    || nargs + args.nret > ARRAY_SIZE(args.args))
 		return -EINVAL;
 
 	/* Copy in args. */
 	if (copy_from_user(args.args, uargs->args,
-			   args.nargs * sizeof(rtas_arg_t)) != 0)
+			   nargs * sizeof(rtas_arg_t)) != 0)
 		return -EFAULT;
 
 	spin_lock_irqsave(&rtas.lock, flags);
@@ -449,14 +463,15 @@ asmlinkage int ppc_rtas(struct rtas_args
 	enter_rtas((void *)__pa((unsigned long)&get_paca()->xRtas));
 	args = get_paca()->xRtas;
 
+	args.rets  = (rtas_arg_t *)&(args.args[nargs]);
 	if (args.rets[0] == -1)
 		log_rtas_error(&args);
 
 	spin_unlock_irqrestore(&rtas.lock, flags);
 
 	/* Copy out args. */
-	if (copy_to_user(uargs->args + args.nargs,
-			 args.args + args.nargs,
+	if (copy_to_user(uargs->args + nargs,
+			 args.args + nargs,
 			 args.nret * sizeof(rtas_arg_t)) != 0)
 		return -EFAULT;
 
diff -purN linux-2.5/arch/ppc64/kernel/rtasd.c linuxppc64-2.5/arch/ppc64/kernel/rtasd.c
--- linux-2.5/arch/ppc64/kernel/rtasd.c	2004-02-23 16:39:10.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/rtasd.c	2004-02-27 15:42:39.000000000 +0000
@@ -121,10 +121,11 @@ static int log_rtas_len(char * buf)
 
 		/* extended header */
 		len += err->extended_log_length;
-
-		if (len > RTAS_ERROR_LOG_MAX)
-			len = RTAS_ERROR_LOG_MAX;
 	}
+
+	if (len > rtas_error_log_max)
+		len = rtas_error_log_max;
+
 	return len;
 }
 
@@ -332,6 +333,10 @@ static int get_eventscan_parms(void)
 		printk(KERN_ERR "rtasd: truncated error log from %d to %d bytes\n", rtas_error_log_max, RTAS_ERROR_LOG_MAX);
 		rtas_error_log_max = RTAS_ERROR_LOG_MAX;
 	}
+
+	/* Make room for the sequence number */
+	rtas_error_log_buffer_max = rtas_error_log_max + sizeof(int);
+
 	of_node_put(node);
 
 	return 0;
@@ -407,8 +412,7 @@ static int rtasd(void *unused)
 
 	if (surveillance_timeout != -1) {
 		DEBUG("enabling surveillance\n");
-		if (enable_surveillance(surveillance_timeout))
-			goto error_vfree;
+		enable_surveillance(surveillance_timeout);
 		DEBUG("surveillance enabled\n");
 	}
 
@@ -432,10 +436,6 @@ static int rtasd(void *unused)
 			cpu = first_cpu_const(mk_cpumask_const(cpu_online_map));
 	}
 
-error_vfree:
-	if (rtas_log_buf)
-		vfree(rtas_log_buf);
-	rtas_log_buf = NULL;
 error:
 	/* Should delete proc entries */
 	return -EINVAL;
@@ -463,9 +463,6 @@ static int __init rtas_init(void)
 	if (kernel_thread(rtasd, 0, CLONE_FS) < 0)
 		printk(KERN_ERR "Failed to start RTAS daemon\n");
 
-	/* Make room for the sequence number */
-	rtas_error_log_buffer_max = rtas_error_log_max + sizeof(int);
-
 	return 0;
 }
 
diff -purN linux-2.5/arch/ppc64/kernel/setup.c linuxppc64-2.5/arch/ppc64/kernel/setup.c
--- linux-2.5/arch/ppc64/kernel/setup.c	2004-02-23 16:39:09.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/setup.c	2004-02-27 20:53:50.000000000 +0000
@@ -25,6 +25,7 @@
 #include <linux/version.h>
 #include <linux/tty.h>
 #include <linux/root_dev.h>
+#include <linux/notifier.h>
 #include <asm/io.h>
 #include <asm/prom.h>
 #include <asm/processor.h>
@@ -93,6 +94,13 @@ unsigned long SYSRQ_KEY;
 
 struct machdep_calls ppc_md;
 
+static int ppc64_panic_event(struct notifier_block *, unsigned long, void *);
+
+static struct notifier_block ppc64_panic_block = {
+	notifier_call: ppc64_panic_event,
+	priority: INT_MIN /* may not return; must be done last */
+};
+
 /*
  * Perhaps we can put the pmac screen_info[] here
  * on pmac as well so we don't need the ifdef's.
@@ -215,6 +223,7 @@ void setup_system(unsigned long r3, unsi
 	if (systemcfg->platform & PLATFORM_PSERIES) {
 		early_console_initialized = 1;
 		register_console(&udbg_console);
+		__irq_offset_value = NUM_ISA_INTERRUPTS;
 		finish_device_tree();
 		chrp_init(r3, r4, r5, r6, r7);
 
@@ -318,6 +327,14 @@ EXPORT_SYMBOL(machine_halt);
 unsigned long ppc_proc_freq;
 unsigned long ppc_tb_freq;
 
+static int ppc64_panic_event(struct notifier_block *this,
+                             unsigned long event, void *ptr)
+{
+	ppc_md.panic();         /* May not return */
+	return NOTIFY_DONE;
+}
+
+
 #ifdef CONFIG_SMP
 DEFINE_PER_CPU(unsigned int, pvr);
 #endif
@@ -421,7 +438,6 @@ void parse_cmd_line(unsigned long r3, un
 	if ((initrd_start == 0) && r3 && r4 && r4 != 0xdeadbeef) {
 		initrd_start = (r3 >= KERNELBASE) ? r3 : (unsigned long)__va(r3);
 		initrd_end = initrd_start + r4;
-		ROOT_DEV = Root_RAM0;
 		initrd_below_start_ok = 1;
 	}
 #endif
@@ -546,7 +562,6 @@ int parse_bootinfo(void)
 		case BI_INITRD:
 			initrd_start = (unsigned long)__va(rec->data[0]);
 			initrd_end = initrd_start + rec->data[1];
-			ROOT_DEV = Root_RAM0;
 			initrd_below_start_ok = 1;
 			break;
 #endif /* CONFIG_BLK_DEV_INITRD */
@@ -614,6 +629,9 @@ void __init setup_arch(char **cmdline_p)
 	/* reboot on panic */
 	panic_timeout = 180;
 
+	if (ppc_md.panic)
+		notifier_chain_register(&panic_notifier_list, &ppc64_panic_block);
+
 	init_mm.start_code = PAGE_OFFSET;
 	init_mm.end_code = (unsigned long) _etext;
 	init_mm.end_data = (unsigned long) _edata;
diff -purN linux-2.5/arch/ppc64/kernel/signal.c linuxppc64-2.5/arch/ppc64/kernel/signal.c
--- linux-2.5/arch/ppc64/kernel/signal.c	2004-01-31 08:15:29.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/signal.c	2004-02-01 02:40:22.000000000 +0000
@@ -96,7 +96,7 @@ long sys_rt_sigsuspend(sigset_t *unewset
 	regs->gpr[3] = EINTR;
 	regs->ccr |= 0x10000000;
 	while (1) {
-		current->state = TASK_INTERRUPTIBLE;
+		set_current_state(TASK_INTERRUPTIBLE);
 		schedule();
 		if (do_signal(&saveset, regs))
 			return regs->gpr[3];
diff -purN linux-2.5/arch/ppc64/kernel/signal32.c linuxppc64-2.5/arch/ppc64/kernel/signal32.c
--- linux-2.5/arch/ppc64/kernel/signal32.c	2004-02-25 10:31:13.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/signal32.c	2004-02-26 05:41:43.000000000 +0000
@@ -271,7 +271,7 @@ long sys32_sigsuspend(old_sigset_t mask,
 	regs->gpr[3] = EINTR;
 	regs->ccr |= 0x10000000;
 	while (1) {
-		current->state = TASK_INTERRUPTIBLE;
+		set_current_state(TASK_INTERRUPTIBLE);
 		schedule();
 		if (do_signal32(&saveset, regs))
 			/*
@@ -589,7 +589,7 @@ int sys32_rt_sigsuspend(compat_sigset_t*
 	regs->gpr[3] = EINTR;
 	regs->ccr |= 0x10000000;
 	while (1) {
-		current->state = TASK_INTERRUPTIBLE;
+		set_current_state(TASK_INTERRUPTIBLE);
 		schedule();
 		if (do_signal32(&saveset, regs))
 			/*
diff -purN linux-2.5/arch/ppc64/kernel/smp.c linuxppc64-2.5/arch/ppc64/kernel/smp.c
--- linux-2.5/arch/ppc64/kernel/smp.c	2004-02-23 16:39:11.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/smp.c	2004-02-25 04:28:40.000000000 +0000
@@ -53,6 +53,10 @@
 #include <asm/cputable.h>
 #include <asm/system.h>
 
+#ifdef CONFIG_KDB
+#include <linux/kdb.h>
+#endif
+
 int smp_threads_ready;
 unsigned long cache_decay_ticks;
 
@@ -75,6 +79,23 @@ void smp_call_function_interrupt(void);
 extern long register_vpa(unsigned long flags, unsigned long proc,
 			 unsigned long vpa);
 
+#ifdef CONFIG_KDB
+	/* save regs here before calling kdb_ipi */
+struct pt_regs *kdb_smp_regs[NR_CPUS];
+	
+/* called for each processor.. drop each into kdb. */
+static void smp_kdb_stop_proc(void *dummy)
+{
+    kdb_ipi(kdb_smp_regs[smp_processor_id()], NULL);
+}
+	
+void smp_kdb_stop(void)
+{
+    int ret=0;
+    ret = smp_call_function(smp_kdb_stop_proc, NULL, 1, 0);
+}
+#endif
+
 /* Low level assembly function used to backup CPU 0 state */
 extern void __save_cpu_setup(void);
 
@@ -378,6 +399,9 @@ void smp_message_recv(int msg, struct pt
 {
 	switch(msg) {
 	case PPC_MSG_CALL_FUNCTION:
+#ifdef CONFIG_KDB
+	        kdb_smp_regs[smp_processor_id()]=regs;
+#endif
 		smp_call_function_interrupt();
 		break;
 	case PPC_MSG_RESCHEDULE: 
@@ -676,6 +700,7 @@ int __devinit __cpu_up(unsigned int cpu)
 	return 0;
 }
 
+extern unsigned int default_distrib_server;
 /* Activate a secondary processor. */
 int __devinit start_secondary(void *unused)
 {
@@ -698,6 +723,15 @@ int __devinit start_secondary(void *unus
 	if (cur_cpu_spec->firmware_features & FW_FEATURE_SPLPAR) {
 		vpa_init(cpu); 
 	}
+
+#ifdef CONFIG_IRQ_ALL_CPUS
+	/* Put the calling processor into the GIQ.  This is really only 
+	 * necessary from a secondary thread as the OF start-cpu interface
+	 * performs this function for us on primary threads.
+	 */
+	/* TODO: 9005 is #defined in rtas-proc.c -- move to a header */
+	rtas_set_indicator(9005, default_distrib_server, 1);
+#endif
 #endif
 
 	local_irq_enable();
diff -purN linux-2.5/arch/ppc64/kernel/stab.c linuxppc64-2.5/arch/ppc64/kernel/stab.c
--- linux-2.5/arch/ppc64/kernel/stab.c	2004-02-27 22:59:30.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/stab.c	2004-03-01 02:24:58.000000000 +0000
@@ -196,7 +196,7 @@ int ste_allocate(unsigned long ea)
 	esid = GET_ESID(ea);
 	__ste_allocate(esid, vsid);
 	/* Order update */
-	asm volatile("sync":::"memory");
+	asm volatile("sync":::"memory"); 
 
 	return 0;
 }
@@ -270,10 +270,10 @@ void flush_stab(struct task_struct *tsk,
 		/* Invalidate all entries. */
 		ste = stab;
 
-		/* Never flush the first entry. */
+		/* Never flush the first entry. */ 
 		ste += 1;
 		for (entry = 1;
-		     entry < (PAGE_SIZE / sizeof(STE));
+		     entry < (PAGE_SIZE / sizeof(STE)); 
 		     entry++, ste++) {
 			unsigned long ea;
 			ea = ste->dw0.dw0.esid << SID_SHIFT;
diff -purN linux-2.5/arch/ppc64/kernel/vio.c linuxppc64-2.5/arch/ppc64/kernel/vio.c
--- linux-2.5/arch/ppc64/kernel/vio.c	2004-03-01 16:04:41.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/vio.c	2004-03-02 02:42:30.000000000 +0000
@@ -25,7 +25,6 @@
 #include <asm/ppcdebug.h>
 #include <asm/vio.h>
 #include <asm/hvcall.h>
-#include "open_pic.h"
 
 #define DBGENTER() pr_debug("%s entered\n", __FUNCTION__)
 
@@ -255,7 +254,7 @@ struct vio_dev * __devinit vio_register_
 			printk(KERN_ERR "Unable to allocate interrupt "
 			       "number for %s\n", of_node->full_name);
 		} else
-			viodev->irq = openpic_to_irq(virq);
+			viodev->irq = irq_offset_up(virq);
 	}
 
 	/* init generic 'struct device' fields: */
@@ -581,5 +580,4 @@ struct bus_type vio_bus_type = {
 	.match = vio_bus_match,
 };
 
-EXPORT_SYMBOL(plpar_hcall_norets);
-EXPORT_SYMBOL(plpar_hcall_8arg_2ret);
+EXPORT_SYMBOL(vio_bus_type);
diff -purN linux-2.5/arch/ppc64/kernel/viopath.c linuxppc64-2.5/arch/ppc64/kernel/viopath.c
--- linux-2.5/arch/ppc64/kernel/viopath.c	2004-02-27 05:25:15.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/viopath.c	2004-03-02 03:00:22.000000000 +0000
@@ -35,7 +35,6 @@
 #include <linux/vmalloc.h>
 #include <linux/string.h>
 #include <linux/proc_fs.h>
-#include <linux/device.h>
 #include <linux/dma-mapping.h>
 #include <linux/wait.h>
 
@@ -49,8 +48,6 @@
 #include <asm/iSeries/iSeries_proc.h>
 #include <asm/iSeries/vio.h>
 
-extern struct device *iSeries_vio_dev;
-
 /* Status of the path to each other partition in the system.
  * This is overkill, since we will only ever establish connections
  * to our hosting partition and the primary partition on the system.
diff -purN linux-2.5/arch/ppc64/kernel/xics.c linuxppc64-2.5/arch/ppc64/kernel/xics.c
--- linux-2.5/arch/ppc64/kernel/xics.c	2004-02-25 02:54:12.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/xics.c	2004-02-26 10:56:03.000000000 +0000
@@ -58,7 +58,6 @@ struct hw_interrupt_type xics_8259_pic =
 static struct radix_tree_root irq_map = RADIX_TREE_INIT(GFP_KERNEL);
 
 #define XICS_IPI		2
-#define XICS_IRQ_OFFSET		0x10
 #define XICS_IRQ_SPURIOUS	0
 
 /* Want a priority other than 0.  Various HW issues require this. */
@@ -91,7 +90,8 @@ static struct xics_ipl *xics_per_cpu[NR_
 static int xics_irq_8259_cascade = 0;
 static int xics_irq_8259_cascade_real = 0;
 static unsigned int default_server = 0xFF;
-static unsigned int default_distrib_server = 0;
+/* also referenced in smp.c... */
+unsigned int default_distrib_server = 0;
 
 /*
  * XICS only has a single IPI, so encode the messages per CPU
@@ -216,7 +216,7 @@ xics_ops pSeriesLP_ops = {
 
 static unsigned int xics_startup(unsigned int virq)
 {
-	virq -= XICS_IRQ_OFFSET;
+	virq = irq_offset_down(virq);
 	if (radix_tree_insert(&irq_map, virt_irq_to_real(virq),
 			      &virt_irq_to_real_map[virq]) == -ENOMEM)
 		printk(KERN_CRIT "Out of memory creating real -> virtual"
@@ -235,26 +235,50 @@ static unsigned int real_irq_to_virt(uns
 	return ptr - virt_irq_to_real_map;
 }
 
+/* XXX Fix this when we clean up large irq support */
+extern cpumask_t get_irq_affinity(unsigned int irq);
+
+static int get_irq_server(unsigned int irq)
+{
+	cpumask_t cpumask = irq_affinity[irq];
+	cpumask_t allcpus = CPU_MASK_ALL;
+	cpumask_t tmp = CPU_MASK_NONE;
+	unsigned int server;
+	
+#ifdef CONFIG_IRQ_ALL_CPUS
+	/* For the moment only implement delivery to all cpus or one cpu */
+	if (smp_threads_ready) {
+		if (cpus_equal(cpumask, allcpus)) {
+			server = default_distrib_server;
+		} else {
+			cpus_and(tmp, cpu_online_map, cpumask);
+
+			if (cpus_empty(tmp))
+				server = default_distrib_server;
+			else
+				server = get_hard_smp_processor_id(first_cpu(tmp));
+		}
+	} else {
+		server = default_server;
+	}
+#else
+	server = default_server;
+#endif
+	return server;
+
+}
+
 static void xics_enable_irq(unsigned int virq)
 {
 	unsigned int irq;
 	long call_status;
 	unsigned int server;
 
-	virq -= XICS_IRQ_OFFSET;
-	irq = virt_irq_to_real(virq);
+	irq = virt_irq_to_real(irq_offset_down(virq));
 	if (irq == XICS_IPI)
 		return;
 
-#ifdef CONFIG_IRQ_ALL_CPUS
-	if (smp_threads_ready)
-		server = default_distrib_server;
-	else
-		server = default_server;
-#else
-	server = default_server;
-#endif
-
+	server = get_irq_server(virq);
 	call_status = rtas_call(ibm_set_xive, 3, 1, NULL, irq, server,
 				DEFAULT_PRIORITY);
 	if (call_status != 0) {
@@ -275,6 +299,7 @@ static void xics_enable_irq(unsigned int
 static void xics_disable_real_irq(unsigned int irq)
 {
 	long call_status;
+	unsigned int server;
 
 	if (irq == XICS_IPI)
 		return;
@@ -286,9 +311,9 @@ static void xics_disable_real_irq(unsign
 		return;
 	}
 
+	server = get_irq_server(irq);
 	/* Have to set XIVE to 0xff to be able to remove a slot */
-	call_status = rtas_call(ibm_set_xive, 3, 1, NULL, irq, default_server,
-				0xff);
+	call_status = rtas_call(ibm_set_xive, 3, 1, NULL, irq, server, 0xff);
 	if (call_status != 0) {
 		printk(KERN_ERR "xics_disable_irq: irq=%x: ibm_set_xive(0xff)"
 		       " returned %lx\n", irq, call_status);
@@ -300,25 +325,25 @@ static void xics_disable_irq(unsigned in
 {
 	unsigned int irq;
 
-	virq -= XICS_IRQ_OFFSET;
-	irq = virt_irq_to_real(virq);
+	irq = virt_irq_to_real(irq_offset_down(virq));
 	xics_disable_real_irq(irq);
 }
 
-static void xics_end_irq(unsigned int	irq)
+static void xics_end_irq(unsigned int irq)
 {
 	int cpu = smp_processor_id();
 
 	iosync();
-	ops->xirr_info_set(cpu, ((0xff<<24) |
-				 (virt_irq_to_real(irq-XICS_IRQ_OFFSET))));
+	ops->xirr_info_set(cpu, ((0xff << 24) |
+				 (virt_irq_to_real(irq_offset_down(irq)))));
+
 }
 
 static void xics_mask_and_ack_irq(unsigned int irq)
 {
 	int cpu = smp_processor_id();
 
-	if (irq < XICS_IRQ_OFFSET) {
+	if (irq < irq_offset_value()) {
 		i8259_pic.ack(irq);
 		iosync();
 		ops->xirr_info_set(cpu, ((0xff<<24) |
@@ -342,7 +367,8 @@ int xics_get_irq(struct pt_regs *regs)
 		irq = i8259_irq(cpu);
 		if (irq == -1) {
 			/* Spurious cascaded interrupt.  Still must ack xics */
-                        xics_end_irq(XICS_IRQ_OFFSET + xics_irq_8259_cascade);
+			xics_end_irq(irq_offset_up(xics_irq_8259_cascade));
+
 			irq = -1;
 		}
 	} else if (vec == XICS_IRQ_SPURIOUS) {
@@ -354,7 +380,7 @@ int xics_get_irq(struct pt_regs *regs)
 			       " disabling it.\n", vec);
 			xics_disable_real_irq(vec);
 		} else
-			irq += XICS_IRQ_OFFSET;
+			irq = irq_offset_up(irq);
 	}
 	return irq;
 }
@@ -530,9 +556,9 @@ nextnode:
 	xics_8259_pic.enable = i8259_pic.enable;
 	xics_8259_pic.disable = i8259_pic.disable;
 	for (i = 0; i < 16; ++i)
-		irq_desc[i].handler = &xics_8259_pic;
+		get_irq_desc(i)->handler = &xics_8259_pic;
 	for (; i < NR_IRQS; ++i)
-		irq_desc[i].handler = &xics_pic;
+		get_irq_desc(i)->handler = &xics_pic;
 
 	ops->cppr_info(boot_cpuid, 0xff);
 	iosync();
@@ -548,7 +574,7 @@ static int __init xics_setup_i8259(void)
 {
 	if (naca->interrupt_controller == IC_PPC_XIC &&
 	    xics_irq_8259_cascade != -1) {
-		if (request_irq(xics_irq_8259_cascade + XICS_IRQ_OFFSET,
+		if (request_irq(irq_offset_up(xics_irq_8259_cascade),
 				no_action, 0, "8259 cascade", 0))
 			printk(KERN_ERR "xics_init_IRQ: couldn't get 8259 cascade\n");
 		i8259_init();
@@ -563,15 +589,15 @@ void xics_request_IPIs(void)
 	virt_irq_to_real_map[XICS_IPI] = XICS_IPI;
 
 	/* IPIs are marked SA_INTERRUPT as they must run with irqs disabled */
-	request_irq(XICS_IPI + XICS_IRQ_OFFSET, xics_ipi_action, SA_INTERRUPT,
+	request_irq(irq_offset_up(XICS_IPI), xics_ipi_action, SA_INTERRUPT,
 		    "IPI", 0);
-	irq_desc[XICS_IPI+XICS_IRQ_OFFSET].status |= IRQ_PER_CPU;
+	get_irq_desc(irq_offset_up(XICS_IPI))->status |= IRQ_PER_CPU;
 }
 #endif
 
 static void xics_set_affinity(unsigned int virq, cpumask_t cpumask)
 {
-        irq_desc_t *desc = irq_desc + virq;
+	irq_desc_t *desc = get_irq_desc(virq);
 	unsigned int irq;
 	unsigned long flags;
 	long status;
@@ -580,8 +606,7 @@ static void xics_set_affinity(unsigned i
 	cpumask_t allcpus = CPU_MASK_ALL;
 	cpumask_t tmp = CPU_MASK_NONE;
 
-	virq -= XICS_IRQ_OFFSET;
-	irq = virt_irq_to_real(virq);
+	irq = virt_irq_to_real(irq_offset_down(virq));
 	if (irq == XICS_IPI)
 		return;
 
@@ -602,7 +627,7 @@ static void xics_set_affinity(unsigned i
 		cpus_and(tmp, cpu_online_map, cpumask);
 		if (cpus_empty(tmp))
 			goto out;
-		newmask = get_hard_smp_processor_id(first_cpu(cpumask));
+		newmask = get_hard_smp_processor_id(first_cpu(tmp));
 	}
 
 	status = rtas_call(ibm_set_xive, 3, 1, NULL,
diff -purN linux-2.5/arch/ppc64/mm/numa.c linuxppc64-2.5/arch/ppc64/mm/numa.c
--- linux-2.5/arch/ppc64/mm/numa.c	2004-03-02 03:01:22.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/mm/numa.c	2004-03-02 12:20:19.000000000 +0000
@@ -119,6 +119,7 @@ static int __init parse_numa_properties(
 		unsigned long size = 0;
 		int numa_domain;
 		int ranges;
+		int propsize;
 
 		tmp1 = (int *)get_property(memory, "reg", NULL);
 		if (!tmp1)
@@ -145,10 +146,21 @@ new_range:
 		if ((start + size) > MAX_MEMORY)
 			BUG();
 
+		/* Some versions of OF sometimes have an empty property for
+		 * associativity, so we need to get the size too.
+		 */
 		tmp2 = (int *)get_property(memory, "ibm,associativity",
-					   NULL);
+					   &propsize);
 		if (!tmp2)
 			continue;
+
+		if (!propsize) {
+			printk(KERN_INFO "Buggy OF? Empty ibm,associativity "
+			       "property for %s. Disabling NUMA.\n",
+			       memory->full_name);
+			goto err;
+		}
+
 		numa_domain = tmp2[depth];
 
 		/* FIXME */
diff -purN linux-2.5/drivers/block/viodasd.c linuxppc64-2.5/drivers/block/viodasd.c
--- linux-2.5/drivers/block/viodasd.c	2004-03-02 03:01:22.000000000 +0000
+++ linuxppc64-2.5/drivers/block/viodasd.c	2004-03-02 12:17:10.000000000 +0000
@@ -38,7 +38,6 @@
 #include <linux/errno.h>
 #include <linux/init.h>
 #include <linux/string.h>
-#include <linux/device.h>
 #include <linux/dma-mapping.h>
 #include <linux/completion.h>
 
@@ -77,8 +76,6 @@ static spinlock_t	viodasd_spinlock = SPI
 
 #define DEVICE_NO(cell)	((struct viodasd_device *)(cell) - &viodasd_devices[0])
 
-extern struct device *iSeries_vio_dev;
-
 struct open_data {
 	u64	disk_size;
 	u16	max_disk;
diff -purN linux-2.5/drivers/cdrom/Makefile linuxppc64-2.5/drivers/cdrom/Makefile
--- linux-2.5/drivers/cdrom/Makefile	2003-02-03 22:19:36.000000000 +0000
+++ linuxppc64-2.5/drivers/cdrom/Makefile	2003-12-19 02:44:07.000000000 +0000
@@ -20,3 +20,4 @@ obj-$(CONFIG_OPTCD)		+= optcd.o
 obj-$(CONFIG_SBPCD)		+= sbpcd.o      cdrom.o
 obj-$(CONFIG_SJCD)		+= sjcd.o
 obj-$(CONFIG_CDU535)		+= sonycd535.o
+obj-$(CONFIG_VIOCD)		+= viocd.o	cdrom.o
diff -purN linux-2.5/drivers/cdrom/viocd.c linuxppc64-2.5/drivers/cdrom/viocd.c
--- linux-2.5/drivers/cdrom/viocd.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/cdrom/viocd.c	2004-02-12 12:12:02.000000000 +0000
@@ -0,0 +1,793 @@
+/* -*- linux-c -*-
+ *  drivers/cdrom/viocd.c
+ *
+ ***************************************************************************
+ *  iSeries Virtual CD Rom
+ *
+ *  Authors: Dave Boutcher <boutcher@us.ibm.com>
+ *           Ryan Arnold <ryanarn@us.ibm.com>
+ *           Colin Devilbiss <devilbis@us.ibm.com>
+ *
+ * (C) Copyright 2000-2003 IBM Corporation
+ * 
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) anyu later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.  
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *************************************************************************** 
+ * This routine provides access to CD ROM drives owned and managed by an 
+ * OS/400 partition running on the same box as this Linux partition.
+ *
+ * All operations are performed by sending messages back and forth to 
+ * the OS/400 partition.  
+ */
+
+#include <linux/config.h>
+#include <linux/major.h>
+#include <linux/blkdev.h>
+#include <linux/cdrom.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/device.h>
+#include <linux/dma-mapping.h>
+#include <linux/proc_fs.h>
+#include <linux/module.h>
+
+#include <asm/iSeries/HvTypes.h>
+#include <asm/iSeries/HvLpEvent.h>
+#include <asm/iSeries/vio.h>
+#include <asm/iSeries/iSeries_proc.h>
+
+/* Decide on the proper naming convention to use for our device */
+#ifdef CONFIG_VIOCD_AZTECH
+#define DEVICE_NAME			"Aztech CD-ROM"
+#define MAJOR_NR			AZTECH_CDROM_MAJOR
+#define VIOCD_DEVICE			"aztcd"
+#define VIOCD_DEVICE_OFFSET		0
+#else
+#define DEVICE_NAME			"viocd"
+#define MAJOR_NR			VIOCD_MAJOR
+#define VIOCD_DEVICE			"iseries/vcd%c"
+#define VIOCD_DEVICE_OFFSET		'a'
+#endif
+#define VIOCD_DEVICE_DEVFS		"viocd/%d"
+#define VIOCD_DEVICE_OFFSET_DEVFS	0
+
+#define VIOCD_VERS "1.04"
+
+extern struct device *iSeries_vio_dev;
+
+#define signalLpEventFast HvCallEvent_signalLpEventFast
+
+struct viocdlpevent {
+	struct HvLpEvent event;
+	u32 mReserved1;
+	u16 mVersion;
+	u16 mSubTypeRc;
+	u16 mDisk;
+	u16 mFlags;
+	u32 mToken;
+	u64 mOffset;		// On open, the max number of disks
+	u64 mLen;		// On open, the size of the disk
+	u32 mBlockSize;		// Only set on open
+	u32 mMediaSize;		// Only set on open
+};
+
+enum viocdsubtype {
+	viocdopen = 0x0001,
+	viocdclose = 0x0002,
+	viocdread = 0x0003,
+	viocdwrite = 0x0004,
+	viocdlockdoor = 0x0005,
+	viocdgetinfo = 0x0006,
+	viocdcheck = 0x0007
+};
+
+/*
+ * Should probably make this a module parameter....sigh
+ */
+#define VIOCD_MAX_CD 8
+
+static const struct vio_error_entry viocd_err_table[] = {
+	{0x0201, EINVAL, "Invalid Range"},
+	{0x0202, EINVAL, "Invalid Token"},
+	{0x0203, EIO, "DMA Error"},
+	{0x0204, EIO, "Use Error"},
+	{0x0205, EIO, "Release Error"},
+	{0x0206, EINVAL, "Invalid CD"},
+	{0x020C, EROFS, "Read Only Device"},
+	{0x020D, EIO, "Changed or Missing Volume (or Varied Off?)"},
+	{0x020E, EIO, "Optical System Error (Varied Off?)"},
+	{0x02FF, EIO, "Internal Error"},
+	{0x3010, EIO, "Changed Volume"},
+	{0xC100, EIO, "Optical System Error"},
+	{0x0000, 0, NULL},
+};
+
+/*
+ * This is the structure we use to exchange info between driver and interrupt
+ * handler
+ */
+struct viocd_waitevent {
+	struct semaphore *sem;
+	int rc;
+	u16 subtypeRc;
+	int changed;
+};
+
+/* this is a lookup table for the true capabilities of a device */
+struct capability_entry {
+	char *type;
+	int capability;
+};
+
+static struct capability_entry capability_table[] __initdata = {
+	{ "6330", CDC_LOCK | CDC_DVD_RAM },
+	{ "6321", CDC_LOCK },
+	{ "632B", 0 },
+	{ NULL  , CDC_LOCK },
+};
+
+/* These are our internal structures for keeping track of devices */
+static int viocd_numdev;
+
+struct cdrom_info {
+	char rsrcname[10];
+	char type[4];
+	char model[3];
+};
+static struct cdrom_info viocd_unitinfo[VIOCD_MAX_CD];
+
+struct disk_info{
+	u32 useCount;
+	u32 blocksize;
+	u32 mediasize;
+	struct gendisk *viocd_disk;
+	struct cdrom_device_info viocd_info;
+};
+static struct disk_info viocd_diskinfo[VIOCD_MAX_CD];
+
+#define DEVICE_NR(di)	((di) - &viocd_diskinfo[0])
+#define VIOCDI		viocd_diskinfo[deviceno].viocd_info
+
+static request_queue_t *viocd_queue;
+static spinlock_t viocd_lock;
+static spinlock_t viocd_reqlock;
+
+#define MAX_CD_REQ 1
+
+static int viocd_blk_open(struct inode *inode, struct file *file)
+{
+	struct disk_info *di = inode->i_bdev->bd_disk->private_data;
+	return cdrom_open(&di->viocd_info, inode, file);
+}
+
+static int viocd_blk_release(struct inode *inode, struct file *file)
+{
+	struct disk_info *di = inode->i_bdev->bd_disk->private_data;
+	return cdrom_release(&di->viocd_info, file);
+}
+
+static int viocd_blk_ioctl(struct inode *inode, struct file *file,
+			unsigned cmd, unsigned long arg)
+{
+	struct disk_info *di = inode->i_bdev->bd_disk->private_data;
+	return cdrom_ioctl(&di->viocd_info, inode, cmd, arg);
+}
+
+static int viocd_blk_media_changed(struct gendisk *disk)
+{
+	struct disk_info *di = disk->private_data;
+	return cdrom_media_changed(&di->viocd_info);
+}
+
+struct block_device_operations viocd_fops = {
+	.owner =		THIS_MODULE,
+	.open =			viocd_blk_open,
+	.release =		viocd_blk_release,
+	.ioctl =		viocd_blk_ioctl,
+	.media_changed =	viocd_blk_media_changed,
+};
+
+static void viocd_end_request(struct request *req, int uptodate,
+		int num_sectors)
+{
+	if (!uptodate)
+		num_sectors = req->current_nr_sectors;
+	if (end_that_request_first(req, uptodate, num_sectors))
+		return;
+	end_that_request_last(req);
+}
+
+/* Get info on CD devices from OS/400 */
+static void __init get_viocd_info(void)
+{
+	dma_addr_t dmaaddr;
+	HvLpEvent_Rc hvrc;
+	int i;
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	struct viocd_waitevent we;
+
+	/* If we don't have a host, bail out */
+	if (viopath_hostLp == HvLpIndexInvalid)
+		return;
+
+	dmaaddr = dma_map_single(iSeries_vio_dev, viocd_unitinfo,
+			sizeof(viocd_unitinfo), DMA_FROM_DEVICE);
+	if (dmaaddr == 0xFFFFFFFF) {
+		printk(KERN_WARNING_VIO "error allocating tce\n");
+		return;
+	}
+
+	we.sem = &Semaphore;
+
+	hvrc = signalLpEventFast(viopath_hostLp,
+			HvLpEvent_Type_VirtualIo,
+			viomajorsubtype_cdio | viocdgetinfo,
+			HvLpEvent_AckInd_DoAck, HvLpEvent_AckType_ImmediateAck,
+			viopath_sourceinst(viopath_hostLp),
+			viopath_targetinst(viopath_hostLp),
+			(u64)&we, VIOVERSION << 16, dmaaddr, 0,
+			sizeof(viocd_unitinfo), 0);
+	if (hvrc != HvLpEvent_Rc_Good) {
+		printk(KERN_WARNING_VIO "cdrom error sending event. rc %d\n",
+				(int)hvrc);
+		return;
+	}
+
+	/* wait for completion */
+	down(&Semaphore);
+
+	dma_unmap_single(iSeries_vio_dev, dmaaddr, sizeof(viocd_unitinfo),
+			DMA_FROM_DEVICE);
+
+	if (we.rc) {
+		const struct vio_error_entry *err =
+			vio_lookup_rc(viocd_err_table, we.subtypeRc);
+		printk(KERN_WARNING_VIO "bad rc %d:0x%04X on getinfo: %s\n",
+				we.rc, we.subtypeRc, err->msg);
+		return;
+	}
+
+	for (i = 0; (i < VIOCD_MAX_CD) && viocd_unitinfo[i].rsrcname[0]; i++)
+		viocd_numdev++;
+}
+
+static int viocd_open(struct cdrom_device_info *cdi, int purpose)
+{
+	DECLARE_MUTEX_LOCKED(Semaphore);
+        struct disk_info *diskinfo = cdi->handle;
+	int device_no = DEVICE_NR(diskinfo);
+	HvLpEvent_Rc hvrc;
+	struct viocd_waitevent we;
+
+	/* If we don't have a host, bail out */
+	if ((viopath_hostLp == HvLpIndexInvalid) || (device_no >= viocd_numdev))
+		return -ENODEV;
+
+	we.sem = &Semaphore;
+	hvrc = signalLpEventFast(viopath_hostLp, HvLpEvent_Type_VirtualIo,
+			viomajorsubtype_cdio | viocdopen,
+			HvLpEvent_AckInd_DoAck, HvLpEvent_AckType_ImmediateAck,
+			viopath_sourceinst(viopath_hostLp),
+			viopath_targetinst(viopath_hostLp),
+			(u64)&we, VIOVERSION << 16, ((u64)device_no << 48),
+			0, 0, 0);
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO "bad rc on signalLpEventFast %d\n",
+		       (int)hvrc);
+		return -EIO;
+	}
+
+	down(&Semaphore);
+
+	if (we.rc) {
+		const struct vio_error_entry *err =
+			vio_lookup_rc(viocd_err_table, we.subtypeRc);
+		printk(KERN_WARNING_VIO "bad rc %d:0x%04X on open: %s\n",
+				we.rc, we.subtypeRc, err->msg);
+		return -err->errno;
+	}
+
+	if (diskinfo->useCount == 0) {
+		if (diskinfo->blocksize > 0) {
+			blk_queue_hardsect_size(viocd_queue,
+					diskinfo->blocksize);
+			set_capacity(diskinfo->viocd_disk,
+					diskinfo->mediasize *
+					diskinfo->blocksize / 512);
+		} else
+			set_capacity(diskinfo->viocd_disk, 0xFFFFFFFFFFFFFFFF);
+	}
+	return 0;
+}
+
+static void viocd_release(struct cdrom_device_info *cdi)
+{
+	int device_no = DEVICE_NR((struct disk_info *)cdi->handle);
+	HvLpEvent_Rc hvrc;
+
+	/* If we don't have a host, bail out */
+	if ((viopath_hostLp == HvLpIndexInvalid) ||
+			(device_no >= viocd_numdev))
+		return;
+
+	hvrc = signalLpEventFast(viopath_hostLp, HvLpEvent_Type_VirtualIo,
+			viomajorsubtype_cdio | viocdclose,
+			HvLpEvent_AckInd_NoAck, HvLpEvent_AckType_ImmediateAck,
+			viopath_sourceinst(viopath_hostLp),
+			viopath_targetinst(viopath_hostLp), 0,
+			VIOVERSION << 16, ((u64)device_no << 48), 0, 0, 0);
+	if (hvrc != 0)
+		printk(KERN_WARNING_VIO "bad rc on signalLpEventFast %d\n",
+				(int)hvrc);
+}
+
+/* Send a read or write request to OS/400 */
+static int send_request(struct request *req)
+{
+	HvLpEvent_Rc hvrc;
+	struct disk_info *diskinfo = req->rq_disk->private_data;
+	int device_no = DEVICE_NR(diskinfo);
+	u64 start = req->sector * 512;
+	u64 len;
+	int reading = rq_data_dir(req) == READ;
+	int dir = reading ? DMA_FROM_DEVICE : DMA_TO_DEVICE;
+	u16 command = viomajorsubtype_cdio | (reading ? viocdread : viocdwrite);
+	dma_addr_t dmaaddr;
+	int nsg, ntce;
+	struct scatterlist sg[30];
+
+	if ((req->sector + req->nr_sectors) >
+			get_capacity(diskinfo->viocd_disk)) {
+		printk(KERN_WARNING_VIO
+				"viocd%d; access position %lx, past size %lx\n",
+				device_no, req->sector + req->nr_sectors,
+				get_capacity(diskinfo->viocd_disk));
+		return -1;
+	}
+
+        nsg = blk_rq_map_sg(req->q, req, sg);
+	if (!nsg) {
+		printk(KERN_WARNING_VIO
+				"error setting up scatter/gather list\n");
+		return -1;
+	}
+
+	ntce = dma_map_sg(iSeries_vio_dev, sg, nsg, dir);
+	if (!ntce) {
+		printk(KERN_WARNING_VIO "error allocating sg tces\n");
+		return -1;
+	}
+	dmaaddr = sg[0].dma_address;
+	len = sg[0].dma_length;
+	if (ntce > 1) {
+		printk("viocd: unmapping %d extra sg entries\n", ntce - 1);
+		dma_unmap_sg(iSeries_vio_dev, &sg[1], ntce - 1, dir);
+	}
+	if (dmaaddr == 0xFFFFFFFF) {
+		printk(KERN_WARNING_VIO
+				"error allocating tce for address %p len %ld\n",
+				req->buffer, len);
+		return -1;
+	}
+
+	hvrc = signalLpEventFast(viopath_hostLp, HvLpEvent_Type_VirtualIo,
+			command, HvLpEvent_AckInd_DoAck,
+			HvLpEvent_AckType_ImmediateAck,
+			viopath_sourceinst(viopath_hostLp),
+			viopath_targetinst(viopath_hostLp),
+			(u64)req, VIOVERSION << 16,
+			((u64)device_no << 48) | dmaaddr, start, len, 0);
+	if (hvrc != HvLpEvent_Rc_Good) {
+		printk(KERN_WARNING_VIO "hv error on op %d\n", (int)hvrc);
+		return -1;
+	}
+
+	return 0;
+}
+
+
+static int rwreq;
+
+static void do_viocd_request(request_queue_t *q)
+{
+	for (; rwreq < MAX_CD_REQ;) {
+		struct request *req;
+		int device_no;
+
+		if ((req = elv_next_request(q)) == NULL)
+			return;
+		/* remove the current request from the queue */
+		blkdev_dequeue_request(req);
+
+		/* check for any kind of error */
+		device_no = DEVICE_NR((struct disk_info *)req->rq_disk->private_data);
+		if (device_no > viocd_numdev) {
+			printk(KERN_WARNING_VIO "Invalid device number %d",
+					device_no);
+			viocd_end_request(req, 0, 0);
+		} else if (send_request(req) < 0) {
+			printk(KERN_WARNING_VIO
+					"unable to send message to OS/400!");
+			viocd_end_request(req, 0, 0);
+		} else {
+			spin_lock(&viocd_lock);
+			++rwreq;
+			spin_unlock(&viocd_lock);
+		}
+	}
+}
+
+static int viocd_media_changed(struct cdrom_device_info *cdi, int disc_nr)
+{
+	struct viocd_waitevent we;
+	HvLpEvent_Rc hvrc;
+	int device_no = DEVICE_NR((struct disk_info *)cdi->handle);
+
+	/* This semaphore is raised in the interrupt handler */
+	DECLARE_MUTEX_LOCKED(Semaphore);
+
+	/* Check that we are dealing with a valid hosting partition */
+	if (viopath_hostLp == HvLpIndexInvalid) {
+		printk(KERN_WARNING_VIO "Invalid hosting partition\n");
+		return -EIO;
+	}
+
+	we.sem = &Semaphore;
+
+	/* Send the open event to OS/400 */
+	hvrc = signalLpEventFast(viopath_hostLp, HvLpEvent_Type_VirtualIo,
+			viomajorsubtype_cdio | viocdcheck,
+			HvLpEvent_AckInd_DoAck, HvLpEvent_AckType_ImmediateAck,
+			viopath_sourceinst(viopath_hostLp),
+			viopath_targetinst(viopath_hostLp),
+			(u64)&we, VIOVERSION << 16, ((u64)device_no << 48),
+			0, 0, 0);
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO "bad rc on signalLpEventFast %d\n",
+				(int)hvrc);
+		return -EIO;
+	}
+
+	/* Wait for the interrupt handler to get the response */
+	down(&Semaphore);
+
+	/* Check the return code.  If bad, assume no change */
+	if (we.rc) {
+		const struct vio_error_entry *err =
+			vio_lookup_rc(viocd_err_table, we.subtypeRc);
+		printk(KERN_WARNING_VIO
+				"bad rc %d:0x%04X on check_change: %s; Assuming no change\n",
+				we.rc, we.subtypeRc, err->msg);
+		return 0;
+	}
+
+	return we.changed;
+}
+
+static int viocd_lock_door(struct cdrom_device_info *cdi, int locking)
+{
+	HvLpEvent_Rc hvrc;
+	u64 device_no = DEVICE_NR((struct disk_info *)cdi->handle);
+	/* NOTE: flags is 1 or 0 so it won't overwrite the device_no */
+	u64 flags = !!locking;
+	/* This semaphore is raised in the interrupt handler */
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	struct viocd_waitevent we = { .sem = &Semaphore };
+
+	/* Check that we are dealing with a valid hosting partition */
+	if (viopath_hostLp == HvLpIndexInvalid) {
+		printk(KERN_WARNING_VIO "Invalid hosting partition\n");
+		return -EIO;
+	}
+
+	we.sem = &Semaphore;
+
+	/* Send the lockdoor event to OS/400 */
+	hvrc = signalLpEventFast(viopath_hostLp, HvLpEvent_Type_VirtualIo,
+			viomajorsubtype_cdio | viocdlockdoor,
+			HvLpEvent_AckInd_DoAck, HvLpEvent_AckType_ImmediateAck,
+			viopath_sourceinst(viopath_hostLp),
+			viopath_targetinst(viopath_hostLp),
+			(u64)&we, VIOVERSION << 16,
+			(device_no << 48) | (flags << 32), 0, 0, 0);
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO "bad rc on signalLpEventFast %d\n",
+				(int)hvrc);
+		return -EIO;
+	}
+
+	/* Wait for the interrupt handler to get the response */
+	down(&Semaphore);
+
+	if (we.rc != 0)
+		return -EIO;
+	return 0;
+}
+
+/* This routine handles incoming CD LP events */
+static void vioHandleCDEvent(struct HvLpEvent *event)
+{
+	struct viocdlpevent *bevent = (struct viocdlpevent *)event;
+	struct viocd_waitevent *pwe;
+
+	if (event == NULL)
+		/* Notification that a partition went away! */
+		return;
+	/* First, we should NEVER get an int here...only acks */
+	if (event->xFlags.xFunction == HvLpEvent_Function_Int) {
+		printk(KERN_WARNING_VIO
+				"Yikes! got an int in viocd event handler!\n");
+		if (event->xFlags.xAckInd == HvLpEvent_AckInd_DoAck) {
+			event->xRc = HvLpEvent_Rc_InvalidSubtype;
+			HvCallEvent_ackLpEvent(event);
+		}
+	}
+
+	switch (event->xSubtype & VIOMINOR_SUBTYPE_MASK) {
+	case viocdopen:
+printk(KERN_INFO "viocdopen event: size %lu blocks %u mediasize %u\n", bevent->mLen, bevent->mBlockSize, bevent->mMediaSize);
+		viocd_diskinfo[bevent->mDisk].blocksize = bevent->mBlockSize;
+		viocd_diskinfo[bevent->mDisk].mediasize = bevent->mMediaSize;
+		/* FALLTHROUGH !! */
+	case viocdgetinfo:
+	case viocdlockdoor:
+		pwe = (struct viocd_waitevent *)event->xCorrelationToken;
+		pwe->rc = event->xRc;
+		pwe->subtypeRc = bevent->mSubTypeRc;
+		up(pwe->sem);
+		break;
+
+	case viocdclose:
+		break;
+
+	case viocdwrite:
+	case viocdread:
+	{
+		unsigned long flags;
+		int dir =
+			((event->xSubtype & VIOMINOR_SUBTYPE_MASK) == viocdread)
+			? DMA_FROM_DEVICE : DMA_TO_DEVICE;
+		struct request *req;
+
+		/*
+		 * Since this is running in interrupt mode, we need to
+		 * make sure we're not stepping on any global I/O operations
+		 */
+		spin_lock_irqsave(&viocd_reqlock, flags);
+		dma_unmap_single(iSeries_vio_dev, bevent->mToken, bevent->mLen,
+				dir);
+		req = (struct request *)bevent->event.xCorrelationToken;
+		spin_lock(&viocd_lock);
+		--rwreq;
+		spin_unlock(&viocd_lock);
+
+		if (event->xRc != HvLpEvent_Rc_Good) {
+			const struct vio_error_entry *err =
+				vio_lookup_rc(viocd_err_table,
+						bevent->mSubTypeRc);
+			printk(KERN_WARNING_VIO
+					"request %p failed with rc %d:0x%04X: %s\n",
+					req, event->xRc,
+					bevent->mSubTypeRc, err->msg);
+			viocd_end_request(req, 0, 0);
+		} else
+			viocd_end_request(req, 1, bevent->mLen >> 9);
+
+		/* restart handling of incoming requests */
+		spin_unlock_irqrestore(&viocd_reqlock, flags);
+		blk_run_queue(viocd_queue);
+		break;
+	}
+	case viocdcheck:
+		pwe = (struct viocd_waitevent *)event->xCorrelationToken;
+		pwe->rc = event->xRc;
+		pwe->subtypeRc = bevent->mSubTypeRc;
+		pwe->changed = bevent->mFlags;
+		up(pwe->sem);
+		break;
+
+	default:
+		printk(KERN_WARNING_VIO
+				"message with invalid subtype %0x04X!\n",
+				event->xSubtype & VIOMINOR_SUBTYPE_MASK);
+		if (event->xFlags.xAckInd == HvLpEvent_AckInd_DoAck) {
+			event->xRc = HvLpEvent_Rc_InvalidSubtype;
+			HvCallEvent_ackLpEvent(event);
+		}
+	}
+}
+
+static struct cdrom_device_ops viocd_dops = {
+	.open = viocd_open,
+	.release = viocd_release,
+	.media_changed = viocd_media_changed,
+	.lock_door = viocd_lock_door,
+	.capability = CDC_CLOSE_TRAY | CDC_OPEN_TRAY | CDC_LOCK | CDC_SELECT_SPEED | CDC_SELECT_DISC | CDC_MULTI_SESSION | CDC_MCN | CDC_MEDIA_CHANGED | CDC_PLAY_AUDIO | CDC_RESET | CDC_IOCTLS | CDC_DRIVE_STATUS | CDC_GENERIC_PACKET | CDC_CD_R | CDC_CD_RW | CDC_DVD | CDC_DVD_R | CDC_DVD_RAM
+};
+
+static int proc_read(char *buf, char **start, off_t offset, int blen,
+		int *eof, void *data)
+{
+	int len = 0;
+	int i;
+
+	for (i = 0; i < viocd_numdev; i++) {
+		len += sprintf(buf + len,
+				"viocd device %d is iSeries resource %10.10s type %4.4s, model %3.3s\n",
+				i, viocd_unitinfo[i].rsrcname,
+				viocd_unitinfo[i].type,
+				viocd_unitinfo[i].model);
+	}
+	*eof = 1;
+	return len;
+}
+
+
+static void viocd_proc_init(struct proc_dir_entry *iSeries_proc)
+{
+	struct proc_dir_entry *ent;
+	ent = create_proc_entry("viocd", S_IFREG | S_IRUSR, iSeries_proc);
+	if (!ent)
+		return;
+	ent->nlink = 1;
+	ent->data = NULL;
+	ent->read_proc = proc_read;
+}
+
+static void viocd_proc_delete(struct proc_dir_entry *iSeries_proc)
+{
+	remove_proc_entry("viocd", iSeries_proc);
+}
+
+static int __init find_capability(const char *type)
+{
+	struct capability_entry *entry;
+
+	for(entry = capability_table; entry->type; ++entry)
+		if(!strncmp(entry->type, type, 4))
+			break;
+	return entry->capability;
+}
+
+static int __init viocd_init(void)
+{
+	struct gendisk *gendisk;
+	int deviceno, rc;
+	int ret = 0;
+
+	if (viopath_hostLp == HvLpIndexInvalid)
+		vio_set_hostlp();
+
+	/* If we don't have a host, bail out */
+	if (viopath_hostLp == HvLpIndexInvalid)
+		return -ENODEV;
+
+	rc = viopath_open(viopath_hostLp, viomajorsubtype_cdio, MAX_CD_REQ + 2);
+	if (rc) {
+		printk(KERN_WARNING_VIO
+				"error opening path to host partition %d\n",
+				viopath_hostLp);
+		return rc;
+	}
+
+	/* Initialize our request handler */
+	rwreq = 0;
+	vio_setHandler(viomajorsubtype_cdio, vioHandleCDEvent);
+
+	get_viocd_info();
+	if (viocd_numdev == 0)
+		goto out_undo_vio;
+
+	printk(KERN_INFO_VIO
+			"%s: iSeries Virtual CD vers %s, major %d, max disks %d, hosting partition %d\n",
+			DEVICE_NAME, VIOCD_VERS, MAJOR_NR, VIOCD_MAX_CD,
+			viopath_hostLp);
+
+	ret = -EIO;
+	if (register_blkdev(MAJOR_NR, "viocd") != 0) {
+		printk(KERN_WARNING_VIO
+				"Unable to get major %d for viocd CD-ROM\n",
+				MAJOR_NR);
+		goto out_undo_vio;
+	}
+
+	ret = -ENOMEM;
+	spin_lock_init(&viocd_reqlock);
+	viocd_queue = blk_init_queue(do_viocd_request, &viocd_reqlock);
+	if (viocd_queue == NULL)
+		goto out_unregister;
+	blk_queue_max_hw_segments(viocd_queue, 1);
+
+	/* initialize units */
+	for (deviceno = 0; deviceno < viocd_numdev; deviceno++) {
+		VIOCDI.ops = &viocd_dops;
+		VIOCDI.speed = 4;
+		VIOCDI.capacity = 1;
+		VIOCDI.handle = &viocd_diskinfo[deviceno];
+		VIOCDI.mask = ~find_capability(viocd_unitinfo[deviceno].type);
+		sprintf(VIOCDI.name, VIOCD_DEVICE,
+				VIOCD_DEVICE_OFFSET + deviceno);
+
+		if (register_cdrom(&VIOCDI) != 0) {
+			printk(KERN_WARNING_VIO
+					"Cannot register viocd CD-ROM %s!\n",
+					VIOCDI.name);
+			continue;
+		}
+		printk(KERN_INFO_VIO
+				"cd %s is iSeries resource %10.10s type %4.4s, model %3.3s\n",
+				VIOCDI.name, viocd_unitinfo[deviceno].rsrcname,
+				viocd_unitinfo[deviceno].type,
+				viocd_unitinfo[deviceno].model);
+		if ((gendisk = alloc_disk(1)) == NULL) {
+			printk(KERN_WARNING_VIO
+					"Cannot create gendisk for %s!\n",
+					VIOCDI.name);
+			unregister_cdrom(&VIOCDI);
+			continue;
+		}
+		gendisk->major = MAJOR_NR;
+		gendisk->first_minor = deviceno;
+		strncpy(gendisk->disk_name, VIOCDI.name, 16);
+		sprintf(gendisk->devfs_name, VIOCD_DEVICE_DEVFS,
+				VIOCD_DEVICE_OFFSET_DEVFS + deviceno);
+		gendisk->queue = viocd_queue;
+		gendisk->fops = &viocd_fops;
+		gendisk->flags = GENHD_FL_CD;
+		set_capacity(gendisk, 0);
+		gendisk->private_data = &viocd_diskinfo[deviceno];
+		add_disk(gendisk);
+		viocd_diskinfo[deviceno].viocd_disk = gendisk;
+	}
+
+	iSeries_proc_callback(&viocd_proc_init);
+
+	return 0;
+
+out_unregister:
+	unregister_blkdev(MAJOR_NR, "viocd");
+out_undo_vio:
+	vio_clearHandler(viomajorsubtype_cdio);
+	viopath_close(viopath_hostLp, viomajorsubtype_cdio, MAX_CD_REQ + 2);
+	return ret;
+}
+
+static void __exit viocd_exit(void)
+{
+	int deviceno;
+
+	for (deviceno = 0; deviceno < viocd_numdev; deviceno++) {
+		if (unregister_cdrom(&VIOCDI) != 0)
+			printk(KERN_WARNING_VIO
+					"Cannot unregister viocd CD-ROM %s!\n",
+					VIOCDI.name);
+		del_gendisk(viocd_diskinfo[deviceno].viocd_disk);
+		put_disk(viocd_diskinfo[deviceno].viocd_disk);
+		kfree(viocd_diskinfo[deviceno].viocd_disk);
+	}
+	if ((unregister_blkdev(MAJOR_NR, "viocd") == -EINVAL)) {
+		printk(KERN_WARNING_VIO "can't unregister viocd\n");
+		return;
+	}
+	blk_cleanup_queue(viocd_queue);
+
+	iSeries_proc_callback(&viocd_proc_delete);
+
+	viopath_close(viopath_hostLp, viomajorsubtype_cdio, MAX_CD_REQ+2);
+	vio_clearHandler(viomajorsubtype_cdio);
+}
+
+module_init(viocd_init);
+module_exit(viocd_exit);
+MODULE_LICENSE("GPL");
diff -purN linux-2.5/drivers/char/Kconfig linuxppc64-2.5/drivers/char/Kconfig
--- linux-2.5/drivers/char/Kconfig	2004-02-23 05:24:06.000000000 +0000
+++ linuxppc64-2.5/drivers/char/Kconfig	2004-02-27 21:16:22.000000000 +0000
@@ -588,6 +588,20 @@ config HVC_CONSOLE
 	  console. This driver allows each pSeries partition to have a console
 	  which is accessed via the HMC.
 
+config HVCS
+	tristate "IBM Hypervisor Virtual Console Server support"
+	depends on PPC_PSERIES
+	help
+	  Partitionable IBM Power5 ppc64 machines allow hosting of
+	  firmware virtual consoles from one Linux partition by
+	  another Linux partition.  This driver allows console data
+	  from Linux partitions to be accessed through TTY device
+	  interfaces in the device tree on a Linux partition running
+	  this driver.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called hvcs.ko.
+
 config PC9800_OLDLP
 	tristate "NEC PC-9800 old-style printer port support"
 	depends on X86_PC9800 && !PARPORT
diff -purN linux-2.5/drivers/char/Makefile linuxppc64-2.5/drivers/char/Makefile
--- linux-2.5/drivers/char/Makefile	2004-02-05 05:20:40.000000000 +0000
+++ linuxppc64-2.5/drivers/char/Makefile	2004-02-27 21:16:22.000000000 +0000
@@ -43,7 +43,8 @@ obj-$(CONFIG_SH_SCI)		+= sh-sci.o generi
 obj-$(CONFIG_HVC_CONSOLE)	+= hvc_console.o
 obj-$(CONFIG_RAW_DRIVER)	+= raw.o
 obj-$(CONFIG_SGI_L1_SERIAL)	+= sn_serial.o
-obj-$(CONFIG_VIOCONS) += viocons.o
+obj-$(CONFIG_VIOCONS)		+= viocons.o
+obj-$(CONFIG_HVCS)		+= hvcs.o
 
 obj-$(CONFIG_PRINTER) += lp.o
 obj-$(CONFIG_TIPAR) += tipar.o
diff -purN linux-2.5/drivers/char/hvc_console.c linuxppc64-2.5/drivers/char/hvc_console.c
--- linux-2.5/drivers/char/hvc_console.c	2004-02-05 21:11:06.000000000 +0000
+++ linuxppc64-2.5/drivers/char/hvc_console.c	2004-02-17 20:54:24.000000000 +0000
@@ -27,23 +27,18 @@
 #include <linux/tty_flip.h>
 #include <linux/sched.h>
 #include <linux/kbd_kern.h>
-#include <asm/uaccess.h>
 #include <linux/spinlock.h>
 #include <linux/cpumask.h>
-
-extern int hvc_count(int *);
-extern int hvc_get_chars(int index, char *buf, int count);
-extern int hvc_put_chars(int index, const char *buf, int count);
+#include <asm/uaccess.h>
+#include <asm/hvconsole.h>
 
 #define HVC_MAJOR	229
 #define HVC_MINOR	0
 
-#define MAX_NR_HVC_CONSOLES	4
-
 #define TIMEOUT		((HZ + 99) / 100)
 
 static struct tty_driver *hvc_driver;
-static int hvc_offset;
+static int count;
 #ifdef CONFIG_MAGIC_SYSRQ
 static int sysrq_pressed;
 #endif
@@ -52,6 +47,10 @@ static int sysrq_pressed;
 
 #define __ALIGNED__	__attribute__((__aligned__(8)))
 
+/* This driver speaks only in "indexes", i.e. logical consoles starting at 0.
+ * The ppc64 backend converts those indexes (e.g. hvc0) to whatever the
+ * ultimate "vterm number" that the platform understands. */
+
 struct hvc_struct {
 	spinlock_t lock;
 	int index;
@@ -112,7 +111,7 @@ static void hvc_push(struct hvc_struct *
 {
 	int n;
 
-	n = hvc_put_chars(hp->index + hvc_offset, hp->outbuf, hp->n_outbuf);
+	n = hvc_put_chars(hp->index, hp->outbuf, hp->n_outbuf);
 	if (n <= 0) {
 		if (n == 0)
 			return;
@@ -192,7 +191,7 @@ static void hvc_poll(int index)
 		for (;;) {
 			if (TTY_FLIPBUF_SIZE - tty->flip.count < sizeof(buf))
 				break;
-			n = hvc_get_chars(index + hvc_offset, buf, sizeof(buf));
+			n = hvc_get_chars(index, buf, sizeof(buf));
 			if (n <= 0)
 				break;
 			for (i = 0; i < n; ++i) {
@@ -258,13 +257,7 @@ static struct tty_operations hvc_ops = {
 
 int __init hvc_init(void)
 {
-	int num = hvc_count(&hvc_offset);
-	int i;
-
-	if (num > MAX_NR_HVC_CONSOLES)
-		num = MAX_NR_HVC_CONSOLES;
-
-	hvc_driver = alloc_tty_driver(num);
+	hvc_driver = alloc_tty_driver(count);
 	if (!hvc_driver)
 		return -ENOMEM;
 
@@ -278,23 +271,20 @@ int __init hvc_init(void)
 	hvc_driver->init_termios = tty_std_termios;
 	hvc_driver->flags = TTY_DRIVER_REAL_RAW;
 	tty_set_operations(hvc_driver, &hvc_ops);
-	for (i = 0; i < num; i++) {
-		hvc_struct[i].lock = SPIN_LOCK_UNLOCKED;
-		hvc_struct[i].index = i;
-	}
 
 	if (tty_register_driver(hvc_driver))
 		panic("Couldn't register hvc console driver\n");
 
-	if (num > 0)
+	if (count > 0)
 		kernel_thread(khvcd, NULL, CLONE_KERNEL);
+	else
+		printk(KERN_WARNING "no virtual consoles found\n");
 
 	return 0;
 }
+device_initcall(hvc_init);
 
-static void __exit hvc_exit(void)
-{
-}
+/***** console (not tty) code: *****/
 
 void hvc_console_print(struct console *co, const char *b, unsigned count)
 {
@@ -314,7 +304,7 @@ void hvc_console_print(struct console *c
 				--count;
 			}
 		} else {
-			r = hvc_put_chars(co->index + hvc_offset, c, i);
+			r = hvc_put_chars(co->index, c, i);
 			if (r < 0) {
 				/* throw away chars on error */
 				i = 0;
@@ -336,7 +326,7 @@ static struct tty_driver *hvc_console_de
 static int __init hvc_console_setup(struct console *co, char *options)
 {
 	if (co->index < 0 || co->index >= MAX_NR_HVC_CONSOLES
-	    || co->index >= hvc_count(&hvc_offset))
+	    || co->index >= count)
 		return -1;
 	return 0;
 }
@@ -350,12 +340,27 @@ struct console hvc_con_driver = {
 	.index		= -1,
 };
 
+/* hvc_instantiate - called once per discovered vterm by hvc_find_vterms */
+int hvc_instantiate(void)
+{
+	struct hvc_struct *hvc;
+
+	if (count >= MAX_NR_HVC_CONSOLES)
+		return -1;
+
+	hvc = &hvc_struct[count];
+	hvc->lock = SPIN_LOCK_UNLOCKED;
+	hvc->index = count;
+
+	count++;
+
+	return 0;
+}
+
 static int __init hvc_console_init(void)
 {
+	hvc_find_vterms(); /* populate hvc_struct[] early */
 	register_console(&hvc_con_driver);
 	return 0;
 }
 console_initcall(hvc_console_init);
-
-module_init(hvc_init);
-module_exit(hvc_exit);
diff -purN linux-2.5/drivers/char/hvcs.c linuxppc64-2.5/drivers/char/hvcs.c
--- linux-2.5/drivers/char/hvcs.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/char/hvcs.c	2004-02-27 21:16:22.000000000 +0000
@@ -0,0 +1,1106 @@
+/*
+ * IBM eServer Hypervisor Virtual Console Server Device Driver
+ * Copyright (C) 2003, 2004 IBM Corp.
+ *  Ryan S. Arnold (rsa@us.ibm.com)
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
+ *
+ *
+ * Author(s) :  Ryan S. Arnold <rsa@us.ibm.com>
+ *
+ * This is the device driver for the IBM Hypervisor Virtual Console
+ * Server, "hvcs".  The IBM hvcs provides a TTY interface to allow
+ * Linux user space applications access to the system consoles of
+ * partitioned RPA supported operating systems (Linux and AIX)
+ * running on the same partitioned IBM POWER architecture eServer.
+ * Physical hardware consoles per partition do not exist on these
+ * platforms and system consoles are interacted with through
+ * firmware interfaces utilized by this driver.
+ *
+ * This driver's first responsiblity is for registering itself as the
+ * device driver for the vio bus's firmware defined vty-server@
+ * vdevices.  These devices are virtual terminal devices that are
+ * created and configured by the "Super Admin Authority" using the
+ * IBM Hardware Management Console (which views them as "VSerial
+ * Client Adapters").  A vty-server@ vdevice is architected to allow
+ * "partner" connections to one or more target vty-client vdevices.
+ * Firmware reports these devices as vty@ vdevices ("VSerial Server
+ * Adapters" to the HMC).  The initial release of this driver only
+ * supports a single configured partner connection between a
+ * vty-server@ vdevice and a vty@ vdevice due to current firmware
+ * function limitations.
+ *
+ * Vty@ vdevices are firmware defined virtual terminal devices
+ * that console device drivers recognize as the origin and
+ * destination device of an RPA supported OS's console input and
+ * output.  A vty@ vdevice can be connected to as an output source
+ * and input target by a number of vty-server@ vdevices on the
+ * eServer, such as the Hardware Management Console, a serial console,
+ * and this console server (executing on the same or another Linux
+ * partition).
+ *
+ * Firmware is tasked with exposing vty-server@ and vty@ adapters
+ * to the POWER Linux virtual I/O bus.  On the backend (not user
+ * accessible) this driver is implemented as a vio device driver so
+ * that it can receive notification of vty-server@ vdevice lifetimes
+ * from the vio bus after it registers to handle vty-server@ lifetime
+ * notifications (.probe and .remove).
+ *
+ * On the front end this driver is implemented as a tty driver to
+ * allow it to interact with user space applications via the linux
+ * kernel's tty interface.  A block of major and minor numbers for the
+ * vty-server@ vdevices are generated dynamically by the TTY layer
+ * when this driver is registered as a tty driver.  From this the tty
+ * layer does the work of providing the major and minor numbers to the
+ * user via sysfs attributes.
+ *
+ * According to specification a vty@, while configurably many-to-one,
+ * may only be actively connected to a single vty-server@ at one time.
+ * This means that if the IBM Hardware Management Console is currently
+ * hosting the console for a target Linux or AIX partition then
+ * attempting to open the TTY device to that partition's console
+ * using the hvcs will return -EBUSY with every open attempt until
+ * the IBM Hardware Management Console frees the connection between
+ * its vty-server@ vdevice and the desired partition's vty@ vdevice.
+ *
+ * Similarily, only one vty-server@ adapter on the eServer may have a
+ * connection to a target RPA partition vty@ vdevice at one time.
+ * Conversely, a vty-server@ vdevice may only be connected to a single
+ * vty@ vdevice at one time even though it may have several configured
+ * vty@ partner possibilities.
+ *
+ * Firmware does not provide notification of partner changes to this
+ * driver.  This means that an eServer admin may add or remove
+ * partner vty@ vdevices from a vty-server@ vdevice's partner list
+ * and the changes will not be signaled to the vty-server@ vdevice.
+ * Firmware only notifies the driver when a vty-server@ vdevice is
+ * added to or removed from the system.
+ *
+ * Terminology:
+ *
+ * "vty-server@" and "vty@" are firmware defined device naming
+ * conventions which appear in a Linux device tree concatenated with
+ * a per partition unique unit address.
+ *
+ * "partner" describes a configured vty@ vdevice in respect to the
+ * vty-server@ vdevice that can connect to it.  A vty@ vdevice can be
+ * a partner to more than one vty-server@ vdevice.
+ *
+ * This driver was originally written to cooperate with the 2.6 Linux
+ * kernel as a kernel driver module.  If you notice that this driver
+ * utilizes deprecated techniques please notify the author(s).
+ *
+ */
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/major.h>
+#include <linux/kernel.h>
+#include <linux/moduleparam.h>
+#include <linux/interrupt.h>
+#include <linux/spinlock.h>
+#include <asm/uaccess.h>
+#include <linux/stat.h>
+#include <linux/kobject.h>
+#include <linux/sched.h>
+#include <linux/tty.h>
+#include <linux/tty_flip.h>
+#include <linux/device.h>
+#include <asm/vio.h>
+#include <linux/list.h>
+#include <asm/hvconsole.h>
+
+MODULE_AUTHOR("Ryan S. Arnold <rsa@us.ibm.com>");
+MODULE_DESCRIPTION("IBM hvcs (Hypervisor Virtual Console Server) Driver");
+MODULE_LICENSE("GPL");
+
+/* TODO:
+ * Do I need to grab a kobject ref to the hvcs_struct each time
+ * I use the hvcs_struct instance?
+ *
+ * How many of the function forward declarations are really
+ * required?
+ *
+ * Remove dev_node sysfs entry once I figure out how to inform the
+ * user about what the device node entry is that cooresponds to a
+ * vty-server@3000000* entry.
+ *
+ * Investigate issue surrounding hvcs_throttle() and
+ * hvcs_unthrottle() callbacks and apparent dataloss.  This may
+ * require fixes to hvc_console to determine who (this driver or
+ * hvc_console) is actually dropping data.
+ *
+ * Write Documentation/hvcs.txt help file for this driver.
+ *
+ * Develop test plan.
+ *
+ */
+
+/* Since the Linux TTY code does not currently (2-04-2004) support
+ * dynamic addition of tty derived devices and we shouldn't
+ * allocate thousands of tty_device pointers when the number of
+ * vty-server@ & vty@ partner connections will most often be much
+ * lower than this, we'll arbitrarily allocate HVCS_DEFAULT_SERVER_ADAPTERS
+ * tty_structs and cdev's by default when we register the tty_driver.
+ * This can be overridden using an insmod parameter.
+ */
+#define HVCS_DEFAULT_SERVER_ADAPTERS	64
+
+/* The user can't specify with insmod more than HVCS_MAX_SERVER_ADAPTERS
+ * hvcs device nodes as a sanity check.  Theoretically there can be
+ * over 1 Billion vty-server@ & vty@ partner vdevice connections.
+ */
+#define HVCS_MAX_SERVER_ADAPTERS	1024
+
+/* We let Linux assign us a Major number and we start the minors at
+ * ZERO.  There is no mapping between minor number and the target
+ * partition.  The mapping of minor number is related to the order
+ * the vty-server@ vdevices are exposed to this driver via the
+ * hvcs_probe function.
+ */
+#define HVCS_MINOR_START	0
+
+#define __ALIGNED__	__attribute__((__aligned__(8)))
+
+/* Converged location code string length + 1 null terminator */
+#define CLC_LENGTH		80
+
+/* How much data can firmware send with each hvterm_put_chars()?
+ * Maybe this should be moved into an architecture specific area. */
+#define HVCS_BUFF_LEN	16
+
+#define HVCS_MAX_FROM_USER	4096
+
+/* This value is used to take the place of a command line parameter
+ * when the module is inserted.  It starts as -1 and stays as such if
+ * the user doesn't specify a module insmod parameter.  If they DO
+ * specify one then it is set to the value of the integer passed in.
+ */
+static int hvcs_parm_num_devs = -1;
+module_param(hvcs_parm_num_devs, int, 0);
+
+static const char hvcs_driver_name[] = "hvcs";
+static const char hvcs_device_node[] = "hvcs";
+static const char hvcs_driver_string[]
+	= "IBM hvcs (Hypervisor Virtual Console Server) Driver";
+
+/* Status of partner info rescan triggered via sysfs. */
+static int hvcs_rescan_status = 0;
+
+static struct tty_driver *hvcs_tty_driver;
+
+/* This is used to associate a vty-server@ vdevice as it is exposed to
+ * the driver with a preallocated tty_struct.index.  The dev node and
+ * hvcs index numbers are not re-used after device removal otherwise
+ * removing vdevices and adding a new one would link a /dev/hvcs*
+ * entry to a different vty-server@ vdevice than it did before the
+ * removal.  This means that a newly exposed vty-server@ vdevice will
+ * always map to an incrementally higher /dev/hvcs* entry than last
+ * exposed vty-server@ vdevice.
+ */
+static int hvcs_struct_count = -1;
+
+/* One vty-server@ vdevice per hvcs_struct */
+struct hvcs_struct {
+	struct list_head next; /* list management */
+	struct vio_dev *vdev;
+	struct tty_struct *tty;
+	unsigned int open_count;
+	/* this index identifies this hvcs device as the
+	 * complement to a tty index. */
+	unsigned int index;
+	unsigned int p_unit_address; /* partner unit address */
+	unsigned int p_partition_ID; /* partner partition ID */
+	char p_location_code[CLC_LENGTH];
+	char name[32];
+	int enabled; /* there are tty's open against this device */
+	struct kobject kobj; /* ref count & hvcs_struct lifetime */
+	struct work_struct read_work;
+};
+
+/* Require to back map a kobject to its containing object */
+#define from_kobj(kobj) container_of(kobj, struct hvcs_struct, kobj)
+
+static struct list_head hvcs_structs = LIST_HEAD_INIT(hvcs_structs);
+
+/* Be careful when adding flags to this line discipline.  Don't add
+ * anything that will cause the line discipline to put this driver
+ * into canonical mode.  We can't go into canonical mode because we
+ * need to be able to get THROTTLE and UNTHROTTLE callbacks.
+ */
+struct termios hvcs_tty_termios = {
+	.c_iflag = IGNBRK | IGNPAR,
+	/* Enable implementation-defined output processing. */
+	/* Map NL to CR-NL on output */
+	/*.c_oflag = OPOST, ONLCR,
+	.c_iflag = ICRNL, */
+	.c_cflag = B38400 | CS8 | CREAD | HUPCL,
+	/* NOTICE NO ECHO or we'll go into recursive loop echoing chars
+	 * back and forth with the console drivers. */
+	.c_cc = INIT_C_CC
+};
+
+static void hvcs_read_task(unsigned long data);
+static void hvcs_unthrottle(struct tty_struct *tty);
+static void hvcs_throttle(struct tty_struct *tty);
+static irqreturn_t hvcs_handle_interrupt(int irq, void *dev_instance, struct pt_regs *regs);
+
+static int hvcs_write(struct tty_struct *tty, int from_user, const unsigned char *buf, int count);
+static int hvcs_write_room(struct tty_struct *tty);
+static int hvcs_chars_in_buffer(struct tty_struct *tty);
+
+static int hvcs_has_pi(struct hvcs_struct *hvcsd);
+static void hvcs_set_pi(struct hvcs_partner_info *pi, struct hvcs_struct *hvcsd);
+static int hvcs_get_pi(struct hvcs_struct *hvcsd);
+static int hvcs_rescan_devices_list(void);
+
+static int hvcs_partner_connect(struct hvcs_struct *hvcsd);
+static void hvcs_partner_free(struct hvcs_struct *hvcsd);
+
+static int hvcs_enable_device(struct hvcs_struct *hvcsd);
+static void hvcs_disable_device(struct hvcs_struct *hvcsd);
+
+static void destroy_hvcs_struct(struct kobject *kobj);
+static int hvcs_open(struct tty_struct *tty, struct file *filp);
+static void hvcs_close(struct tty_struct *tty, struct file *filp);
+static void hvcs_hangup(struct tty_struct * tty);
+
+static void hvcs_create_device_attrs(struct hvcs_struct *hvcsd);
+static void hvcs_remove_device_attrs(struct hvcs_struct *hvcsd);
+static void hvcs_create_driver_attrs(void);
+static void hvcs_remove_driver_attrs(void);
+
+static int __devinit hvcs_probe(struct vio_dev *dev, const struct vio_device_id *id);
+static void __devexit hvcs_remove(struct vio_dev *dev);
+static int __init hvcs_module_init(void);
+static void __exit hvcs_module_exit(void);
+
+/* This task is scheduled to execute out of the read data interrupt
+ * handler, the hvcs_unthrottle, and be rescheduled out of itself.
+ * This should never get called from both unthrottle and the
+ * read_task at the same time because if the tty isn't throttled the
+ * unthrottle function will never call this function and if the tty is
+ * throttled then only unthrottle will call this function because vio
+ * interrupts will be turned off until this function enables them.
+ * We only want to enable interrupts when hvterm_get_chars() returns
+ * zero.
+ */
+
+static void hvcs_read_task(unsigned long data)
+{
+	struct hvcs_struct *hvcsd = (struct hvcs_struct *)data;
+	unsigned int unit_address = hvcsd->vdev->unit_address;
+	struct tty_struct *tty = hvcsd->tty;
+	char buf[HVCS_BUFF_LEN] __ALIGNED__;
+	int got;
+	int i;
+
+	/* It is possible that hvcs_close was interrupted and this
+	 * task was scheduled during the middle of hvcs_close().  */
+	if (!hvcsd->enabled || !tty) {
+		return;
+	}
+
+	while ((tty->flip.count + HVCS_BUFF_LEN) < TTY_FLIPBUF_SIZE ) {
+		memset(&buf[0], 0x00, HVCS_BUFF_LEN);
+		got = hvterm_get_chars(unit_address, &buf[0], HVCS_BUFF_LEN);
+
+		if (!got) {
+			if (tty->flip.count)
+				tty_flip_buffer_push(tty);
+			vio_enable_interrupts(hvcsd->vdev);
+			return;
+		}
+		for(i = 0; i < got; i++) {
+			tty_insert_flip_char(tty, buf[i], TTY_NORMAL);
+		}
+
+	}
+	if (tty->flip.count) {
+		tty_flip_buffer_push(tty);
+	}
+	/* reschedule because the flip buffer is full and we may have
+	 * more data to pull from the hypervisor */
+	schedule_delayed_work(&hvcsd->read_work, 1);
+	return;
+}
+
+/* This is the callback from the tty layer that tells us that the flip
+ * buffer has more space.
+ */
+static void hvcs_unthrottle(struct tty_struct *tty)
+{
+	struct hvcs_struct *hvcsd = (struct hvcs_struct *)tty->driver_data;
+	printk(KERN_INFO "HVCS: tty unthrottled, re-scheduling"
+			" read task.\n");
+	schedule_delayed_work(&hvcsd->read_work, 1);
+}
+
+static void hvcs_throttle(struct tty_struct *tty)
+{
+	struct hvcs_struct *hvcsd = (struct hvcs_struct *)tty->driver_data;
+
+	printk(KERN_INFO "HVCS: tty throttled.\n");
+
+	/* We'll re-schedule once the tty executes the
+	 * unthrottle callback. */
+	cancel_delayed_work(&hvcsd->read_work);
+}
+
+/* If the device is being removed we don't have to worry about this
+ * interrupt handler taking any further interrupts because they are
+ * disabled which means the hvcs_struct will always be valid in this
+ * handler.
+ */
+static irqreturn_t hvcs_handle_interrupt(int irq, void *dev_instance, struct pt_regs *regs)
+{
+	struct hvcs_struct *hvcsd = (struct hvcs_struct *)dev_instance;
+
+	vio_disable_interrupts(hvcsd->vdev);
+	schedule_work(&hvcsd->read_work);
+
+	return IRQ_HANDLED;
+}
+
+static struct vio_device_id hvcs_driver_table[] __devinitdata= {
+	{"serial-server", "hvterm2"},
+	{ 0,}
+};
+MODULE_DEVICE_TABLE(vio, hvcs_driver_table);
+
+/* callback when the kboject ref count reaches zero */
+static void destroy_hvcs_struct(struct kobject *kobj)
+{
+	struct hvcs_struct *hvcsd = from_kobj(kobj);
+
+	list_del(&(hvcsd->next));
+
+	kfree(hvcsd);
+
+	printk(KERN_INFO "HVCS: Last kobj to hvcs_struct released,"
+		" hvcs_struct freed.\n");
+}
+
+static struct kobj_type hvcs_kobj_type = {
+	.release = destroy_hvcs_struct,
+};
+
+static int __devinit hvcs_probe(
+	struct vio_dev *dev,
+	const struct vio_device_id *id)
+{
+	struct hvcs_struct *hvcsd;
+
+	if (!dev || !id) {
+		printk(KERN_ERR "hvcs_probe: called with invalid"
+			" device or id pointer.\n");
+	}
+
+	printk(KERN_INFO "HVCS: Added vty-server@%X.\n", dev->unit_address);
+
+	hvcsd = kmalloc(sizeof(*hvcsd), GFP_KERNEL);
+	if (!hvcsd) {
+		return -ENODEV;
+	}
+
+	/* hvcsd->tty is zeroed out with the memset */
+	memset(hvcsd, 0x00, sizeof(*hvcsd));
+
+	/* Automatically incs the refcount the first time */
+	kobject_init(&hvcsd->kobj);
+	/* Set up the callback for terminating the hvcs_struct's life */
+	hvcsd->kobj.ktype = &hvcs_kobj_type;
+
+	hvcsd->vdev = dev;
+	dev->driver_data = hvcsd;
+	sprintf(hvcsd->name,"%X",dev->unit_address);
+
+	hvcsd->index = ++hvcs_struct_count;
+
+	INIT_WORK(&hvcsd->read_work, hvcs_read_task, (unsigned long)hvcsd);
+
+	hvcsd->enabled = 0;
+
+	/* This will populate the hvcs_struct's partner info fields
+	 * for the first time. */
+	if(hvcs_get_pi(hvcsd)) {
+		printk(KERN_ERR "hvcs_probe : Failed to fetch partner"
+			" info for vty-server@%X.\n",
+			hvcsd->vdev->unit_address);
+	}
+
+	/* If a user app opens a tty that corresponds to this vty-server@
+	 * vdevice before the hvcs_struct has been added to the devices
+	 * list then the user app will get -ENODEV.
+	 */
+
+	list_add_tail(&(hvcsd->next), &hvcs_structs);
+
+	hvcs_create_device_attrs(hvcsd);
+
+	/* DON'T enable interrupts here because there is no user */
+	return 0;
+}
+
+static void __devexit hvcs_remove(struct vio_dev *dev)
+{
+	struct hvcs_struct *hvcsd = (struct hvcs_struct *)dev->driver_data;
+
+	if (!hvcsd)
+		return;
+
+	printk(KERN_INFO "HVCS: Removed vty-server@%X.\n",
+		dev->unit_address);
+
+	/* By this time the vty-server@ vdevice won't be getting any
+	 * more interrups but we might get a callback from the tty. */
+	cancel_delayed_work(&hvcsd->read_work);
+	flush_scheduled_work();
+
+	/* If hvcs_remove is called after a tty_hangup has been issued
+	 * (indicating that there are no connections) or before a user
+	 * has attempted to open the device then the device will not be
+	 * enabled and thus we don't need to do any cleanup.  */
+	if (hvcsd->enabled) {
+		hvcs_disable_device(hvcsd);
+	}
+
+	if (hvcsd->tty) {
+		/* This is a scheduled function which will
+		 * auto chain call hvcs_hangup. */
+		tty_hangup(hvcsd->tty);
+	}
+
+	hvcs_remove_device_attrs(hvcsd);
+
+	/* Let the last holder of this object cause it to be removed,
+	 * which would probably be tty_hangup.
+	 */
+	kobject_put (&hvcsd->kobj);
+};
+
+static struct vio_driver hvcs_vio_driver = {
+	.name		= &hvcs_driver_name,
+	.id_table	= hvcs_driver_table,
+	.probe		= hvcs_probe,
+	.remove		= __devexit_p(hvcs_remove),
+};
+
+/* Only called from hvcs_get_pi please */
+static void hvcs_set_pi(struct hvcs_partner_info *pi, struct hvcs_struct *hvcsd)
+{
+	int clclength;
+
+	hvcsd->p_unit_address = pi->unit_address;
+	hvcsd->p_partition_ID  = pi->partition_ID;
+	clclength = strlen(&pi->location_code[0]);
+	if(clclength > CLC_LENGTH - 1) {
+		clclength = CLC_LENGTH - 1;
+	}
+	/* copy the null-term char too */
+	strncpy(&hvcsd->p_location_code[0],
+			&pi->location_code[0], clclength + 1);
+}
+
+/* Traverse the list and add the partner info that is found to the
+ * hvcs_struct struct entry. NOTE: At this time I know that partner
+ * info will return a single entry but in the future there may be
+ * multiple partner info entries per vty-server@ vdevice and you'll
+ * want to zero out that list and reset it.  If for some reason you
+ * have an old version of this driver and there IS more than one
+ * partner info then hvcsd->p_* will have the last partner info
+ * data from the firmware query.
+ */
+static int hvcs_get_pi(struct hvcs_struct *hvcsd)
+{
+	/* struct hvcs_partner_info *head_pi = NULL; */
+	struct hvcs_partner_info *pi = NULL;
+	unsigned int unit_address = hvcsd->vdev->unit_address;
+	struct list_head head;
+	int retval;
+
+	retval = hvcs_get_partner_info(unit_address, &head);
+	if (retval) {
+		printk(KERN_ERR "HVCS: Failed to fetch partner"
+			" info for vty-server@%x.\n",unit_address);
+		return retval;
+	}
+
+	/* nixes the values if the partner vty@ went away */
+	hvcsd->p_unit_address = 0;
+	hvcsd->p_partition_ID = 0;
+
+	list_for_each_entry(pi, &head, node) {
+		hvcs_set_pi(pi,hvcsd);
+	}
+
+	hvcs_free_partner_info(&head);
+	return 0;
+}
+
+/* This function is executed by the driver "rescan" sysfs entry */
+static int hvcs_rescan_devices_list(void)
+{
+	struct hvcs_struct *hvcsd = NULL;
+
+	/* Locking issues? */
+	list_for_each_entry(hvcsd, &hvcs_structs, next) {
+		hvcs_get_pi(hvcsd);
+	}
+
+	return 0;
+}
+
+/* Farm this off into its own function because it could be
+ * more complex once multiple partners support is added. */
+static int hvcs_has_pi(struct hvcs_struct *hvcsd)
+{
+	if ((!hvcsd->p_unit_address) || (!hvcsd->p_partition_ID))
+		return 0;
+	return 1;
+}
+
+/* NOTE: It is possible that the super admin removed
+ * a partner vty@ vdevice and then added a diff vty@
+ * vdevice as the new partner.  At this point this
+ * could connect a console to a different target
+ * partition.
+ */
+static int hvcs_partner_connect(struct hvcs_struct *hvcsd)
+{
+	int retval;
+	unsigned int unit_address = hvcsd->vdev->unit_address;
+
+	/* If there wasn't any pi when the vdevice was added it
+	 * doesn't meant there isn't any now.  This driver isn't
+	 * notified when a new partner vty@ is added to a
+	 * vty-server@ vdevice so we discover changes on our own.
+	 * Please see comments in hvcs_register_connection() for
+	 * justification of this bizarre code. */
+	retval = hvcs_register_connection(unit_address,
+			hvcsd->p_partition_ID,
+			hvcsd->p_unit_address);
+	if (!retval)
+		return 0;
+	else if (retval != -EINVAL)
+		return retval;
+
+	/* As per the spec re-get the pi and try again if -EINVAL
+	 * after the first connection attempt. */
+	if (hvcs_get_pi(hvcsd))
+		return -ENOMEM;
+
+	if (!hvcs_has_pi(hvcsd))
+		return -ENODEV;
+
+	retval = hvcs_register_connection(unit_address,
+			hvcsd->p_partition_ID,
+			hvcsd->p_unit_address);
+	if (retval != -EINVAL)
+		return retval;
+
+	/* EBUSY is the most likely scenario though the vty@ could have
+	 * been removed or there really could be an hcall error due to the
+	 * parameter data. */
+	printk(KERN_INFO "HVCS: vty-server@ or partner"
+			" vty@ is busy.  Try again later.\n");
+	return -EBUSY;
+}
+
+static void hvcs_partner_free(struct hvcs_struct *hvcsd)
+{
+	int retval;
+	do {
+		/* it will return -EBUSY if the operation would take too
+		 * long to complete synchronously.
+		 */
+		retval = hvcs_free_connection(hvcsd->vdev->unit_address);
+	} while (retval == -EBUSY);
+}
+
+static int hvcs_enable_device(struct hvcs_struct *hvcsd)
+{
+	int retval;
+	/* It is possible that the vty-server@ vdevice was removed
+	 * between the time that the conn was registered and now.
+	 */
+	if ((retval = request_irq(hvcsd->vdev->irq,
+			&hvcs_handle_interrupt, SA_INTERRUPT,
+			"ibmhvcs", hvcsd)) != 0) {
+		printk(KERN_ERR "hvcs_enable_device : failed to request"
+			" irq for vty-server@%X with retval :%d.\n",
+			hvcsd->vdev->unit_address, retval);
+		hvcs_partner_free(hvcsd);
+		return -ENODEV;
+	}
+
+	/* It is possible the vty-server@ vdevice was removed
+	 * after the irq was requested but before we have time
+	 * to enabled interrupts.
+	 */
+	if (vio_enable_interrupts(hvcsd->vdev) != H_Success) {
+		printk(KERN_ERR "hvcs_enable_device : failed to enable"
+			" interrtups for vty-server@%X.\n",
+			hvcsd->vdev->unit_address);
+		/* These can fail but we'll just ignore them for now. */
+		free_irq(hvcsd->vdev->irq, hvcsd);
+		hvcs_partner_free(hvcsd);
+		return -ENODEV;
+	}
+
+	return 0;
+}
+
+static void hvcs_disable_device(struct hvcs_struct *hvcsd)
+{
+	if(!hvcsd->enabled)
+		return;
+
+	hvcsd->enabled = 0;
+	/* Any one of these might fail at any time due to the
+	 * vty-server@ vdevice's availability during the call.
+	 */
+	vio_disable_interrupts(hvcsd->vdev);
+	hvcs_partner_free(hvcsd);
+	printk(KERN_INFO "HVCS: Freed vty-server@%X and"
+			" partner vty@%X:%d.\n",
+			hvcsd->vdev->unit_address,
+			hvcsd->p_unit_address,
+			(unsigned int)hvcsd->p_partition_ID);
+	free_irq(hvcsd->vdev->irq, hvcsd);
+}
+
+/* This always increments the kobject ref count if the call is
+ * successful.  Please remember to dec when you are done with
+ * the instance. */
+struct hvcs_struct *hvcs_get_by_index(int index)
+{
+	struct hvcs_struct *hvcsd = NULL;
+	struct list_head *element;
+	struct list_head *safe_temp;
+	/* We can immediately discard OOB requests */
+	if (index >= 0 && index < HVCS_MAX_SERVER_ADAPTERS) {
+		list_for_each_safe(element, safe_temp, &hvcs_structs) {
+			hvcsd = list_entry(element, struct hvcs_struct, next);
+			if (hvcsd->index == index) {
+				kobject_get(&hvcsd->kobj);
+				break;
+			}
+		}
+	}
+	return hvcsd;
+}
+
+/* This is invoked via the tty_open interface when a user app
+ * connects to the /dev node.
+ */
+static int hvcs_open(struct tty_struct *tty, struct file *filp)
+{
+	struct hvcs_struct *hvcsd = NULL;
+	int retval = 0;
+
+	if (tty->driver_data)
+		goto fast_open;
+
+	/* Is there a vty-server@ adapter that shares the same index? */
+	if (!(hvcsd = hvcs_get_by_index(tty->index))) return -ENODEV;
+
+	printk(KERN_INFO "HVCS: First open of vty-server@%X.\n",
+		hvcsd->vdev->unit_address );
+
+	if((retval = hvcs_partner_connect(hvcsd)))
+		goto error_release;
+
+	/* This will free the connection if it fails */
+	if((retval = hvcs_enable_device(hvcsd)))
+		goto error_release;
+
+	hvcsd->open_count = 1;
+	hvcsd->enabled = 1;
+	hvcsd->tty = tty;
+	tty->driver_data = hvcsd;
+
+	goto open_success;
+
+fast_open:
+	hvcsd = (struct hvcs_struct *)tty->driver_data;
+
+	if (!kobject_get(&hvcsd->kobj)) {
+		printk(KERN_ERR "hvcs_open: Kobject of open"
+			" hvcs doesn't exist.\n");
+		return -EFAULT; /* Is this the right return value? */
+	}
+
+	hvcsd->open_count++;
+
+open_success:
+	return 0;
+
+error_release:
+	kobject_put(&hvcsd->kobj);
+	return retval;
+}
+
+static void hvcs_close(struct tty_struct *tty, struct file *filp)
+{
+	struct hvcs_struct *hvcsd;
+
+	/* Is someone trying to close the file associated with
+	 * this device after we have hung up?  If so
+	 * tty->driver_data wouldn't be valid.
+	 */
+	if (tty_hung_up_p(filp)) {
+		printk(KERN_INFO "hvcs_close: tty is hung up.\n");
+		return;
+	}
+
+	/* No driver_data means that this close was probably
+	 * issued after a failed hvcs_open by the tty layer's
+	 * release_dev() api and we can just exit cleanly.
+	 */
+	if (!tty->driver_data) {
+		printk(KERN_INFO "hvcs_close: No tty->driver_data.\n");
+		return;
+	}
+
+	hvcsd = (struct hvcs_struct *)tty->driver_data;
+
+	if (--hvcsd->open_count == 0) {
+		hvcs_disable_device(hvcsd);
+
+		/* This line is important because it tells hvcs_open
+		 * that this device needs to be re-configured the
+		 * next time hvcs_open is called.
+		 */
+		hvcsd->tty->driver_data = NULL;
+		hvcsd->tty = NULL;
+		hvcsd->p_partition_ID = 0;
+		hvcsd->p_unit_address = 0;
+		memset(&hvcsd->p_location_code[0], 0x00, CLC_LENGTH);
+	} else if (hvcsd->open_count < 0) {
+		printk(KERN_ERR "hvcs_close : vty-server@%X"
+			" is missmanaged with open_count: %d.\n",
+			hvcsd->vdev->unit_address, hvcsd->open_count);
+	}
+
+	kobject_put(&hvcsd->kobj);
+}
+
+static void hvcs_hangup(struct tty_struct * tty)
+{
+	struct hvcs_struct *hvcsd = (struct hvcs_struct *)tty->driver_data;
+
+	/* If the vty-server@ vdevice disappeared then the device
+	 * would already be disabled.  Otherwise the hangup was
+	 * indicated from a sighup?
+	 */
+	hvcs_disable_device(hvcsd);
+
+	/* One big fat close regardless of the open_count */
+	hvcsd->tty->driver_data = NULL;
+	hvcsd->tty = NULL;
+	hvcsd->p_partition_ID = 0;
+	hvcsd->p_unit_address = 0;
+	memset(&hvcsd->p_location_code[0], 0x00, CLC_LENGTH);
+
+	/* We need to kobject_put() for every open_count we have
+	 * since the tty_hangup() function doesn't invoke a close
+	 * per open connection on a non-console device. */
+	while(hvcsd->open_count) {
+		--hvcsd->open_count;
+		/* The final put will trigger destruction of the
+		 * hvcs_struct */
+		kobject_put(&hvcsd->kobj);
+	}
+}
+
+/*NOTE: This is almost always from_user since user level apps interact
+ * with the /dev devices. I'm going out on a limb here and trusting
+ * that if hvcs_write gets called and interrupted by hvcs_remove
+ * (which removes the target device and executes tty_hangup()) that
+ * tty_hangup will allow hvcs_write time to complete execution before
+ * it terminates our device. */
+static int hvcs_write(struct tty_struct *tty, int from_user, const unsigned char *buf, int count)
+{
+	struct hvcs_struct *hvcsd = (struct hvcs_struct *)tty->driver_data;
+	unsigned int unit_address;
+	unsigned char *charbuf;
+	int total_sent = 0;
+	int tosend = 0;
+	int sent = 0;
+
+	/* If they don't check the return code off of their open they
+	 * may attempt this even if there is no connected device. */
+	if (!hvcsd) {
+		printk(KERN_ERR "hvcs_write: No enabled vty-server@"
+			" adapter associated with this tty.\n");
+		return -ENODEV;
+	}
+
+	/* Reasonable size to prevent user level flooding */
+	if (count > HVCS_MAX_FROM_USER)
+		count = HVCS_MAX_FROM_USER;
+
+	unit_address = hvcsd->vdev->unit_address;
+
+	/* Somehow an open succeded but the device was removed or the
+	 * connection terminated between the vty-server@ vdevice and
+	 * partner vty@ vdevice during the middle of a write
+	 * operation? */
+	if (!hvcsd->enabled)
+		return -ENODEV;
+
+	if (!from_user) {
+		charbuf = (unsigned char *)buf;
+	} else {
+		/* This isn't so important to do if we don't spinlock
+		 * around the copy_from_user but we'll leave it here
+		 * anyway because there may be locking issues in the
+		 * future. */
+		charbuf = kmalloc(count, GFP_KERNEL);
+		if(!charbuf)
+			return -ENOMEM;
+
+		count -= copy_from_user(charbuf,buf,count);
+	}
+
+	while (count > 0) {
+		tosend = min(count, HVCS_BUFF_LEN);
+
+		/* won't return partial writes */
+		sent = hvterm_put_chars(unit_address, &charbuf[total_sent], tosend);
+		if (sent <= 0) {
+			break;
+		}
+
+		total_sent+=sent;
+		count-=sent;
+	}
+
+	if (from_user)
+		kfree(charbuf);
+
+	if (sent == -1)
+		return -ENODEV;
+	else
+		return total_sent;
+}
+
+static int hvcs_write_room(struct tty_struct *tty)
+{
+	struct hvcs_struct *hvcsd = (struct hvcs_struct *)tty->driver_data;
+
+	if (!hvcsd || !hvcsd->enabled)
+		return 0;
+	else
+		return HVCS_MAX_FROM_USER;
+}
+
+static int hvcs_chars_in_buffer(struct tty_struct *tty)
+{
+	return HVCS_MAX_FROM_USER;
+}
+
+static struct tty_operations hvcs_ops = {
+	.open = hvcs_open,
+	.close = hvcs_close,
+	.hangup = hvcs_hangup,
+	.write = hvcs_write,
+	.write_room = hvcs_write_room,
+	.chars_in_buffer = hvcs_chars_in_buffer,
+	.unthrottle = hvcs_unthrottle,
+	.throttle = hvcs_throttle,
+};
+
+static int __init hvcs_module_init(void)
+{
+	int rc;
+	int num_ttys_to_alloc;
+
+	printk(KERN_INFO "Initializing %s\n", hvcs_driver_string);
+
+	/* Has the user specified an overload with an insmod param? */
+	if( hvcs_parm_num_devs <= 0 ||
+		(hvcs_parm_num_devs > HVCS_MAX_SERVER_ADAPTERS)) {
+		num_ttys_to_alloc = HVCS_DEFAULT_SERVER_ADAPTERS;
+	} else {
+		num_ttys_to_alloc = hvcs_parm_num_devs;
+	}
+
+	hvcs_tty_driver = alloc_tty_driver(num_ttys_to_alloc);
+	if (!hvcs_tty_driver)
+		return -ENOMEM;
+
+	hvcs_tty_driver->owner = THIS_MODULE;
+
+	hvcs_tty_driver->driver_name = hvcs_driver_name;
+	hvcs_tty_driver->name = hvcs_device_node;
+
+	/* We'll let the system assign us a major number,
+	 * indicated by leaving it blank  */
+
+	hvcs_tty_driver->minor_start = HVCS_MINOR_START;
+	hvcs_tty_driver->type = TTY_DRIVER_TYPE_SYSTEM;
+
+	/* We role our own so that we DONT ECHO.  We can't echo
+	 * because the adapter we are connecting to already echoes
+	 * by default and this would throw us into a horrible
+	 * recursive echo-echo-echo loop. */
+	hvcs_tty_driver->init_termios = hvcs_tty_termios;
+	hvcs_tty_driver->flags = TTY_DRIVER_REAL_RAW;
+
+	tty_set_operations(hvcs_tty_driver, &hvcs_ops);
+
+	/* The following call will result in sysfs entries that denote
+	 * the dynamically assigned major and minor numbers for our
+	 * deices. */
+	if(tty_register_driver(hvcs_tty_driver)) {
+		printk(KERN_ERR "hvcs_module_init: registration of"
+			" hvcs as tty driver failed.\n");
+		put_tty_driver(hvcs_tty_driver);
+		return rc;
+	}
+
+	rc = vio_register_driver(&hvcs_vio_driver);
+
+	/* This needs to be done AFTER the vio_register_driver() call
+	 * or else the kobjects won't be initialized properly.
+	 */
+	hvcs_create_driver_attrs();
+
+	printk(KERN_INFO "HVCS: driver module inserted.\n");
+
+	return rc;
+}
+
+static void __exit hvcs_module_exit(void)
+{
+	hvcs_remove_driver_attrs();
+
+	vio_unregister_driver(&hvcs_vio_driver);
+
+	tty_unregister_driver(hvcs_tty_driver);
+
+	put_tty_driver(hvcs_tty_driver);
+
+	printk(KERN_INFO "HVCS: driver module removed.\n");
+}
+
+module_init(hvcs_module_init);
+module_exit(hvcs_module_exit);
+
+static inline struct hvcs_struct *from_vio_dev(struct vio_dev *viod)
+{
+	return (struct hvcs_struct *)viod->driver_data;
+}
+/* The sysfs interface for the driver and devices */
+
+static ssize_t hvcs_partner_vtys_show(struct device *dev, char *buf)
+{
+	struct vio_dev *viod = to_vio_dev(dev);
+	struct hvcs_struct *hvcsd = from_vio_dev(viod);
+	return sprintf(buf, "vty@%X\n", hvcsd->p_unit_address);
+}
+static DEVICE_ATTR(partner_vtys, S_IRUGO, hvcs_partner_vtys_show, NULL);
+
+static ssize_t hvcs_partner_clcs_show(struct device *dev, char *buf)
+{
+	struct vio_dev *viod = to_vio_dev(dev);
+	struct hvcs_struct *hvcsd = from_vio_dev(viod);
+	return sprintf(buf, "%s\n", &hvcsd->p_location_code[0]);
+}
+static DEVICE_ATTR(partner_clcs, S_IRUGO, hvcs_partner_clcs_show, NULL);
+
+static ssize_t hvcs_dev_node_show(struct device *dev, char *buf)
+{
+	struct vio_dev *viod = to_vio_dev(dev);
+	struct hvcs_struct *hvcsd = from_vio_dev(viod);
+	return sprintf(buf, "/dev/%s%d\n",hvcs_device_node, hvcsd->index);
+}
+static DEVICE_ATTR(dev_node, S_IRUGO, hvcs_dev_node_show, NULL);
+
+static ssize_t hvcs_current_vty_store(struct device *dev, const char * buf, size_t count)
+{
+	/* Don't need this feature at the present time. */
+	printk(KERN_INFO "HVCS: Denied current_vty change: -EPERM.\n");
+	return -EPERM;
+}
+
+static ssize_t hvcs_current_vty_show(struct device *dev, char *buf)
+{
+	struct vio_dev *viod = to_vio_dev(dev);
+	struct hvcs_struct *hvcsd = from_vio_dev(viod);
+	return sprintf(buf, "%s\n", &hvcsd->p_location_code[0]);
+}
+
+static DEVICE_ATTR(current_vty,
+	S_IRUGO | S_IWUSR, hvcs_current_vty_show, hvcs_current_vty_store);
+
+static struct attribute *hvcs_attrs[] = {
+	&dev_attr_partner_vtys.attr,
+	&dev_attr_partner_clcs.attr,
+	&dev_attr_dev_node.attr,
+	&dev_attr_current_vty.attr,
+	NULL,
+};
+
+static struct attribute_group hvcs_attr_group = {
+	.attrs = hvcs_attrs,
+};
+
+static void hvcs_create_device_attrs(struct hvcs_struct *hvcsd)
+{
+	struct vio_dev *vdev = hvcsd->vdev;
+	sysfs_create_group(&vdev->dev.kobj, &hvcs_attr_group);
+}
+
+static void hvcs_remove_device_attrs(struct hvcs_struct *hvcsd)
+{
+	struct vio_dev *vdev = hvcsd->vdev;
+	sysfs_remove_group(&vdev->dev.kobj, &hvcs_attr_group);
+}
+
+static ssize_t hvcs_rescan_show(struct device_driver *ddp, char *buf)
+{
+	/* A 1 means it is updating, a 0 means it is done updating */
+	return snprintf(buf, PAGE_SIZE, "%d\n", hvcs_rescan_status);
+}
+
+static ssize_t hvcs_rescan_store(struct device_driver *ddp, const char * buf, size_t count)
+{
+	if ((simple_strtol(buf,NULL,0) != 1)
+		&& (hvcs_rescan_status != 0))
+		return -EINVAL;
+
+	hvcs_rescan_status = 1;
+	printk(KERN_INFO "HVCS: rescanning partner info for all"
+		" vty-server@ vdevices.\n");
+	hvcs_rescan_devices_list();
+	hvcs_rescan_status = 0;
+	return count;
+}
+static DRIVER_ATTR(rescan,
+	S_IRUGO | S_IWUSR, hvcs_rescan_show, hvcs_rescan_store);
+
+static void hvcs_create_driver_attrs(void)
+{
+	struct device_driver *driverfs = &(hvcs_vio_driver.driver);
+	driver_create_file(driverfs, &driver_attr_rescan);
+}
+
+static void hvcs_remove_driver_attrs(void)
+{
+	struct device_driver *driverfs = &(hvcs_vio_driver.driver);
+	driver_remove_file(driverfs, &driver_attr_rescan);
+}
diff -purN linux-2.5/drivers/net/Kconfig linuxppc64-2.5/drivers/net/Kconfig
--- linux-2.5/drivers/net/Kconfig	2004-02-27 22:23:58.000000000 +0000
+++ linuxppc64-2.5/drivers/net/Kconfig	2004-03-01 23:51:10.000000000 +0000
@@ -2104,10 +2104,19 @@ config IXGB_NAPI
 endmenu
 
 
-config VETH
+config ISERIES_VETH
 	tristate "iSeries Virtual Ethernet driver support"
 	depends on NETDEVICES && PPC_ISERIES
 
+config IBMVETH
+	tristate "IBM LAN Virtual Ethernet support"
+	depends on NET_ETHERNET && PPC_PSERIES
+	---help---
+	  This is the Virtual Ethernet adapter driver.  
+
+	  To compile this driver as a module, choose M here and read
+	  <file:Documentation/networking/net-modules.txt>. 
+
 config FDDI
 	bool "FDDI driver support"
 	depends on NETDEVICES && (PCI || EISA)
diff -purN linux-2.5/drivers/net/Makefile linuxppc64-2.5/drivers/net/Makefile
--- linux-2.5/drivers/net/Makefile	2004-02-17 16:05:28.000000000 +0000
+++ linuxppc64-2.5/drivers/net/Makefile	2004-02-26 21:01:16.000000000 +0000
@@ -45,6 +45,7 @@ obj-$(CONFIG_SIS190) += sis190.o
 obj-$(CONFIG_SIS900) += sis900.o
 obj-$(CONFIG_YELLOWFIN) += yellowfin.o
 obj-$(CONFIG_ACENIC) += acenic.o
+obj-$(CONFIG_ISERIES_VETH) += iseries_veth.o
 obj-$(CONFIG_NATSEMI) += natsemi.o
 obj-$(CONFIG_NS83820) += ns83820.o
 obj-$(CONFIG_STNIC) += stnic.o 8390.o
diff -purN linux-2.5/drivers/net/e1000/e1000_main.c linuxppc64-2.5/drivers/net/e1000/e1000_main.c
--- linux-2.5/drivers/net/e1000/e1000_main.c	2004-02-20 22:09:55.000000000 +0000
+++ linuxppc64-2.5/drivers/net/e1000/e1000_main.c	2004-02-26 05:41:58.000000000 +0000
@@ -1582,7 +1582,7 @@ e1000_tx_map(struct e1000_adapter *adapt
 
 	i = tx_ring->next_to_use;
 
-	while(len) {
+	while(len && count < E1000_DESC_UNUSED(&adapter->tx_ring)) {
 		buffer_info = &tx_ring->buffer_info[i];
 		size = min(len, max_per_txd);
 #ifdef NETIF_F_TSO
@@ -1619,7 +1619,7 @@ e1000_tx_map(struct e1000_adapter *adapt
 		len = frag->size;
 		offset = frag->page_offset;
 
-		while(len) {
+		while(len && count < E1000_DESC_UNUSED(&adapter->tx_ring)) {
 			buffer_info = &tx_ring->buffer_info[i];
 			size = min(len, max_per_txd);
 #ifdef NETIF_F_TSO
diff -purN linux-2.5/drivers/net/ibmveth.c linuxppc64-2.5/drivers/net/ibmveth.c
--- linux-2.5/drivers/net/ibmveth.c	2004-03-01 16:38:07.000000000 +0000
+++ linuxppc64-2.5/drivers/net/ibmveth.c	2004-03-02 03:48:28.000000000 +0000
@@ -527,7 +527,10 @@ static int ibmveth_open(struct net_devic
 	ibmveth_debug_printk("registering irq 0x%x\n", netdev->irq);
 	if((rc = request_irq(netdev->irq, &ibmveth_interrupt, 0, netdev->name, netdev)) != 0) {
 		ibmveth_error_printk("unable to request irq 0x%x, rc %d\n", netdev->irq, rc);
-		h_free_logical_lan(adapter->vdev->unit_address);
+		do {
+			rc = h_free_logical_lan(adapter->vdev->unit_address);
+		} while H_isLongBusy(rc);
+
 		ibmveth_cleanup(adapter);
 		return rc;
 	}
@@ -556,7 +559,9 @@ static int ibmveth_close(struct net_devi
 	cancel_delayed_work(&adapter->replenish_task);
 	flush_scheduled_work();
 
-	lpar_rc = h_free_logical_lan(adapter->vdev->unit_address);
+	do {
+		lpar_rc = h_free_logical_lan(adapter->vdev->unit_address);
+	} while H_isLongBusy(lpar_rc);
 
 	if(lpar_rc != H_Success)
 	{
@@ -617,6 +622,8 @@ static int ibmveth_start_xmit(struct sk_
 	union ibmveth_buf_desc desc[IbmVethMaxSendFrags];
 	unsigned long lpar_rc;
 	int nfrags = 0, curfrag;
+	unsigned long correlator;
+	unsigned int retry_count;
 
 	if ((skb_shinfo(skb)->nr_frags + 1) > IbmVethMaxSendFrags) {
 		adapter->stats.tx_dropped++;
@@ -674,8 +681,8 @@ static int ibmveth_start_xmit(struct sk_
 	}
 
 	/* send the frame. Arbitrarily set retrycount to 1024 */
-	unsigned long correlator = 0;
-	unsigned int retry_count = 1024;
+	correlator = 0;
+	retry_count = 1024;
 	do {
 		lpar_rc = h_send_logical_lan(adapter->vdev->unit_address,
 					     desc[0].desc,
diff -purN linux-2.5/drivers/net/iseries_veth.c linuxppc64-2.5/drivers/net/iseries_veth.c
--- linux-2.5/drivers/net/iseries_veth.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/net/iseries_veth.c	2004-02-23 20:52:59.000000000 +0000
@@ -0,0 +1,1340 @@
+/* File veth.c created by Kyle A. Lucke on Mon Aug  7 2000. */
+/*
+ * IBM eServer iSeries Virtual Ethernet Device Driver
+ * Copyright (C) 2001 Kyle A. Lucke (klucke@us.ibm.com), IBM Corp.
+ * Substantially cleaned up by:
+ * Copyright (C) 2003 David Gibson <dwg@au1.ibm.com>, IBM Corporation.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *                                                                      
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307
+ * USA
+ *
+ *
+ * This module implements the virtual ethernet device for iSeries LPAR
+ * Linux.  It uses hypervisor message passing to enable an
+ * ethernet-like network device communicating between partitions on
+ * the iSeries.
+ *
+ * The iSeries LPAR hypervisor currently allows for up to 16 different
+ * virtual ethernets.  These are all dynamically configurable on
+ * OS/400 partitions, but dynamic configuration is not supported under
+ * Linux yet.  An ethXX network device will be created for each
+ * virtual ethernet this partition is connected to.
+ *
+ * - This driver is responsible for routing packets to and from other
+ *   partitions.  The MAC addresses used by the virtual ethernets
+ *   contains meaning and must not be modified.
+ *
+ * - Having 2 virtual ethernets to the same remote partition DOES NOT
+ *   double the available bandwidth.  The 2 devices will share the
+ *   available hypervisor bandwidth.
+ *
+ * - If you send a packet to your own mac address, it will just be
+ *   dropped, you won't get it on the receive side.
+ *
+ * - Multicast is implemented by sending the frame frame to every
+ *   other partition.  It is the responsibility of the receiving
+ *   partition to filter the addresses desired.
+ *
+ * Tunable parameters:
+ *
+ * VETH_NUMBUFFERS: This compile time option defaults to 120.  It
+ * controls how much memory Linux will allocate per remote partition
+ * it is communicating with.  It can be thought of as the maximum
+ * number of packets outstanding to a remote partition at a time.
+ */
+
+#include <linux/config.h>
+#include <linux/module.h>
+#include <linux/version.h>
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <linux/ioport.h>
+#include <linux/pci.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/skbuff.h>
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/mm.h>
+#include <linux/ethtool.h>
+#include <asm/iSeries/mf.h>
+#include <asm/uaccess.h>
+
+#include <asm/iSeries/HvLpConfig.h>
+#include <asm/iSeries/HvTypes.h>
+#include <asm/iSeries/HvLpEvent.h>
+#include <asm/iommu.h>
+
+#include "iseries_veth.h"
+
+extern struct pci_dev *iSeries_veth_dev;
+
+#define veth_printk(prio, fmt, args...) \
+	printk(prio "%s: " fmt, __FILE__, ## args)
+
+#define veth_error(fmt, args...) \
+	printk(KERN_ERR "(%s:%3.3d) ERROR: " fmt, __FILE__, __LINE__ , ## args)
+
+#define VETH_NUMBUFFERS		120
+
+static int veth_open(struct net_device *dev);
+static int veth_close(struct net_device *dev);
+static int veth_start_xmit(struct sk_buff *skb, struct net_device *dev);
+static int veth_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd);
+static void veth_handle_event(struct HvLpEvent *, struct pt_regs *);
+static void veth_handle_ack(struct VethLpEvent *);
+static void veth_handle_int(struct VethLpEvent *);
+static void veth_init_connection(struct veth_lpar_connection *cnx, u8 rlp);
+static void veth_open_connection(u8);
+static void veth_finish_open_connection(void *parm);
+static void veth_close_connection(u8);
+static void veth_set_multicast_list(struct net_device *dev);
+
+static void veth_take_cap(struct veth_lpar_connection *, struct VethLpEvent *);
+static void veth_take_cap_ack(struct veth_lpar_connection *, struct VethLpEvent *);
+static void veth_take_monitor_ack(struct veth_lpar_connection *,
+				  struct VethLpEvent *);
+static void veth_recycle_msg(struct veth_lpar_connection *, struct veth_msg *);
+static void veth_monitor_ack_task(void *);
+static void veth_receive(struct veth_lpar_connection *, struct VethLpEvent *);
+static struct net_device_stats *veth_get_stats(struct net_device *dev);
+static int veth_change_mtu(struct net_device *dev, int new_mtu);
+static void veth_timed_ack(unsigned long connectionPtr);
+static void veth_failMe(struct veth_lpar_connection *cnx);
+
+static struct VethFabricMgr *mFabricMgr; /* = NULL */
+static struct net_device *veth_devices[HVMAXARCHITECTEDVIRTUALLANS];
+static int veth_num_devices; /* = 0 */
+
+#define VETH_MAX_MTU		9000
+
+MODULE_AUTHOR("Kyle Lucke <klucke@us.ibm.com>");
+MODULE_DESCRIPTION("iSeries Virtual ethernet driver");
+MODULE_LICENSE("GPL");
+
+static int VethModuleReopen = 1;
+
+static inline u64 veth_dma_addr(void *p)
+{
+	return 0x8000000000000000LL | virt_to_absolute((unsigned long) p);
+}
+
+static inline HvLpEvent_Rc 
+veth_signalevent(struct veth_lpar_connection *cnx, u16 subtype, 
+		 HvLpEvent_AckInd ackind, HvLpEvent_AckType acktype,
+		 u64 token,
+		 u64 data1, u64 data2, u64 data3, u64 data4, u64 data5)
+{
+	return HvCallEvent_signalLpEventFast(cnx->remote_lp,
+					     HvLpEvent_Type_VirtualLan,
+					     subtype, ackind, acktype,
+					     cnx->src_inst,
+					     cnx->dst_inst,
+					     token, data1, data2, data3,
+					     data4, data5);
+}
+
+static inline HvLpEvent_Rc veth_signaldata(struct veth_lpar_connection *cnx,
+					   u16 subtype, u64 token, void *data)
+{
+	u64 *p = (u64 *) data;
+
+	return veth_signalevent(cnx, subtype, HvLpEvent_AckInd_NoAck,
+				HvLpEvent_AckType_ImmediateAck,
+				token, p[0], p[1], p[2], p[3], p[4]);
+}
+
+struct veth_allocation {
+	struct completion c;
+	int num;
+};
+
+static void veth_complete_allocation(void *parm, int number)
+{
+	struct veth_allocation *vc = (struct veth_allocation *)parm;
+
+	vc->num = number;
+	complete(&vc->c);
+}
+
+static int veth_allocate_events(HvLpIndex rlp, int number)
+{
+	struct veth_allocation vc = { COMPLETION_INITIALIZER(vc.c), 0 };
+
+	mf_allocateLpEvents(rlp, HvLpEvent_Type_VirtualLan,
+			    sizeof(struct VethLpEvent), number,
+			    &veth_complete_allocation, &vc);
+	wait_for_completion(&vc.c);
+
+	return vc.num;
+}
+
+struct net_device * __init veth_probe_one(int vlan)
+{
+	struct net_device *dev;
+	struct veth_port *port;
+	int i, rc;
+
+	dev = alloc_etherdev(sizeof (struct veth_port));
+	if (! dev) {
+		veth_error("Unable to allocate net_device structure!\n");
+		return NULL;
+	}
+
+	port = (struct veth_port *) dev->priv;
+
+	spin_lock_init(&port->pending_gate);
+	rwlock_init(&port->mcast_gate);
+
+	for (i = 0; i < HVMAXARCHITECTEDLPS; i++) {
+		HvLpVirtualLanIndexMap map;
+
+		if (i == mFabricMgr->this_lp)
+			continue;
+		map = HvLpConfig_getVirtualLanIndexMapForLp(i);
+		if (map & (0x8000 >> vlan))
+			port->lpar_map |= (1 << i);
+	}
+
+	dev->dev_addr[0] = 0x02;
+	dev->dev_addr[1] = 0x01;
+	dev->dev_addr[2] = 0xff;
+	dev->dev_addr[3] = vlan;
+	dev->dev_addr[4] = 0xff;
+	dev->dev_addr[5] = HvLpConfig_getLpIndex_outline();
+
+	dev->mtu = VETH_MAX_MTU;
+
+	memcpy(&port->mac_addr, dev->dev_addr, 6);
+
+	dev->open = veth_open;
+	dev->hard_start_xmit = veth_start_xmit;
+	dev->stop = veth_close;
+	dev->get_stats = veth_get_stats;
+	dev->change_mtu = veth_change_mtu;
+	dev->set_mac_address = NULL;
+	dev->set_multicast_list = veth_set_multicast_list;
+	dev->do_ioctl = veth_ioctl;
+
+	rc = register_netdev(dev);
+	if (rc != 0) {
+		veth_printk(KERN_ERR,
+			    "Failed to register an ethernet device for vlan %d\n",
+			    vlan);
+		free_netdev(dev);
+		return NULL;
+	}
+
+	veth_printk(KERN_DEBUG, "%s attached to iSeries vlan %d (lpar_map=0x%04x)\n",
+		    dev->name, vlan, port->lpar_map);
+
+	return dev;
+}
+
+int __init veth_probe(void)
+{
+	HvLpIndexMap vlan_map = HvLpConfig_getVirtualLanIndexMap();
+	int i;
+
+	memset(veth_devices, 0, sizeof(veth_devices));
+
+	for (i = 0; vlan_map != 0; vlan_map <<= 1, i++) {
+		struct net_device *dev = NULL;
+
+		if (! (vlan_map & 0x8000))
+			continue;
+
+		dev = veth_probe_one(i);
+
+		if (dev) {
+			mFabricMgr->netdev[i] = dev;
+			veth_devices[veth_num_devices] = dev;
+			veth_num_devices++;
+		}
+	}
+
+	if (veth_num_devices == 0)
+		return -ENODEV;
+
+	return 0;
+}
+
+void __exit veth_module_cleanup(void)
+{
+	int i;
+	struct VethFabricMgr *fm = mFabricMgr;
+
+	if (! mFabricMgr)
+		return;
+
+	VethModuleReopen = 0;
+	
+	for (i = 0; i < HVMAXARCHITECTEDLPS; ++i) {
+		struct veth_lpar_connection *cnx = &mFabricMgr->connection[i];
+		unsigned long flags;
+
+		spin_lock_irqsave(&cnx->status_gate, flags);
+		veth_close_connection(i);
+		spin_unlock_irqrestore(&cnx->status_gate, flags);
+	}
+	
+	flush_scheduled_work();
+	
+	HvLpEvent_unregisterHandler(HvLpEvent_Type_VirtualLan);
+	
+	mb();
+	mFabricMgr = NULL;
+	mb();
+	
+	for (i = 0; i < HVMAXARCHITECTEDLPS; ++i) {
+		struct veth_lpar_connection *cnx = &fm->connection[i];
+
+		if (cnx->mNumberAllocated + cnx->mNumberRcvMsgs > 0) {
+			mf_deallocateLpEvents(cnx->remote_lp,
+					      HvLpEvent_Type_VirtualLan,
+					      cnx->mNumberAllocated
+					      + cnx->mNumberRcvMsgs,
+					      NULL, NULL);
+		}
+		
+		if (cnx->msgs)
+			kfree(cnx->msgs);
+	}
+	
+	for (i = 0; i < HVMAXARCHITECTEDVIRTUALLANS; ++i) {
+		struct net_device *dev;
+
+		if (! fm->netdev[i])
+			continue;
+
+		dev = fm->netdev[i];
+		fm->netdev[i] = NULL;
+
+		mb();
+			
+		if (dev) {
+			unregister_netdev(dev);
+			free_netdev(dev);
+		}
+	}
+	
+	kfree(fm);
+}
+
+module_exit(veth_module_cleanup);
+
+int __init veth_module_init(void)
+{
+	int i;
+	int this_lp;
+	int rc;
+
+	mFabricMgr = kmalloc(sizeof (struct VethFabricMgr), GFP_KERNEL);
+	if (! mFabricMgr) {
+		veth_error("Unable to allocate fabric manager\n");
+		return -ENOMEM;
+	}
+
+	memset(mFabricMgr, 0, sizeof (*mFabricMgr));
+
+	this_lp = HvLpConfig_getLpIndex_outline();
+	mFabricMgr->this_lp = this_lp;
+
+	for (i = 0; i < HVMAXARCHITECTEDLPS; ++i) {
+		struct veth_lpar_connection *cnx = &mFabricMgr->connection[i];
+
+		veth_init_connection(cnx, i);
+	}
+
+	rc = veth_probe();
+	if (rc != 0) {
+		veth_module_cleanup();
+		return rc;
+	}
+
+	HvLpEvent_registerHandler(HvLpEvent_Type_VirtualLan, &veth_handle_event);
+
+	/* Run through the active lps and open connections to the ones
+	 * we need to */
+	/* FIXME: is there any reason to do this backwards? */
+	for (i = HVMAXARCHITECTEDLPS - 1; i >= 0; --i) {
+		struct veth_lpar_connection *cnx = &mFabricMgr->connection[i];
+
+		if ( (i == this_lp) 
+		     || ! HvLpConfig_doLpsCommunicateOnVirtualLan(this_lp, i) )
+			continue;
+
+		spin_lock_irq(&cnx->status_gate);
+		veth_open_connection(i);
+		spin_unlock_irq(&cnx->status_gate);
+	}
+
+	return 0;
+}
+
+module_init(veth_module_init);
+
+static int veth_open(struct net_device *dev)
+{
+	struct veth_port *port = (struct veth_port *) dev->priv;
+
+	memset(&port->stats, 0, sizeof (port->stats));
+
+	netif_start_queue(dev);
+
+	return 0;
+}
+
+static int veth_close(struct net_device *dev)
+{
+	netif_stop_queue(dev);
+
+	return 0;
+}
+
+static struct net_device_stats *veth_get_stats(struct net_device *dev)
+{
+	struct veth_port *port = (struct veth_port *) dev->priv;
+
+	return &port->stats;
+}
+
+static int veth_change_mtu(struct net_device *dev, int new_mtu)
+{
+	if ((new_mtu < 68) || (new_mtu > VETH_MAX_MTU))
+		return -EINVAL;
+	dev->mtu = new_mtu;
+	return 0;
+}
+
+static int veth_transmit_to_one(struct sk_buff *skb, HvLpIndex rlp,
+				struct net_device *dev)
+{
+	struct veth_lpar_connection *cnx = mFabricMgr->connection + rlp;
+	HvLpEvent_Rc rc;
+	u32 dma_address, dma_length;
+	struct veth_msg *msg = NULL;
+
+	if (! cnx->status.ready)
+		return 2;
+
+	if ((skb->len - 14) > VETH_MAX_MTU)
+		return 2;
+
+	VETHSTACKPOP(&cnx->msg_stack, msg);
+
+	if (! msg)
+		return 1;
+
+	dma_length = skb->len;
+	dma_address = pci_map_single(iSeries_veth_dev, skb->data,
+				     dma_length, PCI_DMA_TODEVICE);
+	
+	/* Is it really necessary to check the length and address
+	 * fields of the first entry here? */
+	if (dma_address != NO_TCE) {
+		msg->skb = skb;
+		msg->data.addr[0] = dma_address;
+		msg->data.len[0] = dma_length;
+		msg->data.eof = 1;
+		set_bit(0, &(msg->in_use));
+		rc = veth_signaldata(cnx, VethEventTypeFrames,
+				     msg->token, &msg->data);
+	} else {
+		struct veth_port *port = (struct veth_port *) dev->priv;
+		rc = -1;	/* Bad return code */
+		port->stats.tx_errors++;
+	}
+	
+	if (rc != HvLpEvent_Rc_Good) {
+		msg->skb = NULL;
+		/* need to set in use to make veth_recycle_msg in case
+		 * this was a mapping failure */
+		set_bit(0, &msg->in_use);
+		veth_recycle_msg(cnx, msg);
+		return 2;
+	}
+
+	return 0;
+}
+
+static HvLpIndexMap veth_transmit_to_many(struct sk_buff *skb,
+					  HvLpIndexMap lpmask,
+					  struct net_device *dev)
+{
+	struct veth_port *port = (struct veth_port *) dev->priv;
+	int i;
+	int rc;
+
+	for (i = 0; i < HVMAXARCHITECTEDLPS; i++) {
+		struct sk_buff *clone;
+
+		if (! lpmask & (1<<i))
+			continue;
+		
+		clone = skb_clone(skb, GFP_ATOMIC);
+
+		if (! clone) {
+			veth_error("%s: skb_clone failed %p\n",
+				   dev->name, skb);
+			continue;
+		}
+		/* the ack handles deleting the skb */
+		rc = veth_transmit_to_one(clone, i, dev);
+		
+		/* tx failed, we need to free the skb */
+		if (rc != 0)
+			dev_kfree_skb(clone);
+			
+		/* if we didn't fail from lack of buffers, the tx as a
+		 * whole is successful */
+		if (rc != 1)
+			lpmask &= ~(1<<i);
+	}
+
+	if (! lpmask) {
+		port->stats.tx_packets++;
+		port->stats.tx_bytes += skb->len;
+	}
+
+	return lpmask;
+}
+
+static int veth_start_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	unsigned char *frame = skb->data;
+	struct veth_port *port = (struct veth_port *) dev->priv;
+	unsigned long flags;
+	HvLpIndexMap lpmask;
+
+	BUG_ON(! mFabricMgr);
+
+	if (! (frame[0] & 0x01)) {
+		/* unicast packet */
+		HvLpIndex rlp = frame[5];
+
+		if ( ! ((1 << rlp) & port->lpar_map) ) {
+			dev_kfree_skb(skb);
+			return 0;
+		}
+
+		lpmask = 1 << rlp;
+	} else {
+		lpmask = port->lpar_map;
+	}
+
+	lpmask = veth_transmit_to_many(skb, lpmask, dev);
+
+	if (lpmask) {
+		spin_lock_irqsave(&port->pending_gate, flags);
+		if (port->pending_skb) {
+			veth_error("%s: Tx while skb was pending!\n", dev->name);
+			dev_kfree_skb(skb);
+			return 1;
+		}
+
+		port->pending_skb = skb;
+		port->pending_lpmask = lpmask;
+		netif_stop_queue(dev);
+
+		spin_unlock_irqrestore(&port->pending_gate, flags);
+	}
+
+	dev_kfree_skb(skb);
+	return 0;
+}
+
+static int veth_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
+{
+#ifdef SIOCETHTOOL
+	struct ethtool_cmd ecmd;
+
+	if (cmd != SIOCETHTOOL)
+		return -EOPNOTSUPP;
+	if (copy_from_user(&ecmd, ifr->ifr_data, sizeof (ecmd)))
+		return -EFAULT;
+	switch (ecmd.cmd) {
+	case ETHTOOL_GSET:
+		ecmd.supported = (SUPPORTED_1000baseT_Full
+				  | SUPPORTED_Autoneg | SUPPORTED_FIBRE);
+		ecmd.advertising = (SUPPORTED_1000baseT_Full
+				    | SUPPORTED_Autoneg | SUPPORTED_FIBRE);
+
+		ecmd.port = PORT_FIBRE;
+		ecmd.transceiver = XCVR_INTERNAL;
+		ecmd.phy_address = 0;
+		ecmd.speed = SPEED_1000;
+		ecmd.duplex = DUPLEX_FULL;
+		ecmd.autoneg = AUTONEG_ENABLE;
+		ecmd.maxtxpkt = 120;
+		ecmd.maxrxpkt = 120;
+		if (copy_to_user(ifr->ifr_data, &ecmd, sizeof(ecmd)))
+			return -EFAULT;
+		return 0;
+
+	case ETHTOOL_GDRVINFO:{
+			struct ethtool_drvinfo info = { ETHTOOL_GDRVINFO };
+			strncpy(info.driver, "veth", sizeof(info.driver) - 1);
+			info.driver[sizeof(info.driver) - 1] = '\0';
+			strncpy(info.version, "1.0", sizeof(info.version) - 1);
+			if (copy_to_user(ifr->ifr_data, &info, sizeof(info)))
+				return -EFAULT;
+			return 0;
+		}
+		/* get link status */
+	case ETHTOOL_GLINK:{
+			struct ethtool_value edata = { ETHTOOL_GLINK };
+			edata.data = 1;
+			if (copy_to_user(ifr->ifr_data, &edata, sizeof(edata)))
+				return -EFAULT;
+			return 0;
+		}
+
+	default:
+		break;
+	}
+
+#endif
+	return -EOPNOTSUPP;
+}
+
+static void veth_set_multicast_list(struct net_device *dev)
+{
+	struct veth_port *port = (struct veth_port *) dev->priv;
+	unsigned long flags;
+
+	write_lock_irqsave(&port->mcast_gate, flags);
+
+	if (dev->flags & IFF_PROMISC) {	/* set promiscuous mode */
+		printk(KERN_INFO "%s: Promiscuous mode enabled.\n",
+		       dev->name);
+		port->promiscuous = 1;
+	} else if ( (dev->flags & IFF_ALLMULTI)
+		    || (dev->mc_count > VETH_MAX_MCAST) ) {
+		port->all_mcast = 1;
+	} else {
+		struct dev_mc_list *dmi = dev->mc_list;
+		int i;
+
+		/* Update table */
+		port->num_mcast = 0;
+		
+		for (i = 0; i < dev->mc_count; i++) {
+			u8 *addr = dmi->dmi_addr;
+			u64 xaddr = 0;
+
+			if (addr[0] & 0x01) {/* multicast address? */
+				memcpy(&xaddr, addr, ETH_ALEN);
+				port->mcast_addr[port->num_mcast] = xaddr;
+				port->num_mcast++;
+			}
+			dmi = dmi->next;
+		}
+	}
+
+	write_unlock_irqrestore(&port->mcast_gate, flags);
+}
+
+static void veth_handle_event(struct HvLpEvent *event, struct pt_regs *regs)
+{
+	struct VethLpEvent *veth_event = (struct VethLpEvent *)event;
+
+	if (event->xFlags.xFunction == HvLpEvent_Function_Ack)
+		veth_handle_ack(veth_event);
+	else if (event->xFlags.xFunction == HvLpEvent_Function_Int)
+		veth_handle_int(veth_event);
+}
+
+static void veth_handle_ack(struct VethLpEvent *event)
+{
+	HvLpIndex rlp = event->base_event.xTargetLp;
+	struct veth_lpar_connection *cnx = &mFabricMgr->connection[rlp];
+
+	switch (event->base_event.xSubtype) {
+	case VethEventTypeCap:
+		veth_take_cap_ack(cnx, event);
+		break;
+	case VethEventTypeMonitor:
+		veth_take_monitor_ack(cnx, event);
+		break;
+	default:
+		veth_error("Unknown ack type %d from lpar %d\n",
+			   event->base_event.xSubtype, rlp);
+	};
+}
+
+static void veth_handle_int(struct VethLpEvent *event)
+{
+	HvLpIndex rlp = event->base_event.xSourceLp;
+	struct veth_lpar_connection *cnx = &mFabricMgr->connection[rlp];
+	int i;
+
+	switch (event->base_event.xSubtype) {
+	case VethEventTypeCap:
+		veth_take_cap(cnx, event);
+		break;
+	case VethEventTypeMonitor:
+		/* do nothing... this'll hang out here til we're dead,
+		 * and the hypervisor will return it for us. */
+		break;
+	case VethEventTypeFramesAck:
+		for (i = 0; i < VETH_MAX_ACKS_PER_MSG; ++i) {
+			u16 msgnum = event->u.frames_ack_data.token[i];
+
+			if (msgnum < cnx->num_msgs)
+				veth_recycle_msg(cnx, cnx->msgs + msgnum);
+		}
+		break;
+	case VethEventTypeFrames:
+		veth_receive(cnx, event);
+		break;
+	default:
+		veth_error("Unknown interrupt type %d from lpar %d\n",
+			   event->base_event.xSubtype, rlp);
+	};
+}
+
+static void veth_failMe(struct veth_lpar_connection *cnx)
+{
+	cnx->status.ready = 0;
+}
+
+static void veth_init_connection(struct veth_lpar_connection *cnx, u8 rlp)
+{
+	struct veth_msg *msgs;
+	HvLpIndex this_lp = mFabricMgr->this_lp;
+	int i;
+
+	veth_failMe(cnx);
+
+	cnx->remote_lp = rlp;
+
+	spin_lock_init(&cnx->ack_gate);
+	spin_lock_init(&cnx->status_gate);
+
+	cnx->status.got_cap = 0;
+	cnx->status.got_cap_ack = 0;
+
+	INIT_WORK(&cnx->finish_open_wq, veth_finish_open_connection, cnx);
+	INIT_WORK(&cnx->monitor_ack_wq, veth_monitor_ack_task, cnx);
+
+	init_timer(&cnx->ack_timer);
+	cnx->ack_timer.function = veth_timed_ack;
+	cnx->ack_timer.data = (unsigned long) cnx;
+
+	if ( (rlp == this_lp) 
+	     || ! HvLpConfig_doLpsCommunicateOnVirtualLan(this_lp, rlp) )
+		return;
+
+	msgs = kmalloc(VETH_NUMBUFFERS * sizeof(struct veth_msg), GFP_KERNEL);
+	if (! msgs)
+		return;
+
+	cnx->msgs = msgs;
+	memset(msgs, 0, VETH_NUMBUFFERS * sizeof(struct veth_msg));
+	spin_lock_init(&cnx->msg_stack.lock);
+
+	for (i = 0; i < VETH_NUMBUFFERS; i++) {
+		msgs[i].token = i;
+		VETHSTACKPUSH(&cnx->msg_stack, msgs + i);
+	}
+
+	cnx->num_msgs = VETH_NUMBUFFERS;
+
+	cnx->mNumberAllocated = veth_allocate_events(rlp, 2);
+
+	if (cnx->mNumberAllocated < 2) {
+		veth_error("Couldn't allocate base msgs for lpar %d, only got %d\n",
+			   cnx->remote_lp, cnx->mNumberAllocated);
+		veth_failMe(cnx);
+		return;
+	}
+
+	cnx->mNumberRcvMsgs = veth_allocate_events(cnx->remote_lp,
+						   VETH_NUMBUFFERS);
+}
+
+static void veth_open_connection(u8 rlp)
+{
+	struct veth_lpar_connection *cnx = &mFabricMgr->connection[rlp];
+	HvLpEvent_Rc rc;
+	u64 *rawcap = (u64 *) &cnx->local_caps;
+
+	if (! cnx->msgs || (cnx->mNumberAllocated < 2)
+	    || ! cnx->mNumberRcvMsgs) {
+		veth_failMe(cnx);
+		return;
+	}
+
+	spin_lock_irq(&cnx->ack_gate);
+
+	memset(&cnx->pending_acks, 0xff, sizeof (cnx->pending_acks));
+	cnx->num_pending_acks = 0;
+
+	HvCallEvent_openLpEventPath(rlp, HvLpEvent_Type_VirtualLan);
+
+	cnx->status.open = 1;
+
+	cnx->src_inst = 
+		HvCallEvent_getSourceLpInstanceId(rlp,
+						  HvLpEvent_Type_VirtualLan);
+	cnx->dst_inst =
+		HvCallEvent_getTargetLpInstanceId(rlp,
+						  HvLpEvent_Type_VirtualLan);
+
+	spin_unlock_irq(&cnx->ack_gate);
+
+	cnx->local_caps.num_buffers = cnx->num_msgs;
+
+	if (cnx->num_msgs < 10)
+		cnx->local_caps.ack_threshold = 1;
+	else if (cnx->num_msgs < 20)
+		cnx->local_caps.ack_threshold = 4;
+	else if (cnx->num_msgs < 40)
+		cnx->local_caps.ack_threshold = 10;
+	else
+		cnx->local_caps.ack_threshold = 20;
+
+	cnx->local_caps.ack_timeout = VETH_ACKTIMEOUT;
+
+	rc = veth_signalevent(cnx, VethEventTypeCap,
+			      HvLpEvent_AckInd_DoAck,
+			      HvLpEvent_AckType_ImmediateAck,
+			      0, rawcap[0], rawcap[1], rawcap[2], rawcap[3],
+			      rawcap[4]);
+
+	if ( (rc == HvLpEvent_Rc_PartitionDead)
+	     || (rc == HvLpEvent_Rc_PathClosed)) {
+		/* Never mind we'll resend out caps when we get the
+		 * caps from the other end comes up and sends
+		 * theirs */
+		return;
+	} else if (rc != HvLpEvent_Rc_Good) {
+		veth_error("Couldn't send capabilities to lpar %d, rc=%x\n",
+				  cnx->remote_lp, (int) rc);
+		veth_failMe(cnx);
+		return;
+	}
+
+	cnx->status.sent_caps = 1;
+}
+
+static void veth_finish_open_connection(void *parm)
+{
+	struct veth_lpar_connection *cnx = (struct veth_lpar_connection *)parm;
+	struct VethCapData *remote_caps = &cnx->remote_caps;
+	u64 num_acks_needed = 0;
+	HvLpEvent_Rc rc;
+
+	spin_lock_irq(&cnx->status_gate);
+
+	memcpy(remote_caps, &cnx->cap_event.u.caps_data,
+	       sizeof(*remote_caps));
+
+	/* Convert timer to jiffies */
+	if (cnx->local_caps.ack_timeout)
+		cnx->ack_timeout = remote_caps->ack_timeout * HZ / 1000000;
+	else
+		cnx->ack_timeout = VETH_ACKTIMEOUT * HZ / 1000000;
+
+	if ( (remote_caps->num_buffers == 0)
+	     || (remote_caps->ack_threshold > VETH_MAX_ACKS_PER_MSG)
+	     || (remote_caps->ack_threshold == 0) 
+	     || (cnx->ack_timeout == 0) ) {
+		veth_error("Received incompatible capabilities from lpar %d\n",
+			   cnx->remote_lp);
+		cnx->cap_event.base_event.xRc = HvLpEvent_Rc_InvalidSubtypeData;
+		HvCallEvent_ackLpEvent((struct HvLpEvent *)&cnx->cap_event);
+
+		veth_failMe(cnx);
+		goto out;
+	}
+
+	num_acks_needed = (remote_caps->num_buffers / remote_caps->ack_threshold) + 1;
+
+	if (cnx->mNumberLpAcksAlloced < num_acks_needed) {
+		int num;
+		
+		num_acks_needed = num_acks_needed - cnx->mNumberLpAcksAlloced;
+		
+		spin_unlock_irq(&cnx->status_gate);
+		
+		num = veth_allocate_events(cnx->remote_lp, num_acks_needed);
+		
+		if (num > 0)
+			cnx->mNumberLpAcksAlloced += num;
+		spin_lock_irq(&cnx->status_gate);
+	}
+	
+	if (cnx->mNumberLpAcksAlloced < num_acks_needed) {
+		veth_error("Couldn't allocate all the frames ack events for lpar %d\n",
+			   cnx->remote_lp);
+
+		cnx->cap_event.base_event.xRc = HvLpEvent_Rc_BufferNotAvailable;
+		HvCallEvent_ackLpEvent((struct HvLpEvent *)&cnx->cap_event);
+
+		veth_failMe(cnx);
+		goto out;
+	}
+
+	rc = HvCallEvent_ackLpEvent((struct HvLpEvent *)&cnx->cap_event);
+	if (rc != HvLpEvent_Rc_Good) {
+		veth_error("Failed to ack remote cap for lpar %d with rc %x\n",
+			   cnx->remote_lp, (int) rc);
+		veth_failMe(cnx);
+		goto out;
+	}
+
+	if (cnx->cap_ack_event.base_event.xRc != HvLpEvent_Rc_Good) {
+		veth_printk(KERN_ERR, "Bad rc(%d) from lpar %d on capabilities\n",
+			    cnx->cap_ack_event.base_event.xRc, cnx->remote_lp);
+		veth_failMe(cnx);
+		goto out;
+	}
+
+	/* Send the monitor */
+	rc = veth_signalevent(cnx, VethEventTypeMonitor,
+			      HvLpEvent_AckInd_DoAck,
+			      HvLpEvent_AckType_DeferredAck,
+			      0, 0, 0, 0, 0, 0);
+	
+	if (rc != HvLpEvent_Rc_Good) {
+		veth_error("Monitor send to lpar %d failed with rc %x\n",
+				  cnx->remote_lp, (int) rc);
+		veth_failMe(cnx);
+		goto out;
+	}
+
+	cnx->status.ready = 1;
+	
+	/* Start the ACK timer */
+	cnx->ack_timer.expires = jiffies + cnx->ack_timeout;
+	add_timer(&cnx->ack_timer);
+
+ out:
+	spin_unlock_irq(&cnx->status_gate);
+}
+
+static void veth_close_connection(u8 rlp)
+{
+	struct veth_lpar_connection *cnx = &mFabricMgr->connection[rlp];
+	unsigned long flags;
+
+	del_timer_sync(&cnx->ack_timer);
+
+	cnx->status.sent_caps = 0;
+	cnx->status.got_cap = 0;
+	cnx->status.got_cap_ack = 0;
+
+	if (cnx->status.open) {
+		int i;		
+
+		HvCallEvent_closeLpEventPath(rlp, HvLpEvent_Type_VirtualLan);
+		cnx->status.open = 0;
+		veth_failMe(cnx);
+
+		/* reset ack data */
+		spin_lock_irqsave(&cnx->ack_gate, flags);
+
+		memset(&cnx->pending_acks, 0xff, sizeof (cnx->pending_acks));
+		cnx->num_pending_acks = 0;
+
+		spin_unlock_irqrestore(&cnx->ack_gate, flags);
+
+		/* Clean up any leftover messages */
+		for (i = 0; i < cnx->num_msgs; ++i)
+			veth_recycle_msg(cnx, cnx->msgs + i);
+	}
+
+}
+
+static void veth_take_cap(struct veth_lpar_connection *cnx, struct VethLpEvent *event)
+{
+	unsigned long flags;
+	HvLpEvent_Rc rc;
+
+	spin_lock_irqsave(&cnx->status_gate, flags);
+
+	if (cnx->status.got_cap) {
+		veth_error("Received a second capabilities from lpar %d\n",
+			   cnx->remote_lp);
+		event->base_event.xRc = HvLpEvent_Rc_BufferNotAvailable;
+		HvCallEvent_ackLpEvent((struct HvLpEvent *) event);
+		goto out;
+	}
+
+	memcpy(&cnx->cap_event, event, sizeof (cnx->cap_event));
+	/* If we failed to send caps out before (presumably because
+	 * the target lpar was down), send them now. */
+	if (! cnx->status.sent_caps) {
+		u64 *rawcap = (u64 *) &cnx->local_caps;
+
+		cnx->dst_inst =
+			HvCallEvent_getTargetLpInstanceId(cnx->remote_lp,
+							  HvLpEvent_Type_VirtualLan);
+
+		rc = veth_signalevent(cnx, VethEventTypeCap,
+				      HvLpEvent_AckInd_DoAck,
+				      HvLpEvent_AckType_ImmediateAck,
+				      0, rawcap[0], rawcap[1], rawcap[2], rawcap[3],
+				      rawcap[4]);
+		if ( (rc == HvLpEvent_Rc_PartitionDead)
+		     || (rc == HvLpEvent_Rc_PathClosed)) {
+			veth_error("Partition down when resending capabilities!!\n");
+			goto out;
+		} else if (rc != HvLpEvent_Rc_Good) {
+			veth_error("Couldn't send cap to lpar %d, rc %x\n",
+				   cnx->remote_lp, (int) rc);
+			veth_failMe(cnx);
+			goto out;
+		}
+		cnx->status.sent_caps = 1;
+	}
+
+	cnx->status.got_cap = 1;
+	if (cnx->status.got_cap_ack)
+		schedule_work(&cnx->finish_open_wq);
+
+ out:
+	spin_unlock_irqrestore(&cnx->status_gate, flags);
+}
+
+static void veth_take_cap_ack(struct veth_lpar_connection *cnx,
+			      struct VethLpEvent *event)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&cnx->status_gate, flags);
+
+	if (cnx->status.got_cap_ack) {
+		veth_error("Received a second capabilities ack from lpar %d\n",
+			   cnx->remote_lp);
+		goto out;
+	}
+
+	memcpy(&cnx->cap_ack_event, event, sizeof(&cnx->cap_ack_event));
+	cnx->status.got_cap_ack = 1;
+
+	if (cnx->status.got_cap)
+		schedule_work(&cnx->finish_open_wq);
+
+ out:
+	spin_unlock_irqrestore(&cnx->status_gate, flags);
+}
+
+static void veth_take_monitor_ack(struct veth_lpar_connection *cnx,
+				  struct VethLpEvent *event)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&cnx->status_gate, flags);
+
+	veth_printk(KERN_DEBUG, "Monitor ack returned for lpar %d\n", cnx->remote_lp);
+
+	if (cnx->status.monitor_ack_pending) {
+		veth_error("Received a monitor ack from lpar %d while already processing one\n",
+			   cnx->remote_lp);
+		goto out;
+	}
+
+	schedule_work(&cnx->monitor_ack_wq);
+
+ out:
+	spin_unlock_irqrestore(&cnx->status_gate, flags);
+}
+
+static void veth_recycle_msg(struct veth_lpar_connection *cnx,
+			     struct veth_msg *myMsg)
+{
+	u32 dma_address, dma_length;
+	int i;
+
+	if (test_and_clear_bit(0, &myMsg->in_use)) {
+		dma_address = myMsg->data.addr[0];
+		dma_length = myMsg->data.len[0];
+
+		pci_unmap_single(iSeries_veth_dev, dma_address, dma_length,
+				 PCI_DMA_TODEVICE);
+
+		if (myMsg->skb) {
+			dev_kfree_skb_any(myMsg->skb);
+			myMsg->skb = NULL;
+		}
+
+		memset(&myMsg->data, 0, sizeof(myMsg->data));
+		VETHSTACKPUSH(&cnx->msg_stack, myMsg);
+	} else {
+		if (cnx->status.open)
+			veth_error("Bogus frames ack from lpar %d (index=%d)\n",
+				   cnx->remote_lp, myMsg->token);
+	}
+
+	for (i = 0; i < veth_num_devices; i++) {
+		struct net_device *dev = veth_devices[i];
+		struct veth_port *port = (struct veth_port *)dev->priv;
+		unsigned long flags;
+
+		if (! (port->lpar_map & (1<<cnx->remote_lp)))
+			continue;
+
+		spin_lock_irqsave(&port->pending_gate, flags);
+		if (port->pending_skb) {
+			port->pending_lpmask =
+				veth_transmit_to_many(port->pending_skb,
+						      port->pending_lpmask,
+						      dev);
+			if (! port->pending_lpmask) {
+				dev_kfree_skb_any(port->pending_skb);
+				port->pending_skb = NULL;
+				netif_start_queue(dev);
+			}
+		}
+		spin_unlock_irqrestore(&port->pending_gate, flags);
+	}
+}
+
+static void veth_monitor_ack_task(void *parm)
+{
+	struct veth_lpar_connection *cnx = (struct veth_lpar_connection *) parm;
+
+	spin_lock_irq(&cnx->status_gate);
+
+	veth_failMe(cnx);
+
+	if (cnx->status.open) {
+		veth_close_connection(cnx->remote_lp);
+
+		udelay(100);
+	}
+
+	if (VethModuleReopen)
+		veth_open_connection(cnx->remote_lp);
+
+	cnx->status.monitor_ack_pending = 0;
+
+	spin_unlock_irq(&cnx->status_gate);
+}
+
+static inline int veth_frame_wanted(struct veth_port *port, u64 mac_addr)
+{
+	int wanted = 0;
+	int i;
+	unsigned long flags;
+
+	if ( (mac_addr == port->mac_addr)
+	     || (mac_addr == 0xffffffffffff0000)
+	     || port->promiscuous )
+		return 1;
+	
+	if (! (((char *) &mac_addr)[0] & 0x01))
+		return 0;
+
+	read_lock_irqsave(&port->mcast_gate, flags);
+
+	if (port->all_mcast) {
+		wanted = 1;
+		goto out;
+	}
+
+	for (i = 0; i < port->num_mcast; ++i) {
+		if (port->mcast_addr[i] == mac_addr) {
+			wanted = 1;
+			break;
+		}
+	}
+
+ out:
+	read_unlock_irqrestore(&port->mcast_gate, flags);
+
+	return wanted;
+}
+
+struct dma_chunk {
+	u64 addr;
+	u64 size;
+};
+
+#define VETH_MAX_PAGES_PER_FRAME ( (VETH_MAX_MTU+PAGE_SIZE-2)/PAGE_SIZE + 1 )
+
+static inline void veth_build_dma_list(struct dma_chunk *list, unsigned char *p,
+				      unsigned long length)
+{
+	unsigned long done;
+	int i = 1;
+
+	/* FIXME: skbs are continguous in real addresses.  Do we
+	 * really need to break it into PAGE_SIZE chunks, or can we do
+	 * it just at the granularity of iSeries real->absolute
+	 * mapping? */
+	list[0].addr = veth_dma_addr(p);
+	list[0].size = min(length,
+			   PAGE_SIZE - ((unsigned long)p & ~PAGE_MASK));
+
+	done = list[0].size;
+	while (done < length) {
+		list[i].addr = veth_dma_addr(p + done);
+		list[i].size = min(done, PAGE_SIZE);
+		done += list[i].size;
+		i++;
+	}
+}
+
+static void veth_receive(struct veth_lpar_connection *cnx, struct VethLpEvent *event)
+{
+	struct VethFramesData *senddata = &event->u.frames_data;
+	int startchunk = 0;
+	int nchunks;
+	unsigned long flags;
+	HvLpDma_Rc rc;
+
+	do {
+		u16 length = 0;
+		struct sk_buff *skb;
+		struct dma_chunk local_list[VETH_MAX_PAGES_PER_FRAME];
+		struct dma_chunk remote_list[VETH_MAX_FRAMES_PER_MSG];
+		u64 dest;
+		HvLpVirtualLanIndex vlan;
+		struct net_device *dev;
+		struct veth_port *port;
+
+		/* FIXME: do we need this? */
+		memset(local_list, 0, sizeof(local_list));
+		memset(remote_list, 0, sizeof(VETH_MAX_FRAMES_PER_MSG));
+
+		nchunks = 0;
+
+		/* a 0 address marks the end of the valid entries */
+		if (senddata->addr[startchunk] == 0)
+			break;
+
+		/* make sure that we have at least 1 EOF entry in the
+		 * remaining entries */
+		if (! (senddata->eof >> startchunk)) {
+			veth_error("missing EOF frag in event: 0x%x startchunk=%d\n",
+				   (unsigned) senddata->eof, startchunk);
+			break;
+		}
+
+		/* build list of chunks in this frame */
+		do {
+			remote_list[nchunks].addr =
+				(u64) senddata->addr[startchunk + nchunks] << 32;
+			remote_list[nchunks].size =
+				senddata->len[startchunk + nchunks];
+			length += remote_list[nchunks].size;
+		} while (! (senddata->eof & (1 << (startchunk + nchunks++))));
+
+		/* length == total length of all chunks */
+		/* nchunks == # of chunks in this frame */
+
+		if ((length - ETH_HLEN) > VETH_MAX_MTU)
+			continue;
+
+		skb = alloc_skb(length, GFP_ATOMIC);
+		if (!skb)
+			continue;
+
+		veth_build_dma_list(local_list, skb->data, length);
+
+		rc = HvCallEvent_dmaBufList(HvLpEvent_Type_VirtualLan,
+					    event->base_event.xSourceLp,
+					    HvLpDma_Direction_RemoteToLocal,
+					    cnx->src_inst,
+					    cnx->dst_inst,
+					    HvLpDma_AddressType_RealAddress,
+					    HvLpDma_AddressType_TceIndex,
+					    veth_dma_addr(&local_list),
+					    veth_dma_addr(&remote_list),
+					    length);
+		if (rc != HvLpDma_Rc_Good) {
+			dev_kfree_skb_irq(skb);
+			continue;
+		}
+		
+		vlan = skb->data[9];
+		dev = mFabricMgr->netdev[vlan];
+		port = (struct veth_port *)dev->priv;
+		dest = *((u64 *) skb->data) & 0xFFFFFFFFFFFF0000;
+
+		if ((vlan > HVMAXARCHITECTEDVIRTUALLANS) || !port) {
+			dev_kfree_skb_irq(skb);
+			continue;
+		}
+		if (! veth_frame_wanted(port, dest)) {
+			dev_kfree_skb_irq(skb);
+			continue;
+		}
+			
+		skb_put(skb, length);
+		skb->dev = dev;
+		skb->protocol = eth_type_trans(skb, dev);
+		skb->ip_summed = CHECKSUM_NONE;
+		netif_rx(skb);	/* send it up */
+		port->stats.rx_packets++;
+		port->stats.rx_bytes += length;
+	} while (startchunk += nchunks, startchunk < VETH_MAX_FRAMES_PER_MSG);
+
+	/* Ack it */
+	spin_lock_irqsave(&cnx->ack_gate, flags);
+	
+	if (cnx->num_pending_acks < VETH_MAX_ACKS_PER_MSG) {
+		cnx->pending_acks[cnx->num_pending_acks] =
+			event->base_event.xCorrelationToken;
+		++cnx->num_pending_acks;
+		
+		if (cnx->num_pending_acks == cnx->remote_caps.ack_threshold) {
+			rc = veth_signaldata(cnx, VethEventTypeFramesAck,
+					     0, &cnx->pending_acks);
+			
+			if (rc != HvLpEvent_Rc_Good)
+				veth_error("Error 0x%x acking frames from lpar %d!\n",
+					   (unsigned)rc, cnx->remote_lp);
+			
+			cnx->num_pending_acks = 0;
+			memset(&cnx->pending_acks, 0xff, sizeof(cnx->pending_acks));
+		}
+		
+	}
+	
+	spin_unlock_irqrestore(&cnx->ack_gate, flags);
+}
+
+static void veth_timed_ack(unsigned long ptr)
+{
+	unsigned long flags;
+	HvLpEvent_Rc rc;
+	struct veth_lpar_connection *cnx = (struct veth_lpar_connection *) ptr;
+
+	/* Ack all the events */
+	spin_lock_irqsave(&cnx->ack_gate, flags);
+
+	if (cnx->num_pending_acks > 0) {
+		rc = veth_signaldata(cnx, VethEventTypeFramesAck,
+				     0, &cnx->pending_acks);
+		if (rc != HvLpEvent_Rc_Good)
+			veth_error("Error 0x%x acking frames from lpar %d!\n", 
+				   (unsigned) rc, cnx->remote_lp);
+
+		cnx->num_pending_acks = 0;
+		memset(&cnx->pending_acks, 0xff, sizeof(cnx->pending_acks));
+	}
+
+	spin_unlock_irqrestore(&cnx->ack_gate, flags);
+
+	/* Reschedule the timer */
+	cnx->ack_timer.expires = jiffies + cnx->ack_timeout;
+	add_timer(&cnx->ack_timer);
+}
diff -purN linux-2.5/drivers/net/iseries_veth.h linuxppc64-2.5/drivers/net/iseries_veth.h
--- linux-2.5/drivers/net/iseries_veth.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/net/iseries_veth.h	2003-12-31 05:38:48.000000000 +0000
@@ -0,0 +1,141 @@
+/* File veth.h created by Kyle A. Lucke on Mon Aug  7 2000. */
+
+#ifndef _ISERIES_VETH_H
+#define _ISERIES_VETH_H
+
+#define VethEventTypeCap	(0)
+#define VethEventTypeFrames	(1)
+#define VethEventTypeMonitor	(2)
+#define VethEventTypeFramesAck	(3)
+
+#define VETH_MAX_ACKS_PER_MSG	(20)
+#define VETH_MAX_FRAMES_PER_MSG	(6)
+
+struct VethFramesData {
+	u32 addr[VETH_MAX_FRAMES_PER_MSG];
+	u16 len[VETH_MAX_FRAMES_PER_MSG];
+	u32 eof:VETH_MAX_FRAMES_PER_MSG;
+	u32 mReserved:(32-VETH_MAX_FRAMES_PER_MSG);
+};
+
+struct VethFramesAckData {
+	u16 token[VETH_MAX_ACKS_PER_MSG];
+};
+
+struct VethCapData {
+	u8 caps_version;
+	u8 rsvd1;
+	u16 num_buffers;
+	u16 ack_threshold;
+	u16 rsvd2;
+	u32 ack_timeout;
+	u32 rsvd3;
+	u64 rsvd4[3];
+};
+
+struct VethLpEvent {
+	struct HvLpEvent base_event;
+	union {
+		struct VethCapData caps_data;
+		struct VethFramesData frames_data;
+		struct VethFramesAckData frames_ack_data;
+	} u;
+
+};
+
+#define VETH_ACKTIMEOUT 	(1000000) /* microseconds */
+#define VETH_MAX_MCAST		(12)
+
+#define HVMAXARCHITECTEDVIRTUALLANS (16)
+
+#define VETHSTACK(T) \
+	struct VethStack##T { \
+		struct T *head; \
+		spinlock_t lock; \
+	}
+#define VETHSTACKPUSH(s, p) \
+	do { \
+		unsigned long flags; \
+		spin_lock_irqsave(&(s)->lock,flags); \
+		(p)->next = (s)->head; \
+		(s)->head = (p); \
+		spin_unlock_irqrestore(&(s)->lock, flags); \
+	} while (0)
+
+#define VETHSTACKPOP(s,p) \
+	do { \
+		unsigned long flags; \
+		spin_lock_irqsave(&(s)->lock,flags); \
+		(p) = (s)->head; \
+		if ((s)->head) \
+			(s)->head = (s)->head->next; \
+		spin_unlock_irqrestore(&(s)->lock, flags); \
+	} while (0)
+
+struct veth_msg {
+	struct veth_msg *next;
+	struct VethFramesData data;
+	int token;
+	unsigned long in_use;
+	struct sk_buff *skb;
+};
+
+struct veth_lpar_connection {
+	HvLpIndex remote_lp;
+	struct work_struct finish_open_wq;
+	struct work_struct monitor_ack_wq;
+	struct timer_list ack_timer;
+	u32 num_msgs;
+	struct veth_msg *msgs;
+
+	HvLpInstanceId src_inst;
+	HvLpInstanceId dst_inst;
+
+	spinlock_t status_gate;
+	struct {
+		u64 open:1;
+		u64 ready:1;
+		u64 sent_caps:1;
+		u64 got_cap:1;
+		u64 got_cap_ack:1;
+		u64 monitor_ack_pending:1;
+	} status;
+	struct VethLpEvent cap_event, cap_ack_event;
+
+	spinlock_t ack_gate;
+	u16 pending_acks[VETH_MAX_ACKS_PER_MSG];
+	u32 num_pending_acks;
+
+	int mNumberAllocated;
+	int mNumberRcvMsgs;
+	int mNumberLpAcksAlloced;
+	struct VethCapData local_caps;
+	struct VethCapData remote_caps;
+	u32 ack_timeout;
+
+	VETHSTACK(veth_msg) msg_stack;
+};
+
+struct veth_port {
+	struct net_device_stats stats;
+	u64 mac_addr;
+	HvLpIndexMap lpar_map;
+
+	spinlock_t pending_gate;
+	struct sk_buff *pending_skb;
+	HvLpIndexMap pending_lpmask;
+
+	rwlock_t mcast_gate;
+	int promiscuous;
+	int all_mcast;
+	int num_mcast;
+	u64 mcast_addr[VETH_MAX_MCAST];
+};
+
+struct VethFabricMgr {
+	HvLpIndex this_lp;
+	struct veth_lpar_connection connection[HVMAXARCHITECTEDLPS];
+	struct net_device *netdev[HVMAXARCHITECTEDVIRTUALLANS];
+};
+
+#endif	/* _ISERIES_VETH_H */
diff -purN linux-2.5/drivers/pci/hotplug/rpaphp_core.c linuxppc64-2.5/drivers/pci/hotplug/rpaphp_core.c
--- linux-2.5/drivers/pci/hotplug/rpaphp_core.c	2004-02-28 01:22:51.000000000 +0000
+++ linuxppc64-2.5/drivers/pci/hotplug/rpaphp_core.c	2004-03-02 11:34:18.000000000 +0000
@@ -31,6 +31,7 @@
 #include <linux/smp.h>
 #include <linux/smp_lock.h>
 #include <linux/init.h>
+#include <asm/eeh.h>       /* for eeh_add_device() */
 #include <asm/rtas.h>		/* rtas_call */
 #include <asm/pci-bridge.h>	/* for pci_controller */
 #include "../pci.h"		/* for pci_add_new_bus*/
@@ -474,6 +475,7 @@ static struct pci_dev *rpaphp_config_ada
 		}
 
 		dev = rpaphp_find_pci_dev(slot->dn->child);
+		eeh_add_device(dev);
 	} else {
 		/* slot is not enabled */
 		err("slot doesn't have pci_dev structure\n");
@@ -497,6 +499,7 @@ static int rpaphp_unconfig_adapter(struc
 	}
 
 	/* remove the device from the pci core */
+	eeh_remove_device(slot->dev);
 	pci_remove_bus_device(slot->dev);
 
 	pci_dev_put(slot->dev);
diff -purN linux-2.5/drivers/pci/pci.ids linuxppc64-2.5/drivers/pci/pci.ids
--- linux-2.5/drivers/pci/pci.ids	2004-01-30 16:24:17.000000000 +0000
+++ linuxppc64-2.5/drivers/pci/pci.ids	2004-03-01 19:33:22.000000000 +0000
@@ -1820,6 +1820,8 @@
 	0010  DAC960PX
 	0050  AcceleRAID 352/170/160 support Device
 	b166  Gemstone chipset SCSI controller
+		1014 0266  Dual Channel PCI-X U320 SCSI Adapter
+		1014 0278  Dual Channel PCI-X U320 SCSI RAID Adapter
 	ba55  eXtremeRAID 1100 support Device
 	ba56  eXtremeRAID 2000/3000 support Device
 106a  Aten Research Inc
diff -purN linux-2.5/drivers/scsi/Kconfig linuxppc64-2.5/drivers/scsi/Kconfig
--- linux-2.5/drivers/scsi/Kconfig	2004-02-18 09:50:09.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/Kconfig	2004-03-01 19:33:22.000000000 +0000
@@ -725,6 +725,23 @@ config SCSI_IPS
 	  To compile this driver as a module, choose M here: the
 	  module will be called ips.
 
+config SCSI_IBMVSCSI
+	tristate "IBM Virtual SCSI support"
+	depends on PPC_PSERIES || PPC_ISERIES
+	help
+	  This is the IBM Virtual SCSI Client
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called ibmvscsic.
+
+config SCSI_IBMVSCSIS
+	tristate "IBM Virtual SCSI Server support"
+	depends on PPC_PSERIES
+	help
+	  This is the IBM Virtual SCSI Server
+	  To compile this driver as a module, choose M here: the
+	  module will be called ibmvscsis.
+
 config SCSI_INITIO
 	tristate "Initio 9100U(W) support"
 	depends on PCI && SCSI && BROKEN
@@ -941,6 +958,31 @@ config SCSI_SYM53C8XX_IOMAPPED
 	  the card.  This is significantly slower then using memory
 	  mapped IO.  Most people should answer N.
 
+config SCSI_IPR
+	tristate "IBM Power Linux RAID adapter support"
+	depends on PCI && SCSI
+	enable FW_LOADER
+	---help---
+	  This driver supports the IBM Power Linux family RAID adapters.
+	  This includes IBM pSeries 5712, 5703, 5709, and 570A, as well
+	  as IBM iSeries 5702, 5703, 5709, and 570A.
+
+config SCSI_IPR_TRACE
+	bool "enable driver internal trace"
+	depends on SCSI_IPR
+	help
+	  If you say Y here, the driver will trace all commands issued
+	  to the adapter. Performance impact is minimal. Trace can be
+	  dumped using /sys/bus/class/scsi_host/hostXX/trace.
+
+config SCSI_IPR_DUMP
+	bool "enable adapter dump support"
+	depends on SCSI_IPR
+	help
+	  If you say Y here, the driver will support adapter crash dump.
+	  If you enable this support, the iprdump daemon can be used
+	  to capture adapter failure analysis information.
+
 config SCSI_ZALON
 	tristate "Zalon SCSI support"
 	depends on GSC && SCSI
diff -purN linux-2.5/drivers/scsi/Makefile linuxppc64-2.5/drivers/scsi/Makefile
--- linux-2.5/drivers/scsi/Makefile	2004-02-07 16:38:00.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/Makefile	2004-03-01 19:33:22.000000000 +0000
@@ -109,11 +109,13 @@ obj-$(CONFIG_SCSI_FCAL)		+= fcal.o
 obj-$(CONFIG_SCSI_CPQFCTS)	+= cpqfc.o
 obj-$(CONFIG_SCSI_LASI700)	+= lasi700.o 53c700.o
 obj-$(CONFIG_SCSI_NSP32)	+= nsp32.o
+obj-$(CONFIG_SCSI_IPR)		+= ipr.o
 obj-$(CONFIG_SCSI_SATA_SVW)	+= libata.o sata_svw.o
 obj-$(CONFIG_SCSI_ATA_PIIX)	+= libata.o ata_piix.o
 obj-$(CONFIG_SCSI_SATA_PROMISE)	+= libata.o sata_promise.o
 obj-$(CONFIG_SCSI_SATA_SIL)	+= libata.o sata_sil.o
 obj-$(CONFIG_SCSI_SATA_VIA)	+= libata.o sata_via.o
+obj-$(CONFIG_SCSI_IBMVSCSI)	+= ibmvscsi/
 
 obj-$(CONFIG_ARM)		+= arm/
 
diff -purN linux-2.5/drivers/scsi/ibmvscsi/Makefile linuxppc64-2.5/drivers/scsi/ibmvscsi/Makefile
--- linux-2.5/drivers/scsi/ibmvscsi/Makefile	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/ibmvscsi/Makefile	2004-02-27 21:38:44.000000000 +0000
@@ -0,0 +1,7 @@
+obj-$(CONFIG_SCSI_IBMVSCSI)	+= ibmvscsic.o
+
+ibmvscsic-y			+= ibmvscsi.o
+ibmvscsic-$(CONFIG_PPC_ISERIES)	+= iseries_vscsi.o 
+ibmvscsic-$(CONFIG_PPC_PSERIES)	+= rpa_vscsi.o 
+ 
+obj-$(CONFIG_SCSI_IBMVSCSIS)	+= ibmvscsis.o
diff -purN linux-2.5/drivers/scsi/ibmvscsi/ibmvscsi.c linuxppc64-2.5/drivers/scsi/ibmvscsi/ibmvscsi.c
--- linux-2.5/drivers/scsi/ibmvscsi/ibmvscsi.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/ibmvscsi/ibmvscsi.c	2004-02-26 05:04:12.000000000 +0000
@@ -0,0 +1,995 @@
+/* ------------------------------------------------------------
+ * ibmvscsi.c
+ * (C) Copyright IBM Corporation 1994, 2004
+ * Authors: Colin DeVilbiss (devilbis@us.ibm.com)
+ *          Santiago Leon (santil@us.ibm.com)
+ *          Dave Boutcher (sleddog@us.ibm.com)
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307
+ * USA
+ *
+ * ------------------------------------------------------------
+ * Emulation of a SCSI host adapter for Virtual I/O devices
+ *
+ * This driver supports the SCSI adapter implemented by the IBM
+ * Power5 firmware.  That SCSI adapter is not a physical adapter,
+ * but allows Linux SCSI peripheral drivers to directly
+ * access devices in another logical partition on the physical system.
+ *
+ * The virtual adapter(s) are present in the open firmware device
+ * tree just like real adapters.
+ *
+ * One of the capabilities provided on these systems is the ability
+ * to DMA between partitions.  The architecture states that for VSCSI,
+ * the server side is allowed to DMA to and from the client.  The client
+ * is never trusted to DMA to or from the server directly.
+ *
+ * Messages are sent between partitions on a "Command/Response Queue" 
+ * (CRQ), which is just a buffer of 16 byte entries in the receiver's 
+ * Senders cannot access the buffer directly, but send messages by
+ * making a hypervisor call and passing in the 16 bytes.  The hypervisor
+ * puts the message in the next 16 byte space in round-robbin fashion,
+ * turns on the high order bit of the message (the valid bit), and 
+ * generates an interrupt to the receiver (if interrupts are turned on.) 
+ * The receiver just turns off the valid bit when they have copied out
+ * the message.
+ *
+ * The VSCSI client builds a SCSI Remote Protocol (SRP) Information Unit
+ * (IU) (as defined in the T10 standard available at www.t10.org), gets 
+ * a DMA address for the message, and sends it to the server as the
+ * payload of a CRQ message.  The server DMAs the SRP IU and processes it,
+ * including doing any additional data transfers.  When it is done, it
+ * DMAs the SRP response back to the same address as the request came from,
+ * and sends a CRQ message back to inform the client that the request has
+ * completed.
+ *
+ * Note that some of the underlying infrastructure is different between
+ * machines conforming to the "RS/6000 Platform Architecture" (RPA) and
+ * the older iSeries hypervisor models.  To support both, some low level
+ * routines have been broken out into rpa_vscsi.c and iseries_vscsi.c.
+ * The Makefile should pick one, not two, not zero, of these.
+ *
+ * TODO: This is currently pretty tied to the IBM i/pSeries hypervisor
+ * interfaces.  It would be really nice to abstract this above an RDMA
+ * layer.
+ */
+
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/dma-mapping.h>
+#include <scsi/scsi.h>
+#include <scsi/scsi_cmnd.h>
+#include <scsi/scsi_host.h>
+#include <scsi/scsi_device.h>
+#include "ibmvscsi.h"
+
+/* The values below are somewhat arbitrary default values, but 
+ * OS/400 will use 3 busses (disks, CDs, tapes, I think.)
+ * Note that there are 3 bits of channel value, 6 bits of id, and
+ * 5 bits of LUN.
+ */
+static int max_id = 64;
+static int max_channel = 3;
+
+MODULE_DESCRIPTION("IBM Virtual SCSI");
+MODULE_AUTHOR("Dave Boutcher");
+MODULE_LICENSE("GPL");
+
+module_param_named(max_id, max_id, int, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(max_id, "Largest ID value for each channel");
+module_param_named(max_channel, max_channel, int, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(max_channel, "Largest channel value");
+
+/* ------------------------------------------------------------
+ * Routines for the event pool and event structs
+ */
+/**
+ * initialize_event_pool: - Allocates and initializes the event pool for a host
+ * @pool:	event_pool to be initialized
+ * @size:	Number of events in pool
+ * @hostdata:	ibmvscsi_host_data who owns the event pool
+ *
+ * Returns zero on success.
+*/
+static int initialize_event_pool(struct event_pool *pool,
+				 int size, struct ibmvscsi_host_data *hostdata)
+{
+	int i;
+
+	pool->size = size;
+	pool->events = kmalloc(pool->size * sizeof(*pool->events), GFP_KERNEL);
+	if (!pool->events)
+		return -ENOMEM;
+	memset(pool->events, 0x00, pool->size * sizeof(*pool->events));
+
+	pool->iu_storage =
+	    dma_alloc_coherent(hostdata->dev,
+			       pool->size * sizeof(*pool->iu_storage),
+			       &pool->iu_token, 0);
+	if (!pool->iu_storage) {
+		kfree(pool->events);
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < pool->size; ++i) {
+		struct srp_event_struct *evt = &pool->events[i];
+		memset(&evt->crq, 0x00, sizeof(evt->crq));
+		evt->crq.valid = 0x80;
+		evt->crq.IU_length = sizeof(*evt->evt);
+		evt->crq.IU_data_ptr = pool->iu_token + sizeof(*evt->evt) * i;
+		evt->evt = pool->iu_storage + i;
+		evt->hostdata = hostdata;
+	}
+
+	return 0;
+}
+
+/**
+ * release_event_pool: - Frees memory of an event pool of a host
+ * @pool:	event_pool to be released
+ * @hostdata:	ibmvscsi_host_data who owns the even pool
+ *
+ * Returns zero on success.
+*/
+static void release_event_pool(struct event_pool *pool,
+			       struct ibmvscsi_host_data *hostdata)
+{
+	int i, in_use = 0;
+	for (i = 0; i < pool->size; ++i)
+		if (pool->events[i].in_use)
+			++in_use;
+	if (in_use)
+		printk(KERN_WARNING
+		       "ibmvscsi: releasing event pool with %d "
+		       "events still in use?\n",
+		       in_use);
+	kfree(pool->events);
+	dma_free_coherent(hostdata->dev,
+			  pool->size * sizeof(*pool->iu_storage),
+			  pool->iu_storage, pool->iu_token);
+}
+
+/**
+ * ibmvscsi_valid_event_struct: - Determines if event is valid.
+ * @pool:	event_pool that contains the event
+ * @evt:	srp_event_struct to be checked for validity
+ *
+ * Returns zero if event is invalid, one otherwise.
+*/
+int ibmvscsi_valid_event_struct(struct event_pool *pool,
+				struct srp_event_struct *evt)
+{
+	int index = evt - pool->events;
+	if (index < 0 || index >= pool->size)	/* outside of bounds */
+		return 0;
+	if (evt != pool->events + index)	/* unaligned */
+		return 0;
+	return 1;
+}
+
+/**
+ * ibmvscsi_free-event_struct: - Changes status of event to "free"
+ * @pool:	event_pool that contains the event
+ * @evt:	srp_event_struct to be modified
+ *
+*/
+static void ibmvscsi_free_event_struct(struct event_pool *pool,
+				       struct srp_event_struct *evt)
+{
+	if (!ibmvscsi_valid_event_struct(pool, evt)) {
+		printk(KERN_ERR
+		       "ibmvscsi: Freeing invalid event_struct %p "
+		       "(not in pool %p)\n",
+		       evt, pool->events);
+		return;
+	}
+	if (!evt->in_use) {
+		printk(KERN_ERR
+		       "ibmvscsi: Freeing event_struct %p "
+		       "which is not in use!\n",
+		       evt);
+		return;
+	}
+	evt->in_use = 0;
+}
+
+/**
+ * ibmvscsi_get_event_struct: - Gets the next free event in pool
+ * @pool:	event_pool that contains the events to be searched
+ *
+ * Returns the next event in "free" state, and NULL if none are free.
+ * Note that no synchronization is done here, we assume the host_lock
+ * will syncrhonze things.
+*/
+static
+struct srp_event_struct *ibmvscsi_get_event_struct(struct event_pool *pool)
+{
+	struct srp_event_struct *cur, *last = pool->events + pool->size;
+
+	for (cur = pool->events; cur < last; ++cur)
+		if (!cur->in_use) {
+			cur->in_use = 1;
+			break;
+		}
+
+	if (cur >= last) {
+		printk(KERN_ERR "ibmvscsi: found no event struct in pool!\n");
+		return NULL;
+	}
+
+	return cur;
+}
+
+/**
+ * evt_struct_for: - Initializes the next free event
+ * @pool:	event_pool that contains events to be searched
+ * @evt:	VIOSRP_IU that the event will point to
+ * @cmnd:	The scsi cmnd object for this event.  Can be NULL
+ * @done:	Callback function when event is processed
+ *
+ * Returns the initialized event, and NULL if there are no free events
+ */
+static
+struct srp_event_struct *evt_struct_for(struct event_pool *pool,
+					union VIOSRP_IU *evt,
+					struct scsi_cmnd *cmnd,
+					void (*done) (struct srp_event_struct
+						      *))
+{
+	struct srp_event_struct *evt_struct = ibmvscsi_get_event_struct(pool);
+	if (!evt_struct)
+		return NULL;
+
+	*evt_struct->evt = *evt;
+	evt_struct->evt->srp.generic.tag = (u64) (unsigned long)evt_struct;
+
+	evt_struct->cmnd = cmnd;
+	evt_struct->done = done;
+	return evt_struct;
+}
+
+/* ------------------------------------------------------------
+ * Routines for receiving SCSI responses from the hosting partition
+ */
+/**
+ * unmap_direct_data: - Unmap address pointed by SRP_CMD
+ * @cmd:	SRP_CMD whose additional_data member will be unmapped
+ * @dev:	device for which the memory is mapped
+ *
+*/
+static void unmap_direct_data(struct SRP_CMD *cmd, struct device *dev)
+{
+	struct memory_descriptor *data =
+	    (struct memory_descriptor *)cmd->additional_data;
+	dma_unmap_single(dev, data->virtual_address, data->length,
+			 DMA_BIDIRECTIONAL);
+}
+
+/**
+ * unmap_direct_data: - Unmap array of address pointed by SRP_CMD
+ * @cmd:	SRP_CMD whose additional_data member will be unmapped
+ * @dev:	device for which the memory is mapped
+ *
+*/
+static void unmap_indirect_data(struct SRP_CMD *cmd, struct device *dev)
+{
+	struct indirect_descriptor *indirect =
+	    (struct indirect_descriptor *)cmd->additional_data;
+	int i, num_mapped = indirect->head.length / sizeof(indirect->list[0]);
+	for (i = 0; i < num_mapped; ++i) {
+		struct memory_descriptor *data = &indirect->list[i];
+		dma_unmap_single(dev,
+				 data->virtual_address,
+				 data->length, DMA_BIDIRECTIONAL);
+	}
+}
+
+/**
+ * unmap_direct_data: - Unmap data pointed in SRP_CMD based on the format
+ * @cmd:	SRP_CMD whose additional_data member will be unmapped
+ * @dev:	device for which the memory is mapped
+ *
+*/
+static void unmap_cmd_data(struct SRP_CMD *cmd, struct device *dev)
+{
+	if ((cmd->data_out_format == SRP_NO_BUFFER) &&
+	    (cmd->data_in_format == SRP_NO_BUFFER))
+		return;
+	else if ((cmd->data_out_format == SRP_DIRECT_BUFFER) ||
+		 (cmd->data_in_format == SRP_DIRECT_BUFFER))
+		unmap_direct_data(cmd, dev);
+	else
+		unmap_indirect_data(cmd, dev);
+}
+
+/**
+ * map_sg_data: - Maps dma for a scatterlist and initializes decriptor fields
+ * @cmd:	Scsi_Cmnd with the scatterlist
+ * @srp_cmd:	SRP_CMD that contains the memory descriptor
+ * @dev:	device for which to map dma memory
+ *
+ * Called by map_data_for_srp_cmd() when building srp cmd from scsi cmd.
+ * Returns 1 on success.
+*/
+static int map_sg_data(struct scsi_cmnd *cmd,
+		       struct SRP_CMD *srp_cmd, struct device *dev)
+{
+
+	int i, sg_mapped;
+	u64 total_length = 0;
+	struct scatterlist *sg = cmd->request_buffer;
+	struct memory_descriptor *data =
+	    (struct memory_descriptor *)srp_cmd->additional_data;
+	struct indirect_descriptor *indirect =
+	    (struct indirect_descriptor *)data;
+	sg_mapped = dma_map_sg(dev, sg, cmd->use_sg, DMA_BIDIRECTIONAL);
+
+	/* special case; we can use a single direct descriptor */
+	if (sg_mapped == 1) {
+		if (cmd->sc_data_direction == DMA_TO_DEVICE)
+			srp_cmd->data_out_format = SRP_DIRECT_BUFFER;
+		else
+			srp_cmd->data_in_format = SRP_DIRECT_BUFFER;
+		data->virtual_address = sg[0].dma_address;
+		data->length = sg[0].dma_length;
+		data->memory_handle = 0;
+		return 1;
+	}
+
+	if (sg_mapped > MAX_INDIRECT_BUFS) {
+		printk(KERN_ERR
+		       "ibmvscsi: More than %d mapped sg entries, got %d\n",
+		       MAX_INDIRECT_BUFS, sg_mapped);
+		return 0;
+	}
+
+	if (cmd->sc_data_direction == DMA_TO_DEVICE) {
+		srp_cmd->data_out_format = SRP_INDIRECT_BUFFER;
+		srp_cmd->data_out_count = sg_mapped;
+	} else {
+		srp_cmd->data_in_format = SRP_INDIRECT_BUFFER;
+		srp_cmd->data_in_count = sg_mapped;
+	}
+	indirect->head.virtual_address = 0;
+	indirect->head.length = sg_mapped * sizeof(indirect->list[0]);
+	indirect->head.memory_handle = 0;
+	for (i = 0; i < sg_mapped; ++i) {
+		struct memory_descriptor *descr = &indirect->list[i];
+		struct scatterlist *sg_entry = &sg[i];
+		descr->virtual_address = sg_entry->dma_address;
+		descr->length = sg_entry->dma_length;
+		descr->memory_handle = 0;
+		total_length += sg_entry->dma_length;
+	}
+	indirect->total_length = total_length;
+
+	return 1;
+}
+
+/**
+ * map_sg_data: - Maps memory and initializes memory decriptor fields
+ * @cmd:	struct scsi_cmnd with the memory to be mapped
+ * @srp_cmd:	SRP_CMD that contains the memory descriptor
+ * @dev:	device for which to map dma memory
+ *
+ * Called by map_data_for_srp_cmd() when building srp cmd from scsi cmd.
+ * Returns 1 on success.
+*/
+static int map_single_data(struct scsi_cmnd *cmd,
+			   struct SRP_CMD *srp_cmd, struct device *dev)
+{
+	struct memory_descriptor *data =
+	    (struct memory_descriptor *)srp_cmd->additional_data;
+
+	data->virtual_address =
+	    (u64) (unsigned long)dma_map_single(dev, cmd->request_buffer,
+						cmd->request_bufflen,
+						DMA_BIDIRECTIONAL);
+	if (data->virtual_address == 0xFFFFFFFF) {
+		printk(KERN_ERR
+		       "ibmvscsi: Unable to map request_buffer for command!\n");
+		return 0;
+	}
+	data->length = cmd->request_bufflen;
+	data->memory_handle = 0 /* viopath_sourceinst(viopath_hostLp) */ ;
+
+	if (cmd->sc_data_direction == DMA_TO_DEVICE)
+		srp_cmd->data_out_format = SRP_DIRECT_BUFFER;
+	else
+		srp_cmd->data_in_format = SRP_DIRECT_BUFFER;
+
+	return 1;
+}
+
+/**
+ * map_data_for_srp_cmd: - Calls functions to map data for srp cmds
+ * @cmd:	struct scsi_cmnd with the memory to be mapped
+ * @srp_cmd:	SRP_CMD that contains the memory descriptor
+ * @dev:	dma device for which to map dma memory
+ *
+ * Called by scsi_cmd_to_srp_cmd() when converting scsi cmds to srp cmds 
+ * Returns 1 on success.
+*/
+static int map_data_for_srp_cmd(struct scsi_cmnd *cmd,
+				struct SRP_CMD *srp_cmd, struct device *dev)
+{
+	switch (cmd->sc_data_direction) {
+	case DMA_FROM_DEVICE:
+	case DMA_TO_DEVICE:
+		break;
+	case DMA_NONE:
+		return 1;
+	case DMA_BIDIRECTIONAL:
+		printk(KERN_ERR
+		       "ibmvscsi: Can't map DMA_BIDIRECTIONAL to read/write\n");
+		return 0;
+	default:
+		printk(KERN_ERR
+		       "ibmvscsi: Unknown data direction 0x%02x; can't map!\n",
+		       cmd->sc_data_direction);
+		return 0;
+	}
+
+	if (!cmd->request_buffer)
+		return 1;
+	if (cmd->use_sg)
+		return map_sg_data(cmd, srp_cmd, dev);
+	return map_single_data(cmd, srp_cmd, dev);
+}
+
+/* ------------------------------------------------------------
+ * Routines for sending and receiving SRPs
+ */
+/**
+ * ibmvscsi_send_srp_event: - Transforms event to u64 array and calls send_crq()
+ * @evt_struct:	evt_struct to be sent
+ * @hostdata:	ibmvscsi_host_data of host
+ *
+ * Returns the value returned from ibmvscsi_send_crq(). (Zero for success)
+ * Note that this routine assumes that host_lock is held for synchronization
+*/
+static int ibmvscsi_send_srp_event(struct srp_event_struct *evt_struct,
+				   struct ibmvscsi_host_data *hostdata)
+{
+	struct scsi_cmnd *cmnd;
+	u64 *crq_as_u64 = (u64 *) & evt_struct->crq;
+
+	/* If we have exhausted our request limit, just queue this request.
+	 * Note that there are rare cases involving driver generated requests 
+	 * (such as task management requests) that the mid layer may think we
+	 * can handle more requests (can_queue) when we actually can't
+	 */
+	if (atomic_dec_if_positive(&hostdata->request_limit) < 0) {
+		printk("ibmvscsi: Warning, request_limit exceeded\n");
+		return SCSI_MLQUEUE_HOST_BUSY;
+	}
+
+	/* Add this to the sent list.  We need to do this 
+	 * before we actually send 
+	 * in case it comes back REALLY fast
+	 */
+	list_add_tail(&evt_struct->list, &hostdata->sent);
+
+	if (ibmvscsi_send_crq(hostdata, crq_as_u64[0], crq_as_u64[1]) != 0) {
+		list_del(&evt_struct->list);
+
+		cmnd = evt_struct->cmnd;
+		printk(KERN_ERR "ibmvscsi: failed to send event struct\n");
+		unmap_cmd_data(&evt_struct->evt->srp.cmd, hostdata->dev);
+		ibmvscsi_free_event_struct(&hostdata->pool, evt_struct);
+		cmnd->result = DID_ERROR << 16;
+		evt_struct->cmnd_done(cmnd);
+	}
+
+	return 0;
+}
+
+/**
+ * handle_cmd_rsp: -  Handle responses from commands
+ * @evt_struct:	srp_event_struct to be handled
+ *
+ * Used as a callback by when sending scsi cmds (by scsi_cmd_to_event_struct). 
+ * Gets called by ibmvscsi_handle_crq()
+*/
+static void handle_cmd_rsp(struct srp_event_struct *evt_struct)
+{
+	struct SRP_RSP *rsp = &evt_struct->evt->srp.rsp;
+	struct scsi_cmnd *cmnd = (struct scsi_cmnd *)evt_struct->cmnd;
+
+	if (cmnd) {
+		cmnd->result |= rsp->status;
+		if (((cmnd->result >> 1) & 0x1f) == CHECK_CONDITION)
+			memcpy(cmnd->sense_buffer,
+			       rsp->sense_and_response_data,
+			       rsp->sense_data_list_length);
+		unmap_cmd_data(&evt_struct->cmd, evt_struct->hostdata->dev);
+
+		if (rsp->dounder)
+			cmnd->resid = rsp->data_out_residual_count;
+		else if (rsp->diunder)
+			cmnd->resid = rsp->data_in_residual_count;
+	}
+
+	if (evt_struct->cmnd_done) {
+		evt_struct->cmnd_done(cmnd);
+	}
+}
+
+/* ------------------------------------------------------------
+ * Routines for queuing individual SCSI commands to the hosting partition
+ */
+
+/**
+ * lun_from_dev: - Returns the lun of the scsi device
+ * @dev:	struct scsi_device
+ *
+*/
+static inline u16 lun_from_dev(struct scsi_device *dev)
+{
+	return (0x2 << 14) | (dev->id << 8) | (dev->channel << 5) | dev->lun;
+}
+
+/**
+ * scsi_cmd_to_srp_cmd: - Initializes srp cmd with data from scsi cmd
+ * @cmd:	source struct scsi_cmnd
+ * @srp_cmd:	target SRP_CMD
+ * @hostdata:	ibmvscsi_host_data of host
+ *
+ * Returns 1 on success.
+*/
+static int scsi_cmd_to_srp_cmd(struct scsi_cmnd *cmd,
+			       struct SRP_CMD *srp_cmd,
+			       struct ibmvscsi_host_data *hostdata)
+{
+	u16 lun = lun_from_dev(cmd->device);
+	memset(srp_cmd, 0x00, sizeof(*srp_cmd));
+
+	srp_cmd->type = SRP_CMD_TYPE;
+	memcpy(srp_cmd->cdb, cmd->cmnd, sizeof(cmd->cmnd));
+	srp_cmd->lun = ((u64) lun) << 48;
+
+	return map_data_for_srp_cmd(cmd, srp_cmd, hostdata->dev);
+}
+
+/**
+ * scsi_cmd_to_event_struct: - Initializes a srp_event_struct 
+ *                             with data from scsi cmd
+ * @cmd:	Source struct scsi_cmnd
+ * @done:	Callback function to be called when cmd is completed
+ * @hostdata:	ibmvscsi_host_data of host
+ *
+ * Returns the srp_event_struct to be used or NULL if not successful.
+*/
+static struct srp_event_struct *scsi_cmd_to_event_struct(struct scsi_cmnd *cmd,
+							 void (*done) (struct
+								       scsi_cmnd
+								       *),
+							 struct
+							 ibmvscsi_host_data
+							 *hostdata)
+{
+	struct SRP_CMD srp_cmd;
+	struct srp_event_struct *evt_struct;
+
+	if (!scsi_cmd_to_srp_cmd(cmd, &srp_cmd, hostdata)) {
+		printk(KERN_ERR "ibmvscsi: couldn't convert cmd to SRP_CMD\n");
+		return NULL;
+	}
+
+	evt_struct = evt_struct_for(&hostdata->pool,
+				    (union VIOSRP_IU *)&srp_cmd,
+				    (void *)cmd, handle_cmd_rsp);
+	if (!evt_struct) {
+		printk(KERN_ERR "ibmvscsi: evt_struct_for() returned NULL\n");
+		return NULL;
+	}
+
+	evt_struct->cmd = srp_cmd;
+	evt_struct->cmnd_done = done;
+	evt_struct->crq.timeout = cmd->timeout;
+	return evt_struct;
+}
+
+/**
+ * ibmvscsi_queue: - The queuecommand function of the scsi template 
+ * @cmd:	struct scsi_cmnd to be executed
+ * @done:	Callback function to be called when cmd is completed
+ *
+ * Always returns zero
+*/
+static
+int ibmvscsi_queue(struct scsi_cmnd *cmd, void (*done) (struct scsi_cmnd *))
+{
+	struct ibmvscsi_host_data *hostdata =
+	    (struct ibmvscsi_host_data *)&cmd->device->host->hostdata;
+	struct srp_event_struct *evt_struct =
+	    scsi_cmd_to_event_struct(cmd, done, hostdata);
+
+	if (!evt_struct) {
+		printk(KERN_ERR
+		       "ibmvscsi: can't convert struct scsi_cmnd to event\n");
+		return SCSI_MLQUEUE_HOST_BUSY;
+	}
+
+	evt_struct->crq.format = VIOSRP_SRP_FORMAT;
+
+	return ibmvscsi_send_srp_event(evt_struct, hostdata);
+}
+
+/* ------------------------------------------------------------
+ * Routines for driver initialization
+ */
+/**
+ * login_rsp: - Handle response to SRP login request
+ * @evt_struct:	srp_event_struct with the response
+ *
+ * Used as a "done" callback by when sending srp_login. Gets called
+ * by ibmvscsi_handle_crq()
+*/
+static void login_rsp(struct srp_event_struct *evt_struct)
+{
+	struct ibmvscsi_host_data *hostdata = evt_struct->hostdata;
+	switch (evt_struct->evt->srp.generic.type) {
+	case SRP_LOGIN_RSP_TYPE:	/* it worked! */
+		break;
+	case SRP_LOGIN_REJ_TYPE:	/* refused! */
+		printk(KERN_INFO "ibmvscsi: SRP_LOGIN_REQ rejected\n");
+		/* Login failed.  we are still scsi_blocked */
+		atomic_set(&hostdata->request_limit, -1);
+		return;
+	default:
+		printk(KERN_ERR
+		       "ibmvscsi: Invalid login response typecode 0x%02x!\n",
+		       evt_struct->evt->srp.generic.type);
+		/* Login failed.  we are still scsi_blocked */
+		atomic_set(&hostdata->request_limit, -1);
+		return;
+	}
+
+	printk(KERN_INFO "ibmvscsi: SRP_LOGIN succeeded\n");
+
+	/* Now we know what the real request-limit is */
+	atomic_set(&hostdata->request_limit,
+		   evt_struct->evt->srp.login_rsp.request_limit_delta);
+
+	hostdata->host->can_queue =
+	    evt_struct->evt->srp.login_rsp.request_limit_delta;
+
+	/* Now allow commands (such the queued up scans) to go */
+	scsi_unblock_requests(hostdata->host);
+
+	return;
+}
+
+/**
+ * send_srp_login: - Sends the srp login
+ * @hostdata:	ibmvscsi_host_data of host
+ * 
+ * Returns zero if successful.
+*/
+static int send_srp_login(struct ibmvscsi_host_data *hostdata)
+{
+	int rc;
+	unsigned long flags;
+
+	struct SRP_LOGIN_REQ req = {
+		.type = SRP_LOGIN_REQ_TYPE,
+		.max_requested_initiator_to_target_iulen = sizeof(union SRP_IU),
+		.required_buffer_formats = 0x0002	/* direct and indirect */
+	};
+	struct srp_event_struct *evt_struct = evt_struct_for(&hostdata->pool,
+							     (union VIOSRP_IU *)
+							     &req,
+							     NULL,
+							     login_rsp);
+
+	if (!evt_struct) {
+		printk(KERN_ERR
+		       "ibmvscsi: couldn't allocate an event for login req!\n");
+		return FAILED;
+	}
+
+	/* Start out with a request limit of 1, since this is negotiated in
+	 * the login request we are just sending
+	 */
+	atomic_set(&hostdata->request_limit, 1);
+	evt_struct->crq.format = VIOSRP_SRP_FORMAT;
+
+	spin_lock_irqsave(hostdata->host->host_lock, flags);
+	rc = ibmvscsi_send_srp_event(evt_struct, hostdata);
+	spin_unlock_irqrestore(hostdata->host->host_lock, flags);
+	return rc;
+};
+
+/**
+ * sync_completion: Signal that a synchronous command has completed
+ */
+static void sync_completion(struct srp_event_struct *evt_struct)
+{
+	complete(&evt_struct->comp);
+}
+
+/**
+ * ibmvscsi_abort: Abort a command...from scsi host template
+ * send this over to the server and wait synchronously for the response
+ */
+static int ibmvscsi_abort(struct scsi_cmnd *cmd)
+{
+	struct ibmvscsi_host_data *hostdata =
+	    *(struct ibmvscsi_host_data **)&cmd->device->host->hostdata;
+	union VIOSRP_IU iu;
+	struct SRP_TSK_MGMT *tsk_mgmt = &iu.srp.tsk_mgmt;
+	struct SRP_RSP *rsp;
+	struct srp_event_struct *evt;
+	struct srp_event_struct *tmp_evt, *found_evt;
+	u16 lun = lun_from_dev(cmd->device);
+
+	/* First, find this command in our sent list so we can figure
+	 * out the correct tag
+	 */
+	found_evt = NULL;
+	list_for_each_entry(tmp_evt, &hostdata->sent, list) {
+		if (tmp_evt->cmnd == cmd) {
+			found_evt = tmp_evt;
+			break;
+		}
+	}
+
+	/* Set up an abort SRP command */
+	memset(&iu, 0x00, sizeof(iu));
+	tsk_mgmt->type = SRP_TSK_MGMT_TYPE;
+	tsk_mgmt->lun = ((u64) lun) << 48;
+	tsk_mgmt->task_mgmt_flags = 0x01;	/* ABORT TASK */
+	tsk_mgmt->managed_task_tag = (u64) (unsigned long)found_evt;
+
+	printk(KERN_INFO "ibmvscsi: aborting command. lun 0x%lx, tag 0x%lx\n",
+	       tsk_mgmt->lun, tsk_mgmt->managed_task_tag);
+
+	evt = evt_struct_for(&hostdata->pool, &iu, NULL, sync_completion);
+	if (!evt) {
+		printk(KERN_ERR "ibmvscsi: failed to allocate abort() event\n");
+		return FAILED;
+	}
+
+	init_completion(&evt->comp);
+	if (ibmvscsi_send_srp_event(evt, hostdata) != 0) {
+		printk(KERN_ERR "ibmvscsi: failed to send abort() event\n");
+		ibmvscsi_free_event_struct(&hostdata->pool, evt);
+		return FAILED;
+	}
+
+	wait_for_completion(&evt->comp);
+
+	if (evt->evt->srp.generic.type != SRP_RSP_TYPE) {
+		printk(KERN_ERR "ibmvscsi: bad TSK_MGMT response type 0x%02x\n",
+		       evt->evt->srp.generic.type);
+	}
+
+	rsp = &evt->evt->srp.rsp;
+	if (!rsp->rspvalid || (rsp->response_data_list_length != 4)) {
+		printk(KERN_ERR "ibmvscsi: bad TSK_MGMT response\n");
+	}
+
+	if (rsp->sense_and_response_data[3] != 0x00) {
+		printk(KERN_ERR "ibmvscsi: ABORT failed: rsp_code 0x%02x\n",
+		       rsp->sense_and_response_data[3]);
+	}
+
+	cmd->result = (DID_ABORT << 16);
+	list_del(&tmp_evt->list);
+	unmap_cmd_data(&tmp_evt->cmd, tmp_evt->hostdata->dev);
+	ibmvscsi_free_event_struct(&tmp_evt->hostdata->pool, tmp_evt);
+	atomic_inc(&hostdata->request_limit);
+	printk(KERN_INFO
+	       "ibmvscsi: successfully aborted task tag 0x%lx\n",
+	       tsk_mgmt->managed_task_tag);
+	return SUCCESS;
+}
+
+/**
+ * purge_requests: Our virtual adapter just shut down.  purge any sent requests 
+ * @hostdata:    the adapter
+ */
+static void purge_requests(struct ibmvscsi_host_data *hostdata)
+{
+	struct srp_event_struct *tmp_evt, *pos;
+	unsigned long flags;
+
+	spin_lock_irqsave(hostdata->host->host_lock, flags);
+	list_for_each_entry_safe(tmp_evt, pos, &hostdata->sent, list) {
+		tmp_evt->cmnd->result = (DID_ERROR << 16);
+		list_del(&tmp_evt->list);
+		unmap_cmd_data(&tmp_evt->cmd, tmp_evt->hostdata->dev);
+		ibmvscsi_free_event_struct(&tmp_evt->hostdata->pool, tmp_evt);
+		if (tmp_evt->cmnd_done) {
+			spin_unlock_irqrestore(hostdata->host->host_lock,
+					       flags);
+			tmp_evt->cmnd_done(tmp_evt->cmnd);
+			spin_lock_irqsave(hostdata->host->host_lock, flags);
+		}
+	}
+	spin_unlock_irqrestore(hostdata->host->host_lock, flags);
+}
+
+/**
+ * ibmvscsi_handle_crq: - Handles and frees received events in the CRQ
+ * @crq:	Command/Response queue
+ * @hostdata:	ibmvscsi_host_data of host
+ *
+*/
+void ibmvscsi_handle_crq(struct VIOSRP_CRQ *crq,
+			 struct ibmvscsi_host_data *hostdata)
+{
+	unsigned long flags;
+	struct srp_event_struct *evt_struct =
+	    (struct srp_event_struct *)crq->IU_data_ptr;
+	switch (crq->valid) {
+	case 0xC0:		/* initialization */
+		switch (crq->format) {
+		case 0x01:	/* Initialization message */
+			printk(KERN_INFO "ibmvscsi: partner initialized\n");
+			/* Send back a response */
+			ibmvscsi_send_crq(hostdata, 0xC002000000000000, 0);
+
+			/* Now login */
+			send_srp_login(hostdata);
+			break;
+		case 0x02:	/* Initialization response */
+			printk(KERN_INFO
+			       "ibmvscsi: partner initialization complete\n");
+
+			/* Now login */
+			send_srp_login(hostdata);
+			break;
+		default:
+			printk(KERN_ERR "ibmvscsi: unknown crq message type\n");
+		}
+		return;
+	case 0xFF:		/* Hypervisor telling us the connection is closed */
+		printk(KERN_INFO "ibmvscsi: Virtual adapter failed!\n");
+
+		scsi_block_requests(hostdata->host);
+		atomic_set(&hostdata->request_limit, -1);
+		purge_requests(hostdata);
+		return;
+	case 0x80:		/* real payload */
+		break;
+	default:
+		printk(KERN_ERR
+		       "ibmvscsi: got an invalid message type 0x%02x\n",
+		       crq->valid);
+		return;
+	}
+
+	/* The only kind of payload CRQs we should get are responses to 
+	 * things we send. Make sure this response is to something we 
+	 * actually sent
+	 */
+	if (!ibmvscsi_valid_event_struct(&hostdata->pool, evt_struct)) {
+		printk(KERN_ERR
+		       "ibmvscsi: returned correlation_token 0x%p is invalid!\n",
+		       (void *)crq->IU_data_ptr);
+		return;
+	}
+
+	if (crq->format == VIOSRP_SRP_FORMAT)
+		atomic_add(evt_struct->evt->srp.rsp.request_limit_delta,
+			   &hostdata->request_limit);
+
+	if (evt_struct->done)
+		evt_struct->done(evt_struct);
+	else
+		printk(KERN_ERR
+		       "ibmvscsi: returned done() is NULL; not running it!\n");
+
+	/*
+	 * Lock the host_lock before messing with these structures, since we
+	 * are running in a task context
+	 */
+	spin_lock_irqsave(evt_struct->hostdata->host->host_lock, flags);
+	list_del(&evt_struct->list);
+	ibmvscsi_free_event_struct(&evt_struct->hostdata->pool, evt_struct);
+	spin_unlock_irqrestore(evt_struct->hostdata->host->host_lock, flags);
+}
+
+/* ------------------------------------------------------------
+ * SCSI driver registration
+ */
+static struct scsi_host_template driver_template = {
+	.name = "SCSI host adapter emulator for RPA/iSeries Virtual I/O",
+	.proc_name = "ibmvscsi",
+	.queuecommand = ibmvscsi_queue,
+	.eh_abort_handler = ibmvscsi_abort,
+	.can_queue = 1,		/* Updated after SRP_LOGIN */
+	.this_id = -1,
+	.sg_tablesize = MAX_INDIRECT_BUFS,
+	.cmd_per_lun = 1,
+	.use_clustering = DISABLE_CLUSTERING,
+	.emulated = 1
+};
+
+/**
+ * Called by bus code for each adapter
+ */
+struct ibmvscsi_host_data *ibmvscsi_probe(struct device *dev)
+{
+	struct ibmvscsi_host_data *hostdata;
+	struct Scsi_Host *host;
+	int rc;
+
+	host = scsi_host_alloc(&driver_template, sizeof(*hostdata));
+	if (!host) {
+		printk(KERN_ERR "ibmvscsi: couldn't allocate host data\n");
+		goto scsi_host_alloc_failed;
+	}
+
+	hostdata = (struct ibmvscsi_host_data *)host->hostdata;
+	memset(hostdata, 0x00, sizeof(*hostdata));
+	INIT_LIST_HEAD(&hostdata->sent);
+	hostdata->host = host;
+	hostdata->dev = dev;
+	atomic_set(&hostdata->request_limit, -1);
+
+	if (ibmvscsi_init_crq_queue(&hostdata->queue, hostdata) != 0) {
+		printk(KERN_ERR "ibmvscsi: couldn't initialize crq\n");
+		goto init_crq_failed;
+	}
+	if (initialize_event_pool(&hostdata->pool,
+				  IBMVSCSI_MAX_REQUESTS, hostdata) != 0) {
+		printk(KERN_ERR "ibmvscsi: couldn't initialize event pool\n");
+		goto init_pool_failed;
+	}
+
+	/* Try to send an initialization message.  Note that this is allowed
+	 * to fail if the other end is not active.
+	 */
+	rc = ibmvscsi_send_crq(hostdata, 0xC001000000000000, 0);
+
+	host->max_lun = 8;
+	host->max_id = max_id;
+	host->max_channel = max_channel;
+
+	/* Block requests until we get the SRP login back */
+	scsi_block_requests(host);
+
+	if (!scsi_add_host(hostdata->host, hostdata->dev)) {
+		/* If we can communicate with our server, scan
+		 * for devices */
+		if (rc == 0) {
+			scsi_scan_host(host);
+		}
+		return hostdata;
+	}
+
+	printk(KERN_ERR "ibmvscsi: couldn't add host\n");
+
+	release_event_pool(&hostdata->pool, hostdata);
+      init_pool_failed:
+	ibmvscsi_release_crq_queue(&hostdata->queue, hostdata);
+      init_crq_failed:
+	scsi_host_put(host);
+      scsi_host_alloc_failed:
+	return NULL;
+}
+
+void ibmvscsi_remove(struct ibmvscsi_host_data *hostdata)
+{
+	release_event_pool(&hostdata->pool, hostdata);
+	ibmvscsi_release_crq_queue(&hostdata->queue, hostdata);
+
+	scsi_remove_host(hostdata->host);
+	scsi_host_put(hostdata->host);
+	return;
+}
diff -purN linux-2.5/drivers/scsi/ibmvscsi/ibmvscsi.h linuxppc64-2.5/drivers/scsi/ibmvscsi/ibmvscsi.h
--- linux-2.5/drivers/scsi/ibmvscsi/ibmvscsi.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/ibmvscsi/ibmvscsi.h	2004-02-25 16:57:37.000000000 +0000
@@ -0,0 +1,109 @@
+/* ------------------------------------------------------------
+ * ibmvscsi.h
+ * (C) Copyright IBM Corporation 1994, 2003
+ * Authors: Colin DeVilbiss (devilbis@us.ibm.com)
+ *          Santiago Leon (santil@us.ibm.com)
+ *          Dave Boutcher (sleddog@us.ibm.com)
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307
+ * USA
+ *
+ * ------------------------------------------------------------
+ * Emulation of a SCSI host adapter for Virtual I/O devices
+ *
+ * This driver allows the Linux SCSI peripheral drivers to directly
+ * access devices in the hosting partition, either on an iSeries
+ * hypervisor system or a converged hypervisor system.
+ */
+#ifndef IBMVSCSI_H
+#define IBMVSCSI_H
+#include <linux/types.h>
+#include <linux/list.h>
+#include <linux/completion.h>
+#include <linux/workqueue.h>
+#include "viosrp.h"
+
+struct scsi_cmnd;
+struct Scsi_Host;
+/**
+ * Work out the number of scatter/gather buffers we support
+ */
+static const struct SRP_CMD *fake_srp_cmd = NULL;
+enum {
+	IBMVSCSI_MAX_REQUESTS = 50,
+	MAX_INDIRECT_BUFS = (sizeof(fake_srp_cmd->additional_data) -
+			     sizeof(struct indirect_descriptor)) /
+	    sizeof(struct memory_descriptor)
+};
+
+/* ------------------------------------------------------------
+ * Data Structures
+ */
+/* an RPA command/response transport queue */
+struct crq_queue {
+	struct VIOSRP_CRQ *msgs;
+	int size, cur;
+	dma_addr_t msg_token;
+	spinlock_t lock;
+};
+
+/* a unit of work for the hosting partition */
+struct srp_event_struct {
+	union VIOSRP_IU *evt;
+	struct scsi_cmnd *cmnd;
+	struct list_head list;
+	void (*done) (struct srp_event_struct *);
+	struct VIOSRP_CRQ crq;
+	struct ibmvscsi_host_data *hostdata;
+	char in_use;
+	struct SRP_CMD cmd;
+	void (*cmnd_done) (struct scsi_cmnd *);
+	struct completion comp;
+};
+
+/* a pool of event structs for use */
+struct event_pool {
+	struct srp_event_struct *events;
+	u32 size;
+	union VIOSRP_IU *iu_storage;
+	dma_addr_t iu_token;
+};
+
+/* all driver data associated with a host adapter */
+struct ibmvscsi_host_data {
+	atomic_t request_limit;
+	struct device *dev;
+	struct event_pool pool;
+	struct crq_queue queue;
+	struct work_struct srp_task;
+	struct list_head sent;
+	struct Scsi_Host *host;
+};
+
+/* routines for managing a command/response queue */
+int ibmvscsi_init_crq_queue(struct crq_queue *queue,
+			    struct ibmvscsi_host_data *hostdata);
+void ibmvscsi_release_crq_queue(struct crq_queue *queue,
+				struct ibmvscsi_host_data *hostdata);
+void ibmvscsi_handle_crq(struct VIOSRP_CRQ *crq,
+			 struct ibmvscsi_host_data *hostdata);
+int ibmvscsi_send_crq(struct ibmvscsi_host_data *hostdata,
+		      u64 word1, u64 word2);
+
+/* Probe/remove routines */
+struct ibmvscsi_host_data *ibmvscsi_probe(struct device *dev);
+void ibmvscsi_remove(struct ibmvscsi_host_data *hostdata);
+
+#endif				/* IBMVSCSI_H */
diff -purN linux-2.5/drivers/scsi/ibmvscsi/ibmvscsis.c linuxppc64-2.5/drivers/scsi/ibmvscsi/ibmvscsis.c
--- linux-2.5/drivers/scsi/ibmvscsi/ibmvscsis.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/ibmvscsi/ibmvscsis.c	2004-02-27 23:13:25.000000000 +0000
@@ -0,0 +1,2387 @@
+/**************************************************************************/
+/* -*- -linux- -*-                                                        */
+/* IBM eServer i/pSeries Virtual SCSI Target Driver                       */
+/* Copyright (C) 2003 Dave Boutcher (boutcher@us.ibm.com) IBM Corp.       */
+/*                                                                        */
+/*  This program is free software; you can redistribute it and/or modify  */
+/*  it under the terms of the GNU General Public License as published by  */
+/*  the Free Software Foundation; either version 2 of the License, or     */
+/*  (at your option) any later version.                                   */
+/*                                                                        */
+/*  This program is distributed in the hope that it will be useful,       */
+/*  but WITHOUT ANY WARRANTY; without even the implied warranty of        */
+/*  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the         */
+/*  GNU General Public License for more details.                          */
+/*                                                                        */
+/*  You should have received a copy of the GNU General Public License     */
+/*  along with this program; if not, write to the Free Software           */
+/*  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  */
+/*                                                                   USA  */
+/*                                                                        */
+/* This module contains the eServer virtual SCSI target code.  The driver */
+/* takes SRP requests from the virtual SCSI client (the linux version is  */
+/* int ibmvscsi.c, but there can be other clients, like AIX or OF) and    */
+/* passes them on to real devices in this system.                         */
+/*                                                                        */ 
+/* The basic hierarchy (and somewhat the organization of this file) is    */ 
+/* that SCSI CDBs are in SRPs are in CRQs.                                */ 
+/*                                                                        */ 
+/**************************************************************************/
+/*
+  TODO:
+  - Support redirecting SRP SCSI requests to a real SCSI driver
+*/
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/interrupt.h>
+#include <linux/list.h>
+#include <linux/proc_fs.h>
+#include <linux/pagemap.h>
+#include <linux/sched.h>
+#include <linux/pci.h>
+#include <linux/blkdev.h>
+#include <linux/fs.h>
+#include <linux/bio.h>
+
+#include <asm/hvcall.h>
+#include <asm/vio.h>
+#include <asm/iommu.h>
+
+#include "../scsi.h"
+#include "viosrp.h"
+
+MODULE_DESCRIPTION("IBM Virtual SCSI Target");
+MODULE_AUTHOR("Dave Boutcher");
+MODULE_LICENSE("GPL");
+
+char ibmvscsis_driver_name[] = "ibmvscsis";
+static int ibmvscsis_debug = 0;
+
+/*
+ * Quick macro to enable/disable interrupts
+ * TODO: move to vio.h to be common with ibmvscsi.c
+ */
+#define h_vio_signal(ua, mode) \
+  plpar_hcall_norets(H_VIO_SIGNAL, ua, mode)
+
+/* 
+ * These are indexes into the following table, and have to match!!!
+ */
+#define SENSE_SUCCESS       0
+#define SENSE_ABORT         1
+#define SENSE_INVALID_ID    2
+#define SENSE_DEVICE_FAULT  3
+#define SENSE_DEVICE_BUSY   4 
+#define SENSE_UNIT_OFFLINE  5
+#define SENSE_INVALID_CMD   6
+#define SENSE_INTERMEDIATE  7
+#define SENSE_WRITE_PROT    8
+
+static unsigned char ibmvscsis_sense_data[][3] =
+{
+/*
+ * Sense key lookup table
+ * Format: SenseKey,AdditionalSenseCode,AdditionalSenseCodeQualifier
+ * Adapted from 3w-xxxx.h
+ */
+	{0x00, 0x00, 0x00}, /* Success           */
+	{0x0b, 0x00, 0x00}, /* Aborted command   */
+	{0x0b, 0x14, 0x00}, /* ID not found      */
+	{0x04, 0x00, 0x00}, /* Device fault      */
+	{0x0b, 0x00, 0x00}, /* Device busy       */
+	{0x02, 0x04, 0x00}, /* Unit offline      */
+	{0x20, 0x04, 0x00}, /* Invalid Command   */
+	{0x10, 0x00, 0x00}, /* Intermediate      */
+	{0x07, 0x27, 0x00}, /* Write Protected   */
+};
+
+/*
+ * SCSI defined structure for inquiry data
+ * TODO: Seral number is currently messed up if you do
+ *       scsiinfo.  I'm not sure why and I think it comes out of 
+ *       here
+ */
+struct inquiry_data {                                                                                                            
+	u8 qual_type;                                                                                                            
+	u8 rmb_reserve;                                                                                                          
+	u8 version;                                                                                                              
+	u8 aerc_naca_hisup_format;                                                                                               
+	u8 addl_len;         
+	u8 sccs_reserved;                                                                                                        
+	u8 bque_encserv_vs_multip_mchngr_reserved;                                                                               
+	u8 reladr_reserved_linked_cmdqueue_vs;                                                                                   
+	char vendor[8];
+	char product[16];
+	char revision[4];
+	char vendor_specific[20];
+	char reserved1[2];
+	char version_descriptor[16];
+	char reserved2[22];
+};
+
+/*
+ * Our proc dir entry under /proc/drivers.  We use proc to configure
+ * this driver right now.
+ * TODO: For 2.5 move to sysfs
+ */
+#ifdef CONFIG_PROC_FS
+#define IBMVSCSIS_PROC_DIR "ibmvscsis"
+static struct proc_dir_entry *ibmvscsis_proc_dir;
+#endif
+
+extern int vio_num_address_cells;
+
+/* 
+ * an RPA command/response transport queue.  This is our structure
+ * that points to the actual queue.  feel free to modify this structure
+ * as needed
+ */
+struct crq_queue {
+	struct VIOSRP_CRQ *msgs;
+	int size, cur;
+	dma_addr_t msg_token;
+	spinlock_t lock;
+};
+
+/*
+ * This structure tracks our fundamental unit of work.  Whenever
+ * an SRP Information Unit (IU) arrives, we track all the good stuff
+ * here
+ */
+struct iu_entry {
+	union VIOSRP_IU *iu;
+	struct server_adapter *adapter;
+	struct list_head next;
+	dma_addr_t iu_token;
+	struct {
+		dma_addr_t remote_token;
+		char *data_buffer;
+		dma_addr_t data_token;
+		long data_len;
+		struct vdev *vd;
+		char in_use:1;
+		char diunder:1;
+		char diover:1;
+		char dounder:1;
+		char doover:1;
+		char write:1;
+		char linked:1;
+		int data_out_residual_count;
+		int data_in_residual_count;
+		int ioerr;
+	} req;
+};
+
+/* 
+ * a pool of ius for use 
+ */
+struct iu_pool {
+	spinlock_t lock;
+	struct list_head iu_entries;
+	struct iu_entry *list;
+	union VIOSRP_IU *iu_storage;
+	dma_addr_t iu_token;
+	u32 size;
+};
+
+/*
+ * Represents a single device that someone told us about
+ * that we treat as a LUN
+ */
+struct vdev {
+	struct list_head list;
+	char type; /* 'B' for block, 'S' for SCSI */
+	atomic_t refcount;
+	int disabled;
+	u64 lun;
+	struct {
+		struct block_device *bdev;
+		dev_t dev;
+		long blksize;
+		long lastlba;
+		int ro;
+	} b;
+};
+
+/*
+ * Represents a bus.  target #'s in SCSI are 6 bits long,
+ * so you can have 64 targets per bus
+ */
+#define TARGETS_PER_BUS (64)
+#define BUS_PER_ADAPTER (8)
+struct vbus {
+	struct vdev *vdev[TARGETS_PER_BUS];
+};
+
+/*
+ * Buffer cache
+ */
+struct dma_buffer {
+	dma_addr_t token;
+	char *addr;
+	size_t len;
+};
+#define DMA_BUFFER_CACHE_SIZE (16)
+#define DMA_BUFFER_INIT_COUNT (4)
+#define DMA_BUFFER_INIT_LEN (PAGE_SIZE*16)
+
+/* all driver data associated with a host adapter */
+struct server_adapter {
+	struct vio_dev *dma_dev;     
+	struct crq_queue queue;
+	struct work_struct crq_work;
+	struct work_struct endio_work;
+	atomic_t crq_task_count;  /* TODO: this is only for debugging. get rid of it */ 
+	atomic_t endio_task_count; /* TODO: this is only for debugging. get rid of it */ 
+	struct iu_pool pool;
+	spinlock_t lock;
+	struct bio *bio_done;
+	struct bio *bio_donetail;
+	struct vbus *vbus[8];
+	int nvdevs;
+	char name[32]; 
+	unsigned long liobn;
+	unsigned long riobn;
+	
+	/* This ugly expression allocates a bit array of 
+	 * in-use flags large enough for the number of buffers
+	 */
+	unsigned long dma_buffer_use[(DMA_BUFFER_CACHE_SIZE + 
+				      sizeof (unsigned long) -1)
+				     /sizeof(unsigned long)];
+	struct dma_buffer dma_buffer[DMA_BUFFER_CACHE_SIZE];
+
+	/* Statistics only */
+	atomic_t iu_count;  /* number allocated */
+	atomic_t bio_count;  /* number allocated */
+	atomic_t crq_processed;	
+	atomic_t interrupts;
+	atomic_t read_processed;
+	atomic_t write_processed;
+	atomic_t buffers_allocated;
+	atomic_t errors;
+};
+
+/* 
+ * Forward declarations
+ */
+static long send_rsp(struct iu_entry *iue, int status);
+
+/*
+ * The following are lifted from usb.h
+ */
+#define DEBUG 1
+#ifdef DEBUG
+#define dbg(format, arg...) if (ibmvscsis_debug) printk(KERN_WARNING __FILE__ ": " format , ## arg)
+#else
+#define dbg(format, arg...) do {} while (0)
+#endif
+#define err(format, arg...) printk(KERN_ERR __FILE__ ": " format , ## arg)
+#define info(format, arg...) printk(KERN_INFO __FILE__ ": " format  , ## arg)
+#define warn(format, arg...) printk(KERN_WARNING __FILE__ ": " format , ## arg)
+
+/* ==============================================================
+ * Utility Routines
+ * ==============================================================
+ */
+/*
+ * return an 8 byte lun given a bus, target, lun.  
+ * Today this only supports single level luns.  Should we add a level or a
+ * 64 bit LUN as input to support multi-level luns?
+ */
+u64 make_lun(unsigned int bus, unsigned int target, unsigned int lun) {
+	u16 result = (0x8000 |
+		      ((target & 0x003f) << 8) |
+		      ((bus & 0x0007) << 5) |
+		      (lun & 0x001f));
+	return ((u64)result) << 48;
+}
+
+/*
+ * Given an 8 byte LUN, return the first level bus/target/lun.
+ * Today this doesn't support multi-level LUNs
+ */
+#define GETBUS(x) ((int)((((u64)(x)) >> 53) & 0x0007))
+#define GETTARGET(x) ((int)((((u64)(x)) >> 56) & 0x003f))
+#define GETLUN(x) ((int)((((u64)(x)) >> 48) & 0x001f))
+
+static u8 getcontrolbyte(u8 *cdb) {
+        return cdb[COMMAND_SIZE(cdb[0])-1];
+}
+
+static u8 getlink(struct iu_entry *iue) {
+	return (getcontrolbyte(iue->iu->srp.cmd.cdb) & 0x01);
+}
+
+/*
+ * Given an SRP, figure out the data in length
+ */
+static int did_len(struct SRP_CMD *cmd) {
+	struct memory_descriptor *md;
+	struct indirect_descriptor *id;
+	int offset = cmd->additional_cdb_len*4;
+
+	switch (cmd->data_out_format) {
+	case SRP_NO_BUFFER:
+		offset += 0;
+		break;
+	case SRP_DIRECT_BUFFER:
+		offset += sizeof(struct memory_descriptor);
+		break;
+	case SRP_INDIRECT_BUFFER:
+		offset += sizeof(struct indirect_descriptor) 
+			+ ((cmd->data_out_count - 1) *sizeof(struct memory_descriptor));
+		break;
+	default:
+		err("ibmvscsis: did_len Invalid data_out_format %d\n",cmd->data_out_format);
+		return 0;
+	}
+
+	switch (cmd->data_in_format) {
+	case SRP_NO_BUFFER:
+		return 0;
+	case SRP_DIRECT_BUFFER:
+		md = (struct memory_descriptor *)(cmd->additional_data + offset);
+		return md->length;
+	case SRP_INDIRECT_BUFFER:
+		id = (struct indirect_descriptor *)(cmd->additional_data + offset);
+		return id->total_length;
+	default:
+		err("ibmvscsis: Invalid data_in_format %d\n",cmd->data_in_format);
+		return 0;
+	}
+}
+
+/* 
+ * We keep a pool of IUs, this routine builds the pool.  The pool is 
+ * per-adapter.  The size of the pool is negotiated as part of the SRP
+ * login, where we negotiate the number of requests (IUs) the client
+ * can send us.  This routine is not synchronized.
+ */
+static int initialize_iu_pool(struct server_adapter *adapter, int size)
+{
+	struct iu_pool *pool = &adapter->pool;
+	int i;
+
+	pool->size = size;
+	pool->lock = SPIN_LOCK_UNLOCKED;
+	INIT_LIST_HEAD(&pool->iu_entries);
+
+	pool->list = kmalloc(pool->size * sizeof(*pool->list), GFP_KERNEL);
+	if(!pool->list) {
+		err("Error: no memory for IU list\n");
+		return -ENOMEM;
+	}
+	memset(pool->list, 0x00, pool->size * sizeof(*pool->list));
+
+	pool->iu_storage = vio_alloc_consistent(adapter->dma_dev, pool->size * sizeof(*pool->iu_storage), &pool->iu_token);
+	if(!pool->iu_storage) {
+		err("Error: no memory for IU pool\n");
+		kfree(pool->list);
+		return -ENOMEM;
+	}
+
+	for(i = 0; i < pool->size; ++i) {
+		pool->list[i].iu = pool->iu_storage + i;
+		pool->list[i].iu_token = pool->iu_token + sizeof(*pool->iu_storage) * i;
+		pool->list[i].adapter = adapter;
+		list_add_tail(&pool->list[i].next, &pool->iu_entries);
+	}
+
+	return 0;
+}
+
+/*
+ * Free the pool we allocated in initialize_iu_pool
+ */
+static void release_iu_pool(struct server_adapter *adapter)
+{
+	struct iu_pool *pool = &adapter->pool;
+	int i, in_use = 0;
+	for(i = 0; i < pool->size; ++i)
+		if(pool->list[i].req.in_use)
+			++in_use;
+	if(in_use)
+		err("ibmvscsis: releasing event pool with %d events still in use?\n", in_use);
+	kfree(pool->list);
+	vio_free_consistent(adapter->dma_dev, pool->size * sizeof(*pool->iu_storage), pool->iu_storage, pool->iu_token);
+}
+
+/*
+ * Get an IU from the pool.  Return NULL of the pool is empty.  This
+ * routine is syncronized by a lock.  The routine sets all the important
+ * fields to 0
+ */
+static struct iu_entry *get_iu(struct server_adapter *adapter) {
+	struct iu_entry *e;
+	unsigned long flags;
+
+	spin_lock_irqsave(&adapter->pool.lock,flags);
+	if (!list_empty(&adapter->pool.iu_entries)) {
+		e = list_entry(adapter->pool.iu_entries.next, struct iu_entry, next);
+		list_del(adapter->pool.iu_entries.next);
+
+		if (e->req.in_use) {
+			err("Found in-use iue in pool!");
+		}
+
+		memset(&e->req,0x00,sizeof(e->req));
+
+		e->req.in_use = 1;
+	} else {
+		e = NULL;
+	}
+	
+	spin_unlock_irqrestore(&adapter->pool.lock, flags);
+	atomic_inc(&adapter->iu_count);
+	return e;
+}
+
+/* 
+ * Return an IU to the pool.  This routine is synchronized
+ */
+static void free_iu(struct iu_entry *iue) {
+	unsigned long flags;
+	if (iue->req.vd) {
+		atomic_dec(&iue->req.vd->refcount);
+	}
+
+	spin_lock_irqsave(&iue->adapter->pool.lock,flags);
+	if (iue->req.in_use == 0) {
+		warn("ibmvscsis: Internal error, freeing iue twice!\n");
+	} else {
+		iue->req.in_use = 0;
+		list_add_tail(&iue->next, &iue->adapter->pool.iu_entries);
+	}
+	spin_unlock_irqrestore(&iue->adapter->pool.lock,flags);
+	atomic_dec(&iue->adapter->iu_count);
+}
+
+/*
+ * Get a CRQ from the inter-partition queue.
+ */
+static struct VIOSRP_CRQ *crq_queue_next_crq(struct crq_queue *queue)
+{
+	struct VIOSRP_CRQ *crq;
+	unsigned long flags;
+
+	spin_lock_irqsave(&queue->lock,flags);
+	crq = &queue->msgs[queue->cur];
+	if(crq->valid & 0x80) {
+		if(++queue->cur == queue->size)
+			queue->cur = 0;
+	}
+	else
+		crq = NULL;
+	spin_unlock_irqrestore(&queue->lock,flags);
+
+	return crq;
+}
+
+/* 
+ * Make the RDMA hypervisor call.  There should be a better way to do this
+ * than inline assembler.
+ * TODO: Fix the inline assembler
+ */
+static long h_copy_rdma(long length, 
+			unsigned long sliobn, unsigned long slioba, 
+			unsigned long dliobn, unsigned long dlioba) {
+	long lpar_rc = 0;
+	__asm__ __volatile__(" li 3,0x110 \n\t"
+			     " mr 4, %1 \n\t"
+			     " mr 5, %2 \n\t"
+			     " mr 6, %3 \n\t"
+			     " mr 7, %4 \n\t"
+			     " mr 8, %5 \n\t"
+			     " .long 0x44000022 \n\t"
+			     " mr %0, 3 \n\t"
+			     : "=&r"(lpar_rc)
+			     : "r" (length), "r" (sliobn), "r" (slioba), "r" (dliobn), "r" (dlioba)
+			     : "r0","r3", "r4", "r5", "r6", "r7", "r8", "cr0", "cr1", "ctr", "xer", "memory");
+	return lpar_rc;
+}
+
+/*
+ * Send an SRP to another partition using the CRQ.
+ */
+static int send_srp(struct iu_entry *iue, u64 length)
+{
+	long rc, rc1;
+	union {
+		struct VIOSRP_CRQ cooked;
+		u64 raw[2];
+	} crq;
+
+
+	/* First copy the SRP */
+	rc = h_copy_rdma(length,
+			 iue->adapter->liobn,
+			 iue->iu_token,
+			 iue->adapter->riobn,
+			 iue->req.remote_token);
+
+	if (rc) {
+		err("Error: In send_srp, h_copy_rdma rc %ld\n",rc);
+	}
+
+	crq.cooked.valid      = 0x80;
+	crq.cooked.format     = VIOSRP_SRP_FORMAT;
+	crq.cooked.reserved   = 0x00;
+	crq.cooked.timeout    = 0x00;
+	crq.cooked.IU_length  =length;
+	crq.cooked.IU_data_ptr=iue->iu->srp.generic.tag;
+
+	if (rc == 0) {
+		crq.cooked.status  =0x99; /* TODO: is this right? */
+	} else {
+		crq.cooked.status  =0x00;
+	}
+
+	rc1 = plpar_hcall_norets(H_SEND_CRQ, iue->adapter->dma_dev->unit_address, 
+				 crq.raw[0], crq.raw[1]);
+
+	if (rc1) {
+		err("ibmvscscsis Error: In send_srp, h_send_crq rc %ld\n",rc1);
+		return rc1;
+	}
+
+	return rc;
+}
+
+/*
+ * Send data to a single SRP memory descriptor
+ * Returns amount of data sent, or negative value on error
+ */
+static long send_md_data(dma_addr_t stoken, int len, struct memory_descriptor *md, struct server_adapter *adapter) {
+	int tosend;
+	long rc;
+
+	if (len < md->length)
+		tosend = len;
+	else 
+		tosend = md->length;
+
+	rc =  h_copy_rdma(tosend,
+			  adapter->liobn,
+			  stoken,
+			  adapter->riobn,
+			  md->virtual_address);
+
+	if (rc != H_Success) {
+		err("Error sending data with h_copy_rdma, rc %ld\n", rc);
+		return -1;
+	}
+	
+	return tosend;
+}
+
+/*
+ * Send data to the SRP data_in buffers
+ * Returns amount of data sent, or negative value on error
+ */
+static long send_cmd_data(dma_addr_t stoken, int len, struct iu_entry *iue) {
+	struct SRP_CMD *cmd = &iue->iu->srp.cmd;
+	struct memory_descriptor *md;
+	struct indirect_descriptor *id;
+	int offset = 0;
+	int total_length = 0;
+	int i;
+	int thislen;
+	int bytes;
+	int sentlen = 0;
+
+	offset = cmd->additional_cdb_len*4;
+	
+	switch (cmd->data_out_format) {
+	case SRP_NO_BUFFER:
+		offset += 0;
+		break;
+	case SRP_DIRECT_BUFFER:
+		offset += sizeof(struct memory_descriptor);
+		break;
+	case SRP_INDIRECT_BUFFER:
+		offset += sizeof(struct indirect_descriptor) 
+			+ ((cmd->data_out_count - 1) *sizeof(struct memory_descriptor));
+		break;
+	default:
+		err("Error: did_len Invalid data_out_format %d\n",cmd->data_out_format);
+		return 0;
+	}
+
+	switch (cmd->data_in_format) {
+	case SRP_NO_BUFFER:
+		return 0;
+	case SRP_DIRECT_BUFFER:
+		md = (struct memory_descriptor *)(cmd->additional_data + offset);
+		return send_md_data(stoken, len, md, iue->adapter);
+	}
+
+	if (cmd->data_in_format != SRP_INDIRECT_BUFFER) {
+		err("Error: send_cmd_data Invalid data_in_format %d\n",cmd->data_in_format);
+		return 0;
+	}
+
+	id = (struct indirect_descriptor *)(cmd->additional_data + offset);
+
+	total_length = id->total_length;
+
+	/* Work through the partial memory descriptor list */
+	for (i=0; ((i<cmd->data_in_count) && (len)); i++) {
+		if (len > id->list[i].length) {
+			thislen = id->list[i].length;
+		} else {
+			thislen = len;
+		}
+
+		bytes = send_md_data(stoken+sentlen, thislen, id->list+i, iue->adapter);
+		if (bytes < 0)
+			return bytes;
+
+		if (bytes != thislen) {
+			warn("Error: Tried to send %d, sent %d\n",thislen, bytes);
+		}
+		
+		sentlen += bytes;
+		total_length -= bytes;
+		len -= bytes;
+	}
+
+	if (len) {
+		warn("Left over data sending to indirect buffer\n");
+		iue->req.diover = 1;
+		iue->req.data_in_residual_count = len;
+	}
+
+	return sentlen;
+}
+
+/*
+ * Get data from the other partition from a single SRP memory descriptor
+ * Returns amount of data sent, or negative value on error
+ */
+static long get_md_data(dma_addr_t ttoken, int len, struct memory_descriptor *md, struct server_adapter *adapter) {
+	int toget;
+	long rc;
+
+	if (len < md->length)
+		toget = len;
+	else 
+		toget = md->length;
+
+	rc =  h_copy_rdma(toget,
+			  adapter->riobn,
+			  md->virtual_address,
+			  adapter->liobn,
+			  ttoken);
+					  
+	if (rc != H_Success) {
+		err("Error sending data with h_copy_rdma, rc %ld\n", rc);
+		return -1;
+	}
+	
+	return toget;
+}
+
+/*
+ * Get data from an SRP data in area.
+ * Returns amount of data sent, or negative value on error
+ */
+static long get_cmd_data(dma_addr_t stoken, int len, struct iu_entry *iue) {
+	struct SRP_CMD *cmd = &iue->iu->srp.cmd;
+	struct memory_descriptor *md;
+	struct indirect_descriptor *id;
+	int offset = 0;
+	int total_length = 0;
+	int i;
+	int thislen;
+	int bytes;
+	int sentlen = 0;
+
+	offset = cmd->additional_cdb_len*4;
+	
+	switch (cmd->data_out_format) {
+	case SRP_NO_BUFFER:
+		return 0;
+		break;
+	case SRP_DIRECT_BUFFER:
+		md = (struct memory_descriptor *)(cmd->additional_data + offset);
+		return get_md_data(stoken, len, md, iue->adapter);
+		break;
+	}
+		
+	if (cmd->data_out_format != SRP_INDIRECT_BUFFER) {
+		err("get_cmd_data Invalid data_out_format %d\n",cmd->data_out_format);
+		return 0;
+	}
+	
+	id = (struct indirect_descriptor *)(cmd->additional_data + offset);
+	
+	total_length = id->total_length;
+
+	/* Work through the partial memory descriptor list */
+	for (i=0; ((i<cmd->data_out_count) && (len)); i++) {
+		if (len > id->list[i].length) {
+			thislen = id->list[i].length;
+		} else {
+			thislen = len;
+		}
+
+		bytes = get_md_data(stoken+sentlen, thislen, id->list+i, iue->adapter);
+		if (bytes < 0)
+			return bytes;
+
+		if (bytes != thislen) {
+			err("Tried to send %d, sent %d\n",thislen, bytes);
+		}
+		
+		sentlen += bytes;
+		total_length -= bytes;
+		len -= bytes;
+	}
+
+	if (len) {
+		warn("Left over data get indirect buffer\n");
+	}
+
+	return sentlen;
+}
+
+/*
+ * Get some data buffers to start.  This doesn't lock the adapter structure!
+ */
+static void init_data_buffer(struct server_adapter *adapter) {
+	int i;
+	
+	for (i=0; i<DMA_BUFFER_INIT_COUNT; i++) {
+		if (adapter->dma_buffer[i].addr == NULL) {
+			adapter->dma_buffer[i].addr = (char *)
+				vio_alloc_consistent(adapter->dma_dev, 
+						     DMA_BUFFER_INIT_LEN,
+						     &adapter->dma_buffer[i].token);
+			adapter->dma_buffer[i].len = DMA_BUFFER_INIT_LEN;
+			dbg("data buf %p token %8.8x, len %ld\n",
+			     adapter->dma_buffer[i].addr,
+			     adapter->dma_buffer[i].token,
+			     adapter->dma_buffer[i].len);
+			atomic_inc(&adapter->buffers_allocated);
+		}
+	}
+
+	return;
+}
+/*
+ * Get a memory buffer that includes a mapped TCE.  
+ */
+static void get_data_buffer(char **buffer, dma_addr_t *data_token, size_t len, struct server_adapter *adapter) {
+	int i;
+	
+	for (i=0; i<DMA_BUFFER_CACHE_SIZE; i++) {
+		if ((adapter->dma_buffer[i].addr) &&
+		    (adapter->dma_buffer[i].len >= len) &&
+		    (!test_and_set_bit(i,adapter->dma_buffer_use)))
+		{
+			*buffer = adapter->dma_buffer[i].addr;
+			*data_token = adapter->dma_buffer[i].token;
+			return;
+		}
+	}
+
+	/* Couldn't get a buffer!  Try and get a new one */
+	*buffer = (char *)vio_alloc_consistent(adapter->dma_dev, len,
+					       data_token);
+	atomic_inc(&adapter->buffers_allocated);
+	return;
+}
+
+/*
+ * Free a memory buffer that includes a mapped TCE.  
+ */
+static void free_data_buffer(char *buffer, dma_addr_t data_token, size_t len, struct server_adapter *adapter) {
+	int i;
+	
+	/* First see if this buffer is already in the cache */
+	for (i=0; i<DMA_BUFFER_CACHE_SIZE; i++) {
+		if (adapter->dma_buffer[i].addr == buffer) {
+			if (adapter->dma_buffer[i].token != data_token) {
+				err("Inconsistent data buffer pool info!\n");
+			}
+			if (!test_and_clear_bit(i,adapter->dma_buffer_use)) {
+				err("Freeing data buffer twice!\n");
+			}
+			return;
+		}
+	}
+	
+	/* See if there is an empty slot in our list */
+	for (i=0; i<DMA_BUFFER_CACHE_SIZE; i++) {
+		if (!test_and_set_bit(i,adapter->dma_buffer_use)) {
+			if (adapter->dma_buffer[i].addr == NULL) {
+				adapter->dma_buffer[i].addr = buffer;
+				adapter->dma_buffer[i].token = data_token;
+				clear_bit(i,adapter->dma_buffer_use);
+				return;
+			} else {
+				clear_bit(i,adapter->dma_buffer_use);
+			}
+		}
+	}
+
+	/* Now see if there is a smaller buffer we should throw out */
+	for (i=0; i<DMA_BUFFER_CACHE_SIZE; i++) {
+		if (!test_and_set_bit(i,adapter->dma_buffer_use)) {
+			if (adapter->dma_buffer[i].len < len) {
+				vio_free_consistent(adapter->dma_dev, 
+						    adapter->dma_buffer[i].len,
+						    adapter->dma_buffer[i].addr,
+						    adapter->dma_buffer[i].token);
+			
+				atomic_dec(&adapter->buffers_allocated);
+
+				adapter->dma_buffer[i].addr = buffer;
+				adapter->dma_buffer[i].token = data_token;
+				adapter->dma_buffer[i].len = len;
+				clear_bit(i,adapter->dma_buffer_use);
+				return;
+			} else {
+				clear_bit(i,adapter->dma_buffer_use);
+			}
+		}
+	}
+
+	/* No space to cache this.  Give it back to the kernel */
+	vio_free_consistent(adapter->dma_dev, len,
+			    buffer, data_token);
+	atomic_dec(&adapter->buffers_allocated);
+}
+
+/*
+ * Release all the data buffers
+ */
+static void release_data_buffer(struct server_adapter *adapter) {
+	int i;
+	int free_in_use = 0;
+	
+	for (i=0; i<DMA_BUFFER_INIT_COUNT; i++) {
+		if (adapter->dma_buffer[i].addr != NULL) {
+			if (test_bit(i,adapter->dma_buffer_use)) {
+				free_in_use++;
+			}
+			vio_free_consistent(adapter->dma_dev, 
+					    adapter->dma_buffer[i].len,
+					    adapter->dma_buffer[i].addr,
+					    adapter->dma_buffer[i].token);
+			
+			atomic_dec(&adapter->buffers_allocated);
+		}
+	}
+	
+	if (free_in_use) {
+		err("Freeing %d in-use data buffers\n", free_in_use);
+	}
+	return;
+}
+
+/*
+ * the routine that gets called on end_io of our bios.  We basically
+ * schedule the processing to be done in our task, since we don't want
+ * do things like RDMA in someone else's interrupt handler
+ *
+ * Each iu request may result in multiple bio requests.  only proceed
+ * when all the bio requests have done.
+ */
+static int ibmvscsis_end_io(struct bio *bio, unsigned int nbytes, int error)
+{
+	struct iu_entry *iue = (struct iu_entry *)bio->bi_private;
+	struct server_adapter *adapter = iue->adapter;
+	unsigned long flags;
+
+	if (!test_bit(BIO_UPTODATE, &bio->bi_flags)) {
+		iue->req.ioerr = 1;
+	};
+
+	/* Add the bio to the done queue */
+	spin_lock_irqsave(&adapter->lock, flags);
+	if (adapter->bio_donetail) {
+		adapter->bio_donetail->bi_next = bio;
+		adapter->bio_donetail = bio;
+	} else
+		adapter->bio_done = adapter->bio_donetail = bio;
+	spin_unlock_irqrestore(&adapter->lock, flags);
+
+	/* Schedule the task */
+	schedule_work(&adapter->endio_work);
+
+	return 0;
+}
+
+/*
+ * Find the vdev structure from the LUN field in an SRP IUE
+ * Note that this routine bumps a refcount field in the vdev.
+ * Normally this is done when free_iu is called.
+ */
+static struct vdev *find_device(struct iu_entry *iue) {
+	u16 *lun = (u16 *)&iue->iu->srp.cmd.lun;
+	u32 bus = (lun[0] & 0x00E0) >> 5;
+	u32 target = (lun[0] & 0x3F00) >> 8;
+	u32 slun = (lun[0] & 0x001F);
+	struct vdev *vd;
+	unsigned long flags;
+
+	/* If asking for a lun other than 0, return nope */
+	if (slun) {
+		return NULL;
+	}
+
+	/* Only from SRP CMD */
+	if (iue->iu->srp.generic.type != SRP_CMD_TYPE)
+		return NULL;
+
+	/* if not a recognized LUN format, return NULL */
+	if ((lun[0] & 0xC000) != 0x8000) 
+		return NULL;
+
+	spin_lock_irqsave(&iue->adapter->lock,flags);
+	if (iue->adapter->vbus[bus] == NULL) {
+		spin_unlock_irqrestore(&iue->adapter->lock,flags);
+		return NULL;
+	}
+
+	vd =  iue->adapter->vbus[bus]->vdev[target];
+
+	if ((vd == NULL) || (vd->disabled)) {
+		spin_unlock_irqrestore(&iue->adapter->lock,flags);
+		return NULL;
+	}
+
+	if (vd) {
+		atomic_inc(&vd->refcount);
+	}
+	spin_unlock_irqrestore(&iue->adapter->lock,flags);
+
+	return vd;
+}
+
+/*
+ * Process BH buffer completions.  When the end_io routine gets called 
+ * we queue the bio on an internal queue and start a task to process them
+ */
+static void endio_task(void *data)
+{
+	struct server_adapter *adapter = (struct server_adapter *)data;
+	struct bio *bio;
+	int bytes;
+	unsigned long flags;
+
+	if (atomic_inc_return(&adapter->endio_task_count) > 1) {
+		err("In endio_task twice!!!\n");
+	}
+
+	do {
+		spin_lock_irqsave(&adapter->lock,flags);
+		if ((bio = adapter->bio_done)) {
+			if (bio == adapter->bio_donetail)
+				adapter->bio_donetail = NULL;
+			adapter->bio_done = bio->bi_next;
+			bio->bi_next = NULL;
+		}
+		spin_unlock_irqrestore(&adapter->lock,flags);
+		
+		if (bio) {
+			struct iu_entry *iue = (struct iu_entry *)bio->bi_private;
+
+			if (!iue->req.ioerr ) {
+				/* return data if this was a read */
+				if (!iue->req.write) {
+					bytes = send_cmd_data(iue->req.data_token, iue->req.data_len, iue);
+					if (bytes != iue->req.data_len) {
+						err("Error sending data on response (tried %d, sent %d\n",
+						    bio->bi_size, bytes);
+						send_rsp(iue, SENSE_ABORT);
+					} else {
+						send_rsp(iue, SENSE_SUCCESS);
+					}
+				} else {
+					send_rsp(iue, SENSE_SUCCESS);
+				}
+			} else {
+				err("Block operation failed\n");
+				send_rsp(iue, SENSE_DEVICE_FAULT);
+			}
+
+			free_data_buffer(iue->req.data_buffer, iue->req.data_token, iue->req.data_len, adapter); 
+				
+			free_iu(iue);
+			
+			bio_put(bio);
+			atomic_dec(&adapter->bio_count);
+		}
+	} while (bio);
+	atomic_dec(&adapter->endio_task_count);
+}
+
+/* ==============================================================
+ * SCSI Command Emulation Routines
+ * ==============================================================
+ */
+
+/*
+ * Process an inquiry SCSI Command
+ */
+static void process_inquiry(struct iu_entry *iue) {
+	struct inquiry_data *id;
+	dma_addr_t data_token;
+	int bytes;
+
+	id = (struct inquiry_data *)vio_alloc_consistent(iue->adapter->dma_dev, 
+							sizeof(*id),
+							&data_token);
+	memset(id,0x00,sizeof(*id));
+
+	/* If we have a valid device */
+	if (iue->req.vd) {
+		dbg("  inquiry returning device\n");
+		id->qual_type = 0x00;    /* Direct Access    */
+		id->rmb_reserve = 0x00;  /* TODO: CD is removable  */
+		id->version = 0x84;      /* ISO/IE                 */
+		id->aerc_naca_hisup_format = 0x22; /* naca & format 0x02 */
+		id->addl_len = sizeof(*id)-4;   /* sizeof(*this) - 4 */
+		id->bque_encserv_vs_multip_mchngr_reserved = 0x00; 
+		id->reladr_reserved_linked_cmdqueue_vs = 0x02; /* CMDQ   */
+		memcpy(id->vendor,"IBM     ",8);
+		memcpy(id->product,"VSCSI blkdev    ",16);
+		memcpy(id->revision,"0001",4);
+	} else {
+		dbg("  inquiry returning no device\n");
+		id->qual_type = 0x7F; /* Not supported, no device */
+	}
+		
+	bytes = send_cmd_data(data_token, sizeof(*id), iue);
+		
+	vio_free_consistent(iue->adapter->dma_dev, 
+			    sizeof(*id),
+			    id,
+			    data_token);
+
+	if (bytes < 0) {
+		send_rsp(iue, SENSE_DEVICE_FAULT);
+	} else {
+		send_rsp(iue, SENSE_SUCCESS);
+	}
+	
+	free_iu(iue);
+}
+
+/*
+ * Handle an I/O.  Called by WRITE6, WRITE10, etc
+ */
+static void process_rw(char *cmd, int rw, struct iu_entry *iue, long lba, long len) {
+	char *buffer;
+	struct bio * bio;
+	int bytes;
+	int num_biovec;
+	int cur_biovec;
+
+	dbg("%s %16.16lx[%d:%d:%d][%d:%d] lba %ld len %ld reladr %d link %d\n",
+	    cmd,
+	    iue->iu->srp.cmd.lun,
+	    GETBUS(iue->iu->srp.cmd.lun),
+	    GETTARGET(iue->iu->srp.cmd.lun),
+	    GETLUN(iue->iu->srp.cmd.lun),
+	    MAJOR(iue->req.vd->b.dev),
+	    MINOR(iue->req.vd->b.dev),
+	    lba,
+	    len / iue->req.vd->b.blksize,
+	    iue->iu->srp.cmd.cdb[1] & 0x01,
+	    iue->req.linked);
+
+	if (rw == WRITE) {
+		atomic_inc(&iue->adapter->write_processed);
+	} else if (rw == READ) {
+		atomic_inc(&iue->adapter->read_processed);
+	} else {
+		err("Major internal error...rw not read or write\n");
+		send_rsp(iue, SENSE_DEVICE_FAULT); 
+		
+		free_iu(iue);
+		return;
+	}
+	
+	if (len == 0) {
+		warn("Zero length I/O\n");
+		send_rsp(iue, SENSE_INVALID_CMD); 
+		
+		free_iu(iue);
+		return;
+	}
+	
+	/* Writing to a read-only device */
+	if ((rw == WRITE) && (iue->req.vd->b.ro)) {
+		warn("WRITE op to r/o device\n");
+		send_rsp(iue, SENSE_WRITE_PROT);
+		
+		free_iu(iue);
+		return;
+	}
+
+	get_data_buffer(&buffer, &iue->req.data_token, len, iue->adapter);
+	iue->req.data_buffer = buffer;
+	iue->req.data_len = len;
+	if (buffer == NULL) {
+		err("Not able to get a data buffer (%lu pages)\n", len/PAGE_SIZE);
+		send_rsp(iue, SENSE_DEVICE_FAULT); 
+		
+		free_iu(iue);
+		return;
+	}
+
+	/* if reladr */
+	if (iue->iu->srp.cmd.cdb[1] & 0x01) {
+		lba = lba+iue->req.vd->b.lastlba;
+	}
+
+	/* If this command is linked, Keep this lba */
+	if (iue->req.linked) {
+		iue->req.vd->b.lastlba = lba;
+	} else {
+		iue->req.vd->b.lastlba = 0;
+	}
+
+	if (rw == WRITE) {
+		iue->req.write = 1;
+		/* Get the data */
+		bytes = get_cmd_data(iue->req.data_token, len, iue);
+		if (bytes != len) {
+			err("Error transferring data\n");
+			send_rsp(iue, SENSE_DEVICE_FAULT); 
+			
+			free_iu(iue);
+			return;
+		}
+	}
+	
+	num_biovec = (len-1)/PAGE_CACHE_SIZE + 1;
+
+	bio = bio_alloc(GFP_ATOMIC, num_biovec);
+	if (!bio) {
+		/* Ouch.  couldn't get a bio.  Mark this I/O as 
+		 * in error, then decrement the outstanding bio.
+		 * If there are still outstanding bio, they will send
+		 * the error and free the IU.  If there are none, we
+		 * should do it here
+		 */
+		iue->req.ioerr = 1;
+		err("Not able to get a bio\n");
+		send_rsp(iue, SENSE_DEVICE_FAULT); 
+		free_iu(iue);
+		return;
+	}
+
+	atomic_inc(&iue->adapter->bio_count);
+	bio->bi_size = len;
+	bio->bi_bdev = iue->req.vd->b.bdev;
+	bio->bi_sector = lba;
+	bio->bi_end_io = &ibmvscsis_end_io;
+	bio->bi_private = iue;
+	bio->bi_rw = (rw == WRITE) ? 1 : 0;
+	bio->bi_phys_segments = 1;
+	bio->bi_hw_segments = 1;
+			
+	/* This all assumes that the buffers we get are page-aligned */
+	for (cur_biovec=0; cur_biovec<num_biovec; cur_biovec++) {
+		long thislen;
+
+		if (len > PAGE_CACHE_SIZE) {
+			thislen = PAGE_CACHE_SIZE;
+		} else {
+			thislen = len;
+		}
+			
+		bio->bi_io_vec[cur_biovec].bv_page = virt_to_page(buffer);
+		bio->bi_io_vec[cur_biovec].bv_len = thislen;
+		bio->bi_io_vec[cur_biovec].bv_offset = (unsigned long)buffer & PAGE_OFFSET_MASK;
+		bio->bi_vcnt++;
+
+		len -= thislen;
+		buffer += thislen;
+	}
+	generic_make_request(bio);
+}
+
+/*
+ * Process a READ6
+ */
+static void processRead6(struct iu_entry *iue) {
+	long lba = (*((u32 *)(iue->iu->srp.cmd.cdb))) & 0x001FFFFF;
+	long len = iue->iu->srp.cmd.cdb[4];
+	
+	/* Length of 0 indicates 256 */
+	if (len == 0) {
+		len = 256;
+	}
+	
+	len = len * iue->req.vd->b.blksize;
+	
+	process_rw("Read6", READ, iue,lba,len);
+}
+
+/*
+ * Process a READ10
+ */
+static void processRead10(struct iu_entry *iue) {
+	long lba = *((u32 *)(iue->iu->srp.cmd.cdb+2));
+	long len = *((u16 *)(iue->iu->srp.cmd.cdb+7))*iue->req.vd->b.blksize;
+
+	process_rw("Read10", READ, iue,lba,len);
+}
+
+/*
+ * Process a READ10
+ */
+static void processRead12(struct iu_entry *iue) {
+	long lba = *((u32 *)(iue->iu->srp.cmd.cdb+2));
+	long len = *((u32 *)(iue->iu->srp.cmd.cdb+6))*iue->req.vd->b.blksize;
+
+	process_rw("Read12", READ, iue,lba,len);
+}
+
+
+static void processWrite6(struct iu_entry *iue) {
+	long lba = (*((u32 *)(iue->iu->srp.cmd.cdb))) & 0x001FFFFF;
+	long len = iue->iu->srp.cmd.cdb[4];
+	
+	/* Length of 0 indicates 256 */
+	if (len == 0) {
+		len = 256;
+	}
+	
+	len = len * iue->req.vd->b.blksize;
+	
+	process_rw("Write6", WRITE, iue,lba,len);
+}
+
+static void processWrite10(struct iu_entry *iue) {
+	long lba = *((u32 *)(iue->iu->srp.cmd.cdb+2));
+	long len = *((u16 *)(iue->iu->srp.cmd.cdb+7))*iue->req.vd->b.blksize;
+
+	process_rw("Write10", WRITE, iue,lba,len);
+}
+
+static void processWrite12(struct iu_entry *iue) {
+	long lba = *((u32 *)(iue->iu->srp.cmd.cdb+2));
+	long len = *((u32 *)(iue->iu->srp.cmd.cdb+6))*iue->req.vd->b.blksize;
+
+	process_rw("Write12", WRITE, iue,lba,len);
+}
+
+/*
+ * Handle Read Capacity
+ */
+static void processReadCapacity(struct iu_entry *iue) {
+	struct ReadCapacityData {
+		u32 blocks;
+		u32 blocksize;
+	} *cap;
+	dma_addr_t data_token;
+	int bytes;
+
+	cap = (struct ReadCapacityData *)vio_alloc_consistent(iue->adapter->dma_dev, 
+							      sizeof(*cap),
+							      &data_token);
+
+	cap->blocksize = iue->req.vd->b.blksize;
+	cap->blocks = iue->req.vd->b.bdev->bd_inode->i_size / iue->req.vd->b.blksize;
+
+	info("Reporting capacity as %u block of size %u\n",cap->blocks, cap->blocksize);
+	
+	bytes = send_cmd_data(data_token, sizeof(*cap), iue);
+	
+	vio_free_consistent(iue->adapter->dma_dev, 
+			    sizeof(*cap),
+			    cap,
+			    data_token);
+
+	if (bytes != sizeof(*cap)) {
+		err("Error sending read capacity data. bytes %d, wanted %ld\n",bytes,sizeof(*cap));
+	}
+	
+	send_rsp(iue, SENSE_SUCCESS);
+	
+	free_iu(iue);
+}
+
+/*
+ * Process Mode Sense
+ * TODO: I know scsiinfo asks for a bunch of mode pages not implemented here.
+ *       Also, we need to act differently for virtual disk and virtual CD
+ */
+#define MODE_SENSE_BUFFER_SIZE (512)
+static void processModeSense(struct iu_entry *iue) {
+	dma_addr_t data_token;
+	int bytes;
+	
+	u8 *mode = (u8 *)vio_alloc_consistent(iue->adapter->dma_dev, 
+					      MODE_SENSE_BUFFER_SIZE,
+					      &data_token);
+	/* which page */
+	switch (iue->iu->srp.cmd.cdb[2]) {
+	case 0:
+	case 0x3f:
+		mode[1] = 0x00; /* Default medium */
+		if (iue->req.vd->b.ro) {
+			mode[2] = 0x80; /* device specific  */
+		} else {
+			mode[2] = 0x00; /* device specific  */
+		}
+		/* note the DPOFUA bit is set to zero! */
+		mode[3] = 0x08; /* block descriptor length */
+		*((u32 *)&mode[4]) = iue->req.vd->b.bdev->bd_inode->i_size / iue->req.vd->b.blksize;
+		*((u32 *)&mode[8]) = iue->req.vd->b.blksize;
+		bytes = mode[0] = 12; /* length */
+		break;
+		
+	case 0x08: /* Cache page */
+		/* length should be 4 */
+		if (iue->iu->srp.cmd.cdb[4] != 4 && iue->iu->srp.cmd.cdb[4] != 0x20) {
+			send_rsp(iue, SENSE_INVALID_CMD); 
+			vio_free_consistent(iue->adapter->dma_dev, 
+					    MODE_SENSE_BUFFER_SIZE,
+					    mode,
+					    data_token);
+			free_iu(iue);
+			return;
+		}
+
+		mode[1] = 0x00; /* Default medium */
+		if (iue->req.vd->b.ro) {
+			mode[2] = 0x80; /* device specific */
+		} else {
+			mode[2] = 0x00; /* device specific */
+		}
+		/* note the DPOFUA bit is set to zero! */
+		mode[3] = 0x08; /* block descriptor length */
+		*((u32 *)&mode[4]) = iue->req.vd->b.bdev->bd_inode->i_size / iue->req.vd->b.blksize;
+		*((u32 *)&mode[8]) = iue->req.vd->b.blksize; 
+		
+		/* Cache page */
+		mode[12] = 0x08; /* page */
+		mode[13] = 0x12; /* page length */
+		mode[14] = 0x01; /* no cache (0x04 for read/write cache) */
+		
+		bytes = mode[0] = 12 + mode[13]; /* length */
+		break;
+	default:
+		warn("Request for unknown mode page %d\n",iue->iu->srp.cmd.cdb[2]);
+		send_rsp(iue, SENSE_INVALID_CMD); 
+		vio_free_consistent(iue->adapter->dma_dev, 
+				    MODE_SENSE_BUFFER_SIZE,
+				    mode,
+				    data_token);
+		free_iu(iue);
+		return;
+	}
+		
+	bytes = send_cmd_data(data_token, bytes, iue);
+	
+	vio_free_consistent(iue->adapter->dma_dev, 
+			    MODE_SENSE_BUFFER_SIZE,
+			    mode,
+			    data_token);
+
+	send_rsp(iue, SENSE_SUCCESS);
+	
+	free_iu(iue);
+	return;
+}
+
+/*
+ * Report LUNS command.
+ */
+static void processReportLUNs(struct iu_entry *iue) {
+	int listsize = did_len(&iue->iu->srp.cmd);
+	dma_addr_t data_token;
+	int index = 2;  /* Start after the two entries (length and LUN0) */
+	int bus;
+	int target;
+	int bytes;
+	unsigned long flags;
+
+	u64 *lunlist = (u64 *)vio_alloc_consistent(iue->adapter->dma_dev, 
+						   listsize,
+						   &data_token);
+
+	memset(lunlist,0x00,listsize);
+	
+	/* work out list size in units of u64 */
+	listsize = listsize / 8;
+
+	if (listsize < 1) {
+		warn("report luns buffer too small\n");
+		send_rsp(iue, SENSE_INVALID_CMD); 
+		free_iu(iue);
+	}
+	
+	spin_lock_irqsave(&iue->adapter->lock,flags);
+
+	/* send lunlist of size 1 when requesting lun is not all zeros */
+	if (iue->req.vd->lun != 0x0LL) {
+		*lunlist = ((u64)1*8) << 32;
+		goto send_lunlist;
+	}
+
+	/* return the total number of luns plus LUN0 in bytes */
+	*lunlist = (((u64)((iue->adapter->nvdevs+1)*8)) << 32);
+
+	dbg("reporting %d luns\n",iue->adapter->nvdevs+1);
+	/* loop through the bus */
+	for (bus = 0; bus < BUS_PER_ADAPTER; bus++) {
+		/* If this bus exists */
+		if (iue->adapter->vbus[bus]) {
+			/* loop through the targets */
+			for (target = 0; target < TARGETS_PER_BUS; target++)
+			{
+				/* If the target exists */
+				if (iue->adapter->vbus[bus]->vdev[target]) {
+					if ((index < listsize) &&
+					    (!iue->adapter->vbus[bus]->vdev[target]->disabled)) {
+						lunlist[index++] = iue->adapter->vbus[bus]->vdev[target]->lun;
+						dbg("  lun %16.16lx\n",iue->adapter->vbus[bus]->vdev[target]->lun);
+					}
+				}
+			}
+		}
+	}
+	
+send_lunlist:
+	spin_unlock_irqrestore(&iue->adapter->lock,flags);
+
+	bytes = send_cmd_data(data_token, (index * 8), iue);
+	
+	vio_free_consistent(iue->adapter->dma_dev, 
+			    listsize * 8,
+			    lunlist,
+			    data_token);
+
+	if (bytes != (index * 8)) {
+		err("Error sending report luns data. bytes %d, wanted %d\n",bytes,index * 4);
+		send_rsp(iue, SENSE_ABORT);
+	} else {
+		send_rsp(iue, SENSE_SUCCESS);
+	}
+	
+	free_iu(iue);
+	return;}
+
+/*
+ * Process an IU.  
+ *
+ * Note that THIS routine is responsible for returning the IU from the pool
+ * The current assumption is that all the process routines called from here
+ * are, in turn, responsible for freeing the IU
+ */
+static void process_cmd(struct iu_entry *iue)
+{
+	union VIOSRP_IU *iu = iue->iu;
+
+	iue->req.vd = find_device(iue);	
+
+	if ((iue->req.vd == NULL) && 
+	    (iu->srp.cmd.cdb[0] != REPORT_LUNS) &&
+	    (iu->srp.cmd.cdb[0] != INQUIRY)) {
+		dbg("Cmd %2.2x for unknown LUN %16.16lx\n",
+		    iu->srp.cmd.cdb[0],
+		    iue->iu->srp.cmd.lun);
+		send_rsp(iue, SENSE_INVALID_ID);
+		free_iu(iue);
+		return;
+	} 
+
+	iue->req.linked = getlink(iue);
+
+	switch(iu->srp.cmd.cdb[0]) {
+	case READ_6:
+		processRead6(iue);
+		break;
+	case READ_10:
+		processRead10(iue);
+		break;
+	case READ_12:
+		processRead12(iue);
+		break;
+	case WRITE_6:
+		processWrite6(iue);
+		break;
+	case WRITE_10:
+		processWrite10(iue);
+		break;
+	case WRITE_12:
+		processWrite12(iue);
+		break;
+	case REPORT_LUNS:
+		dbg("REPORT LUNS lun %16.16lx\n",iue->iu->srp.cmd.lun);
+		processReportLUNs(iue);
+		break;
+	case INQUIRY:
+		dbg("INQUIRY lun %16.16lx\n",iue->iu->srp.cmd.lun);
+		process_inquiry(iue);
+		break;
+	case READ_CAPACITY:
+		dbg("READ CAPACITY lun %16.16lx\n",iue->iu->srp.cmd.lun);
+		processReadCapacity(iue);
+		break;
+	case MODE_SENSE:
+		dbg("MODE SENSE lun %16.16lx\n",iue->iu->srp.cmd.lun);
+		processModeSense(iue);
+		break;
+	case TEST_UNIT_READY:
+		/* we already know the device exists */
+		dbg("TEST UNIT READY lun %16.16lx\n",iue->iu->srp.cmd.lun);
+		send_rsp(iue, SENSE_SUCCESS);
+		free_iu(iue);
+		break;
+	case START_STOP:
+		/* just respond OK */
+		dbg("START_STOP lun %16.16lx\n",iue->iu->srp.cmd.lun);
+		send_rsp(iue, SENSE_SUCCESS);
+		free_iu(iue);
+		break;
+	default:
+		warn("Unsupported SCSI Command 0x%2.2x\n",iu->srp.cmd.cdb[0]);
+		send_rsp(iue, SENSE_INVALID_CMD);
+		free_iu(iue);
+	}
+}
+
+/* ==============================================================
+ * SRP Processing Routines
+ * ==============================================================
+ */
+/*
+ * Process an incoming SRP Login request
+ */
+static void process_login(struct iu_entry *iue)
+{
+	union VIOSRP_IU *iu = iue->iu;
+	u64 tag = iu->srp.generic.tag;
+
+	/* TODO handle case that requested size is wrong and buffer format is wrong */
+	memset(iu,0x00,sizeof(struct SRP_LOGIN_RSP));
+	iu->srp.login_rsp.type                          = SRP_LOGIN_RSP_TYPE;
+	iu->srp.login_rsp.request_limit_delta           = iue->adapter->pool.size;
+	iu->srp.login_rsp.tag                           = tag;
+	iu->srp.login_rsp.max_initiator_to_target_iulen = sizeof(union SRP_IU);
+	iu->srp.login_rsp.max_target_to_initiator_iulen = sizeof(union SRP_IU);
+	iu->srp.login_rsp.supported_buffer_formats      = 0x0002; /* direct and indirect */
+	iu->srp.login_rsp.multi_channel_result          = 0x00; /* TODO fix if we were already logged in */
+	
+	send_srp(iue,sizeof(iu->srp.login_rsp));
+}
+
+/*
+ * Send an SRP response that includes sense data
+ */
+static long send_rsp(struct iu_entry *iue, int status) {
+	u8 *sense = iue->iu->srp.rsp.sense_and_response_data;
+	u64 tag = iue->iu->srp.generic.tag;
+	union VIOSRP_IU *iu = iue->iu;
+
+	if (status != SENSE_SUCCESS) {
+		atomic_inc(&iue->adapter->errors);
+	}
+
+	/* If the linked bit is on and status is good */
+	if ((iue->req.linked) && (status == SENSE_SUCCESS)) {
+		status = SENSE_INTERMEDIATE;
+	}
+
+	memset(iu,0x00,sizeof(struct SRP_RSP));
+	iu->srp.rsp.type                          = SRP_RSP_TYPE;
+	iu->srp.rsp.request_limit_delta           = 1;
+	iu->srp.rsp.tag                           = tag;
+
+	iu->srp.rsp.diunder                       = iue->req.diunder;
+	iu->srp.rsp.diover                        = iue->req.diover;
+	iu->srp.rsp.dounder                       = iue->req.dounder;
+	iu->srp.rsp.doover                        = iue->req.doover;
+
+	iu->srp.rsp.data_in_residual_count        = iue->req.data_in_residual_count;
+	iu->srp.rsp.data_out_residual_count       = iue->req.data_out_residual_count;
+
+	iu->srp.rsp.rspvalid                      = 0;
+
+	iu->srp.rsp.response_data_list_length     = 0;
+
+	if (status) {
+		iu->srp.rsp.status                        = SAM_STAT_CHECK_CONDITION;
+		iu->srp.rsp.snsvalid                      = 1;
+		iu->srp.rsp.sense_data_list_length        = 18; /* TODO be smarter about this */
+
+		/* Valid bit and 'current errors' */
+		sense[0] = (0x1 << 7 | 0x70);
+		
+		/* Sense key */
+		sense[2] = ibmvscsis_sense_data[status][0];
+		
+		/* Additional sense length */
+		sense[7] = 0xa; /* 10 bytes */
+		
+		/* Additional sense code */
+		sense[12] = ibmvscsis_sense_data[status][1];
+		
+		/* Additional sense code qualifier */
+		sense[13] = ibmvscsis_sense_data[status][2];
+	} else {
+		iu->srp.rsp.status                        = 0;
+	}
+
+	send_srp(iue,sizeof(iu->srp.rsp));
+
+	return 0;
+}
+
+static void process_iu(struct VIOSRP_CRQ *crq, struct server_adapter *adapter)
+{
+	struct iu_entry *iue = get_iu(adapter);
+	union VIOSRP_IU *iu;
+	int queued = 0;
+	long rc;
+
+	if (iue == NULL) {
+		/* TODO Yikes! */
+		warn("Error getting IU from pool, other side exceeded limit\n");
+		return;
+	}
+
+	iue->req.remote_token = crq->IU_data_ptr;
+
+	rc = h_copy_rdma(crq->IU_length,
+			 iue->adapter->riobn,
+			 iue->req.remote_token,
+			 adapter->liobn,
+			 iue->iu_token);
+
+	iu = iue->iu;
+	
+	if (rc) {
+		err("Got rc %ld from h_copy_rdma\n",rc);
+	}
+	
+	if (crq->format == VIOSRP_MAD_FORMAT) {
+		switch (iu->srp.generic.type) {
+		case VIOSRP_EMPTY_IU_TYPE:
+			warn("Unsupported EMPTY MAD IU\n");
+			break;
+		case VIOSRP_ERROR_LOG_TYPE:
+			warn("Unsupported ERROR LOG MAD IU\n");
+			break;
+		case VIOSRP_ADAPTER_INFO_TYPE:
+			warn("Unsupported ADAPTER INFO MAD IU\n");
+			break;
+		default:
+			warn("Unsupported MAD type %d\n",
+			     iu->srp.generic.type);
+		}
+	} else {
+		switch (iu->srp.generic.type) {
+		case SRP_LOGIN_REQ_TYPE:
+			dbg("SRP LOGIN\n");
+			process_login(iue);
+			break;
+		case SRP_LOGIN_RSP_TYPE:
+			warn("Unsupported LOGIN_RSP SRP IU\n");
+			break;
+		case SRP_I_LOGOUT_TYPE:
+			warn("Unsupported I_LOGOUT SRP IU\n");
+			break;
+		case SRP_T_LOGOUT_TYPE:
+			warn("Unsupported T_LOGOUT SRP IU\n");
+			break;
+		case SRP_TSK_MGMT_TYPE:
+			warn("Unsupported TSK_MGMT SRP IU\n");
+			break;
+		case SRP_CMD_TYPE:
+			process_cmd(iue);
+			queued = 1;
+			break;
+		case SRP_RSP_TYPE:
+			warn("Unsupported RSP SRP IU\n");
+			break;
+		case SRP_CRED_REQ_TYPE:
+			warn("Unsupported CRED_REQ SRP IU\n");
+			break;
+		case SRP_CRED_RSP_TYPE:
+			warn("Unsupported CRED_RSP SRP IU\n");
+			break;
+		case SRP_AER_REQ_TYPE:
+			warn("Unsupported AER_REQ SRP IU\n");
+			break;
+		case SRP_AER_RSP_TYPE:
+			warn("Unsupported AER_RSP SRP IU\n");
+			break;
+		default:
+			warn("Unsupported SRP type %d\n",
+			     iu->srp.generic.type);
+		}
+	}
+
+	/* 
+	 * If no one has queued the IU for further work, free it 
+	 * Note that this is kind of an ugly design based on setting
+	 * this variable up above in cases where the routine we call
+	 * is responsible for freeing the IU
+	 */
+	if (!queued) 
+		free_iu(iue);
+}
+
+/* ==============================================================
+ * CRQ Processing Routines
+ * ==============================================================
+ */
+
+/*
+ * Handle a CRQ event
+ */
+static void handle_crq(struct VIOSRP_CRQ *crq, struct server_adapter *adapter)
+{
+	switch(crq->valid) {
+	case 0xC0: /* initialization */
+		switch(crq->format) {
+		case 0x01:
+			info("Partner just initialized\n");
+			plpar_hcall_norets(H_SEND_CRQ, adapter->dma_dev->unit_address, 
+					   0xC002000000000000, 0);
+			break;
+		case 0x02:
+			info("ibmvscsis: partner initialization complete\n");
+			break;
+		default:
+			err("Unknwn CRQ format %d\n",crq->format);
+		}
+		return;
+	case 0xFF: /* transport event */
+		info("ibmvscsis: partner closed\n");
+		return;
+	case 0x80:  /* real payload */
+	{
+		switch (crq->format) {
+		case  VIOSRP_SRP_FORMAT:
+		case VIOSRP_MAD_FORMAT:
+			process_iu(crq, adapter);
+			break;
+		case VIOSRP_OS400_FORMAT:
+			warn("Unsupported OS400 format CRQ\n");
+			break;
+				
+		case VIOSRP_AIX_FORMAT:
+			warn("Unsupported AIX format CRQ\n");
+			break;
+				
+		case VIOSRP_LINUX_FORMAT:
+			warn("Unsupported LINUX format CRQ\n");
+			break;
+				
+		case VIOSRP_INLINE_FORMAT:
+			warn("Unsupported _INLINE_ format CRQ\n");
+			break;
+				
+		default:
+			err("Unsupported CRQ format %d\n",crq->format);
+		}
+	}
+	break;
+	default:
+		err("ibmvscsis: got an invalid message type 0x%02x!?\n", crq->valid);
+		return;
+	}
+	
+}
+
+/*
+ * Task to handle CRQs and completions
+ */
+static void crq_task(void *data) {
+	struct server_adapter *adapter = (struct server_adapter *)data;
+	struct VIOSRP_CRQ *crq;
+	long rc;
+
+	if (atomic_inc_return(&adapter->crq_task_count) > 1) {
+		err("In crq_task twice!!!\n");
+	}
+
+	/* Loop through and process CRQs */
+	while((crq = crq_queue_next_crq(&adapter->queue)) != NULL) {
+		atomic_inc(&adapter->crq_processed);
+		handle_crq(crq, adapter);
+		crq->valid = 0x00;
+	}
+
+	/* If an interrupt comes in after this the task should be
+	 * rescheduled.
+	 */
+	rc = h_vio_signal(adapter->dma_dev->unit_address, 1);
+	if (rc != 0) {
+		err("Error %ld enabling interrupts!!!\n",rc);
+	}
+	atomic_dec(&adapter->crq_task_count);
+}
+
+/*
+ * Handle the interrupt that occurs when something is placed on our CRQ
+ */
+static irqreturn_t handle_interrupt(int irq, void *dev_instance, struct pt_regs *regs)
+{
+	struct server_adapter *adapter = (struct server_adapter *)dev_instance;
+	long rc;
+
+	rc = h_vio_signal(adapter->dma_dev->unit_address, 0);
+	if (rc != 0) {
+		err(" Error %ld disabling interrupts!!!\n",rc);
+	}
+
+	atomic_inc(&adapter->interrupts);
+	
+	schedule_work(&adapter->crq_work);
+
+	return IRQ_HANDLED;
+}
+
+/* 
+ * Initialize our CRQ
+ * return zero on success, non-zero on failure 
+ */
+static int initialize_crq_queue(struct crq_queue *queue, struct server_adapter *adapter)
+{
+	int rc;
+
+	queue->msgs = (struct VIOSRP_CRQ *)get_zeroed_page(GFP_KERNEL);
+	if(!queue->msgs)
+		goto malloc_failed;
+	queue->size = PAGE_SIZE / sizeof(*queue->msgs);
+
+	if((queue->msg_token = vio_map_single(adapter->dma_dev, 
+					      queue->msgs, 
+					      queue->size * sizeof(*queue->msgs), 
+					      PCI_DMA_BIDIRECTIONAL)) == NO_TCE)
+		goto map_failed;
+
+	rc = plpar_hcall_norets(H_REG_CRQ, adapter->dma_dev->unit_address, queue->msg_token, PAGE_SIZE);
+
+	/* If we opened successfully, send an init message */
+	if (rc == 0) {
+		plpar_hcall_norets(H_SEND_CRQ, adapter->dma_dev->unit_address, 
+				   0xC001000000000000, 0);
+	} else if (rc == 2) {
+		/* Other end is still closed.  This is normal */
+		info("connection registered, other end closed\n");
+	} else {
+		err("couldn't register crq--rc 0x%x\n", rc);
+		goto reg_crq_failed;
+	}
+
+	if(request_irq(adapter->dma_dev->irq, &handle_interrupt, SA_INTERRUPT, "ibmvscsis", adapter) != 0) 
+		goto req_irq_failed;
+
+	queue->cur = 0;
+	queue->lock = SPIN_LOCK_UNLOCKED;
+
+	return 0;
+
+ req_irq_failed:
+	plpar_hcall_norets(H_FREE_CRQ, adapter->dma_dev->unit_address);
+ reg_crq_failed:
+	vio_unmap_single(adapter->dma_dev, queue->msg_token, queue->size * sizeof(*queue->msgs), PCI_DMA_BIDIRECTIONAL);
+ map_failed:
+	free_page((unsigned long)queue->msgs);
+ malloc_failed:
+	return -1;
+}
+
+/*
+ * Release the CRQ
+ */
+static void release_crq_queue(struct crq_queue *queue, struct server_adapter *adapter)
+{
+	info("releasing crq\n");
+	free_irq(adapter->dma_dev->irq, adapter);
+	plpar_hcall_norets(H_FREE_CRQ, adapter->dma_dev->unit_address);
+	vio_unmap_single(adapter->dma_dev, queue->msg_token, queue->size * sizeof(*queue->msgs), PCI_DMA_BIDIRECTIONAL);
+	free_page((unsigned long)queue->msgs);
+}
+
+/* ==============================================================
+ * Module Management
+ * ==============================================================
+ */
+/*
+ * Add a block device as a SCSI LUN
+ */
+static void add_block_device(int majo, int mino, int bus, int target, struct server_adapter *adapter, int ro) {
+	struct vdev *vd;
+	struct vbus *newbus = NULL;
+	mode_t mode;
+	unsigned long flags;
+	dev_t kd = MKDEV(majo, mino);
+
+	if (bus >= BUS_PER_ADAPTER) {
+		err("Invalid bus %u specified\n",bus);
+		return;
+	}
+
+	if (target >= TARGETS_PER_BUS) {
+		err("Invalid target %u specified\n",bus);
+		return;
+	}
+
+	vd = (struct vdev *)kmalloc(sizeof(struct vdev), GFP_KERNEL);
+	if (vd == NULL) {
+		err("Unable to allocate memory for vdev structure");
+		return;
+	}
+	
+	memset(vd,0x00,sizeof(*vd));
+	vd->type = 'B';
+	vd->lun = make_lun(bus,target,0);
+	vd->b.dev = kd;
+	vd->b.bdev = bdget(kd);
+	vd->b.blksize = 512; 
+	vd->b.ro = ro;
+
+	if (ro) {
+		mode = FMODE_READ;
+	} else {
+		mode = FMODE_READ | FMODE_WRITE;
+	}
+
+	if (blkdev_get(vd->b.bdev, mode, 0) != 0) {
+		err("Error opening block device\n");
+		kfree(vd);
+		return;
+	} 
+
+	if (adapter->vbus[bus] == NULL) {
+		newbus = (struct vbus *)kmalloc(sizeof(struct vbus), GFP_KERNEL);
+		memset(newbus,0x00,sizeof(*newbus));
+	}
+	
+	spin_lock_irqsave(&adapter->lock,flags);
+	if ((newbus) && (adapter->vbus[bus] == NULL)) {
+		adapter->vbus[bus] = newbus;
+		newbus = NULL;
+	}
+	
+	if (adapter->vbus[bus]->vdev[target] != NULL) {
+		spin_unlock_irqrestore(&adapter->lock,flags);
+		err("Error: Duplicate vdev as lun 0x%lx\n",vd->lun);
+		kfree(vd);
+		return;
+	}
+	
+	adapter->vbus[bus]->vdev[target] = vd;
+	adapter->nvdevs++;
+	
+	spin_unlock_irqrestore(&adapter->lock,flags);
+	
+	if (newbus)
+		kfree(newbus);
+		
+	info("Adding block device %d:%d as %sLUN 0x%lx\n",
+	     majo,
+	     mino,
+	     ro ? "read only " : "",
+	     vd->lun);
+	return;
+}
+
+static void remove_block_device(int bus, int target, struct server_adapter *adapter) {
+	struct vdev *vd;
+	unsigned long flags;
+	
+	spin_lock_irqsave(&adapter->lock,flags);
+
+	if ((!adapter->vbus[bus]) || 
+	    (!adapter->vbus[bus]->vdev[target])) {
+		spin_unlock_irqrestore(&adapter->lock,flags);
+		err("Error removing non-existant device at bus %d, target %d\n",
+		    bus,target);
+		return;
+	}
+	
+	vd = adapter->vbus[bus]->vdev[target];
+	
+	if (vd->disabled) {
+		spin_unlock_irqrestore(&adapter->lock,flags);
+		err("Device at bus %d, target %d removed twice\n",
+		    bus,target);
+		return;
+	}
+
+	adapter->nvdevs--;
+
+	vd->disabled = 1;
+	
+	spin_unlock_irqrestore(&adapter->lock,flags);
+
+	/* Wait while any users of this device finish.  Note there should
+	 * be no new users, since we have marked this disabled
+	 *
+	 * We just poll here, since we are blocking a proc_write
+	 */
+	while (atomic_read(&vd->refcount)) {
+                schedule_timeout(HZ / 4); /* 1/4 second */
+	}
+	
+	spin_lock_irqsave(&adapter->lock,flags);
+	adapter->vbus[bus]->vdev[target] = NULL;
+	spin_unlock_irqrestore(&adapter->lock,flags);
+		
+	if (blkdev_put(vd->b.bdev)) {
+		err("Error closing block device!\n");
+	}
+	kfree(vd);
+
+	info("Removed block device at %d:%d\n",bus,target);
+}
+
+/*
+ * Handle read from our proc system file.  There is one of these
+ * files per adapter
+ */
+static int ibmvscsis_proc_read(char *page, char **start, off_t off,
+			       int count, int *eof, void *data)
+{
+	struct server_adapter *adapter = (struct server_adapter *)data;
+	int len = 0;
+	int bus;
+	int target;
+	struct vdev *vd;
+	unsigned long flags;
+
+	len += sprintf(page + len, "IBM VSCSI Server: %s\n",adapter->name);
+	len += sprintf(page + len, "interrupts: %10d\t\tread ops:   %10d\n",
+		       atomic_read(&adapter->interrupts),
+		       atomic_read(&adapter->read_processed));
+	len += sprintf(page + len, "crq msgs:   %10d\t\twrite ops:  %10d\n",
+		       atomic_read(&adapter->crq_processed),
+		       atomic_read(&adapter->write_processed));
+	len += sprintf(page + len, "iu alloc:   %10d\t\tbio alloc:  %10d\n",
+		       atomic_read(&adapter->iu_count),
+		       atomic_read(&adapter->bio_count));
+
+	len += sprintf(page + len, "buf alloc:  %10d\t\terrors:     %10d\n",
+		       atomic_read(&adapter->buffers_allocated),
+		       atomic_read(&adapter->errors));
+
+	spin_lock_irqsave(&adapter->lock,flags);
+
+	/* loop through the bus */
+	for (bus = 0; bus < BUS_PER_ADAPTER; bus++) {
+		/* If this bus exists */
+		if (adapter->vbus[bus]) {
+			/* loop through the targets */
+			for (target = 0; target < TARGETS_PER_BUS; target++)
+			{
+				/* If the target exists */
+				if (adapter->vbus[bus]->vdev[target]) {
+					vd = adapter->vbus[bus]->vdev[target];
+					if (vd->type == 'B') {
+						len += sprintf(page + len, "Block Device Major %d, Minor %d, Bus %d, Target %d LUN %d\n",
+							       MAJOR(vd->b.dev),
+							       MINOR(vd->b.dev),
+							       GETBUS(vd->lun),
+							       GETTARGET(vd->lun),
+							       GETLUN(vd->lun));
+					}
+				}
+			}
+		}
+	}
+
+	spin_unlock_irqrestore(&adapter->lock,flags);
+
+	*eof = 1;
+	return len;
+}
+
+/*
+ * Handle proc file system write.  One per adapter, currently used just
+ * to add virtual devices to our adapters.
+ */
+static int ibmvscsis_proc_write(struct file *file, const char *buffer,
+				unsigned long count, void *data){
+	int offset = 0;
+	int bytes;
+	char token[10];
+	char type[4];
+	int majo = -1;
+	int mino = -1;
+	unsigned int bus = -1;
+	unsigned int target = -1;
+	int ro = 0;
+
+	struct server_adapter *adapter = (struct server_adapter *)data;
+
+	if (sscanf(buffer+offset,"%9s%n",token,&bytes) != 1) {
+		err("Error on read of proc file\n");
+		return count;
+	}
+	offset += bytes;
+
+	if (strcmp(token,"add") == 0) {
+		
+		if (sscanf(buffer+offset,"%3s%n",type,&bytes) != 1) {
+			err("Error on read of proc file\n");
+			return count;
+		}
+		offset += bytes;
+		
+		sscanf(buffer+offset,"%i%n",&majo,&bytes);
+		offset += bytes;
+		
+		sscanf(buffer+offset,"%i%n",&mino,&bytes);
+		offset += bytes;
+		
+		sscanf(buffer+offset,"%i%n",&bus,&bytes);
+		offset += bytes;
+		
+		sscanf(buffer+offset,"%i%n",&target,&bytes);
+		offset += bytes;
+		
+		if (strcmp(type,"b") == 0) {
+			ro = 0;
+		} else if (strcmp(type,"br") == 0) {
+			ro = 1;
+		} else {
+			err("Invalid type %s on add request\n",type);
+			return count;
+		}
+		
+		if ((majo == -1) || (mino == -1)) {
+			err("Ignoring command %s device %d::%ds type %s\n",
+			    token,majo, mino,type);
+			return count;
+		}
+		add_block_device(majo,mino,bus,target, adapter, ro);
+	} else if (strcmp(token,"remove") == 0) {
+		if (sscanf(buffer+offset,"%3s%n",type,&bytes) != 1) {
+			err("Error on read of proc file\n");
+			return count;
+		}
+		offset += bytes;
+		
+		sscanf(buffer+offset,"%i%n",&bus,&bytes);
+		offset += bytes;
+		
+		sscanf(buffer+offset,"%i%n",&target,&bytes);
+		offset += bytes;
+		
+		if ((strcmp(type,"b") != 0) &&
+		    (strcmp(type,"br") != 0)) {
+			err("Ignoring remove command for invalid type %s\n",type);
+			return count;
+		}
+		
+		if ((bus == -1) || (target == -1)) {
+			err("Ignoring remove command bus %d target%ds\n",
+			    bus,target);
+			return count;
+		}
+		
+		remove_block_device(bus, target, adapter);
+	} else if (strcmp(token,"debug") == 0) {
+		ibmvscsis_debug = 1;
+		dbg("debugging on\n");
+	} else {
+		err("Ignoring command %s\n",token);
+	}
+	
+	return count;
+}
+ 
+static void ibmvscsis_proc_register_driver(void)
+{
+#ifdef CONFIG_PROC_FS
+	ibmvscsis_proc_dir = create_proc_entry(IBMVSCSIS_PROC_DIR, S_IFDIR, proc_root_driver);
+#endif
+}
+
+static void ibmvscsis_proc_unregister_driver(void)
+{
+#ifdef CONFIG_PROC_FS
+	remove_proc_entry(IBMVSCSIS_PROC_DIR, proc_root_driver);
+#endif
+}
+
+static void ibmvscsis_proc_register_adapter(struct server_adapter *adapter)
+{
+#ifdef CONFIG_PROC_FS
+	struct proc_dir_entry *entry;
+	entry = create_proc_entry(adapter->name, S_IFREG, ibmvscsis_proc_dir);
+	entry->data = (void *) adapter;
+	entry->read_proc = ibmvscsis_proc_read;
+	entry->write_proc = ibmvscsis_proc_write;
+#endif
+}
+
+static void ibmvscsis_proc_unregister_adapter(struct server_adapter *adapter)
+{
+#ifdef CONFIG_PROC_FS
+	remove_proc_entry(adapter->name, ibmvscsis_proc_dir);
+#endif
+}
+
+static int ibmvscsis_probe(struct vio_dev *dev, const struct vio_device_id *id)
+{
+	struct server_adapter *adapter;
+	int rc;
+	unsigned int *dma_window;
+	unsigned int dma_window_property_size;
+	
+	info("entering probe for UA 0x%x\n", dev->unit_address);
+
+	adapter = kmalloc(sizeof(*adapter), GFP_KERNEL);
+	if(!adapter) {
+		err("couldn't kmalloc adapter structure\n");
+		return -1;
+	}
+	memset(adapter, 0x00, sizeof(*adapter));
+	adapter->dma_dev = dev;
+	dev->driver_data = adapter;
+	sprintf(adapter->name,"%x",dev->unit_address);
+	adapter->lock = SPIN_LOCK_UNLOCKED;
+
+	dma_window = (unsigned int *)vio_get_attribute(dev, "ibm,my-dma-window", &dma_window_property_size);
+	if(!dma_window) {
+		warn("Couldn't find ibm,my-dma-window property\n");
+	}
+	
+	adapter->liobn = dma_window[0];
+	/* RPA docs say that #address-cells is always 1 for virtual
+	   devices, but some older boxes' OF returns 2.  This should
+	   be removed by GA, unless there is legacy OFs that still
+	   have 2 or 3 for #address-cells */
+	/*adapter->riobn = dma_window[2+vio_num_address_cells];*/
+
+	/* This is just an ugly kludge. Remove as soon as the OF for all
+	   machines actually follow the spec and encodes the offset field
+	   as phys-encode (that is, #address-cells wide)*/
+	if (dma_window_property_size == 24) {
+		adapter->riobn = dma_window[3];
+	} else if (dma_window_property_size == 40) {
+		adapter->riobn = dma_window[5];
+	} else {
+		warn("ibmvscsis: Invalid size of ibm,my-dma-window=%i\n", dma_window_property_size);
+	}
+
+	INIT_WORK(&adapter->crq_work, crq_task, adapter);
+	INIT_WORK(&adapter->endio_work, endio_task, adapter);
+
+	/* Initialize the buffer cache */
+	init_data_buffer(adapter);
+
+	/* Arbitrarily support 16 IUs right now */
+	rc = initialize_iu_pool(adapter, 16);
+	if (rc) {
+		kfree(adapter);
+		return rc;
+	}
+
+	rc = initialize_crq_queue(&adapter->queue, adapter);
+	if (rc != 0) {
+		kfree(adapter);
+		return rc;
+	}
+
+	rc = h_vio_signal(adapter->dma_dev->unit_address, 1);
+	if (rc != 0) {
+		err("Error %d enabling interrupts!!!\n",rc);
+	}
+
+	ibmvscsis_proc_register_adapter(adapter);
+
+	return 0;
+}
+
+
+static int ibmvscsis_remove(struct vio_dev *dev)
+{
+	int bus;
+	int target;
+	
+	struct server_adapter *adapter = (struct server_adapter *)dev->driver_data;
+
+	info("entering remove for UA 0x%x\n", dev->unit_address);
+
+	ibmvscsis_proc_unregister_adapter(adapter);
+
+	/* 
+	 * No one should be adding any devices at this point because we blew 
+	 * away the proc file system entry 
+	 *
+	 * Loop through the bus
+	 */
+	for (bus = 0; bus < BUS_PER_ADAPTER; bus++) {
+		/* If this bus exists */
+		if (adapter->vbus[bus]) {
+			/* loop through the targets */
+			for (target = 0; target < TARGETS_PER_BUS; target++)
+			{
+				/* If the target exists */
+				if (adapter->vbus[bus]->vdev[target]) {
+					remove_block_device(bus,target,adapter);
+				}
+			}
+		}
+	}
+
+	release_crq_queue(&adapter->queue, adapter);
+
+	release_iu_pool(adapter);
+
+	release_data_buffer(adapter);
+
+	kfree(adapter);
+
+	return 0;
+}
+
+static struct vio_device_id ibmvscsis_device_table[] __devinitdata= {
+	{ "v-scsi-host", "IBM,v-scsi-host" },
+	{ 0,}
+};
+
+MODULE_DEVICE_TABLE(vio, ibmvscsis_device_table);
+
+static struct vio_driver ibmvscsis_driver = {
+	.name        = ibmvscsis_driver_name,
+	.id_table    = ibmvscsis_device_table,
+	.probe       = ibmvscsis_probe,
+	.remove      = ibmvscsis_remove,
+};
+
+static int mod_init(void) {
+	int rc;
+
+	info("ibmvscsis initialized\n");
+
+	ibmvscsis_proc_register_driver();
+
+	rc = vio_register_driver(&ibmvscsis_driver);
+	
+	if (rc) {
+		warn("rc %d from vio_register_driver\n",rc);
+	}
+
+	return rc;
+}
+
+static void mod_exit(void) {
+	info("terminated\n");
+
+	vio_unregister_driver(&ibmvscsis_driver); 
+
+	ibmvscsis_proc_unregister_driver(); 
+}
+
+module_init(mod_init);
+module_exit(mod_exit);
diff -purN linux-2.5/drivers/scsi/ibmvscsi/iseries_vscsi.c linuxppc64-2.5/drivers/scsi/ibmvscsi/iseries_vscsi.c
--- linux-2.5/drivers/scsi/ibmvscsi/iseries_vscsi.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/ibmvscsi/iseries_vscsi.c	2004-02-25 16:57:38.000000000 +0000
@@ -0,0 +1,144 @@
+/* ------------------------------------------------------------
+ * iSeries_vscsi.c
+ * (C) Copyright IBM Corporation 1994, 2003
+ * Authors: Colin DeVilbiss (devilbis@us.ibm.com)
+ *          Santiago Leon (santil@us.ibm.com)
+ *          Dave Boutcher (sleddog@us.ibm.com)
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307
+ * USA
+ *
+ * ------------------------------------------------------------
+ * iSeries-specific functions of the SCSI host adapter for Virtual I/O devices
+ *
+ * This driver allows the Linux SCSI peripheral drivers to directly
+ * access devices in the hosting partition, either on an iSeries
+ * hypervisor system or a converged hypervisor system.
+ */
+
+#include <asm/iSeries/vio.h>
+#include <asm/iSeries/HvLpEvent.h>
+#include <asm/iSeries/HvTypes.h>
+#include <asm/iSeries/HvLpConfig.h>
+#include "ibmvscsi.h"
+
+/* global variables */
+extern struct device *iSeries_vio_dev;
+static struct ibmvscsi_host_data *single_host_data;
+
+/* ------------------------------------------------------------
+ * Routines for direct interpartition interaction
+ */
+struct VIOSRPLpEvent {
+	struct HvLpEvent lpevt;	/* 0x00-0x17          */
+	u32 reserved1;		/* 0x18-0x1B; unused  */
+	u16 version;		/* 0x1C-0x1D; unused  */
+	u16 subtype_rc;		/* 0x1E-0x1F; unused  */
+	struct VIOSRP_CRQ crq;	/* 0x20-0x3F          */
+};
+
+/** 
+ * standard interface for handling logical partition events.
+ */
+static void ibmvscsi_handle_event(struct HvLpEvent *lpevt)
+{
+	struct VIOSRPLpEvent *evt = (struct VIOSRPLpEvent *)lpevt;
+
+	if (!evt) {
+		printk(KERN_ERR "ibmvscsi: received null event\n");
+		return;
+	}
+
+	if (single_host_data == NULL) {
+		printk(KERN_ERR
+		       "ibmvscsi: received event, no adapter present\n");
+		return;
+	}
+
+	ibmvscsi_handle_crq(&evt->crq, single_host_data);
+}
+
+/* ------------------------------------------------------------
+ * Routines for driver initialization
+ */
+int ibmvscsi_init_crq_queue(struct crq_queue *queue,
+			    struct ibmvscsi_host_data *hostdata)
+{
+	int rc;
+
+	rc = viopath_open(viopath_hostLp, viomajorsubtype_scsi, 0);
+	if (rc < 0) {
+		printk("viopath_open failed with rc %d in open_event_path\n",
+		       rc);
+		goto viopath_open_failed;
+	}
+
+	rc = vio_setHandler(viomajorsubtype_scsi, ibmvscsi_handle_event);
+	if (rc < 0) {
+		printk("vio_setHandler failed with rc %d in open_event_path\n",
+		       rc);
+		goto vio_setHandler_failed;
+	}
+	return 0;
+
+      vio_setHandler_failed:
+	viopath_close(viopath_hostLp, viomajorsubtype_scsi,
+		      IBMVSCSI_MAX_REQUESTS);
+      viopath_open_failed:
+	return -1;
+}
+
+void ibmvscsi_release_crq_queue(struct crq_queue *queue,
+				struct ibmvscsi_host_data *hostdata)
+{
+	vio_clearHandler(viomajorsubtype_scsi);
+	viopath_close(viopath_hostLp, viomajorsubtype_scsi,
+		      IBMVSCSI_MAX_REQUESTS);
+}
+
+/**
+ * ibmvscsi_send_crq: - Send a CRQ
+ * @hostdata:	the adapter
+ * @word1:	the first 64 bits of the data
+ * @word2:	the second 64 bits of the data
+ */
+int ibmvscsi_send_crq(struct ibmvscsi_host_data *hostdata, u64 word1, u64 word2)
+{
+	single_host_data = hostdata;
+	return HvCallEvent_signalLpEventFast(viopath_hostLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_scsi,
+					     HvLpEvent_AckInd_NoAck,
+					     HvLpEvent_AckType_ImmediateAck,
+					     viopath_sourceinst(viopath_hostLp),
+					     viopath_targetinst(viopath_hostLp),
+					     0,
+					     VIOVERSION << 16, word1, word2, 0,
+					     0);
+}
+
+int __init ibmvscsi_module_init(void)
+{
+	single_host_data = ibmvscsi_probe(iSeries_vio_dev);
+	return (single_host_data == NULL);
+}
+
+void __exit ibmvscsi_module_exit(void)
+{
+	ibmvscsi_remove(single_host_data);
+}
+
+module_init(ibmvscsi_module_init);
+module_exit(ibmvscsi_module_exit);
diff -purN linux-2.5/drivers/scsi/ibmvscsi/rpa_vscsi.c linuxppc64-2.5/drivers/scsi/ibmvscsi/rpa_vscsi.c
--- linux-2.5/drivers/scsi/ibmvscsi/rpa_vscsi.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/ibmvscsi/rpa_vscsi.c	2004-02-25 16:57:38.000000000 +0000
@@ -0,0 +1,281 @@
+/* ------------------------------------------------------------
+ * rpa_vscsi.c
+ * (C) Copyright IBM Corporation 1994, 2003
+ * Authors: Colin DeVilbiss (devilbis@us.ibm.com)
+ *          Santiago Leon (santil@us.ibm.com)
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307
+ * USA
+ *
+ * ------------------------------------------------------------
+ * RPA-specific functions of the SCSI host adapter for Virtual I/O devices
+ *
+ * This driver allows the Linux SCSI peripheral drivers to directly
+ * access devices in the hosting partition, either on an iSeries
+ * hypervisor system or a converged hypervisor system.
+ */
+
+#include <asm/vio.h>
+#include <asm/iommu.h>
+#include <asm/hvcall.h>
+#include <linux/dma-mapping.h>
+#include <linux/interrupt.h>
+#include "ibmvscsi.h"
+
+/* ------------------------------------------------------------
+ * Routines for managing the command/response queue
+ */
+/**
+ * ibmvscsi_handle_event: - Interrupt handler for crq events
+ * @irq:	number of irq to handle, not used
+ * @dev_instance: ibmvscsi_host_data of host that received interrupt
+ * @regs:	pt_regs with registers
+ *
+ * Disables interrupts and schedules srp_task
+ * Always returns IRQ_HANDLED
+ */
+static irqreturn_t ibmvscsi_handle_event(int irq,
+					 void *dev_instance,
+					 struct pt_regs *regs)
+{
+	struct ibmvscsi_host_data *hostdata =
+	    (struct ibmvscsi_host_data *)dev_instance;
+	vio_disable_interrupts(to_vio_dev(hostdata->dev));
+	schedule_work(&hostdata->srp_task);
+	return IRQ_HANDLED;
+}
+
+/**
+ * release_crq_queue: - Deallocates data and unregisters CRQ
+ * @queue:	crq_queue to initialize and register
+ * @host_data:	ibmvscsi_host_data of host
+ *
+ * Frees irq, deallocates a page for messages, unmaps dma, and unregisters
+ * the crq with the hypervisor.
+ */
+void ibmvscsi_release_crq_queue(struct crq_queue *queue,
+				struct ibmvscsi_host_data *hostdata)
+{
+	struct vio_dev *vdev = to_vio_dev(hostdata->dev);
+	free_irq(vdev->irq, (void *)hostdata);
+	plpar_hcall_norets(H_FREE_CRQ, vdev->unit_address);
+	dma_unmap_single(hostdata->dev,
+			 queue->msg_token,
+			 queue->size * sizeof(*queue->msgs),
+			 PCI_DMA_BIDIRECTIONAL);
+	free_page((unsigned long)queue->msgs);
+}
+
+/**
+ * crq_queue_next_crq: - Returns the next entry in message queue
+ * @queue:	crq_queue to use
+ *
+ * Returns pointer to next entry in queue, or NULL if there are no new 
+ * entried in the CRQ.
+ */
+static struct VIOSRP_CRQ *crq_queue_next_crq(struct crq_queue *queue)
+{
+	struct VIOSRP_CRQ *crq;
+	unsigned long flags;
+
+	spin_lock_irqsave(&queue->lock, flags);
+	crq = &queue->msgs[queue->cur];
+	if (crq->valid & 0x80) {
+		if (++queue->cur == queue->size)
+			queue->cur = 0;
+	} else
+		crq = NULL;
+	spin_unlock_irqrestore(&queue->lock, flags);
+
+	return crq;
+}
+
+/**
+ * ibmvscsi_send_crq: - Send a CRQ
+ * @hostdata:	the adapter
+ * @word1:	the first 64 bits of the data
+ * @word2:	the second 64 bits of the data
+ */
+int ibmvscsi_send_crq(struct ibmvscsi_host_data *hostdata, u64 word1, u64 word2)
+{
+	struct vio_dev *vdev = to_vio_dev(hostdata->dev);
+
+	return plpar_hcall_norets(H_SEND_CRQ, vdev->unit_address, word1, word2);
+}
+
+/**
+ * ibmvscsi_task: - Process srps asynchronously
+ * @data:	ibmvscsi_host_data of host
+ */
+static void ibmvscsi_task(void *data)
+{
+	struct ibmvscsi_host_data *hostdata = (struct ibmvscsi_host_data *)data;
+	struct vio_dev *vdev = to_vio_dev(hostdata->dev);
+	struct VIOSRP_CRQ *crq;
+	int done = 0;
+
+	while (!done) {
+		/* Pull all the valid messages off the CRQ */
+		while ((crq = crq_queue_next_crq(&hostdata->queue)) != NULL) {
+			ibmvscsi_handle_crq(crq, hostdata);
+			crq->valid = 0x00;
+		}
+
+		vio_enable_interrupts(vdev);
+		if ((crq = crq_queue_next_crq(&hostdata->queue)) != NULL) {
+			vio_disable_interrupts(vdev);
+			ibmvscsi_handle_crq(crq, hostdata);
+			crq->valid = 0x00;
+		} else {
+			done = 1;
+		}
+	}
+}
+
+/**
+ * initialize_crq_queue: - Initializes and registers CRQ with hypervisor
+ * @queue:	crq_queue to initialize and register
+ * @hostdata:	ibmvscsi_host_data of host
+ *
+ * Allocates a page for messages, maps it for dma, and registers
+ * the crq with the hypervisor.
+ * Returns zero on success.
+ */
+int ibmvscsi_init_crq_queue(struct crq_queue *queue,
+			    struct ibmvscsi_host_data *hostdata)
+{
+	int rc;
+	struct vio_dev *vdev = to_vio_dev(hostdata->dev);
+
+	queue->msgs = (struct VIOSRP_CRQ *)get_zeroed_page(GFP_KERNEL);
+
+	if (!queue->msgs)
+		goto malloc_failed;
+	queue->size = PAGE_SIZE / sizeof(*queue->msgs);
+
+	if ((queue->msg_token = dma_map_single(hostdata->dev,
+					       queue->msgs,
+					       queue->size *
+					       sizeof(*queue->msgs),
+					       PCI_DMA_BIDIRECTIONAL)) ==
+	    NO_TCE)
+		goto map_failed;
+
+	rc = plpar_hcall_norets(H_REG_CRQ,
+				vdev->unit_address,
+				queue->msg_token, PAGE_SIZE);
+	if (rc == 2) {
+		/* Adapter is good, but other end is not ready */
+		printk(KERN_WARNING "ibmvscsi: Partner adapter not ready\n");
+	} else if (rc != 0) {
+		printk(KERN_WARNING
+		       "ibmvscsi: couldn't register crq--rc 0x%x\n", rc);
+		goto reg_crq_failed;
+	}
+
+	if (request_irq(vdev->irq,
+			ibmvscsi_handle_event,
+			0, "ibmvscsi", (void *)hostdata) != 0) {
+		printk(KERN_ERR "ibmvscsi: couldn't register irq 0x%x\n",
+		       vdev->irq);
+		goto req_irq_failed;
+	}
+
+	rc = vio_enable_interrupts(vdev);
+	if (rc != 0) {
+		printk(KERN_ERR "ibmvscsi:  Error %d enabling interrupts!!!\n",
+		       rc);
+		goto req_irq_failed;
+	}
+
+	queue->cur = 0;
+	queue->lock = SPIN_LOCK_UNLOCKED;
+
+	INIT_WORK(&hostdata->srp_task, (void *)ibmvscsi_task, hostdata);
+
+	return 0;
+
+      req_irq_failed:
+	plpar_hcall_norets(H_FREE_CRQ, vdev->unit_address);
+      reg_crq_failed:
+	dma_unmap_single(hostdata->dev,
+			 queue->msg_token,
+			 queue->size * sizeof(*queue->msgs),
+			 PCI_DMA_BIDIRECTIONAL);
+      map_failed:
+	free_page((unsigned long)queue->msgs);
+      malloc_failed:
+	return -1;
+}
+
+/**
+ * rpa_device_table: Used by vio.c to match devices in the device tree we 
+ * support.
+ */
+static struct vio_device_id rpa_device_table[] __devinitdata = {
+	{"scsi-3", "IBM,v-scsi"},	/* Note: This entry can go away when 
+					   all the firmware is up to date */
+	{"vscsi", "IBM,v-scsi"},
+	{0,}
+};
+
+/**
+ * rpa_probe: The callback from the virtual I/O bus code.
+ * @vdev     : The vio specific device structure
+ * @id       : the device id..we don't currently use it
+ */
+static int rpa_probe(struct vio_dev *vdev, const struct vio_device_id *id)
+{
+	struct ibmvscsi_host_data *hostdata = ibmvscsi_probe(&vdev->dev);
+	if (hostdata) {
+		vdev->driver_data = hostdata;
+		return 0;
+	} else {
+		return -1;
+	}
+}
+
+/**
+ * rpa_remove: The callback from the virtual I/O bus code to remove a device
+ * @vdev     : The vio specific device structure
+ */
+static int rpa_remove(struct vio_dev *vdev)
+{
+	struct ibmvscsi_host_data *hostdata =
+	    (struct ibmvscsi_host_data *)vdev->driver_data;
+	ibmvscsi_remove(hostdata);
+	return 0;
+}
+
+MODULE_DEVICE_TABLE(vio, rpa_device_table);
+static struct vio_driver ibmvscsi_driver = {
+	.name = "ibmvscsi",
+	.id_table = rpa_device_table,
+	.probe = rpa_probe,
+	.remove = rpa_remove
+};
+
+int __init ibmvscsi_module_init(void)
+{
+	return vio_register_driver(&ibmvscsi_driver);
+}
+
+void __exit ibmvscsi_module_exit(void)
+{
+	vio_unregister_driver(&ibmvscsi_driver);
+}
+
+module_init(ibmvscsi_module_init);
+module_exit(ibmvscsi_module_exit);
diff -purN linux-2.5/drivers/scsi/ibmvscsi/srp.h linuxppc64-2.5/drivers/scsi/ibmvscsi/srp.h
--- linux-2.5/drivers/scsi/ibmvscsi/srp.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/ibmvscsi/srp.h	2004-02-25 16:57:38.000000000 +0000
@@ -0,0 +1,225 @@
+/*****************************************************************************/
+/* srp.h -- SCSI RDMA Protocol definitions                                   */
+/*                                                                           */
+/* Written By: Colin Devilbis, IBM Corporation                               */
+/*                                                                           */
+/* Copyright (C) 2003 IBM Corporation                                        */
+/*                                                                           */
+/* This program is free software; you can redistribute it and/or modify      */
+/* it under the terms of the GNU General Public License as published by      */
+/* the Free Software Foundation; either version 2 of the License, or         */
+/* (at your option) any later version.                                       */
+/*                                                                           */
+/* This program is distributed in the hope that it will be useful,           */
+/* but WITHOUT ANY WARRANTY; without even the implied warranty of            */
+/* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the             */
+/* GNU General Public License for more details.                              */
+/*                                                                           */
+/* You should have received a copy of the GNU General Public License         */
+/* along with this program; if not, write to the Free Software               */
+/* Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA */
+/*                                                                           */
+/*                                                                           */
+/* This file contains structures and definitions for the SCSI RDMA Protocol  */
+/* (SRP) as defined in the T10 standard available at www.t10.org.  This      */
+/* file was based on the 16a version of the standard                         */
+/*                                                                           */
+/*****************************************************************************/
+#ifndef SRP_H
+#define SRP_H
+
+#define PACKED __attribute__((packed))
+
+enum SRP_TYPES {
+	SRP_LOGIN_REQ_TYPE = 0x00,
+	SRP_LOGIN_RSP_TYPE = 0xC0,
+	SRP_LOGIN_REJ_TYPE = 0x80,
+	SRP_I_LOGOUT_TYPE = 0x03,
+	SRP_T_LOGOUT_TYPE = 0x80,
+	SRP_TSK_MGMT_TYPE = 0x01,
+	SRP_CMD_TYPE = 0x02,
+	SRP_RSP_TYPE = 0xC1,
+	SRP_CRED_REQ_TYPE = 0x81,
+	SRP_CRED_RSP_TYPE = 0x41,
+	SRP_AER_REQ_TYPE = 0x82,
+	SRP_AER_RSP_TYPE = 0x42
+};
+
+enum SRP_DESCRIPTOR_FORMATS {
+	SRP_NO_BUFFER = 0x00,
+	SRP_DIRECT_BUFFER = 0x01,
+	SRP_INDIRECT_BUFFER = 0x02
+};
+
+struct memory_descriptor {
+	u64 virtual_address;
+	u32 memory_handle;
+	u32 length;
+};
+
+struct indirect_descriptor {
+	struct memory_descriptor head;
+	u64 total_length;
+	struct memory_descriptor list[1];
+};
+
+struct SRP_GENERIC {
+	u8 type;
+	u8 reserved1[7];
+	u64 tag;
+};
+
+struct SRP_LOGIN_REQ {
+	u8 type;
+	u8 reserved1[7];
+	u64 tag;
+	u32 max_requested_initiator_to_target_iulen;
+	u32 reserved2;
+	u16 required_buffer_formats;
+	u8 reserved3:6;
+	u8 multi_channel_action:2;
+	u8 reserved4;
+	u32 reserved5;
+	u8 initiator_port_identifier[16];
+	u8 target_port_identifier[16];
+};
+
+struct SRP_LOGIN_RSP {
+	u8 type;
+	u8 reserved1[3];
+	u32 request_limit_delta;
+	u64 tag;
+	u32 max_initiator_to_target_iulen;
+	u32 max_target_to_initiator_iulen;
+	u16 supported_buffer_formats;
+	u8 reserved2:6;
+	u8 multi_channel_result:2;
+	u8 reserved3;
+	u8 reserved4[24];
+};
+
+struct SRP_LOGIN_REJ {
+	u8 type;
+	u8 reserved1[3];
+	u32 reason;
+	u64 tag;
+	u64 reserved2;
+	u16 supported_buffer_formats;
+	u8 reserved3[6];
+};
+
+struct SRP_I_LOGOUT {
+	u8 type;
+	u8 reserved1[7];
+	u64 tag;
+};
+
+struct SRP_T_LOGOUT {
+	u8 type;
+	u8 reserved1[3];
+	u32 reason;
+	u64 tag;
+};
+
+struct SRP_TSK_MGMT {
+	u8 type;
+	u8 reserved1[7];
+	u64 tag;
+	u32 reserved2;
+	u64 lun PACKED;
+	u8 reserved3;
+	u8 reserved4;
+	u8 task_mgmt_flags;
+	u8 reserved5;
+	u64 managed_task_tag;
+	u64 reserved6;
+};
+
+struct SRP_CMD {
+	u8 type;
+	u32 reserved1 PACKED;
+	u8 data_out_format:4;
+	u8 data_in_format:4;
+	u8 data_out_count;
+	u8 data_in_count;
+	u64 tag;
+	u32 reserved2;
+	u64 lun PACKED;
+	u8 reserved3;
+	u8 reserved4:5;
+	u8 task_attribute:3;
+	u8 reserved5;
+	u8 additional_cdb_len;
+	u8 cdb[16];
+	u8 additional_data[0x100 - 0x30];
+};
+
+struct SRP_RSP {
+	u8 type;
+	u8 reserved1[3];
+	u32 request_limit_delta;
+	u64 tag;
+	u16 reserved2;
+	u8 reserved3:2;
+	u8 diunder:1;
+	u8 diover:1;
+	u8 dounder:1;
+	u8 doover:1;
+	u8 snsvalid:1;
+	u8 rspvalid:1;
+	u8 status;
+	u32 data_in_residual_count;
+	u32 data_out_residual_count;
+	u32 sense_data_list_length;
+	u32 response_data_list_length;
+	u8 sense_and_response_data[18];
+};
+
+struct SRP_CRED_REQ {
+	u8 type;
+	u8 reserved1[3];
+	u32 request_limit_delta;
+	u64 tag;
+};
+
+struct SRP_CRED_RSP {
+	u8 type;
+	u8 reserved1[7];
+	u64 tag;
+};
+
+struct SRP_AER_REQ {
+	u8 type;
+	u8 reserved1[3];
+	u32 request_limit_delta;
+	u64 tag;
+	u32 reserved2;
+	u64 lun;
+	u32 sense_data_list_length;
+	u32 reserved3;
+	u8 sense_data[20];
+};
+
+struct SRP_AER_RSP {
+	u8 type;
+	u8 reserved1[7];
+	u64 tag;
+};
+
+union SRP_IU {
+	struct SRP_GENERIC generic;
+	struct SRP_LOGIN_REQ login_req;
+	struct SRP_LOGIN_RSP login_rsp;
+	struct SRP_LOGIN_REJ login_rej;
+	struct SRP_I_LOGOUT i_logout;
+	struct SRP_T_LOGOUT t_logout;
+	struct SRP_TSK_MGMT tsk_mgmt;
+	struct SRP_CMD cmd;
+	struct SRP_RSP rsp;
+	struct SRP_CRED_REQ cred_req;
+	struct SRP_CRED_RSP cred_rsp;
+	struct SRP_AER_REQ aer_req;
+	struct SRP_AER_RSP aer_rsp;
+};
+
+#endif
diff -purN linux-2.5/drivers/scsi/ibmvscsi/viosrp.h linuxppc64-2.5/drivers/scsi/ibmvscsi/viosrp.h
--- linux-2.5/drivers/scsi/ibmvscsi/viosrp.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/ibmvscsi/viosrp.h	2004-02-25 16:57:38.000000000 +0000
@@ -0,0 +1,126 @@
+/*****************************************************************************/
+/* srp.h -- SCSI RDMA Protocol definitions                                   */
+/*                                                                           */
+/* Written By: Colin Devilbis, IBM Corporation                               */
+/*                                                                           */
+/* Copyright (C) 2003 IBM Corporation                                        */
+/*                                                                           */
+/* This program is free software; you can redistribute it and/or modify      */
+/* it under the terms of the GNU General Public License as published by      */
+/* the Free Software Foundation; either version 2 of the License, or         */
+/* (at your option) any later version.                                       */
+/*                                                                           */
+/* This program is distributed in the hope that it will be useful,           */
+/* but WITHOUT ANY WARRANTY; without even the implied warranty of            */
+/* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the             */
+/* GNU General Public License for more details.                              */
+/*                                                                           */
+/* You should have received a copy of the GNU General Public License         */
+/* along with this program; if not, write to the Free Software               */
+/* Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA */
+/*                                                                           */
+/*                                                                           */
+/* This file contains structures and definitions for IBM RPA (RS/6000        */
+/* platform architecture) implementation of the SRP (SCSI RDMA Protocol)     */
+/* standard.  SRP is used on IBM iSeries and pSeries platforms to send SCSI  */
+/* commands between logical partitions.                                      */
+/*                                                                           */
+/* SRP Information Units (IUs) are sent on a "Command/Response Queue" (CRQ)  */
+/* between partitions.  The definitions in this file are architected,        */
+/* and cannot be changed without breaking compatibility with other versions  */
+/* of Linux and other operating systems (AIX, OS/400) that talk this protocol*/
+/* between logical partitions                                                */
+/*****************************************************************************/
+#ifndef VIOSRP_H
+#define VIOSRP_H
+#include "srp.h"
+
+enum VIOSRP_CRQ_FORMATS {
+	VIOSRP_SRP_FORMAT = 0x01,
+	VIOSRP_MAD_FORMAT = 0x02,
+	VIOSRP_OS400_FORMAT = 0x03,
+	VIOSRP_AIX_FORMAT = 0x04,
+	VIOSRP_LINUX_FORMAT = 0x06,
+	VIOSRP_INLINE_FORMAT = 0x07
+};
+
+struct VIOSRP_CRQ {
+	u8 valid;		/* used by RPA */
+	u8 format;		/* SCSI vs out-of-band */
+	u8 reserved;
+	u8 status;		/* non-scsi failure? (e.g. DMA failure) */
+	u16 timeout;		/* in seconds */
+	u16 IU_length;		/* in bytes */
+	u64 IU_data_ptr;	/* the TCE for transferring data */
+};
+
+/* MADs are Management requests above and beyond the IUs defined in the SRP
+ * standard.  
+ */
+enum VIOSRP_MAD_TYPES {
+	VIOSRP_EMPTY_IU_TYPE = 0x01,
+	VIOSRP_ERROR_LOG_TYPE = 0x02,
+	VIOSRP_ADAPTER_INFO_TYPE = 0x03,
+	VIOSRP_HOST_CONFIG_TYPE = 0x04
+};
+
+/* 
+ * Common MAD header
+ */
+struct MAD_COMMON {
+	u32 type;
+	u16 status;
+	u16 length;
+	u64 tag;
+};
+
+/*
+ * All SRP (and MAD) requests normally flow from the
+ * client to the server.  There is no way for the server to send
+ * an asynchronous message back to the client.  The Empty IU is used
+ * to hang out a meaningless request to the server so that it can respond
+ * asynchrouously with something like a SCSI AER 
+ */
+struct VIOSRP_EMPTY_IU {
+	struct MAD_COMMON common;
+	u64 buffer;
+	u32 port;
+};
+
+struct VIOSRP_ERROR_LOG {
+	struct MAD_COMMON common;
+	u64 buffer;
+};
+
+struct VIOSRP_ADAPTER_INFO {
+	struct MAD_COMMON common;
+	u64 buffer;
+};
+
+struct VIOSRP_HOST_CONFIG {
+	struct MAD_COMMON common;
+	u64 buffer;
+};
+
+union MAD_IU {
+	struct VIOSRP_EMPTY_IU empty_iu;
+	struct VIOSRP_ERROR_LOG error_log;
+	struct VIOSRP_ADAPTER_INFO adapter_info;
+	struct VIOSRP_HOST_CONFIG host_config;
+};
+
+union VIOSRP_IU {
+	union SRP_IU srp;
+	union MAD_IU mad;
+};
+
+struct MAD_ADAPTER_INFO_DATA {
+	char srp_version[8];
+	char partition_name[96];
+	u32 partition_number;
+	u32 mad_version;
+	u32 os_type;
+	u32 port_max_txu[8];	/* per-port maximum transfer */
+};
+
+#endif
diff -purN linux-2.5/drivers/scsi/ipr.c linuxppc64-2.5/drivers/scsi/ipr.c
--- linux-2.5/drivers/scsi/ipr.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/ipr.c	2004-03-01 19:33:23.000000000 +0000
@@ -0,0 +1,5884 @@
+/*
+ * ipr.c -- driver for IBM Power Linux RAID adapters
+ *
+ * Written By: Brian King, IBM Corporation
+ *
+ * Copyright (C) 2003, 2004 IBM Corporation
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ *
+ */
+
+/*
+ * Notes:
+ *
+ * This driver is used to control the following SCSI adapters:
+ *
+ * IBM iSeries: 5702, 5703, 2780, 5709, 570A, 570B
+ *
+ * IBM pSeries: PCI-X Dual Channel Ultra 320 SCSI RAID Adapter
+ *              PCI-X Dual Channel Ultra 320 SCSI Adapter
+ *              PCI-X Dual Channel Ultra 320 SCSI RAID Enablement Card
+ *              Embedded SCSI adapter on p615 and p655 systems
+ *
+ * Supported Hardware Features:
+ *	- Ultra 320 SCSI controller
+ *	- PCI-X host interface
+ *	- Embedded PowerPC RISC Processor and Hardware XOR DMA Engine
+ *	- Non-Volatile Write Cache
+ *	- Supports attachment of non-RAID disks, tape, and optical devices
+ *	- RAID Levels 0, 5, 10
+ *	- Hot spare
+ *	- Background Parity Checking
+ *	- Background Data Scrubbing
+ *	- Ability to increase the capacity of an existing RAID 5 disk array
+ *		by adding disks
+ *
+ * Driver Features:
+ *	- Tagged command queuing
+ *	- Adapter microcode download
+ *	- PCI hot plug
+ *	- SCSI device hot plug
+ *
+ */
+
+#include <linux/config.h>
+#include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <linux/kernel.h>
+#include <linux/ioport.h>
+#include <linux/delay.h>
+#include <linux/pci.h>
+#include <linux/wait.h>
+#include <linux/spinlock.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/blkdev.h>
+#include <linux/firmware.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <asm/io.h>
+#include <asm/irq.h>
+#include <asm/processor.h>
+#include <scsi/scsi.h>
+#include <scsi/scsi_host.h>
+#include <scsi/scsi_tcq.h>
+#include <scsi/scsi_eh.h>
+#include <scsi/scsi_cmnd.h>
+#include <scsi/scsi_request.h>
+#include "ipr.h"
+
+/*
+ *   Global Data
+ */
+static struct list_head ipr_ioa_head = LIST_HEAD_INIT(ipr_ioa_head);
+static unsigned int ipr_log_level = IPR_DEFAULT_LOG_LEVEL;
+static unsigned int ipr_max_speed = 1;
+static int ipr_testmode = 0;
+static spinlock_t ipr_driver_lock = SPIN_LOCK_UNLOCKED;
+
+/* This table describes the differences between DMA controller chips */
+static const struct ipr_chip_cfg_t ipr_chip_cfg[] = {
+	{ /* Gemstone */
+		.mailbox = 0x0042C,
+		.cache_line_size = 0x20,
+		{
+			.set_interrupt_mask_reg = 0x0022C,
+			.clr_interrupt_mask_reg = 0x00230,
+			.sense_interrupt_mask_reg = 0x0022C,
+			.clr_interrupt_reg = 0x00228,
+			.sense_interrupt_reg = 0x00224,
+			.ioarrin_reg = 0x00404,
+			.sense_uproc_interrupt_reg = 0x00214,
+			.set_uproc_interrupt_reg = 0x00214,
+			.clr_uproc_interrupt_reg = 0x00218
+		}
+	},
+	{ /* Snipe */
+		.mailbox = 0x0052C,
+		.cache_line_size = 0x20,
+		{
+			.set_interrupt_mask_reg = 0x00288,
+			.clr_interrupt_mask_reg = 0x0028C,
+			.sense_interrupt_mask_reg = 0x00288,
+			.clr_interrupt_reg = 0x00284,
+			.sense_interrupt_reg = 0x00280,
+			.ioarrin_reg = 0x00504,
+			.sense_uproc_interrupt_reg = 0x00290,
+			.set_uproc_interrupt_reg = 0x00290,
+			.clr_uproc_interrupt_reg = 0x00294
+		}
+	},
+};
+
+static int ipr_max_bus_speeds [] = {
+	IPR_80MBs_SCSI_RATE, IPR_U160_SCSI_RATE, IPR_U320_SCSI_RATE
+};
+
+MODULE_AUTHOR("Brian King <brking@us.ibm.com>");
+MODULE_DESCRIPTION("IBM Power RAID SCSI Adapter Driver");
+module_param_named(max_speed, ipr_max_speed, uint, 0);
+MODULE_PARM_DESC(max_speed, "Maximum bus speed (0-2). Default: 1=U160. Speeds: 0=80 MB/s, 1=U160, 2=U320");
+module_param_named(log_level, ipr_log_level, uint, 0);
+MODULE_PARM_DESC(log_level, "Set to 0 - 4 for increasing verbosity of device driver");
+module_param_named(testmode, ipr_testmode, int, 0);
+MODULE_PARM_DESC(testmode, "DANGEROUS!!! Allows unsupported configurations");
+MODULE_LICENSE("GPL");
+
+static const char ipr_version[] = {"ipr version=" IPR_DRIVER_VERSION};
+
+static const char *ipr_gpdd_dev_end_states[] = {
+	"Command complete",
+	"Terminated by host",
+	"Terminated by device reset",
+	"Terminated by bus reset",
+	"Unknown",
+	"Command not started"
+};
+
+static const char *ipr_gpdd_dev_bus_phases[] = {
+	"Bus free",
+	"Arbitration",
+	"Selection",
+	"Message out",
+	"Command",
+	"Message in",
+	"Data out",
+	"Data in",
+	"Status",
+	"Reselection",
+	"Unknown"
+};
+
+/*  A constant array of IOASCs/URCs/Error Messages */
+static const
+struct ipr_error_table_t ipr_error_table[] = {
+	{0x00000000, 1, 1,
+	"8155: An unknown error was received"},
+	{0x00330000, 0, 0,
+	"Soft underlength error"},
+	{0x005A0000, 0, 0,
+	"Command to be cancelled not found"},
+	{0x00808000, 0, 0,
+	"Qualified success"},
+	{0x01080000, 1, 1,
+	"FFFE: Soft device bus error recovered by the IOA"},
+	{0x01170600, 0, 1,
+	"FFF9: Device sector reassign successful"},
+	{0x01170900, 0, 1,
+	"FFF7: Media error recovered by device rewrite procedures"},
+	{0x01180200, 0, 1,
+	"7001: IOA sector reassignment successful"},
+	{0x01180500, 0, 1,
+	"FFF9: Soft media error. Sector reassignment recommended"},
+	{0x01180600, 0, 1,
+	"FFF7: Media error recovered by IOA rewrite procedures"},
+	{0x01418000, 0, 1,
+	"FF3D: Soft PCI bus error recovered by the IOA"},
+	{0x01440000, 1, 1,
+	"FFF6: Device hardware error recovered by the IOA"},
+	{0x01448100, 0, 1,
+	"FFF6: Device hardware error recovered by the device"},
+	{0x01448200, 1, 1,
+	"FF3D: Soft IOA error recovered by the IOA"},
+	{0x01448300, 0, 1,
+	"FFFA: Undefined device response recovered by the IOA"},
+	{0x014A0000, 1, 1,
+	"FFF6: Device bus error, message or command phase"},
+	{0x015D0000, 0, 1,
+	"FFF6: Failure prediction threshold exceeded"},
+	{0x015D9200, 0, 1,
+	"8009: Impending cache battery pack failure"},
+	{0x02040400, 0, 0,
+	"34FF: Disk device format in progress"},
+	{0x023F0000, 0, 0,
+	"Synchronization required"},
+	{0x024E0000, 0, 0,
+	"No ready, IOA shutdown"},
+	{0x02670100, 0, 1,
+	"3020: Storage subsystem configuration error"},
+	{0x03110B00, 0, 0,
+	"FFF5: Medium error, data unreadable, recommend reassign"},
+	{0x03110C00, 0, 0,
+	"7000: Medium error, data unreadable, do not reassign"},
+	{0x03310000, 0, 1,
+	"FFF3: Disk media format bad"},
+	{0x04050000, 0, 1,
+	"3002: Addressed device failed to respond to selection"},
+	{0x04080000, 1, 1,
+	"3100: Device bus error"},
+	{0x04080100, 0, 1,
+	"3109: IOA timed out a device command"},
+	{0x04088000, 0, 0,
+	"3120: SCSI bus is not operational"},
+	{0x04118000, 0, 1,
+	"9000: IOA reserved area data check"},
+	{0x04118100, 0, 1,
+	"9001: IOA reserved area invalid data pattern"},
+	{0x04118200, 0, 1,
+	"9002: IOA reserved area LRC error"},
+	{0x04320000, 0, 1,
+	"102E: Out of alternate sectors for disk storage"},
+	{0x04330000, 1, 1,
+	"FFF4: Data transfer underlength error"},
+	{0x04338000, 1, 1,
+	"FFF4: Data transfer overlength error"},
+	{0x043E0100, 0, 1,
+	"3400: Logical unit failure"},
+	{0x04408500, 0, 1,
+	"FFF4: Device microcode is corrupt"},
+	{0x04418000, 1, 1,
+	"8150: PCI bus error"},
+	{0x04430000, 1, 0,
+	"Unsupported device bus message received"},
+	{0x04440000, 1, 1,
+	"FFF4: Disk device problem"},
+	{0x04448200, 1, 1,
+	"8150: Permanent IOA failure"},
+	{0x04448300, 0, 1,
+	"3010: Disk device returned wrong response to IOA"},
+	{0x04448400, 0, 1,
+	"8151: IOA microcode error"},
+	{0x04448500, 0, 0,
+	"Device bus status error"},
+	{0x04448600, 0, 1,
+	"8157: IOA error requiring IOA reset to recover"},
+	{0x04490000, 0, 0,
+	"Message reject received from the device"},
+	{0x04449200, 0, 1,
+	"8008: A permanent cache battery pack failure occurred"},
+	{0x0444A000, 0, 1,
+	"9090: Disk unit has been modified after the last known status"},
+	{0x0444A200, 0, 1,
+	"9081: IOA detected device error"},
+	{0x0444A300, 0, 1,
+	"9082: IOA detected device error"},
+	{0x044A0000, 1, 1,
+	"3110: Device bus error, message or command phase"},
+	{0x04670400, 0, 1,
+	"9091: Incorrect hardware configuration change has been detected"},
+	{0x046E0000, 0, 1,
+	"FFF4: Command to logical unit failed"},
+	{0x05240000, 1, 0,
+	"Illegal request, invalid request type or request packet"},
+	{0x05250000, 0, 0,
+	"Illegal request, invalid resource handle"},
+	{0x05260000, 0, 0,
+	"Illegal request, invalid field in parameter list"},
+	{0x05260100, 0, 0,
+	"Illegal request, parameter not supported"},
+	{0x05260200, 0, 0,
+	"Illegal request, parameter value invalid"},
+	{0x052C0000, 0, 0,
+	"Illegal request, command sequence error"},
+	{0x06040500, 0, 1,
+	"9031: Array protection temporarily suspended, protection resuming"},
+	{0x06040600, 0, 1,
+	"9040: Array protection temporarily suspended, protection resuming"},
+	{0x06290000, 0, 1,
+	"FFFB: SCSI bus was reset"},
+	{0x06290500, 0, 0,
+	"FFFE: SCSI bus transition to single ended"},
+	{0x06290600, 0, 0,
+	"FFFE: SCSI bus transition to LVD"},
+	{0x06298000, 0, 1,
+	"FFFB: SCSI bus was reset by another initiator"},
+	{0x063F0300, 0, 1,
+	"3029: A device replacement has occurred"},
+	{0x064C8000, 0, 1,
+	"9051: IOA cache data exists for a missing or failed device"},
+	{0x06670100, 0, 1,
+	"9025: Disk unit is not supported at its physical location"},
+	{0x06670600, 0, 1,
+	"3020: IOA detected a SCSI bus configuration error"},
+	{0x06678000, 0, 1,
+	"3150: SCSI bus configuration error"},
+	{0x06690200, 0, 1,
+	"9041: Array protection temporarily suspended"},
+	{0x066B0200, 0, 1,
+	"9030: Array no longer protected due to missing or failed disk unit"},
+	{0x07270000, 0, 0,
+	"Failure due to other device"},
+	{0x07278000, 0, 1,
+	"9008: IOA does not support functions expected by devices"},
+	{0x07278100, 0, 1,
+	"9010: Cache data associated with attached devices cannot be found"},
+	{0x07278200, 0, 1,
+	"9011: Cache data belongs to devices other than those attached"},
+	{0x07278400, 0, 1,
+	"9020: Array missing 2 or more devices with only 1 device present"},
+	{0x07278500, 0, 1,
+	"9021: Array missing 2 or more devices with 2 or more devices present"},
+	{0x07278600, 0, 1,
+	"9022: Exposed array is missing a required device"},
+	{0x07278700, 0, 1,
+	"9023: Array member(s) not at required physical locations"},
+	{0x07278800, 0, 1,
+	"9024: Array not functional due to present hardware configuration"},
+	{0x07278900, 0, 1,
+	"9026: Array not functional due to present hardware configuration"},
+	{0x07278A00, 0, 1,
+	"9027: Array is missing a device and parity is out of sync"},
+	{0x07278B00, 0, 1,
+	"9028: Maximum number of arrays already exist"},
+	{0x07278C00, 0, 1,
+	"9050: Required cache data cannot be located for a disk unit"},
+	{0x07278D00, 0, 1,
+	"9052: Cache data exists for a device that has been modified"},
+	{0x07278F00, 0, 1,
+	"9054: IOA resources not available due to previous problems"},
+	{0x07279100, 0, 1,
+	"9092: Disk unit requires initialization before use"},
+	{0x07279200, 0, 1,
+	"9029: Incorrect hardware configuration change has been detected"},
+	{0x07279600, 0, 1,
+	"9060: One or more disk pairs are missing from an array"},
+	{0x07279700, 0, 1,
+	"9061: One or more disks are missing from an array"},
+	{0x07279800, 0, 1,
+	"9062: One or more disks are missing from an array"},
+	{0x07279900, 0, 1,
+	"9063: Maximum number of functional arrays has been exceeded"},
+	{0x0B260000, 0, 0,
+	"Aborted command, invalid descriptor."},
+	{0x0B5A0000, 0, 0,
+	"Command terminated by host"}
+};
+
+static const struct ipr_ses_table_entry ipr_ses_table[] = {
+	{ "2104-DL1        ", "XXXXXXXXXXXXXXXX", 80 },
+	{ "2104-TL1        ", "XXXXXXXXXXXXXXXX", 80 },
+	{ "HSBP07M P U2SCSI", "XXXXXXXXXXXXXXXX", 80 }, /* Hidive 7 slot */
+	{ "HSBP05M P U2SCSI", "XXXXXXXXXXXXXXXX", 80 }, /* Hidive 5 slot */
+	{ "HSBP05M S U2SCSI", "XXXXXXXXXXXXXXXX", 80 }, /* Bowtie */
+	{ "HSBP06E ASU2SCSI", "XXXXXXXXXXXXXXXX", 80 }, /* MartinFenning */
+	{ "2104-DU3        ", "XXXXXXXXXXXXXXXX", 160 },
+	{ "2104-TU3        ", "XXXXXXXXXXXXXXXX", 160 },
+	{ "HSBP04C RSU2SCSI", "XXXXXXX*XXXXXXXX", 160 },
+	{ "HSBP06E RSU2SCSI", "XXXXXXX*XXXXXXXX", 160 },
+	{ "St  V1S2        ", "XXXXXXXXXXXXXXXX", 160 },
+	{ "HSBPD4M  PU3SCSI", "XXXXXXX*XXXXXXXX", 160 },
+	{ "VSBPD1H   U3SCSI", "XXXXXXX*XXXXXXXX", 160 }
+};
+
+/*
+ *  Function Prototypes
+ */
+static int ipr_reset_alert(struct ipr_cmnd *);
+static void ipr_process_ccn(struct ipr_cmnd *);
+static void ipr_process_error(struct ipr_cmnd *);
+static void ipr_reset_ioa_job(struct ipr_cmnd *);
+static void ipr_initiate_ioa_reset(struct ipr_ioa_cfg *,
+				   enum ipr_shutdown_type);
+
+#ifdef CONFIG_SCSI_IPR_TRACE
+/**
+ * ipr_trc_hook - Add a trace entry to the driver trace
+ * @ipr_cmd:	ipr command struct
+ * @type:		trace type
+ * @add_data:	additional data
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_trc_hook(struct ipr_cmnd *ipr_cmd,
+			 u8 type, u32 add_data)
+{
+	struct ipr_trace_entry *trace_entry;
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+
+	trace_entry = &ioa_cfg->trace[ioa_cfg->trace_index++];
+	trace_entry->time = jiffies;
+	trace_entry->op_code = ipr_cmd->ioarcb.cmd_pkt.cdb[0];
+	trace_entry->type = type;
+	trace_entry->cmd_index = ipr_cmd->cmd_index;
+	trace_entry->res_handle = ipr_cmd->ioarcb.res_handle;
+	trace_entry->add_data = add_data;
+}
+#else
+#define ipr_trc_hook(ipr_cmd, type, add_data) do { } while(0)
+#endif
+
+/**
+ * ipr_reinit_ipr_cmnd - Re-initialize an IPR Cmnd block for reuse
+ * @ipr_cmd:	ipr command struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_reinit_ipr_cmnd(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;
+	struct ipr_ioasa *ioasa = &ipr_cmd->ioasa;
+
+	memset(&ioarcb->cmd_pkt, 0, sizeof(struct ipr_cmd_pkt));
+	ioarcb->write_data_transfer_length = 0;
+	ioarcb->read_data_transfer_length = 0;
+	ioarcb->write_ioadl_len = 0;
+	ioarcb->read_ioadl_len = 0;
+	ioasa->ioasc = 0;
+	ioasa->residual_data_len = 0;
+
+	ipr_cmd->scsi_cmd = NULL;
+	ipr_cmd->sense_buffer[0] = 0;
+	ipr_cmd->dma_use_sg = 0;
+}
+
+/**
+ * ipr_init_ipr_cmnd - Initialize an IPR Cmnd block
+ * @ipr_cmd:	ipr command struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_init_ipr_cmnd(struct ipr_cmnd *ipr_cmd)
+{
+	ipr_reinit_ipr_cmnd(ipr_cmd);
+	init_timer(&ipr_cmd->timer);
+}
+
+/**
+ * ipr_get_free_ipr_cmnd - Get a free IPR Cmnd block
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ * 	pointer to ipr command struct
+ **/
+static
+struct ipr_cmnd *ipr_get_free_ipr_cmnd(struct ipr_ioa_cfg *ioa_cfg)
+{
+	struct ipr_cmnd *ipr_cmd;
+
+	ipr_cmd = list_entry(ioa_cfg->free_q.next, struct ipr_cmnd, queue);
+	list_del(&ipr_cmd->queue);
+	ipr_init_ipr_cmnd(ipr_cmd);
+
+	return ipr_cmd;
+}
+
+/**
+ * ipr_unmap_sglist - Unmap scatterlist if mapped
+ * @ioa_cfg:	ioa config struct
+ * @ipr_cmd:	ipr command struct
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_unmap_sglist(struct ipr_ioa_cfg *ioa_cfg,
+			     struct ipr_cmnd *ipr_cmd)
+{
+	struct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;
+
+	if (ipr_cmd->dma_use_sg) {
+		if (scsi_cmd->use_sg > 0) {
+			pci_unmap_sg(ioa_cfg->pdev, scsi_cmd->request_buffer,
+				     scsi_cmd->use_sg,
+				     scsi_cmd->sc_data_direction);
+		} else {
+			pci_unmap_single(ioa_cfg->pdev, ipr_cmd->dma_handle,
+					 scsi_cmd->request_bufflen,
+					 scsi_cmd->sc_data_direction);
+		}
+	}
+}
+
+/**
+ * ipr_mask_and_clear_all_interrupts - Mask and clears all interrupts on the adapter
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_mask_and_clear_all_interrupts(struct ipr_ioa_cfg *ioa_cfg)
+{
+	volatile u32 int_reg;
+
+	/* Stop new interrupts */
+	ioa_cfg->allow_interrupts = 0;
+
+	/* Set interrupt mask to stop all new interrupts */
+	writel(~0, ioa_cfg->regs.set_interrupt_mask_reg);
+
+	/* Clear any pending interrupts */
+	writel(~0, ioa_cfg->regs.clr_interrupt_reg);
+	int_reg = readl(ioa_cfg->regs.sense_interrupt_reg);
+}
+
+/**
+ * ipr_save_pcix_cmd_reg - Save PCI-X command register
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ * 	0 on success / -EIO on failure
+ **/
+static int ipr_save_pcix_cmd_reg(struct ipr_ioa_cfg *ioa_cfg)
+{
+	int pcix_cmd_reg = pci_find_capability(ioa_cfg->pdev, PCI_CAP_ID_PCIX);
+
+	if (pcix_cmd_reg == 0) {
+		dev_err(&ioa_cfg->pdev->dev, "Failed to save PCI-X command register\n");
+		return -EIO;
+	}
+
+	if (pci_read_config_word(ioa_cfg->pdev, pcix_cmd_reg,
+				 &ioa_cfg->saved_pcix_cmd_reg) != PCIBIOS_SUCCESSFUL) {
+		dev_err(&ioa_cfg->pdev->dev, "Failed to save PCI-X command register\n");
+		return -EIO;
+	}
+
+	ioa_cfg->saved_pcix_cmd_reg |= PCI_X_CMD_DPERR_E | PCI_X_CMD_ERO;
+	return 0;
+}
+
+/**
+ * ipr_set_pcix_cmd_reg - Setup PCI-X command register
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ * 	0 on success / -EIO on failure
+ **/
+static int ipr_set_pcix_cmd_reg(struct ipr_ioa_cfg *ioa_cfg)
+{
+	int pcix_cmd_reg = pci_find_capability(ioa_cfg->pdev, PCI_CAP_ID_PCIX);
+
+	if (pcix_cmd_reg) {
+		if (pci_write_config_word(ioa_cfg->pdev, pcix_cmd_reg,
+					  ioa_cfg->saved_pcix_cmd_reg) != PCIBIOS_SUCCESSFUL) {
+			dev_err(&ioa_cfg->pdev->dev, "Failed to setup PCI-X command register\n");
+			return -EIO;
+		}
+	} else {
+		dev_err(&ioa_cfg->pdev->dev,
+			"Failed to setup PCI-X command register\n");
+		return -EIO;
+	}
+
+	return 0;
+}
+
+/**
+ * ipr_scsi_eh_done - mid-layer done function for aborted ops
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function is invoked by the interrupt handler for
+ * ops generated by the SCSI mid-layer which are being aborted.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_scsi_eh_done(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;
+
+	scsi_cmd->result |= (DID_ERROR << 16);
+
+	ipr_unmap_sglist(ioa_cfg, ipr_cmd);
+	scsi_cmd->scsi_done(scsi_cmd);
+	list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+}
+
+/**
+ * ipr_fail_all_ops - Fails all outstanding ops.
+ * @ioa_cfg:	ioa config struct
+ *
+ * This function fails all outstanding ops.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_fail_all_ops(struct ipr_ioa_cfg *ioa_cfg)
+{
+	struct ipr_cmnd *ipr_cmd, *temp;
+
+	ENTER;
+	list_for_each_entry_safe(ipr_cmd, temp, &ioa_cfg->pending_q, queue) {
+		list_del(&ipr_cmd->queue);
+
+		ipr_cmd->ioasa.ioasc = cpu_to_be32(IPR_IOASC_IOA_WAS_RESET);
+		ipr_cmd->ioasa.ilid = cpu_to_be32(IPR_DRIVER_ILID);
+
+		if (ipr_cmd->scsi_cmd)
+			ipr_cmd->done = ipr_scsi_eh_done;
+
+		ipr_trc_hook(ipr_cmd, IPR_TRACE_FINISH, IPR_IOASC_IOA_WAS_RESET);
+		del_timer(&ipr_cmd->timer);
+		ipr_cmd->done(ipr_cmd);
+	}
+
+	LEAVE;
+}
+
+/**
+ * ipr_do_req -  Send driver initiated requests.
+ * @ipr_cmd:		ipr command struct
+ * @done:			done function
+ * @timeout_func:	timeout function
+ * @timeout:		timeout value
+ *
+ * This function sends the specified command to the adapter with the
+ * timeout given. The done function is invoked on command completion.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_do_req(struct ipr_cmnd *ipr_cmd,
+		       void (*done) (struct ipr_cmnd *),
+		       void (*timeout_func) (struct ipr_cmnd *), u32 timeout)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+
+	list_add_tail(&ipr_cmd->queue, &ioa_cfg->pending_q);
+
+	ipr_cmd->done = done;
+
+	ipr_cmd->timer.data = (unsigned long) ipr_cmd;
+	ipr_cmd->timer.expires = jiffies + timeout;
+	ipr_cmd->timer.function = (void (*)(unsigned long))timeout_func;
+
+	add_timer(&ipr_cmd->timer);
+
+	ipr_trc_hook(ipr_cmd, IPR_TRACE_START, 0);
+
+	mb();
+	writel(be32_to_cpu(ipr_cmd->ioarcb.ioarcb_host_pci_addr),
+	       ioa_cfg->regs.ioarrin_reg);
+}
+
+/**
+ * ipr_internal_cmd_done - Op done function for an internally generated op.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function is the op done function for an internally generated,
+ * blocking op. It simply wakes the sleeping thread.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_internal_cmd_done(struct ipr_cmnd *ipr_cmd)
+{
+	complete(&ipr_cmd->completion);
+}
+
+/**
+ * ipr_send_blocking_cmd - Send command and sleep on its completion.
+ * @ipr_cmd:	ipr command struct
+ * @timeout_func:	function to invoke if command times out
+ * @timeout:	timeout
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_send_blocking_cmd(struct ipr_cmnd *ipr_cmd,
+				  void (*timeout_func) (struct ipr_cmnd *ipr_cmd),
+				  u32 timeout)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+
+	init_completion(&ipr_cmd->completion);
+	ipr_do_req(ipr_cmd, ipr_internal_cmd_done, timeout_func, timeout);
+
+	spin_unlock_irq(ioa_cfg->host->host_lock);
+	wait_for_completion(&ipr_cmd->completion);
+	spin_lock_irq(ioa_cfg->host->host_lock);
+}
+
+/**
+ * ipr_send_hcam - Send an HCAM to the adapter.
+ * @ioa_cfg:	ioa config struct
+ * @type:		HCAM type
+ * @hostrcb:	hostrcb struct
+ *
+ * This function will send a Host Controlled Async command to the adapter.
+ * If HCAMs are currently not allowed to be issued to the adapter, it will
+ * place the hostrcb on the free queue.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_send_hcam(struct ipr_ioa_cfg *ioa_cfg, u8 type,
+			  struct ipr_hostrcb *hostrcb)
+{
+	struct ipr_cmnd *ipr_cmd;
+	struct ipr_ioarcb *ioarcb;
+
+	if (ioa_cfg->allow_cmds) {
+		ipr_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);
+		list_add_tail(&ipr_cmd->queue, &ioa_cfg->pending_q);
+		list_add_tail(&hostrcb->queue, &ioa_cfg->hostrcb_pending_q);
+
+		ipr_cmd->hostrcb = hostrcb;
+		ioarcb = &ipr_cmd->ioarcb;
+
+		ioarcb->res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);
+		ioarcb->cmd_pkt.request_type = IPR_RQTYPE_HCAM;
+		ioarcb->cmd_pkt.cdb[0] = IPR_HOST_CONTROLLED_ASYNC;
+		ioarcb->cmd_pkt.cdb[1] = type;
+		ioarcb->cmd_pkt.cdb[7] = (sizeof(struct ipr_hostrcb) >> 8) & 0xff;
+		ioarcb->cmd_pkt.cdb[8] = sizeof(struct ipr_hostrcb) & 0xff;
+
+		ioarcb->read_data_transfer_length =
+			cpu_to_be32(sizeof(struct ipr_hostrcb));
+		ioarcb->read_ioadl_len =
+			cpu_to_be32(sizeof(struct ipr_ioadl_desc));
+		ipr_cmd->ioadl[0].flags_and_data_len =
+			cpu_to_be32(IPR_IOADL_FLAGS_READ_LAST | sizeof(struct ipr_hostrcb));
+		ipr_cmd->ioadl[0].address = cpu_to_be32(hostrcb->hostrcb_dma);
+
+		if (type == IPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE)
+			ipr_cmd->done = ipr_process_ccn;
+		else
+			ipr_cmd->done = ipr_process_error;
+
+		mb();
+		writel(be32_to_cpu(ipr_cmd->ioarcb.ioarcb_host_pci_addr),
+		       ioa_cfg->regs.ioarrin_reg);
+	} else {
+		list_add_tail(&hostrcb->queue, &ioa_cfg->hostrcb_free_q);
+	}
+}
+
+/**
+ * ipr_init_res_entry - Initialize a resource entry struct.
+ * @res:	resource entry struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_init_res_entry(struct ipr_resource_entry *res)
+{
+	res->needs_sync_complete = 1;
+	res->in_erp = 0;
+	res->add_to_ml = 0;
+	res->del_from_ml = 0;
+	res->resetting_device = 0;
+	res->tcq_active = 0;
+	res->qdepth = IPR_MAX_CMD_PER_LUN;
+	res->sdev = NULL;
+}
+
+/**
+ * ipr_handle_config_change - Handle a config change from the adapter
+ * @ioa_cfg:	ioa config struct
+ * @hostrcb:	hostrcb
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_handle_config_change(struct ipr_ioa_cfg *ioa_cfg,
+			      struct ipr_hostrcb *hostrcb)
+{
+	struct ipr_resource_entry *res = NULL;
+	struct ipr_config_table_entry *cfgte;
+	u32 is_ndn = 1;
+
+	cfgte = &hostrcb->ccn.cfgte;
+
+	list_for_each_entry(res, &ioa_cfg->used_res_q, queue) {
+		if (res->cfgte.res_handle == cfgte->res_handle) {
+			is_ndn = 0;
+			break;
+		}
+	}
+
+	if (is_ndn) {
+		if (list_empty(&ioa_cfg->free_res_q)) {
+			ipr_send_hcam(ioa_cfg,
+				      IPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE,
+				      hostrcb);
+			return;
+		}
+
+		res = list_entry(ioa_cfg->free_res_q.next,
+				 struct ipr_resource_entry, queue);
+
+		list_del(&res->queue);
+		ipr_init_res_entry(res);
+		res->add_to_ml = 1;
+		list_add_tail(&res->queue, &ioa_cfg->used_res_q);
+	}
+
+	memcpy(&res->cfgte, cfgte, sizeof(struct ipr_config_table_entry));
+
+	if (hostrcb->notify_type == IPR_HOST_RCB_NOTIF_TYPE_REM_ENTRY) {
+		if (res->sdev) {
+			res->del_from_ml = 1;
+			schedule_work(&ioa_cfg->work_q);
+		} else
+			list_move_tail(&res->queue, &ioa_cfg->free_res_q);
+	} else if (res->add_to_ml)
+		schedule_work(&ioa_cfg->work_q);
+
+	ipr_send_hcam(ioa_cfg, IPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE, hostrcb);
+}
+
+/**
+ * ipr_process_ccn - Op done function for a CCN.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function is the op done function for a configuration
+ * change notification host controlled async from the adapter.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_process_ccn(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_hostrcb *hostrcb = ipr_cmd->hostrcb;
+	u32 ioasc = be32_to_cpu(ipr_cmd->ioasa.ioasc);
+
+	list_del(&hostrcb->queue);
+
+	if (ioasc) {
+		if (ioasc != IPR_IOASC_IOA_WAS_RESET) {
+			dev_err(&ioa_cfg->pdev->dev,
+				"Host RCB failed with IOASC: 0x%08X\n", ioasc);
+		}
+
+		ipr_send_hcam(ioa_cfg, IPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE, hostrcb);
+	} else {
+		ipr_handle_config_change(ioa_cfg, hostrcb);
+	}
+}
+
+/**
+ * ipr_log_vpd - Log the passed VPD to the error log.
+ * @vpids:			vendor/product id struct
+ * @serial_num:		serial number string
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_log_vpd(struct ipr_std_inq_vpids *vpids, u8 *serial_num)
+{
+	char buffer[max_t(int, sizeof(struct ipr_std_inq_vpids),
+			  IPR_SERIAL_NUM_LEN) + 1];
+
+	memcpy(buffer, vpids, sizeof(struct ipr_std_inq_vpids));
+	buffer[sizeof(struct ipr_std_inq_vpids)] = '\0';
+	ipr_err("Vendor/Product ID: %s\n", buffer);
+
+	memcpy(buffer, serial_num, IPR_SERIAL_NUM_LEN);
+	buffer[IPR_SERIAL_NUM_LEN] = '\0';
+	ipr_err("    Serial Number: %s\n", buffer);
+}
+
+/**
+ * ipr_log_cache_error - Log a cache error.
+ * @ioa_cfg:	ioa config struct
+ * @hostrcb:	hostrcb struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_log_cache_error(struct ipr_ioa_cfg *ioa_cfg,
+				struct ipr_hostrcb *hostrcb)
+{
+	struct ipr_hostrcb_type_02_error *error = &hostrcb->error.type_02_error;
+
+	ipr_err("-----Current Configuration-----\n");
+	ipr_err("I/O Processor Information:\n");
+	ipr_log_vpd(&error->ioa_vpids, error->ioa_sn);
+	ipr_err("Cache Adapter Card Information:\n");
+	ipr_log_vpd(&error->cfc_vpids, error->cfc_sn);
+
+	ipr_err("-----Expected Configuration-----\n");
+	ipr_err("I/O Processor Information:\n");
+	ipr_log_vpd(&error->ioa_last_attached_to_cfc_vpids,
+		    error->ioa_last_attached_to_cfc_sn);
+	ipr_err("Cache Adapter Card Information:\n");
+	ipr_log_vpd(&error->cfc_last_attached_to_ioa_vpids,
+		    error->cfc_last_attached_to_ioa_sn);
+
+	ipr_err("Additional IOA Data: %08X %08X %08X\n",
+		     be32_to_cpu(error->ioa_data[0]),
+		     be32_to_cpu(error->ioa_data[1]),
+		     be32_to_cpu(error->ioa_data[2]));
+}
+
+/**
+ * ipr_log_config_error - Log a configuration error.
+ * @ioa_cfg:	ioa config struct
+ * @hostrcb:	hostrcb struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_log_config_error(struct ipr_ioa_cfg *ioa_cfg,
+				 struct ipr_hostrcb *hostrcb)
+{
+	int errors_logged, i;
+	struct ipr_hostrcb_device_data_entry *dev_entry;
+
+	errors_logged = be32_to_cpu(hostrcb->error.type_03_error.errors_logged);
+
+	ipr_err("Device Errors Detected/Logged: %d/%d\n",
+		be32_to_cpu(hostrcb->error.type_03_error.errors_detected),
+		errors_logged);
+
+	dev_entry = hostrcb->error.type_03_error.dev_entry;
+
+	for (i = 0; i < errors_logged; i++, dev_entry++) {
+		ipr_err_separator;
+
+		if (dev_entry->dev_res_addr.bus >= IPR_MAX_NUM_BUSES) {
+			ipr_err("Device %d: missing\n", i + 1);
+		} else {
+			ipr_err("Device %d: %d:%d:%d:%d\n", i + 1,
+				ioa_cfg->host->host_no, dev_entry->dev_res_addr.bus,
+				dev_entry->dev_res_addr.target, dev_entry->dev_res_addr.lun);
+		}
+		ipr_log_vpd(&dev_entry->dev_vpids, dev_entry->dev_sn);
+
+		ipr_err("-----New Device Information-----\n");
+		ipr_log_vpd(&dev_entry->new_dev_vpids, dev_entry->new_dev_sn);
+
+		ipr_err("I/O Processor Information:\n");
+		ipr_log_vpd(&dev_entry->ioa_last_with_dev_vpids,
+			    dev_entry->ioa_last_with_dev_sn);
+
+		ipr_err("Cache Adapter Card Information:\n");
+		ipr_log_vpd(&dev_entry->cfc_last_with_dev_vpids,
+			    dev_entry->cfc_last_with_dev_sn);
+
+		ipr_err("Additional IOA Data: %08X %08X %08X %08X %08X\n",
+			be32_to_cpu(dev_entry->ioa_data[0]),
+			be32_to_cpu(dev_entry->ioa_data[1]),
+			be32_to_cpu(dev_entry->ioa_data[2]),
+			be32_to_cpu(dev_entry->ioa_data[3]),
+			be32_to_cpu(dev_entry->ioa_data[4]));
+	}
+}
+
+/**
+ * ipr_log_array_error - Log an array configuration error.
+ * @ioa_cfg:	ioa config struct
+ * @hostrcb:	hostrcb struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_log_array_error(struct ipr_ioa_cfg *ioa_cfg,
+				struct ipr_hostrcb *hostrcb)
+{
+	int i;
+	struct ipr_hostrcb_type_04_error *error;
+	struct ipr_hostrcb_array_data_entry *array_entry;
+	u8 zero_sn[IPR_SERIAL_NUM_LEN];
+
+	memset(zero_sn, '0', IPR_SERIAL_NUM_LEN);
+
+	error = &hostrcb->error.type_04_error;
+
+	ipr_err_separator;
+
+	ipr_err("RAID %s Array Configuration: %d:%d:%d:%d\n",
+		error->protection_level,
+		ioa_cfg->host->host_no,
+		error->last_func_vset_res_addr.bus,
+		error->last_func_vset_res_addr.target,
+		error->last_func_vset_res_addr.lun);
+
+	ipr_err_separator;
+
+	array_entry = error->array_member;
+
+	for (i = 0; i < 18; i++) {
+		if (!memcmp(array_entry->serial_num, zero_sn, IPR_SERIAL_NUM_LEN))
+			continue;
+
+		if (error->exposed_mode_adn == i) {
+			ipr_err("Exposed Array Member %d:\n", i);
+		} else {
+			ipr_err("Array Member %d:\n", i);
+		}
+
+		ipr_log_vpd(&array_entry->vpids, array_entry->serial_num);
+
+		if (array_entry->dev_res_addr.bus >= IPR_MAX_NUM_BUSES) {
+			ipr_err("Current Location: unknown\n");
+		} else {
+			ipr_err("Current Location: %d:%d:%d:%d\n",
+				ioa_cfg->host->host_no,
+				array_entry->dev_res_addr.bus,
+				array_entry->dev_res_addr.target,
+				array_entry->dev_res_addr.lun);
+		}
+
+		if (array_entry->dev_res_addr.bus >= IPR_MAX_NUM_BUSES) {
+			ipr_err("Expected Location: unknown\n");
+		} else {
+			ipr_err("Expected Location: %d:%d:%d:%d\n",
+				ioa_cfg->host->host_no,
+				array_entry->expected_dev_res_addr.bus,
+				array_entry->expected_dev_res_addr.target,
+				array_entry->expected_dev_res_addr.lun);
+		}
+
+		ipr_err_separator;
+
+		if (i == 9)
+			array_entry = error->array_member2;
+		else
+			array_entry++;
+	}
+}
+
+/**
+ * ipr_log_generic_error - Log an adapter error.
+ * @ioa_cfg:	ioa config struct
+ * @hostrcb:	hostrcb struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_log_generic_error(struct ipr_ioa_cfg *ioa_cfg,
+				  struct ipr_hostrcb *hostrcb)
+{
+	int i;
+	int ioa_data_len = be32_to_cpu(hostrcb->length);
+
+	if (ioa_data_len == 0)
+		return;
+
+	ipr_err("IOA Error Data:\n");
+	ipr_err("Offset    0 1 2 3  4 5 6 7  8 9 A B  C D E F\n");
+
+	for (i = 0; i < ioa_data_len / 4; i += 4) {
+		ipr_err("%08X: %08X %08X %08X %08X\n", i*4,
+			be32_to_cpu(hostrcb->raw.data[i]),
+			be32_to_cpu(hostrcb->raw.data[i+1]),
+			be32_to_cpu(hostrcb->raw.data[i+2]),
+			be32_to_cpu(hostrcb->raw.data[i+3]));
+	}
+}
+
+/**
+ * ipr_get_error - Find the specfied IOASC in the ipr_error_table.
+ * @ioasc:	IOASC
+ *
+ * This function will return the index of into the ipr_error_table
+ * for the specified IOASC. If the IOASC is not in the table,
+ * 0 will be returned, which points to the entry used for unknown errors.
+ *
+ * Return value:
+ * 	index into the ipr_error_table
+ **/
+static u32 ipr_get_error(u32 ioasc)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(ipr_error_table); i++)
+		if (ipr_error_table[i].ioasc == ioasc)
+			return i;
+
+	return 0;
+}
+
+/**
+ * ipr_handle_log_data - Log an adapter error.
+ * @ioa_cfg:	ioa config struct
+ * @hostrcb:	hostrcb struct
+ *
+ * This function logs an adapter error to the system.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_handle_log_data(struct ipr_ioa_cfg *ioa_cfg,
+				struct ipr_hostrcb *hostrcb)
+{
+	u32 ioasc;
+	int error_index;
+
+	if (hostrcb->notify_type != IPR_HOST_RCB_NOTIF_TYPE_ERROR_LOG_ENTRY)
+		return;
+
+	if (hostrcb->notifications_lost == IPR_HOST_RCB_NOTIFICATIONS_LOST)
+		dev_err(&ioa_cfg->pdev->dev, "Error notifications lost\n");
+
+	ioasc = be32_to_cpu(hostrcb->error.failing_dev_ioasc);
+
+	if ((ioasc == IPR_IOASC_BUS_WAS_RESET) ||
+	    (ioasc == IPR_IOASC_BUS_WAS_RESET_BY_OTHER)) {
+		/* Tell the midlayer we had a bus reset so it will handle the UA properly */
+		scsi_report_bus_reset(ioa_cfg->host,
+				      hostrcb->error.failing_dev_res_addr.bus);
+	}
+
+	error_index = ipr_get_error(ioasc);
+
+	if (!ipr_error_table[error_index].log_hcam)
+		return;
+
+	if (ipr_is_device(&hostrcb->error.failing_dev_res_addr)) {
+		ipr_res_err(ioa_cfg, hostrcb->error.failing_dev_res_addr,
+			    "%s\n", ipr_error_table[error_index].error);
+	} else {
+		dev_err(&ioa_cfg->pdev->dev, "%s\n",
+			ipr_error_table[error_index].error);
+	}
+
+	/* Set indication we have logged an error */
+	ioa_cfg->errors_logged++;
+
+	switch (hostrcb->overlay_id) {
+	case IPR_HOST_RCB_OVERLAY_ID_1:
+		ipr_log_generic_error(ioa_cfg, hostrcb);
+		break;
+	case IPR_HOST_RCB_OVERLAY_ID_2:
+		ipr_log_cache_error(ioa_cfg, hostrcb);
+		break;
+	case IPR_HOST_RCB_OVERLAY_ID_3:
+		ipr_log_config_error(ioa_cfg, hostrcb);
+		break;
+	case IPR_HOST_RCB_OVERLAY_ID_4:
+	case IPR_HOST_RCB_OVERLAY_ID_6:
+		ipr_log_array_error(ioa_cfg, hostrcb);
+		break;
+	case IPR_HOST_RCB_OVERLAY_ID_DEFAULT:
+		ipr_log_generic_error(ioa_cfg, hostrcb);
+		break;
+	default:
+		dev_err(&ioa_cfg->pdev->dev,
+			"Unknown error received. Overlay ID: %d\n",
+			hostrcb->overlay_id);
+		break;
+	}
+}
+
+/**
+ * ipr_process_error - Op done function for an adapter error log.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function is the op done function for an error log host
+ * controlled async from the adapter. It will log the error and
+ * send the HCAM back to the adapter.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_process_error(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_hostrcb *hostrcb = ipr_cmd->hostrcb;
+	u32 ioasc = be32_to_cpu(ipr_cmd->ioasa.ioasc);
+
+	list_del(&hostrcb->queue);
+
+	if (!ioasc) {
+		ipr_handle_log_data(ioa_cfg, hostrcb);
+	} else if (ioasc != IPR_IOASC_IOA_WAS_RESET) {
+		dev_err(&ioa_cfg->pdev->dev,
+			"Host RCB failed with IOASC: 0x%08X\n", ioasc);
+	}
+
+	ipr_send_hcam(ioa_cfg, IPR_HCAM_CDB_OP_CODE_LOG_DATA, hostrcb);
+}
+
+/**
+ * ipr_timeout -  An internally generated op has timed out.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function blocks host requests and initiates an
+ * adapter reset.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_timeout(struct ipr_cmnd *ipr_cmd)
+{
+	unsigned long lock_flags = 0;
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+
+	ENTER;
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+
+	ioa_cfg->errors_logged++;
+	dev_err(&ioa_cfg->pdev->dev,
+		"Adapter being reset due to command timeout.\n");
+
+	if (WAIT_FOR_DUMP == ioa_cfg->sdt_state)
+		ioa_cfg->sdt_state = GET_DUMP;
+
+	if (!ioa_cfg->in_reset_reload || ioa_cfg->reset_cmd == ipr_cmd)
+		ipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	LEAVE;
+}
+
+/**
+ * ipr_reset_reload - Reset/Reload the IOA
+ * @ioa_cfg:		ioa config struct
+ * @shutdown_type:	shutdown type
+ *
+ * This function resets the adapter and re-initializes it.
+ * This function assumes that all new host commands have been stopped.
+ * Return value:
+ * 	SUCCESS / FAILED
+ **/
+static int ipr_reset_reload(struct ipr_ioa_cfg *ioa_cfg,
+			    enum ipr_shutdown_type shutdown_type)
+{
+	if (ioa_cfg->ioa_is_dead)
+		return FAILED;
+
+	if (ioa_cfg->in_reset_reload) {
+		spin_unlock_irq(ioa_cfg->host->host_lock);
+		wait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);
+		spin_lock_irq(ioa_cfg->host->host_lock);
+
+		/* If we got hit with a host reset while we were already resetting
+		   the adapter for some reason, and the reset failed. */
+		if (ioa_cfg->ioa_is_dead) {
+			ipr_trace;
+			return FAILED;
+		}
+	} else {
+		ipr_initiate_ioa_reset(ioa_cfg, shutdown_type);
+
+		if (ioa_cfg->in_reset_reload) {
+			spin_unlock_irq(ioa_cfg->host->host_lock);
+			wait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);
+			spin_lock_irq(ioa_cfg->host->host_lock);
+		}
+
+		if (ioa_cfg->ioa_is_dead) {
+			ipr_trace;
+			return FAILED;
+		}
+	}
+
+	return SUCCESS;
+}
+
+/**
+ * ipr_find_ses_entry - Find matching SES in SES table
+ * @res:	resource entry struct of SES
+ *
+ * Return value:
+ * 	pointer to SES table entry / NULL on failure
+ **/
+static const struct ipr_ses_table_entry *
+ipr_find_ses_entry(struct ipr_resource_entry *res)
+{
+	int i, j, matches;
+	const struct ipr_ses_table_entry *ste = ipr_ses_table;
+
+	for (i = 0; i < ARRAY_SIZE(ipr_ses_table); i++, ste++) {
+		for (j = 0, matches = 0; j < IPR_PROD_ID_LEN; j++) {
+			if (ste->compare_product_id_byte[j] == 'X') {
+				if (res->cfgte.std_inq_data.vpids.product_id[j] == ste->product_id[j])
+					matches++;
+				else
+					break;
+			} else
+				matches++;
+		}
+
+		if (matches == IPR_PROD_ID_LEN)
+			return ste;
+	}
+
+	return NULL;
+}
+
+/**
+ * ipr_get_max_scsi_speed - Determine max SCSI speed for a given bus
+ * @ioa_cfg:	ioa config struct
+ * @bus:		SCSI bus
+ * @bus_width:	bus width
+ *
+ * Return value:
+ *	SCSI bus speed in units of 100KHz, 1600 is 160 MHz
+ *	For a 2-byte wide SCSI bus, the maximum transfer speed is
+ *	twice the maximum transfer rate (e.g. for a wide enabled bus,
+ *	max 160MHz = max 320MB/sec).
+ **/
+static u32 ipr_get_max_scsi_speed(struct ipr_ioa_cfg *ioa_cfg, u8 bus, u8 bus_width)
+{
+	struct ipr_resource_entry *res;
+	const struct ipr_ses_table_entry *ste;
+	u32 max_xfer_rate = IPR_MAX_SCSI_RATE(bus_width);
+
+	/* Loop through each config table entry in the config table buffer */
+	list_for_each_entry(res, &ioa_cfg->used_res_q, queue) {
+		if (!(IPR_IS_SES_DEVICE(res->cfgte.std_inq_data)))
+			continue;
+
+		if (bus != res->cfgte.res_addr.bus)
+			continue;
+
+		if (!(ste = ipr_find_ses_entry(res)))
+			continue;
+
+		max_xfer_rate = (ste->max_bus_speed_limit * 10) / (bus_width / 8);
+	}
+
+	return max_xfer_rate;
+}
+
+/**
+ * ipr_wait_iodbg_ack - Wait for an IODEBUG ACK from the IOA
+ * @ioa_cfg:		ioa config struct
+ * @max_delay:		max delay in micro-seconds to wait
+ *
+ * Waits for an IODEBUG ACK from the IOA, doing busy looping.
+ *
+ * Return value:
+ * 	0 on success / other on failure
+ **/
+static int ipr_wait_iodbg_ack(struct ipr_ioa_cfg *ioa_cfg, int max_delay)
+{
+	volatile u32 pcii_reg;
+	int delay = 1;
+
+	/* Read interrupt reg until IOA signals IO Debug Acknowledge */
+	while (delay < max_delay) {
+		pcii_reg = readl(ioa_cfg->regs.sense_interrupt_reg);
+
+		if (pcii_reg & IPR_PCII_IO_DEBUG_ACKNOWLEDGE)
+			return 0;
+
+		/* udelay cannot be used if delay is more than a few milliseconds */
+		if ((delay / 1000) > MAX_UDELAY_MS)
+			mdelay(delay / 1000);
+		else
+			udelay(delay);
+
+		delay += delay;
+	}
+	return -EIO;
+}
+
+/**
+ * ipr_get_ldump_data_section - Dump IOA memory
+ * @ioa_cfg:			ioa config struct
+ * @start_addr:			adapter address to dump
+ * @dest:				destination kernel buffer
+ * @length_in_words:	length to dump in 4 byte words
+ *
+ * Return value:
+ * 	0 on success / -EIO on failure
+ **/
+static int ipr_get_ldump_data_section(struct ipr_ioa_cfg *ioa_cfg,
+				      u32 start_addr,
+				      u32 *dest, u32 length_in_words)
+{
+	volatile u32 temp_pcii_reg;
+	int i, delay = 0;
+
+	/* Write IOA interrupt reg starting LDUMP state  */
+	writel((IPR_UPROCI_RESET_ALERT | IPR_UPROCI_IO_DEBUG_ALERT),
+	       ioa_cfg->regs.set_uproc_interrupt_reg);
+	mb();
+
+	/* Wait for IO debug acknowledge */
+	if (ipr_wait_iodbg_ack(ioa_cfg,
+			       IPR_LDUMP_MAX_LONG_ACK_DELAY_IN_USEC)) {
+		dev_err(&ioa_cfg->pdev->dev,
+			"IOA dump long data transfer timeout\n");
+		return -EIO;
+	}
+
+	/* Signal LDUMP interlocked - clear IO debug ack */
+	writel(IPR_PCII_IO_DEBUG_ACKNOWLEDGE,
+	       ioa_cfg->regs.clr_interrupt_reg);
+
+	/* Write Mailbox with starting address */
+	writel(start_addr, ioa_cfg->ioa_mailbox);
+
+	/* Signal address valid - clear IOA Reset alert */
+	writel(IPR_UPROCI_RESET_ALERT,
+	       ioa_cfg->regs.clr_uproc_interrupt_reg);
+	mb();
+
+	for (i = 0; i < length_in_words; i++) {
+		/* Wait for IO debug acknowledge */
+		if (ipr_wait_iodbg_ack(ioa_cfg,
+				       IPR_LDUMP_MAX_SHORT_ACK_DELAY_IN_USEC)) {
+			dev_err(&ioa_cfg->pdev->dev,
+				"IOA dump short data transfer timeout\n");
+			return -EIO;
+		}
+
+		/* Read data from mailbox and increment destination pointer */
+		*dest = cpu_to_be32(readl(ioa_cfg->ioa_mailbox));
+		dest++;
+
+		/* For all but the last word of data, signal data received */
+		if (i < (length_in_words - 1)) {
+			/* Signal dump data received - Clear IO debug Ack */
+			writel(IPR_PCII_IO_DEBUG_ACKNOWLEDGE,
+			       ioa_cfg->regs.clr_interrupt_reg);
+			mb();
+		}
+	}
+
+	/* Signal end of block transfer. Set reset alert then clear IO debug ack */
+	writel(IPR_UPROCI_RESET_ALERT,
+	       ioa_cfg->regs.set_uproc_interrupt_reg);
+
+	writel(IPR_UPROCI_IO_DEBUG_ALERT,
+	       ioa_cfg->regs.clr_uproc_interrupt_reg);
+
+	/* Signal dump data received - Clear IO debug Ack */
+	writel(IPR_PCII_IO_DEBUG_ACKNOWLEDGE,
+	       ioa_cfg->regs.clr_interrupt_reg);
+	mb();
+
+	/* Wait for IOA to signal LDUMP exit - IOA reset alert will be cleared */
+	while (delay < IPR_LDUMP_MAX_SHORT_ACK_DELAY_IN_USEC) {
+		temp_pcii_reg =
+		    readl(ioa_cfg->regs.sense_uproc_interrupt_reg);
+
+		if (!(temp_pcii_reg & IPR_UPROCI_RESET_ALERT))
+			return 0;
+
+		udelay(10);
+		delay += 10;
+	}
+
+	return 0;
+}
+
+#ifdef CONFIG_SCSI_IPR_DUMP
+/**
+ * ipr_sdt_copy - Copy Smart Dump Table to kernel buffer
+ * @ioa_cfg:		ioa config struct
+ * @pci_address:	adapter address
+ * @length:			length of data to copy
+ *
+ * Copy data from PCI adapter to kernel buffer.
+ * Note: length MUST be a 4 byte multiple
+ * Return value:
+ * 	0 on success / other on failure
+ **/
+static int ipr_sdt_copy(struct ipr_ioa_cfg *ioa_cfg,
+			unsigned long pci_address, u32 length)
+{
+	int bytes_copied = 0;
+	int cur_len, rc, rem_len, rem_page_len;
+	u32 *page;
+	unsigned long lock_flags = 0;
+	struct ipr_ioa_dump *ioa_dump = &ioa_cfg->dump->ioa_dump;
+
+	while ((bytes_copied < length) &&
+	       ((ioa_dump->hdr.len + bytes_copied) < IPR_MAX_IOA_DUMP_SIZE)) {
+		if ((ioa_dump->page_offset >= PAGE_SIZE) ||
+		    (ioa_dump->page_offset == 0)) {
+			page = (u32 *)__get_free_page(GFP_ATOMIC);
+
+			if (!page) {
+				ipr_trace;
+				return bytes_copied;
+			}
+
+			ioa_dump->page_offset = 0;
+			ioa_dump->ioa_data[ioa_dump->next_page_index] = page;
+			ioa_dump->next_page_index++;
+		} else
+			page = ioa_dump->ioa_data[ioa_dump->next_page_index - 1];
+
+		rem_len = length - bytes_copied;
+		rem_page_len = PAGE_SIZE - ioa_dump->page_offset;
+		cur_len = min(rem_len, rem_page_len);
+
+		spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+		if (ioa_cfg->sdt_state == ABORT_DUMP) {
+			rc = -EIO;
+		} else {
+			rc = ipr_get_ldump_data_section(ioa_cfg,
+							pci_address + bytes_copied,
+							&page[ioa_dump->page_offset / 4],
+							(cur_len / sizeof(u32)));
+		}
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+
+		if (!rc) {
+			ioa_dump->page_offset += cur_len;
+			bytes_copied += cur_len;
+		} else {
+			ipr_trace;
+			break;
+		}
+		schedule();
+	}
+
+	return bytes_copied;
+}
+
+/**
+ * ipr_init_dump_entry_hdr - Initialize a dump entry header.
+ * @hdr:	dump entry header struct
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_init_dump_entry_hdr(struct ipr_dump_entry_header *hdr)
+{
+	hdr->eye_catcher = IPR_DUMP_EYE_CATCHER;
+	hdr->num_elems = 1;
+	hdr->offset = sizeof(*hdr);
+	hdr->status = IPR_DUMP_STATUS_SUCCESS;
+}
+
+/**
+ * ipr_dump_version_data - Fill in the driver version in the dump.
+ * @ioa_cfg:	ioa config struct
+ * @driver_dump:	driver dump struct
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_dump_version_data(struct ipr_ioa_cfg *ioa_cfg,
+				  struct ipr_driver_dump *driver_dump)
+{
+	ipr_init_dump_entry_hdr(&driver_dump->version_entry.hdr);
+	driver_dump->version_entry.hdr.len =
+		sizeof(struct ipr_dump_version_entry) -
+		sizeof(struct ipr_dump_entry_header);
+	driver_dump->version_entry.hdr.data_type = IPR_DUMP_DATA_TYPE_ASCII;
+	driver_dump->version_entry.hdr.id = IPR_DUMP_DRIVER_VERSION_ID;
+	strcpy(driver_dump->version_entry.version, IPR_DRIVER_VERSION);
+	driver_dump->hdr.num_entries++;
+}
+
+/**
+ * ipr_dump_trace_data - Fill in the IOA trace in the dump.
+ * @ioa_cfg:	ioa config struct
+ * @driver_dump:	driver dump struct
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_dump_trace_data(struct ipr_ioa_cfg *ioa_cfg,
+				   struct ipr_driver_dump *driver_dump)
+{
+	ipr_init_dump_entry_hdr(&driver_dump->trace_entry.hdr);
+	driver_dump->trace_entry.hdr.len =
+		sizeof(struct ipr_dump_trace_entry) -
+		sizeof(struct ipr_dump_entry_header);
+	driver_dump->trace_entry.hdr.data_type = IPR_DUMP_DATA_TYPE_BINARY;
+	driver_dump->trace_entry.hdr.id = IPR_DUMP_TRACE_ID;
+	memcpy(driver_dump->trace_entry.trace, ioa_cfg->trace, IPR_TRACE_SIZE);
+	driver_dump->hdr.num_entries++;
+}
+
+/**
+ * ipr_dump_location_data - Fill in the IOA location in the dump.
+ * @ioa_cfg:	ioa config struct
+ * @driver_dump:	driver dump struct
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_dump_location_data(struct ipr_ioa_cfg *ioa_cfg,
+				   struct ipr_driver_dump *driver_dump)
+{
+	ipr_init_dump_entry_hdr(&driver_dump->location_entry.hdr);
+	driver_dump->location_entry.hdr.len =
+		sizeof(struct ipr_dump_location_entry) -
+		sizeof(struct ipr_dump_entry_header);
+	driver_dump->location_entry.hdr.data_type = IPR_DUMP_DATA_TYPE_ASCII;
+	driver_dump->location_entry.hdr.id = IPR_DUMP_LOCATION_ID;
+	strcpy(driver_dump->location_entry.location, ioa_cfg->pdev->dev.bus_id);
+	driver_dump->hdr.num_entries++;
+}
+
+/**
+ * ipr_get_ioa_dump - Perform a dump of the driver and adapter.
+ * @ioa_cfg:	ioa config struct
+ * @dump:		dump struct
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_get_ioa_dump(struct ipr_ioa_cfg *ioa_cfg, struct ipr_dump *dump)
+{
+	unsigned long start_addr, sdt_word;
+	unsigned long lock_flags = 0;
+	struct ipr_driver_dump *driver_dump = &dump->driver_dump;
+	struct ipr_ioa_dump *ioa_dump = &dump->ioa_dump;
+	u32 num_entries, start_off, end_off;
+	u32 bytes_to_copy, bytes_copied, rc;
+	struct ipr_sdt *sdt;
+	int i;
+
+	ENTER;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+
+	if (ioa_cfg->sdt_state != GET_DUMP)
+		return;
+
+	start_addr = readl(ioa_cfg->ioa_mailbox);
+
+	if (!ipr_sdt_is_fmt2(start_addr)) {
+		dev_err(&ioa_cfg->pdev->dev,
+			"Invalid dump table format: %lx\n", start_addr);
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		return;
+	}
+
+	dev_err(&ioa_cfg->pdev->dev, "Dump of IOA initiated\n");
+
+	driver_dump->hdr.eye_catcher = IPR_DUMP_EYE_CATCHER;
+
+	/* Initialize the overall dump header */
+	driver_dump->hdr.len = sizeof(struct ipr_driver_dump);
+	driver_dump->hdr.num_entries = 1;
+	driver_dump->hdr.first_entry_offset = sizeof(struct ipr_dump_header);
+	driver_dump->hdr.status = IPR_DUMP_STATUS_SUCCESS;
+	driver_dump->hdr.os = IPR_DUMP_OS_LINUX;
+	driver_dump->hdr.driver_name = IPR_DUMP_DRIVER_NAME;
+
+	ipr_dump_version_data(ioa_cfg, driver_dump);
+	ipr_dump_location_data(ioa_cfg, driver_dump);
+	ipr_dump_trace_data(ioa_cfg, driver_dump);
+
+	/* Update dump_header */
+	driver_dump->hdr.len += sizeof(struct ipr_dump_entry_header);
+
+	/* IOA Dump entry */
+	ipr_init_dump_entry_hdr(&ioa_dump->hdr);
+	ioa_dump->format = IPR_SDT_FMT2;
+	ioa_dump->hdr.len = 0;
+	ioa_dump->hdr.data_type = IPR_DUMP_DATA_TYPE_BINARY;
+	ioa_dump->hdr.id = IPR_DUMP_IOA_DUMP_ID;
+
+	/* First entries in sdt are actually a list of dump addresses and
+	 lengths to gather the real dump data.  sdt represents the pointer
+	 to the ioa generated dump table.  Dump data will be extracted based
+	 on entries in this table */
+	sdt = &ioa_dump->sdt;
+
+	rc = ipr_get_ldump_data_section(ioa_cfg, start_addr, (u32 *)sdt,
+					sizeof(struct ipr_sdt) / sizeof(u32));
+
+	/* Smart Dump table is ready to use and the first entry is valid */
+	if (rc || (be32_to_cpu(sdt->hdr.state) != IPR_FMT2_SDT_READY_TO_USE)) {
+		dev_err(&ioa_cfg->pdev->dev,
+			"Dump of IOA failed. Dump table not valid: %d, %X.\n",
+			rc, be32_to_cpu(sdt->hdr.state));
+		driver_dump->hdr.status = IPR_DUMP_STATUS_FAILED;
+		ioa_cfg->sdt_state = DUMP_OBTAINED;
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		return;
+	}
+
+	num_entries = be32_to_cpu(sdt->hdr.num_entries_used);
+
+	if (num_entries > IPR_NUM_SDT_ENTRIES)
+		num_entries = IPR_NUM_SDT_ENTRIES;
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+
+	for (i = 0; i < num_entries; i++) {
+		if (ioa_dump->hdr.len > IPR_MAX_IOA_DUMP_SIZE)
+			break;
+
+		if (sdt->entry[i].flags & IPR_SDT_VALID_ENTRY) {
+			sdt_word = be32_to_cpu(sdt->entry[i].bar_str_offset);
+			start_off = sdt_word & IPR_FMT2_MBX_ADDR_MASK;
+			end_off = be32_to_cpu(sdt->entry[i].end_offset);
+
+			if (ipr_sdt_is_fmt2(sdt_word) && sdt_word) {
+				bytes_to_copy = end_off - start_off;
+				if (bytes_to_copy > IPR_MAX_IOA_DUMP_SIZE) {
+					sdt->entry[i].flags &= ~IPR_SDT_VALID_ENTRY;
+					continue;
+				}
+
+				/* Copy data from adapter to driver buffers */
+				bytes_copied = ipr_sdt_copy(ioa_cfg, sdt_word,
+							    bytes_to_copy);
+
+				ioa_dump->hdr.len += bytes_copied;
+
+				if (bytes_copied != bytes_to_copy) {
+					dev_err(&ioa_cfg->pdev->dev, "Dump of IOA completed.\n");
+					driver_dump->hdr.status = IPR_DUMP_STATUS_QUAL_SUCCESS;
+					driver_dump->hdr.len += ioa_dump->hdr.len;
+					ioa_cfg->sdt_state = DUMP_OBTAINED;
+					return;
+				}
+			}
+		}
+	}
+
+	dev_err(&ioa_cfg->pdev->dev, "Dump of IOA completed.\n");
+
+	/* Update dump_header */
+	driver_dump->hdr.len += ioa_dump->hdr.len;
+	wmb();
+	ioa_cfg->sdt_state = DUMP_OBTAINED;
+	LEAVE;
+}
+
+#else
+#define ipr_get_ioa_dump(ioa_cfg, dump) do { } while(0)
+#endif
+
+/**
+ * ipr_worker_thread - Worker thread
+ * @data:		ioa config struct
+ *
+ * Called at task level from a work thread. This function takes care
+ * of adding and removing device from the mid-layer as configuration
+ * changes are detected by the adapter.
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_worker_thread(void *data)
+{
+	unsigned long lock_flags;
+	struct ipr_resource_entry *res;
+	struct scsi_device *sdev;
+	struct ipr_dump *dump;
+	struct ipr_ioa_cfg *ioa_cfg = data;
+	u8 bus, target, lun;
+	int did_work;
+
+	ENTER;
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+
+	if (ioa_cfg->sdt_state == GET_DUMP) {
+		dump = ioa_cfg->dump;
+		if (!dump || !kobject_get(&dump->kobj)) {
+			spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+			return;
+		}
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		ipr_get_ioa_dump(ioa_cfg, dump);
+		kobject_put(&dump->kobj);
+
+		spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+		if (ioa_cfg->sdt_state == DUMP_OBTAINED)
+			ipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		return;
+	}
+
+restart:
+	do {
+		did_work = 0;
+		if (!ioa_cfg->allow_cmds) {
+			spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+			return;
+		}
+
+		list_for_each_entry(res, &ioa_cfg->used_res_q, queue) {
+			if (res->del_from_ml && res->sdev) {
+				did_work = 1;
+				sdev = res->sdev;
+				if (!scsi_device_get(sdev)) {
+					spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+					scsi_remove_device(sdev);
+					scsi_device_put(sdev);
+					spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+				}
+				break;
+			}
+		}
+	} while(did_work);
+
+	list_for_each_entry(res, &ioa_cfg->used_res_q, queue) {
+		if (res->add_to_ml) {
+			bus = res->cfgte.res_addr.bus;
+			target = res->cfgte.res_addr.target;
+			lun = res->cfgte.res_addr.lun;
+			spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+			scsi_add_device(ioa_cfg->host, bus, target, lun);
+			spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+			goto restart;
+		}
+	}
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	LEAVE;
+}
+
+#ifdef CONFIG_SCSI_IPR_TRACE
+/**
+ * ipr_read_trace - Dump the adapter trace
+ * @kobj:		kobject struct
+ * @buf:		buffer
+ * @off:		offset
+ * @count:		buffer size
+ *
+ * Return value:
+ *	number of bytes printed to buffer
+ **/
+static ssize_t ipr_read_trace(struct kobject *kobj, char *buf,
+			      loff_t off, size_t count)
+{
+	struct class_device *cdev = container_of(kobj,struct class_device,kobj);
+	struct Scsi_Host *shost = class_to_shost(cdev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;
+	unsigned long lock_flags = 0;
+	int size = IPR_TRACE_SIZE;
+	char *src = (char *)ioa_cfg->trace;
+
+	if (off > size)
+		return 0;
+	if (off + count > size) {
+		size -= off;
+		count = size;
+	}
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	memcpy(buf, &src[off], count);
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	return count;
+}
+
+static struct bin_attribute ipr_trace_attr = {
+	.attr =	{
+		.name = "trace",
+		.mode = S_IRUGO,
+	},
+	.size = 0,
+	.read = ipr_read_trace,
+};
+#endif
+
+/**
+ * ipr_show_fw_version - Show the firmware version
+ * @class_dev:	class device struct
+ * @buf:		buffer
+ *
+ * Return value:
+ *	number of bytes printed to buffer
+ **/
+static ssize_t ipr_show_fw_version(struct class_device *class_dev, char *buf)
+{
+	struct Scsi_Host *shost = class_to_shost(class_dev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;
+	struct ipr_inquiry_page3 *ucode_vpd = &ioa_cfg->vpd_cbs->page3_data;
+	unsigned long lock_flags = 0;
+	int len;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	len = snprintf(buf, PAGE_SIZE, "%02X%02X%02X%02X\n",
+		       ucode_vpd->major_release, ucode_vpd->card_type,
+		       ucode_vpd->minor_release[0],
+		       ucode_vpd->minor_release[1]);
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	return len;
+}
+
+static struct class_device_attribute ipr_fw_version_attr = {
+	.attr = {
+		.name =		"fw_version",
+		.mode =		S_IRUGO,
+	},
+	.show = ipr_show_fw_version,
+};
+
+/**
+ * ipr_show_log_level - Show the adapter's error logging level
+ * @class_dev:	class device struct
+ * @buf:		buffer
+ *
+ * Return value:
+ * 	number of bytes printed to buffer
+ **/
+static ssize_t ipr_show_log_level(struct class_device *class_dev, char *buf)
+{
+	struct Scsi_Host *shost = class_to_shost(class_dev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;
+	unsigned long lock_flags = 0;
+	int len;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	len = snprintf(buf, PAGE_SIZE, "%d\n", ioa_cfg->log_level);
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	return len;
+}
+
+/**
+ * ipr_store_log_level - Change the adapter's error logging level
+ * @class_dev:	class device struct
+ * @buf:		buffer
+ *
+ * Return value:
+ * 	number of bytes printed to buffer
+ **/
+static ssize_t ipr_store_log_level(struct class_device *class_dev,
+				   const char *buf, size_t count)
+{
+	struct Scsi_Host *shost = class_to_shost(class_dev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;
+	unsigned long lock_flags = 0;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	ioa_cfg->log_level = simple_strtoul(buf, NULL, 10);
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	return strlen(buf);
+}
+
+static struct class_device_attribute ipr_log_level_attr = {
+	.attr = {
+		.name =		"log_level",
+		.mode =		S_IRUGO | S_IWUSR,
+	},
+	.show = ipr_show_log_level,
+	.store = ipr_store_log_level
+};
+
+/**
+ * ipr_store_diagnostics - IOA Diagnostics interface
+ * @class_dev:	class_device struct
+ * @buf:		buffer
+ * @count:		buffer size
+ *
+ * This function will reset the adapter and wait a reasonable
+ * amount of time for any errors that the adapter might log.
+ *
+ * Return value:
+ * 	count on success / other on failure
+ **/
+static ssize_t ipr_store_diagnostics(struct class_device *class_dev,
+				     const char *buf, size_t count)
+{
+	struct Scsi_Host *shost = class_to_shost(class_dev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;
+	unsigned long lock_flags = 0;
+	int rc = count;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EACCES;
+
+	wait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	ioa_cfg->errors_logged = 0;
+	ipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NORMAL);
+
+	if (ioa_cfg->in_reset_reload) {
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		wait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);
+
+		/* Wait for a second for any errors to be logged */
+		schedule_timeout(HZ);
+	} else {
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		return -EIO;
+	}
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	if (ioa_cfg->in_reset_reload || ioa_cfg->errors_logged)
+		rc = -EIO;
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+
+	return rc;
+}
+
+static struct class_device_attribute ipr_diagnostics_attr = {
+	.attr = {
+		.name =		"run_diagnostics",
+		.mode =		S_IWUSR,
+	},
+	.store = ipr_store_diagnostics
+};
+
+/**
+ * ipr_store_reset_adapter - Reset the adapter
+ * @class_dev:	class_device struct
+ * @buf:		buffer
+ * @count:		buffer size
+ *
+ * This function will reset the adapter.
+ *
+ * Return value:
+ * 	count on success / other on failure
+ **/
+static ssize_t ipr_store_reset_adapter(struct class_device *class_dev,
+				       const char *buf, size_t count)
+{
+	struct Scsi_Host *shost = class_to_shost(class_dev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;
+	unsigned long lock_flags;
+	int result = count;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EACCES;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	if (!ioa_cfg->in_reset_reload)
+		ipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NORMAL);
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	wait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);
+
+	return result;
+}
+
+static struct class_device_attribute ipr_ioa_reset_attr = {
+	.attr = {
+		.name =		"reset_host",
+		.mode =		S_IWUSR,
+	},
+	.store = ipr_store_reset_adapter
+};
+
+/**
+ * ipr_alloc_ucode_buffer - Allocates a microcode download buffer
+ * @buf_len:		buffer length
+ *
+ * Allocates a DMA'able buffer in chunks and assembles a scatter/gather
+ * list to use for microcode download
+ *
+ * Return value:
+ * 	pointer to sglist / NULL on failure
+ **/
+static struct ipr_sglist *ipr_alloc_ucode_buffer(int buf_len)
+{
+	int sg_size, order, bsize_elem, num_elem, i, j;
+	struct ipr_sglist *sglist;
+	struct scatterlist *scatterlist;
+	struct page *page;
+
+	/* Get the minimum size per scatter/gather element */
+	sg_size = buf_len / (IPR_MAX_SGLIST - 1);
+
+	/* Get the actual size per element */
+	order = get_order(sg_size);
+
+	/* Determine the actual number of bytes per element */
+	bsize_elem = PAGE_SIZE * (1 << order);
+
+	/* Determine the actual number of sg entries needed */
+	if (buf_len % bsize_elem)
+		num_elem = (buf_len / bsize_elem) + 1;
+	else
+		num_elem = buf_len / bsize_elem;
+
+	/* Allocate a scatter/gather list for the DMA */
+	sglist = kmalloc(sizeof(struct ipr_sglist) +
+			 (sizeof(struct scatterlist) * (num_elem - 1)),
+			 GFP_KERNEL);
+
+	if (sglist == NULL) {
+		ipr_trace;
+		return NULL;
+	}
+
+	memset(sglist, 0, sizeof(struct ipr_sglist) +
+	       (sizeof(struct scatterlist) * (num_elem - 1)));
+
+	scatterlist = sglist->scatterlist;
+
+	sglist->order = order;
+	sglist->num_sg = num_elem;
+
+	/* Allocate a bunch of sg elements */
+	for (i = 0; i < num_elem; i++) {
+		page = alloc_pages(GFP_KERNEL, order);
+		if (!page) {
+			ipr_trace;
+
+			/* Free up what we already allocated */
+			for (j = i - 1; j >= 0; j--)
+				__free_pages(scatterlist[j].page, order);
+			kfree(sglist);
+			return NULL;
+		}
+
+		scatterlist[i].page = page;
+	}
+
+	return sglist;
+}
+
+/**
+ * ipr_free_ucode_buffer - Frees a microcode download buffer
+ * @p_dnld:		scatter/gather list pointer
+ *
+ * Free a DMA'able ucode download buffer previously allocated with
+ * ipr_alloc_ucode_buffer
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_free_ucode_buffer(struct ipr_sglist *sglist)
+{
+	int i;
+
+	for (i = 0; i < sglist->num_sg; i++)
+		__free_pages(sglist->scatterlist[i].page, sglist->order);
+
+	kfree(sglist);
+}
+
+/**
+ * ipr_copy_ucode_buffer - Copy user buffer to kernel buffer
+ * @sglist:		scatter/gather list pointer
+ * @buffer:		buffer pointer
+ * @len:		buffer length
+ *
+ * Copy a microcode image from a user buffer into a buffer allocated by
+ * ipr_alloc_ucode_buffer
+ *
+ * Return value:
+ * 	0 on success / other on failure
+ **/
+static int ipr_copy_ucode_buffer(struct ipr_sglist *sglist,
+				 u8 *buffer, u32 len)
+{
+	int bsize_elem, i, result = 0;
+	struct scatterlist *scatterlist;
+	void *kaddr;
+
+	/* Determine the actual number of bytes per element */
+	bsize_elem = PAGE_SIZE * (1 << sglist->order);
+
+	scatterlist = sglist->scatterlist;
+
+	for (i = 0; i < (len / bsize_elem); i++, buffer += bsize_elem) {
+		kaddr = kmap(scatterlist[i].page);
+		memcpy(kaddr, buffer, bsize_elem);
+		kunmap(scatterlist[i].page);
+
+		scatterlist[i].length = bsize_elem;
+
+		if (result != 0) {
+			ipr_trace;
+			return result;
+		}
+	}
+
+	if (len % bsize_elem) {
+		kaddr = kmap(scatterlist[i].page);
+		memcpy(kaddr, buffer, len % bsize_elem);
+		kunmap(scatterlist[i].page);
+
+		scatterlist[i].length = len % bsize_elem;
+	}
+
+	sglist->buffer_len = len;
+	return result;
+}
+
+/**
+ * ipr_map_ucode_buffer - Map a microcode download buffer
+ * @ipr_cmd:	ipr command struct
+ * @sglist:		scatter/gather list
+ * @len:		total length of download buffer
+ *
+ * Maps a microcode download scatter/gather list for DMA and
+ * builds the IOADL.
+ *
+ * Return value:
+ * 	0 on success / -EIO on failure
+ **/
+static int ipr_map_ucode_buffer(struct ipr_cmnd *ipr_cmd,
+				struct ipr_sglist *sglist, int len)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;
+	struct ipr_ioadl_desc *ioadl = ipr_cmd->ioadl;
+	struct scatterlist *scatterlist = sglist->scatterlist;
+	int i;
+
+	ipr_cmd->dma_use_sg = pci_map_sg(ioa_cfg->pdev, scatterlist,
+					 sglist->num_sg, DMA_TO_DEVICE);
+
+	ioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;
+	ioarcb->write_data_transfer_length = cpu_to_be32(len);
+	ioarcb->write_ioadl_len =
+		cpu_to_be32(sizeof(struct ipr_ioadl_desc) * ipr_cmd->dma_use_sg);
+
+	for (i = 0; i < ipr_cmd->dma_use_sg; i++) {
+		ioadl[i].flags_and_data_len =
+			cpu_to_be32(IPR_IOADL_FLAGS_WRITE | sg_dma_len(&scatterlist[i]));
+		ioadl[i].address =
+			cpu_to_be32(sg_dma_address(&scatterlist[i]));
+	}
+
+	if (likely(ipr_cmd->dma_use_sg)) {
+		ioadl[i-1].flags_and_data_len |=
+			cpu_to_be32(IPR_IOADL_FLAGS_LAST);
+	}
+	else {
+		dev_err(&ioa_cfg->pdev->dev, "pci_map_sg failed!\n");
+		return -EIO;
+	}
+
+	return 0;
+}
+
+/**
+ * ipr_store_update_fw - Update the firmware on the adapter
+ * @class_dev:	class_device struct
+ * @buf:		buffer
+ * @count:		buffer size
+ *
+ * This function will update the firmware on the adapter.
+ *
+ * Return value:
+ * 	count on success / other on failure
+ **/
+static ssize_t ipr_store_update_fw(struct class_device *class_dev,
+				       const char *buf, size_t count)
+{
+	struct Scsi_Host *shost = class_to_shost(class_dev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;
+	struct ipr_ucode_image_header *image_hdr;
+	const struct firmware *fw_entry;
+	struct ipr_sglist *sglist;
+	unsigned long lock_flags;
+	char fname[100];
+	char *src;
+	int len, result, dnld_size;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EACCES;
+
+	len = snprintf(fname, 99, "%s", buf);
+	fname[len-1] = '\0';
+
+	if(request_firmware(&fw_entry, fname, &ioa_cfg->pdev->dev)) {
+		dev_err(&ioa_cfg->pdev->dev, "Firmware file %s not found\n", fname);
+		return -EIO;
+	}
+
+	image_hdr = (struct ipr_ucode_image_header *)fw_entry->data;
+
+	if (be32_to_cpu(image_hdr->header_length) > fw_entry->size ||
+	    ioa_cfg->vpd_cbs->page3_data.card_type != image_hdr->card_type) {
+		dev_err(&ioa_cfg->pdev->dev, "Invalid microcode buffer\n");
+		release_firmware(fw_entry);
+		return -EINVAL;
+	}
+
+	src = (u8 *)image_hdr + be32_to_cpu(image_hdr->header_length);
+	dnld_size = fw_entry->size - be32_to_cpu(image_hdr->header_length);
+	sglist = ipr_alloc_ucode_buffer(dnld_size);
+
+	if (!sglist) {
+		dev_err(&ioa_cfg->pdev->dev, "Microcode buffer allocation failed\n");
+		release_firmware(fw_entry);
+		return -ENOMEM;
+	}
+
+	result = ipr_copy_ucode_buffer(sglist, src, dnld_size);
+
+	if (result) {
+		dev_err(&ioa_cfg->pdev->dev,
+			"Microcode buffer copy to DMA buffer failed\n");
+		ipr_free_ucode_buffer(sglist);
+		release_firmware(fw_entry);
+		return result;
+	}
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+
+	if (ioa_cfg->ucode_sglist) {
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		dev_err(&ioa_cfg->pdev->dev,
+			"Microcode download already in progress\n");
+		ipr_free_ucode_buffer(sglist);
+		release_firmware(fw_entry);
+		return -EIO;
+	}
+
+	ioa_cfg->ucode_sglist = sglist;
+	ipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NORMAL);
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	wait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	ioa_cfg->ucode_sglist = NULL;
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+
+	ipr_free_ucode_buffer(sglist);
+	release_firmware(fw_entry);
+
+	return count;
+}
+
+static struct class_device_attribute ipr_update_fw_attr = {
+	.attr = {
+		.name =		"update_fw",
+		.mode =		S_IWUSR,
+	},
+	.store = ipr_store_update_fw
+};
+
+static struct class_device_attribute *ipr_ioa_attrs[] = {
+	&ipr_fw_version_attr,
+	&ipr_log_level_attr,
+	&ipr_diagnostics_attr,
+	&ipr_ioa_reset_attr,
+	&ipr_update_fw_attr,
+	NULL,
+};
+
+#ifdef CONFIG_SCSI_IPR_DUMP
+/**
+ * ipr_read_dump - Dump the adapter
+ * @kobj:		kobject struct
+ * @buf:		buffer
+ * @off:		offset
+ * @count:		buffer size
+ *
+ * Return value:
+ *	number of bytes printed to buffer
+ **/
+static ssize_t ipr_read_dump(struct kobject *kobj, char *buf,
+			      loff_t off, size_t count)
+{
+	struct class_device *cdev = container_of(kobj,struct class_device,kobj);
+	struct Scsi_Host *shost = class_to_shost(cdev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;
+	struct ipr_dump *dump;
+	unsigned long lock_flags = 0;
+	char *src;
+	int len;
+	size_t rc = count;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EACCES;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	dump = ioa_cfg->dump;
+
+	if (ioa_cfg->sdt_state != DUMP_OBTAINED || !dump || !kobject_get(&dump->kobj)) {
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		return 0;
+	}
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+
+	if (off > dump->driver_dump.hdr.len) {
+		kobject_put(&dump->kobj);
+		return 0;
+	}
+
+	if (off + count > dump->driver_dump.hdr.len) {
+		count = dump->driver_dump.hdr.len - off;
+		rc = count;
+	}
+
+	if (count && off < sizeof(dump->driver_dump)) {
+		if (off + count > sizeof(dump->driver_dump))
+			len = sizeof(dump->driver_dump) - off;
+		else
+			len = count;
+		src = (u8 *)&dump->driver_dump + off;
+		memcpy(buf, src, len);
+		buf += len;
+		off += len;
+		count -= len;
+	}
+
+	off -= sizeof(dump->driver_dump);
+
+	if (count && off < offsetof(struct ipr_ioa_dump, ioa_data)) {
+		if (off + count > offsetof(struct ipr_ioa_dump, ioa_data))
+			len = offsetof(struct ipr_ioa_dump, ioa_data) - off;
+		else
+			len = count;
+		src = (u8 *)&dump->ioa_dump + off;
+		memcpy(buf, src, len);
+		buf += len;
+		off += len;
+		count -= len;
+	}
+
+	off -= offsetof(struct ipr_ioa_dump, ioa_data);
+
+	while (count) {
+		if ((off & PAGE_MASK) != ((off + count) & PAGE_MASK))
+			len = PAGE_ALIGN(off) - off;
+		else
+			len = count;
+		src = (u8 *)dump->ioa_dump.ioa_data[(off & PAGE_MASK) >> PAGE_SHIFT];
+		src += off & ~PAGE_MASK;
+		memcpy(buf, src, len);
+		buf += len;
+		off += len;
+		count -= len;
+	}
+
+	kobject_put(&dump->kobj);
+	return rc;
+}
+
+/**
+ * ipr_release_dump - Free adapter dump memory
+ * @kobj:	kobject struct
+ *
+ * Return value:
+ *	nothing
+ **/
+static void ipr_release_dump(struct kobject *kobj)
+{
+	struct ipr_dump *dump = container_of(kobj,struct ipr_dump,kobj);
+	struct ipr_ioa_cfg *ioa_cfg = dump->ioa_cfg;
+	unsigned long lock_flags = 0;
+	int i;
+
+	ENTER;
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	ioa_cfg->dump = NULL;
+	ioa_cfg->sdt_state = INACTIVE;
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+
+	for (i = 0; i < dump->ioa_dump.next_page_index; i++)
+		free_page((unsigned long) dump->ioa_dump.ioa_data[i]);
+
+	kfree(dump);
+	LEAVE;
+}
+
+static struct kobj_type ipr_dump_kobj_type = {
+	.release = ipr_release_dump,
+};
+
+/**
+ * ipr_alloc_dump - Prepare for adapter dump
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ *	0 on success / other on failure
+ **/
+static int ipr_alloc_dump(struct ipr_ioa_cfg *ioa_cfg)
+{
+	struct ipr_dump *dump;
+	unsigned long lock_flags = 0;
+
+	ENTER;
+	dump = kmalloc(sizeof(struct ipr_dump), GFP_KERNEL);
+
+	if (!dump) {
+		ipr_err("Dump memory allocation failed\n");
+		return -ENOMEM;
+	}
+
+	memset(dump, 0, sizeof(struct ipr_dump));
+	kobject_init(&dump->kobj);
+	dump->kobj.ktype = &ipr_dump_kobj_type;
+	dump->ioa_cfg = ioa_cfg;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+
+	if (INACTIVE != ioa_cfg->sdt_state) {
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		kfree(dump);
+		return 0;
+	}
+
+	ioa_cfg->dump = dump;
+	ioa_cfg->sdt_state = WAIT_FOR_DUMP;
+	if (ioa_cfg->ioa_is_dead && !ioa_cfg->dump_taken) {
+		ioa_cfg->dump_taken = 1;
+		schedule_work(&ioa_cfg->work_q);
+	}
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+
+	LEAVE;
+	return 0;
+}
+
+/**
+ * ipr_free_dump - Free adapter dump memory
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ *	0 on success / other on failure
+ **/
+static int ipr_free_dump(struct ipr_ioa_cfg *ioa_cfg)
+{
+	struct ipr_dump *dump;
+	unsigned long lock_flags = 0;
+
+	ENTER;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	dump = ioa_cfg->dump;
+	if (!dump) {
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		return 0;
+	}
+
+	ioa_cfg->dump = NULL;
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+
+	kobject_put(&dump->kobj);
+
+	LEAVE;
+	return 0;
+}
+
+/**
+ * ipr_write_dump - Setup dump state of adapter
+ * @kobj:		kobject struct
+ * @buf:		buffer
+ * @off:		offset
+ * @count:		buffer size
+ *
+ * Return value:
+ *	number of bytes printed to buffer
+ **/
+static ssize_t ipr_write_dump(struct kobject *kobj, char *buf,
+			      loff_t off, size_t count)
+{
+	struct class_device *cdev = container_of(kobj,struct class_device,kobj);
+	struct Scsi_Host *shost = class_to_shost(cdev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;
+	int rc;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EACCES;
+
+	if (buf[0] == '1')
+		rc = ipr_alloc_dump(ioa_cfg);
+	else if (buf[0] == '0')
+		rc = ipr_free_dump(ioa_cfg);
+	else
+		return -EINVAL;
+
+	if (rc)
+		return rc;
+	else
+		return count;
+}
+
+static struct bin_attribute ipr_dump_attr = {
+	.attr =	{
+		.name = "dump",
+		.mode = S_IRUSR | S_IWUSR,
+	},
+	.size = 0,
+	.read = ipr_read_dump,
+	.write = ipr_write_dump
+};
+#else
+static int ipr_free_dump(struct ipr_ioa_cfg *ioa_cfg) { return 0; };
+#endif
+
+/**
+ * ipr_store_queue_depth - Change the device's queue depth
+ * @dev:	device struct
+ * @buf:	buffer
+ *
+ * Return value:
+ * 	number of bytes printed to buffer
+ **/
+static ssize_t ipr_store_queue_depth(struct device *dev,
+				    const char *buf, size_t count)
+{
+	struct scsi_device *sdev = to_scsi_device(dev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)sdev->host->hostdata;
+	struct ipr_resource_entry *res;
+	int qdepth = simple_strtoul(buf, NULL, 10);
+	int tagged = 0;
+	unsigned long lock_flags = 0;
+	ssize_t len = -ENXIO;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	res = (struct ipr_resource_entry *)sdev->hostdata;
+	if (res) {
+		res->qdepth = qdepth;
+
+		if (ipr_is_gscsi(res) && res->tcq_active)
+			tagged = MSG_ORDERED_TAG;
+
+		len = strlen(buf);
+	}
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	scsi_adjust_queue_depth(sdev, tagged, qdepth);
+	return len;
+}
+
+static struct device_attribute ipr_queue_depth_attr = {
+	.attr = {
+		.name = 	"queue_depth",
+		.mode =		S_IRUSR | S_IWUSR,
+	},
+	.store = ipr_store_queue_depth
+};
+
+/**
+ * ipr_show_tcq_enable - Show if the device is enabled for tcqing
+ * @dev:	device struct
+ * @buf:	buffer
+ *
+ * Return value:
+ * 	number of bytes printed to buffer
+ **/
+static ssize_t ipr_show_tcq_enable(struct device *dev, char *buf)
+{
+	struct scsi_device *sdev = to_scsi_device(dev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)sdev->host->hostdata;
+	struct ipr_resource_entry *res;
+	unsigned long lock_flags = 0;
+	ssize_t len = -ENXIO;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	res = (struct ipr_resource_entry *)sdev->hostdata;
+	if (res)
+		len = snprintf(buf, PAGE_SIZE, "%d\n", res->tcq_active);
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	return len;
+}
+
+/**
+ * ipr_store_tcq_enable - Change the device's TCQing state
+ * @dev:	device struct
+ * @buf:	buffer
+ *
+ * Return value:
+ * 	number of bytes printed to buffer
+ **/
+static ssize_t ipr_store_tcq_enable(struct device *dev,
+				    const char *buf, size_t count)
+{
+	struct scsi_device *sdev = to_scsi_device(dev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)sdev->host->hostdata;
+	struct ipr_resource_entry *res;
+	unsigned long lock_flags = 0;
+	int tcq_active = simple_strtoul(buf, NULL, 10);
+	int qdepth = IPR_MAX_CMD_PER_LUN;
+	int tagged = 0;
+	ssize_t len = -ENXIO;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+
+	res = (struct ipr_resource_entry *)sdev->hostdata;
+
+	if (res) {
+		res->tcq_active = 0;
+		qdepth = res->qdepth;
+
+		if (ipr_is_gscsi(res) && sdev->tagged_supported) {
+			if (tcq_active) {
+				tagged = MSG_ORDERED_TAG;
+				res->tcq_active = 1;
+			}
+
+			len = strlen(buf);
+		} else if (tcq_active) {
+			len = -EINVAL;
+		}
+	}
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	scsi_adjust_queue_depth(sdev, tagged, qdepth);
+	return len;
+}
+
+static struct device_attribute ipr_tcqing_attr = {
+	.attr = {
+		.name = 	"tcq_enable",
+		.mode =		S_IRUSR | S_IWUSR,
+	},
+	.store = ipr_store_tcq_enable,
+	.show = ipr_show_tcq_enable
+};
+
+/**
+ * ipr_show_adapter_handle - Show the adapter's resource handle for this device
+ * @dev:	device struct
+ * @buf:	buffer
+ *
+ * Return value:
+ * 	number of bytes printed to buffer
+ **/
+static ssize_t ipr_show_adapter_handle(struct device *dev, char *buf)
+{
+	struct scsi_device *sdev = to_scsi_device(dev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)sdev->host->hostdata;
+	struct ipr_resource_entry *res;
+	unsigned long lock_flags = 0;
+	ssize_t len = -ENXIO;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	res = (struct ipr_resource_entry *)sdev->hostdata;
+	if (res)
+		len = snprintf(buf, PAGE_SIZE, "%08X\n", res->cfgte.res_handle);
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	return len;
+}
+
+static struct device_attribute ipr_adapter_handle_attr = {
+	.attr = {
+		.name = 	"adapter_handle",
+		.mode =		S_IRUSR,
+	},
+	.show = ipr_show_adapter_handle
+};
+
+static struct device_attribute *ipr_dev_attrs[] = {
+	&ipr_queue_depth_attr,
+	&ipr_tcqing_attr,
+	&ipr_adapter_handle_attr,
+	NULL,
+};
+
+/**
+ * ipr_biosparam - Return the HSC mapping
+ * @sdev:			scsi device struct
+ * @block_device:	block device pointer
+ * @capacity:		capacity of the device
+ * @parm:			Array containing returned HSC values.
+ *
+ * This function generates the HSC parms that fdisk uses.
+ * We want to make sure we return something that places partitions
+ * on 4k boundaries for best performance with the IOA.
+ *
+ * Return value:
+ * 	0 on success
+ **/
+static int ipr_biosparam(struct scsi_device *sdev,
+			 struct block_device *block_device,
+			 sector_t capacity, int *parm)
+{
+	int heads, sectors, cylinders;
+
+	heads = 128;
+	sectors = 32;
+
+	cylinders = capacity;
+	sector_div(cylinders, (128 * 32));
+
+	/* return result */
+	parm[0] = heads;
+	parm[1] = sectors;
+	parm[2] = cylinders;
+
+	return 0;
+}
+
+/**
+ * ipr_slave_destroy - Unconfigure a SCSI device
+ * @sdev:	scsi device struct
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_slave_destroy(struct scsi_device *sdev)
+{
+	struct ipr_resource_entry *res;
+	struct ipr_ioa_cfg *ioa_cfg;
+	unsigned long lock_flags = 0;
+
+	ioa_cfg = (struct ipr_ioa_cfg *) sdev->host->hostdata;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	res = (struct ipr_resource_entry *) sdev->hostdata;
+	if (res) {
+		if (res->del_from_ml)
+			list_move_tail(&res->queue, &ioa_cfg->free_res_q);
+
+		sdev->hostdata = NULL;
+		res->sdev = NULL;
+	}
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+}
+
+/**
+ * ipr_slave_configure - Configure a SCSI device
+ * @sdev:	scsi device struct
+ *
+ * This function configures the specified scsi device.
+ *
+ * Return value:
+ * 	0 on success
+ **/
+static int ipr_slave_configure(struct scsi_device *sdev)
+{
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *) sdev->host->hostdata;
+	struct ipr_resource_entry *res;
+	unsigned long lock_flags = 0;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	res = sdev->hostdata;
+	if (res) {
+		if (ipr_is_af_dasd_device(res))
+			sdev->type = TYPE_RAID;
+		else if (ipr_is_vset_device(res))
+			sdev->allow_restart = 1;
+		if (ipr_is_af_dasd_device(res) || ipr_is_ioa_resource(res))
+			sdev->scsi_level = 4;
+
+		scsi_adjust_queue_depth(sdev, 0, res->qdepth);
+	}
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	return 0;
+}
+
+/**
+ * ipr_slave_alloc - Prepare for commands to a device.
+ * @sdev:	scsi device struct
+ *
+ * This function saves a pointer to the resource entry
+ * in the scsi device struct if the device exists. We
+ * can then use this pointer in ipr_queuecommand when
+ * handling new commands.
+ *
+ * Return value:
+ * 	0 on success
+ **/
+static int ipr_slave_alloc(struct scsi_device *sdev)
+{
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *) sdev->host->hostdata;
+	struct ipr_resource_entry *res;
+	unsigned long lock_flags;
+
+	sdev->hostdata = NULL;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+
+	list_for_each_entry(res, &ioa_cfg->used_res_q, queue) {
+		if ((res->cfgte.res_addr.bus == sdev->channel) &&
+		    (res->cfgte.res_addr.target == sdev->id) &&
+		    (res->cfgte.res_addr.lun == sdev->lun)) {
+			res->sdev = sdev;
+			res->add_to_ml = 0;
+			sdev->hostdata = res;
+			res->needs_sync_complete = 1;
+			break;
+		}
+	}
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+
+	return 0;
+}
+
+/**
+ * ipr_eh_host_reset - Reset the host adapter
+ * @scsi_cmd:	scsi command struct
+ *
+ * Return value:
+ * 	SUCCESS / FAILED
+ **/
+static int ipr_eh_host_reset(struct scsi_cmnd * scsi_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg;
+	int rc;
+
+	ENTER;
+	ioa_cfg = (struct ipr_ioa_cfg *) scsi_cmd->device->host->hostdata;
+
+	dev_err(&ioa_cfg->pdev->dev,
+		"Adapter being reset as a result of error recovery.\n");
+
+	if (WAIT_FOR_DUMP == ioa_cfg->sdt_state)
+		ioa_cfg->sdt_state = GET_DUMP;
+
+	rc = ipr_reset_reload(ioa_cfg, IPR_SHUTDOWN_ABBREV);
+
+	LEAVE;
+	return rc;
+}
+
+/**
+ * ipr_eh_dev_reset - Reset the device
+ * @scsi_cmd:	scsi command struct
+ *
+ * This function issues a device reset to the affected device.
+ * A LUN reset will be sent to the device first. If that does
+ * not work, a target reset will be sent.
+ *
+ * Return value:
+ *	SUCCESS / FAILED
+ **/
+static int ipr_eh_dev_reset(struct scsi_cmnd * scsi_cmd)
+{
+	struct ipr_cmnd *ipr_cmd;
+	struct ipr_ioa_cfg *ioa_cfg;
+	struct ipr_resource_entry *res;
+	struct ipr_cmd_pkt *cmd_pkt;
+	u32 ioasc;
+
+	ENTER;
+	ioa_cfg = (struct ipr_ioa_cfg *) scsi_cmd->device->host->hostdata;
+	res = scsi_cmd->device->hostdata;
+
+	if (!ipr_is_gscsi(res) && !ipr_is_vset_device(res))
+		return FAILED;
+
+	/*
+	 * If we are currently going through reset/reload, return failed. This will force the
+	 * mid-layer to call ipr_eh_host_reset, which will then go to sleep and wait for the
+	 * reset to complete
+	 */
+	if (ioa_cfg->in_reset_reload)
+		return FAILED;
+	if (ioa_cfg->ioa_is_dead)
+		return FAILED;
+
+	list_for_each_entry(ipr_cmd, &ioa_cfg->pending_q, queue) {
+		if (ipr_cmd->ioarcb.res_handle == res->cfgte.res_handle) {
+			if (ipr_cmd->scsi_cmd)
+				ipr_cmd->done = ipr_scsi_eh_done;
+		}
+	}
+
+	res->resetting_device = 1;
+
+	ipr_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);
+
+	ipr_cmd->ioarcb.res_handle = res->cfgte.res_handle;
+	cmd_pkt = &ipr_cmd->ioarcb.cmd_pkt;
+	cmd_pkt->request_type = IPR_RQTYPE_IOACMD;
+	cmd_pkt->flags_hi |= IPR_FLAGS_HI_SYNC_COMPLETE;
+	cmd_pkt->cdb[0] = IPR_RESET_DEVICE;
+	cmd_pkt->cdb[2] = IPR_RESET_TYPE_SELECT | IPR_LUN_RESET | IPR_TARGET_RESET;
+
+	ipr_send_blocking_cmd(ipr_cmd, ipr_timeout, IPR_DEVICE_RESET_TIMEOUT);
+
+	ioasc = be32_to_cpu(ipr_cmd->ioasa.ioasc);
+
+	res->resetting_device = 0;
+
+	list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+
+	LEAVE;
+	return (IPR_IOASC_SENSE_KEY(ioasc) ? FAILED : SUCCESS);
+}
+
+/**
+ * ipr_cancel_op - Cancel specified op
+ * @scsi_cmd:	scsi command struct
+ *
+ * This function cancels specified op.
+ *
+ * Return value:
+ *	SUCCESS / FAILED
+ **/
+static int ipr_cancel_op(struct scsi_cmnd * scsi_cmd)
+{
+	struct ipr_cmnd *ipr_cmd;
+	struct ipr_ioa_cfg *ioa_cfg;
+	struct ipr_resource_entry *res;
+	struct ipr_cmd_pkt *cmd_pkt;
+	u32 ioasc, ioarcb_addr;
+	int op_found = 0;
+
+	ENTER;
+	ioa_cfg = (struct ipr_ioa_cfg *)scsi_cmd->device->host->hostdata;
+	res = scsi_cmd->device->hostdata;
+
+	if (!res || (!ipr_is_gscsi(res) && !ipr_is_vset_device(res)))
+		return FAILED;
+
+	list_for_each_entry(ipr_cmd, &ioa_cfg->pending_q, queue) {
+		if (ipr_cmd->scsi_cmd == scsi_cmd) {
+			ipr_cmd->done = ipr_scsi_eh_done;
+			op_found = 1;
+			break;
+		}
+	}
+
+	if (!op_found)
+		return SUCCESS;
+
+	ioarcb_addr = be32_to_cpu(ipr_cmd->ioarcb.ioarcb_host_pci_addr);
+
+	ipr_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);
+	ipr_cmd->ioarcb.res_handle = res->cfgte.res_handle;
+	cmd_pkt = &ipr_cmd->ioarcb.cmd_pkt;
+	cmd_pkt->request_type = IPR_RQTYPE_IOACMD;
+	cmd_pkt->cdb[0] = IPR_ABORT_TASK;
+	cmd_pkt->cdb[2] = (ioarcb_addr >> 24) & 0xff;
+	cmd_pkt->cdb[3] = (ioarcb_addr >> 16) & 0xff;
+	cmd_pkt->cdb[4] = (ioarcb_addr >> 8) & 0xff;
+	cmd_pkt->cdb[5] = ioarcb_addr & 0xff;
+
+	ipr_send_blocking_cmd(ipr_cmd, ipr_timeout, IPR_ABORT_TASK_TIMEOUT);
+	ioasc = be32_to_cpu(ipr_cmd->ioasa.ioasc);
+	list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+	res->needs_sync_complete = 1;
+
+	LEAVE;
+	return (IPR_IOASC_SENSE_KEY(ioasc) ? FAILED : SUCCESS);
+}
+
+/**
+ * ipr_eh_abort - Abort a single op
+ * @scsi_cmd:	scsi command struct
+ *
+ * Return value:
+ * 	SUCCESS / FAILED
+ **/
+static int ipr_eh_abort(struct scsi_cmnd * scsi_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg;
+
+	ENTER;
+	ioa_cfg = (struct ipr_ioa_cfg *) scsi_cmd->device->host->hostdata;
+
+	/* If we are currently going through reset/reload, return failed. This will force the
+	   mid-layer to call ipr_eh_host_reset, which will then go to sleep and wait for the
+	   reset to complete */
+	if (ioa_cfg->in_reset_reload)
+		return FAILED;
+	if (ioa_cfg->ioa_is_dead)
+		return FAILED;
+	if (!scsi_cmd->device->hostdata)
+		return FAILED;
+
+	LEAVE;
+	return ipr_cancel_op(scsi_cmd);
+}
+
+/**
+ * ipr_handle_other_interrupt - Handle "other" interrupts
+ * @ioa_cfg:	ioa config struct
+ * @int_reg:	interrupt register
+ *
+ * Return value:
+ * 	IRQ_NONE / IRQ_HANDLED
+ **/
+static irqreturn_t ipr_handle_other_interrupt(struct ipr_ioa_cfg *ioa_cfg,
+					      volatile u32 int_reg)
+{
+	irqreturn_t rc = IRQ_HANDLED;
+
+	if (int_reg & IPR_PCII_IOA_TRANS_TO_OPER) {
+		/* Mask the interrupt */
+		writel(IPR_PCII_IOA_TRANS_TO_OPER, ioa_cfg->regs.set_interrupt_mask_reg);
+
+		/* Clear the interrupt */
+		writel(IPR_PCII_IOA_TRANS_TO_OPER, ioa_cfg->regs.clr_interrupt_reg);
+		int_reg = readl(ioa_cfg->regs.sense_interrupt_reg);
+
+		del_timer(&ioa_cfg->reset_cmd->timer);
+		ipr_reset_ioa_job(ioa_cfg->reset_cmd);
+	} else {
+		if (int_reg & IPR_PCII_IOA_UNIT_CHECKED)
+			ioa_cfg->ioa_unit_checked = 1;
+		else
+			dev_err(&ioa_cfg->pdev->dev,
+				"Permanent IOA failure. 0x%08X\n", int_reg);
+
+		if (WAIT_FOR_DUMP == ioa_cfg->sdt_state)
+			ioa_cfg->sdt_state = GET_DUMP;
+
+		ipr_mask_and_clear_all_interrupts(ioa_cfg);
+		ipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);
+	}
+
+	return rc;
+}
+
+/**
+ * ipr_isr - Interrupt service routine
+ * @irq:	irq number
+ * @devp:	pointer to ioa config struct
+ * @regs:	pt_regs struct
+ *
+ * Return value:
+ * 	IRQ_NONE / IRQ_HANDLED
+ **/
+static irqreturn_t ipr_isr(int irq, void *devp, struct pt_regs *regs)
+{
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)devp;
+	unsigned long lock_flags = 0;
+	volatile u32 int_reg, int_mask_reg;
+	u32 ioasc;
+	u16 cmd_index;
+	struct ipr_cmnd *ipr_cmd;
+	irqreturn_t rc = IRQ_NONE;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+
+	/* If interrupts are disabled, ignore the interrupt */
+	if (!ioa_cfg->allow_interrupts) {
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		return IRQ_NONE;
+	}
+
+	int_mask_reg = readl(ioa_cfg->regs.sense_interrupt_mask_reg);
+	int_reg = readl(ioa_cfg->regs.sense_interrupt_reg) & ~int_mask_reg;
+
+	/* If an interrupt on the adapter did not occur, ignore it */
+	if (unlikely((int_reg & IPR_PCII_OPER_INTERRUPTS) == 0)) {
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		return IRQ_NONE;
+	}
+
+	while (1) {
+		ipr_cmd = NULL;
+
+		while ((be32_to_cpu(*ioa_cfg->hrrq_curr) & IPR_HRRQ_TOGGLE_BIT) ==
+		       ioa_cfg->toggle_bit) {
+
+			cmd_index = (be32_to_cpu(*ioa_cfg->hrrq_curr) &
+				     IPR_HRRQ_REQ_RESP_HANDLE_MASK) >> IPR_HRRQ_REQ_RESP_HANDLE_SHIFT;
+
+			if (unlikely(cmd_index >= IPR_NUM_CMD_BLKS)) {
+				ioa_cfg->errors_logged++;
+				dev_err(&ioa_cfg->pdev->dev, "Invalid response handle from IOA\n");
+
+				if (WAIT_FOR_DUMP == ioa_cfg->sdt_state)
+					ioa_cfg->sdt_state = GET_DUMP;
+
+				ipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);
+				spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+				return IRQ_HANDLED;
+			}
+
+			ipr_cmd = ioa_cfg->ipr_cmnd_list[cmd_index];
+
+			ioasc = be32_to_cpu(ipr_cmd->ioasa.ioasc);
+
+			ipr_trc_hook(ipr_cmd, IPR_TRACE_FINISH, ioasc);
+
+			list_del(&ipr_cmd->queue);
+			del_timer(&ipr_cmd->timer);
+			ipr_cmd->done(ipr_cmd);
+
+			rc = IRQ_HANDLED;
+
+			if (ioa_cfg->hrrq_curr < ioa_cfg->hrrq_end) {
+				ioa_cfg->hrrq_curr++;
+			} else {
+				ioa_cfg->hrrq_curr = ioa_cfg->hrrq_start;
+				ioa_cfg->toggle_bit ^= 1u;
+			}
+		}
+
+		if (ipr_cmd != NULL) {
+			/* Clear the PCI interrupt */
+			writel(IPR_PCII_HRRQ_UPDATED, ioa_cfg->regs.clr_interrupt_reg);
+			int_reg = readl(ioa_cfg->regs.sense_interrupt_reg) & ~int_mask_reg;
+		} else
+			break;
+	}
+
+	if (unlikely(rc == IRQ_NONE))
+		rc = ipr_handle_other_interrupt(ioa_cfg, int_reg);
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	return rc;
+}
+
+/**
+ * ipr_build_ioadl - Build a scatter/gather list and map the buffer
+ * @ioa_cfg:	ioa config struct
+ * @ipr_cmd:	ipr command struct
+ *
+ * Return value:
+ * 	0 on success / -1 on failure
+ **/
+static int ipr_build_ioadl(struct ipr_ioa_cfg *ioa_cfg,
+			   struct ipr_cmnd *ipr_cmd)
+{
+	int i;
+	struct scatterlist *sglist;
+	u32 length;
+	u32 ioadl_flags = 0;
+	struct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;
+	struct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;
+	struct ipr_ioadl_desc *ioadl = ipr_cmd->ioadl;
+
+	length = scsi_cmd->request_bufflen;
+
+	if (length == 0)
+		return 0;
+
+	if (scsi_cmd->use_sg) {
+		ipr_cmd->dma_use_sg = pci_map_sg(ioa_cfg->pdev,
+						 scsi_cmd->request_buffer,
+						 scsi_cmd->use_sg,
+						 scsi_cmd->sc_data_direction);
+
+		if (scsi_cmd->sc_data_direction == DMA_TO_DEVICE) {
+			ioadl_flags = IPR_IOADL_FLAGS_WRITE;
+			ioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;
+			ioarcb->write_data_transfer_length = cpu_to_be32(length);
+			ioarcb->write_ioadl_len =
+				cpu_to_be32(sizeof(struct ipr_ioadl_desc) * ipr_cmd->dma_use_sg);
+		} else if (scsi_cmd->sc_data_direction == DMA_FROM_DEVICE) {
+			ioadl_flags = IPR_IOADL_FLAGS_READ;
+			ioarcb->read_data_transfer_length = cpu_to_be32(length);
+			ioarcb->read_ioadl_len =
+				cpu_to_be32(sizeof(struct ipr_ioadl_desc) * ipr_cmd->dma_use_sg);
+		}
+
+		sglist = scsi_cmd->request_buffer;
+
+		for (i = 0; i < ipr_cmd->dma_use_sg; i++) {
+			ioadl[i].flags_and_data_len =
+				cpu_to_be32(ioadl_flags | sg_dma_len(&sglist[i]));
+			ioadl[i].address =
+				cpu_to_be32(sg_dma_address(&sglist[i]));
+		}
+
+		if (likely(ipr_cmd->dma_use_sg)) {
+			ioadl[i-1].flags_and_data_len |=
+				cpu_to_be32(IPR_IOADL_FLAGS_LAST);
+			return 0;
+		} else
+			dev_err(&ioa_cfg->pdev->dev, "pci_map_sg failed!\n");
+	} else {
+		if (scsi_cmd->sc_data_direction == DMA_TO_DEVICE) {
+			ioadl_flags = IPR_IOADL_FLAGS_WRITE;
+			ioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;
+			ioarcb->write_data_transfer_length = cpu_to_be32(length);
+			ioarcb->write_ioadl_len = cpu_to_be32(sizeof(struct ipr_ioadl_desc));
+		} else if (scsi_cmd->sc_data_direction == DMA_FROM_DEVICE) {
+			ioadl_flags = IPR_IOADL_FLAGS_READ;
+			ioarcb->read_data_transfer_length = cpu_to_be32(length);
+			ioarcb->read_ioadl_len = cpu_to_be32(sizeof(struct ipr_ioadl_desc));
+		}
+
+		ipr_cmd->dma_handle = pci_map_single(ioa_cfg->pdev,
+						     scsi_cmd->buffer, length,
+						     scsi_cmd->sc_data_direction);
+
+		if (likely(!pci_dma_error(ipr_cmd->dma_handle))) {
+			ipr_cmd->dma_use_sg = 1;
+			ioadl[0].flags_and_data_len =
+				cpu_to_be32(ioadl_flags | length | IPR_IOADL_FLAGS_LAST);
+			ioadl[0].address = cpu_to_be32(ipr_cmd->dma_handle);
+			return 0;
+		} else
+			dev_err(&ioa_cfg->pdev->dev, "pci_map_single failed!\n");
+	}
+
+	return -1;
+}
+
+/**
+ * ipr_get_task_attributes - Translate SPI Q-Tag to task attributes
+ * @scsi_cmd:	scsi command struct
+ *
+ * Return value:
+ * 	task attributes
+ **/
+static u8 ipr_get_task_attributes(struct scsi_cmnd *scsi_cmd)
+{
+	u8 tag[2];
+	u8 rc = IPR_FLAGS_LO_UNTAGGED_TASK;
+
+	if (scsi_populate_tag_msg(scsi_cmd, tag)) {
+		switch (tag[0]) {
+		case MSG_SIMPLE_TAG:
+			rc = IPR_FLAGS_LO_SIMPLE_TASK;
+			break;
+		case MSG_HEAD_TAG:
+			rc = IPR_FLAGS_LO_HEAD_OF_Q_TASK;
+			break;
+		case MSG_ORDERED_TAG:
+			rc = IPR_FLAGS_LO_ORDERED_TASK;
+			break;
+		};
+	}
+
+	return rc;
+}
+
+/**
+ * ipr_erp_done - Process completion of ERP for a device
+ * @ipr_cmd:		ipr command struct
+ *
+ * This function copies the sense buffer into the scsi_cmd
+ * struct and pushes the scsi_done function.
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_erp_done(struct ipr_cmnd *ipr_cmd)
+{
+	struct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;
+	struct ipr_resource_entry *res = scsi_cmd->device->hostdata;
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	u32 ioasc = be32_to_cpu(ipr_cmd->ioasa.ioasc);
+
+	if (IPR_IOASC_SENSE_KEY(ioasc) > 0) {
+		scsi_cmd->result |= (DID_ERROR << 16);
+		ipr_sdev_err(scsi_cmd->device,
+			     "Request Sense failed with IOASC: 0x%08X\n", ioasc);
+	} else {
+		memcpy(scsi_cmd->sense_buffer, ipr_cmd->sense_buffer,
+		       SCSI_SENSE_BUFFERSIZE);
+	}
+
+	res->needs_sync_complete = 1;
+	ipr_unmap_sglist(ioa_cfg, ipr_cmd);
+	list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+	scsi_cmd->scsi_done(scsi_cmd);
+}
+
+/**
+ * ipr_reinit_ipr_cmnd_for_erp - Re-initialize a cmnd block to be used for ERP
+ * @ipr_cmd:	ipr command struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_reinit_ipr_cmnd_for_erp(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioarcb *ioarcb;
+	struct ipr_ioasa *ioasa;
+
+	ioarcb = &ipr_cmd->ioarcb;
+	ioasa = &ipr_cmd->ioasa;
+
+	memset(&ioarcb->cmd_pkt, 0, sizeof(struct ipr_cmd_pkt));
+	ioarcb->write_data_transfer_length = 0;
+	ioarcb->read_data_transfer_length = 0;
+	ioarcb->write_ioadl_len = 0;
+	ioarcb->read_ioadl_len = 0;
+	ioasa->ioasc = 0;
+	ioasa->residual_data_len = 0;
+}
+
+/**
+ * ipr_erp_request_sense - Send request sense to a device
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function sends a request sense to a device as a result
+ * of a check condition.
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_erp_request_sense(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_cmd_pkt *cmd_pkt = &ipr_cmd->ioarcb.cmd_pkt;
+
+	ipr_reinit_ipr_cmnd_for_erp(ipr_cmd);
+
+	cmd_pkt->request_type = IPR_RQTYPE_SCSICDB;
+	cmd_pkt->cdb[0] = REQUEST_SENSE;
+	cmd_pkt->cdb[4] = SCSI_SENSE_BUFFERSIZE;
+	cmd_pkt->flags_hi |= IPR_FLAGS_HI_SYNC_OVERRIDE;
+	cmd_pkt->flags_hi |= IPR_FLAGS_HI_NO_ULEN_CHK;
+	cmd_pkt->timeout = cpu_to_be16(IPR_REQUEST_SENSE_TIMEOUT / HZ);
+
+	ipr_cmd->ioadl[0].flags_and_data_len =
+		cpu_to_be32(IPR_IOADL_FLAGS_READ_LAST | SCSI_SENSE_BUFFERSIZE);
+	ipr_cmd->ioadl[0].address =
+		cpu_to_be32(ipr_cmd->sense_buffer_dma);
+
+	ipr_cmd->ioarcb.read_ioadl_len =
+		cpu_to_be32(sizeof(struct ipr_ioadl_desc));
+	ipr_cmd->ioarcb.read_data_transfer_length =
+		cpu_to_be32(SCSI_SENSE_BUFFERSIZE);
+
+	ipr_do_req(ipr_cmd, ipr_erp_done, ipr_timeout,
+		   IPR_REQUEST_SENSE_TIMEOUT * IPR_TIMEOUT_MULTIPLIER);
+}
+
+/**
+ * ipr_erp_cancel_all - Send cancel all to a device
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function sends a cancel all to a device to clear the
+ * queue. If we are running TCQ on the device, QERR is set to 1,
+ * which means all outstanding ops have been dropped on the floor.
+ * Cancel all will return them to us.
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_erp_cancel_all(struct ipr_cmnd *ipr_cmd)
+{
+	struct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;
+	struct ipr_resource_entry *res = scsi_cmd->device->hostdata;
+	struct ipr_cmd_pkt *cmd_pkt;
+
+	res->in_erp = 1;
+
+	ipr_reinit_ipr_cmnd_for_erp(ipr_cmd);
+
+	cmd_pkt = &ipr_cmd->ioarcb.cmd_pkt;
+	cmd_pkt->request_type = IPR_RQTYPE_IOACMD;
+	cmd_pkt->cdb[0] = IPR_CANCEL_ALL_REQUESTS;
+
+	ipr_do_req(ipr_cmd, ipr_erp_request_sense, ipr_timeout,
+		   IPR_CANCEL_ALL_TIMEOUT);
+}
+
+/**
+ * ipr_dump_ioasa - Dump contents of IOASA
+ * @ioa_cfg:	ioa config struct
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function is invoked by the interrupt handler when ops
+ * fail. It will log the IOASA if appropriate. Only called
+ * for GPDD ops.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_dump_ioasa(struct ipr_ioa_cfg *ioa_cfg,
+			   struct ipr_cmnd *ipr_cmd)
+{
+	int i;
+	u16 data_len;
+	u32 ioasc;
+	struct ipr_ioasa *ioasa = &ipr_cmd->ioasa;
+	u32 *ioasa_data = (u32 *)ioasa;
+	int error_index;
+
+	ioasc = be32_to_cpu(ioasa->ioasc) & IPR_IOASC_IOASC_MASK;
+
+	if (0 == ioasc)
+		return;
+
+	if (ioa_cfg->log_level < IPR_DEFAULT_LOG_LEVEL)
+		return;
+
+	error_index = ipr_get_error(ioasc);
+
+	if (ioa_cfg->log_level < IPR_MAX_LOG_LEVEL) {
+		/* Don't log an error if the IOA already logged one */
+		if (ioasa->ilid != 0)
+			return;
+
+		if (ipr_error_table[error_index].log_ioasa == 0)
+			return;
+	}
+
+	ipr_sdev_err(ipr_cmd->scsi_cmd->device, "%s\n",
+		     ipr_error_table[error_index].error);
+
+	if ((ioasa->gpdd.device_end_state <= ARRAY_SIZE(ipr_gpdd_dev_end_states)) &&
+	    (ioasa->gpdd.device_bus_phase <= ARRAY_SIZE(ipr_gpdd_dev_bus_phases))) {
+		ipr_sdev_err(ipr_cmd->scsi_cmd->device,
+			     "Device End state: %s Phase: %s\n",
+			     ipr_gpdd_dev_end_states[ioasa->gpdd.device_end_state],
+			     ipr_gpdd_dev_bus_phases[ioasa->gpdd.device_bus_phase]);
+	}
+
+	if (sizeof(struct ipr_ioasa) < be16_to_cpu(ioasa->ret_stat_len))
+		data_len = sizeof(struct ipr_ioasa);
+	else
+		data_len = be16_to_cpu(ioasa->ret_stat_len);
+
+	ipr_err("IOASA Dump:\n");
+
+	for (i = 0; i < data_len / 4; i += 4) {
+		ipr_err("%08X: %08X %08X %08X %08X\n", i*4,
+			be32_to_cpu(ioasa_data[i]),
+			be32_to_cpu(ioasa_data[i+1]),
+			be32_to_cpu(ioasa_data[i+2]),
+			be32_to_cpu(ioasa_data[i+3]));
+	}
+}
+
+/**
+ * ipr_gen_sense - Generate SCSI sense data from an IOASA
+ * @ioasa:		IOASA
+ * @sense_buf:	sense data buffer
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_gen_sense(struct ipr_cmnd *ipr_cmd)
+{
+	u32 failing_lba;
+	u8 *sense_buf = ipr_cmd->scsi_cmd->sense_buffer;
+	struct ipr_resource_entry *res = ipr_cmd->scsi_cmd->device->hostdata;
+	struct ipr_ioasa *ioasa = &ipr_cmd->ioasa;
+	u32 ioasc = be32_to_cpu(ioasa->ioasc);
+
+	memset(sense_buf, 0, SCSI_SENSE_BUFFERSIZE);
+
+	if (ioasc >= IPR_FIRST_DRIVER_IOASC)
+		return;
+
+	ipr_cmd->scsi_cmd->result = SAM_STAT_CHECK_CONDITION;
+
+	if (ipr_is_vset_device(res) &&
+	    ioasc == IPR_IOASC_MED_DO_NOT_REALLOC &&
+	    ioasa->vset.failing_lba_hi != 0) {
+		sense_buf[0] = 0x72;
+		sense_buf[1] = IPR_IOASC_SENSE_KEY(ioasc);
+		sense_buf[2] = IPR_IOASC_SENSE_CODE(ioasc);
+		sense_buf[3] = IPR_IOASC_SENSE_QUAL(ioasc);
+
+		sense_buf[7] = 12;
+		sense_buf[8] = 0;
+		sense_buf[9] = 0x0A;
+		sense_buf[10] = 0x80;
+
+		failing_lba = be32_to_cpu(ioasa->vset.failing_lba_hi);
+
+		sense_buf[12] = (failing_lba & 0xff000000) >> 24;
+		sense_buf[13] = (failing_lba & 0x00ff0000) >> 16;
+		sense_buf[14] = (failing_lba & 0x0000ff00) >> 8;
+		sense_buf[15] = failing_lba & 0x000000ff;
+
+		failing_lba = be32_to_cpu(ioasa->vset.failing_lba_lo);
+
+		sense_buf[16] = (failing_lba & 0xff000000) >> 24;
+		sense_buf[17] = (failing_lba & 0x00ff0000) >> 16;
+		sense_buf[18] = (failing_lba & 0x0000ff00) >> 8;
+		sense_buf[19] = failing_lba & 0x000000ff;
+	} else {
+		sense_buf[0] = 0x70;
+		sense_buf[2] = IPR_IOASC_SENSE_KEY(ioasc);
+		sense_buf[12] = IPR_IOASC_SENSE_CODE(ioasc);
+		sense_buf[13] = IPR_IOASC_SENSE_QUAL(ioasc);
+
+		/* Illegal request */
+		if ((IPR_IOASC_SENSE_KEY(ioasc) == 0x05) &&
+		    (be32_to_cpu(ioasa->ioasc_specific) & IPR_FIELD_POINTER_VALID)) {
+			sense_buf[7] = 10;	/* additional length */
+
+			/* IOARCB was in error */
+			if (IPR_IOASC_SENSE_CODE(ioasc) == 0x24)
+				sense_buf[15] = 0xC0;
+			else	/* Parameter data was invalid */
+				sense_buf[15] = 0x80;
+
+			sense_buf[16] =
+			    ((IPR_FIELD_POINTER_MASK &
+			      be32_to_cpu(ioasa->ioasc_specific)) >> 8) & 0xff;
+			sense_buf[17] =
+			    (IPR_FIELD_POINTER_MASK &
+			     be32_to_cpu(ioasa->ioasc_specific)) & 0xff;
+		} else {
+			if (ioasc == IPR_IOASC_MED_DO_NOT_REALLOC) {
+				if (ipr_is_vset_device(res))
+					failing_lba = be32_to_cpu(ioasa->vset.failing_lba_lo);
+				else
+					failing_lba = be32_to_cpu(ioasa->dasd.failing_lba);
+
+				sense_buf[0] |= 0x80;	/* Or in the Valid bit */
+				sense_buf[3] = (failing_lba & 0xff000000) >> 24;
+				sense_buf[4] = (failing_lba & 0x00ff0000) >> 16;
+				sense_buf[5] = (failing_lba & 0x0000ff00) >> 8;
+				sense_buf[6] = failing_lba & 0x000000ff;
+			}
+
+			sense_buf[7] = 6;	/* additional length */
+		}
+	}
+}
+
+/**
+ * ipr_erp_start - Process an error response for a SCSI op
+ * @ioa_cfg:	ioa config struct
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function determines whether or not to initiate ERP
+ * on the affected device.
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_erp_start(struct ipr_ioa_cfg *ioa_cfg,
+			      struct ipr_cmnd *ipr_cmd)
+{
+	struct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;
+	struct ipr_resource_entry *res = scsi_cmd->device->hostdata;
+	u32 ioasc = be32_to_cpu(ipr_cmd->ioasa.ioasc);
+
+	if (!res) {
+		ipr_scsi_eh_done(ipr_cmd);
+		return;
+	}
+
+	if (ipr_is_gscsi(res))
+		ipr_dump_ioasa(ioa_cfg, ipr_cmd);
+	else
+		ipr_gen_sense(ipr_cmd);
+
+	switch (ioasc & IPR_IOASC_IOASC_MASK) {
+	case IPR_IOASC_ABORTED_CMD_TERM_BY_HOST:
+		scsi_cmd->result |= (DID_ERROR << 16);
+		break;
+	case IPR_IOASC_IR_RESOURCE_HANDLE:
+		scsi_cmd->result |= (DID_NO_CONNECT << 16);
+		break;
+	case IPR_IOASC_HW_SEL_TIMEOUT:
+		scsi_cmd->result |= (DID_NO_CONNECT << 16);
+		res->needs_sync_complete = 1;
+		break;
+	case IPR_IOASC_SYNC_REQUIRED:
+		if (!res->in_erp) {
+			res->needs_sync_complete = 1;
+			scsi_cmd->result |= (BUSY << 1);
+		} else {
+			scsi_cmd->result |= (DID_ERROR << 16);
+		}
+		break;
+	case IPR_IOASC_MED_DO_NOT_REALLOC: /* prevent retries */
+		scsi_cmd->result |= (DID_PASSTHROUGH << 16);
+		break;
+	case IPR_IOASC_BUS_WAS_RESET:
+	case IPR_IOASC_BUS_WAS_RESET_BY_OTHER:
+		/*
+		 * Report the bus reset and ask for a retry. The device
+		 * will give CC/UA the next command.
+		 */
+		if (!res->resetting_device)
+			scsi_report_bus_reset(ioa_cfg->host, scsi_cmd->device->channel);
+		scsi_cmd->result |= (DID_ERROR << 16);
+		res->needs_sync_complete = 1;
+		break;
+	case IPR_IOASC_HW_DEV_BUS_STATUS:
+		scsi_cmd->result |= IPR_IOASC_SENSE_STATUS(ioasc);
+		if (IPR_IOASC_SENSE_STATUS(ioasc) == SAM_STAT_CHECK_CONDITION) {
+			ipr_erp_cancel_all(ipr_cmd);
+			return;
+		}
+		break;
+	case IPR_IOASC_NR_INIT_CMD_REQUIRED:
+		break;
+	default:
+		scsi_cmd->result |= (DID_ERROR << 16);
+		if (!ipr_is_vset_device(res))
+			res->needs_sync_complete = 1;
+		break;
+	}
+
+	ipr_unmap_sglist(ioa_cfg, ipr_cmd);
+	list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+	scsi_cmd->scsi_done(scsi_cmd);
+}
+
+/**
+ * ipr_scsi_done - mid-layer done function
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function is invoked by the interrupt handler for
+ * ops generated by the SCSI mid-layer
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_scsi_done(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;
+	u32 ioasc = be32_to_cpu(ipr_cmd->ioasa.ioasc);
+
+	scsi_cmd->resid = be32_to_cpu(ipr_cmd->ioasa.residual_data_len);
+
+	if (likely(IPR_IOASC_SENSE_KEY(ioasc) == 0)) {
+		ipr_unmap_sglist(ioa_cfg, ipr_cmd);
+		list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+		scsi_cmd->scsi_done(scsi_cmd);
+	} else
+		ipr_erp_start(ioa_cfg, ipr_cmd);
+}
+
+/**
+ * ipr_save_ioafp_mode_select - Save adapters mode select data
+ * @ioa_cfg:	ioa config struct
+ * @scsi_cmd:	scsi command struct
+ *
+ * This function saves mode select data for the adapter to
+ * use following an adapter reset.
+ *
+ * Return value:
+ *	0 on success / SCSI_MLQUEUE_HOST_BUSY on failure
+ **/
+static int ipr_save_ioafp_mode_select(struct ipr_ioa_cfg *ioa_cfg,
+				       struct scsi_cmnd *scsi_cmd)
+{
+	if (!ioa_cfg->saved_mode_pages) {
+		ioa_cfg->saved_mode_pages  = kmalloc(sizeof(struct ipr_mode_pages),
+						     GFP_ATOMIC);
+		if (!ioa_cfg->saved_mode_pages) {
+			dev_err(&ioa_cfg->pdev->dev,
+				"IOA mode select buffer allocation failed\n");
+			return SCSI_MLQUEUE_HOST_BUSY;
+		}
+	}
+
+	memcpy(ioa_cfg->saved_mode_pages, scsi_cmd->buffer, scsi_cmd->cmnd[4]);
+	ioa_cfg->saved_mode_page_len = scsi_cmd->cmnd[4];
+	return 0;
+}
+
+/**
+ * ipr_queuecommand - Queue a mid-layer request
+ * @scsi_cmd:	scsi command struct
+ * @done:		done function
+ *
+ * This function queues a request generated by the mid-layer.
+ *
+ * Return value:
+ *	0 on success
+ *	SCSI_MLQUEUE_DEVICE_BUSY if device is busy
+ *	SCSI_MLQUEUE_HOST_BUSY if host is busy
+ **/
+static int ipr_queuecommand(struct scsi_cmnd *scsi_cmd,
+			    void (*done) (struct scsi_cmnd *))
+{
+	struct ipr_ioa_cfg *ioa_cfg;
+	struct ipr_resource_entry *res;
+	void (*timeout_func) (struct scsi_cmnd *);
+	struct ipr_ioarcb *ioarcb;
+	struct ipr_cmnd *ipr_cmd;
+	int rc = 0;
+	int timeout;
+
+	scsi_cmd->scsi_done = done;
+	ioa_cfg = (struct ipr_ioa_cfg *)scsi_cmd->device->host->hostdata;
+	res = scsi_cmd->device->hostdata;
+	scsi_cmd->result = (DID_OK << 16);
+
+	/*
+	 * We are currently blocking all devices due to a host reset
+	 * We have told the host to stop giving us new requests, but
+	 * ERP ops don't count. FIXME
+	 */
+	if (unlikely(!ioa_cfg->allow_cmds))
+		return SCSI_MLQUEUE_HOST_BUSY;
+
+	/*
+	 * FIXME - Create scsi_set_device_offline, scsi_set_host_offline
+	 * interfaces and we can remove some of these checks
+	 */
+	if (unlikely(ioa_cfg->ioa_is_dead || !res || res->del_from_ml)) {
+		memset(scsi_cmd->sense_buffer, 0, SCSI_SENSE_BUFFERSIZE);
+		scsi_cmd->result = (DID_NO_CONNECT << 16);
+		scsi_cmd->scsi_done(scsi_cmd);
+		return 0;
+	}
+
+	ipr_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);
+	ioarcb = &ipr_cmd->ioarcb;
+	list_add_tail(&ipr_cmd->queue, &ioa_cfg->pending_q);
+
+	memcpy(ioarcb->cmd_pkt.cdb, scsi_cmd->cmnd, scsi_cmd->cmd_len);
+	ipr_cmd->scsi_cmd = scsi_cmd;
+	ioarcb->res_handle = res->cfgte.res_handle;
+
+	/*
+	 * Double the timeout value to use as we will use the adapter
+	 * as the primary timing mechanism. FIXME
+	 */
+	timeout_func = (void (*)(struct scsi_cmnd *)) scsi_cmd->eh_timeout.function;
+	timeout = scsi_cmd->timeout_per_command;
+	scsi_add_timer(scsi_cmd, timeout * IPR_TIMEOUT_MULTIPLIER, timeout_func);
+
+	ipr_cmd->done = ipr_scsi_done;
+	ipr_trc_hook(ipr_cmd, IPR_TRACE_START, IPR_GET_PHYS_LOC(res->cfgte.res_addr));
+
+	if (ipr_is_gscsi(res) || ipr_is_vset_device(res)) {
+		if ((timeout / HZ) > IPR_MAX_SECOND_RADIX_TIMEOUT) {
+			ioarcb->cmd_pkt.timeout =
+				cpu_to_be16(((timeout / HZ) / 60) | IPR_TIMEOUT_MINUTE_RADIX);
+		} else {
+			ioarcb->cmd_pkt.timeout = cpu_to_be16(timeout / HZ);
+		}
+
+		if (scsi_cmd->underflow == 0)
+			ioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_NO_ULEN_CHK;
+
+		if (res->needs_sync_complete) {
+			ioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_SYNC_COMPLETE;
+			res->needs_sync_complete = 0;
+		}
+
+		ioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_NO_LINK_DESC;
+		ioarcb->cmd_pkt.flags_lo |= IPR_FLAGS_LO_DELAY_AFTER_RST;
+		ioarcb->cmd_pkt.flags_lo |= IPR_FLAGS_LO_ALIGNED_BFR;
+		ioarcb->cmd_pkt.flags_lo |= ipr_get_task_attributes(scsi_cmd);
+	}
+
+	if (!ipr_is_gscsi(res) && scsi_cmd->cmnd[0] >= 0xC0)
+		ioarcb->cmd_pkt.request_type = IPR_RQTYPE_IOACMD;
+
+	if (ipr_is_ioa_resource(res) && scsi_cmd->cmnd[0] == MODE_SELECT)
+		rc = ipr_save_ioafp_mode_select(ioa_cfg, scsi_cmd);
+
+	if (likely(rc == 0))
+		rc = ipr_build_ioadl(ioa_cfg, ipr_cmd);
+
+	if (likely(rc == 0)) {
+		mb();
+		writel(be32_to_cpu(ipr_cmd->ioarcb.ioarcb_host_pci_addr),
+		       ioa_cfg->regs.ioarrin_reg);
+	} else {
+		 list_move_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+		 return SCSI_MLQUEUE_HOST_BUSY;
+	}
+
+	return 0;
+}
+
+/**
+ * ipr_info - Get information about the card/driver
+ * @scsi_host:	scsi host struct
+ *
+ * Return value:
+ * 	pointer to buffer with description string
+ **/
+static const char * ipr_ioa_info(struct Scsi_Host *host)
+{
+	static char buffer[512];
+	struct ipr_ioa_cfg *ioa_cfg;
+	unsigned long lock_flags = 0;
+
+	ioa_cfg = (struct ipr_ioa_cfg *) host->hostdata;
+
+	spin_lock_irqsave(host->host_lock, lock_flags);
+	sprintf(buffer, "IBM %X Storage Adapter", ioa_cfg->type);
+	spin_unlock_irqrestore(host->host_lock, lock_flags);
+
+	return buffer;
+}
+
+static struct scsi_host_template driver_template = {
+	.name = "IPR",
+	.info = ipr_ioa_info,
+	.queuecommand = ipr_queuecommand,
+	.eh_abort_handler = ipr_eh_abort,
+	.eh_device_reset_handler = ipr_eh_dev_reset,
+	.eh_host_reset_handler = ipr_eh_host_reset,
+	.slave_alloc = ipr_slave_alloc,
+	.slave_configure = ipr_slave_configure,
+	.slave_destroy = ipr_slave_destroy,
+	.bios_param = ipr_biosparam,
+	.can_queue = IPR_MAX_COMMANDS,
+	.this_id = -1,
+	.sg_tablesize = IPR_MAX_SGLIST,
+	.max_sectors = IPR_MAX_SECTORS,
+	.cmd_per_lun = IPR_MAX_CMD_PER_LUN,
+	.use_clustering = ENABLE_CLUSTERING,
+	.shost_attrs = ipr_ioa_attrs,
+	.sdev_attrs = ipr_dev_attrs,
+	.proc_name = IPR_NAME
+};
+
+#ifdef CONFIG_PPC_PSERIES
+static const u16 ipr_blocked_processors[] = {
+	PV_NORTHSTAR,
+	PV_PULSAR,
+	PV_POWER4,
+	PV_ICESTAR,
+	PV_SSTAR,
+	PV_POWER4p,
+	PV_630,
+	PV_630p
+};
+
+/**
+ * ipr_invalid_adapter - Determine if this adapter is supported on this hardware
+ * @ioa_cfg:	ioa cfg struct
+ *
+ * Adapters that use Gemstone revision < 3.1 do not work reliably on
+ * certain pSeries hardware. This function determines if the given
+ * adapter is in one of these confgurations or not.
+ *
+ * Return value:
+ * 	1 if adapter is not supported / 0 if adapter is supported
+ **/
+static int ipr_invalid_adapter(struct ipr_ioa_cfg *ioa_cfg)
+{
+	u8 rev_id;
+	int i;
+
+	if (ioa_cfg->type == 0x5702) {
+		if (pci_read_config_byte(ioa_cfg->pdev, PCI_REVISION_ID,
+					 &rev_id) == PCIBIOS_SUCCESSFUL) {
+			if (rev_id < 4) {
+				for (i = 0; i < ARRAY_SIZE(ipr_blocked_processors); i++){
+					if (__is_processor(ipr_blocked_processors[i]))
+						return 1;
+				}
+			}
+		}
+	}
+	return 0;
+}
+#else
+#define ipr_invalid_adapter(ioa_cfg) 0
+#endif
+
+/**
+ * ipr_ioa_bringdown_done - IOA bring down completion.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function processes the completion of an adapter bring down.
+ * It wakes any reset sleepers.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_RETURN
+ **/
+static int ipr_ioa_bringdown_done(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+
+	ENTER;
+	ioa_cfg->in_reset_reload = 0;
+	ioa_cfg->reset_retries = 0;
+	list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+	wake_up_all(&ioa_cfg->reset_wait_q);
+
+	spin_unlock_irq(ioa_cfg->host->host_lock);
+	scsi_unblock_requests(ioa_cfg->host);
+	spin_lock_irq(ioa_cfg->host->host_lock);
+	LEAVE;
+
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_ioa_reset_done - IOA reset completion.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function processes the completion of an adapter reset.
+ * It schedules any necessary mid-layer add/removes and
+ * wakes any reset sleepers.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_RETURN
+ **/
+static int ipr_ioa_reset_done(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_resource_entry *res;
+	struct ipr_hostrcb *hostrcb, *temp;
+	int i = 0;
+
+	ENTER;
+	ioa_cfg->in_reset_reload = 0;
+	ioa_cfg->allow_cmds = 1;
+	ioa_cfg->reset_cmd = NULL;
+
+	list_for_each_entry(res, &ioa_cfg->used_res_q, queue) {
+		if (res->add_to_ml || res->del_from_ml) {
+			ipr_trace;
+			schedule_work(&ioa_cfg->work_q);
+			break;
+		}
+	}
+
+	list_for_each_entry_safe(hostrcb, temp, &ioa_cfg->hostrcb_free_q, queue) {
+		list_del(&hostrcb->queue);
+		if (i++ < IPR_NUM_LOG_HCAMS)
+			ipr_send_hcam(ioa_cfg, IPR_HCAM_CDB_OP_CODE_LOG_DATA, hostrcb);
+		else
+			ipr_send_hcam(ioa_cfg, IPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE, hostrcb);
+	}
+
+	dev_info(&ioa_cfg->pdev->dev, "IOA initialized.\n");
+
+	ioa_cfg->reset_retries = 0;
+	list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+	wake_up_all(&ioa_cfg->reset_wait_q);
+
+	spin_unlock_irq(ioa_cfg->host->host_lock);
+	scsi_unblock_requests(ioa_cfg->host);
+	spin_lock_irq(ioa_cfg->host->host_lock);
+
+	if (!ioa_cfg->allow_cmds)
+		scsi_block_requests(ioa_cfg->host);
+
+	LEAVE;
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_set_sup_dev_dflt - Initialize a Set Supported Device buffer
+ * @supported_dev:	supported device struct
+ * @vpids:			vendor product id struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_set_sup_dev_dflt(struct ipr_supported_device *supported_dev,
+				 struct ipr_std_inq_vpids *vpids)
+{
+	memset(supported_dev, 0, sizeof(struct ipr_supported_device));
+	memcpy(&supported_dev->vpids, vpids, sizeof(struct ipr_std_inq_vpids));
+	supported_dev->num_records = 1;
+	supported_dev->data_length =
+		cpu_to_be16(sizeof(struct ipr_supported_device));
+	supported_dev->reserved = 0;
+}
+
+/**
+ * ipr_set_supported_devs - Send Set Supported Devices for a device
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function send a Set Supported Devices to the adapter
+ *
+ * Return value:
+ * 	IPR_RC_JOB_CONTINUE / IPR_RC_JOB_RETURN
+ **/
+static int ipr_set_supported_devs(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_supported_device *supp_dev = &ioa_cfg->vpd_cbs->supp_dev;
+	struct ipr_ioadl_desc *ioadl = ipr_cmd->ioadl;
+	struct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;
+	struct ipr_resource_entry *res = ipr_cmd->res;
+
+	ipr_cmd->job_step = ipr_ioa_reset_done;
+
+	list_for_each_entry_continue(res, &ioa_cfg->used_res_q, queue) {
+		if (!ipr_is_af_dasd_device(res))
+			continue;
+
+		ipr_cmd->res = res;
+		ipr_set_sup_dev_dflt(supp_dev, &res->cfgte.std_inq_data.vpids);
+
+		ioarcb->res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);
+		ioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;
+		ioarcb->cmd_pkt.request_type = IPR_RQTYPE_IOACMD;
+
+		ioarcb->cmd_pkt.cdb[0] = IPR_SET_SUPPORTED_DEVICES;
+		ioarcb->cmd_pkt.cdb[7] = (sizeof(struct ipr_supported_device) >> 8) & 0xff;
+		ioarcb->cmd_pkt.cdb[8] = sizeof(struct ipr_supported_device) & 0xff;
+
+		ioadl->flags_and_data_len = cpu_to_be32(IPR_IOADL_FLAGS_WRITE_LAST |
+							sizeof(struct ipr_supported_device));
+		ioadl->address = cpu_to_be32(ioa_cfg->vpd_cbs_dma +
+					     offsetof(struct ipr_misc_cbs, supp_dev));
+		ioarcb->write_ioadl_len = cpu_to_be32(sizeof(struct ipr_ioadl_desc));
+		ioarcb->write_data_transfer_length =
+			cpu_to_be32(sizeof(struct ipr_supported_device));
+
+		ipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout,
+			   IPR_SET_SUP_DEVICE_TIMEOUT);
+
+		ipr_cmd->job_step = ipr_set_supported_devs;
+		return IPR_RC_JOB_RETURN;
+	}
+
+	return IPR_RC_JOB_CONTINUE;
+}
+
+/**
+ * ipr_get_mode_page - Locate specified mode page
+ * @mode_pages:	mode page buffer
+ * @page_code:	page code to find
+ * @len:		minimum required length for mode page
+ *
+ * Return value:
+ * 	pointer to mode page / NULL on failure
+ **/
+static void *ipr_get_mode_page(struct ipr_mode_pages *mode_pages,
+			       u32 page_code, u32 len)
+{
+	struct ipr_mode_page_hdr *mode_hdr;
+	u32 page_length;
+	u32 length;
+
+	if (!mode_pages || (mode_pages->hdr.length == 0))
+		return NULL;
+
+	length = (mode_pages->hdr.length + 1) - 4 - mode_pages->hdr.block_desc_len;
+	mode_hdr = (struct ipr_mode_page_hdr *)
+		(mode_pages->data + mode_pages->hdr.block_desc_len);
+
+	while (length) {
+		if (IPR_GET_MODE_PAGE_CODE(mode_hdr) == page_code) {
+			if (mode_hdr->page_length >= (len - sizeof(struct ipr_mode_page_hdr)))
+				return mode_hdr;
+			break;
+		} else {
+			page_length = (sizeof(struct ipr_mode_page_hdr) +
+				       mode_hdr->page_length);
+			length -= page_length;
+			mode_hdr = (struct ipr_mode_page_hdr *)
+				((unsigned long)mode_hdr + page_length);
+		}
+	}
+	return NULL;
+}
+
+/**
+ * ipr_check_term_power - Check for term power errors
+ * @ioa_cfg:	ioa config struct
+ * @mode_pages:	IOAFP mode pages buffer
+ *
+ * Check the IOAFP's mode page 28 for term power errors
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_check_term_power(struct ipr_ioa_cfg *ioa_cfg,
+				 struct ipr_mode_pages *mode_pages)
+{
+	int i;
+	int entry_length;
+	struct ipr_dev_bus_entry *bus;
+	struct ipr_mode_page28 *mode_page;
+
+	mode_page = ipr_get_mode_page(mode_pages, 0x28,
+				      sizeof(struct ipr_mode_page28));
+
+	entry_length = mode_page->entry_length;
+
+	bus = mode_page->bus;
+
+	for (i = 0; i < mode_page->num_entries; i++) {
+		if (bus->flags & IPR_SCSI_ATTR_NO_TERM_PWR) {
+			dev_err(&ioa_cfg->pdev->dev,
+				"Term power is absent on scsi bus %d\n",
+				bus->res_addr.bus);
+		}
+
+		bus = (struct ipr_dev_bus_entry *)((char *)bus + entry_length);
+	}
+}
+
+/**
+ * ipr_scsi_bus_speed_limit - Limit the SCSI speed based on SES table
+ * @ioa_cfg:	ioa config struct
+ *
+ * Looks through the config table checking for SES devices. If
+ * the SES device is in the SES table indicating a maximum SCSI
+ * bus speed, the speed is limited for the bus.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_scsi_bus_speed_limit(struct ipr_ioa_cfg *ioa_cfg)
+{
+	u32 max_xfer_rate;
+	int i;
+
+	for (i = 0; i < IPR_MAX_NUM_BUSES; i++) {
+		max_xfer_rate = ipr_get_max_scsi_speed(ioa_cfg, i,
+						       ioa_cfg->bus_attr[i].bus_width);
+
+		if (max_xfer_rate < ioa_cfg->bus_attr[i].max_xfer_rate)
+			ioa_cfg->bus_attr[i].max_xfer_rate = max_xfer_rate;
+	}
+}
+
+/**
+ * ipr_modify_ioafp_mode_page_28 - Modify IOAFP Mode Page 28
+ * @ioa_cfg:	ioa config struct
+ * @mode_pages:	mode page 28 buffer
+ *
+ * Updates mode page 28 based on driver configuration
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_modify_ioafp_mode_page_28(struct ipr_ioa_cfg *ioa_cfg,
+					  	struct ipr_mode_pages *mode_pages)
+{
+	int i, entry_length;
+	struct ipr_dev_bus_entry *bus;
+	struct ipr_bus_attributes *bus_attr;
+	struct ipr_mode_page28 *mode_page;
+
+	mode_page = ipr_get_mode_page(mode_pages, 0x28,
+				      sizeof(struct ipr_mode_page28));
+
+	entry_length = mode_page->entry_length;
+
+	/* Loop for each device bus entry */
+	for (i = 0, bus = mode_page->bus;
+	     i < mode_page->num_entries;
+	     i++, bus = (struct ipr_dev_bus_entry *)((u8 *)bus + entry_length)) {
+		if (bus->res_addr.bus > IPR_MAX_NUM_BUSES) {
+			dev_err(&ioa_cfg->pdev->dev,
+				"Invalid resource address reported: 0x%08X\n",
+				IPR_GET_PHYS_LOC(bus->res_addr));
+			continue;
+		}
+
+		bus_attr = &ioa_cfg->bus_attr[i];
+		bus->extended_reset_delay = IPR_EXTENDED_RESET_DELAY;
+		bus->bus_width = bus_attr->bus_width;
+		bus->max_xfer_rate = cpu_to_be32(bus_attr->max_xfer_rate);
+		bus->flags &= ~IPR_SCSI_ATTR_QAS_MASK;
+		if (bus_attr->qas_enabled)
+			bus->flags |= IPR_SCSI_ATTR_ENABLE_QAS;
+		else
+			bus->flags |= IPR_SCSI_ATTR_DISABLE_QAS;
+	}
+}
+
+/**
+ * ipr_build_mode_select - Build a mode select command
+ * @ipr_cmd:	ipr command struct
+ * @res_handle:	resource handle to send command to
+ * @parm:		Byte 2 of Mode Sense command
+ * @dma_addr:	DMA buffer address
+ * @xfer_len:	data transfer length
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_build_mode_select(struct ipr_cmnd *ipr_cmd,
+				  u32 res_handle, u8 parm, u32 dma_addr,
+				  u8 xfer_len)
+{
+	struct ipr_ioadl_desc *ioadl = ipr_cmd->ioadl;
+	struct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;
+
+	ioarcb->res_handle = res_handle;
+	ioarcb->cmd_pkt.request_type = IPR_RQTYPE_SCSICDB;
+	ioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;
+	ioarcb->cmd_pkt.cdb[0] = MODE_SELECT;
+	ioarcb->cmd_pkt.cdb[1] = parm;
+	ioarcb->cmd_pkt.cdb[4] = xfer_len;
+
+	ioadl->flags_and_data_len =
+		cpu_to_be32(IPR_IOADL_FLAGS_WRITE_LAST | xfer_len);
+	ioadl->address = cpu_to_be32(dma_addr);
+	ioarcb->write_ioadl_len = cpu_to_be32(sizeof(struct ipr_ioadl_desc));
+	ioarcb->write_data_transfer_length = cpu_to_be32(xfer_len);
+}
+
+/**
+ * ipr_ioafp_mode_select_page28 - Issue Mode Select Page 28 to IOA
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function sets up the SCSI bus attributes and sends
+ * a Mode Select for Page 28 to activate them.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_RETURN
+ **/
+static int ipr_ioafp_mode_select_page28(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_mode_pages *mode_pages = &ioa_cfg->vpd_cbs->mode_pages;
+	int length;
+
+	ENTER;
+	if (ioa_cfg->saved_mode_pages) {
+		memcpy(mode_pages, ioa_cfg->saved_mode_pages,
+		       ioa_cfg->saved_mode_page_len);
+		length = ioa_cfg->saved_mode_page_len;
+	} else {
+		ipr_scsi_bus_speed_limit(ioa_cfg);
+		ipr_check_term_power(ioa_cfg, mode_pages);
+		ipr_modify_ioafp_mode_page_28(ioa_cfg, mode_pages);
+		length = mode_pages->hdr.length + 1;
+		mode_pages->hdr.length = 0;
+	}
+
+	ipr_build_mode_select(ipr_cmd, cpu_to_be32(IPR_IOA_RES_HANDLE), 0x11,
+			      ioa_cfg->vpd_cbs_dma + offsetof(struct ipr_misc_cbs, mode_pages),
+			      length);
+
+	ipr_cmd->job_step = ipr_set_supported_devs;
+	ipr_cmd->res = list_entry(ioa_cfg->used_res_q.next,
+				  struct ipr_resource_entry, queue);
+
+	ipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);
+
+	LEAVE;
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_build_mode_sense - Builds a mode sense command
+ * @ipr_cmd:	ipr command struct
+ * @res:		resource entry struct
+ * @parm:		Byte 2 of mode sense command
+ * @dma_addr:	DMA address of mode sense buffer
+ * @xfer_len:	Size of DMA buffer
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_build_mode_sense(struct ipr_cmnd *ipr_cmd,
+				 u32 res_handle,
+				 u8 parm, u32 dma_addr, u8 xfer_len)
+{
+	struct ipr_ioadl_desc *ioadl = ipr_cmd->ioadl;
+	struct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;
+
+	ioarcb->res_handle = res_handle;
+	ioarcb->cmd_pkt.cdb[0] = MODE_SENSE;
+	ioarcb->cmd_pkt.cdb[2] = parm;
+	ioarcb->cmd_pkt.cdb[4] = xfer_len;
+	ioarcb->cmd_pkt.request_type = IPR_RQTYPE_SCSICDB;
+
+	ioadl->flags_and_data_len =
+		cpu_to_be32(IPR_IOADL_FLAGS_READ_LAST | xfer_len);
+	ioadl->address = cpu_to_be32(dma_addr);
+	ioarcb->read_ioadl_len = cpu_to_be32(sizeof(struct ipr_ioadl_desc));
+	ioarcb->read_data_transfer_length = cpu_to_be32(xfer_len);
+}
+
+/**
+ * ipr_ioafp_mode_sense_page28 - Issue Mode Sense Page 28 to IOA
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function send a Page 28 mode sense to the IOA to
+ * retrieve SCSI bus attributes.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_RETURN
+ **/
+static int ipr_ioafp_mode_sense_page28(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+
+	ENTER;
+	ipr_build_mode_sense(ipr_cmd, cpu_to_be32(IPR_IOA_RES_HANDLE),
+			     0x28, ioa_cfg->vpd_cbs_dma +
+			     offsetof(struct ipr_misc_cbs, mode_pages),
+			     sizeof(struct ipr_mode_pages));
+
+	ipr_cmd->job_step = ipr_ioafp_mode_select_page28;
+
+	ipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);
+
+	LEAVE;
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_init_res_table - Initialize the resource table
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function looks through the existing resource table, comparing
+ * it with the config table. This function will take care of old/new
+ * devices and schedule adding/removing them from the mid-layer
+ * as appropriate.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_CONTINUE
+ **/
+static int ipr_init_res_table(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_resource_entry *res, *temp;
+	struct ipr_config_table_entry *cfgte;
+	int found, i;
+	LIST_HEAD(old_res);
+
+	ENTER;
+	if (ioa_cfg->cfg_table->hdr.flags & IPR_UCODE_DOWNLOAD_REQ)
+		dev_err(&ioa_cfg->pdev->dev, "Microcode download required\n");
+
+	list_for_each_entry_safe(res, temp, &ioa_cfg->used_res_q, queue)
+		list_move_tail(&res->queue, &old_res);
+
+	for (i = 0; i < ioa_cfg->cfg_table->hdr.num_entries; i++) {
+		cfgte = &ioa_cfg->cfg_table->dev[i];
+		found = 0;
+
+		list_for_each_entry_safe(res, temp, &old_res, queue) {
+			if (!memcmp(&res->cfgte.res_addr,
+				    &cfgte->res_addr, sizeof(cfgte->res_addr))) {
+				list_move_tail(&res->queue, &ioa_cfg->used_res_q);
+				found = 1;
+				break;
+			}
+		}
+
+		if (!found) {
+			if (list_empty(&ioa_cfg->free_res_q)) {
+				dev_err(&ioa_cfg->pdev->dev, "Too many devices attached\n");
+				break;
+			}
+
+			found = 1;
+			res = list_entry(ioa_cfg->free_res_q.next,
+					 struct ipr_resource_entry, queue);
+			list_move_tail(&res->queue, &ioa_cfg->used_res_q);
+			ipr_init_res_entry(res);
+			if (ioa_cfg->allow_ml_add_del) {
+				ipr_trace;
+				res->add_to_ml = 1;
+			}
+		}
+
+		if (found)
+			memcpy(&res->cfgte, cfgte, sizeof(struct ipr_config_table_entry));
+	}
+
+	list_for_each_entry_safe(res, temp, &old_res, queue) {
+		if (ioa_cfg->allow_ml_add_del && res->sdev) {
+			ipr_trace;
+			res->del_from_ml = 1;
+			list_move_tail(&res->queue, &ioa_cfg->used_res_q);
+		} else {
+			list_move_tail(&res->queue, &ioa_cfg->free_res_q);
+		}
+	}
+
+	ipr_cmd->job_step = ipr_ioafp_mode_sense_page28;
+
+	LEAVE;
+	return IPR_RC_JOB_CONTINUE;
+}
+
+/**
+ * ipr_ioafp_query_ioa_cfg - Send a Query IOA Config to the adapter.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function sends a Query IOA Configuration command
+ * to the adapter to retrieve the IOA configuration table.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_RETURN
+ **/
+static int ipr_ioafp_query_ioa_cfg(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;
+	struct ipr_ioadl_desc *ioadl = ipr_cmd->ioadl;
+
+	ENTER;
+	ioarcb->cmd_pkt.request_type = IPR_RQTYPE_IOACMD;
+	ioarcb->res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);
+
+	ioarcb->cmd_pkt.cdb[0] = IPR_QUERY_IOA_CONFIG;
+	ioarcb->cmd_pkt.cdb[7] = (sizeof(struct ipr_config_table) >> 8) & 0xff;
+	ioarcb->cmd_pkt.cdb[8] = sizeof(struct ipr_config_table) & 0xff;
+
+	ioarcb->read_ioadl_len = cpu_to_be32(sizeof(struct ipr_ioadl_desc));
+	ioarcb->read_data_transfer_length =
+		cpu_to_be32(sizeof(struct ipr_config_table));
+
+	ioadl->address = cpu_to_be32(ioa_cfg->cfg_table_dma);
+	ioadl->flags_and_data_len =
+		cpu_to_be32(IPR_IOADL_FLAGS_READ_LAST | sizeof(struct ipr_config_table));
+
+	ipr_cmd->job_step = ipr_init_res_table;
+
+	ipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);
+
+	LEAVE;
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_ioafp_inquiry - Send an Inquiry to the adapter.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This utility function sends an inquiry to the adapter.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_ioafp_inquiry(struct ipr_cmnd *ipr_cmd, u8 flags, u8 page,
+			      u32 dma_addr, u8 xfer_len)
+{
+	struct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;
+	struct ipr_ioadl_desc *ioadl = ipr_cmd->ioadl;
+
+	ENTER;
+	ioarcb->cmd_pkt.request_type = IPR_RQTYPE_SCSICDB;
+	ioarcb->res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);
+
+	ioarcb->cmd_pkt.cdb[0] = INQUIRY;
+	ioarcb->cmd_pkt.cdb[1] = flags;
+	ioarcb->cmd_pkt.cdb[2] = page;
+	ioarcb->cmd_pkt.cdb[4] = xfer_len;
+
+	ioarcb->read_ioadl_len = cpu_to_be32(sizeof(struct ipr_ioadl_desc));
+	ioarcb->read_data_transfer_length = cpu_to_be32(xfer_len);
+
+	ioadl->address = cpu_to_be32(dma_addr);
+	ioadl->flags_and_data_len =
+		cpu_to_be32(IPR_IOADL_FLAGS_READ_LAST | xfer_len);
+
+	ipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);
+	LEAVE;
+}
+
+/**
+ * ipr_ioafp_page3_inquiry - Send a Page 3 Inquiry to the adapter.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function sends a Page 3 inquiry to the adapter
+ * to retrieve software VPD information.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_CONTINUE / IPR_RC_JOB_RETURN
+ **/
+static int ipr_ioafp_page3_inquiry(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	char type[5];
+
+	ENTER;
+
+	/* Grab the type out of the VPD and store it away */
+	memcpy(type, ioa_cfg->vpd_cbs->ioa_vpd.std_inq_data.vpids.product_id, 4);
+	type[4] = '\0';
+	ioa_cfg->type = simple_strtoul((char *)type, NULL, 16);
+
+	ipr_cmd->job_step = ipr_ioafp_query_ioa_cfg;
+
+	ipr_ioafp_inquiry(ipr_cmd, 1, 3,
+			  ioa_cfg->vpd_cbs_dma + offsetof(struct ipr_misc_cbs, page3_data),
+			  sizeof(struct ipr_inquiry_page3));
+
+	LEAVE;
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_ioafp_std_inquiry - Send a Standard Inquiry to the adapter.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function sends a standard inquiry to the adapter.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_RETURN
+ **/
+static int ipr_ioafp_std_inquiry(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+
+	ENTER;
+	ipr_cmd->job_step = ipr_ioafp_page3_inquiry;
+
+	ipr_ioafp_inquiry(ipr_cmd, 0, 0,
+			  ioa_cfg->vpd_cbs_dma + offsetof(struct ipr_misc_cbs, ioa_vpd),
+			  sizeof(struct ipr_ioa_vpd));
+
+	LEAVE;
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_ioafp_indentify_hrrq - Send Identify Host RRQ.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function send an Identify Host Request Response Queue
+ * command to establish the HRRQ with the adapter.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_RETURN
+ **/
+static int ipr_ioafp_indentify_hrrq(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;
+
+	ENTER;
+	dev_info(&ioa_cfg->pdev->dev, "Starting IOA initialization sequence.\n");
+
+	ioarcb->cmd_pkt.cdb[0] = IPR_ID_HOST_RR_Q;
+	ioarcb->res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);
+
+	ioarcb->cmd_pkt.request_type = IPR_RQTYPE_IOACMD;
+	ioarcb->cmd_pkt.cdb[2] =
+		((u32) ioa_cfg->host_rrq_dma >> 24) & 0xff;
+	ioarcb->cmd_pkt.cdb[3] =
+		((u32) ioa_cfg->host_rrq_dma >> 16) & 0xff;
+	ioarcb->cmd_pkt.cdb[4] =
+		((u32) ioa_cfg->host_rrq_dma >> 8) & 0xff;
+	ioarcb->cmd_pkt.cdb[5] =
+		((u32) ioa_cfg->host_rrq_dma) & 0xff;
+	ioarcb->cmd_pkt.cdb[7] =
+		((sizeof(u32) * IPR_NUM_CMD_BLKS) >> 8) & 0xff;
+	ioarcb->cmd_pkt.cdb[8] =
+		(sizeof(u32) * IPR_NUM_CMD_BLKS) & 0xff;
+
+	ipr_cmd->job_step = ipr_ioafp_std_inquiry;
+
+	ipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);
+
+	LEAVE;
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_reset_timer_done - Adapter reset timer function
+ * @ipr_cmd:	ipr command struct
+ *
+ * Description: This function is used in adapter reset processing
+ * for timing events. If the reset_cmd pointer in the IOA
+ * config struct is not this adapter's we are doing nested
+ * resets and fail_all_ops will take care of freeing the
+ * command block.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_reset_timer_done(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	unsigned long lock_flags = 0;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+
+	if (ioa_cfg->reset_cmd == ipr_cmd) {
+		list_del(&ipr_cmd->queue);
+		ipr_cmd->done(ipr_cmd);
+	}
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+}
+
+/**
+ * ipr_reset_start_timer - Start a timer for adapter reset job
+ * @ipr_cmd:	ipr command struct
+ * @timeout:	timeout value
+ *
+ * Description: This function is used in adapter reset processing
+ * for timing events. If the reset_cmd pointer in the IOA
+ * config struct is not this adapter's we are doing nested
+ * resets and fail_all_ops will take care of freeing the
+ * command block.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_reset_start_timer(struct ipr_cmnd *ipr_cmd,
+				  unsigned long timeout)
+{
+	list_add_tail(&ipr_cmd->queue, &ipr_cmd->ioa_cfg->pending_q);
+	ipr_cmd->done = ipr_reset_ioa_job;
+
+	ipr_cmd->timer.data = (unsigned long) ipr_cmd;
+	ipr_cmd->timer.expires = jiffies + timeout;
+	ipr_cmd->timer.function = (void (*)(unsigned long))ipr_reset_timer_done;
+	add_timer(&ipr_cmd->timer);
+}
+
+/**
+ * ipr_init_ioa_mem - Initialize ioa_cfg control block
+ * @ioa_cfg:	ioa cfg struct
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_init_ioa_mem(struct ipr_ioa_cfg *ioa_cfg)
+{
+	memset(ioa_cfg->host_rrq, 0, sizeof(u32) * IPR_NUM_CMD_BLKS);
+
+	/* Initialize Host RRQ pointers */
+	ioa_cfg->hrrq_start = ioa_cfg->host_rrq;
+	ioa_cfg->hrrq_end = &ioa_cfg->host_rrq[IPR_NUM_CMD_BLKS - 1];
+	ioa_cfg->hrrq_curr = ioa_cfg->hrrq_start;
+	ioa_cfg->toggle_bit = 1;
+
+	/* Zero out config table */
+	memset(ioa_cfg->cfg_table, 0, sizeof(struct ipr_config_table));
+}
+
+/**
+ * ipr_reset_enable_ioa - Enable the IOA following a reset.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function reinitializes some control blocks and
+ * enables destructive diagnostics on the adapter.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_RETURN
+ **/
+static int ipr_reset_enable_ioa(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	volatile u32 temp_reg;
+
+	ENTER;
+	ipr_init_ioa_mem(ioa_cfg);
+
+	/* Enable interrupts */
+	ioa_cfg->allow_interrupts = 1;
+	writel(IPR_PCII_OPER_INTERRUPTS, ioa_cfg->regs.clr_interrupt_mask_reg);
+	temp_reg = readl(ioa_cfg->regs.sense_interrupt_mask_reg);
+
+	/* Enable destructive diagnostics on IOA */
+	writel(IPR_DOORBELL, ioa_cfg->regs.set_uproc_interrupt_reg);
+
+	dev_info(&ioa_cfg->pdev->dev, "Initializing IOA.\n");
+
+	ipr_cmd->timer.data = (unsigned long) ipr_cmd;
+	ipr_cmd->timer.expires = jiffies + IPR_OPERATIONAL_TIMEOUT;
+	ipr_cmd->timer.function = (void (*)(unsigned long))ipr_timeout;
+	add_timer(&ipr_cmd->timer);
+	ipr_cmd->job_step = ipr_ioafp_indentify_hrrq;
+
+	LEAVE;
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_reset_wait_for_dump - Wait for a dump to timeout.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function is invoked when an adapter dump has run out
+ * of processing time.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_CONTINUE
+ **/
+static int ipr_reset_wait_for_dump(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+
+	if (ioa_cfg->sdt_state == GET_DUMP)
+		ioa_cfg->sdt_state = ABORT_DUMP;
+
+	ipr_cmd->job_step = ipr_reset_alert;
+
+	return IPR_RC_JOB_CONTINUE;
+}
+
+/**
+ * ipr_unit_check_no_data - Log a unit check/no data error log
+ * @ioa_cfg:		ioa config struct
+ *
+ * Logs an error indicating the adapter unit checked, but for some
+ * reason, we were unable to fetch the unit check buffer.
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_unit_check_no_data(struct ipr_ioa_cfg *ioa_cfg)
+{
+	ioa_cfg->errors_logged++;
+	dev_err(&ioa_cfg->pdev->dev, "IOA unit check with no data\n");
+}
+
+/**
+ * ipr_get_unit_check_buffer - Get the unit check buffer from the IOA
+ * @ioa_cfg:		ioa config struct
+ *
+ * Fetches the unit check buffer from the adapter by clocking the data
+ * through the mailbox register.
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_get_unit_check_buffer(struct ipr_ioa_cfg *ioa_cfg)
+{
+	unsigned long mailbox;
+	struct ipr_hostrcb *hostrcb;
+	struct ipr_uc_sdt sdt;
+	int rc, length;
+
+	mailbox = readl(ioa_cfg->ioa_mailbox);
+
+	if (!ipr_sdt_is_fmt2(mailbox)) {
+		ipr_unit_check_no_data(ioa_cfg);
+		return;
+	}
+
+	memset(&sdt, 0, sizeof(struct ipr_uc_sdt));
+	rc = ipr_get_ldump_data_section(ioa_cfg, mailbox, (u32 *) &sdt,
+					(sizeof(struct ipr_uc_sdt)) / sizeof(u32));
+
+	if (rc || (be32_to_cpu(sdt.hdr.state) != IPR_FMT2_SDT_READY_TO_USE) ||
+	    !(sdt.entry[0].flags & IPR_SDT_VALID_ENTRY)) {
+		ipr_unit_check_no_data(ioa_cfg);
+		return;
+	}
+
+	/* Find length of the first sdt entry (UC buffer) */
+	length = (be32_to_cpu(sdt.entry[0].end_offset) -
+		  be32_to_cpu(sdt.entry[0].bar_str_offset)) & IPR_FMT2_MBX_ADDR_MASK;
+
+	hostrcb = list_entry(ioa_cfg->hostrcb_free_q.next,
+			     struct ipr_hostrcb, queue);
+	list_del(&hostrcb->queue);
+	memset(hostrcb, 0, sizeof(*hostrcb));
+
+	rc = ipr_get_ldump_data_section(ioa_cfg,
+					be32_to_cpu(sdt.entry[0].bar_str_offset),
+					(u32 *)hostrcb,
+					min(length, (int)(sizeof(*hostrcb) / sizeof(u32))));
+
+	if (!rc)
+		ipr_handle_log_data(ioa_cfg, hostrcb);
+	else
+		ipr_unit_check_no_data(ioa_cfg);
+
+	list_add_tail(&hostrcb->queue, &ioa_cfg->hostrcb_free_q);
+}
+
+/**
+ * ipr_reset_restore_cfg_space - Restore PCI config space.
+ * @ipr_cmd:	ipr command struct
+ *
+ * Description: This function restores the saved PCI config space of
+ * the adapter, fails all outstanding ops back to the callers, and
+ * fetches the dump/unit check if applicable to this reset.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_CONTINUE / IPR_RC_JOB_RETURN
+ **/
+static int ipr_reset_restore_cfg_space(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	int rc;
+
+	ENTER;
+	rc = pci_restore_state(ioa_cfg->pdev, ioa_cfg->pci_cfg_buf);
+
+	if (rc != PCIBIOS_SUCCESSFUL) {
+		ipr_cmd->ioasa.ioasc = cpu_to_be32(IPR_IOASC_PCI_ACCESS_ERROR);
+		return IPR_RC_JOB_CONTINUE;
+	}
+
+	if (ipr_set_pcix_cmd_reg(ioa_cfg)) {
+		ipr_cmd->ioasa.ioasc = cpu_to_be32(IPR_IOASC_PCI_ACCESS_ERROR);
+		return IPR_RC_JOB_CONTINUE;
+	}
+
+	ipr_fail_all_ops(ioa_cfg);
+
+	if (ioa_cfg->ioa_unit_checked) {
+		ioa_cfg->ioa_unit_checked = 0;
+		ipr_get_unit_check_buffer(ioa_cfg);
+		ipr_cmd->job_step = ipr_reset_alert;
+		ipr_reset_start_timer(ipr_cmd, 0);
+		return IPR_RC_JOB_RETURN;
+	}
+
+	if (ioa_cfg->in_ioa_bringdown) {
+		ipr_cmd->job_step = ipr_ioa_bringdown_done;
+	} else {
+		ipr_cmd->job_step = ipr_reset_enable_ioa;
+
+		if (GET_DUMP == ioa_cfg->sdt_state) {
+			ipr_reset_start_timer(ipr_cmd, IPR_DUMP_TIMEOUT);
+			ipr_cmd->job_step = ipr_reset_wait_for_dump;
+			schedule_work(&ioa_cfg->work_q);
+			return IPR_RC_JOB_RETURN;
+		}
+	}
+
+	ENTER;
+	return IPR_RC_JOB_CONTINUE;
+}
+
+/**
+ * ipr_reset_start_bist - Run BIST on the adapter.
+ * @ipr_cmd:	ipr command struct
+ *
+ * Description: This function runs BIST on the adapter, then delays 2 seconds.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_CONTINUE / IPR_RC_JOB_RETURN
+ **/
+static int ipr_reset_start_bist(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	int rc;
+
+	ENTER;
+	rc = pci_write_config_byte(ioa_cfg->pdev, PCI_BIST, PCI_BIST_START);
+
+	if (rc != PCIBIOS_SUCCESSFUL) {
+		ipr_cmd->ioasa.ioasc = cpu_to_be32(IPR_IOASC_PCI_ACCESS_ERROR);
+		rc = IPR_RC_JOB_CONTINUE;
+	} else {
+		ipr_cmd->job_step = ipr_reset_restore_cfg_space;
+		ipr_reset_start_timer(ipr_cmd, IPR_WAIT_FOR_BIST_TIMEOUT);
+		rc = IPR_RC_JOB_RETURN;
+	}
+
+	LEAVE;
+	return rc;
+}
+
+/**
+ * ipr_reset_allowed - Query whether or not IOA can be reset
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ * 	0 if reset not allowed / non-zero if reset is allowed
+ **/
+static int ipr_reset_allowed(struct ipr_ioa_cfg *ioa_cfg)
+{
+	volatile u32 temp_reg;
+
+	temp_reg = readl(ioa_cfg->regs.sense_interrupt_reg);
+	return ((temp_reg & IPR_PCII_CRITICAL_OPERATION) == 0);
+}
+
+/**
+ * ipr_reset_wait_to_start_bist - Wait for permission to reset IOA.
+ * @ipr_cmd:	ipr command struct
+ *
+ * Description: This function waits for adapter permission to run BIST,
+ * then runs BIST. If the adapter does not give permission after a
+ * reasonable time, we will reset the adapter anyway. The impact of
+ * resetting the adapter without warning the adapter is the risk of
+ * losing the persistent error log on the adapter. If the adapter is
+ * reset while it is writing to the flash on the adapter, the flash
+ * segment will have bad ECC and be zeroed.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_CONTINUE / IPR_RC_JOB_RETURN
+ **/
+static int ipr_reset_wait_to_start_bist(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	int rc = IPR_RC_JOB_RETURN;
+
+	if (!ipr_reset_allowed(ioa_cfg) && ipr_cmd->time_left) {
+		ipr_cmd->time_left -= IPR_CHECK_FOR_RESET_TIMEOUT;
+		ipr_reset_start_timer(ipr_cmd, IPR_CHECK_FOR_RESET_TIMEOUT);
+	} else {
+		ipr_cmd->job_step = ipr_reset_start_bist;
+		rc = IPR_RC_JOB_CONTINUE;
+	}
+
+	return rc;
+}
+
+/**
+ * ipr_reset_alert_part2 - Alert the adapter of a pending reset
+ * @ipr_cmd:	ipr command struct
+ *
+ * Description: This function alerts the adapter that it will be reset.
+ * If memory space is not currently enabled, proceed directly
+ * to running BIST on the adapter. The timer must always be started
+ * so we guarantee we do not run BIST from ipr_isr.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_RETURN
+ **/
+static int ipr_reset_alert(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	u16 cmd_reg;
+	int rc;
+
+	ENTER;
+	rc = pci_read_config_word(ioa_cfg->pdev, PCI_COMMAND, &cmd_reg);
+
+	if ((rc == PCIBIOS_SUCCESSFUL) && (cmd_reg & PCI_COMMAND_MEMORY)) {
+		ipr_mask_and_clear_all_interrupts(ioa_cfg);
+		writel(IPR_UPROCI_RESET_ALERT, ioa_cfg->regs.set_uproc_interrupt_reg);
+		ipr_cmd->job_step = ipr_reset_wait_to_start_bist;
+	} else {
+		ipr_cmd->job_step = ipr_reset_start_bist;
+	}
+
+	ipr_cmd->time_left = IPR_WAIT_FOR_RESET_TIMEOUT;
+	ipr_reset_start_timer(ipr_cmd, IPR_CHECK_FOR_RESET_TIMEOUT);
+
+	LEAVE;
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_reset_ucode_download_done - Microcode download completion
+ * @ipr_cmd:	ipr command struct
+ *
+ * Description: This function unmaps the microcode download buffer.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_CONTINUE
+ **/
+static int ipr_reset_ucode_download_done(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_sglist *sglist = ioa_cfg->ucode_sglist;
+
+	pci_unmap_sg(ioa_cfg->pdev, sglist->scatterlist,
+		     sglist->num_sg, DMA_TO_DEVICE);
+
+	ipr_cmd->job_step = ipr_reset_alert;
+	return IPR_RC_JOB_CONTINUE;
+}
+
+/**
+ * ipr_reset_ucode_download - Download microcode to the adapter
+ * @ipr_cmd:	ipr command struct
+ *
+ * Description: This function checks to see if it there is microcode
+ * to download to the adapter. If there is, a download is performed.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_CONTINUE / IPR_RC_JOB_RETURN
+ **/
+static int ipr_reset_ucode_download(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_sglist *sglist = ioa_cfg->ucode_sglist;
+
+	ENTER;
+	ipr_cmd->job_step = ipr_reset_alert;
+
+	if (!sglist)
+		return IPR_RC_JOB_CONTINUE;
+
+	ipr_cmd->ioarcb.res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);
+	ipr_cmd->ioarcb.cmd_pkt.request_type = IPR_RQTYPE_SCSICDB;
+	ipr_cmd->ioarcb.cmd_pkt.cdb[0] = WRITE_BUFFER;
+	ipr_cmd->ioarcb.cmd_pkt.cdb[1] = 5;
+	ipr_cmd->ioarcb.cmd_pkt.cdb[6] = (sglist->buffer_len & 0xff0000) >> 16;
+	ipr_cmd->ioarcb.cmd_pkt.cdb[7] = (sglist->buffer_len & 0x00ff00) >> 8;
+	ipr_cmd->ioarcb.cmd_pkt.cdb[8] = sglist->buffer_len & 0x0000ff;
+
+	if (ipr_map_ucode_buffer(ipr_cmd, sglist, sglist->buffer_len)) {
+		dev_err(&ioa_cfg->pdev->dev,
+			"Failed to map microcode download buffer\n");
+		return IPR_RC_JOB_CONTINUE;
+	}
+
+	ipr_cmd->job_step = ipr_reset_ucode_download_done;
+
+	ipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout,
+		   IPR_WRITE_BUFFER_TIMEOUT);
+
+	LEAVE;
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_reset_shutdown_ioa - Shutdown the adapter
+ * @ipr_cmd:	ipr command struct
+ *
+ * Description: This function issues an adapter shutdown of the
+ * specified type to the specified adapter as part of the
+ * adapter reset job.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_CONTINUE / IPR_RC_JOB_RETURN
+ **/
+static int ipr_reset_shutdown_ioa(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	enum ipr_shutdown_type shutdown_type = ipr_cmd->shutdown_type;
+	unsigned long timeout;
+	int rc = IPR_RC_JOB_CONTINUE;
+
+	ENTER;
+	if (shutdown_type != IPR_SHUTDOWN_NONE && !ioa_cfg->ioa_is_dead) {
+		ipr_cmd->ioarcb.res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);
+		ipr_cmd->ioarcb.cmd_pkt.request_type = IPR_RQTYPE_IOACMD;
+		ipr_cmd->ioarcb.cmd_pkt.cdb[0] = IPR_IOA_SHUTDOWN;
+		ipr_cmd->ioarcb.cmd_pkt.cdb[1] = shutdown_type;
+
+		if (shutdown_type == IPR_SHUTDOWN_ABBREV)
+			timeout = IPR_ABBREV_SHUTDOWN_TIMEOUT;
+		else if (shutdown_type == IPR_SHUTDOWN_PREPARE_FOR_NORMAL)
+			timeout = IPR_INTERNAL_TIMEOUT;
+		else
+			timeout = IPR_SHUTDOWN_TIMEOUT;
+
+		ipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, timeout);
+
+		rc = IPR_RC_JOB_RETURN;
+		ipr_cmd->job_step = ipr_reset_ucode_download;
+	} else
+		ipr_cmd->job_step = ipr_reset_alert;
+
+	LEAVE;
+	return rc;
+}
+
+/**
+ * ipr_reset_ioa_job - Adapter reset job
+ * @ipr_cmd:	ipr command struct
+ *
+ * Description: This function is the job router for the adapter reset job.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_reset_ioa_job(struct ipr_cmnd *ipr_cmd)
+{
+	u32 rc, ioasc;
+	unsigned long scratch = ipr_cmd->scratch;
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+
+	do {
+		ioasc = be32_to_cpu(ipr_cmd->ioasa.ioasc);
+
+		if (ioa_cfg->reset_cmd != ipr_cmd) {
+			/*
+			 * We are doing nested adapter resets and this is
+			 * not the current reset job.
+			 */
+			list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+			return;
+		}
+
+		if (ioasc) {
+			dev_err(&ioa_cfg->pdev->dev,
+				"0x%02X failed with IOASC: 0x%08X\n",
+				ipr_cmd->ioarcb.cmd_pkt.cdb[0], ioasc);
+
+			ipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);
+			list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+			return;
+		}
+
+		ipr_reinit_ipr_cmnd(ipr_cmd);
+		ipr_cmd->scratch = scratch;
+		rc = ipr_cmd->job_step(ipr_cmd);
+	} while(rc == IPR_RC_JOB_CONTINUE);
+}
+
+/**
+ * ipr_initiate_ioa_reset - Initiate an adapter reset
+ * @ioa_cfg:		ioa config struct
+ * @shutdown_type:	shutdown type
+ *
+ * Description: This function will initiate the reset of the given adapter.
+ * If the caller needs to wait on the completion of the reset,
+ * the caller must sleep on the reset_wait_q.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_initiate_ioa_reset(struct ipr_ioa_cfg *ioa_cfg,
+				   enum ipr_shutdown_type shutdown_type)
+{
+	struct ipr_cmnd *ipr_cmd;
+
+	if (ioa_cfg->ioa_is_dead)
+		return;
+
+	if (ioa_cfg->in_reset_reload && ioa_cfg->sdt_state == GET_DUMP)
+		ioa_cfg->sdt_state = ABORT_DUMP;
+
+	if (ioa_cfg->reset_retries++ > IPR_NUM_RESET_RELOAD_RETRIES) {
+		dev_err(&ioa_cfg->pdev->dev,
+			"IOA taken offline - error recovery failed.\n");
+
+		ioa_cfg->reset_retries = 0;
+		ioa_cfg->ioa_is_dead = 1;
+
+		if (ioa_cfg->in_ioa_bringdown) {
+			ioa_cfg->reset_cmd = NULL;
+			ioa_cfg->in_reset_reload = 0;
+			ipr_fail_all_ops(ioa_cfg);
+			wake_up_all(&ioa_cfg->reset_wait_q);
+
+			spin_unlock_irq(ioa_cfg->host->host_lock);
+			scsi_unblock_requests(ioa_cfg->host);
+			spin_lock_irq(ioa_cfg->host->host_lock);
+			return;
+		} else {
+			ioa_cfg->in_ioa_bringdown = 1;
+			shutdown_type = IPR_SHUTDOWN_NONE;
+		}
+	}
+
+	ioa_cfg->in_reset_reload = 1;
+	ioa_cfg->allow_cmds = 0;
+	scsi_block_requests(ioa_cfg->host);
+
+	ipr_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);
+	ioa_cfg->reset_cmd = ipr_cmd;
+	ipr_cmd->job_step = ipr_reset_shutdown_ioa;
+	ipr_cmd->shutdown_type = shutdown_type;
+
+	ipr_reset_ioa_job(ipr_cmd);
+}
+
+/**
+ * ipr_probe_ioa_part2 - Initializes IOAs found in ipr_probe_ioa(..)
+ * @ioa_cfg:	ioa cfg struct
+ *
+ * Description: This is the second phase of adapter intialization
+ * This function takes care of initilizing the adapter to the point
+ * where it can accept new commands.
+
+ * Return value:
+ * 	0 on sucess / -EIO on failure
+ **/
+static int __devinit ipr_probe_ioa_part2(struct ipr_ioa_cfg *ioa_cfg)
+{
+	int rc = 0;
+	unsigned long host_lock_flags = 0;
+
+	ENTER;
+	spin_lock_irqsave(ioa_cfg->host->host_lock, host_lock_flags);
+
+	dev_dbg(&ioa_cfg->pdev->dev, "ioa_cfg adx: 0x%p\n", ioa_cfg);
+
+	ipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, host_lock_flags);
+	wait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);
+	spin_lock_irqsave(ioa_cfg->host->host_lock, host_lock_flags);
+
+	if (ioa_cfg->ioa_is_dead) {
+		rc = -EIO;
+	} else if (ipr_invalid_adapter(ioa_cfg)) {
+		if (!ipr_testmode)
+			rc = -EIO;
+
+		dev_err(&ioa_cfg->pdev->dev,
+			"Adapter not supported in this hardware configuration.\n");
+	}
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, host_lock_flags);
+
+	LEAVE;
+	return rc;
+}
+
+/**
+ * ipr_free_cmd_blks - Frees command blocks allocated for an adapter
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_free_cmd_blks(struct ipr_ioa_cfg *ioa_cfg)
+{
+	int i;
+
+	for (i = 0; i < IPR_NUM_CMD_BLKS; i++) {
+		if (ioa_cfg->ipr_cmnd_list[i])
+			pci_pool_free(ioa_cfg->ipr_cmd_pool,
+				      ioa_cfg->ipr_cmnd_list[i],
+				      ioa_cfg->ipr_cmnd_list_dma[i]);
+
+		ioa_cfg->ipr_cmnd_list[i] = NULL;
+	}
+
+	if (ioa_cfg->ipr_cmd_pool)
+		pci_pool_destroy (ioa_cfg->ipr_cmd_pool);
+
+	ioa_cfg->ipr_cmd_pool = NULL;
+}
+
+/**
+ * ipr_free_mem - Frees memory allocated for an adapter
+ * @ioa_cfg:	ioa cfg struct
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_free_mem(struct ipr_ioa_cfg *ioa_cfg)
+{
+	int i;
+
+	kfree(ioa_cfg->res_entries);
+	pci_free_consistent(ioa_cfg->pdev, sizeof(struct ipr_misc_cbs),
+			    ioa_cfg->vpd_cbs, ioa_cfg->vpd_cbs_dma);
+	ipr_free_cmd_blks(ioa_cfg);
+	pci_free_consistent(ioa_cfg->pdev, sizeof(u32) * IPR_NUM_CMD_BLKS,
+			    ioa_cfg->host_rrq, ioa_cfg->host_rrq_dma);
+	pci_free_consistent(ioa_cfg->pdev, sizeof(struct ipr_config_table),
+			    ioa_cfg->cfg_table,
+			    ioa_cfg->cfg_table_dma);
+
+	for (i = 0; i < IPR_NUM_HCAMS; i++) {
+		pci_free_consistent(ioa_cfg->pdev,
+				    sizeof(struct ipr_hostrcb),
+				    ioa_cfg->hostrcb[i],
+				    ioa_cfg->hostrcb_dma[i]);
+	}
+
+	ipr_free_dump(ioa_cfg);
+	kfree(ioa_cfg->saved_mode_pages);
+	kfree(ioa_cfg->trace);
+}
+
+/**
+ * ipr_free_all_resources - Free all allocated resources for an adapter.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function frees all allocated resources for the
+ * specified adapter.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_free_all_resources(struct ipr_ioa_cfg *ioa_cfg)
+{
+	ENTER;
+	free_irq(ioa_cfg->pdev->irq, ioa_cfg);
+	iounmap((void *) ioa_cfg->hdw_dma_regs);
+	release_mem_region(ioa_cfg->hdw_dma_regs_pci,
+			   pci_resource_len(ioa_cfg->pdev, 0));
+	ipr_free_mem(ioa_cfg);
+	scsi_host_put(ioa_cfg->host);
+	LEAVE;
+}
+
+/**
+ * ipr_alloc_cmd_blks - Allocate command blocks for an adapter
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ * 	0 on success / -ENOMEM on allocation failure
+ **/
+static int __devinit ipr_alloc_cmd_blks(struct ipr_ioa_cfg *ioa_cfg)
+{
+	struct ipr_cmnd *ipr_cmd;
+	struct ipr_ioarcb *ioarcb;
+	u32 dma_addr;
+	int i;
+
+	ioa_cfg->ipr_cmd_pool = pci_pool_create (IPR_NAME, ioa_cfg->pdev,
+						 sizeof(struct ipr_cmnd), 8, 0);
+
+	if (!ioa_cfg->ipr_cmd_pool)
+		return -ENOMEM;
+
+	for (i = 0; i < IPR_NUM_CMD_BLKS; i++) {
+		ipr_cmd = pci_pool_alloc (ioa_cfg->ipr_cmd_pool, SLAB_KERNEL, &dma_addr);
+
+		if (!ipr_cmd) {
+			ipr_free_cmd_blks(ioa_cfg);
+			return -ENOMEM;
+		}
+
+		memset(ipr_cmd, 0, sizeof(*ipr_cmd));
+		ioa_cfg->ipr_cmnd_list[i] = ipr_cmd;
+		ioa_cfg->ipr_cmnd_list_dma[i] = dma_addr;
+
+		ioarcb = &ipr_cmd->ioarcb;
+		ioarcb->ioarcb_host_pci_addr = cpu_to_be32(dma_addr);
+		ioarcb->host_response_handle = cpu_to_be32(i << 2);
+		ioarcb->write_ioadl_addr =
+			cpu_to_be32(dma_addr + offsetof(struct ipr_cmnd, ioadl));
+		ioarcb->read_ioadl_addr = ioarcb->write_ioadl_addr;
+		ioarcb->ioasa_host_pci_addr =
+			cpu_to_be32(dma_addr + offsetof(struct ipr_cmnd, ioasa));
+		ioarcb->ioasa_len = cpu_to_be16(sizeof(struct ipr_ioasa));
+		ipr_cmd->cmd_index = i;
+		ipr_cmd->ioa_cfg = ioa_cfg;
+		ipr_cmd->sense_buffer_dma = dma_addr +
+			offsetof(struct ipr_cmnd, sense_buffer);
+
+		list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+	}
+
+	return 0;
+}
+
+/**
+ * ipr_alloc_mem - Allocate memory for an adapter
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ * 	0 on success / non-zero for error
+ **/
+static int __devinit ipr_alloc_mem(struct ipr_ioa_cfg *ioa_cfg)
+{
+	int i;
+
+	ENTER;
+	ioa_cfg->res_entries = kmalloc(sizeof(struct ipr_resource_entry) *
+				       IPR_MAX_PHYSICAL_DEVS, GFP_KERNEL);
+
+	if (!ioa_cfg->res_entries)
+		goto cleanup;
+
+	memset(ioa_cfg->res_entries, 0,
+	       sizeof(struct ipr_resource_entry) * IPR_MAX_PHYSICAL_DEVS);
+
+	for (i = 0; i < IPR_MAX_PHYSICAL_DEVS; i++)
+		list_add_tail(&ioa_cfg->res_entries[i].queue, &ioa_cfg->free_res_q);
+
+	ioa_cfg->vpd_cbs = pci_alloc_consistent(ioa_cfg->pdev,
+						sizeof(struct ipr_misc_cbs),
+						&ioa_cfg->vpd_cbs_dma);
+
+	if (!ioa_cfg->vpd_cbs)
+		goto cleanup;
+
+	if (ipr_alloc_cmd_blks(ioa_cfg))
+		goto cleanup;
+
+	ioa_cfg->host_rrq = pci_alloc_consistent(ioa_cfg->pdev,
+						 sizeof(u32) * IPR_NUM_CMD_BLKS,
+						 &ioa_cfg->host_rrq_dma);
+
+	if (!ioa_cfg->host_rrq)
+		goto cleanup;
+
+	ioa_cfg->cfg_table = pci_alloc_consistent(ioa_cfg->pdev,
+						  sizeof(struct ipr_config_table),
+						  &ioa_cfg->cfg_table_dma);
+
+	if (!ioa_cfg->cfg_table)
+		goto cleanup;
+
+	for (i = 0; i < IPR_NUM_HCAMS; i++) {
+		ioa_cfg->hostrcb[i] = pci_alloc_consistent(ioa_cfg->pdev,
+							   sizeof(struct ipr_hostrcb),
+							   &ioa_cfg->hostrcb_dma[i]);
+
+		if (!ioa_cfg->hostrcb[i])
+			goto cleanup;
+
+		memset(ioa_cfg->hostrcb[i], 0, sizeof(struct ipr_hostrcb));
+		ioa_cfg->hostrcb[i]->hostrcb_dma = ioa_cfg->hostrcb_dma[i];
+		list_add_tail(&ioa_cfg->hostrcb[i]->queue, &ioa_cfg->hostrcb_free_q);
+	}
+
+	ioa_cfg->trace = kmalloc(sizeof(struct ipr_trace_entry) *
+				 IPR_NUM_TRACE_ENTRIES, GFP_KERNEL);
+
+	if (!ioa_cfg->trace)
+		goto cleanup;
+
+	memset(ioa_cfg->trace, 0,
+	       sizeof(struct ipr_trace_entry) * IPR_NUM_TRACE_ENTRIES);
+
+	LEAVE;
+	return 0;
+
+cleanup:
+	ipr_free_mem(ioa_cfg);
+
+	LEAVE;
+	return -ENOMEM;
+}
+
+/**
+ * ipr_initialize_bus_attr - Initialize SCSI bus attributes to default values
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void __devinit ipr_initialize_bus_attr(struct ipr_ioa_cfg *ioa_cfg)
+{
+	int i;
+
+	for (i = 0; i < IPR_MAX_NUM_BUSES; i++) {
+		ioa_cfg->bus_attr[i].bus = i;
+		ioa_cfg->bus_attr[i].qas_enabled = 0;
+		ioa_cfg->bus_attr[i].bus_width = IPR_DEFAULT_BUS_WIDTH;
+		if (ipr_max_speed < ARRAY_SIZE(ipr_max_bus_speeds))
+			ioa_cfg->bus_attr[i].max_xfer_rate = ipr_max_bus_speeds[ipr_max_speed];
+		else
+			ioa_cfg->bus_attr[i].max_xfer_rate = IPR_U160_SCSI_RATE;
+	}
+}
+
+/**
+ * ipr_init_ioa_cfg - Initialize IOA config struct
+ * @ioa_cfg:	ioa config struct
+ * @host:		scsi host struct
+ * @pdev:		PCI dev struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void __devinit ipr_init_ioa_cfg(struct ipr_ioa_cfg *ioa_cfg,
+				       struct Scsi_Host *host, struct pci_dev *pdev)
+{
+	ioa_cfg->host = host;
+	ioa_cfg->pdev = pdev;
+	ioa_cfg->log_level = ipr_log_level;
+	sprintf(ioa_cfg->eye_catcher, IPR_EYECATCHER);
+	sprintf(ioa_cfg->trace_start, IPR_TRACE_START_LABEL);
+	sprintf(ioa_cfg->ipr_free_label, IPR_FREEQ_LABEL);
+	sprintf(ioa_cfg->ipr_pending_label, IPR_PENDQ_LABEL);
+	sprintf(ioa_cfg->cfg_table_start, IPR_CFG_TBL_START);
+	sprintf(ioa_cfg->resource_table_label, IPR_RES_TABLE_LABEL);
+	sprintf(ioa_cfg->ipr_hcam_label, IPR_HCAM_LABEL);
+	sprintf(ioa_cfg->ipr_cmd_label, IPR_CMD_LABEL);
+
+	INIT_LIST_HEAD(&ioa_cfg->free_q);
+	INIT_LIST_HEAD(&ioa_cfg->pending_q);
+	INIT_LIST_HEAD(&ioa_cfg->hostrcb_free_q);
+	INIT_LIST_HEAD(&ioa_cfg->hostrcb_pending_q);
+	INIT_LIST_HEAD(&ioa_cfg->free_res_q);
+	INIT_LIST_HEAD(&ioa_cfg->used_res_q);
+	INIT_WORK(&ioa_cfg->work_q, ipr_worker_thread, ioa_cfg);
+	init_waitqueue_head(&ioa_cfg->reset_wait_q);
+	ioa_cfg->sdt_state = INACTIVE;
+
+	ipr_initialize_bus_attr(ioa_cfg);
+
+	host->max_id = IPR_MAX_NUM_TARGETS_PER_BUS;
+	host->max_lun = IPR_MAX_NUM_LUNS_PER_TARGET;
+	host->max_channel = IPR_MAX_BUS_TO_SCAN;
+	host->unique_id = host->host_no;
+	host->max_cmd_len = IPR_MAX_CDB_LEN;
+	pci_set_drvdata(pdev, ioa_cfg);
+
+	memcpy(&ioa_cfg->regs, &ioa_cfg->chip_cfg->regs, sizeof(ioa_cfg->regs));
+
+	ioa_cfg->regs.set_interrupt_mask_reg += ioa_cfg->hdw_dma_regs;
+	ioa_cfg->regs.clr_interrupt_mask_reg += ioa_cfg->hdw_dma_regs;
+	ioa_cfg->regs.sense_interrupt_mask_reg += ioa_cfg->hdw_dma_regs;
+	ioa_cfg->regs.clr_interrupt_reg += ioa_cfg->hdw_dma_regs;
+	ioa_cfg->regs.sense_interrupt_reg += ioa_cfg->hdw_dma_regs;
+	ioa_cfg->regs.ioarrin_reg += ioa_cfg->hdw_dma_regs;
+	ioa_cfg->regs.sense_uproc_interrupt_reg += ioa_cfg->hdw_dma_regs;
+	ioa_cfg->regs.set_uproc_interrupt_reg += ioa_cfg->hdw_dma_regs;
+	ioa_cfg->regs.clr_uproc_interrupt_reg += ioa_cfg->hdw_dma_regs;
+}
+
+/**
+ * ipr_probe_ioa - Allocates memory and does first stage of initialization
+ * @pdev:		PCI device struct
+ * @dev_id:		PCI device id struct
+ *
+ * Return value:
+ * 	0 on success / non-zero on failure
+ **/
+static int __devinit ipr_probe_ioa(struct pci_dev *pdev,
+				   const struct pci_device_id *dev_id)
+{
+	struct ipr_ioa_cfg *ioa_cfg;
+	struct Scsi_Host *host;
+	unsigned long ipr_regs, ipr_regs_pci;
+	u32 rc = PCIBIOS_SUCCESSFUL;
+
+	ENTER;
+
+	if ((rc = pci_enable_device(pdev))) {
+		dev_err(&pdev->dev, "Cannot enable adapter\n");
+		return rc;
+	}
+
+	dev_info(&pdev->dev, "Found IOA with IRQ: %d\n", pdev->irq);
+
+	host = scsi_host_alloc(&driver_template, sizeof(*ioa_cfg));
+
+	if (!host) {
+		dev_err(&pdev->dev, "call to scsi_host_alloc failed!\n");
+		return -ENOMEM;
+	}
+
+	ioa_cfg = (struct ipr_ioa_cfg *)host->hostdata;
+	memset(ioa_cfg, 0, sizeof(struct ipr_ioa_cfg));
+
+	ioa_cfg->chip_cfg = (const struct ipr_chip_cfg_t *)dev_id->driver_data;
+
+	ipr_regs_pci = pci_resource_start(pdev, 0);
+
+	if (!request_mem_region(ipr_regs_pci,
+				pci_resource_len(pdev, 0), IPR_NAME)) {
+		dev_err(&pdev->dev,
+			"Couldn't register memory range of registers\n");
+		scsi_host_put(host);
+		return -ENOMEM;
+	}
+
+	ipr_regs = (unsigned long)ioremap(ipr_regs_pci,
+					  pci_resource_len(pdev, 0));
+
+	if (!ipr_regs) {
+		dev_err(&pdev->dev,
+			"Couldn't map memory range of registers\n");
+		release_mem_region(ipr_regs_pci, pci_resource_len(pdev, 0));
+		scsi_host_put(host);
+		return -ENOMEM;
+	}
+
+	ioa_cfg->hdw_dma_regs = ipr_regs;
+	ioa_cfg->hdw_dma_regs_pci = ipr_regs_pci;
+	ioa_cfg->ioa_mailbox = ioa_cfg->chip_cfg->mailbox + ipr_regs;
+
+	ipr_init_ioa_cfg(ioa_cfg, host, pdev);
+
+	pci_set_master(pdev);
+	rc = pci_set_dma_mask(pdev, 0xffffffff);
+
+	if (rc != PCIBIOS_SUCCESSFUL) {
+		dev_err(&pdev->dev, "Failed to set PCI DMA mask\n");
+		rc = -EIO;
+		goto cleanup_nomem;
+	}
+
+	rc = pci_write_config_byte(pdev, PCI_CACHE_LINE_SIZE,
+				   ioa_cfg->chip_cfg->cache_line_size);
+
+	if (rc != PCIBIOS_SUCCESSFUL) {
+		dev_err(&pdev->dev, "Write of cache line size failed\n");
+		rc = -EIO;
+		goto cleanup_nomem;
+	}
+
+	/* Save away PCI config space for use following IOA reset */
+	rc = pci_save_state(pdev, ioa_cfg->pci_cfg_buf);
+
+	if (rc != PCIBIOS_SUCCESSFUL) {
+		dev_err(&pdev->dev, "Failed to save PCI config space\n");
+		rc = -EIO;
+		goto cleanup_nomem;
+	}
+
+	if ((rc = ipr_save_pcix_cmd_reg(ioa_cfg)))
+		goto cleanup_nomem;
+
+	if ((rc = ipr_alloc_mem(ioa_cfg)))
+		goto cleanup;
+
+	ipr_mask_and_clear_all_interrupts(ioa_cfg);
+
+	rc = request_irq(pdev->irq, ipr_isr, SA_SHIRQ, IPR_NAME, ioa_cfg);
+
+	if (rc) {
+		dev_err(&pdev->dev, "Couldn't register IRQ %d! rc=%d\n",
+			pdev->irq, rc);
+		goto cleanup_nolog;
+	}
+
+	spin_lock(&ipr_driver_lock);
+	list_add_tail(&ioa_cfg->queue, &ipr_ioa_head);
+	spin_unlock(&ipr_driver_lock);
+
+	LEAVE;
+	return 0;
+
+cleanup:
+	dev_err(&pdev->dev, "Couldn't allocate enough memory for device driver!\n");
+cleanup_nolog:
+	ipr_free_mem(ioa_cfg);
+cleanup_nomem:
+	iounmap((void *) ipr_regs);
+	release_mem_region(ipr_regs_pci, pci_resource_len(pdev, 0));
+	scsi_host_put(host);
+
+	return rc;
+}
+
+/**
+ * ipr_scan_vsets - Scans for VSET devices
+ * @ioa_cfg:	ioa config struct
+ *
+ * Description: Since the VSET resources do not follow SAM in that we can have
+ * sparse LUNs with no LUN 0, we have to scan for these ourselves.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_scan_vsets(struct ipr_ioa_cfg *ioa_cfg)
+{
+	int target, lun;
+
+	for (target = 0; target < IPR_MAX_NUM_TARGETS_PER_BUS; target++)
+		for (lun = 0; lun < IPR_MAX_NUM_VSET_LUNS_PER_TARGET; lun++ )
+			scsi_add_device(ioa_cfg->host, IPR_VSET_BUS, target, lun);
+}
+
+/**
+ * ipr_initiate_ioa_bringdown - Bring down an adapter
+ * @ioa_cfg:		ioa config struct
+ * @shutdown_type:	shutdown type
+ *
+ * Description: This function will initiate bringing down the adapter.
+ * This consists of issuing an IOA shutdown to the adapter
+ * to flush the cache, and running BIST.
+ * If the caller needs to wait on the completion of the reset,
+ * the caller must sleep on the reset_wait_q.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_initiate_ioa_bringdown(struct ipr_ioa_cfg *ioa_cfg,
+				       enum ipr_shutdown_type shutdown_type)
+{
+	ENTER;
+	if (ioa_cfg->sdt_state == WAIT_FOR_DUMP)
+		ioa_cfg->sdt_state = ABORT_DUMP;
+	ioa_cfg->reset_retries = 0;
+	ioa_cfg->in_ioa_bringdown = 1;
+	ipr_initiate_ioa_reset(ioa_cfg, shutdown_type);
+	LEAVE;
+}
+
+/**
+ * __ipr_remove - Remove a single adapter
+ * @pdev:	pci device struct
+ *
+ * Adapter hot plug remove entry point.
+ *
+ * Return value:
+ * 	none
+ **/
+static void __ipr_remove(struct pci_dev *pdev)
+{
+	unsigned long host_lock_flags = 0;
+	struct ipr_ioa_cfg *ioa_cfg = pci_get_drvdata(pdev);
+	ENTER;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, host_lock_flags);
+	ipr_initiate_ioa_bringdown(ioa_cfg, IPR_SHUTDOWN_NORMAL);
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, host_lock_flags);
+	wait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);
+	spin_lock_irqsave(ioa_cfg->host->host_lock, host_lock_flags);
+
+	spin_lock(&ipr_driver_lock);
+	list_del(&ioa_cfg->queue);
+	spin_unlock(&ipr_driver_lock);
+
+	if (ioa_cfg->sdt_state == ABORT_DUMP)
+		ioa_cfg->sdt_state = WAIT_FOR_DUMP;
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, host_lock_flags);
+
+	ipr_free_all_resources(ioa_cfg);
+
+	LEAVE;
+}
+
+/**
+ * ipr_remove - IOA hot plug remove entry point
+ * @pdev:	pci device struct
+ *
+ * Adapter hot plug remove entry point.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_remove(struct pci_dev *pdev)
+{
+	struct ipr_ioa_cfg *ioa_cfg = pci_get_drvdata(pdev);
+
+	ENTER;
+
+	ioa_cfg->allow_cmds = 0;
+	flush_scheduled_work();
+	ipr_remove_trace_file(&ioa_cfg->host->shost_classdev.kobj,
+			      &ipr_trace_attr);
+	ipr_remove_dump_file(&ioa_cfg->host->shost_classdev.kobj,
+			     &ipr_dump_attr);
+	scsi_remove_host(ioa_cfg->host);
+
+	__ipr_remove(pdev);
+
+	LEAVE;
+}
+
+/**
+ * ipr_probe - Adapter hot plug add entry point
+ *
+ * Return value:
+ * 	0 on success / non-zero on failure
+ **/
+static int __devinit ipr_probe(struct pci_dev *pdev,
+			       const struct pci_device_id *dev_id)
+{
+	struct ipr_ioa_cfg *ioa_cfg;
+	int rc;
+
+	rc = ipr_probe_ioa(pdev, dev_id);
+
+	if (rc)
+		return rc;
+
+	ioa_cfg = pci_get_drvdata(pdev);
+	rc = ipr_probe_ioa_part2(ioa_cfg);
+
+	if (rc) {
+		__ipr_remove(pdev);
+		return rc;
+	}
+
+	rc = scsi_add_host(ioa_cfg->host, &pdev->dev);
+
+	if (rc) {
+		__ipr_remove(pdev);
+		return rc;
+	}
+
+	rc = ipr_create_trace_file(&ioa_cfg->host->shost_classdev.kobj,
+				   &ipr_trace_attr);
+
+	if (rc) {
+		scsi_remove_host(ioa_cfg->host);
+		__ipr_remove(pdev);
+		return rc;
+	}
+
+	rc = ipr_create_dump_file(&ioa_cfg->host->shost_classdev.kobj,
+				   &ipr_dump_attr);
+
+	if (rc) {
+		ipr_remove_trace_file(&ioa_cfg->host->shost_classdev.kobj,
+				      &ipr_trace_attr);
+		scsi_remove_host(ioa_cfg->host);
+		__ipr_remove(pdev);
+		return rc;
+	}
+
+	ioa_cfg->allow_ml_add_del = 1;
+	scsi_scan_host(ioa_cfg->host);
+	ipr_scan_vsets(ioa_cfg);
+	scsi_add_device(ioa_cfg->host, IPR_IOA_BUS, IPR_IOA_TARGET, IPR_IOA_LUN);
+	return 0;
+}
+
+/**
+ * ipr_shutdown - Shutdown handler.
+ * @dev:	device struct
+ *
+ * This function is invoked upon system shutdown/reboot. It will issue
+ * an adapter shutdown to the adapter to flush the write cache.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_shutdown(struct device *dev)
+{
+	struct ipr_ioa_cfg *ioa_cfg = pci_get_drvdata(to_pci_dev(dev));
+	unsigned long lock_flags = 0;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	ipr_initiate_ioa_bringdown(ioa_cfg, IPR_SHUTDOWN_NORMAL);
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	wait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);
+}
+
+static struct pci_device_id ipr_pci_table[] __devinitdata = {
+	{ PCI_VENDOR_ID_MYLEX, PCI_DEVICE_ID_IBM_GEMSTONE,
+		PCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_5702,
+		0, 0, (kernel_ulong_t)&ipr_chip_cfg[0] },
+	{ PCI_VENDOR_ID_MYLEX, PCI_DEVICE_ID_IBM_GEMSTONE,
+		PCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_5703,
+	      0, 0, (kernel_ulong_t)&ipr_chip_cfg[0] },
+	{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_SNIPE,
+		PCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_2780,
+		0, 0, (kernel_ulong_t)&ipr_chip_cfg[1] },
+	{ }
+};
+MODULE_DEVICE_TABLE(pci, ipr_pci_table);
+
+static struct pci_driver ipr_driver = {
+	.name = IPR_NAME,
+	.id_table = ipr_pci_table,
+	.probe = ipr_probe,
+	.remove = ipr_remove,
+	.driver = {
+		.shutdown = ipr_shutdown,
+	},
+};
+
+/**
+ * ipr_init - Module entry point
+ *
+ * Return value:
+ * 	0 on success / non-zero on failure
+ **/
+static int __init ipr_init(void)
+{
+	ipr_info("IBM Power RAID SCSI Device Driver version: %s\n",
+		 IPR_DRIVER_VERSION);
+
+	pci_register_driver(&ipr_driver);
+
+	return 0;
+}
+
+/**
+ * ipr_exit - Module unload
+ *
+ * Module unload entry point.
+ *
+ * Return value:
+ * 	none
+ **/
+static void __exit ipr_exit(void)
+{
+	pci_unregister_driver(&ipr_driver);
+}
+
+module_init(ipr_init);
+module_exit(ipr_exit);
diff -purN linux-2.5/drivers/scsi/ipr.h linuxppc64-2.5/drivers/scsi/ipr.h
--- linux-2.5/drivers/scsi/ipr.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/ipr.h	2004-03-01 19:33:24.000000000 +0000
@@ -0,0 +1,1245 @@
+/*
+ * ipr.h -- driver for IBM Power Linux RAID adapters
+ *
+ * Written By: Brian King, IBM Corporation
+ *
+ * Copyright (C) 2003, 2004 IBM Corporation
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ *
+ */
+
+#ifndef _IPR_H
+#define _IPR_H
+
+#include <linux/types.h>
+#include <linux/completion.h>
+#include <linux/list.h>
+#include <scsi/scsi.h>
+#include <scsi/scsi_cmnd.h>
+#ifdef CONFIG_KDB
+#include <asm/kdb.h>
+#endif
+
+/*
+ * Literals
+ */
+#define IPR_DRIVER_VERSION "2.0.0-7 (February 27, 2004)"
+
+/*
+ * IPR_DBG_TRACE: Setting this to 1 will turn on some general function tracing
+ *			resulting in a bunch of extra debugging printks to the console
+ *
+ * IPR_DEBUG:	Setting this to 1 will turn on some error path tracing.
+ *			Enables the ipr_trace macro.
+ */
+#ifdef IPR_DEBUG_ALL
+#define IPR_DEBUG				1
+#define IPR_DBG_TRACE			1
+#else
+#define IPR_DEBUG				0
+#define IPR_DBG_TRACE			0
+#endif
+
+/*
+ * IPR_MAX_CMD_PER_LUN: This defines the maximum number of outstanding
+ *	ops per device for devices not running tagged command queuing.
+ *	This can be adjusted at runtime through sysfs device attributes.
+ */
+#define IPR_MAX_CMD_PER_LUN				6
+
+/*
+ * IPR_NUM_BASE_CMD_BLKS: This defines the maximum number of
+ *	ops the mid-layer can send to the adapter.
+ */
+#define IPR_NUM_BASE_CMD_BLKS				100
+
+#define IPR_SUBS_DEV_ID_2780	0x0264
+#define IPR_SUBS_DEV_ID_5702	0x0266
+#define IPR_SUBS_DEV_ID_5703	0x0278
+
+#define IPR_NAME				"ipr"
+
+/*
+ * Return codes
+ */
+#define IPR_RC_JOB_CONTINUE		1
+#define IPR_RC_JOB_RETURN		2
+
+/* FIXME - these should exist on all architectures */
+#ifndef pci_dma_error
+#if defined(CONFIG_PPC64)
+#define pci_dma_error(x) ((x) == ((dma_addr_t)-1))
+#else
+#define pci_dma_error(x) (0)
+#endif
+#endif
+
+/*
+ * IOASCs
+ */
+#define IPR_IOASC_NR_INIT_CMD_REQUIRED		0x02040200
+#define IPR_IOASC_SYNC_REQUIRED			0x023f0000
+#define IPR_IOASC_MED_DO_NOT_REALLOC		0x03110C00
+#define IPR_IOASC_HW_SEL_TIMEOUT			0x04050000
+#define IPR_IOASC_HW_DEV_BUS_STATUS			0x04448500
+#define	IPR_IOASC_IOASC_MASK			0xFFFFFF00
+#define	IPR_IOASC_SCSI_STATUS_MASK		0x000000FF
+#define IPR_IOASC_IR_RESOURCE_HANDLE		0x05250000
+#define IPR_IOASC_BUS_WAS_RESET			0x06290000
+#define IPR_IOASC_BUS_WAS_RESET_BY_OTHER		0x06298000
+#define IPR_IOASC_ABORTED_CMD_TERM_BY_HOST	0x0B5A0000
+
+#define IPR_FIRST_DRIVER_IOASC			0x10000000
+#define IPR_IOASC_IOA_WAS_RESET			0x10000001
+#define IPR_IOASC_PCI_ACCESS_ERROR			0x10000002
+
+#define IPR_NUM_LOG_HCAMS				2
+#define IPR_NUM_CFG_CHG_HCAMS				2
+#define IPR_NUM_HCAMS	(IPR_NUM_LOG_HCAMS + IPR_NUM_CFG_CHG_HCAMS)
+#define IPR_MAX_NUM_TARGETS_PER_BUS			0x10
+#define IPR_MAX_NUM_LUNS_PER_TARGET			256
+#define IPR_MAX_NUM_VSET_LUNS_PER_TARGET	8
+#define IPR_VSET_BUS					0xff
+#define IPR_IOA_BUS						0xff
+#define IPR_IOA_TARGET					0xff
+#define IPR_IOA_LUN						0xff
+#define IPR_MAX_NUM_BUSES				4
+#define IPR_MAX_BUS_TO_SCAN				IPR_MAX_NUM_BUSES
+
+#define IPR_NUM_RESET_RELOAD_RETRIES		3
+
+/* We need resources for HCAMS, IOA reset, IOA bringdown, and ERP */
+#define IPR_NUM_INTERNAL_CMD_BLKS	(IPR_NUM_HCAMS + \
+                                     ((IPR_NUM_RESET_RELOAD_RETRIES + 1) * 2) + 3)
+
+#define IPR_MAX_COMMANDS		IPR_NUM_BASE_CMD_BLKS
+#define IPR_NUM_CMD_BLKS		(IPR_NUM_BASE_CMD_BLKS + \
+						IPR_NUM_INTERNAL_CMD_BLKS)
+
+#define IPR_MAX_PHYSICAL_DEVS				192
+
+#define IPR_MAX_SGLIST					64
+#define IPR_MAX_SECTORS					512
+#define IPR_MAX_CDB_LEN					16
+
+#define IPR_DEFAULT_BUS_WIDTH				16
+#define IPR_80MBs_SCSI_RATE		((80 * 10) / (IPR_DEFAULT_BUS_WIDTH / 8))
+#define IPR_U160_SCSI_RATE	((160 * 10) / (IPR_DEFAULT_BUS_WIDTH / 8))
+#define IPR_U320_SCSI_RATE	((320 * 10) / (IPR_DEFAULT_BUS_WIDTH / 8))
+#define IPR_MAX_SCSI_RATE(width) ((320 * 10) / ((width) / 8))
+
+#define IPR_IOA_RES_HANDLE				0xffffffff
+#define IPR_IOA_RES_ADDR				0x00ffffff
+
+/*
+ * Adapter Commands
+ */
+#define IPR_RESET_DEVICE				0xC3
+#define	IPR_RESET_TYPE_SELECT				0x80
+#define	IPR_LUN_RESET					0x40
+#define	IPR_TARGET_RESET					0x20
+#define	IPR_BUS_RESET					0x10
+#define IPR_ID_HOST_RR_Q				0xC4
+#define IPR_QUERY_IOA_CONFIG				0xC5
+#define IPR_ABORT_TASK					0xC7
+#define IPR_CANCEL_ALL_REQUESTS			0xCE
+#define IPR_HOST_CONTROLLED_ASYNC			0xCF
+#define	IPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE	0x01
+#define	IPR_HCAM_CDB_OP_CODE_LOG_DATA		0x02
+#define IPR_SET_SUPPORTED_DEVICES			0xFB
+#define IPR_IOA_SHUTDOWN				0xF7
+#define	IPR_WR_BUF_DOWNLOAD_AND_SAVE			0x05
+
+/*
+ * Timeouts
+ */
+#define IPR_TIMEOUT_MULTIPLIER		2
+#define IPR_MAX_SECOND_RADIX_TIMEOUT	0x3fff
+#define IPR_TIMEOUT_MINUTE_RADIX		0x4000
+#define IPR_SHUTDOWN_TIMEOUT			(10 * 60 * HZ)
+#define IPR_ABBREV_SHUTDOWN_TIMEOUT		(10 * HZ)
+#define IPR_DEVICE_RESET_TIMEOUT		(30 * HZ)
+#define IPR_CANCEL_ALL_TIMEOUT		(30 * HZ)
+#define IPR_ABORT_TASK_TIMEOUT		(30 * HZ)
+#define IPR_INTERNAL_TIMEOUT			(30 * HZ)
+#define IPR_WRITE_BUFFER_TIMEOUT		(10 * 60 * HZ)
+#define IPR_SET_SUP_DEVICE_TIMEOUT		(2 * 60 * HZ)
+#define IPR_REQUEST_SENSE_TIMEOUT		(30 * HZ)
+#define IPR_OPERATIONAL_TIMEOUT		(5 * 60 * HZ)
+#define IPR_WAIT_FOR_RESET_TIMEOUT		(2 * HZ)
+#define IPR_CHECK_FOR_RESET_TIMEOUT		(HZ / 10)
+#define IPR_WAIT_FOR_BIST_TIMEOUT		(2 * HZ)
+#define IPR_DUMP_TIMEOUT			(15 * HZ)
+
+/*
+ * SCSI Literals
+ */
+#define IPR_VENDOR_ID_LEN			8
+#define IPR_PROD_ID_LEN				16
+#define IPR_SERIAL_NUM_LEN			8
+
+/*
+ * Hardware literals
+ */
+#define IPR_FMT2_MBX_ADDR_MASK				0x0fffffff
+#define IPR_FMT2_MBX_BAR_SEL_MASK			0xf0000000
+#define IPR_FMT2_MKR_BAR_SEL_SHIFT			28
+#define IPR_GET_FMT2_BAR_SEL(mbx) \
+(((mbx) & IPR_FMT2_MBX_BAR_SEL_MASK) >> IPR_FMT2_MKR_BAR_SEL_SHIFT)
+#define IPR_SDT_FMT2_BAR0_SEL				0x0
+#define IPR_SDT_FMT2_BAR1_SEL				0x1
+#define IPR_SDT_FMT2_BAR2_SEL				0x2
+#define IPR_SDT_FMT2_BAR3_SEL				0x3
+#define IPR_SDT_FMT2_BAR4_SEL				0x4
+#define IPR_SDT_FMT2_BAR5_SEL				0x5
+#define IPR_SDT_FMT2_EXP_ROM_SEL			0x8
+#define IPR_FMT2_SDT_READY_TO_USE			0xC4D4E3F2
+#define IPR_DOORBELL					0x82800000
+
+#define IPR_PCII_IOA_TRANS_TO_OPER			(0x80000000 >> 0)
+#define IPR_PCII_IOARCB_XFER_FAILED			(0x80000000 >> 3)
+#define IPR_PCII_IOA_UNIT_CHECKED			(0x80000000 >> 4)
+#define IPR_PCII_NO_HOST_RRQ				(0x80000000 >> 5)
+#define IPR_PCII_CRITICAL_OPERATION			(0x80000000 >> 6)
+#define IPR_PCII_IO_DEBUG_ACKNOWLEDGE		(0x80000000 >> 7)
+#define IPR_PCII_IOARRIN_LOST				(0x80000000 >> 27)
+#define IPR_PCII_MMIO_ERROR				(0x80000000 >> 28)
+#define IPR_PCII_PROC_ERR_STATE			(0x80000000 >> 29)
+#define IPR_PCII_HRRQ_UPDATED				(0x80000000 >> 30)
+#define IPR_PCII_CORE_ISSUED_RST_REQ		(0x80000000 >> 31)
+
+#define IPR_PCII_ERROR_INTERRUPTS \
+(IPR_PCII_IOARCB_XFER_FAILED | IPR_PCII_IOA_UNIT_CHECKED | \
+IPR_PCII_NO_HOST_RRQ | IPR_PCII_IOARRIN_LOST | IPR_PCII_MMIO_ERROR)
+
+#define IPR_PCII_OPER_INTERRUPTS \
+(IPR_PCII_ERROR_INTERRUPTS | IPR_PCII_HRRQ_UPDATED | IPR_PCII_IOA_TRANS_TO_OPER)
+
+#define IPR_UPROCI_RESET_ALERT			(0x80000000 >> 7)
+#define IPR_UPROCI_IO_DEBUG_ALERT			(0x80000000 >> 9)
+
+#define IPR_LDUMP_MAX_LONG_ACK_DELAY_IN_USEC		200000	/* 200 ms */
+#define IPR_LDUMP_MAX_SHORT_ACK_DELAY_IN_USEC		200000	/* 200 ms */
+
+/*
+ * Dump literals
+ */
+#define IPR_MAX_IOA_DUMP_SIZE				(4 * 1024 * 1024)
+#define IPR_NUM_SDT_ENTRIES				511
+#define IPR_MAX_NUM_DUMP_PAGES	((IPR_MAX_IOA_DUMP_SIZE / PAGE_SIZE) + 1)
+
+/*
+ * Misc literals
+ */
+#define IPR_NUM_IOADL_ENTRIES			IPR_MAX_SGLIST
+
+/*
+ * Adapter interface types
+ */
+
+struct ipr_res_addr {
+	u8 reserved;
+	u8 bus;
+	u8 target;
+	u8 lun;
+#define IPR_GET_PHYS_LOC(res_addr) \
+	(((res_addr).bus << 16) | ((res_addr).target << 8) | (res_addr).lun)
+}__attribute__((packed, aligned (4)));
+
+struct ipr_std_inq_vpids {
+	u8 vendor_id[IPR_VENDOR_ID_LEN];
+	u8 product_id[IPR_PROD_ID_LEN];
+}__attribute__((packed));
+
+struct ipr_std_inq_data {
+	u8 peri_qual_dev_type;
+#define IPR_STD_INQ_PERI_QUAL(peri) ((peri) >> 5)
+#define IPR_STD_INQ_PERI_DEV_TYPE(peri) ((peri) & 0x1F)
+
+	u8 removeable_medium_rsvd;
+#define IPR_STD_INQ_REMOVEABLE_MEDIUM 0x80
+
+#define IPR_IS_DASD_DEVICE(std_inq) \
+((IPR_STD_INQ_PERI_DEV_TYPE((std_inq).peri_qual_dev_type) == TYPE_DISK) && \
+!(((std_inq).removeable_medium_rsvd) & IPR_STD_INQ_REMOVEABLE_MEDIUM))
+
+#define IPR_IS_SES_DEVICE(std_inq) \
+(IPR_STD_INQ_PERI_DEV_TYPE((std_inq).peri_qual_dev_type) == TYPE_ENCLOSURE)
+
+	u8 version;
+	u8 aen_naca_fmt;
+	u8 additional_len;
+	u8 sccs_rsvd;
+	u8 bq_enc_multi;
+	u8 sync_cmdq_flags;
+
+	struct ipr_std_inq_vpids vpids;
+
+	u8 ros_rsvd_ram_rsvd[4];
+
+	u8 serial_num[IPR_SERIAL_NUM_LEN];
+}__attribute__ ((packed));
+
+struct ipr_config_table_entry {
+	u8 service_level;
+	u8 array_id;
+	u8 flags;
+#define IPR_IS_IOA_RESOURCE	0x80
+#define IPR_IS_ARRAY_MEMBER 0x20
+#define IPR_IS_HOT_SPARE	0x10
+
+	u8 rsvd_subtype;
+#define IPR_RES_SUBTYPE(res) (((res)->cfgte.rsvd_subtype) & 0x0f)
+#define IPR_SUBTYPE_AF_DASD			0
+#define IPR_SUBTYPE_GENERIC_SCSI	1
+#define IPR_SUBTYPE_VOLUME_SET		2
+
+	struct ipr_res_addr res_addr;
+	u32 res_handle;
+	u32 reserved4[2];
+	struct ipr_std_inq_data std_inq_data;
+}__attribute__ ((packed, aligned (4)));
+
+struct ipr_config_table_hdr {
+	u8 num_entries;
+	u8 flags;
+#define IPR_UCODE_DOWNLOAD_REQ	0x10
+	u16 reserved;
+}__attribute__((packed, aligned (4)));
+
+struct ipr_config_table {
+	struct ipr_config_table_hdr hdr;
+	struct ipr_config_table_entry dev[IPR_MAX_PHYSICAL_DEVS];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_hostrcb_cfg_ch_not {
+	struct ipr_config_table_entry cfgte;
+	u8 reserved[936];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_supported_device {
+	u16 data_length;
+	u8 reserved;
+	u8 num_records;
+	struct ipr_std_inq_vpids vpids;
+	u8 reserved2[16];
+}__attribute__((packed, aligned (4)));
+
+/* Command packet structure */
+struct ipr_cmd_pkt {
+	u16 reserved;		/* Reserved by IOA */
+	u8 request_type;
+#define IPR_RQTYPE_SCSICDB		0x00
+#define IPR_RQTYPE_IOACMD		0x01
+#define IPR_RQTYPE_HCAM			0x02
+
+	u8 luntar_luntrn;
+
+	u8 flags_hi;
+#define IPR_FLAGS_HI_WRITE_NOT_READ		0x80
+#define IPR_FLAGS_HI_NO_ULEN_CHK		0x20
+#define IPR_FLAGS_HI_SYNC_OVERRIDE		0x10
+#define IPR_FLAGS_HI_SYNC_COMPLETE		0x08
+#define IPR_FLAGS_HI_NO_LINK_DESC		0x04
+
+	u8 flags_lo;
+#define IPR_FLAGS_LO_ALIGNED_BFR		0x20
+#define IPR_FLAGS_LO_DELAY_AFTER_RST	0x10
+#define IPR_FLAGS_LO_UNTAGGED_TASK		0x00
+#define IPR_FLAGS_LO_SIMPLE_TASK		0x02
+#define IPR_FLAGS_LO_ORDERED_TASK		0x04
+#define IPR_FLAGS_LO_HEAD_OF_Q_TASK		0x06
+#define IPR_FLAGS_LO_ACA_TASK			0x08
+
+	u8 cdb[16];
+	u16 timeout;
+}__attribute__ ((packed, aligned(4)));
+
+/* IOA Request Control Block    128 bytes  */
+struct ipr_ioarcb {
+	u32 ioarcb_host_pci_addr;
+	u32 reserved;
+	u32 res_handle;
+	u32 host_response_handle;
+	u32 reserved1;
+	u32 reserved2;
+	u32 reserved3;
+
+	u32 write_data_transfer_length;
+	u32 read_data_transfer_length;
+	u32 write_ioadl_addr;
+	u32 write_ioadl_len;
+	u32 read_ioadl_addr;
+	u32 read_ioadl_len;
+
+	u32 ioasa_host_pci_addr;
+	u16 ioasa_len;
+	u16 reserved4;
+
+	struct ipr_cmd_pkt cmd_pkt;
+
+	u32 add_cmd_parms_len;
+	u32 add_cmd_parms[10];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_ioadl_desc {
+	u32 flags_and_data_len;
+#define IPR_IOADL_FLAGS_MASK		0xff000000
+#define IPR_IOADL_GET_FLAGS(x) (be32_to_cpu(x) & IPR_IOADL_FLAGS_MASK)
+#define IPR_IOADL_DATA_LEN_MASK		0x00ffffff
+#define IPR_IOADL_GET_DATA_LEN(x) (be32_to_cpu(x) & IPR_IOADL_DATA_LEN_MASK)
+#define IPR_IOADL_FLAGS_READ		0x48000000
+#define IPR_IOADL_FLAGS_READ_LAST	0x49000000
+#define IPR_IOADL_FLAGS_WRITE		0x68000000
+#define IPR_IOADL_FLAGS_WRITE_LAST	0x69000000
+#define IPR_IOADL_FLAGS_LAST		0x01000000
+
+	u32 address;
+}__attribute__((packed, aligned (8)));
+
+struct ipr_ioasa_vset {
+	u32 failing_lba_hi;
+	u32 failing_lba_lo;
+	u32 ioa_data[22];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_ioasa_af_dasd {
+	u32 failing_lba;
+}__attribute__((packed, aligned (4)));
+
+struct ipr_ioasa_gpdd {
+	u8 device_end_state;
+	u8 device_bus_phase;
+	u16 reserved;
+	u32 ioa_data[23];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_ioasa_raw {
+	u32 ioa_data[24];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_ioasa {
+	u32 ioasc;
+#define IPR_IOASC_SENSE_KEY(ioasc) ((ioasc) >> 24)
+#define IPR_IOASC_SENSE_CODE(ioasc) (((ioasc) & 0x00ff0000) >> 16)
+#define IPR_IOASC_SENSE_QUAL(ioasc) (((ioasc) & 0x0000ff00) >> 8)
+#define IPR_IOASC_SENSE_STATUS(ioasc) ((ioasc) & 0x000000ff)
+
+	u16 ret_stat_len;	/* Length of the returned IOASA */
+
+	u16 avail_stat_len;	/* Total Length of status available. */
+
+	u32 residual_data_len;	/* number of bytes in the host data */
+	/* buffers that were not used by the IOARCB command. */
+
+	u32 ilid;
+#define IPR_NO_ILID			0
+#define IPR_DRIVER_ILID		0xffffffff
+
+	u32 fd_ioasc;
+
+	u32 fd_phys_locator;
+
+	u32 fd_res_handle;
+
+	u32 ioasc_specific;	/* status code specific field */
+#define IPR_IOASC_SPECIFIC_MASK		0x00ffffff
+#define IPR_FIELD_POINTER_VALID		(0x80000000 >> 8)
+#define IPR_FIELD_POINTER_MASK		0x0000ffff
+
+	union {
+		struct ipr_ioasa_vset vset;
+		struct ipr_ioasa_af_dasd dasd;
+		struct ipr_ioasa_gpdd gpdd;
+		struct ipr_ioasa_raw raw;
+	};
+}__attribute__((packed, aligned (4)));
+
+struct ipr_mode_parm_hdr {
+	u8 length;
+	u8 medium_type;
+	u8 device_spec_parms;
+	u8 block_desc_len;
+}__attribute__((packed));
+
+struct ipr_mode_pages {
+	struct ipr_mode_parm_hdr hdr;
+	u8 data[255 - sizeof(struct ipr_mode_parm_hdr)];
+}__attribute__((packed));
+
+struct ipr_mode_page_hdr {
+	u8 ps_page_code;
+#define IPR_MODE_PAGE_PS	0x80
+#define IPR_GET_MODE_PAGE_CODE(hdr) ((hdr)->ps_page_code & 0x3F)
+	u8 page_length;
+}__attribute__ ((packed));
+
+struct ipr_dev_bus_entry {
+	struct ipr_res_addr res_addr;
+	u8 flags;
+#define IPR_SCSI_ATTR_ENABLE_QAS			0x80
+#define IPR_SCSI_ATTR_DISABLE_QAS			0x40
+#define IPR_SCSI_ATTR_QAS_MASK				0xC0
+#define IPR_SCSI_ATTR_ENABLE_TM				0x20
+#define IPR_SCSI_ATTR_NO_TERM_PWR			0x10
+#define IPR_SCSI_ATTR_TM_SUPPORTED			0x08
+#define IPR_SCSI_ATTR_LVD_TO_SE_NOT_ALLOWED	0x04
+
+	u8 scsi_id;
+	u8 bus_width;
+	u8 extended_reset_delay;
+#define IPR_EXTENDED_RESET_DELAY	7
+
+	u32 max_xfer_rate;
+
+	u8 spinup_delay;
+	u8 reserved3;
+	u16 reserved4;
+}__attribute__((packed, aligned (4)));
+
+struct ipr_mode_page28 {
+	struct ipr_mode_page_hdr hdr;
+	u8 num_entries;
+	u8 entry_length;
+	struct ipr_dev_bus_entry bus[0];
+}__attribute__((packed));
+
+struct ipr_ioa_vpd {
+	struct ipr_std_inq_data std_inq_data;
+	u8 ascii_part_num[12];
+	u8 reserved[40];
+	u8 ascii_plant_code[4];
+}__attribute__((packed));
+
+struct ipr_inquiry_page3 {
+	u8 peri_qual_dev_type;
+	u8 page_code;
+	u8 reserved1;
+	u8 page_length;
+	u8 ascii_len;
+	u8 reserved2[3];
+	u8 load_id[4];
+	u8 major_release;
+	u8 card_type;
+	u8 minor_release[2];
+	u8 ptf_number[4];
+	u8 patch_number[4];
+}__attribute__((packed));
+
+struct ipr_hostrcb_device_data_entry {
+	struct ipr_std_inq_vpids dev_vpids;
+	u8 dev_sn[IPR_SERIAL_NUM_LEN];
+	struct ipr_res_addr dev_res_addr;
+	struct ipr_std_inq_vpids new_dev_vpids;
+	u8 new_dev_sn[IPR_SERIAL_NUM_LEN];
+	struct ipr_std_inq_vpids ioa_last_with_dev_vpids;
+	u8 ioa_last_with_dev_sn[IPR_SERIAL_NUM_LEN];
+	struct ipr_std_inq_vpids cfc_last_with_dev_vpids;
+	u8 cfc_last_with_dev_sn[IPR_SERIAL_NUM_LEN];
+	u32 ioa_data[5];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_hostrcb_array_data_entry {
+	struct ipr_std_inq_vpids vpids;
+	u8 serial_num[IPR_SERIAL_NUM_LEN];
+	struct ipr_res_addr expected_dev_res_addr;
+	struct ipr_res_addr dev_res_addr;
+}__attribute__((packed, aligned (4)));
+
+struct ipr_hostrcb_type_ff_error {
+	u32 ioa_data[246];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_hostrcb_type_01_error {
+	u32 seek_counter;
+	u32 read_counter;
+	u8 sense_data[32];
+	u32 ioa_data[236];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_hostrcb_type_02_error {
+	struct ipr_std_inq_vpids ioa_vpids;
+	u8 ioa_sn[IPR_SERIAL_NUM_LEN];
+	struct ipr_std_inq_vpids cfc_vpids;
+	u8 cfc_sn[IPR_SERIAL_NUM_LEN];
+	struct ipr_std_inq_vpids ioa_last_attached_to_cfc_vpids;
+	u8 ioa_last_attached_to_cfc_sn[IPR_SERIAL_NUM_LEN];
+	struct ipr_std_inq_vpids cfc_last_attached_to_ioa_vpids;
+	u8 cfc_last_attached_to_ioa_sn[IPR_SERIAL_NUM_LEN];
+	u32 ioa_data[3];
+	u8 reserved[844];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_hostrcb_type_03_error {
+	struct ipr_std_inq_vpids ioa_vpids;
+	u8 ioa_sn[IPR_SERIAL_NUM_LEN];
+	struct ipr_std_inq_vpids cfc_vpids;
+	u8 cfc_sn[IPR_SERIAL_NUM_LEN];
+	u32 errors_detected;
+	u32 errors_logged;
+	u8 ioa_data[12];
+	struct ipr_hostrcb_device_data_entry dev_entry[3];
+	u8 reserved[444];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_hostrcb_type_04_error {
+	struct ipr_std_inq_vpids ioa_vpids;
+	u8 ioa_sn[IPR_SERIAL_NUM_LEN];
+	struct ipr_std_inq_vpids cfc_vpids;
+	u8 cfc_sn[IPR_SERIAL_NUM_LEN];
+	u8 ioa_data[12];
+	struct ipr_hostrcb_array_data_entry array_member[10];
+	u32 exposed_mode_adn;
+	u32 array_id;
+	struct ipr_std_inq_vpids incomp_dev_vpids;
+	u8 incomp_dev_sn[IPR_SERIAL_NUM_LEN];
+	u32 ioa_data2;
+	struct ipr_hostrcb_array_data_entry array_member2[8];
+	struct ipr_res_addr last_func_vset_res_addr;
+	u8 vset_serial_num[IPR_SERIAL_NUM_LEN];
+	u8 protection_level[8];
+	u8 reserved[124];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_hostrcb_error {
+	u32 failing_dev_ioasc;
+	struct ipr_res_addr failing_dev_res_addr;
+	u32 failing_dev_res_handle;
+	u32 prc;
+	union {
+		struct ipr_hostrcb_type_ff_error type_ff_error;
+		struct ipr_hostrcb_type_01_error type_01_error;
+		struct ipr_hostrcb_type_02_error type_02_error;
+		struct ipr_hostrcb_type_03_error type_03_error;
+		struct ipr_hostrcb_type_04_error type_04_error;
+	};
+}__attribute__((packed, aligned (4)));
+
+struct ipr_hostrcb_raw {
+	u32 data[sizeof(struct ipr_hostrcb_error)/sizeof(u32)];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_hostrcb {
+	u8 op_code;
+#define IPR_HOST_RCB_OP_CODE_CONFIG_CHANGE			0xE1
+#define IPR_HOST_RCB_OP_CODE_LOG_DATA				0xE2
+
+	u8 notify_type;
+#define IPR_HOST_RCB_NOTIF_TYPE_EXISTING_CHANGED	0x00
+#define IPR_HOST_RCB_NOTIF_TYPE_NEW_ENTRY			0x01
+#define IPR_HOST_RCB_NOTIF_TYPE_REM_ENTRY			0x02
+#define IPR_HOST_RCB_NOTIF_TYPE_ERROR_LOG_ENTRY		0x10
+#define IPR_HOST_RCB_NOTIF_TYPE_INFORMATION_ENTRY	0x11
+
+	u8 notifications_lost;
+#define IPR_HOST_RCB_NO_NOTIFICATIONS_LOST			0
+#define IPR_HOST_RCB_NOTIFICATIONS_LOST				0x80
+
+	u8 flags;
+#define IPR_HOSTRCB_INTERNAL_OPER	0x80
+#define IPR_HOSTRCB_ERR_RESP_SENT	0x40
+
+	u8 overlay_id;
+#define IPR_HOST_RCB_OVERLAY_ID_1				0x01
+#define IPR_HOST_RCB_OVERLAY_ID_2				0x02
+#define IPR_HOST_RCB_OVERLAY_ID_3				0x03
+#define IPR_HOST_RCB_OVERLAY_ID_4				0x04
+#define IPR_HOST_RCB_OVERLAY_ID_6				0x06
+#define IPR_HOST_RCB_OVERLAY_ID_DEFAULT			0xFF
+
+	u8 reserved1[3];
+	u32 ilid;
+	u32 time_since_last_ioa_reset;
+	u32 reserved2;
+	u32 length;
+
+	union {
+		struct ipr_hostrcb_error error;
+		struct ipr_hostrcb_cfg_ch_not ccn;
+		struct ipr_hostrcb_raw raw;
+	};
+
+	/* Driver added data */
+	u32 hostrcb_dma;
+	struct list_head queue;
+}__attribute__((packed, aligned (4)));
+
+/* IPR smart dump table structures */
+struct ipr_sdt_entry {
+	u32 bar_str_offset;
+	u32 end_offset;
+	u8 entry_byte;
+	u8 reserved[3];
+
+	u8 flags;
+#define IPR_SDT_ENDIAN		0x80
+#define IPR_SDT_VALID_ENTRY	0x20
+
+	u8 resv;
+	u16 priority;
+}__attribute__((packed, aligned (4)));
+
+struct ipr_sdt_header {
+	u32 state;
+	u32 num_entries;
+	u32 num_entries_used;
+	u32 dump_size;
+}__attribute__((packed, aligned (4)));
+
+struct ipr_sdt {
+	struct ipr_sdt_header hdr;
+	struct ipr_sdt_entry entry[IPR_NUM_SDT_ENTRIES];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_uc_sdt {
+	struct ipr_sdt_header hdr;
+	struct ipr_sdt_entry entry[1];
+}__attribute__((packed, aligned (4)));
+
+/*
+ * Driver types
+ */
+struct ipr_bus_attributes {
+	u8 bus;
+	u8 qas_enabled;
+	u8 bus_width;
+	u8 reserved;
+	u32 max_xfer_rate;
+};
+
+struct ipr_resource_entry {
+	struct ipr_config_table_entry cfgte;
+	u8 needs_sync_complete:1;
+	u8 in_erp:1;
+	u8 add_to_ml:1;
+	u8 del_from_ml:1;
+	u8 resetting_device:1;
+	u8 tcq_active:1;
+
+	int qdepth;
+	struct scsi_device *sdev;
+	struct list_head queue;
+};
+
+struct ipr_resource_hdr {
+	u16 num_entries;
+	u16 reserved;
+};
+
+struct ipr_resource_table {
+	struct ipr_resource_hdr hdr;
+	struct ipr_resource_entry dev[IPR_MAX_PHYSICAL_DEVS];
+};
+
+struct ipr_misc_cbs {
+	struct ipr_ioa_vpd ioa_vpd;
+	struct ipr_inquiry_page3 page3_data;
+	struct ipr_mode_pages mode_pages;
+	struct ipr_supported_device supp_dev;
+};
+
+struct ipr_interrupts {
+	unsigned long set_interrupt_mask_reg;
+	unsigned long clr_interrupt_mask_reg;
+	unsigned long sense_interrupt_mask_reg;
+	unsigned long clr_interrupt_reg;
+
+	unsigned long sense_interrupt_reg;
+	unsigned long ioarrin_reg;
+	unsigned long sense_uproc_interrupt_reg;
+	unsigned long set_uproc_interrupt_reg;
+	unsigned long clr_uproc_interrupt_reg;
+};
+
+struct ipr_chip_cfg_t {
+	u32 mailbox;
+	u8 cache_line_size;
+	struct ipr_interrupts regs;
+};
+
+enum ipr_shutdown_type {
+	IPR_SHUTDOWN_NORMAL = 0x00,
+	IPR_SHUTDOWN_PREPARE_FOR_NORMAL = 0x40,
+	IPR_SHUTDOWN_ABBREV = 0x80,
+	IPR_SHUTDOWN_NONE = 0x100
+};
+
+struct ipr_trace_entry {
+	u32 time;
+
+	u8 op_code;
+	u8 type;
+#define IPR_TRACE_START			0x00
+#define IPR_TRACE_FINISH		0xff
+	u16 cmd_index;
+
+	u32 res_handle;
+	union {
+		u32 ioasc;
+		u32 add_data;
+		u32 res_addr;
+	};
+};
+
+struct ipr_sglist {
+	u32 order;
+	u32 num_sg;
+	u32 buffer_len;
+	struct scatterlist scatterlist[1];
+};
+
+enum ipr_sdt_state {
+	INACTIVE,
+	WAIT_FOR_DUMP,
+	GET_DUMP,
+	ABORT_DUMP,
+	DUMP_OBTAINED
+};
+
+/* Per-controller data */
+struct ipr_ioa_cfg {
+	char eye_catcher[8];
+#define IPR_EYECATCHER			"iprcfg"
+
+	struct list_head queue;
+
+	u8 allow_interrupts:1;
+	u8 in_reset_reload:1;
+	u8 in_ioa_bringdown:1;
+	u8 ioa_unit_checked:1;
+	u8 ioa_is_dead:1;
+	u8 dump_taken:1;
+	u8 allow_cmds:1;
+	u8 allow_ml_add_del:1;
+
+	u16 type; /* CCIN of the card */
+
+	u8 log_level;
+#define IPR_MAX_LOG_LEVEL			4
+#define IPR_DEFAULT_LOG_LEVEL		2
+
+#define IPR_NUM_TRACE_INDEX_BITS	8
+#define IPR_NUM_TRACE_ENTRIES		(1 << IPR_NUM_TRACE_INDEX_BITS)
+#define IPR_TRACE_SIZE	(sizeof(struct ipr_trace_entry) * IPR_NUM_TRACE_ENTRIES)
+	char trace_start[8];
+#define IPR_TRACE_START_LABEL			"trace"
+	struct ipr_trace_entry *trace;
+	u32 trace_index:IPR_NUM_TRACE_INDEX_BITS;
+
+	/*
+	 * Queue for free command blocks
+	 */
+	char ipr_free_label[8];
+#define IPR_FREEQ_LABEL			"free-q"
+	struct list_head free_q;
+
+	/*
+	 * Queue for command blocks outstanding to the adapter
+	 */
+	char ipr_pending_label[8];
+#define IPR_PENDQ_LABEL			"pend-q"
+	struct list_head pending_q;
+
+	char cfg_table_start[8];
+#define IPR_CFG_TBL_START		"cfg"
+	struct ipr_config_table *cfg_table;
+	u32 cfg_table_dma;
+
+	char resource_table_label[8];
+#define IPR_RES_TABLE_LABEL		"res_tbl"
+	struct ipr_resource_entry *res_entries;
+	struct list_head free_res_q;
+	struct list_head used_res_q;
+
+	char ipr_hcam_label[8];
+#define IPR_HCAM_LABEL			"hcams"
+	struct ipr_hostrcb *hostrcb[IPR_NUM_HCAMS];
+	u32 hostrcb_dma[IPR_NUM_HCAMS];
+	struct list_head hostrcb_free_q;
+	struct list_head hostrcb_pending_q;
+
+	u32 *host_rrq;
+	u32 host_rrq_dma;
+#define IPR_HRRQ_REQ_RESP_HANDLE_MASK	0xfffffffc
+#define IPR_HRRQ_RESP_BIT_SET			0x00000002
+#define IPR_HRRQ_TOGGLE_BIT				0x00000001
+#define IPR_HRRQ_REQ_RESP_HANDLE_SHIFT	2
+	volatile u32 *hrrq_start;
+	volatile u32 *hrrq_end;
+	volatile u32 *hrrq_curr;
+	volatile u32 toggle_bit;
+
+	struct ipr_bus_attributes bus_attr[IPR_MAX_NUM_BUSES];
+
+	const struct ipr_chip_cfg_t *chip_cfg;
+
+	unsigned long hdw_dma_regs;	/* iomapped PCI memory space */
+	unsigned long hdw_dma_regs_pci;	/* raw PCI memory space */
+	unsigned long ioa_mailbox;
+	struct ipr_interrupts regs;
+
+	u32 pci_cfg_buf[64];
+	u16 saved_pcix_cmd_reg;
+	u16 reset_retries;
+
+	u32 errors_logged;
+
+	struct Scsi_Host *host;
+	struct pci_dev *pdev;
+	struct ipr_sglist *ucode_sglist;
+	struct ipr_mode_pages *saved_mode_pages;
+	u8 saved_mode_page_len;
+
+	struct work_struct work_q;
+
+	wait_queue_head_t reset_wait_q;
+
+	struct ipr_dump *dump;
+	enum ipr_sdt_state sdt_state;
+
+	struct ipr_misc_cbs *vpd_cbs;
+	u32 vpd_cbs_dma;
+
+	struct pci_pool *ipr_cmd_pool;
+
+	struct ipr_cmnd *reset_cmd;
+
+	char ipr_cmd_label[8];
+#define IPR_CMD_LABEL		"ipr_cmnd"
+	struct ipr_cmnd *ipr_cmnd_list[IPR_NUM_CMD_BLKS];
+	u32 ipr_cmnd_list_dma[IPR_NUM_CMD_BLKS];
+};
+
+struct ipr_cmnd {
+	struct ipr_ioarcb ioarcb;
+	struct ipr_ioasa ioasa;
+	struct ipr_ioadl_desc ioadl[IPR_NUM_IOADL_ENTRIES];
+	struct list_head queue;
+	struct scsi_cmnd *scsi_cmd;
+	struct completion completion;
+	struct timer_list timer;
+	void (*done) (struct ipr_cmnd *);
+	int (*job_step) (struct ipr_cmnd *);
+	u16 cmd_index;
+	u8 sense_buffer[SCSI_SENSE_BUFFERSIZE];
+	dma_addr_t sense_buffer_dma;
+	unsigned short dma_use_sg;
+	dma_addr_t dma_handle;
+	union {
+		enum ipr_shutdown_type shutdown_type;
+		struct ipr_hostrcb *hostrcb;
+		unsigned long time_left;
+		unsigned long scratch;
+		struct ipr_resource_entry *res;
+	};
+
+	struct ipr_ioa_cfg *ioa_cfg;
+};
+
+struct ipr_ses_table_entry {
+	char product_id[17];
+	char compare_product_id_byte[17];
+	u32 max_bus_speed_limit;	/* MB/sec limit for this backplane */
+};
+
+struct ipr_dump_header {
+	u32 eye_catcher;
+#define IPR_DUMP_EYE_CATCHER		0xC5D4E3F2
+	u32 len;
+	u32 num_entries;
+	u32 first_entry_offset;
+	u32 status;
+#define IPR_DUMP_STATUS_SUCCESS			0
+#define IPR_DUMP_STATUS_QUAL_SUCCESS		2
+#define IPR_DUMP_STATUS_FAILED			0xffffffff
+	u32 os;
+#define IPR_DUMP_OS_LINUX	0x4C4E5558
+	u32 driver_name;
+#define IPR_DUMP_DRIVER_NAME	0x49505232
+}__attribute__((packed, aligned (4)));
+
+struct ipr_dump_entry_header {
+	u32 eye_catcher;
+#define IPR_DUMP_EYE_CATCHER		0xC5D4E3F2
+	u32 len;
+	u32 num_elems;
+	u32 offset;
+	u32 data_type;
+#define IPR_DUMP_DATA_TYPE_ASCII	0x41534349
+#define IPR_DUMP_DATA_TYPE_BINARY	0x42494E41
+	u32 id;
+#define IPR_DUMP_IOA_DUMP_ID		0x494F4131
+#define IPR_DUMP_LOCATION_ID		0x4C4F4341
+#define IPR_DUMP_TRACE_ID		0x54524143
+#define IPR_DUMP_DRIVER_VERSION_ID	0x44525652
+#define IPR_DUMP_IOA_CTRL_BLK		0x494F4342
+#define IPR_DUMP_PEND_OPS		0x414F5053
+	u32 status;
+}__attribute__((packed, aligned (4)));
+
+struct ipr_dump_location_entry {
+	struct ipr_dump_entry_header hdr;
+	u8 location[BUS_ID_SIZE];
+}__attribute__((packed));
+
+struct ipr_dump_trace_entry {
+	struct ipr_dump_entry_header hdr;
+	u32 trace[IPR_TRACE_SIZE / sizeof(u32)];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_dump_version_entry {
+	struct ipr_dump_entry_header hdr;
+	u8 version[sizeof(IPR_DRIVER_VERSION)];
+};
+
+struct ipr_driver_dump {
+	struct ipr_dump_header hdr;
+	struct ipr_dump_version_entry version_entry;
+	struct ipr_dump_location_entry location_entry;
+	struct ipr_dump_trace_entry trace_entry;
+}__attribute__((packed));
+
+struct ipr_ioa_dump {
+	struct ipr_dump_entry_header hdr;
+	u32 next_page_index;
+	u32 page_offset;
+	u32 format;
+#define IPR_SDT_FMT2		2
+#define IPR_SDT_UNKNOWN		3
+	u32 reserved;
+	struct ipr_sdt sdt;
+	u32 *ioa_data[IPR_MAX_NUM_DUMP_PAGES];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_dump {
+	struct kobject kobj;
+	struct ipr_ioa_cfg *ioa_cfg;
+	struct ipr_driver_dump driver_dump;
+	struct ipr_ioa_dump ioa_dump;
+};
+
+struct ipr_error_table_t {
+	u32 ioasc;
+	int log_ioasa;
+	int log_hcam;
+	char *error;
+};
+
+struct ipr_software_inq_lid_info {
+    u32  load_id;
+    u32  timestamp[3];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_ucode_image_header {
+    u32 header_length;
+    u32 lid_table_offset;
+    u8 major_release;
+    u8 card_type;
+    u8 minor_release[2];
+    u8 reserved[20];
+    char eyecatcher[16];
+    u32 num_lids;
+    struct ipr_software_inq_lid_info lid[1];
+}__attribute__((packed, aligned (4)));
+
+/*
+ * Macros
+ */
+#if IPR_DEBUG
+#define IPR_DBG_CMD(CMD) { (CMD); }
+#else
+#define IPR_DBG_CMD(CMD)
+#endif
+
+#define ipr_breakpoint_data KERN_ERR IPR_NAME\
+": %s: %s: Line: %d ipr_cfg: %p\n", __FILE__, \
+__FUNCTION__, __LINE__, ipr_cfg
+
+#if defined(CONFIG_KDB) && !defined(CONFIG_PPC_ISERIES)
+#define ipr_breakpoint {printk(ipr_breakpoint_data); KDB_ENTER();}
+#define ipr_breakpoint_or_die {printk(ipr_breakpoint_data); KDB_ENTER();}
+#else
+#define ipr_breakpoint
+#define ipr_breakpoint_or_die panic(ipr_breakpoint_data)
+#endif
+
+#ifdef CONFIG_SCSI_IPR_TRACE
+#define ipr_create_trace_file(kobj, attr) sysfs_create_bin_file(kobj, attr)
+#define ipr_remove_trace_file(kobj, attr) sysfs_remove_bin_file(kobj, attr)
+#else
+#define ipr_create_trace_file(kobj, attr) 0
+#define ipr_remove_trace_file(kobj, attr) do { } while(0)
+#endif
+
+#ifdef CONFIG_SCSI_IPR_DUMP
+#define ipr_create_dump_file(kobj, attr) sysfs_create_bin_file(kobj, attr)
+#define ipr_remove_dump_file(kobj, attr) sysfs_remove_bin_file(kobj, attr)
+#else
+#define ipr_create_dump_file(kobj, attr) 0
+#define ipr_remove_dump_file(kobj, attr) do { } while(0)
+#endif
+
+/*
+ * Error logging macros
+ */
+#define ipr_err(...) printk(KERN_ERR IPR_NAME ": "__VA_ARGS__);
+#define ipr_info(...) printk(KERN_INFO IPR_NAME ": "__VA_ARGS__);
+#define ipr_crit(...) printk(KERN_CRIT IPR_NAME ": "__VA_ARGS__);
+#define ipr_warn(...) printk(KERN_WARNING IPR_NAME": "__VA_ARGS__);
+#define ipr_dbg(...) IPR_DBG_CMD(printk(KERN_DEBUG IPR_NAME ": "__VA_ARGS__));
+
+#define ipr_sdev_printk(level, sdev, fmt, ...) \
+	printk(level IPR_NAME ": %d:%d:%d:%d: " fmt, sdev->host->host_no, \
+		sdev->channel, sdev->id, sdev->lun, ##__VA_ARGS__)
+
+#define ipr_sdev_err(sdev, fmt, ...) \
+	ipr_sdev_printk(KERN_ERR, sdev, fmt, ##__VA_ARGS__)
+
+#define ipr_sdev_info(sdev, fmt, ...) \
+	ipr_sdev_printk(KERN_INFO, sdev, fmt, ##__VA_ARGS__)
+
+#define ipr_res_printk(level, ioa_cfg, res, fmt, ...) \
+	printk(level IPR_NAME ": %d:%d:%d:%d: " fmt, ioa_cfg->host->host_no, \
+		res.bus, res.target, res.lun, ##__VA_ARGS__)
+
+#define ipr_res_err(ioa_cfg, res, fmt, ...) \
+	ipr_res_printk(KERN_ERR, ioa_cfg, res, fmt, ##__VA_ARGS__);
+#define ipr_res_dbg(ioa_cfg, res, fmt, ...) \
+	ipr_res_printk(KERN_DEBUG, ioa_cfg, res, fmt, ##__VA_ARGS__);
+
+#define ipr_trace ipr_dbg("%s: %s: Line: %d\n",\
+	__FILE__, __FUNCTION__, __LINE__);
+
+#if IPR_DBG_TRACE
+#define ENTER printk(KERN_DEBUG IPR_NAME": Entering %s\n", __FUNCTION__);
+#define LEAVE printk(KERN_DEBUG IPR_NAME": Leaving %s\n", __FUNCTION__);
+#else
+#define ENTER
+#define LEAVE
+#endif
+
+#define ipr_err_separator \
+ipr_err("----------------------------------------------------------\n")
+
+
+/*
+ * Inlines
+ */
+
+/**
+ * ipr_is_ioa_resource - Determine if a resource is the IOA
+ * @res:	resource entry struct
+ *
+ * Return value:
+ * 	1 if IOA / 0 if not IOA
+ **/
+static inline int ipr_is_ioa_resource(struct ipr_resource_entry *res)
+{
+	return (res->cfgte.flags & IPR_IS_IOA_RESOURCE) ? 1 : 0;
+}
+
+/**
+ * ipr_is_af_dasd_device - Determine if a resource is an AF DASD
+ * @res:	resource entry struct
+ *
+ * Return value:
+ * 	1 if AF DASD / 0 if not AF DASD
+ **/
+static inline int ipr_is_af_dasd_device(struct ipr_resource_entry *res)
+{
+	if (IPR_IS_DASD_DEVICE(res->cfgte.std_inq_data) &&
+	    !ipr_is_ioa_resource(res) &&
+	    IPR_RES_SUBTYPE(res) == IPR_SUBTYPE_AF_DASD)
+		return 1;
+	else
+		return 0;
+}
+
+/**
+ * ipr_is_vset_device - Determine if a resource is a VSET
+ * @res:	resource entry struct
+ *
+ * Return value:
+ * 	1 if VSET / 0 if not VSET
+ **/
+static inline int ipr_is_vset_device(struct ipr_resource_entry *res)
+{
+	if (IPR_IS_DASD_DEVICE(res->cfgte.std_inq_data) &&
+	    !ipr_is_ioa_resource(res) &&
+	    IPR_RES_SUBTYPE(res) == IPR_SUBTYPE_VOLUME_SET)
+		return 1;
+	else
+		return 0;
+}
+
+/**
+ * ipr_is_gscsi - Determine if a resource is a generic scsi resource
+ * @res:	resource entry struct
+ *
+ * Return value:
+ * 	1 if GSCSI / 0 if not GSCSI
+ **/
+static inline int ipr_is_gscsi(struct ipr_resource_entry *res)
+{
+	if (!ipr_is_ioa_resource(res) &&
+	    IPR_RES_SUBTYPE(res) == IPR_SUBTYPE_GENERIC_SCSI)
+		return 1;
+	else
+		return 0;
+}
+
+/**
+ * ipr_is_device - Determine if resource address is that of a device
+ * @res_addr:	resource address struct
+ *
+ * Return value:
+ * 	1 if AF / 0 if not AF
+ **/
+static inline int ipr_is_device(struct ipr_res_addr *res_addr)
+{
+	if ((res_addr->bus < IPR_MAX_NUM_BUSES) &&
+	    (res_addr->target < IPR_MAX_NUM_TARGETS_PER_BUS))
+		return 1;
+
+	return 0;
+}
+
+/**
+ * ipr_sdt_is_fmt2 - Determine if a SDT address is in format 2
+ * @sdt_word:	SDT address
+ *
+ * Return value:
+ * 	1 if format 2 / 0 if not
+ **/
+static inline int ipr_sdt_is_fmt2(u32 sdt_word)
+{
+	u32 bar_sel = IPR_GET_FMT2_BAR_SEL(sdt_word);
+
+	switch (bar_sel) {
+	case IPR_SDT_FMT2_BAR0_SEL:
+	case IPR_SDT_FMT2_BAR1_SEL:
+	case IPR_SDT_FMT2_BAR2_SEL:
+	case IPR_SDT_FMT2_BAR3_SEL:
+	case IPR_SDT_FMT2_BAR4_SEL:
+	case IPR_SDT_FMT2_BAR5_SEL:
+	case IPR_SDT_FMT2_EXP_ROM_SEL:
+		return 1;
+	};
+
+	return 0;
+}
+
+#endif
diff -purN linux-2.5/drivers/scsi/scsi.c linuxppc64-2.5/drivers/scsi/scsi.c
--- linux-2.5/drivers/scsi/scsi.c	2004-02-28 22:01:12.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/scsi.c	2004-03-01 23:51:12.000000000 +0000
@@ -104,7 +104,7 @@ const char *const scsi_device_types[MAX_
 	"Communications   ",
 	"Unknown          ",
 	"Unknown          ",
-	"Unknown          ",
+	"RAID             ",
 	"Enclosure        ",
 };
 
diff -purN linux-2.5/drivers/scsi/scsi_error.c linuxppc64-2.5/drivers/scsi/scsi_error.c
--- linux-2.5/drivers/scsi/scsi_error.c	2004-01-15 22:14:40.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/scsi_error.c	2004-03-01 19:33:23.000000000 +0000
@@ -37,6 +37,8 @@
 #define SENSE_TIMEOUT (10*HZ)
 #endif
 
+#define START_UNIT_TIMEOUT (30*HZ)
+
 /*
  * These should *probably* be handled by the host itself.
  * Since it is allowed to sleep, it probably should.
@@ -282,6 +284,15 @@ static int scsi_check_sense(struct scsi_
 			(scmd->sense_buffer[13] == 0x01)) {
 			return NEEDS_RETRY;
 		}
+		/*
+		 * if the device is not started, we need to wake
+		 * the error handler to start the motor
+		 */
+		if (scmd->device->allow_restart &&
+		    (scmd->sense_buffer[12] == 0x04) &&
+		    (scmd->sense_buffer[13] == 0x02)) {
+			return FAILED;
+		}
 		return SUCCESS;
 
 		/* these three are not supported */
@@ -829,6 +840,105 @@ static int scsi_try_bus_device_reset(str
 }
 
 /**
+ * scsi_eh_try_stu - Send START_UNIT to device.
+ * @scmd:	Scsi cmd to send START_UNIT
+ *
+ * Return value:
+ *    0 - Device is ready. 1 - Device NOT ready.
+ **/
+static int scsi_eh_try_stu(struct scsi_cmnd *scmd)
+{
+	static unsigned char stu_command[6] = {START_STOP, 0, 0, 0, 1, 0};
+	int rtn;
+
+	if (!scmd->device->allow_restart)
+		return 1;
+
+	memcpy(scmd->cmnd, stu_command, sizeof(stu_command));
+
+	/*
+	 * zero the sense buffer.  the scsi spec mandates that any
+	 * untransferred sense data should be interpreted as being zero.
+	 */
+	memset(scmd->sense_buffer, 0, sizeof(scmd->sense_buffer));
+
+	scmd->request_buffer = NULL;
+	scmd->request_bufflen = 0;
+	scmd->use_sg = 0;
+	scmd->cmd_len = COMMAND_SIZE(scmd->cmnd[0]);
+	scmd->underflow = 0;
+	scmd->sc_data_direction = DMA_NONE;
+
+	rtn = scsi_send_eh_cmnd(scmd, START_UNIT_TIMEOUT);
+
+	/*
+	 * when we eventually call scsi_finish, we really wish to complete
+	 * the original request, so let's restore the original data. (db)
+	 */
+	scsi_setup_cmd_retry(scmd);
+
+	/*
+	 * hey, we are done.  let's look to see what happened.
+	 */
+	SCSI_LOG_ERROR_RECOVERY(3, printk("%s: scmd %p rtn %x\n",
+		__FUNCTION__, scmd, rtn));
+	if (rtn == SUCCESS)
+		return 0;
+	return 1;
+}
+
+ /**
+ * scsi_eh_stu - send START_UNIT if needed
+ * @shost:	scsi host being recovered.
+ * @eh_done_q:	list_head for processed commands.
+ *
+ * Notes:
+ *    If commands are failing due to not ready, initializing command required,
+ *	sending a start unit, if we are allowed to.
+ **/
+static int scsi_eh_stu(struct Scsi_Host *shost,
+			      struct list_head *work_q,
+			      struct list_head *done_q)
+{
+	struct list_head *lh, *lh_sf;
+	struct scsi_cmnd *scmd, *stu_scmd;
+	struct scsi_device *sdev;
+
+	shost_for_each_device(sdev, shost) {
+		stu_scmd = NULL;
+		list_for_each_entry(scmd, work_q, eh_entry)
+			if (scmd->device == sdev && SCSI_SENSE_VALID(scmd) &&
+			    scsi_check_sense(scmd) == FAILED ) {
+				stu_scmd = scmd;
+				break;
+			}
+
+		if (!stu_scmd)
+			continue;
+
+		SCSI_LOG_ERROR_RECOVERY(3, printk("%s: Sending START_UNIT to sdev:"
+						  " 0x%p\n", current->comm, sdev));
+
+		if (!scsi_eh_try_stu(stu_scmd)) {
+			if (!sdev->online || !scsi_eh_tur(stu_scmd)) {
+				list_for_each_safe(lh, lh_sf, work_q) {
+					scmd = list_entry(lh, struct scsi_cmnd, eh_entry);
+					if (scmd->device == sdev)
+						scsi_eh_finish_cmd(scmd, done_q);
+				}
+			}
+		} else {
+			SCSI_LOG_ERROR_RECOVERY(3,
+						printk("%s: START_UNIT failed to sdev:"
+						       " 0x%p\n", current->comm, sdev));
+		}
+	}
+
+	return list_empty(work_q);
+}
+
+
+/**
  * scsi_eh_bus_device_reset - send bdr if needed
  * @shost:	scsi host being recovered.
  * @eh_done_q:	list_head for processed commands.
@@ -1033,7 +1143,9 @@ static int scsi_eh_host_reset(struct lis
 		if (rtn == SUCCESS) {
 			list_for_each_safe(lh, lh_sf, work_q) {
 				scmd = list_entry(lh, struct scsi_cmnd, eh_entry);
-				if (!scmd->device->online || !scsi_eh_tur(scmd)) 
+				if (!scmd->device->online ||
+				    (!scsi_eh_try_stu(scmd) && !scsi_eh_tur(scmd)) ||
+				    !scsi_eh_tur(scmd))
 					scsi_eh_finish_cmd(scmd, done_q);
 			}
 		} else {
@@ -1401,10 +1513,11 @@ static void scsi_eh_ready_devs(struct Sc
 			       struct list_head *work_q,
 			       struct list_head *done_q)
 {
-	if (!scsi_eh_bus_device_reset(shost, work_q, done_q))
-		if (!scsi_eh_bus_reset(shost, work_q, done_q))
-			if (!scsi_eh_host_reset(work_q, done_q))
-				scsi_eh_offline_sdevs(work_q, done_q);
+	if (!scsi_eh_stu(shost, work_q, done_q))
+		if (!scsi_eh_bus_device_reset(shost, work_q, done_q))
+			if (!scsi_eh_bus_reset(shost, work_q, done_q))
+				if (!scsi_eh_host_reset(work_q, done_q))
+					scsi_eh_offline_sdevs(work_q, done_q);
 }
 
 /**
diff -purN linux-2.5/include/asm-ppc/unistd.h linuxppc64-2.5/include/asm-ppc/unistd.h
--- linux-2.5/include/asm-ppc/unistd.h	2004-02-25 10:31:13.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc/unistd.h	2004-02-26 05:42:06.000000000 +0000
@@ -280,6 +280,7 @@
 		register unsigned long __sc_5  __asm__ ("r5");		\
 		register unsigned long __sc_6  __asm__ ("r6");		\
 		register unsigned long __sc_7  __asm__ ("r7");		\
+		register unsigned long __sc_8  __asm__ ("r8");		\
 									\
 		__sc_loadargs_##nr(name, args);				\
 		__asm__ __volatile__					\
@@ -288,10 +289,10 @@
 			: "=&r" (__sc_0),				\
 			  "=&r" (__sc_3),  "=&r" (__sc_4),		\
 			  "=&r" (__sc_5),  "=&r" (__sc_6),		\
-			  "=&r" (__sc_7)				\
+			  "=&r" (__sc_7),  "=&r" (__sc_8)		\
 			: __sc_asm_input_##nr				\
 			: "cr0", "ctr", "memory",			\
-			  "r8", "r9", "r10","r11", "r12");		\
+			  "r9", "r10","r11", "r12");			\
 		__sc_ret = __sc_3;					\
 		__sc_err = __sc_0;					\
 	}								\
@@ -319,6 +320,9 @@
 #define __sc_loadargs_5(name, arg1, arg2, arg3, arg4, arg5)		\
 	__sc_loadargs_4(name, arg1, arg2, arg3, arg4);			\
 	__sc_7 = (unsigned long) (arg5)
+#define __sc_loadargs_6(name, arg1, arg2, arg3, arg4, arg5, arg6)	\
+	__sc_loadargs_5(name, arg1, arg2, arg3, arg4, arg5);		\
+	__sc_8 = (unsigned long) (arg6)
 
 #define __sc_asm_input_0 "0" (__sc_0)
 #define __sc_asm_input_1 __sc_asm_input_0, "1" (__sc_3)
@@ -326,6 +330,7 @@
 #define __sc_asm_input_3 __sc_asm_input_2, "3" (__sc_5)
 #define __sc_asm_input_4 __sc_asm_input_3, "4" (__sc_6)
 #define __sc_asm_input_5 __sc_asm_input_4, "5" (__sc_7)
+#define __sc_asm_input_6 __sc_asm_input_5, "6" (__sc_8)
 
 #define _syscall0(type,name)						\
 type name(void)								\
@@ -363,6 +368,12 @@ type name(type1 arg1, type2 arg2, type3 
 	__syscall_nr(5, type, name, arg1, arg2, arg3, arg4, arg5);	\
 }
 
+#define _syscall6(type,name,type1,arg1,type2,arg2,type3,arg3,type4,arg4,type5,arg5,type6,arg6) \
+type name(type1 arg1, type2 arg2, type3 arg3, type4 arg4, type5 arg5, type6 arg6) \
+{									\
+	__syscall_nr(6, type, name, arg1, arg2, arg3, arg4, arg5, arg6); \
+}
+
 #ifdef __KERNEL__
 
 #define __NR__exit __NR_exit
diff -purN linux-2.5/include/asm-ppc64/dma-mapping.h linuxppc64-2.5/include/asm-ppc64/dma-mapping.h
--- linux-2.5/include/asm-ppc64/dma-mapping.h	2002-12-22 04:36:58.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/dma-mapping.h	2004-02-24 03:48:31.000000000 +0000
@@ -1 +1,75 @@
-#include <asm-generic/dma-mapping.h>
+/* Copyright (C) 2004 IBM
+ *
+ * Implements the generic device dma API for ppc64. Handles
+ * the pci and vio busses
+ */
+
+#ifndef _ASM_DMA_MAPPING_H
+#define _ASM_DMA_MAPPING_H
+
+#include <linux/types.h>
+#include <linux/device.h>
+#include <linux/cache.h>
+/* need struct page definitions */
+#include <linux/mm.h>
+#include <asm/scatterlist.h>
+#include <asm/bug.h>
+
+extern int dma_supported(struct device *dev, u64 mask);
+extern int dma_set_mask(struct device *dev, u64 dma_mask);
+extern void *dma_alloc_coherent(struct device *dev, size_t size,
+		dma_addr_t *dma_handle, int flag);
+extern void dma_free_coherent(struct device *dev, size_t size, void *cpu_addr,
+		dma_addr_t dma_handle);
+extern dma_addr_t dma_map_single(struct device *dev, void *cpu_addr,
+		size_t size, enum dma_data_direction direction);
+extern void dma_unmap_single(struct device *dev, dma_addr_t dma_addr,
+		size_t size, enum dma_data_direction direction);
+extern dma_addr_t dma_map_page(struct device *dev, struct page *page,
+		unsigned long offset, size_t size,
+		enum dma_data_direction direction);
+extern void dma_unmap_page(struct device *dev, dma_addr_t dma_address,
+		size_t size, enum dma_data_direction direction);
+extern int dma_map_sg(struct device *dev, struct scatterlist *sg, int nents,
+		enum dma_data_direction direction);
+extern void dma_unmap_sg(struct device *dev, struct scatterlist *sg,
+		int nhwentries, enum dma_data_direction direction);
+extern void dma_sync_single(struct device *dev, dma_addr_t dma_handle,
+		size_t size, enum dma_data_direction direction);
+extern void dma_sync_sg(struct device *dev, struct scatterlist *sg, int nelems,
+		enum dma_data_direction direction);
+
+/* Now for the API extensions over the pci_ one */
+
+#define dma_alloc_noncoherent(d, s, h, f) dma_alloc_coherent(d, s, h, f)
+#define dma_free_noncoherent(d, s, v, h) dma_free_coherent(d, s, v, h)
+#define dma_is_consistent(d)	(1)
+
+static inline int
+dma_get_cache_alignment(void)
+{
+	/* no easy way to get cache size on all processors, so return
+	 * the maximum possible, to be safe */
+	return (1 << L1_CACHE_SHIFT_MAX);
+}
+
+static inline void
+dma_sync_single_range(struct device *dev, dma_addr_t dma_handle,
+		      unsigned long offset, size_t size,
+		      enum dma_data_direction direction)
+{
+	/* just sync everything, that's all the pci API can do */
+	dma_sync_single(dev, dma_handle, offset+size, direction);
+}
+
+static inline void
+dma_cache_sync(void *vaddr, size_t size,
+	       enum dma_data_direction direction)
+{
+	/* could define this in terms of the dma_cache ... operations,
+	 * but if you get this on a platform, you should convert the platform
+	 * to using the generic device DMA API */
+	BUG();
+}
+
+#endif	/* _ASM_DMA_MAPPING_H */
diff -purN linux-2.5/include/asm-ppc64/eeh.h linuxppc64-2.5/include/asm-ppc64/eeh.h
--- linux-2.5/include/asm-ppc64/eeh.h	2004-02-06 08:24:50.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/eeh.h	2004-03-02 11:34:18.000000000 +0000
@@ -17,15 +17,11 @@
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
 
-/* Start Change Log
- * 2001/10/27 : engebret : Created.
- * End Change Log 
- */
-
-#ifndef _EEH_H
-#define _EEH_H
+#ifndef _PPC64_EEH_H
+#define _PPC64_EEH_H
 
 #include <linux/string.h>
+#include <linux/init.h>
 
 struct pci_dev;
 
@@ -33,22 +29,43 @@ struct pci_dev;
  * a bad page fault if the address is used directly (i.e. these addresses are
  * never actually mapped.  Translation between IO <-> EEH region is 1 to 1.
  */
-#define IO_TOKEN_TO_ADDR(token) (((unsigned long)(token) & ~(0xfUL << REGION_SHIFT)) | \
-				(IO_REGION_ID << REGION_SHIFT))
-#define IO_ADDR_TO_TOKEN(addr) (((unsigned long)(addr) & ~(0xfUL << REGION_SHIFT)) | \
-				(EEH_REGION_ID << REGION_SHIFT))
+#define IO_TOKEN_TO_ADDR(token) \
+	(((unsigned long)(token) & ~(0xfUL << REGION_SHIFT)) | \
+	(IO_REGION_ID << REGION_SHIFT))
+
+#define IO_ADDR_TO_TOKEN(addr) \
+	(((unsigned long)(addr) & ~(0xfUL << REGION_SHIFT)) | \
+	(EEH_REGION_ID << REGION_SHIFT))
 
 /* Values for eeh_mode bits in device_node */
 #define EEH_MODE_SUPPORTED	(1<<0)
 #define EEH_MODE_NOCHECK	(1<<1)
 
-/* This is for profiling only */
-extern unsigned long eeh_total_mmio_ffs;
-
-void eeh_init(void);
-int eeh_get_state(unsigned long ea);
+extern void __init eeh_init(void);
 unsigned long eeh_check_failure(void *token, unsigned long val);
 void *eeh_ioremap(unsigned long addr, void *vaddr);
+void __init pci_addr_cache_build(void);
+
+/**
+ * eeh_add_device - perform EEH initialization for the indicated pci device
+ * @dev: pci device for which to set up EEH
+ *
+ * This routine can be used to perform EEH initialization for PCI
+ * devices that were added after system boot (e.g. hotplug, dlpar).
+ * Whether this actually enables EEH or not for this device depends
+ * on the type of the device, on earlier boot command-line
+ * arguments & etc.
+ */
+void eeh_add_device(struct pci_dev *);
+
+/**
+ * eeh_remove_device - undo EEH setup for the indicated pci device
+ * @dev: pci device to be removed
+ *
+ * This routine should be when a device is removed from a running
+ * system (e.g. by hotplug or dlpar).
+ */
+void eeh_remove_device(struct pci_dev *);
 
 #define EEH_DISABLE		0
 #define EEH_ENABLE		1
@@ -56,18 +73,8 @@ void *eeh_ioremap(unsigned long addr, vo
 #define EEH_RELEASE_DMA		3
 int eeh_set_option(struct pci_dev *dev, int options);
 
-/* Given a PCI device check if eeh should be configured or not.
- * This may look at firmware properties and/or kernel cmdline options.
- */
-int is_eeh_configured(struct pci_dev *dev);
-
-/* Translate a (possible) eeh token to a physical addr.
- * If "token" is not an eeh token it is simply returned under
- * the assumption that it is already a physical addr.
- */
-unsigned long eeh_token_to_phys(unsigned long token);
-
-/* EEH_POSSIBLE_ERROR() -- test for possible MMIO failure.
+/*
+ * EEH_POSSIBLE_ERROR() -- test for possible MMIO failure.
  *
  * Order this macro for performance.
  * If EEH is off for a device and it is a memory BAR, ioremap will
@@ -78,30 +85,22 @@ unsigned long eeh_token_to_phys(unsigned
  * If this macro yields TRUE, the caller relays to eeh_check_failure()
  * which does further tests out of line.
  */
-/* #define EEH_POSSIBLE_IO_ERROR(val) (~(val) == 0) */
-/* #define EEH_POSSIBLE_ERROR(addr, vaddr, val) ((vaddr) != (addr) && EEH_POSSIBLE_IO_ERROR(val) */
-/* This version is rearranged to collect some profiling data */
-#define EEH_POSSIBLE_IO_ERROR(val) (~(val) == 0 && ++eeh_total_mmio_ffs)
-#define EEH_POSSIBLE_ERROR(addr, vaddr, val) (EEH_POSSIBLE_IO_ERROR(val) && (vaddr) != (addr))
+#define EEH_POSSIBLE_IO_ERROR(val, type)	((val) == (type)~0)
+
+/* The vaddr will equal the addr if EEH checking is disabled for
+ * this device.  This is because eeh_ioremap() will not have
+ * remapped to 0xA0, and thus both vaddr and addr will be 0xE0...
+ */
+#define EEH_POSSIBLE_ERROR(addr, vaddr, val, type) \
+		((vaddr) != (addr) && EEH_POSSIBLE_IO_ERROR(val, type))
 
 /* 
  * MMIO read/write operations with EEH support.
- *
- * addr: 64b token of the form 0xA0PPBBDDyyyyyyyy
- *       0xA0     : Unmapped MMIO region
- *       PP       : PHB index (starting at zero)
- *	 BB	  : PCI Bus number under given PHB
- *	 DD	  : PCI devfn under given bus
- *       yyyyyyyy : Virtual address offset
- * 
- * An actual virtual address is produced from this token
- * by masking into the form:
- *   0xE0000000yyyyyyyy
  */
 static inline u8 eeh_readb(void *addr) {
 	volatile u8 *vaddr = (volatile u8 *)IO_TOKEN_TO_ADDR(addr);
 	u8 val = in_8(vaddr);
-	if (EEH_POSSIBLE_ERROR(addr, vaddr, val))
+	if (EEH_POSSIBLE_ERROR(addr, vaddr, val, u8))
 		return eeh_check_failure(addr, val);
 	return val;
 }
@@ -109,10 +108,11 @@ static inline void eeh_writeb(u8 val, vo
 	volatile u8 *vaddr = (volatile u8 *)IO_TOKEN_TO_ADDR(addr);
 	out_8(vaddr, val);
 }
+
 static inline u16 eeh_readw(void *addr) {
 	volatile u16 *vaddr = (volatile u16 *)IO_TOKEN_TO_ADDR(addr);
 	u16 val = in_le16(vaddr);
-	if (EEH_POSSIBLE_ERROR(addr, vaddr, val))
+	if (EEH_POSSIBLE_ERROR(addr, vaddr, val, u16))
 		return eeh_check_failure(addr, val);
 	return val;
 }
@@ -120,10 +120,22 @@ static inline void eeh_writew(u16 val, v
 	volatile u16 *vaddr = (volatile u16 *)IO_TOKEN_TO_ADDR(addr);
 	out_le16(vaddr, val);
 }
+static inline u16 eeh_raw_readw(void *addr) {
+	volatile u16 *vaddr = (volatile u16 *)IO_TOKEN_TO_ADDR(addr);
+	u16 val = in_be16(vaddr);
+	if (EEH_POSSIBLE_ERROR(addr, vaddr, val, u16))
+		return eeh_check_failure(addr, val);
+	return val;
+}
+static inline void eeh_raw_writew(u16 val, void *addr) {
+	volatile u16 *vaddr = (volatile u16 *)IO_TOKEN_TO_ADDR(addr);
+	out_be16(vaddr, val);
+}
+
 static inline u32 eeh_readl(void *addr) {
 	volatile u32 *vaddr = (volatile u32 *)IO_TOKEN_TO_ADDR(addr);
 	u32 val = in_le32(vaddr);
-	if (EEH_POSSIBLE_ERROR(addr, vaddr, val))
+	if (EEH_POSSIBLE_ERROR(addr, vaddr, val, u32))
 		return eeh_check_failure(addr, val);
 	return val;
 }
@@ -131,10 +143,22 @@ static inline void eeh_writel(u32 val, v
 	volatile u32 *vaddr = (volatile u32 *)IO_TOKEN_TO_ADDR(addr);
 	out_le32(vaddr, val);
 }
+static inline u32 eeh_raw_readl(void *addr) {
+	volatile u32 *vaddr = (volatile u32 *)IO_TOKEN_TO_ADDR(addr);
+	u32 val = in_be32(vaddr);
+	if (EEH_POSSIBLE_ERROR(addr, vaddr, val, u32))
+		return eeh_check_failure(addr, val);
+	return val;
+}
+static inline void eeh_raw_writel(u32 val, void *addr) {
+	volatile u32 *vaddr = (volatile u32 *)IO_TOKEN_TO_ADDR(addr);
+	out_be32(vaddr, val);
+}
+
 static inline u64 eeh_readq(void *addr) {
 	volatile u64 *vaddr = (volatile u64 *)IO_TOKEN_TO_ADDR(addr);
 	u64 val = in_le64(vaddr);
-	if (EEH_POSSIBLE_ERROR(addr, vaddr, val))
+	if (EEH_POSSIBLE_ERROR(addr, vaddr, val, u64))
 		return eeh_check_failure(addr, val);
 	return val;
 }
@@ -142,6 +166,17 @@ static inline void eeh_writeq(u64 val, v
 	volatile u64 *vaddr = (volatile u64 *)IO_TOKEN_TO_ADDR(addr);
 	out_le64(vaddr, val);
 }
+static inline u64 eeh_raw_readq(void *addr) {
+	volatile u64 *vaddr = (volatile u64 *)IO_TOKEN_TO_ADDR(addr);
+	u64 val = in_be64(vaddr);
+	if (EEH_POSSIBLE_ERROR(addr, vaddr, val, u64))
+		return eeh_check_failure(addr, val);
+	return val;
+}
+static inline void eeh_raw_writeq(u64 val, void *addr) {
+	volatile u64 *vaddr = (volatile u64 *)IO_TOKEN_TO_ADDR(addr);
+	out_be64(vaddr, val);
+}
 
 static inline void eeh_memset_io(void *addr, int c, unsigned long n) {
 	void *vaddr = (void *)IO_TOKEN_TO_ADDR(addr);
@@ -150,8 +185,15 @@ static inline void eeh_memset_io(void *a
 static inline void eeh_memcpy_fromio(void *dest, void *src, unsigned long n) {
 	void *vsrc = (void *)IO_TOKEN_TO_ADDR(src);
 	memcpy(dest, vsrc, n);
-	/* look for ffff's here at dest[n] */
+	/* Look for ffff's here at dest[n].  Assume that at least 4 bytes
+	 * were copied. Check all four bytes.
+	 */
+	if ((n >= 4) && 
+		(EEH_POSSIBLE_ERROR(src, vsrc, (*((u32 *) dest+n-4)), u32))) {
+		eeh_check_failure(src, (*((u32 *) dest+n-4)));
+	}
 }
+
 static inline void eeh_memcpy_toio(void *dest, void *src, unsigned long n) {
 	void *vdest = (void *)IO_TOKEN_TO_ADDR(dest);
 	memcpy(vdest, src, n);
@@ -169,8 +211,8 @@ static inline u8 eeh_inb(unsigned long p
 	if (_IO_IS_ISA(port) && !_IO_HAS_ISA_BUS)
 		return ~0;
 	val = in_8((u8 *)(port+pci_io_base));
-	if (!_IO_IS_ISA(port) && EEH_POSSIBLE_IO_ERROR(val))
-		return eeh_check_failure((void*)(port+pci_io_base), val);
+	if (!_IO_IS_ISA(port) && EEH_POSSIBLE_IO_ERROR(val, u8))
+		return eeh_check_failure((void*)(port), val);
 	return val;
 }
 
@@ -184,8 +226,8 @@ static inline u16 eeh_inw(unsigned long 
 	if (_IO_IS_ISA(port) && !_IO_HAS_ISA_BUS)
 		return ~0;
 	val = in_le16((u16 *)(port+pci_io_base));
-	if (!_IO_IS_ISA(port) && EEH_POSSIBLE_IO_ERROR(val))
-		return eeh_check_failure((void*)(port+pci_io_base), val);
+	if (!_IO_IS_ISA(port) && EEH_POSSIBLE_IO_ERROR(val, u16))
+		return eeh_check_failure((void*)(port), val);
 	return val;
 }
 
@@ -199,8 +241,8 @@ static inline u32 eeh_inl(unsigned long 
 	if (_IO_IS_ISA(port) && !_IO_HAS_ISA_BUS)
 		return ~0;
 	val = in_le32((u32 *)(port+pci_io_base));
-	if (!_IO_IS_ISA(port) && EEH_POSSIBLE_IO_ERROR(val))
-		return eeh_check_failure((void*)(port+pci_io_base), val);
+	if (!_IO_IS_ISA(port) && EEH_POSSIBLE_IO_ERROR(val, u32))
+		return eeh_check_failure((void*)(port), val);
 	return val;
 }
 
@@ -209,4 +251,23 @@ static inline void eeh_outl(u32 val, uns
 		return out_le32((u32 *)(port+pci_io_base), val);
 }
 
-#endif /* _EEH_H */
+/* in-string eeh macros */
+static inline void eeh_insb(unsigned long port, void * buf, int ns) {
+	_insb((u8 *)(port+pci_io_base), buf, ns);
+	if (!_IO_IS_ISA(port) && EEH_POSSIBLE_IO_ERROR((*(((u8*)buf)+ns-1)), u8))
+		eeh_check_failure((void*)(port), *(u8*)buf);
+}
+
+static inline void eeh_insw_ns(unsigned long port, void * buf, int ns) {
+	_insw_ns((u16 *)(port+pci_io_base), buf, ns);
+	if (!_IO_IS_ISA(port) && EEH_POSSIBLE_IO_ERROR((*(((u16*)buf)+ns-1)), u16))
+		eeh_check_failure((void*)(port), *(u16*)buf);
+}
+
+static inline void eeh_insl_ns(unsigned long port, void * buf, int nl) {
+	_insl_ns((u32 *)(port+pci_io_base), buf, nl);
+	if (!_IO_IS_ISA(port) && EEH_POSSIBLE_IO_ERROR((*(((u32*)buf)+nl-1)), u32))
+		eeh_check_failure((void*)(port), *(u32*)buf);
+}
+
+#endif /* _PPC64_EEH_H */
diff -purN linux-2.5/include/asm-ppc64/hardirq.h linuxppc64-2.5/include/asm-ppc64/hardirq.h
--- linux-2.5/include/asm-ppc64/hardirq.h	2003-09-11 04:07:58.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/hardirq.h	2004-02-18 06:28:00.000000000 +0000
@@ -80,7 +80,8 @@ typedef struct {
 
 #define irq_enter()		(preempt_count() += HARDIRQ_OFFSET)
 
-#ifdef CONFIG_PREEMPT
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_DEBUG_SPINLOCK_SLEEP)
+#include <linux/smp_lock.h>
 # define in_atomic()	((preempt_count() & ~PREEMPT_ACTIVE) != kernel_locked())
 # define IRQ_EXIT_OFFSET (HARDIRQ_OFFSET-1)
 #else
diff -purN linux-2.5/include/asm-ppc64/hvcall.h linuxppc64-2.5/include/asm-ppc64/hvcall.h
--- linux-2.5/include/asm-ppc64/hvcall.h	2004-02-05 21:10:52.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/hvcall.h	2004-02-28 14:18:30.000000000 +0000
@@ -4,6 +4,14 @@
 #define H_Success	0
 #define H_Busy		1	/* Hardware busy -- retry later */
 #define H_Constrained	4	/* Resource request constrained to max allowed */
+#define H_LongBusyStartRange   9900  /* Start of long busy range */
+#define H_LongBusyOrder1msec   9900  /* Long busy, hint that 1msec is a good time to retry */
+#define H_LongBusyOrder10msec  9901  /* Long busy, hint that 10msec is a good time to retry */
+#define H_LongBusyOrder100msec 9902  /* Long busy, hint that 100msec is a good time to retry */
+#define H_LongBusyOrder1sec    9903  /* Long busy, hint that 1sec is a good time to retry */
+#define H_LongBusyOrder10sec   9904  /* Long busy, hint that 10sec is a good time to retry */
+#define H_LongBusyOrder100sec  9905  /* Long busy, hint that 100sec is a good time to retry */
+#define H_LongBusyEndRange     9905  /* End of long busy range */ 
 #define H_Hardware	-1	/* Hardware error */
 #define H_Function	-2	/* Function not supported */
 #define H_Privilege	-3	/* Caller not privileged */
@@ -21,6 +29,16 @@
 #define H_RemoteParm           -15
 #define H_Resource             -16
 
+/* Long Busy is a condition that can be returned by the firmware
+ * when a call cannot be completed now, but the identical call
+ * should be retried later.  This prevents calls blocking in the
+ * firmware for long periods of time. Annoyingly the firmware can return
+ * a range of return codes, hinting at how long we should wait before
+ * retrying.  If you don't care for the hint, the macro below is a good
+ * way to check for the long_busy return codes 
+ */
+#define H_isLongBusy(x)  ((x >= H_LongBusyStartRange) && (x <= H_LongBusyEndRange))
+
 /* Flags */
 #define H_LARGE_PAGE		(1UL<<(63-16))
 #define H_EXACT		    (1UL<<(63-24))	/* Use exact PTE or return H_PTEG_FULL */
@@ -76,7 +94,7 @@
 #define H_PROD		        0xE8
 #define H_GET_PPP		0xEC
 #define H_SET_PPP		0xF0
-#define H_SET_PURR		0xF4
+#define H_PURR			0xF4
 #define H_PIC		        0xF8
 #define H_REG_CRQ		0xFC
 #define H_FREE_CRQ		0x100
@@ -84,6 +102,9 @@
 #define H_SEND_CRQ		0x108
 #define H_COPY_RDMA             0x110
 #define H_POLL_PENDING	        0x1D8
+#define H_VTERM_PARTNER_INFO	0x150
+#define H_REGISTER_VTERM		0x154
+#define H_FREE_VTERM			0x158
 
 /* plpar_hcall() -- Generic call interface using above opcodes
  *
diff -purN linux-2.5/include/asm-ppc64/hvconsole.h linuxppc64-2.5/include/asm-ppc64/hvconsole.h
--- linux-2.5/include/asm-ppc64/hvconsole.h	2004-02-05 21:10:52.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/hvconsole.h	2004-02-27 21:16:22.000000000 +0000
@@ -22,9 +22,39 @@
 #ifndef _PPC64_HVCONSOLE_H
 #define _PPC64_HVCONSOLE_H
 
+#include <linux/list.h>
+
+#define MAX_NR_HVC_CONSOLES	4
+
 extern int hvc_get_chars(int index, char *buf, int count);
 extern int hvc_put_chars(int index, const char *buf, int count);
-extern int hvc_count(int *start_termno);
+extern int hvc_find_vterms(void);
+
+extern int hvc_instantiate(void);
+
+/* hvterm_get/put_chars() do not work with HVSI console protocol; present only
+ * for HVCS console server driver */
+extern int hvterm_get_chars(uint32_t vtermno, char *buf, int count);
+extern int hvterm_put_chars(uint32_t vtermno, const char *buf, int count);
+
+/* Converged Location Code length */
+#define HVCS_CLC_LENGTH	79
+
+struct hvcs_partner_info {
+	/* list management */
+	struct list_head node;
+	/* partner unit address */
+	unsigned int unit_address;
+	/*partner partition ID */
+	unsigned int partition_ID;
+	/* CLC (79 chars) + 1 Null-term char */
+	char location_code[HVCS_CLC_LENGTH + 1];
+};
+
+extern int hvcs_free_partner_info(struct list_head *head);
+extern int hvcs_get_partner_info(unsigned int unit_address, struct list_head *head);
+extern int hvcs_register_connection(unsigned int unit_address, unsigned int p_partition_ID, unsigned int p_unit_address);
+extern int hvcs_free_connection(unsigned int unit_address);
 
 #endif /* _PPC64_HVCONSOLE_H */
 
diff -purN linux-2.5/include/asm-ppc64/hw_irq.h linuxppc64-2.5/include/asm-ppc64/hw_irq.h
--- linux-2.5/include/asm-ppc64/hw_irq.h	2004-01-21 01:50:56.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/hw_irq.h	2004-01-24 06:44:51.000000000 +0000
@@ -75,9 +75,24 @@ static inline void __do_save_and_cli(uns
 
 #endif /* CONFIG_PPC_ISERIES */
 
-#define mask_irq(irq) ({if (irq_desc[irq].handler && irq_desc[irq].handler->disable) irq_desc[irq].handler->disable(irq);})
-#define unmask_irq(irq) ({if (irq_desc[irq].handler && irq_desc[irq].handler->enable) irq_desc[irq].handler->enable(irq);})
-#define ack_irq(irq) ({if (irq_desc[irq].handler && irq_desc[irq].handler->ack) irq_desc[irq].handler->ack(irq);})
+#define mask_irq(irq)						\
+	({							\
+	 	irq_desc_t *desc = get_irq_desc(irq);		\
+		if (desc->handler && desc->handler->disable)	\
+			desc->handler->disable(irq);		\
+	})
+#define unmask_irq(irq)						\
+	({							\
+	 	irq_desc_t *desc = get_irq_desc(irq);		\
+		if (desc->handler && desc->handler->enable)	\
+			desc->handler->enable(irq);		\
+	})
+#define ack_irq(irq)						\
+	({							\
+	 	irq_desc_t *desc = get_irq_desc(irq);		\
+		if (desc->handler && desc->handler->ack)	\
+			desc->handler->ack(irq);		\
+	})
 
 /* Should we handle this via lost interrupts and IPIs or should we don't care like
  * we do now ? --BenH.
diff -purN linux-2.5/include/asm-ppc64/iSeries/vio.h linuxppc64-2.5/include/asm-ppc64/iSeries/vio.h
--- linux-2.5/include/asm-ppc64/iSeries/vio.h	2004-02-23 16:39:09.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/vio.h	2004-03-02 03:00:22.000000000 +0000
@@ -127,4 +127,8 @@ enum viorc {
 	viorc_openRejected = 0x0301
 };
 
+struct device;
+
+extern struct device *iSeries_vio_dev;
+
 #endif /* _ISERIES_VIO_H */
diff -purN linux-2.5/include/asm-ppc64/io.h linuxppc64-2.5/include/asm-ppc64/io.h
--- linux-2.5/include/asm-ppc64/io.h	2004-02-27 23:02:35.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/io.h	2004-03-02 11:34:18.000000000 +0000
@@ -58,6 +58,13 @@ extern unsigned long pci_io_base;
 #define outb(data,addr)		writeb(data,((unsigned long)(addr)))  
 #define outw(data,addr)		writew(data,((unsigned long)(addr)))  
 #define outl(data,addr)		writel(data,((unsigned long)(addr)))
+/*
+ * The *_ns versions below don't do byte-swapping.
+ * Neither do the standard versions now, these are just here
+ * for older code.
+ */
+#define insw_ns(port, buf, ns)	_insw_ns((u16 *)((port)+pci_io_base), (buf), (ns))
+#define insl_ns(port, buf, nl)	_insl_ns((u32 *)((port)+pci_io_base), (buf), (nl))
 #else
 #define __raw_readb(addr)       (*(volatile unsigned char *)(addr))
 #define __raw_readw(addr)       (*(volatile unsigned short *)(addr))
@@ -90,12 +97,16 @@ extern unsigned long pci_io_base;
  * They are only used in practice for transferring buffers which
  * are arrays of bytes, and byte-swapping is not appropriate in
  * that case.  - paulus */
-#define insb(port, buf, ns)	_insb((u8 *)((port)+pci_io_base), (buf), (ns))
-#define outsb(port, buf, ns)	_outsb((u8 *)((port)+pci_io_base), (buf), (ns))
-#define insw(port, buf, ns)	_insw_ns((u16 *)((port)+pci_io_base), (buf), (ns))
-#define outsw(port, buf, ns)	_outsw_ns((u16 *)((port)+pci_io_base), (buf), (ns))
-#define insl(port, buf, nl)	_insl_ns((u32 *)((port)+pci_io_base), (buf), (nl))
-#define outsl(port, buf, nl)	_outsl_ns((u32 *)((port)+pci_io_base), (buf), (nl))
+#define insb(port, buf, ns)	eeh_insb((port), (buf), (ns))
+#define insw(port, buf, ns)	eeh_insw_ns((port), (buf), (ns))
+#define insl(port, buf, nl)	eeh_insl_ns((port), (buf), (nl))
+#define insw_ns(port, buf, ns)	eeh_insw_ns((port), (buf), (ns))
+#define insl_ns(port, buf, nl)	eeh_insl_ns((port), (buf), (nl))
+
+#define outsb(port, buf, ns)  _outsb((u8 *)((port)+pci_io_base), (buf), (ns))
+#define outsw(port, buf, ns)  _outsw_ns((u16 *)((port)+pci_io_base), (buf), (ns))
+#define outsl(port, buf, nl)  _outsl_ns((u32 *)((port)+pci_io_base), (buf), (nl))
+
 #endif
 
 #define readb_relaxed(addr) readb(addr)
@@ -130,9 +141,7 @@ extern void _outsl_ns(volatile u32 *port
  * Neither do the standard versions now, these are just here
  * for older code.
  */
-#define insw_ns(port, buf, ns)	_insw_ns((u16 *)((port)+pci_io_base), (buf), (ns))
 #define outsw_ns(port, buf, ns)	_outsw_ns((u16 *)((port)+pci_io_base), (buf), (ns))
-#define insl_ns(port, buf, nl)	_insl_ns((u32 *)((port)+pci_io_base), (buf), (nl))
 #define outsl_ns(port, buf, nl)	_outsl_ns((u32 *)((port)+pci_io_base), (buf), (nl))
 
 
@@ -204,6 +213,9 @@ static inline void iosync(void)
 
 /*
  * 8, 16 and 32 bit, big and little endian I/O operations, with barrier.
+ * These routines do not perform EEH-related I/O address translation,
+ * and should not be used directly by device drivers.  Use inb/readb
+ * instead.
  */
 static inline int in_8(volatile unsigned char *addr)
 {
diff -purN linux-2.5/include/asm-ppc64/irq.h linuxppc64-2.5/include/asm-ppc64/irq.h
--- linux-2.5/include/asm-ppc64/irq.h	2004-02-25 02:54:12.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/irq.h	2004-02-26 10:56:03.000000000 +0000
@@ -11,6 +11,11 @@
 
 #include <asm/atomic.h>
 
+/*
+ * Maximum number of interrupt sources that we can handle.
+ */
+#define NR_IRQS		512
+
 extern void disable_irq(unsigned int);
 extern void disable_irq_nosync(unsigned int);
 extern void enable_irq(unsigned int);
@@ -18,12 +23,11 @@ extern void enable_irq(unsigned int);
 /* this number is used when no interrupt has been assigned */
 #define NO_IRQ			(-1)
 
-/*
- * this is the maximum number of virtual irqs we will use.
- */
-#define NR_IRQS			512
+#define get_irq_desc(irq) (&irq_desc[(irq)])
 
-#define NUM_8259_INTERRUPTS	16
+/* Define a way to iterate across irqs. */
+#define for_each_irq(i) \
+	for ((i) = 0; (i) < NR_IRQS; ++(i))
 
 /* Interrupt numbers are virtual in case they are sparsely
  * distributed by the hardware.
@@ -41,12 +45,35 @@ static inline unsigned int virt_irq_to_r
 	return virt_irq_to_real_map[virt_irq];
 }
 
+/*
+ * Because many systems have two overlapping names spaces for
+ * interrupts (ISA and XICS for example), and the ISA interrupts
+ * have historically not been easy to renumber, we allow ISA
+ * interrupts to take values 0 - 15, and shift up the remaining
+ * interrupts by 0x10.
+ */
+#define NUM_ISA_INTERRUPTS	0x10
+extern int __irq_offset_value;
+
+static inline int irq_offset_up(int irq)
+{
+	return(irq + __irq_offset_value);
+}
+
+static inline int irq_offset_down(int irq)
+{
+	return(irq - __irq_offset_value);
+}
+
+static inline int irq_offset_value(void)
+{
+	return __irq_offset_value;
+}
+
 static __inline__ int irq_canonicalize(int irq)
 {
 	return irq;
 }
 
-#define NR_MASK_WORDS	((NR_IRQS + 63) / 64)
-
 #endif /* _ASM_IRQ_H */
 #endif /* __KERNEL__ */
diff -purN linux-2.5/include/asm-ppc64/kdb.h linuxppc64-2.5/include/asm-ppc64/kdb.h
--- linux-2.5/include/asm-ppc64/kdb.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/kdb.h	2003-10-13 19:47:30.000000000 +0000
@@ -0,0 +1,82 @@
+/*
+ * Minimalist Kernel Debugger
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *	Scott Lurndal			1999/12/12
+ *		v1.0 restructuring.
+ */
+#if !defined(_ASM_KDB_H)
+#define _ASM_KDB_H
+	/*
+	 * KDB_ENTER() is a macro which causes entry into the kernel
+	 * debugger from any point in the kernel code stream.  If it 
+	 * is intended to be used from interrupt level, it must  use
+	 * a non-maskable entry method.
+	 */
+#define KDB_ENTER()	kdb(KDB_REASON_CALL,0,0);
+
+#ifndef ElfW
+# if ELFCLASSM == ELFCLASS32
+#  define ElfW(x)  Elf32_ ## x
+#  define ELFW(x)  ELF32_ ## x
+# else
+#  define ElfW(x)  Elf64_ ## x
+#  define ELFW(x)  ELF64_ ## x
+# endif
+#endif
+
+	/*
+	 * Define the exception frame for this architecture
+	 */
+struct pt_regs;
+typedef struct pt_regs	*kdb_eframe_t;
+
+	/*
+	 * Needed for exported symbols.
+	 */
+typedef unsigned long kdb_machreg_t;
+
+#define kdb_machreg_fmt		"0x%016lx"
+#define kdb_machreg_fmt0	"0x%016lx"
+#define kdb_bfd_vma_fmt		"0x%016lx"
+#define kdb_bfd_vma_fmt0	"0x%016lx"
+#define kdb_elfw_addr_fmt	"0x%016lx"
+#define kdb_elfw_addr_fmt0	"0x%016lx"
+
+	/*
+	 * Per cpu arch specific kdb state.  Must be in range 0xff000000.
+	 */
+#define KDB_STATE_A_IF		0x01000000	/* Saved IF flag */
+
+	 /*
+	  * Interface from kernel trap handling code to kernel debugger.
+	  */
+extern int	kdba_callback_die(struct pt_regs *, int, long, void*);
+extern int	kdba_callback_bp(struct pt_regs *, int, long, void*);
+extern int	kdba_callback_debug(struct pt_regs *, int, long, void *);
+
+#include <linux/types.h>
+extern int kdba_putarea_size(unsigned long to_xxx, void *from, size_t size);
+extern int kdba_getarea_size(void *to, unsigned long from_xxx, size_t size);
+
+static inline int
+kdba_verify_rw(unsigned long addr, size_t size)
+{
+	unsigned char data[size];
+	return(kdba_getarea_size(data, addr, size) || kdba_putarea_size(addr, data, size));
+}
+
+#endif	/* ASM_KDB_H */
diff -purN linux-2.5/include/asm-ppc64/kdbprivate.h linuxppc64-2.5/include/asm-ppc64/kdbprivate.h
--- linux-2.5/include/asm-ppc64/kdbprivate.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/kdbprivate.h	2003-10-13 19:50:10.000000000 +0000
@@ -0,0 +1,120 @@
+/*
+ * Minimalist Kernel Debugger
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *	Scott Lurndal			1999/12/12
+ *		v1.0 restructuring.
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ */
+#if !defined(_ASM_KDBPRIVATE_H)
+#define _ASM_KDBPRIVATE_H
+
+typedef unsigned long kdb_machinst_t;
+
+	/*
+	 * KDB_MAXBPT describes the total number of breakpoints
+	 * supported by this architecure.  
+	 */
+#define KDB_MAXBPT	4
+	/*
+	 * KDB_MAXHARDBPT describes the total number of hardware
+	 * breakpoint registers that exist.
+	 */
+#define KDB_MAXHARDBPT	 1
+        /*
+         * Provide space for KDB_MAX_COMMANDS commands.
+         */
+#define KDB_MAX_COMMANDS        125
+
+	/*
+	 * Platform specific environment entries
+	 */
+#define KDB_PLATFORM_ENV	"IDMODE=PPC64", "BYTESPERWORD=8", "IDCOUNT=16"
+
+	/*
+	 * Define the direction that the stack grows
+	 */
+#define KDB_STACK_DIRECTION	-1	/* Stack grows down */
+
+	/*
+	 * Support for ia32 debug registers 
+	 */
+typedef struct _kdbhard_bp {
+	kdb_machreg_t	bph_reg;	/* Register this breakpoint uses */
+
+	unsigned int	bph_free:1;	/* Register available for use */
+	unsigned int	bph_data:1;	/* Data Access breakpoint */
+
+	unsigned int	bph_write:1;	/* Write Data breakpoint */
+	unsigned int	bph_mode:2;	/* 0=inst, 1=write, 2=io, 3=read */
+	unsigned int	bph_length:2;	/* 0=1, 1=2, 2=BAD, 3=4 (bytes) */
+} kdbhard_bp_t;
+
+extern kdbhard_bp_t	kdb_hardbreaks[/* KDB_MAXHARDBPT */];
+
+#define PPC64_BREAKPOINT_INSTRUCTION 0x7fe00008    
+#define PPC64_ADJUST_OFFSET 0x00   
+
+#define KDB_HAVE_LONGJMP 
+#ifdef KDB_HAVE_LONGJMP
+typedef struct __kdb_jmp_buf {
+	unsigned int regs[100];
+} kdb_jmp_buf;
+extern int kdb_setjmp(kdb_jmp_buf *);
+extern void kdba_longjmp(kdb_jmp_buf *, int);
+extern kdb_jmp_buf  kdbjmpbuf[];
+#endif	/* KDB_HAVE_LONGJMP */
+
+
+/*
+ A traceback table typically follows each function.
+ The find_tb_table() func will fill in this struct.  Note that the struct
+ is not an exact match with the encoded table defined by the ABI.  It is
+ defined here more for programming convenience.
+ */
+typedef struct {
+    unsigned long	flags;		/* flags: */
+#define KDBTBTAB_FLAGSGLOBALLINK	(1L<<47)
+#define KDBTBTAB_FLAGSISEPROL		(1L<<46)
+#define KDBTBTAB_FLAGSHASTBOFF		(1L<<45)
+#define KDBTBTAB_FLAGSINTPROC		(1L<<44)
+#define KDBTBTAB_FLAGSHASCTL		(1L<<43)
+#define KDBTBTAB_FLAGSTOCLESS		(1L<<42)
+#define KDBTBTAB_FLAGSFPPRESENT		(1L<<41)
+#define KDBTBTAB_FLAGSNAMEPRESENT	(1L<<38)
+#define KDBTBTAB_FLAGSUSESALLOCA	(1L<<37)
+#define KDBTBTAB_FLAGSSAVESCR		(1L<<33)
+#define KDBTBTAB_FLAGSSAVESLR		(1L<<32)
+#define KDBTBTAB_FLAGSSTORESBC		(1L<<31)
+#define KDBTBTAB_FLAGSFIXUP		(1L<<30)
+#define KDBTBTAB_FLAGSPARMSONSTK	(1L<<0)
+    unsigned char	fp_saved;	/* num fp regs saved f(32-n)..f31 */
+    unsigned char	gpr_saved;	/* num gpr's saved */
+    unsigned char	fixedparms;	/* num fixed point parms */
+    unsigned char	floatparms;	/* num float parms */
+    unsigned char	parminfo[32];	/* types of args.  null terminated */
+#define KDBTBTAB_PARMFIXED 1
+#define KDBTBTAB_PARMSFLOAT 2
+#define KDBTBTAB_PARMDFLOAT 3
+    unsigned int	tb_offset;	/* offset from start of func */
+    unsigned long	funcstart;	/* addr of start of function */
+    char		name[64];	/* name of function (null terminated)*/
+    kdb_symtab_t	symtab;		/* fake symtab entry */
+} kdbtbtable_t;
+int kdba_find_tb_table(kdb_machreg_t eip, kdbtbtable_t *tab);
+
+#endif	/* !_ASM_KDBPRIVATE_H */
diff -purN linux-2.5/include/asm-ppc64/machdep.h linuxppc64-2.5/include/asm-ppc64/machdep.h
--- linux-2.5/include/asm-ppc64/machdep.h	2004-02-27 23:02:35.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/machdep.h	2004-03-01 13:40:29.000000000 +0000
@@ -11,6 +11,7 @@
 
 #include <linux/config.h>
 #include <linux/seq_file.h>
+#include <linux/irq.h>
 
 struct pt_regs;
 struct pci_bus;	
@@ -81,6 +82,7 @@ struct machdep_calls {
 	void		(*restart)(char *cmd);
 	void		(*power_off)(void);
 	void		(*halt)(void);
+	void		(*panic)(void);
 
 	int		(*set_rtc_time)(struct rtc_time *);
 	void		(*get_rtc_time)(struct rtc_time *);
diff -purN linux-2.5/include/asm-ppc64/rtas.h linuxppc64-2.5/include/asm-ppc64/rtas.h
--- linux-2.5/include/asm-ppc64/rtas.h	2004-01-19 06:28:25.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/rtas.h	2004-02-27 20:53:50.000000000 +0000
@@ -175,6 +175,7 @@ extern void call_rtas_display_status(cha
 extern void rtas_restart(char *cmd);
 extern void rtas_power_off(void);
 extern void rtas_halt(void);
+extern void rtas_os_term(void);
 extern int rtas_get_sensor(int sensor, int index, int *state);
 extern int rtas_get_power_level(int powerdomain, int *level);
 extern int rtas_set_indicator(int indicator, int index, int new_value);
diff -purN linux-2.5/include/asm-ppc64/smp.h linuxppc64-2.5/include/asm-ppc64/smp.h
--- linux-2.5/include/asm-ppc64/smp.h	2004-02-23 16:39:10.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/smp.h	2004-02-26 10:56:03.000000000 +0000
@@ -67,6 +67,8 @@ extern cpumask_t cpu_available_map;
 #endif
 #define PPC_MSG_DEBUGGER_BREAK  3
 
+extern cpumask_t irq_affinity[];
+
 void smp_init_iSeries(void);
 void smp_init_pSeries(void);
 
diff -purN linux-2.5/include/asm-ppc64/spinlock.h linuxppc64-2.5/include/asm-ppc64/spinlock.h
--- linux-2.5/include/asm-ppc64/spinlock.h	2002-09-10 09:50:15.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/spinlock.h	2004-01-22 06:11:59.000000000 +0000
@@ -6,16 +6,28 @@
  *
  * Copyright (C) 2001 Paul Mackerras <paulus@au.ibm.com>, IBM
  * Copyright (C) 2001 Anton Blanchard <anton@au.ibm.com>, IBM
- *
- * Type of int is used as a full 64b word is not necessary.
+ * Copyright (C) 2002 Dave Engebretsen <engebret@us.ibm.com>, IBM
+ *   Rework to support virtual processors
  *
  * This program is free software; you can redistribute it and/or
  * modify it under the terms of the GNU General Public License
  * as published by the Free Software Foundation; either version
  * 2 of the License, or (at your option) any later version.
  */
+
+#include <linux/config.h>
+#include <asm/hvcall.h>
+
+/*
+ * The following define is being used to select basic or shared processor
+ * locking when running on an RPA platform.  As we do more performance
+ * tuning, I would expect this selection mechanism to change.  Dave E. 
+ */
+#define SPLPAR_LOCKS
+#define HVSC			".long 0x44000022\n"
+
 typedef struct {
-	volatile unsigned int lock;
+	volatile unsigned long lock;
 } spinlock_t;
 
 #ifdef __KERNEL__
@@ -25,15 +37,15 @@ typedef struct {
 
 static __inline__ int _raw_spin_trylock(spinlock_t *lock)
 {
-	unsigned int tmp;
+	unsigned long tmp;
 
 	__asm__ __volatile__(
-"1:	lwarx		%0,0,%1		# spin_trylock\n\
-	cmpwi		0,%0,0\n\
+"1:	ldarx		%0,0,%1		# spin_trylock\n\
+	cmpdi		0,%0,0\n\
 	li		%0,0\n\
 	bne-		2f\n\
 	li		%0,1\n\
-	stwcx.		%0,0,%1\n\
+	stdcx.		13,0,%1\n\
 	bne-		1b\n\
 	isync\n\
 2:"	: "=&r"(tmp)
@@ -43,28 +55,115 @@ static __inline__ int _raw_spin_trylock(
 	return tmp;
 }
 
+/*
+ * Spin lock states:
+ *   0        : Unlocked
+ *   Negative : Locked.  Value is paca pointer (0xc...0) of holder
+ */
+#ifdef CONFIG_PPC_ISERIES
+static __inline__ void _raw_spin_lock(spinlock_t *lock)
+{
+	unsigned long tmp, tmp2;
+
+	__asm__ __volatile__(
+	"b		2f		# spin_lock\n\
+1:"
+	HMT_LOW
+"       ldx		%0,0,%2         # load the lock value\n\
+	cmpdi		0,%0,0          # if not locked, try to acquire\n\
+	beq-		2f\n\
+        lwz             5,0x280(%0)     # load yield counter\n\
+        andi.           %1,5,1          # if even then spin\n\
+        beq             1b\n\
+        lwsync                          # if odd, give up cycles\n\
+        ldx             %1,0,%2         # reverify the lock holder\n\
+        cmpd            %0,%1\n\
+        bne             1b              # new holder so restart\n\
+        li              3,0x25          # yield hcall 0x8-12 \n\
+        rotrdi          3,3,1           #   put the bits in the right spot\n\
+        lhz             4,0x18(%0)      # processor number\n\
+        sldi            4,4,32          #   move into top half of word\n\
+        or              5,5,4           # r5 has yield cnt - or it in\n\
+        li              4,2             # yield to processor\n\
+        li              0,-1            # indicate an hcall\n\
+        sc                              # do the hcall \n\
+        b               1b\n\
+2: \n"
+	HMT_MEDIUM
+" 	ldarx		%0,0,%2\n\
+	cmpdi		0,%0,0\n\
+	bne-		1b\n\
+	stdcx.		13,0,%2\n\
+	bne-		2b\n\
+	isync"
+	: "=&b"(tmp), "=&r"(tmp2)
+	: "r"(&lock->lock)
+	: "r0", "r3", "r4", "r5", "ctr", "cr0", "cr1", "cr2", "cr3", "cr4", 
+	  "xer", "memory");
+}
+#else
+#ifdef SPLPAR_LOCKS
 static __inline__ void _raw_spin_lock(spinlock_t *lock)
 {
-	unsigned int tmp;
+	unsigned long tmp, tmp2;
 
 	__asm__ __volatile__(
 	"b		2f		# spin_lock\n\
 1:"
 	HMT_LOW
-"	lwzx		%0,0,%1\n\
-	cmpwi		0,%0,0\n\
-	bne+		1b\n"
+"       ldx		%0,0,%2         # load the lock value\n\
+	cmpdi		0,%0,0          # if not locked, try to acquire\n\
+	beq-		2f\n\
+        lwz             5,0x280(%0)     # load dispatch counter\n\
+        andi.           %1,5,1          # if even then spin\n\
+        beq             1b\n\
+        lwsync                          # if odd, give up cycles\n\
+        ldx             %1,0,%2         # reverify the lock holder\n\
+        cmpd            %0,%1\n\
+        bne             1b              # new holder so restart\n\
+        li              3,0xE4          # give up the cycles H_CONFER\n\
+        lhz             4,0x18(%0)      # processor number\n\
+                                        # r5 has dispatch cnt already\n"
+	HVSC
+"        b               1b\n\
+2: \n"
 	HMT_MEDIUM
-"2:	lwarx		%0,0,%1\n\
-	cmpwi		0,%0,0\n\
+" 	ldarx		%0,0,%2\n\
+	cmpdi		0,%0,0\n\
 	bne-		1b\n\
-	stwcx.		%2,0,%1\n\
+	stdcx.		13,0,%2\n\
+	bne-		2b\n\
+	isync"
+	: "=&b"(tmp), "=&r"(tmp2)
+	: "r"(&lock->lock)
+	: "r3", "r4", "r5", "cr0", "cr1", "ctr", "xer", "memory");
+}
+#else
+static __inline__ void _raw_spin_lock(spinlock_t *lock)
+{
+	unsigned long tmp;
+
+	__asm__ __volatile__(
+       "b		2f		# spin_lock\n\
+1:"
+	HMT_LOW
+"       ldx		%0,0,%1         # load the lock value\n\
+	cmpdi		0,%0,0          # if not locked, try to acquire\n\
+	bne+		1b\n\
+2: \n"
+	HMT_MEDIUM
+" 	ldarx		%0,0,%1\n\
+	cmpdi		0,%0,0\n\
+	bne-		1b\n\
+	stdcx.		13,0,%1\n\
 	bne-		2b\n\
 	isync"
 	: "=&r"(tmp)
-	: "r"(&lock->lock), "r"(1)
+	: "r"(&lock->lock)
 	: "cr0", "memory");
 }
+#endif
+#endif
 
 static __inline__ void _raw_spin_unlock(spinlock_t *lock)
 {
@@ -81,25 +180,35 @@ static __inline__ void _raw_spin_unlock(
  * can "mix" irq-safe locks - any writer needs to get a
  * irq-safe write-lock, but readers can get non-irqsafe
  * read-locks.
+ *
+ * Write lock states:
+ *   0        : Unlocked
+ *   Positive : Reader count
+ *   Negative : Writer locked.  Value is paca pointer (0xc...0) of holder
+ *
+ * If lock is not held, try to acquire.
+ * If lock is held by a writer, confer cycles to the holder.
+ * If lock is help by reader(s), spin.  Need to experiment with confer all
+ *   option.  This is likely a pretty weak optimization and may not be 
+ *   worth the additional path length.
  */
 typedef struct {
-	volatile signed int lock;
+	volatile signed long lock;
 } rwlock_t;
 
 #define RW_LOCK_UNLOCKED (rwlock_t) { 0 }
 
 static __inline__ int _raw_read_trylock(rwlock_t *rw)
 {
-	unsigned int tmp;
-	unsigned int ret;
+	unsigned long tmp;
+	unsigned long ret;
 
 	__asm__ __volatile__(
-"1:	lwarx		%0,0,%2		# read_trylock\n\
+"1:	ldarx		%0,0,%2		# read_trylock\n\
 	li		%1,0\n\
-	extsw		%0,%0\n\
 	addic.		%0,%0,1\n\
 	ble-		2f\n\
-	stwcx.		%0,0,%2\n\
+	stdcx.		%0,0,%2\n\
 	bne-		1b\n\
 	li		%1,1\n\
 	isync\n\
@@ -110,39 +219,118 @@ static __inline__ int _raw_read_trylock(
 	return ret;
 }
 
+#ifdef CONFIG_PPC_ISERIES
 static __inline__ void _raw_read_lock(rwlock_t *rw)
 {
-	unsigned int tmp;
+	unsigned long tmp, tmp2;
 
 	__asm__ __volatile__(
 	"b		2f		# read_lock\n\
 1:"
 	HMT_LOW
-"	lwax		%0,0,%1\n\
-	cmpwi		0,%0,0\n\
-	blt+		1b\n"
+"	ldx		%0,0,%2\n\
+	cmpdi		0,%0,0\n\
+	bge-		2f\n\
+        lwz             5,0x280(%0)     # load yield counter\n\
+        andi.           %1,5,1          # if even then spin\n\
+        beq             1b\n\
+        lwsync                          # if odd, give up cycles\n\
+        ldx             %1,0,%2         # reverify the lock holder\n\
+        cmpd            %0,%1\n\
+        bne             1b              # new holder so restart\n\
+        li              3,0x25          # yield hcall 0x8-12 \n\
+        rotrdi          3,3,1           #   put the bits in the right spot\n\
+        lhz             4,0x18(%0)      # processor number\n\
+        sldi            4,4,32          #   move into top half of word\n\
+        or              5,5,4           # r5 has yield cnt - or it in\n\
+        li              4,2             # yield to processor\n\
+        li              0,-1            # indicate an hcall\n\
+        sc                              # do the hcall \n\
+2: \n"
 	HMT_MEDIUM
-"2:	lwarx		%0,0,%1\n\
-	extsw		%0,%0\n\
+" 	ldarx		%0,0,%2\n\
 	addic.		%0,%0,1\n\
 	ble-		1b\n\
-	stwcx.		%0,0,%1\n\
+	stdcx.		%0,0,%2\n\
+	bne-		2b\n\
+	isync"
+	: "=&b"(tmp), "=&r"(tmp2)
+	: "r"(&rw->lock)
+	: "r0", "r3", "r4", "r5", "ctr", "cr0", "cr1", "cr2", "cr3", "cr4", 
+	  "xer", "memory");
+}
+#else
+#ifdef SPLPAR_LOCKS
+static __inline__ void _raw_read_lock(rwlock_t *rw)
+{
+	unsigned long tmp, tmp2;
+
+	__asm__ __volatile__(
+	"b		2f		# read_lock\n\
+1:"
+	HMT_LOW
+"	ldx		%0,0,%2\n\
+	cmpdi		0,%0,0\n\
+	bge-		2f\n\
+        lwz             5,0x280(%0)     # load dispatch counter\n\
+        andi.           %1,5,1          # if even then spin\n\
+        beq             1b\n\
+        lwsync                          # if odd, give up cycles\n\
+        ldx             %1,0,%2         # reverify the lock holder\n\
+        cmpd            %0,%1\n\
+        bne             1b              # new holder so restart\n\
+        li              3,0xE4          # give up the cycles H_CONFER\n\
+        lhz             4,0x18(%0)      # processor number\n\
+                                        # r5 has dispatch cnt already\n"
+	HVSC
+"2: \n"
+	HMT_MEDIUM
+" 	ldarx		%0,0,%2\n\
+	addic.		%0,%0,1\n\
+	ble-		1b\n\
+	stdcx.		%0,0,%2\n\
+	bne-		2b\n\
+	isync"
+	: "=&b"(tmp), "=&r"(tmp2)
+	: "r"(&rw->lock)
+	: "r3", "r4", "r5", "cr0", "cr1", "ctr", "xer", "memory");
+}
+#else
+static __inline__ void _raw_read_lock(rwlock_t *rw)
+{
+	unsigned long tmp;
+
+	__asm__ __volatile__(
+	"b		2f		# read_lock\n\
+1:"
+	HMT_LOW
+"	ldx		%0,0,%1\n\
+	cmpdi		0,%0,0\n\
+	blt+		1b\n\
+2: \n"
+	HMT_MEDIUM
+" 	ldarx		%0,0,%1\n\
+	addic.		%0,%0,1\n\
+	ble-		1b\n\
+	stdcx.		%0,0,%1\n\
 	bne-		2b\n\
 	isync"
 	: "=&r"(tmp)
 	: "r"(&rw->lock)
 	: "cr0", "memory");
 }
+#endif
+#endif
 
 static __inline__ void _raw_read_unlock(rwlock_t *rw)
 {
-	unsigned int tmp;
+	unsigned long tmp;
 
 	__asm__ __volatile__(
 	"eieio				# read_unlock\n\
-1:	lwarx		%0,0,%1\n\
+1:	ldarx		%0,0,%1\n\
 	addic		%0,%0,-1\n\
-	stwcx.		%0,0,%1\n\
+	stdcx.		%0,0,%1\n\
 	bne-		1b"
 	: "=&r"(tmp)
 	: "r"(&rw->lock)
@@ -151,15 +339,15 @@ static __inline__ void _raw_read_unlock(
 
 static __inline__ int _raw_write_trylock(rwlock_t *rw)
 {
-	unsigned int tmp;
-	unsigned int ret;
+	unsigned long tmp;
+	unsigned long ret;
 
 	__asm__ __volatile__(
-"1:	lwarx		%0,0,%2		# write_trylock\n\
-	cmpwi		0,%0,0\n\
+"1:	ldarx		%0,0,%2		# write_trylock\n\
+	cmpdi		0,%0,0\n\
 	li		%1,0\n\
 	bne-		2f\n\
-	stwcx.		%3,0,%2\n\
+	stdcx.		13,0,%2\n\
 	bne-		1b\n\
 	li		%1,1\n\
 	isync\n\
@@ -170,28 +358,112 @@ static __inline__ int _raw_write_trylock
 	return ret;
 }
 
+#ifdef CONFIG_PPC_ISERIES
+static __inline__ void _raw_write_lock(rwlock_t *rw)
+{
+	unsigned long tmp, tmp2;
+
+	__asm__ __volatile__(
+	"b		2f		# spin_lock\n\
+1:"
+	HMT_LOW
+"       ldx		%0,0,%2         # load the lock value\n\
+	cmpdi		0,%0,0          # if not locked(0), try to acquire\n\
+	beq-		2f\n\
+        bgt             1b              # negative(0xc..)->cycles to holder\n"
+"3:     lwz             5,0x280(%0)     # load yield counter\n\
+        andi.           %1,5,1          # if even then spin\n\
+        beq             1b\n\
+        lwsync                          # if odd, give up cycles\n\
+        ldx             %1,0,%2         # reverify the lock holder\n\
+        cmpd            %0,%1\n\
+        bne             1b              # new holder so restart\n\
+        lhz             4,0x18(%0)      # processor number\n\
+        sldi            4,4,32          #   move into top half of word\n\
+        or              5,5,4           # r5 has yield cnt - or it in\n\
+        li              3,0x25          # yield hcall 0x8-12 \n\
+        rotrdi          3,3,1           #   put the bits in the right spot\n\
+        li              4,2             # yield to processor\n\
+        li              0,-1            # indicate an hcall\n\
+        sc                              # do the hcall \n\
+2: \n"
+	HMT_MEDIUM
+" 	ldarx		%0,0,%2\n\
+	cmpdi		0,%0,0\n\
+	bne-		1b\n\
+	stdcx.		13,0,%2\n\
+	bne-		2b\n\
+	isync"
+	: "=&b"(tmp), "=&r"(tmp2)
+	: "r"(&rw->lock)
+	: "r0", "r3", "r4", "r5", "ctr", "cr0", "cr1", "cr2", "cr3", "cr4", 
+	  "xer", "memory");
+}
+#else
+#ifdef SPLPAR_LOCKS
+static __inline__ void _raw_write_lock(rwlock_t *rw)
+{
+	unsigned long tmp, tmp2;
+
+	__asm__ __volatile__(
+	"b		2f		# spin_lock\n\
+1:"
+	HMT_LOW
+"       ldx		%0,0,%2         # load the lock value\n\
+        li              3,0xE4          # give up the cycles H_CONFER\n\
+	cmpdi		0,%0,0          # if not locked(0), try to acquire\n\
+	beq-		2f\n\
+        blt             3f              # negative(0xc..)->confer to holder\n\
+        b               1b\n"
+"3:      lwz             5,0x280(%0)     # load dispatch counter\n\
+        andi.           %1,5,1          # if even then spin\n\
+        beq             1b\n\
+        lwsync                          # if odd, give up cycles\n\
+        ldx             %1,0,%2         # reverify the lock holder\n\
+        cmpd            %0,%1\n\
+        bne             1b              # new holder so restart\n\
+        lhz             4,0x18(%0)      # processor number\n\
+                                        # r5 has dispatch cnt already\n"
+	HVSC
+"        b               1b\n\
+2: \n"
+	HMT_MEDIUM
+" 	ldarx		%0,0,%2\n\
+	cmpdi		0,%0,0\n\
+	bne-		1b\n\
+	stdcx.		13,0,%2\n\
+	bne-		2b\n\
+	isync"
+	: "=&b"(tmp), "=&r"(tmp2)
+	: "r"(&rw->lock)
+	: "r3", "r4", "r5", "cr0", "cr1", "ctr", "xer", "memory");
+}
+#else
 static __inline__ void _raw_write_lock(rwlock_t *rw)
 {
-	unsigned int tmp;
+	unsigned long tmp;
 
 	__asm__ __volatile__(
-	"b		2f		# write_lock\n\
+	"b		2f		# spin_lock\n\
 1:"
 	HMT_LOW
-	"lwax		%0,0,%1\n\
-	cmpwi		0,%0,0\n\
-	bne+		1b\n"
+"       ldx		%0,0,%1         # load the lock value\n\
+	cmpdi		0,%0,0          # if not locked(0), try to acquire\n\
+	bne+		1b\n\
+2: \n"
 	HMT_MEDIUM
-"2:	lwarx		%0,0,%1\n\
-	cmpwi		0,%0,0\n\
+" 	ldarx		%0,0,%1\n\
+	cmpdi		0,%0,0\n\
 	bne-		1b\n\
-	stwcx.		%2,0,%1\n\
+	stdcx.		13,0,%1\n\
 	bne-		2b\n\
 	isync"
 	: "=&r"(tmp)
-	: "r"(&rw->lock), "r"(-1)
+	: "r"(&rw->lock)
 	: "cr0", "memory");
 }
+#endif
+#endif
 
 static __inline__ void _raw_write_unlock(rwlock_t *rw)
 {
@@ -214,7 +486,7 @@ static __inline__ int is_write_locked(rw
 
 #define rwlock_init(x)         do { *(x) = RW_LOCK_UNLOCKED; } while(0)
 
-#define rwlock_is_locked(x)	((x)->lock)
+#define rwlock_is_locked(x) ((x)->lock != 0)
 
 #endif /* __KERNEL__ */
 #endif /* __ASM_SPINLOCK_H */
diff -purN linux-2.5/include/asm-ppc64/vio.h linuxppc64-2.5/include/asm-ppc64/vio.h
--- linux-2.5/include/asm-ppc64/vio.h	2004-03-01 16:04:24.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/vio.h	2004-03-01 23:51:14.000000000 +0000
@@ -17,6 +17,7 @@
 #include <linux/init.h>
 #include <linux/errno.h>
 #include <linux/device.h>
+#include <linux/pci.h>
 #include <asm/hvcall.h>
 #include <asm/prom.h>
 #include <asm/scatterlist.h>
@@ -44,8 +45,11 @@ int vio_register_driver(struct vio_drive
 void vio_unregister_driver(struct vio_driver *drv);
 const struct vio_device_id * vio_match_device(const struct vio_device_id *ids, 
 						const struct vio_dev *dev);
+
 struct vio_dev * __devinit vio_register_device(struct device_node *node_vdev);
 void __devinit vio_unregister_device(struct vio_dev *dev);
+struct vio_dev *vio_find_node(struct device_node *vnode);
+
 const void * vio_get_attribute(struct vio_dev *vdev, void* which, int* length);
 int vio_get_irq(struct vio_dev *dev);
 struct iommu_table * vio_build_iommu_table(struct vio_dev *dev);
@@ -65,6 +69,33 @@ void *vio_alloc_consistent(struct vio_de
 void vio_free_consistent(struct vio_dev *dev, size_t size, void *vaddr, 
 			 dma_addr_t dma_handle);
 
+static inline int vio_dma_supported(struct vio_dev *hwdev, u64 mask)
+{
+	return 1;
+}
+
+#define vio_map_page(dev, page, off, size, dir) \
+		vio_map_single(dev, (page_address(page) + (off)), size, dir)
+#define vio_unmap_page(dev,addr,sz,dir) vio_unmap_single(dev,addr,sz,dir)
+
+
+static inline void vio_dma_sync_single(struct vio_dev *hwdev,
+				       dma_addr_t dma_handle,
+				       size_t size, int direction)
+{
+	BUG_ON(direction == PCI_DMA_NONE);
+	/* nothing to do */
+}
+
+static inline void vio_dma_sync_sg(struct vio_dev *hwdev,
+				   struct scatterlist *sg,
+				   int nelems, int direction)
+{
+	BUG_ON(direction == PCI_DMA_NONE);
+	/* nothing to do */
+}
+static inline int vio_set_dma_mask(struct vio_dev *dev, u64 mask) { return -EIO; }
+
 extern struct bus_type vio_bus_type;
 
 struct vio_device_id {
diff -purN linux-2.5/include/linux/pci_ids.h linuxppc64-2.5/include/linux/pci_ids.h
--- linux-2.5/include/linux/pci_ids.h	2004-02-27 05:40:16.000000000 +0000
+++ linuxppc64-2.5/include/linux/pci_ids.h	2004-03-01 19:33:23.000000000 +0000
@@ -433,7 +433,9 @@
 #define PCI_DEVICE_ID_IBM_CPC710_PCI64	0x00fc
 #define PCI_DEVICE_ID_IBM_CPC710_PCI32	0x0105
 #define	PCI_DEVICE_ID_IBM_405GP		0x0156
+#define PCI_DEVICE_ID_IBM_SNIPE		0x0180
 #define PCI_DEVICE_ID_IBM_SERVERAIDI960	0x01bd
+#define PCI_DEVICE_ID_IBM_GEMSTONE		0xB166
 #define PCI_DEVICE_ID_IBM_MPIC_2	0xffff
 
 #define PCI_VENDOR_ID_COMPEX2		0x101a // pci.ids says "AT&T GIS (NCR)"
diff -purN linux-2.5/include/linux/preempt.h linuxppc64-2.5/include/linux/preempt.h
--- linux-2.5/include/linux/preempt.h	2004-02-25 10:42:02.000000000 +0000
+++ linuxppc64-2.5/include/linux/preempt.h	2004-02-26 05:42:10.000000000 +0000
@@ -25,6 +25,17 @@ do { \
 
 asmlinkage void preempt_schedule(void);
 
+#define preempt_check_resched() \
+do { \
+	if (unlikely(test_thread_flag(TIF_NEED_RESCHED))) \
+		preempt_schedule(); \
+} while (0)
+#else
+#define preempt_check_resched()		do { } while (0)
+#endif
+
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_DEBUG_SPINLOCK_SLEEP)
+
 #define preempt_disable() \
 do { \
 	inc_preempt_count(); \
@@ -37,12 +48,6 @@ do { \
 	dec_preempt_count(); \
 } while (0)
 
-#define preempt_check_resched() \
-do { \
-	if (unlikely(test_thread_flag(TIF_NEED_RESCHED))) \
-		preempt_schedule(); \
-} while (0)
-
 #define preempt_enable() \
 do { \
 	preempt_enable_no_resched(); \
diff -purN linux-2.5/include/scsi/scsi.h linuxppc64-2.5/include/scsi/scsi.h
--- linux-2.5/include/scsi/scsi.h	2004-02-11 07:49:41.000000000 +0000
+++ linuxppc64-2.5/include/scsi/scsi.h	2004-03-01 19:33:23.000000000 +0000
@@ -200,6 +200,7 @@ static inline int scsi_status_is_good(in
 #define TYPE_MEDIUM_CHANGER 0x08
 #define TYPE_COMM           0x09    /* Communications device */
 #define TYPE_ENCLOSURE      0x0d    /* Enclosure Services Device */
+#define TYPE_RAID           0x0c
 #define TYPE_NO_LUN         0x7f
 
 /*
diff -purN linux-2.5/include/scsi/scsi_device.h linuxppc64-2.5/include/scsi/scsi_device.h
--- linux-2.5/include/scsi/scsi_device.h	2003-11-23 00:20:46.000000000 +0000
+++ linuxppc64-2.5/include/scsi/scsi_device.h	2004-03-01 19:33:23.000000000 +0000
@@ -94,6 +94,7 @@ struct scsi_device {
 	unsigned skip_ms_page_8:1;	/* do not use MODE SENSE page 0x08 */
 	unsigned skip_ms_page_3f:1;	/* do not use MODE SENSE page 0x3f */
 	unsigned no_start_on_add:1;	/* do not issue start on add */
+	unsigned allow_restart:1; /* issue START_UNIT in error handler */
 
 	unsigned int device_blocked;	/* Device returned QUEUE_FULL. */
 
diff -purN linux-2.5/kernel/sched.c linuxppc64-2.5/kernel/sched.c
--- linux-2.5/kernel/sched.c	2004-02-19 21:08:09.000000000 +0000
+++ linuxppc64-2.5/kernel/sched.c	2004-02-20 08:56:03.000000000 +0000
@@ -729,7 +729,7 @@ void sched_fork(task_t *p)
 	INIT_LIST_HEAD(&p->run_list);
 	p->array = NULL;
 	spin_lock_init(&p->switch_lock);
-#ifdef CONFIG_PREEMPT
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_DEBUG_SPINLOCK_SLEEP)
 	/*
 	 * During context-switch we hold precisely one spinlock, which
 	 * schedule_tail drops. (in the common case it's this_rq()->lock,
@@ -2666,7 +2666,7 @@ void __init init_idle(task_t *idle, int 
 	local_irq_restore(flags);
 
 	/* Set the preempt count _outside_ the spinlocks! */
-#ifdef CONFIG_PREEMPT
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_DEBUG_SPINLOCK_SLEEP)
 	idle->thread_info->preempt_count = (idle->lock_depth >= 0);
 #else
 	idle->thread_info->preempt_count = 0;
