diff -purN linux-2.5/arch/ppc/kernel/misc.S linuxppc64-2.5/arch/ppc/kernel/misc.S
--- linux-2.5/arch/ppc/kernel/misc.S	2003-09-12 16:26:52.000000000 +0000
+++ linuxppc64-2.5/arch/ppc/kernel/misc.S	2003-11-17 18:51:47.000000000 +0000
@@ -1385,3 +1385,4 @@ _GLOBAL(sys_call_table)
 	.long sys_statfs64
 	.long sys_fstatfs64
 	.long ppc_fadvise64_64
+	.long sys_ni_syscall	/* 255 - rtas (used on ppc64) */
diff -purN linux-2.5/arch/ppc64/Kconfig linuxppc64-2.5/arch/ppc64/Kconfig
--- linux-2.5/arch/ppc64/Kconfig	2003-09-26 04:04:08.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/Kconfig	2003-12-17 04:27:52.000000000 +0000
@@ -72,6 +72,21 @@ config PPC64
 	bool
 	default y
 
+# VMX is pSeries only for now until somebody writes the iSeries
+# exception vectors for it
+config ALTIVEC
+	bool "Support for VMX (Altivec) vector unit"
+	depends on PPC_PSERIES
+	default y
+
+config POWER4_ONLY
+	bool "Optimize for POWER4"
+	default n
+	---help---
+	  Cause the compiler to optimize for POWER4 processors. The resulting
+	  binary will not work on POWER3 or RS64 processors when compiled with
+	  binutils 2.15 or later.
+
 config SMP
 	bool "Symmetric multi-processing support"
 	---help---
@@ -130,17 +145,24 @@ config MSCHUNKS
 	depends on PPC_ISERIES
 	default y
 
+
+config PPC_RTAS
+	bool "Proc interface to RTAS"
+	depends on !PPC_ISERIES
+
 config RTAS_FLASH
 	tristate "Firmware flash interface"
-	depends on !PPC_ISERIES
+	depends on PPC_RTAS
 
 config SCANLOG
 	tristate "Scanlog dump interface"
-	depends on !PPC_ISERIES
+	depends on PPC_RTAS
 
-config PPC_RTAS
-	bool "Proc interface to RTAS"
-	depends on !PPC_ISERIES
+config LPARCFG
+	bool "LPAR Configuration Data"
+	help
+	Provide system capacity information via human readable 
+	<key word>=<value> pairs through a /proc/ppc64/lparcfg interface.
 
 endmenu
 
@@ -318,7 +340,7 @@ endmenu
 
 config VIOPATH
 	bool
-	depends on PPC_ISERIES
+	depends on VIOCONS || VIODASD || VIOCD || VIOTAPE || VETH
 	default y
 
 source "arch/ppc64/oprofile/Kconfig"
@@ -353,17 +375,36 @@ config MAGIC_SYSRQ
 	  keys are documented in <file:Documentation/sysrq.txt>. Don't say Y
 	  unless you really know what this hack does.
 
+choice
+	prompt "Kernel Debugger"
+
 config XMON
-	bool "Include xmon kernel debugger"
-	depends on DEBUG_KERNEL
+	bool "XMON"
 	help
 	  Include in-kernel hooks for the xmon kernel monitor/debugger.
 	  Unless you are intending to debug the kernel, say N here.
 
+config KDB
+	bool "KDB"
+	help
+	  Include in-kernel hooks for the kdb kernel monitor/debugger.
+	  Unless you are intending to debug the kernel, say N here.
+
+endchoice
+
+
 config XMON_DEFAULT
 	bool "Enable xmon by default"
 	depends on XMON
 
+config KDB_OFF
+	bool "Turn KDB off as default."
+	depends on KDB
+
+	help
+ 	   KDB will remain built into the kernel, but will be turned off. 
+	   "cat 1 > /proc/sys/kernel/kdb" to turn it on. 
+
 config PPCDBG
 	bool "Include PPCDBG realtime debugging"
 	depends on DEBUG_KERNEL
diff -purN linux-2.5/arch/ppc64/Makefile linuxppc64-2.5/arch/ppc64/Makefile
--- linux-2.5/arch/ppc64/Makefile	2003-09-10 18:34:20.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/Makefile	2003-12-08 16:23:33.000000000 +0000
@@ -15,10 +15,26 @@
 
 KERNELLOAD	:= 0xc000000000000000
 
+ifeq ($(shell uname -m),ppc64)
+CHECKS		= checks
+endif
+
+HAS_BIARCH      := $(shell if $(CC) -m64 -S -o /dev/null -xc /dev/null > /dev/null 2>&1; then echo y; else echo n; fi;)
+ifeq ($(HAS_BIARCH),y)
+AS              := $(AS) -64
+LD              := $(LD) -m elf64ppc
+CC		:= $(CC) -m64
+endif
+
 LDFLAGS		:= -m elf64ppc
 LDFLAGS_vmlinux	:= -Bstatic -e $(KERNELLOAD) -Ttext $(KERNELLOAD)
-CFLAGS		+= -msoft-float -pipe -Wno-uninitialized -mminimal-toc \
-		-mcpu=power4
+CFLAGS		+= -msoft-float -pipe -Wno-uninitialized -mminimal-toc
+
+ifeq ($(CONFIG_POWER4_ONLY),y)
+CFLAGS		+= -mcpu=power4
+else
+CFLAGS		+= -mtune=power4
+endif
 
 have_zero_bss := $(shell if $(CC) -fno-zero-initialized-in-bss -S -o /dev/null -xc /dev/null > /dev/null 2>&1; then echo y; else echo n; fi)
 
@@ -32,6 +48,11 @@ libs-y				+= arch/ppc64/lib/
 core-y				+= arch/ppc64/kernel/
 core-y				+= arch/ppc64/mm/
 core-$(CONFIG_XMON)		+= arch/ppc64/xmon/
+ifeq ($(CONFIG_KDB),y)
+  # Use ifeq for now because kdb subdirs are not in bk yet
+  # Otherwise make mrproper will die because it also cleans core-n
+  core-y			+= arch/ppc64/kdb/
+endif
 drivers-$(CONFIG_OPROFILE)	+= arch/ppc64/oprofile/
 
 boot := arch/ppc64/boot
@@ -47,6 +68,8 @@ BOOTIMAGE := $(bootimage-y)
 install: vmlinux
 	$(Q)$(MAKE) $(build)=$(boot) BOOTIMAGE=$(BOOTIMAGE) $@
 
+all: $(BOOTIMAGE)
+
 archclean:
 	$(Q)$(MAKE) $(clean)=$(boot)
 
diff -purN linux-2.5/arch/ppc64/boot/ppc32-types.h linuxppc64-2.5/arch/ppc64/boot/ppc32-types.h
--- linux-2.5/arch/ppc64/boot/ppc32-types.h	2003-02-07 21:36:11.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/boot/ppc32-types.h	2003-12-17 04:27:52.000000000 +0000
@@ -25,6 +25,12 @@ typedef unsigned int u32;
 typedef signed long long s64;
 typedef unsigned long long u64;
 
+typedef struct {
+	__u32 u[4];
+} __attribute((aligned(16))) __vector128;
+
 #define BITS_PER_LONG 32
 
+typedef __vector128 vector128;
+
 #endif /* _PPC64_TYPES_H */
diff -purN linux-2.5/arch/ppc64/defconfig linuxppc64-2.5/arch/ppc64/defconfig
--- linux-2.5/arch/ppc64/defconfig	2003-09-02 08:05:21.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/defconfig	2003-09-18 16:04:44.000000000 +0000
@@ -23,7 +23,7 @@ CONFIG_SWAP=y
 CONFIG_SYSVIPC=y
 # CONFIG_BSD_PROCESS_ACCT is not set
 CONFIG_SYSCTL=y
-CONFIG_LOG_BUF_SHIFT=16
+CONFIG_LOG_BUF_SHIFT=17
 CONFIG_IKCONFIG=y
 CONFIG_IKCONFIG_PROC=y
 # CONFIG_EMBEDDED is not set
@@ -158,6 +158,7 @@ CONFIG_SCSI_CONSTANTS=y
 # CONFIG_SCSI_INITIO is not set
 # CONFIG_SCSI_INIA100 is not set
 CONFIG_SCSI_SYM53C8XX_2=y
+# CONFIG_SCSI_IBMSIS is not set
 CONFIG_SCSI_SYM53C8XX_DMA_ADDRESSING_MODE=0
 CONFIG_SCSI_SYM53C8XX_DEFAULT_TAGS=16
 CONFIG_SCSI_SYM53C8XX_MAX_TAGS=64
@@ -227,11 +228,10 @@ CONFIG_INET_AH=m
 CONFIG_INET_ESP=m
 CONFIG_INET_IPCOMP=m
 # CONFIG_IPV6 is not set
-# CONFIG_DECNET is not set
-# CONFIG_BRIDGE is not set
-# CONFIG_NETFILTER is not set
-CONFIG_XFRM=y
-CONFIG_XFRM_USER=m
+# CONFIG_XFRM_USER is not set
+
+
+
 
 #
 # SCTP Configuration (EXPERIMENTAL)
@@ -240,7 +240,7 @@ CONFIG_IPV6_SCTP__=y
 # CONFIG_IP_SCTP is not set
 # CONFIG_ATM is not set
 # CONFIG_VLAN_8021Q is not set
-# CONFIG_LLC is not set
+CONFIG_LLC=y
 # CONFIG_X25 is not set
 # CONFIG_LAPB is not set
 # CONFIG_NET_DIVERT is not set
@@ -346,6 +346,11 @@ CONFIG_PPPOE=m
 #
 # Token Ring devices (depends on LLC=y)
 #
+CONFIG_TR=y
+CONFIG_IBMOL=y
+# CONFIG_IBMLS is not set
+# CONFIG_3C359 is not set
+# CONFIG_TMS380TR is not set
 # CONFIG_NET_FC is not set
 # CONFIG_RCPCI is not set
 # CONFIG_SHAPER is not set
@@ -726,6 +731,8 @@ CONFIG_DEBUG_KERNEL=y
 CONFIG_MAGIC_SYSRQ=y
 CONFIG_XMON=y
 CONFIG_XMON_DEFAULT=y
+# CONFIG_KDB is not set
+# CONFIG_KDB_OFF is not set
 # CONFIG_PPCDBG is not set
 # CONFIG_DEBUG_INFO is not set
 
diff -purN linux-2.5/arch/ppc64/kdb/Makefile linuxppc64-2.5/arch/ppc64/kdb/Makefile
--- linux-2.5/arch/ppc64/kdb/Makefile	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/Makefile	2003-10-13 16:16:56.000000000 +0000
@@ -0,0 +1,7 @@
+obj-y		:= kdba_bt.o kdba_bp.o kdba_id.o kdba_io.o ppc-dis.o ppc-opc.o kdbasupport.o 
+
+# Warning: running with a minimal-toc means that kdb_setjmp will break
+# due to saving the wrong r30. A solution would be to move it into setjmp.S
+EXTRA_CFLAGS = -mno-minimal-toc
+
+override CFLAGS := $(CFLAGS) -I. -Iarch/ppc64/kdb
diff -purN linux-2.5/arch/ppc64/kdb/ansidecl.h linuxppc64-2.5/arch/ppc64/kdb/ansidecl.h
--- linux-2.5/arch/ppc64/kdb/ansidecl.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/ansidecl.h	2003-10-13 16:16:57.000000000 +0000
@@ -0,0 +1,198 @@
+/* ANSI and traditional C compatability macros
+   Copyright 1991, 1992, 1996, 1999 Free Software Foundation, Inc.
+   This file is part of the GNU C Library.
+
+This program is free software; you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation; either version 2 of the License, or
+(at your option) any later version.
+
+This program is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; if not, write to the Free Software
+Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  */
+
+/* ANSI and traditional C compatibility macros
+
+   ANSI C is assumed if __STDC__ is #defined.
+
+   Macro	ANSI C definition	Traditional C definition
+   -----	---- - ----------	----------- - ----------
+   PTR		`void *'		`char *'
+   LONG_DOUBLE	`long double'		`double'
+   VOLATILE	`volatile'		`'
+   SIGNED	`signed'		`'
+   PTRCONST	`void *const'		`char *'
+   ANSI_PROTOTYPES  1			not defined
+
+   CONST is also defined, but is obsolete.  Just use const.
+
+   obsolete --     DEFUN (name, arglist, args)
+
+	Defines function NAME.
+
+	ARGLIST lists the arguments, separated by commas and enclosed in
+	parentheses.  ARGLIST becomes the argument list in traditional C.
+
+	ARGS list the arguments with their types.  It becomes a prototype in
+	ANSI C, and the type declarations in traditional C.  Arguments should
+	be separated with `AND'.  For functions with a variable number of
+	arguments, the last thing listed should be `DOTS'.
+
+   obsolete --     DEFUN_VOID (name)
+
+	Defines a function NAME, which takes no arguments.
+
+   obsolete --     EXFUN (name, (prototype))	-- obsolete.
+
+	Replaced by PARAMS.  Do not use; will disappear someday soon.
+	Was used in external function declarations.
+	In ANSI C it is `NAME PROTOTYPE' (so PROTOTYPE should be enclosed in
+	parentheses).  In traditional C it is `NAME()'.
+	For a function that takes no arguments, PROTOTYPE should be `(void)'.
+
+   obsolete --     PROTO (type, name, (prototype)    -- obsolete.
+
+	This one has also been replaced by PARAMS.  Do not use.
+
+   PARAMS ((args))
+
+	We could use the EXFUN macro to handle prototype declarations, but
+	the name is misleading and the result is ugly.  So we just define a
+	simple macro to handle the parameter lists, as in:
+
+	      static int foo PARAMS ((int, char));
+
+	This produces:  `static int foo();' or `static int foo (int, char);'
+
+	EXFUN would have done it like this:
+
+	      static int EXFUN (foo, (int, char));
+
+	but the function is not external...and it's hard to visually parse
+	the function name out of the mess.   EXFUN should be considered
+	obsolete; new code should be written to use PARAMS.
+
+   DOTS is also obsolete.
+
+   Examples:
+
+	extern int printf PARAMS ((const char *format, ...));
+*/
+
+#ifndef	_ANSIDECL_H
+
+#define	_ANSIDECL_H	1
+
+
+/* Every source file includes this file,
+   so they will all get the switch for lint.  */
+/* LINTLIBRARY */
+
+
+#if defined (__STDC__) || defined (_AIX) || (defined (__mips) && defined (_SYSTYPE_SVR4)) || defined(_WIN32)
+/* All known AIX compilers implement these things (but don't always
+   define __STDC__).  The RISC/OS MIPS compiler defines these things
+   in SVR4 mode, but does not define __STDC__.  */
+
+#define	PTR		void *
+#define	PTRCONST	void *CONST
+#define	LONG_DOUBLE	long double
+
+#ifndef IN_GCC
+#define	AND		,
+#define	NOARGS		void
+#define	VOLATILE	volatile
+#define	SIGNED		signed
+#endif /* ! IN_GCC */
+
+#ifndef PARAMS
+#define PARAMS(paramlist)		paramlist
+#endif
+#define ANSI_PROTOTYPES			1
+
+#define VPARAMS(ARGS)			ARGS
+#define VA_START(va_list,var)		va_start(va_list,var)
+
+/* These are obsolete.  Do not use.  */
+#ifndef IN_GCC
+#define CONST				const
+#define DOTS				, ...
+#define PROTO(type, name, arglist)	type name arglist
+#define EXFUN(name, proto)		name proto
+#define DEFUN(name, arglist, args)	name(args)
+#define DEFUN_VOID(name)		name(void)
+#endif /* ! IN_GCC */
+
+#else	/* Not ANSI C.  */
+
+#define	PTR		char *
+#define	PTRCONST	PTR
+#define	LONG_DOUBLE	double
+
+#ifndef IN_GCC
+#define	AND		;
+#define	NOARGS
+#define	VOLATILE
+#define	SIGNED
+#endif /* !IN_GCC */
+
+#ifndef const /* some systems define it in header files for non-ansi mode */
+#define	const
+#endif
+
+#define PARAMS(paramlist)		()
+
+#define VPARAMS(ARGS)			(va_alist) va_dcl
+#define VA_START(va_list,var)		va_start(va_list)
+
+/* These are obsolete.  Do not use.  */
+#ifndef IN_GCC
+#define CONST
+#define DOTS
+#define PROTO(type, name, arglist)	type name ()
+#define EXFUN(name, proto)		name()
+#define DEFUN(name, arglist, args)	name arglist args;
+#define DEFUN_VOID(name)		name()
+#endif /* ! IN_GCC */
+
+#endif	/* ANSI C.  */
+
+/* Define macros for some gcc attributes.  This permits us to use the
+   macros freely, and know that they will come into play for the
+   version of gcc in which they are supported.  */
+
+#if __GNUC__ < 2 || (__GNUC__ == 2 && __GNUC_MINOR__ < 7)
+# define __attribute__(x)
+#endif
+
+#ifndef ATTRIBUTE_UNUSED_LABEL
+# if __GNUC__ < 2 || (__GNUC__ == 2 && __GNUC_MINOR__ < 93)
+#  define ATTRIBUTE_UNUSED_LABEL
+# else
+#  define ATTRIBUTE_UNUSED_LABEL ATTRIBUTE_UNUSED
+# endif /* GNUC < 2.93 */
+#endif /* ATTRIBUTE_UNUSED_LABEL */
+
+#ifndef ATTRIBUTE_UNUSED
+#define ATTRIBUTE_UNUSED __attribute__ ((__unused__))
+#endif /* ATTRIBUTE_UNUSED */
+
+#ifndef ATTRIBUTE_NORETURN
+#define ATTRIBUTE_NORETURN __attribute__ ((__noreturn__))
+#endif /* ATTRIBUTE_NORETURN */
+
+#ifndef ATTRIBUTE_PRINTF
+#define ATTRIBUTE_PRINTF(m, n) __attribute__ ((format (__printf__, m, n)))
+#define ATTRIBUTE_PRINTF_1 ATTRIBUTE_PRINTF(1, 2)
+#define ATTRIBUTE_PRINTF_2 ATTRIBUTE_PRINTF(2, 3)
+#define ATTRIBUTE_PRINTF_3 ATTRIBUTE_PRINTF(3, 4)
+#define ATTRIBUTE_PRINTF_4 ATTRIBUTE_PRINTF(4, 5)
+#define ATTRIBUTE_PRINTF_5 ATTRIBUTE_PRINTF(5, 6)
+#endif /* ATTRIBUTE_PRINTF */
+
+#endif	/* ansidecl.h	*/
diff -purN linux-2.5/arch/ppc64/kdb/bfd.h linuxppc64-2.5/arch/ppc64/kdb/bfd.h
--- linux-2.5/arch/ppc64/kdb/bfd.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/bfd.h	2003-10-13 16:16:57.000000000 +0000
@@ -0,0 +1,3706 @@
+/* DO NOT EDIT!  -*- buffer-read-only: t -*-  This file is automatically 
+   generated from "bfd-in.h", "init.c", "opncls.c", "libbfd.c", 
+   "section.c", "archures.c", "reloc.c", "syms.c", "bfd.c", "archive.c", 
+   "corefile.c", "targets.c" and "format.c".
+   Run "make headers" in your build bfd/ to regenerate.  */
+
+/* Main header file for the bfd library -- portable access to object files.
+   Copyright 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999,
+   2000, 2001
+   Free Software Foundation, Inc.
+   Contributed by Cygnus Support.
+
+This file is part of BFD, the Binary File Descriptor library.
+
+This program is free software; you can redistribute it and/or modify
+it under the terms of the GNU General Public License as published by
+the Free Software Foundation; either version 2 of the License, or
+(at your option) any later version.
+
+This program is distributed in the hope that it will be useful,
+but WITHOUT ANY WARRANTY; without even the implied warranty of
+MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this program; if not, write to the Free Software
+Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  */
+
+#ifndef __BFD_H_SEEN__
+#define __BFD_H_SEEN__
+
+#ifdef __cplusplus
+extern "C" {
+#endif
+
+#include "ansidecl.h"
+#include "symcat.h"
+#if defined (__STDC__) || defined (ALMOST_STDC) || defined (HAVE_STRINGIZE)
+#ifndef SABER
+/* This hack is to avoid a problem with some strict ANSI C preprocessors.
+   The problem is, "32_" is not a valid preprocessing token, and we don't
+   want extra underscores (e.g., "nlm_32_").  The XCONCAT2 macro will
+   cause the inner CONCAT2 macros to be evaluated first, producing
+   still-valid pp-tokens.  Then the final concatenation can be done.  */
+#undef CONCAT4
+#define CONCAT4(a,b,c,d) XCONCAT2(CONCAT2(a,b),CONCAT2(c,d))
+#endif
+#endif
+
+#define BFD_VERSION 211920007
+#define BFD_VERSION_DATE 20011016
+#define BFD_VERSION_STRING "2.11.92.0.7 20011016 Debian\/GNU Linux"
+
+/* The word size used by BFD on the host.  This may be 64 with a 32
+   bit target if the host is 64 bit, or if other 64 bit targets have
+   been selected with --enable-targets, or if --enable-64-bit-bfd.  */
+#define BFD_ARCH_SIZE 64
+
+/* The word size of the default bfd target.  */
+#define BFD_DEFAULT_TARGET_SIZE 32
+
+#define BFD_HOST_64BIT_LONG 1
+#define BFD_HOST_64_BIT long
+#define BFD_HOST_U_64_BIT unsigned long
+
+#if BFD_ARCH_SIZE >= 64
+#define BFD64
+#endif
+
+#ifndef INLINE
+#if __GNUC__ >= 2
+#define INLINE __inline__
+#else
+#define INLINE
+#endif
+#endif
+
+/* forward declaration */
+typedef struct _bfd bfd;
+
+/* To squelch erroneous compiler warnings ("illegal pointer
+   combination") from the SVR3 compiler, we would like to typedef
+   boolean to int (it doesn't like functions which return boolean.
+   Making sure they are never implicitly declared to return int
+   doesn't seem to help).  But this file is not configured based on
+   the host.  */
+/* General rules: functions which are boolean return true on success
+   and false on failure (unless they're a predicate).   -- bfd.doc */
+/* I'm sure this is going to break something and someone is going to
+   force me to change it.  */
+/* typedef enum boolean {false, true} boolean; */
+/* Yup, SVR4 has a "typedef enum boolean" in <sys/types.h>  -fnf */
+/* It gets worse if the host also defines a true/false enum... -sts */
+/* And even worse if your compiler has built-in boolean types... -law */
+#if defined (__GNUG__) && (__GNUC__ > 2 || (__GNUC__ == 2 && __GNUC_MINOR__ >= 6))
+#define TRUE_FALSE_ALREADY_DEFINED
+#endif
+#ifdef MPW
+/* Pre-emptive strike - get the file with the enum.  */
+#include <Types.h>
+#define TRUE_FALSE_ALREADY_DEFINED
+#endif /* MPW */
+#ifndef TRUE_FALSE_ALREADY_DEFINED
+typedef enum bfd_boolean {false, true} boolean;
+#define BFD_TRUE_FALSE
+#else
+/* Use enum names that will appear nowhere else.  */
+typedef enum bfd_boolean {bfd_fffalse, bfd_tttrue} boolean;
+#endif
+
+/* Support for different sizes of target format ints and addresses.
+   If the type `long' is at least 64 bits, BFD_HOST_64BIT_LONG will be
+   set to 1 above.  Otherwise, if gcc is being used, this code will
+   use gcc's "long long" type.  Otherwise, BFD_HOST_64_BIT must be
+   defined above.  */
+
+#ifndef BFD_HOST_64_BIT
+# if BFD_HOST_64BIT_LONG
+#  define BFD_HOST_64_BIT long
+#  define BFD_HOST_U_64_BIT unsigned long
+# else
+#  ifdef __GNUC__
+#   if __GNUC__ >= 2
+#    define BFD_HOST_64_BIT long long
+#    define BFD_HOST_U_64_BIT unsigned long long
+#   endif /* __GNUC__ >= 2 */
+#  endif /* ! defined (__GNUC__) */
+# endif /* ! BFD_HOST_64BIT_LONG */
+#endif /* ! defined (BFD_HOST_64_BIT) */
+
+#ifdef BFD64
+
+#ifndef BFD_HOST_64_BIT
+ #error No 64 bit integer type available
+#endif /* ! defined (BFD_HOST_64_BIT) */
+
+typedef BFD_HOST_U_64_BIT bfd_vma;
+typedef BFD_HOST_64_BIT bfd_signed_vma;
+typedef BFD_HOST_U_64_BIT bfd_size_type;
+typedef BFD_HOST_U_64_BIT symvalue;
+
+#ifndef fprintf_vma
+#if BFD_HOST_64BIT_LONG
+#define sprintf_vma(s,x) sprintf (s, "%016lx", x)
+#define fprintf_vma(f,x) fprintf (f, "%016lx", x)
+#else
+#define _bfd_int64_low(x) ((unsigned long) (((x) & 0xffffffff)))
+#define _bfd_int64_high(x) ((unsigned long) (((x) >> 32) & 0xffffffff))
+#define fprintf_vma(s,x) \
+  fprintf ((s), "%08lx%08lx", _bfd_int64_high (x), _bfd_int64_low (x))
+#define sprintf_vma(s,x) \
+  sprintf ((s), "%08lx%08lx", _bfd_int64_high (x), _bfd_int64_low (x))
+#endif
+#endif
+
+#else /* not BFD64  */
+
+/* Represent a target address.  Also used as a generic unsigned type
+   which is guaranteed to be big enough to hold any arithmetic types
+   we need to deal with.  */
+typedef unsigned long bfd_vma;
+
+/* A generic signed type which is guaranteed to be big enough to hold any
+   arithmetic types we need to deal with.  Can be assumed to be compatible
+   with bfd_vma in the same way that signed and unsigned ints are compatible
+   (as parameters, in assignment, etc).  */
+typedef long bfd_signed_vma;
+
+typedef unsigned long symvalue;
+typedef unsigned long bfd_size_type;
+
+/* Print a bfd_vma x on stream s.  */
+#define fprintf_vma(s,x) fprintf (s, "%08lx", x)
+#define sprintf_vma(s,x) sprintf (s, "%08lx", x)
+
+#endif /* not BFD64  */
+
+/* A pointer to a position in a file.  */
+/* FIXME:  This should be using off_t from <sys/types.h>.
+   For now, try to avoid breaking stuff by not including <sys/types.h> here.
+   This will break on systems with 64-bit file offsets (e.g. 4.4BSD).
+   Probably the best long-term answer is to avoid using file_ptr AND off_t
+   in this header file, and to handle this in the BFD implementation
+   rather than in its interface.  */
+/* typedef off_t	file_ptr; */
+typedef bfd_signed_vma file_ptr;
+typedef bfd_vma ufile_ptr;
+
+extern void bfd_sprintf_vma PARAMS ((bfd *, char *, bfd_vma));
+extern void bfd_fprintf_vma PARAMS ((bfd *, PTR, bfd_vma));
+
+#define printf_vma(x) fprintf_vma(stdout,x)
+#define bfd_printf_vma(abfd,x) bfd_fprintf_vma (abfd,stdout,x)
+
+typedef unsigned int flagword;	/* 32 bits of flags */
+typedef unsigned char bfd_byte;
+
+/** File formats */
+
+typedef enum bfd_format {
+	      bfd_unknown = 0,	/* file format is unknown */
+	      bfd_object,	/* linker/assember/compiler output */
+	      bfd_archive,	/* object archive file */
+	      bfd_core,		/* core dump */
+	      bfd_type_end}	/* marks the end; don't use it! */
+         bfd_format;
+
+/* Values that may appear in the flags field of a BFD.  These also
+   appear in the object_flags field of the bfd_target structure, where
+   they indicate the set of flags used by that backend (not all flags
+   are meaningful for all object file formats) (FIXME: at the moment,
+   the object_flags values have mostly just been copied from backend
+   to another, and are not necessarily correct).  */
+
+/* No flags.  */
+#define BFD_NO_FLAGS   	0x00
+
+/* BFD contains relocation entries.  */
+#define HAS_RELOC   	0x01
+
+/* BFD is directly executable.  */
+#define EXEC_P      	0x02
+
+/* BFD has line number information (basically used for F_LNNO in a
+   COFF header).  */
+#define HAS_LINENO  	0x04
+
+/* BFD has debugging information.  */
+#define HAS_DEBUG   	0x08
+
+/* BFD has symbols.  */
+#define HAS_SYMS    	0x10
+
+/* BFD has local symbols (basically used for F_LSYMS in a COFF
+   header).  */
+#define HAS_LOCALS  	0x20
+
+/* BFD is a dynamic object.  */
+#define DYNAMIC     	0x40
+
+/* Text section is write protected (if D_PAGED is not set, this is
+   like an a.out NMAGIC file) (the linker sets this by default, but
+   clears it for -r or -N).  */
+#define WP_TEXT     	0x80
+
+/* BFD is dynamically paged (this is like an a.out ZMAGIC file) (the
+   linker sets this by default, but clears it for -r or -n or -N).  */
+#define D_PAGED     	0x100
+
+/* BFD is relaxable (this means that bfd_relax_section may be able to
+   do something) (sometimes bfd_relax_section can do something even if
+   this is not set).  */
+#define BFD_IS_RELAXABLE 0x200
+
+/* This may be set before writing out a BFD to request using a
+   traditional format.  For example, this is used to request that when
+   writing out an a.out object the symbols not be hashed to eliminate
+   duplicates.  */
+#define BFD_TRADITIONAL_FORMAT 0x400
+
+/* This flag indicates that the BFD contents are actually cached in
+   memory.  If this is set, iostream points to a bfd_in_memory struct.  */
+#define BFD_IN_MEMORY 0x800
+
+/* symbols and relocation */
+
+/* A count of carsyms (canonical archive symbols).  */
+typedef unsigned long symindex;
+
+/* How to perform a relocation.  */
+typedef const struct reloc_howto_struct reloc_howto_type;
+
+#define BFD_NO_MORE_SYMBOLS ((symindex) ~0)
+
+/* General purpose part of a symbol X;
+   target specific parts are in libcoff.h, libaout.h, etc.  */
+
+#define bfd_get_section(x) ((x)->section)
+#define bfd_get_output_section(x) ((x)->section->output_section)
+#define bfd_set_section(x,y) ((x)->section) = (y)
+#define bfd_asymbol_base(x) ((x)->section->vma)
+#define bfd_asymbol_value(x) (bfd_asymbol_base(x) + (x)->value)
+#define bfd_asymbol_name(x) ((x)->name)
+/*Perhaps future: #define bfd_asymbol_bfd(x) ((x)->section->owner)*/
+#define bfd_asymbol_bfd(x) ((x)->the_bfd)
+#define bfd_asymbol_flavour(x) (bfd_asymbol_bfd(x)->xvec->flavour)
+
+/* A canonical archive symbol.  */
+/* This is a type pun with struct ranlib on purpose! */
+typedef struct carsym {
+  char *name;
+  file_ptr file_offset;		/* look here to find the file */
+} carsym;			/* to make these you call a carsymogen */
+
+/* Used in generating armaps (archive tables of contents).
+   Perhaps just a forward definition would do? */
+struct orl {			/* output ranlib */
+  char **name;			/* symbol name */
+  union {
+    file_ptr pos;
+    bfd *abfd;
+  } u;				/* bfd* or file position */
+  int namidx;			/* index into string table */
+};
+
+/* Linenumber stuff */
+typedef struct lineno_cache_entry {
+  unsigned int line_number;	/* Linenumber from start of function*/
+  union {
+    struct symbol_cache_entry *sym; /* Function name */
+    bfd_vma offset;	    /* Offset into section */
+  } u;
+} alent;
+
+/* object and core file sections */
+
+#define	align_power(addr, align)	\
+	( ((addr) + ((1<<(align))-1)) & (-1 << (align)))
+
+typedef struct sec *sec_ptr;
+
+#define bfd_get_section_name(bfd, ptr) ((ptr)->name + 0)
+#define bfd_get_section_vma(bfd, ptr) ((ptr)->vma + 0)
+#define bfd_get_section_alignment(bfd, ptr) ((ptr)->alignment_power + 0)
+#define bfd_section_name(bfd, ptr) ((ptr)->name)
+#define bfd_section_size(bfd, ptr) (bfd_get_section_size_before_reloc(ptr))
+#define bfd_section_vma(bfd, ptr) ((ptr)->vma)
+#define bfd_section_lma(bfd, ptr) ((ptr)->lma)
+#define bfd_section_alignment(bfd, ptr) ((ptr)->alignment_power)
+#define bfd_get_section_flags(bfd, ptr) ((ptr)->flags + 0)
+#define bfd_get_section_userdata(bfd, ptr) ((ptr)->userdata)
+
+#define bfd_is_com_section(ptr) (((ptr)->flags & SEC_IS_COMMON) != 0)
+
+#define bfd_set_section_vma(bfd, ptr, val) (((ptr)->vma = (ptr)->lma= (val)), ((ptr)->user_set_vma = (boolean)true), true)
+#define bfd_set_section_alignment(bfd, ptr, val) (((ptr)->alignment_power = (val)),true)
+#define bfd_set_section_userdata(bfd, ptr, val) (((ptr)->userdata = (val)),true)
+
+typedef struct stat stat_type;
+
+typedef enum bfd_print_symbol
+{
+  bfd_print_symbol_name,
+  bfd_print_symbol_more,
+  bfd_print_symbol_all
+} bfd_print_symbol_type;
+
+/* Information about a symbol that nm needs.  */
+
+typedef struct _symbol_info
+{
+  symvalue value;
+  char type;
+  const char *name;            /* Symbol name.  */
+  unsigned char stab_type;     /* Stab type.  */
+  char stab_other;             /* Stab other.  */
+  short stab_desc;             /* Stab desc.  */
+  const char *stab_name;       /* String for stab type.  */
+} symbol_info;
+
+/* Get the name of a stabs type code.  */
+
+extern const char *bfd_get_stab_name PARAMS ((int));
+
+/* Hash table routines.  There is no way to free up a hash table.  */
+
+/* An element in the hash table.  Most uses will actually use a larger
+   structure, and an instance of this will be the first field.  */
+
+struct bfd_hash_entry
+{
+  /* Next entry for this hash code.  */
+  struct bfd_hash_entry *next;
+  /* String being hashed.  */
+  const char *string;
+  /* Hash code.  This is the full hash code, not the index into the
+     table.  */
+  unsigned long hash;
+};
+
+/* A hash table.  */
+
+struct bfd_hash_table
+{
+  /* The hash array.  */
+  struct bfd_hash_entry **table;
+  /* The number of slots in the hash table.  */
+  unsigned int size;
+  /* A function used to create new elements in the hash table.  The
+     first entry is itself a pointer to an element.  When this
+     function is first invoked, this pointer will be NULL.  However,
+     having the pointer permits a hierarchy of method functions to be
+     built each of which calls the function in the superclass.  Thus
+     each function should be written to allocate a new block of memory
+     only if the argument is NULL.  */
+  struct bfd_hash_entry *(*newfunc) PARAMS ((struct bfd_hash_entry *,
+					     struct bfd_hash_table *,
+					     const char *));
+   /* An objalloc for this hash table.  This is a struct objalloc *,
+     but we use PTR to avoid requiring the inclusion of objalloc.h.  */
+  PTR memory;
+};
+
+/* Initialize a hash table.  */
+extern boolean bfd_hash_table_init
+  PARAMS ((struct bfd_hash_table *,
+	   struct bfd_hash_entry *(*) (struct bfd_hash_entry *,
+				       struct bfd_hash_table *,
+				       const char *)));
+
+/* Initialize a hash table specifying a size.  */
+extern boolean bfd_hash_table_init_n
+  PARAMS ((struct bfd_hash_table *,
+	   struct bfd_hash_entry *(*) (struct bfd_hash_entry *,
+				       struct bfd_hash_table *,
+				       const char *),
+	   unsigned int size));
+
+/* Free up a hash table.  */
+extern void bfd_hash_table_free PARAMS ((struct bfd_hash_table *));
+
+/* Look up a string in a hash table.  If CREATE is true, a new entry
+   will be created for this string if one does not already exist.  The
+   COPY argument must be true if this routine should copy the string
+   into newly allocated memory when adding an entry.  */
+extern struct bfd_hash_entry *bfd_hash_lookup
+  PARAMS ((struct bfd_hash_table *, const char *, boolean create,
+	   boolean copy));
+
+/* Replace an entry in a hash table.  */
+extern void bfd_hash_replace
+  PARAMS ((struct bfd_hash_table *, struct bfd_hash_entry *old,
+	   struct bfd_hash_entry *nw));
+
+/* Base method for creating a hash table entry.  */
+extern struct bfd_hash_entry *bfd_hash_newfunc
+  PARAMS ((struct bfd_hash_entry *, struct bfd_hash_table *,
+	   const char *));
+
+/* Grab some space for a hash table entry.  */
+extern PTR bfd_hash_allocate PARAMS ((struct bfd_hash_table *,
+				      unsigned int));
+
+/* Traverse a hash table in a random order, calling a function on each
+   element.  If the function returns false, the traversal stops.  The
+   INFO argument is passed to the function.  */
+extern void bfd_hash_traverse PARAMS ((struct bfd_hash_table *,
+				       boolean (*) (struct bfd_hash_entry *,
+						    PTR),
+				       PTR info));
+
+#define COFF_SWAP_TABLE (PTR) &bfd_coff_std_swap_table
+
+/* User program access to BFD facilities */
+
+/* Direct I/O routines, for programs which know more about the object
+   file than BFD does.  Use higher level routines if possible.  */
+
+extern bfd_size_type bfd_bread PARAMS ((PTR, bfd_size_type, bfd *));
+extern bfd_size_type bfd_bwrite PARAMS ((const PTR, bfd_size_type, bfd *));
+extern int bfd_seek PARAMS ((bfd *, file_ptr, int));
+extern ufile_ptr bfd_tell PARAMS ((bfd *));
+extern int bfd_flush PARAMS ((bfd *));
+extern int bfd_stat PARAMS ((bfd *, struct stat *));
+
+/* Deprecated old routines.  */
+#if __GNUC__
+#define bfd_read(BUF, ELTSIZE, NITEMS, ABFD)				\
+  (warn_deprecated ("bfd_read", __FILE__, __LINE__, __FUNCTION__),	\
+   bfd_bread ((BUF), (ELTSIZE) * (NITEMS), (ABFD)))
+#define bfd_write(BUF, ELTSIZE, NITEMS, ABFD)				\
+  (warn_deprecated ("bfd_write", __FILE__, __LINE__, __FUNCTION__),	\
+   bfd_bwrite ((BUF), (ELTSIZE) * (NITEMS), (ABFD)))
+#else
+#define bfd_read(BUF, ELTSIZE, NITEMS, ABFD)				\
+  (warn_deprecated ("bfd_read", (const char *) 0, 0, (const char *) 0), \
+   bfd_bread ((BUF), (ELTSIZE) * (NITEMS), (ABFD)))
+#define bfd_write(BUF, ELTSIZE, NITEMS, ABFD)				\
+  (warn_deprecated ("bfd_write", (const char *) 0, 0, (const char *) 0),\
+   bfd_bwrite ((BUF), (ELTSIZE) * (NITEMS), (ABFD)))
+#endif
+extern void warn_deprecated
+  PARAMS ((const char *, const char *, int, const char *));
+
+/* Cast from const char * to char * so that caller can assign to
+   a char * without a warning.  */
+#define bfd_get_filename(abfd) ((char *) (abfd)->filename)
+#define bfd_get_cacheable(abfd) ((abfd)->cacheable)
+#define bfd_get_format(abfd) ((abfd)->format)
+#define bfd_get_target(abfd) ((abfd)->xvec->name)
+#define bfd_get_flavour(abfd) ((abfd)->xvec->flavour)
+#define bfd_family_coff(abfd) \
+  (bfd_get_flavour (abfd) == bfd_target_coff_flavour || \
+   bfd_get_flavour (abfd) == bfd_target_xcoff_flavour)
+#define bfd_big_endian(abfd) ((abfd)->xvec->byteorder == BFD_ENDIAN_BIG)
+#define bfd_little_endian(abfd) ((abfd)->xvec->byteorder == BFD_ENDIAN_LITTLE)
+#define bfd_header_big_endian(abfd) \
+  ((abfd)->xvec->header_byteorder == BFD_ENDIAN_BIG)
+#define bfd_header_little_endian(abfd) \
+  ((abfd)->xvec->header_byteorder == BFD_ENDIAN_LITTLE)
+#define bfd_get_file_flags(abfd) ((abfd)->flags)
+#define bfd_applicable_file_flags(abfd) ((abfd)->xvec->object_flags)
+#define bfd_applicable_section_flags(abfd) ((abfd)->xvec->section_flags)
+#define bfd_my_archive(abfd) ((abfd)->my_archive)
+#define bfd_has_map(abfd) ((abfd)->has_armap)
+
+#define bfd_valid_reloc_types(abfd) ((abfd)->xvec->valid_reloc_types)
+#define bfd_usrdata(abfd) ((abfd)->usrdata)
+
+#define bfd_get_start_address(abfd) ((abfd)->start_address)
+#define bfd_get_symcount(abfd) ((abfd)->symcount)
+#define bfd_get_outsymbols(abfd) ((abfd)->outsymbols)
+#define bfd_count_sections(abfd) ((abfd)->section_count)
+
+#define bfd_get_symbol_leading_char(abfd) ((abfd)->xvec->symbol_leading_char)
+
+#define bfd_set_cacheable(abfd,bool) (((abfd)->cacheable = (boolean) (bool)), true)
+
+extern boolean bfd_cache_close PARAMS ((bfd *abfd));
+/* NB: This declaration should match the autogenerated one in libbfd.h.  */
+
+extern boolean bfd_record_phdr
+  PARAMS ((bfd *, unsigned long, boolean, flagword, boolean, bfd_vma,
+	   boolean, boolean, unsigned int, struct sec **));
+
+/* Byte swapping routines.  */
+
+bfd_vma		bfd_getb64	   PARAMS ((const unsigned char *));
+bfd_vma 	bfd_getl64	   PARAMS ((const unsigned char *));
+bfd_signed_vma	bfd_getb_signed_64 PARAMS ((const unsigned char *));
+bfd_signed_vma	bfd_getl_signed_64 PARAMS ((const unsigned char *));
+bfd_vma		bfd_getb32	   PARAMS ((const unsigned char *));
+bfd_vma		bfd_getl32	   PARAMS ((const unsigned char *));
+bfd_signed_vma	bfd_getb_signed_32 PARAMS ((const unsigned char *));
+bfd_signed_vma	bfd_getl_signed_32 PARAMS ((const unsigned char *));
+bfd_vma		bfd_getb16	   PARAMS ((const unsigned char *));
+bfd_vma		bfd_getl16	   PARAMS ((const unsigned char *));
+bfd_signed_vma	bfd_getb_signed_16 PARAMS ((const unsigned char *));
+bfd_signed_vma	bfd_getl_signed_16 PARAMS ((const unsigned char *));
+void		bfd_putb64	   PARAMS ((bfd_vma, unsigned char *));
+void		bfd_putl64	   PARAMS ((bfd_vma, unsigned char *));
+void		bfd_putb32	   PARAMS ((bfd_vma, unsigned char *));
+void		bfd_putl32	   PARAMS ((bfd_vma, unsigned char *));
+void		bfd_putb16	   PARAMS ((bfd_vma, unsigned char *));
+void		bfd_putl16	   PARAMS ((bfd_vma, unsigned char *));
+
+/* Byte swapping routines which take size and endiannes as arguments.  */
+
+bfd_vma         bfd_get_bits       PARAMS ((bfd_byte *, int, boolean));
+void            bfd_put_bits       PARAMS ((bfd_vma, bfd_byte *, int, boolean));
+
+/* Externally visible ECOFF routines.  */
+
+#if defined(__STDC__) || defined(ALMOST_STDC)
+struct ecoff_debug_info;
+struct ecoff_debug_swap;
+struct ecoff_extr;
+struct symbol_cache_entry;
+struct bfd_link_info;
+struct bfd_link_hash_entry;
+struct bfd_elf_version_tree;
+#endif
+extern bfd_vma bfd_ecoff_get_gp_value PARAMS ((bfd * abfd));
+extern boolean bfd_ecoff_set_gp_value PARAMS ((bfd *abfd, bfd_vma gp_value));
+extern boolean bfd_ecoff_set_regmasks
+  PARAMS ((bfd *abfd, unsigned long gprmask, unsigned long fprmask,
+	   unsigned long *cprmask));
+extern PTR bfd_ecoff_debug_init
+  PARAMS ((bfd *output_bfd, struct ecoff_debug_info *output_debug,
+	   const struct ecoff_debug_swap *output_swap,
+	   struct bfd_link_info *));
+extern void bfd_ecoff_debug_free
+  PARAMS ((PTR handle, bfd *output_bfd, struct ecoff_debug_info *output_debug,
+	   const struct ecoff_debug_swap *output_swap,
+	   struct bfd_link_info *));
+extern boolean bfd_ecoff_debug_accumulate
+  PARAMS ((PTR handle, bfd *output_bfd, struct ecoff_debug_info *output_debug,
+	   const struct ecoff_debug_swap *output_swap,
+	   bfd *input_bfd, struct ecoff_debug_info *input_debug,
+	   const struct ecoff_debug_swap *input_swap,
+	   struct bfd_link_info *));
+extern boolean bfd_ecoff_debug_accumulate_other
+  PARAMS ((PTR handle, bfd *output_bfd, struct ecoff_debug_info *output_debug,
+	   const struct ecoff_debug_swap *output_swap, bfd *input_bfd,
+	   struct bfd_link_info *));
+extern boolean bfd_ecoff_debug_externals
+  PARAMS ((bfd *abfd, struct ecoff_debug_info *debug,
+	   const struct ecoff_debug_swap *swap,
+	   boolean relocateable,
+	   boolean (*get_extr) (struct symbol_cache_entry *,
+				struct ecoff_extr *),
+	   void (*set_index) (struct symbol_cache_entry *,
+			      bfd_size_type)));
+extern boolean bfd_ecoff_debug_one_external
+  PARAMS ((bfd *abfd, struct ecoff_debug_info *debug,
+	   const struct ecoff_debug_swap *swap,
+	   const char *name, struct ecoff_extr *esym));
+extern bfd_size_type bfd_ecoff_debug_size
+  PARAMS ((bfd *abfd, struct ecoff_debug_info *debug,
+	   const struct ecoff_debug_swap *swap));
+extern boolean bfd_ecoff_write_debug
+  PARAMS ((bfd *abfd, struct ecoff_debug_info *debug,
+	   const struct ecoff_debug_swap *swap, file_ptr where));
+extern boolean bfd_ecoff_write_accumulated_debug
+  PARAMS ((PTR handle, bfd *abfd, struct ecoff_debug_info *debug,
+	   const struct ecoff_debug_swap *swap,
+	   struct bfd_link_info *info, file_ptr where));
+extern boolean bfd_mips_ecoff_create_embedded_relocs
+  PARAMS ((bfd *, struct bfd_link_info *, struct sec *, struct sec *,
+	   char **));
+
+/* Externally visible ELF routines.  */
+
+struct bfd_link_needed_list
+{
+  struct bfd_link_needed_list *next;
+  bfd *by;
+  const char *name;
+};
+
+extern boolean bfd_elf32_record_link_assignment
+  PARAMS ((bfd *, struct bfd_link_info *, const char *, boolean));
+extern boolean bfd_elf64_record_link_assignment
+  PARAMS ((bfd *, struct bfd_link_info *, const char *, boolean));
+extern struct bfd_link_needed_list *bfd_elf_get_needed_list
+  PARAMS ((bfd *, struct bfd_link_info *));
+extern boolean bfd_elf_get_bfd_needed_list
+  PARAMS ((bfd *, struct bfd_link_needed_list **));
+extern boolean bfd_elf32_size_dynamic_sections
+  PARAMS ((bfd *, const char *, const char *, const char *,
+	   const char * const *, struct bfd_link_info *, struct sec **,
+	   struct bfd_elf_version_tree *));
+extern boolean bfd_elf64_size_dynamic_sections
+  PARAMS ((bfd *, const char *, const char *, const char *,
+	   const char * const *, struct bfd_link_info *, struct sec **,
+	   struct bfd_elf_version_tree *));
+extern void bfd_elf_set_dt_needed_name PARAMS ((bfd *, const char *));
+extern void bfd_elf_set_dt_needed_soname PARAMS ((bfd *, const char *));
+extern const char *bfd_elf_get_dt_soname PARAMS ((bfd *));
+extern struct bfd_link_needed_list *bfd_elf_get_runpath_list
+  PARAMS ((bfd *, struct bfd_link_info *));
+
+/* Return an upper bound on the number of bytes required to store a
+   copy of ABFD's program header table entries.  Return -1 if an error
+   occurs; bfd_get_error will return an appropriate code.  */
+extern long bfd_get_elf_phdr_upper_bound PARAMS ((bfd *abfd));
+
+/* Copy ABFD's program header table entries to *PHDRS.  The entries
+   will be stored as an array of Elf_Internal_Phdr structures, as
+   defined in include/elf/internal.h.  To find out how large the
+   buffer needs to be, call bfd_get_elf_phdr_upper_bound.
+
+   Return the number of program header table entries read, or -1 if an
+   error occurs; bfd_get_error will return an appropriate code.  */
+extern int bfd_get_elf_phdrs PARAMS ((bfd *abfd, void *phdrs));
+
+/* Return the arch_size field of an elf bfd, or -1 if not elf.  */
+extern int bfd_get_arch_size PARAMS ((bfd *));
+
+/* Return true if address "naturally" sign extends, or -1 if not elf.  */
+extern int bfd_get_sign_extend_vma PARAMS ((bfd *));
+
+extern boolean bfd_m68k_elf32_create_embedded_relocs
+  PARAMS ((bfd *, struct bfd_link_info *, struct sec *, struct sec *,
+	   char **));
+
+/* SunOS shared library support routines for the linker.  */
+
+extern struct bfd_link_needed_list *bfd_sunos_get_needed_list
+  PARAMS ((bfd *, struct bfd_link_info *));
+extern boolean bfd_sunos_record_link_assignment
+  PARAMS ((bfd *, struct bfd_link_info *, const char *));
+extern boolean bfd_sunos_size_dynamic_sections
+  PARAMS ((bfd *, struct bfd_link_info *, struct sec **, struct sec **,
+	   struct sec **));
+
+/* Linux shared library support routines for the linker.  */
+
+extern boolean bfd_i386linux_size_dynamic_sections
+  PARAMS ((bfd *, struct bfd_link_info *));
+extern boolean bfd_m68klinux_size_dynamic_sections
+  PARAMS ((bfd *, struct bfd_link_info *));
+extern boolean bfd_sparclinux_size_dynamic_sections
+  PARAMS ((bfd *, struct bfd_link_info *));
+
+/* mmap hacks */
+
+struct _bfd_window_internal;
+typedef struct _bfd_window_internal bfd_window_internal;
+
+typedef struct _bfd_window {
+  /* What the user asked for.  */
+  PTR data;
+  bfd_size_type size;
+  /* The actual window used by BFD.  Small user-requested read-only
+     regions sharing a page may share a single window into the object
+     file.  Read-write versions shouldn't until I've fixed things to
+     keep track of which portions have been claimed by the
+     application; don't want to give the same region back when the
+     application wants two writable copies!  */
+  struct _bfd_window_internal *i;
+} bfd_window;
+
+extern void bfd_init_window PARAMS ((bfd_window *));
+extern void bfd_free_window PARAMS ((bfd_window *));
+extern boolean bfd_get_file_window
+  PARAMS ((bfd *, file_ptr, bfd_size_type, bfd_window *, boolean));
+
+/* XCOFF support routines for the linker.  */
+
+extern boolean bfd_xcoff_link_record_set
+  PARAMS ((bfd *, struct bfd_link_info *, struct bfd_link_hash_entry *,
+	   bfd_size_type));
+extern boolean bfd_xcoff_import_symbol
+  PARAMS ((bfd *, struct bfd_link_info *, struct bfd_link_hash_entry *,
+	   bfd_vma, const char *, const char *, const char *, unsigned int));
+extern boolean bfd_xcoff_export_symbol
+  PARAMS ((bfd *, struct bfd_link_info *, struct bfd_link_hash_entry *));
+extern boolean bfd_xcoff_link_count_reloc
+  PARAMS ((bfd *, struct bfd_link_info *, const char *));
+extern boolean bfd_xcoff_record_link_assignment
+  PARAMS ((bfd *, struct bfd_link_info *, const char *));
+extern boolean bfd_xcoff_size_dynamic_sections
+  PARAMS ((bfd *, struct bfd_link_info *, const char *, const char *,
+	   unsigned long, unsigned long, unsigned long, boolean,
+	   int, boolean, boolean, struct sec **));
+
+/* Externally visible COFF routines.  */
+
+#if defined(__STDC__) || defined(ALMOST_STDC)
+struct internal_syment;
+union internal_auxent;
+#endif
+
+extern boolean bfd_coff_get_syment
+  PARAMS ((bfd *, struct symbol_cache_entry *, struct internal_syment *));
+
+extern boolean bfd_coff_get_auxent
+  PARAMS ((bfd *, struct symbol_cache_entry *, int, union internal_auxent *));
+
+extern boolean bfd_coff_set_symbol_class
+  PARAMS ((bfd *, struct symbol_cache_entry *, unsigned int));
+
+extern boolean bfd_m68k_coff_create_embedded_relocs
+  PARAMS ((bfd *, struct bfd_link_info *, struct sec *, struct sec *,
+	   char **));
+
+/* ARM Interworking support.  Called from linker.  */
+extern boolean bfd_arm_allocate_interworking_sections
+  PARAMS ((struct bfd_link_info *));
+
+extern boolean bfd_arm_process_before_allocation
+  PARAMS ((bfd *, struct bfd_link_info *, int));
+
+extern boolean bfd_arm_get_bfd_for_interworking
+  PARAMS ((bfd *, struct bfd_link_info *));
+
+/* PE ARM Interworking support.  Called from linker.  */
+extern boolean bfd_arm_pe_allocate_interworking_sections
+  PARAMS ((struct bfd_link_info *));
+
+extern boolean bfd_arm_pe_process_before_allocation
+  PARAMS ((bfd *, struct bfd_link_info *, int));
+
+extern boolean bfd_arm_pe_get_bfd_for_interworking
+  PARAMS ((bfd *, struct bfd_link_info *));
+
+/* ELF ARM Interworking support.  Called from linker.  */
+extern boolean bfd_elf32_arm_allocate_interworking_sections
+  PARAMS ((struct bfd_link_info *));
+
+extern boolean bfd_elf32_arm_process_before_allocation
+  PARAMS ((bfd *, struct bfd_link_info *, int));
+
+extern boolean bfd_elf32_arm_get_bfd_for_interworking
+  PARAMS ((bfd *, struct bfd_link_info *));
+
+/* TI COFF load page support.  */
+extern void bfd_ticoff_set_section_load_page
+  PARAMS ((struct sec *, int));
+
+extern int bfd_ticoff_get_section_load_page
+  PARAMS ((struct sec *));
+
+/* And more from the source.  */
+void
+bfd_init PARAMS ((void));
+
+bfd *
+bfd_openr PARAMS ((const char *filename, const char *target));
+
+bfd *
+bfd_fdopenr PARAMS ((const char *filename, const char *target, int fd));
+
+bfd *
+bfd_openstreamr PARAMS ((const char *, const char *, PTR));
+
+bfd *
+bfd_openw PARAMS ((const char *filename, const char *target));
+
+boolean
+bfd_close PARAMS ((bfd *abfd));
+
+boolean
+bfd_close_all_done PARAMS ((bfd *));
+
+bfd *
+bfd_create PARAMS ((const char *filename, bfd *templ));
+
+boolean
+bfd_make_writable PARAMS ((bfd *abfd));
+
+boolean
+bfd_make_readable PARAMS ((bfd *abfd));
+
+
+/* Byte swapping macros for user section data.  */
+
+#define bfd_put_8(abfd, val, ptr) \
+                ((void) (*((unsigned char *) (ptr)) = (unsigned char) (val)))
+#define bfd_put_signed_8 \
+               bfd_put_8
+#define bfd_get_8(abfd, ptr) \
+                (*(unsigned char *) (ptr) & 0xff)
+#define bfd_get_signed_8(abfd, ptr) \
+               (((*(unsigned char *) (ptr) & 0xff) ^ 0x80) - 0x80)
+
+#define bfd_put_16(abfd, val, ptr) \
+                BFD_SEND(abfd, bfd_putx16, ((val),(ptr)))
+#define bfd_put_signed_16 \
+                bfd_put_16
+#define bfd_get_16(abfd, ptr) \
+                BFD_SEND(abfd, bfd_getx16, (ptr))
+#define bfd_get_signed_16(abfd, ptr) \
+                BFD_SEND (abfd, bfd_getx_signed_16, (ptr))
+
+#define bfd_put_32(abfd, val, ptr) \
+                BFD_SEND(abfd, bfd_putx32, ((val),(ptr)))
+#define bfd_put_signed_32 \
+                bfd_put_32
+#define bfd_get_32(abfd, ptr) \
+                BFD_SEND(abfd, bfd_getx32, (ptr))
+#define bfd_get_signed_32(abfd, ptr) \
+                BFD_SEND(abfd, bfd_getx_signed_32, (ptr))
+
+#define bfd_put_64(abfd, val, ptr) \
+                BFD_SEND(abfd, bfd_putx64, ((val), (ptr)))
+#define bfd_put_signed_64 \
+                bfd_put_64
+#define bfd_get_64(abfd, ptr) \
+                BFD_SEND(abfd, bfd_getx64, (ptr))
+#define bfd_get_signed_64(abfd, ptr) \
+                BFD_SEND(abfd, bfd_getx_signed_64, (ptr))
+
+#define bfd_get(bits, abfd, ptr)                               \
+                ( (bits) ==  8 ? (bfd_vma) bfd_get_8 (abfd, ptr)       \
+                : (bits) == 16 ? bfd_get_16 (abfd, ptr)        \
+                : (bits) == 32 ? bfd_get_32 (abfd, ptr)        \
+                : (bits) == 64 ? bfd_get_64 (abfd, ptr)        \
+                : (abort (), (bfd_vma) - 1))
+
+#define bfd_put(bits, abfd, val, ptr)                          \
+                ( (bits) ==  8 ? bfd_put_8  (abfd, val, ptr)   \
+                : (bits) == 16 ? bfd_put_16 (abfd, val, ptr)   \
+                : (bits) == 32 ? bfd_put_32 (abfd, val, ptr)   \
+                : (bits) == 64 ? bfd_put_64 (abfd, val, ptr)   \
+                : (abort (), (void) 0))
+
+
+/* Byte swapping macros for file header data.  */
+
+#define bfd_h_put_8(abfd, val, ptr) \
+  bfd_put_8 (abfd, val, ptr)
+#define bfd_h_put_signed_8(abfd, val, ptr) \
+  bfd_put_8 (abfd, val, ptr)
+#define bfd_h_get_8(abfd, ptr) \
+  bfd_get_8 (abfd, ptr)
+#define bfd_h_get_signed_8(abfd, ptr) \
+  bfd_get_signed_8 (abfd, ptr)
+
+#define bfd_h_put_16(abfd, val, ptr) \
+  BFD_SEND (abfd, bfd_h_putx16, (val, ptr))
+#define bfd_h_put_signed_16 \
+  bfd_h_put_16
+#define bfd_h_get_16(abfd, ptr) \
+  BFD_SEND (abfd, bfd_h_getx16, (ptr))
+#define bfd_h_get_signed_16(abfd, ptr) \
+  BFD_SEND (abfd, bfd_h_getx_signed_16, (ptr))
+
+#define bfd_h_put_32(abfd, val, ptr) \
+  BFD_SEND (abfd, bfd_h_putx32, (val, ptr))
+#define bfd_h_put_signed_32 \
+  bfd_h_put_32
+#define bfd_h_get_32(abfd, ptr) \
+  BFD_SEND (abfd, bfd_h_getx32, (ptr))
+#define bfd_h_get_signed_32(abfd, ptr) \
+  BFD_SEND (abfd, bfd_h_getx_signed_32, (ptr))
+
+#define bfd_h_put_64(abfd, val, ptr) \
+  BFD_SEND (abfd, bfd_h_putx64, (val, ptr))
+#define bfd_h_put_signed_64 \
+  bfd_h_put_64
+#define bfd_h_get_64(abfd, ptr) \
+  BFD_SEND (abfd, bfd_h_getx64, (ptr))
+#define bfd_h_get_signed_64(abfd, ptr) \
+  BFD_SEND (abfd, bfd_h_getx_signed_64, (ptr))
+
+/* Refinements on the above, which should eventually go away.  Save
+   cluttering the source with (bfd_vma) and (bfd_byte *) casts.  */
+
+#define H_PUT_64(abfd, val, where) \
+  bfd_h_put_64 ((abfd), (bfd_vma) (val), (bfd_byte *) (where))
+
+#define H_PUT_32(abfd, val, where) \
+  bfd_h_put_32 ((abfd), (bfd_vma) (val), (bfd_byte *) (where))
+
+#define H_PUT_16(abfd, val, where) \
+  bfd_h_put_16 ((abfd), (bfd_vma) (val), (bfd_byte *) (where))
+
+#define H_PUT_8 bfd_h_put_8
+
+#define H_PUT_S64(abfd, val, where) \
+  bfd_h_put_signed_64 ((abfd), (bfd_vma) (val), (bfd_byte *) (where))
+
+#define H_PUT_S32(abfd, val, where) \
+  bfd_h_put_signed_32 ((abfd), (bfd_vma) (val), (bfd_byte *) (where))
+
+#define H_PUT_S16(abfd, val, where) \
+  bfd_h_put_signed_16 ((abfd), (bfd_vma) (val), (bfd_byte *) (where))
+
+#define H_PUT_S8 bfd_h_put_signed_8
+
+#define H_GET_64(abfd, where) \
+  bfd_h_get_64 ((abfd), (bfd_byte *) (where))
+
+#define H_GET_32(abfd, where) \
+  bfd_h_get_32 ((abfd), (bfd_byte *) (where))
+
+#define H_GET_16(abfd, where) \
+  bfd_h_get_16 ((abfd), (bfd_byte *) (where))
+
+#define H_GET_8 bfd_h_get_8
+
+#define H_GET_S64(abfd, where) \
+  bfd_h_get_signed_64 ((abfd), (bfd_byte *) (where))
+
+#define H_GET_S32(abfd, where) \
+  bfd_h_get_signed_32 ((abfd), (bfd_byte *) (where))
+
+#define H_GET_S16(abfd, where) \
+  bfd_h_get_signed_16 ((abfd), (bfd_byte *) (where))
+
+#define H_GET_S8 bfd_h_get_signed_8
+
+
+/* This structure is used for a comdat section, as in PE.  A comdat
+   section is associated with a particular symbol.  When the linker
+   sees a comdat section, it keeps only one of the sections with a
+   given name and associated with a given symbol.  */
+
+struct bfd_comdat_info
+{
+  /* The name of the symbol associated with a comdat section.  */
+  const char *name;
+
+  /* The local symbol table index of the symbol associated with a
+     comdat section.  This is only meaningful to the object file format
+     specific code; it is not an index into the list returned by
+     bfd_canonicalize_symtab.  */
+  long symbol;
+};
+
+typedef struct sec
+{
+  /* The name of the section; the name isn't a copy, the pointer is
+     the same as that passed to bfd_make_section.  */
+
+  const char *name;
+
+  /* A unique sequence number.  */
+
+  int id;
+
+  /* Which section in the bfd; 0..n-1 as sections are created in a bfd.  */
+
+  int index;
+
+  /* The next section in the list belonging to the BFD, or NULL.  */
+
+  struct sec *next;
+
+  /* The field flags contains attributes of the section. Some
+     flags are read in from the object file, and some are
+     synthesized from other information.  */
+
+  flagword flags;
+
+#define SEC_NO_FLAGS   0x000
+
+  /* Tells the OS to allocate space for this section when loading.
+     This is clear for a section containing debug information only.  */
+#define SEC_ALLOC      0x001
+
+  /* Tells the OS to load the section from the file when loading.
+     This is clear for a .bss section.  */
+#define SEC_LOAD       0x002
+
+  /* The section contains data still to be relocated, so there is
+     some relocation information too.  */
+#define SEC_RELOC      0x004
+
+  /* ELF reserves 4 processor specific bits and 8 operating system
+     specific bits in sh_flags; at present we can get away with just
+     one in communicating between the assembler and BFD, but this
+     isn't a good long-term solution.  */
+#define SEC_ARCH_BIT_0 0x008
+
+  /* A signal to the OS that the section contains read only data.  */
+#define SEC_READONLY   0x010
+
+  /* The section contains code only.  */
+#define SEC_CODE       0x020
+
+  /* The section contains data only.  */
+#define SEC_DATA       0x040
+
+  /* The section will reside in ROM.  */
+#define SEC_ROM        0x080
+
+  /* The section contains constructor information. This section
+     type is used by the linker to create lists of constructors and
+     destructors used by <<g++>>. When a back end sees a symbol
+     which should be used in a constructor list, it creates a new
+     section for the type of name (e.g., <<__CTOR_LIST__>>), attaches
+     the symbol to it, and builds a relocation. To build the lists
+     of constructors, all the linker has to do is catenate all the
+     sections called <<__CTOR_LIST__>> and relocate the data
+     contained within - exactly the operations it would peform on
+     standard data.  */
+#define SEC_CONSTRUCTOR 0x100
+
+  /* The section is a constructor, and should be placed at the
+     end of the text, data, or bss section(?).  */
+#define SEC_CONSTRUCTOR_TEXT 0x1100
+#define SEC_CONSTRUCTOR_DATA 0x2100
+#define SEC_CONSTRUCTOR_BSS  0x3100
+
+  /* The section has contents - a data section could be
+     <<SEC_ALLOC>> | <<SEC_HAS_CONTENTS>>; a debug section could be
+     <<SEC_HAS_CONTENTS>>  */
+#define SEC_HAS_CONTENTS 0x200
+
+  /* An instruction to the linker to not output the section
+     even if it has information which would normally be written.  */
+#define SEC_NEVER_LOAD 0x400
+
+  /* The section is a COFF shared library section.  This flag is
+     only for the linker.  If this type of section appears in
+     the input file, the linker must copy it to the output file
+     without changing the vma or size.  FIXME: Although this
+     was originally intended to be general, it really is COFF
+     specific (and the flag was renamed to indicate this).  It
+     might be cleaner to have some more general mechanism to
+     allow the back end to control what the linker does with
+     sections.  */
+#define SEC_COFF_SHARED_LIBRARY 0x800
+
+  /* The section has GOT references.  This flag is only for the
+     linker, and is currently only used by the elf32-hppa back end.
+     It will be set if global offset table references were detected
+     in this section, which indicate to the linker that the section
+     contains PIC code, and must be handled specially when doing a
+     static link.  */
+#define SEC_HAS_GOT_REF 0x4000
+
+  /* The section contains common symbols (symbols may be defined
+     multiple times, the value of a symbol is the amount of
+     space it requires, and the largest symbol value is the one
+     used).  Most targets have exactly one of these (which we
+     translate to bfd_com_section_ptr), but ECOFF has two.  */
+#define SEC_IS_COMMON 0x8000
+
+  /* The section contains only debugging information.  For
+     example, this is set for ELF .debug and .stab sections.
+     strip tests this flag to see if a section can be
+     discarded.  */
+#define SEC_DEBUGGING 0x10000
+
+  /* The contents of this section are held in memory pointed to
+     by the contents field.  This is checked by bfd_get_section_contents,
+     and the data is retrieved from memory if appropriate.  */
+#define SEC_IN_MEMORY 0x20000
+
+  /* The contents of this section are to be excluded by the
+     linker for executable and shared objects unless those
+     objects are to be further relocated.  */
+#define SEC_EXCLUDE 0x40000
+
+  /* The contents of this section are to be sorted based on the sum of
+     the symbol and addend values specified by the associated relocation
+     entries.  Entries without associated relocation entries will be
+     appended to the end of the section in an unspecified order.  */
+#define SEC_SORT_ENTRIES 0x80000
+
+  /* When linking, duplicate sections of the same name should be
+     discarded, rather than being combined into a single section as
+     is usually done.  This is similar to how common symbols are
+     handled.  See SEC_LINK_DUPLICATES below.  */
+#define SEC_LINK_ONCE 0x100000
+
+  /* If SEC_LINK_ONCE is set, this bitfield describes how the linker
+     should handle duplicate sections.  */
+#define SEC_LINK_DUPLICATES 0x600000
+
+  /* This value for SEC_LINK_DUPLICATES means that duplicate
+     sections with the same name should simply be discarded.  */
+#define SEC_LINK_DUPLICATES_DISCARD 0x0
+
+  /* This value for SEC_LINK_DUPLICATES means that the linker
+     should warn if there are any duplicate sections, although
+     it should still only link one copy.  */
+#define SEC_LINK_DUPLICATES_ONE_ONLY 0x200000
+
+  /* This value for SEC_LINK_DUPLICATES means that the linker
+     should warn if any duplicate sections are a different size.  */
+#define SEC_LINK_DUPLICATES_SAME_SIZE 0x400000
+
+  /* This value for SEC_LINK_DUPLICATES means that the linker
+     should warn if any duplicate sections contain different
+     contents.  */
+#define SEC_LINK_DUPLICATES_SAME_CONTENTS 0x600000
+
+  /* This section was created by the linker as part of dynamic
+     relocation or other arcane processing.  It is skipped when
+     going through the first-pass output, trusting that someone
+     else up the line will take care of it later.  */
+#define SEC_LINKER_CREATED 0x800000
+
+  /* This section should not be subject to garbage collection.  */
+#define SEC_KEEP 0x1000000
+
+  /* This section contains "short" data, and should be placed
+     "near" the GP.  */
+#define SEC_SMALL_DATA 0x2000000
+
+  /* This section contains data which may be shared with other
+     executables or shared objects.  */
+#define SEC_SHARED 0x4000000
+
+  /* When a section with this flag is being linked, then if the size of
+     the input section is less than a page, it should not cross a page
+     boundary.  If the size of the input section is one page or more, it
+     should be aligned on a page boundary.  */
+#define SEC_BLOCK 0x8000000
+
+  /* Conditionally link this section; do not link if there are no
+     references found to any symbol in the section.  */
+#define SEC_CLINK 0x10000000
+
+  /* Attempt to merge identical entities in the section.
+     Entity size is given in the entsize field.  */
+#define SEC_MERGE 0x20000000
+
+  /* If given with SEC_MERGE, entities to merge are zero terminated
+     strings where entsize specifies character size instead of fixed
+     size entries.  */
+#define SEC_STRINGS 0x40000000
+
+  /* This section contains data about section groups.  */
+#define SEC_GROUP 0x80000000
+
+  /*  End of section flags.  */
+
+  /* Some internal packed boolean fields.  */
+
+  /* See the vma field.  */
+  unsigned int user_set_vma : 1;
+
+  /* Whether relocations have been processed.  */
+  unsigned int reloc_done : 1;
+
+  /* A mark flag used by some of the linker backends.  */
+  unsigned int linker_mark : 1;
+
+  /* Another mark flag used by some of the linker backends.  Set for
+     output sections that have an input section.  */
+  unsigned int linker_has_input : 1;
+
+  /* A mark flag used by some linker backends for garbage collection.  */
+  unsigned int gc_mark : 1;
+
+  /* Used by the ELF code to mark sections which have been allocated
+     to segments.  */
+  unsigned int segment_mark : 1;
+
+  /* End of internal packed boolean fields.  */
+
+  /*  The virtual memory address of the section - where it will be
+      at run time.  The symbols are relocated against this.  The
+      user_set_vma flag is maintained by bfd; if it's not set, the
+      backend can assign addresses (for example, in <<a.out>>, where
+      the default address for <<.data>> is dependent on the specific
+      target and various flags).  */
+
+  bfd_vma vma;
+
+  /*  The load address of the section - where it would be in a
+      rom image; really only used for writing section header
+      information. */
+
+  bfd_vma lma;
+
+  /* The size of the section in octets, as it will be output.
+     Contains a value even if the section has no contents (e.g., the
+     size of <<.bss>>).  This will be filled in after relocation.  */
+
+  bfd_size_type _cooked_size;
+
+  /* The original size on disk of the section, in octets.  Normally this
+     value is the same as the size, but if some relaxing has
+     been done, then this value will be bigger.  */
+
+  bfd_size_type _raw_size;
+
+  /* If this section is going to be output, then this value is the
+     offset in *bytes* into the output section of the first byte in the
+     input section (byte ==> smallest addressable unit on the
+     target).  In most cases, if this was going to start at the
+     100th octet (8-bit quantity) in the output section, this value
+     would be 100.  However, if the target byte size is 16 bits
+     (bfd_octets_per_byte is "2"), this value would be 50.  */
+
+  bfd_vma output_offset;
+
+  /* The output section through which to map on output.  */
+
+  struct sec *output_section;
+
+  /* The alignment requirement of the section, as an exponent of 2 -
+     e.g., 3 aligns to 2^3 (or 8).  */
+
+  unsigned int alignment_power;
+
+  /* If an input section, a pointer to a vector of relocation
+     records for the data in this section.  */
+
+  struct reloc_cache_entry *relocation;
+
+  /* If an output section, a pointer to a vector of pointers to
+     relocation records for the data in this section.  */
+
+  struct reloc_cache_entry **orelocation;
+
+  /* The number of relocation records in one of the above  */
+
+  unsigned reloc_count;
+
+  /* Information below is back end specific - and not always used
+     or updated.  */
+
+  /* File position of section data.  */
+
+  file_ptr filepos;
+
+  /* File position of relocation info.  */
+
+  file_ptr rel_filepos;
+
+  /* File position of line data.  */
+
+  file_ptr line_filepos;
+
+  /* Pointer to data for applications.  */
+
+  PTR userdata;
+
+  /* If the SEC_IN_MEMORY flag is set, this points to the actual
+     contents.  */
+  unsigned char *contents;
+
+  /* Attached line number information.  */
+
+  alent *lineno;
+
+  /* Number of line number records.  */
+
+  unsigned int lineno_count;
+
+  /* Entity size for merging purposes.  */
+
+  unsigned int entsize;
+
+  /* Optional information about a COMDAT entry; NULL if not COMDAT.  */
+
+  struct bfd_comdat_info *comdat;
+
+  /* When a section is being output, this value changes as more
+     linenumbers are written out.  */
+
+  file_ptr moving_line_filepos;
+
+  /* What the section number is in the target world.  */
+
+  int target_index;
+
+  PTR used_by_bfd;
+
+  /* If this is a constructor section then here is a list of the
+     relocations created to relocate items within it.  */
+
+  struct relent_chain *constructor_chain;
+
+  /* The BFD which owns the section.  */
+
+  bfd *owner;
+
+  /* A symbol which points at this section only */
+  struct symbol_cache_entry *symbol;
+  struct symbol_cache_entry **symbol_ptr_ptr;
+
+  struct bfd_link_order *link_order_head;
+  struct bfd_link_order *link_order_tail;
+} asection ;
+
+/* These sections are global, and are managed by BFD.  The application
+   and target back end are not permitted to change the values in
+   these sections.  New code should use the section_ptr macros rather
+   than referring directly to the const sections.  The const sections
+   may eventually vanish.  */
+#define BFD_ABS_SECTION_NAME "*ABS*"
+#define BFD_UND_SECTION_NAME "*UND*"
+#define BFD_COM_SECTION_NAME "*COM*"
+#define BFD_IND_SECTION_NAME "*IND*"
+
+/* the absolute section */
+extern const asection bfd_abs_section;
+#define bfd_abs_section_ptr ((asection *) &bfd_abs_section)
+#define bfd_is_abs_section(sec) ((sec) == bfd_abs_section_ptr)
+/* Pointer to the undefined section */
+extern const asection bfd_und_section;
+#define bfd_und_section_ptr ((asection *) &bfd_und_section)
+#define bfd_is_und_section(sec) ((sec) == bfd_und_section_ptr)
+/* Pointer to the common section */
+extern const asection bfd_com_section;
+#define bfd_com_section_ptr ((asection *) &bfd_com_section)
+/* Pointer to the indirect section */
+extern const asection bfd_ind_section;
+#define bfd_ind_section_ptr ((asection *) &bfd_ind_section)
+#define bfd_is_ind_section(sec) ((sec) == bfd_ind_section_ptr)
+
+extern const struct symbol_cache_entry * const bfd_abs_symbol;
+extern const struct symbol_cache_entry * const bfd_com_symbol;
+extern const struct symbol_cache_entry * const bfd_und_symbol;
+extern const struct symbol_cache_entry * const bfd_ind_symbol;
+#define bfd_get_section_size_before_reloc(section) \
+     ((section)->reloc_done ? (abort (), (bfd_size_type) 1) \
+                            : (section)->_raw_size)
+#define bfd_get_section_size_after_reloc(section) \
+     ((section)->reloc_done ? (section)->_cooked_size \
+                            : (abort (), (bfd_size_type) 1))
+asection *
+bfd_get_section_by_name PARAMS ((bfd *abfd, const char *name));
+
+char *
+bfd_get_unique_section_name PARAMS ((bfd *abfd,
+    const char *templat,
+    int *count));
+
+asection *
+bfd_make_section_old_way PARAMS ((bfd *abfd, const char *name));
+
+asection *
+bfd_make_section_anyway PARAMS ((bfd *abfd, const char *name));
+
+asection *
+bfd_make_section PARAMS ((bfd *, const char *name));
+
+boolean
+bfd_set_section_flags PARAMS ((bfd *abfd, asection *sec, flagword flags));
+
+void
+bfd_map_over_sections PARAMS ((bfd *abfd,
+    void (*func) (bfd *abfd,
+    asection *sect,
+    PTR obj),
+    PTR obj));
+
+boolean
+bfd_set_section_size PARAMS ((bfd *abfd, asection *sec, bfd_size_type val));
+
+boolean
+bfd_set_section_contents PARAMS ((bfd *abfd, asection *section,
+    PTR data, file_ptr offset,
+    bfd_size_type count));
+
+boolean
+bfd_get_section_contents PARAMS ((bfd *abfd, asection *section,
+    PTR location, file_ptr offset,
+    bfd_size_type count));
+
+boolean
+bfd_copy_private_section_data PARAMS ((bfd *ibfd, asection *isec,
+    bfd *obfd, asection *osec));
+
+#define bfd_copy_private_section_data(ibfd, isection, obfd, osection) \
+     BFD_SEND (obfd, _bfd_copy_private_section_data, \
+               (ibfd, isection, obfd, osection))
+void
+_bfd_strip_section_from_output PARAMS ((struct bfd_link_info *info, asection *section));
+
+enum bfd_architecture
+{
+  bfd_arch_unknown,   /* File arch not known */
+  bfd_arch_obscure,   /* Arch known, not one of these */
+  bfd_arch_m68k,      /* Motorola 68xxx */
+#define bfd_mach_m68000 1
+#define bfd_mach_m68008 2
+#define bfd_mach_m68010 3
+#define bfd_mach_m68020 4
+#define bfd_mach_m68030 5
+#define bfd_mach_m68040 6
+#define bfd_mach_m68060 7
+#define bfd_mach_cpu32  8
+#define bfd_mach_mcf5200  9
+#define bfd_mach_mcf5206e 10
+#define bfd_mach_mcf5307  11
+#define bfd_mach_mcf5407  12
+  bfd_arch_vax,       /* DEC Vax */
+  bfd_arch_i960,      /* Intel 960 */
+    /* The order of the following is important.
+       lower number indicates a machine type that
+       only accepts a subset of the instructions
+       available to machines with higher numbers.
+       The exception is the "ca", which is
+       incompatible with all other machines except
+       "core". */
+
+#define bfd_mach_i960_core      1
+#define bfd_mach_i960_ka_sa     2
+#define bfd_mach_i960_kb_sb     3
+#define bfd_mach_i960_mc        4
+#define bfd_mach_i960_xa        5
+#define bfd_mach_i960_ca        6
+#define bfd_mach_i960_jx        7
+#define bfd_mach_i960_hx        8
+
+  bfd_arch_a29k,      /* AMD 29000 */
+  bfd_arch_sparc,     /* SPARC */
+#define bfd_mach_sparc                 1
+/* The difference between v8plus and v9 is that v9 is a true 64 bit env.  */
+#define bfd_mach_sparc_sparclet        2
+#define bfd_mach_sparc_sparclite       3
+#define bfd_mach_sparc_v8plus          4
+#define bfd_mach_sparc_v8plusa         5 /* with ultrasparc add'ns */
+#define bfd_mach_sparc_sparclite_le    6
+#define bfd_mach_sparc_v9              7
+#define bfd_mach_sparc_v9a             8 /* with ultrasparc add'ns */
+#define bfd_mach_sparc_v8plusb         9 /* with cheetah add'ns */
+#define bfd_mach_sparc_v9b             10 /* with cheetah add'ns */
+/* Nonzero if MACH has the v9 instruction set.  */
+#define bfd_mach_sparc_v9_p(mach) \
+  ((mach) >= bfd_mach_sparc_v8plus && (mach) <= bfd_mach_sparc_v9b \
+   && (mach) != bfd_mach_sparc_sparclite_le)
+  bfd_arch_mips,      /* MIPS Rxxxx */
+#define bfd_mach_mips3000              3000
+#define bfd_mach_mips3900              3900
+#define bfd_mach_mips4000              4000
+#define bfd_mach_mips4010              4010
+#define bfd_mach_mips4100              4100
+#define bfd_mach_mips4111              4111
+#define bfd_mach_mips4300              4300
+#define bfd_mach_mips4400              4400
+#define bfd_mach_mips4600              4600
+#define bfd_mach_mips4650              4650
+#define bfd_mach_mips5000              5000
+#define bfd_mach_mips6000              6000
+#define bfd_mach_mips8000              8000
+#define bfd_mach_mips10000             10000
+#define bfd_mach_mips12000             12000
+#define bfd_mach_mips16                16
+#define bfd_mach_mips5                 5
+#define bfd_mach_mips_sb1              12310201 /* octal 'SB', 01 */
+#define bfd_mach_mipsisa32             32
+#define bfd_mach_mipsisa64             64
+  bfd_arch_i386,      /* Intel 386 */
+#define bfd_mach_i386_i386 0
+#define bfd_mach_i386_i8086 1
+#define bfd_mach_i386_i386_intel_syntax 2
+#define bfd_mach_x86_64 3
+#define bfd_mach_x86_64_intel_syntax 4
+  bfd_arch_we32k,     /* AT&T WE32xxx */
+  bfd_arch_tahoe,     /* CCI/Harris Tahoe */
+  bfd_arch_i860,      /* Intel 860 */
+  bfd_arch_i370,      /* IBM 360/370 Mainframes */
+  bfd_arch_romp,      /* IBM ROMP PC/RT */
+  bfd_arch_alliant,   /* Alliant */
+  bfd_arch_convex,    /* Convex */
+  bfd_arch_m88k,      /* Motorola 88xxx */
+  bfd_arch_pyramid,   /* Pyramid Technology */
+  bfd_arch_h8300,     /* Hitachi H8/300 */
+#define bfd_mach_h8300   1
+#define bfd_mach_h8300h  2
+#define bfd_mach_h8300s  3
+  bfd_arch_pdp11,     /* DEC PDP-11 */
+  bfd_arch_powerpc,   /* PowerPC */
+#define bfd_mach_ppc           0
+#define bfd_mach_ppc_403       403
+#define bfd_mach_ppc_403gc     4030
+#define bfd_mach_ppc_505       505
+#define bfd_mach_ppc_601       601
+#define bfd_mach_ppc_602       602
+#define bfd_mach_ppc_603       603
+#define bfd_mach_ppc_ec603e    6031
+#define bfd_mach_ppc_604       604
+#define bfd_mach_ppc_620       620
+#define bfd_mach_ppc_630       630
+#define bfd_mach_ppc_750       750
+#define bfd_mach_ppc_860       860
+#define bfd_mach_ppc_a35       35
+#define bfd_mach_ppc_rs64ii    642
+#define bfd_mach_ppc_rs64iii   643
+#define bfd_mach_ppc_7400      7400
+  bfd_arch_rs6000,    /* IBM RS/6000 */
+#define bfd_mach_rs6k          0
+#define bfd_mach_rs6k_rs1      6001
+#define bfd_mach_rs6k_rsc      6003
+#define bfd_mach_rs6k_rs2      6002
+  bfd_arch_hppa,      /* HP PA RISC */
+  bfd_arch_d10v,      /* Mitsubishi D10V */
+#define bfd_mach_d10v          0
+#define bfd_mach_d10v_ts2      2
+#define bfd_mach_d10v_ts3      3
+  bfd_arch_d30v,      /* Mitsubishi D30V */
+  bfd_arch_m68hc11,   /* Motorola 68HC11 */
+  bfd_arch_m68hc12,   /* Motorola 68HC12 */
+  bfd_arch_z8k,       /* Zilog Z8000 */
+#define bfd_mach_z8001         1
+#define bfd_mach_z8002         2
+  bfd_arch_h8500,     /* Hitachi H8/500 */
+  bfd_arch_sh,        /* Hitachi SH */
+#define bfd_mach_sh            0
+#define bfd_mach_sh2        0x20
+#define bfd_mach_sh_dsp     0x2d
+#define bfd_mach_sh3        0x30
+#define bfd_mach_sh3_dsp    0x3d
+#define bfd_mach_sh3e       0x3e
+#define bfd_mach_sh4        0x40
+  bfd_arch_alpha,     /* Dec Alpha */
+#define bfd_mach_alpha_ev4  0x10
+#define bfd_mach_alpha_ev5  0x20
+#define bfd_mach_alpha_ev6  0x30
+  bfd_arch_arm,       /* Advanced Risc Machines ARM */
+#define bfd_mach_arm_2         1
+#define bfd_mach_arm_2a        2
+#define bfd_mach_arm_3         3
+#define bfd_mach_arm_3M        4
+#define bfd_mach_arm_4         5
+#define bfd_mach_arm_4T        6
+#define bfd_mach_arm_5         7
+#define bfd_mach_arm_5T        8
+#define bfd_mach_arm_5TE       9
+#define bfd_mach_arm_XScale    10
+  bfd_arch_ns32k,     /* National Semiconductors ns32000 */
+  bfd_arch_w65,       /* WDC 65816 */
+  bfd_arch_tic30,     /* Texas Instruments TMS320C30 */
+  bfd_arch_tic54x,    /* Texas Instruments TMS320C54X */
+  bfd_arch_tic80,     /* TI TMS320c80 (MVP) */
+  bfd_arch_v850,      /* NEC V850 */
+#define bfd_mach_v850          0
+#define bfd_mach_v850e         'E'
+#define bfd_mach_v850ea        'A'
+  bfd_arch_arc,       /* ARC Cores */
+#define bfd_mach_arc_5         0
+#define bfd_mach_arc_6         1
+#define bfd_mach_arc_7         2
+#define bfd_mach_arc_8         3
+  bfd_arch_m32r,      /* Mitsubishi M32R/D */
+#define bfd_mach_m32r          0 /* backwards compatibility */
+#define bfd_mach_m32rx         'x'
+  bfd_arch_mn10200,   /* Matsushita MN10200 */
+  bfd_arch_mn10300,   /* Matsushita MN10300 */
+#define bfd_mach_mn10300               300
+#define bfd_mach_am33          330
+  bfd_arch_fr30,
+#define bfd_mach_fr30          0x46523330
+  bfd_arch_mcore,
+  bfd_arch_ia64,      /* HP/Intel ia64 */
+#define bfd_mach_ia64_elf64    0
+#define bfd_mach_ia64_elf32    1
+  bfd_arch_pj,
+  bfd_arch_avr,       /* Atmel AVR microcontrollers */
+#define bfd_mach_avr1          1
+#define bfd_mach_avr2          2
+#define bfd_mach_avr3          3
+#define bfd_mach_avr4          4
+#define bfd_mach_avr5          5
+  bfd_arch_cris,      /* Axis CRIS */
+  bfd_arch_s390,      /* IBM s390 */
+#define bfd_mach_s390_esa      0
+#define bfd_mach_s390_esame    1
+  bfd_arch_openrisc,  /* OpenRISC */
+  bfd_arch_last
+  };
+
+typedef struct bfd_arch_info
+{
+  int bits_per_word;
+  int bits_per_address;
+  int bits_per_byte;
+  enum bfd_architecture arch;
+  unsigned long mach;
+  const char *arch_name;
+  const char *printable_name;
+  unsigned int section_align_power;
+  /* True if this is the default machine for the architecture.  */
+  boolean the_default;
+  const struct bfd_arch_info * (*compatible)
+       PARAMS ((const struct bfd_arch_info *a,
+                const struct bfd_arch_info *b));
+
+  boolean (*scan) PARAMS ((const struct bfd_arch_info *, const char *));
+
+  const struct bfd_arch_info *next;
+} bfd_arch_info_type;
+const char *
+bfd_printable_name PARAMS ((bfd *abfd));
+
+const bfd_arch_info_type *
+bfd_scan_arch PARAMS ((const char *string));
+
+const char **
+bfd_arch_list PARAMS ((void));
+
+const bfd_arch_info_type *
+bfd_arch_get_compatible PARAMS ((
+    const bfd *abfd,
+    const bfd *bbfd));
+
+void
+bfd_set_arch_info PARAMS ((bfd *abfd, const bfd_arch_info_type *arg));
+
+enum bfd_architecture
+bfd_get_arch PARAMS ((bfd *abfd));
+
+unsigned long
+bfd_get_mach PARAMS ((bfd *abfd));
+
+unsigned int
+bfd_arch_bits_per_byte PARAMS ((bfd *abfd));
+
+unsigned int
+bfd_arch_bits_per_address PARAMS ((bfd *abfd));
+
+const bfd_arch_info_type *
+bfd_get_arch_info PARAMS ((bfd *abfd));
+
+const bfd_arch_info_type *
+bfd_lookup_arch PARAMS ((enum bfd_architecture
+    arch,
+    unsigned long machine));
+
+const char *
+bfd_printable_arch_mach PARAMS ((enum bfd_architecture arch, unsigned long machine));
+
+unsigned int
+bfd_octets_per_byte PARAMS ((bfd *abfd));
+
+unsigned int
+bfd_arch_mach_octets_per_byte PARAMS ((enum bfd_architecture arch,
+    unsigned long machine));
+
+typedef enum bfd_reloc_status
+{
+  /* No errors detected */
+  bfd_reloc_ok,
+
+  /* The relocation was performed, but there was an overflow. */
+  bfd_reloc_overflow,
+
+  /* The address to relocate was not within the section supplied. */
+  bfd_reloc_outofrange,
+
+  /* Used by special functions */
+  bfd_reloc_continue,
+
+  /* Unsupported relocation size requested. */
+  bfd_reloc_notsupported,
+
+  /* Unused */
+  bfd_reloc_other,
+
+  /* The symbol to relocate against was undefined. */
+  bfd_reloc_undefined,
+
+  /* The relocation was performed, but may not be ok - presently
+     generated only when linking i960 coff files with i960 b.out
+     symbols.  If this type is returned, the error_message argument
+     to bfd_perform_relocation will be set.  */
+  bfd_reloc_dangerous
+ }
+ bfd_reloc_status_type;
+
+
+typedef struct reloc_cache_entry
+{
+  /* A pointer into the canonical table of pointers  */
+  struct symbol_cache_entry **sym_ptr_ptr;
+
+  /* offset in section */
+  bfd_size_type address;
+
+  /* addend for relocation value */
+  bfd_vma addend;
+
+  /* Pointer to how to perform the required relocation */
+  reloc_howto_type *howto;
+
+} arelent;
+enum complain_overflow
+{
+  /* Do not complain on overflow. */
+  complain_overflow_dont,
+
+  /* Complain if the bitfield overflows, whether it is considered
+     as signed or unsigned. */
+  complain_overflow_bitfield,
+
+  /* Complain if the value overflows when considered as signed
+     number. */
+  complain_overflow_signed,
+
+  /* Complain if the value overflows when considered as an
+     unsigned number. */
+  complain_overflow_unsigned
+};
+
+struct reloc_howto_struct
+{
+  /*  The type field has mainly a documentary use - the back end can
+      do what it wants with it, though normally the back end's
+      external idea of what a reloc number is stored
+      in this field.  For example, a PC relative word relocation
+      in a coff environment has the type 023 - because that's
+      what the outside world calls a R_PCRWORD reloc.  */
+  unsigned int type;
+
+  /*  The value the final relocation is shifted right by.  This drops
+      unwanted data from the relocation.  */
+  unsigned int rightshift;
+
+  /*  The size of the item to be relocated.  This is *not* a
+      power-of-two measure.  To get the number of bytes operated
+      on by a type of relocation, use bfd_get_reloc_size.  */
+  int size;
+
+  /*  The number of bits in the item to be relocated.  This is used
+      when doing overflow checking.  */
+  unsigned int bitsize;
+
+  /*  Notes that the relocation is relative to the location in the
+      data section of the addend.  The relocation function will
+      subtract from the relocation value the address of the location
+      being relocated.  */
+  boolean pc_relative;
+
+  /*  The bit position of the reloc value in the destination.
+      The relocated value is left shifted by this amount.  */
+  unsigned int bitpos;
+
+  /* What type of overflow error should be checked for when
+     relocating.  */
+  enum complain_overflow complain_on_overflow;
+
+  /* If this field is non null, then the supplied function is
+     called rather than the normal function.  This allows really
+     strange relocation methods to be accomodated (e.g., i960 callj
+     instructions).  */
+  bfd_reloc_status_type (*special_function)
+    PARAMS ((bfd *, arelent *, struct symbol_cache_entry *, PTR, asection *,
+             bfd *, char **));
+
+  /* The textual name of the relocation type.  */
+  char *name;
+
+  /* Some formats record a relocation addend in the section contents
+     rather than with the relocation.  For ELF formats this is the
+     distinction between USE_REL and USE_RELA (though the code checks
+     for USE_REL == 1/0).  The value of this field is TRUE if the
+     addend is recorded with the section contents; when performing a
+     partial link (ld -r) the section contents (the data) will be
+     modified.  The value of this field is FALSE if addends are
+     recorded with the relocation (in arelent.addend); when performing
+     a partial link the relocation will be modified.
+     All relocations for all ELF USE_RELA targets should set this field
+     to FALSE (values of TRUE should be looked on with suspicion).
+     However, the converse is not true: not all relocations of all ELF
+     USE_REL targets set this field to TRUE.  Why this is so is peculiar
+     to each particular target.  For relocs that aren't used in partial
+     links (e.g. GOT stuff) it doesn't matter what this is set to.  */
+  boolean partial_inplace;
+
+  /* The src_mask selects which parts of the read in data
+     are to be used in the relocation sum.  E.g., if this was an 8 bit
+     byte of data which we read and relocated, this would be
+     0x000000ff.  When we have relocs which have an addend, such as
+     sun4 extended relocs, the value in the offset part of a
+     relocating field is garbage so we never use it.  In this case
+     the mask would be 0x00000000.  */
+  bfd_vma src_mask;
+
+  /* The dst_mask selects which parts of the instruction are replaced
+     into the instruction.  In most cases src_mask == dst_mask,
+     except in the above special case, where dst_mask would be
+     0x000000ff, and src_mask would be 0x00000000.  */
+  bfd_vma dst_mask;
+
+  /* When some formats create PC relative instructions, they leave
+     the value of the pc of the place being relocated in the offset
+     slot of the instruction, so that a PC relative relocation can
+     be made just by adding in an ordinary offset (e.g., sun3 a.out).
+     Some formats leave the displacement part of an instruction
+     empty (e.g., m88k bcs); this flag signals the fact.  */
+  boolean pcrel_offset;
+};
+#define HOWTO(C, R, S, B, P, BI, O, SF, NAME, INPLACE, MASKSRC, MASKDST, PC) \
+  { (unsigned) C, R, S, B, P, BI, O, SF, NAME, INPLACE, MASKSRC, MASKDST, PC }
+#define NEWHOWTO(FUNCTION, NAME, SIZE, REL, IN) \
+  HOWTO (0, 0, SIZE, 0, REL, 0, complain_overflow_dont, FUNCTION, \
+         NAME, false, 0, 0, IN)
+
+#define EMPTY_HOWTO(C) \
+  HOWTO ((C), 0, 0, 0, false, 0, complain_overflow_dont, NULL, \
+         NULL, false, 0, 0, false)
+
+#define HOWTO_PREPARE(relocation, symbol)               \
+  {                                                     \
+    if (symbol != (asymbol *) NULL)                     \
+      {                                                 \
+        if (bfd_is_com_section (symbol->section))       \
+          {                                             \
+            relocation = 0;                             \
+          }                                             \
+        else                                            \
+          {                                             \
+            relocation = symbol->value;                 \
+          }                                             \
+      }                                                 \
+  }
+unsigned int
+bfd_get_reloc_size PARAMS ((reloc_howto_type *));
+
+typedef struct relent_chain
+{
+  arelent relent;
+  struct relent_chain *next;
+} arelent_chain;
+bfd_reloc_status_type
+bfd_check_overflow PARAMS ((enum complain_overflow how,
+    unsigned int bitsize,
+    unsigned int rightshift,
+    unsigned int addrsize,
+    bfd_vma relocation));
+
+bfd_reloc_status_type
+bfd_perform_relocation PARAMS ((bfd *abfd,
+    arelent *reloc_entry,
+    PTR data,
+    asection *input_section,
+    bfd *output_bfd,
+    char **error_message));
+
+bfd_reloc_status_type
+bfd_install_relocation PARAMS ((bfd *abfd,
+    arelent *reloc_entry,
+    PTR data, bfd_vma data_start,
+    asection *input_section,
+    char **error_message));
+
+enum bfd_reloc_code_real {
+  _dummy_first_bfd_reloc_code_real,
+
+
+/* Basic absolute relocations of N bits. */
+  BFD_RELOC_64,
+  BFD_RELOC_32,
+  BFD_RELOC_26,
+  BFD_RELOC_24,
+  BFD_RELOC_16,
+  BFD_RELOC_14,
+  BFD_RELOC_8,
+
+/* PC-relative relocations.  Sometimes these are relative to the address
+of the relocation itself; sometimes they are relative to the start of
+the section containing the relocation.  It depends on the specific target.
+
+The 24-bit relocation is used in some Intel 960 configurations. */
+  BFD_RELOC_64_PCREL,
+  BFD_RELOC_32_PCREL,
+  BFD_RELOC_24_PCREL,
+  BFD_RELOC_16_PCREL,
+  BFD_RELOC_12_PCREL,
+  BFD_RELOC_8_PCREL,
+
+/* For ELF. */
+  BFD_RELOC_32_GOT_PCREL,
+  BFD_RELOC_16_GOT_PCREL,
+  BFD_RELOC_8_GOT_PCREL,
+  BFD_RELOC_32_GOTOFF,
+  BFD_RELOC_16_GOTOFF,
+  BFD_RELOC_LO16_GOTOFF,
+  BFD_RELOC_HI16_GOTOFF,
+  BFD_RELOC_HI16_S_GOTOFF,
+  BFD_RELOC_8_GOTOFF,
+  BFD_RELOC_64_PLT_PCREL,
+  BFD_RELOC_32_PLT_PCREL,
+  BFD_RELOC_24_PLT_PCREL,
+  BFD_RELOC_16_PLT_PCREL,
+  BFD_RELOC_8_PLT_PCREL,
+  BFD_RELOC_64_PLTOFF,
+  BFD_RELOC_32_PLTOFF,
+  BFD_RELOC_16_PLTOFF,
+  BFD_RELOC_LO16_PLTOFF,
+  BFD_RELOC_HI16_PLTOFF,
+  BFD_RELOC_HI16_S_PLTOFF,
+  BFD_RELOC_8_PLTOFF,
+
+/* Relocations used by 68K ELF. */
+  BFD_RELOC_68K_GLOB_DAT,
+  BFD_RELOC_68K_JMP_SLOT,
+  BFD_RELOC_68K_RELATIVE,
+
+/* Linkage-table relative. */
+  BFD_RELOC_32_BASEREL,
+  BFD_RELOC_16_BASEREL,
+  BFD_RELOC_LO16_BASEREL,
+  BFD_RELOC_HI16_BASEREL,
+  BFD_RELOC_HI16_S_BASEREL,
+  BFD_RELOC_8_BASEREL,
+  BFD_RELOC_RVA,
+
+/* Absolute 8-bit relocation, but used to form an address like 0xFFnn. */
+  BFD_RELOC_8_FFnn,
+
+/* These PC-relative relocations are stored as word displacements --
+i.e., byte displacements shifted right two bits.  The 30-bit word
+displacement (<<32_PCREL_S2>> -- 32 bits, shifted 2) is used on the
+SPARC.  (SPARC tools generally refer to this as <<WDISP30>>.)  The
+signed 16-bit displacement is used on the MIPS, and the 23-bit
+displacement is used on the Alpha. */
+  BFD_RELOC_32_PCREL_S2,
+  BFD_RELOC_16_PCREL_S2,
+  BFD_RELOC_23_PCREL_S2,
+
+/* High 22 bits and low 10 bits of 32-bit value, placed into lower bits of
+the target word.  These are used on the SPARC. */
+  BFD_RELOC_HI22,
+  BFD_RELOC_LO10,
+
+/* For systems that allocate a Global Pointer register, these are
+displacements off that register.  These relocation types are
+handled specially, because the value the register will have is
+decided relatively late. */
+  BFD_RELOC_GPREL16,
+  BFD_RELOC_GPREL32,
+
+/* Reloc types used for i960/b.out. */
+  BFD_RELOC_I960_CALLJ,
+
+/* SPARC ELF relocations.  There is probably some overlap with other
+relocation types already defined. */
+  BFD_RELOC_NONE,
+  BFD_RELOC_SPARC_WDISP22,
+  BFD_RELOC_SPARC22,
+  BFD_RELOC_SPARC13,
+  BFD_RELOC_SPARC_GOT10,
+  BFD_RELOC_SPARC_GOT13,
+  BFD_RELOC_SPARC_GOT22,
+  BFD_RELOC_SPARC_PC10,
+  BFD_RELOC_SPARC_PC22,
+  BFD_RELOC_SPARC_WPLT30,
+  BFD_RELOC_SPARC_COPY,
+  BFD_RELOC_SPARC_GLOB_DAT,
+  BFD_RELOC_SPARC_JMP_SLOT,
+  BFD_RELOC_SPARC_RELATIVE,
+  BFD_RELOC_SPARC_UA16,
+  BFD_RELOC_SPARC_UA32,
+  BFD_RELOC_SPARC_UA64,
+
+/* I think these are specific to SPARC a.out (e.g., Sun 4). */
+  BFD_RELOC_SPARC_BASE13,
+  BFD_RELOC_SPARC_BASE22,
+
+/* SPARC64 relocations */
+#define BFD_RELOC_SPARC_64 BFD_RELOC_64
+  BFD_RELOC_SPARC_10,
+  BFD_RELOC_SPARC_11,
+  BFD_RELOC_SPARC_OLO10,
+  BFD_RELOC_SPARC_HH22,
+  BFD_RELOC_SPARC_HM10,
+  BFD_RELOC_SPARC_LM22,
+  BFD_RELOC_SPARC_PC_HH22,
+  BFD_RELOC_SPARC_PC_HM10,
+  BFD_RELOC_SPARC_PC_LM22,
+  BFD_RELOC_SPARC_WDISP16,
+  BFD_RELOC_SPARC_WDISP19,
+  BFD_RELOC_SPARC_7,
+  BFD_RELOC_SPARC_6,
+  BFD_RELOC_SPARC_5,
+#define BFD_RELOC_SPARC_DISP64 BFD_RELOC_64_PCREL
+  BFD_RELOC_SPARC_PLT64,
+  BFD_RELOC_SPARC_HIX22,
+  BFD_RELOC_SPARC_LOX10,
+  BFD_RELOC_SPARC_H44,
+  BFD_RELOC_SPARC_M44,
+  BFD_RELOC_SPARC_L44,
+  BFD_RELOC_SPARC_REGISTER,
+
+/* SPARC little endian relocation */
+  BFD_RELOC_SPARC_REV32,
+
+/* Alpha ECOFF and ELF relocations.  Some of these treat the symbol or
+"addend" in some special way.
+For GPDISP_HI16 ("gpdisp") relocations, the symbol is ignored when
+writing; when reading, it will be the absolute section symbol.  The
+addend is the displacement in bytes of the "lda" instruction from
+the "ldah" instruction (which is at the address of this reloc). */
+  BFD_RELOC_ALPHA_GPDISP_HI16,
+
+/* For GPDISP_LO16 ("ignore") relocations, the symbol is handled as
+with GPDISP_HI16 relocs.  The addend is ignored when writing the
+relocations out, and is filled in with the file's GP value on
+reading, for convenience. */
+  BFD_RELOC_ALPHA_GPDISP_LO16,
+
+/* The ELF GPDISP relocation is exactly the same as the GPDISP_HI16
+relocation except that there is no accompanying GPDISP_LO16
+relocation. */
+  BFD_RELOC_ALPHA_GPDISP,
+
+/* The Alpha LITERAL/LITUSE relocs are produced by a symbol reference;
+the assembler turns it into a LDQ instruction to load the address of
+the symbol, and then fills in a register in the real instruction.
+
+The LITERAL reloc, at the LDQ instruction, refers to the .lita
+section symbol.  The addend is ignored when writing, but is filled
+in with the file's GP value on reading, for convenience, as with the
+GPDISP_LO16 reloc.
+
+The ELF_LITERAL reloc is somewhere between 16_GOTOFF and GPDISP_LO16.
+It should refer to the symbol to be referenced, as with 16_GOTOFF,
+but it generates output not based on the position within the .got
+section, but relative to the GP value chosen for the file during the
+final link stage.
+
+The LITUSE reloc, on the instruction using the loaded address, gives
+information to the linker that it might be able to use to optimize
+away some literal section references.  The symbol is ignored (read
+as the absolute section symbol), and the "addend" indicates the type
+of instruction using the register:
+1 - "memory" fmt insn
+2 - byte-manipulation (byte offset reg)
+3 - jsr (target of branch) */
+  BFD_RELOC_ALPHA_LITERAL,
+  BFD_RELOC_ALPHA_ELF_LITERAL,
+  BFD_RELOC_ALPHA_LITUSE,
+
+/* The HINT relocation indicates a value that should be filled into the
+"hint" field of a jmp/jsr/ret instruction, for possible branch-
+prediction logic which may be provided on some processors. */
+  BFD_RELOC_ALPHA_HINT,
+
+/* The LINKAGE relocation outputs a linkage pair in the object file,
+which is filled by the linker. */
+  BFD_RELOC_ALPHA_LINKAGE,
+
+/* The CODEADDR relocation outputs a STO_CA in the object file,
+which is filled by the linker. */
+  BFD_RELOC_ALPHA_CODEADDR,
+
+/* The GPREL_HI/LO relocations together form a 32-bit offset from the
+GP register. */
+  BFD_RELOC_ALPHA_GPREL_HI16,
+  BFD_RELOC_ALPHA_GPREL_LO16,
+
+/* Bits 27..2 of the relocation address shifted right 2 bits;
+simple reloc otherwise. */
+  BFD_RELOC_MIPS_JMP,
+
+/* The MIPS16 jump instruction. */
+  BFD_RELOC_MIPS16_JMP,
+
+/* MIPS16 GP relative reloc. */
+  BFD_RELOC_MIPS16_GPREL,
+
+/* High 16 bits of 32-bit value; simple reloc. */
+  BFD_RELOC_HI16,
+
+/* High 16 bits of 32-bit value but the low 16 bits will be sign
+extended and added to form the final result.  If the low 16
+bits form a negative number, we need to add one to the high value
+to compensate for the borrow when the low bits are added. */
+  BFD_RELOC_HI16_S,
+
+/* Low 16 bits. */
+  BFD_RELOC_LO16,
+
+/* Like BFD_RELOC_HI16_S, but PC relative. */
+  BFD_RELOC_PCREL_HI16_S,
+
+/* Like BFD_RELOC_LO16, but PC relative. */
+  BFD_RELOC_PCREL_LO16,
+
+/* Relocation relative to the global pointer. */
+#define BFD_RELOC_MIPS_GPREL BFD_RELOC_GPREL16
+
+/* Relocation against a MIPS literal section. */
+  BFD_RELOC_MIPS_LITERAL,
+
+/* MIPS ELF relocations. */
+  BFD_RELOC_MIPS_GOT16,
+  BFD_RELOC_MIPS_CALL16,
+#define BFD_RELOC_MIPS_GPREL32 BFD_RELOC_GPREL32
+  BFD_RELOC_MIPS_GOT_HI16,
+  BFD_RELOC_MIPS_GOT_LO16,
+  BFD_RELOC_MIPS_CALL_HI16,
+  BFD_RELOC_MIPS_CALL_LO16,
+  BFD_RELOC_MIPS_SUB,
+  BFD_RELOC_MIPS_GOT_PAGE,
+  BFD_RELOC_MIPS_GOT_OFST,
+  BFD_RELOC_MIPS_GOT_DISP,
+  BFD_RELOC_MIPS_SHIFT5,
+  BFD_RELOC_MIPS_SHIFT6,
+  BFD_RELOC_MIPS_INSERT_A,
+  BFD_RELOC_MIPS_INSERT_B,
+  BFD_RELOC_MIPS_DELETE,
+  BFD_RELOC_MIPS_HIGHEST,
+  BFD_RELOC_MIPS_HIGHER,
+  BFD_RELOC_MIPS_SCN_DISP,
+  BFD_RELOC_MIPS_REL16,
+  BFD_RELOC_MIPS_RELGOT,
+  BFD_RELOC_MIPS_JALR,
+
+
+/* i386/elf relocations */
+  BFD_RELOC_386_GOT32,
+  BFD_RELOC_386_PLT32,
+  BFD_RELOC_386_COPY,
+  BFD_RELOC_386_GLOB_DAT,
+  BFD_RELOC_386_JUMP_SLOT,
+  BFD_RELOC_386_RELATIVE,
+  BFD_RELOC_386_GOTOFF,
+  BFD_RELOC_386_GOTPC,
+
+/* x86-64/elf relocations */
+  BFD_RELOC_X86_64_GOT32,
+  BFD_RELOC_X86_64_PLT32,
+  BFD_RELOC_X86_64_COPY,
+  BFD_RELOC_X86_64_GLOB_DAT,
+  BFD_RELOC_X86_64_JUMP_SLOT,
+  BFD_RELOC_X86_64_RELATIVE,
+  BFD_RELOC_X86_64_GOTPCREL,
+  BFD_RELOC_X86_64_32S,
+
+/* ns32k relocations */
+  BFD_RELOC_NS32K_IMM_8,
+  BFD_RELOC_NS32K_IMM_16,
+  BFD_RELOC_NS32K_IMM_32,
+  BFD_RELOC_NS32K_IMM_8_PCREL,
+  BFD_RELOC_NS32K_IMM_16_PCREL,
+  BFD_RELOC_NS32K_IMM_32_PCREL,
+  BFD_RELOC_NS32K_DISP_8,
+  BFD_RELOC_NS32K_DISP_16,
+  BFD_RELOC_NS32K_DISP_32,
+  BFD_RELOC_NS32K_DISP_8_PCREL,
+  BFD_RELOC_NS32K_DISP_16_PCREL,
+  BFD_RELOC_NS32K_DISP_32_PCREL,
+
+/* PDP11 relocations */
+  BFD_RELOC_PDP11_DISP_8_PCREL,
+  BFD_RELOC_PDP11_DISP_6_PCREL,
+
+/* Picojava relocs.  Not all of these appear in object files. */
+  BFD_RELOC_PJ_CODE_HI16,
+  BFD_RELOC_PJ_CODE_LO16,
+  BFD_RELOC_PJ_CODE_DIR16,
+  BFD_RELOC_PJ_CODE_DIR32,
+  BFD_RELOC_PJ_CODE_REL16,
+  BFD_RELOC_PJ_CODE_REL32,
+
+/* Power(rs6000) and PowerPC relocations. */
+  BFD_RELOC_PPC_B26,
+  BFD_RELOC_PPC_BA26,
+  BFD_RELOC_PPC_TOC16,
+  BFD_RELOC_PPC_B16,
+  BFD_RELOC_PPC_B16_BRTAKEN,
+  BFD_RELOC_PPC_B16_BRNTAKEN,
+  BFD_RELOC_PPC_BA16,
+  BFD_RELOC_PPC_BA16_BRTAKEN,
+  BFD_RELOC_PPC_BA16_BRNTAKEN,
+  BFD_RELOC_PPC_COPY,
+  BFD_RELOC_PPC_GLOB_DAT,
+  BFD_RELOC_PPC_JMP_SLOT,
+  BFD_RELOC_PPC_RELATIVE,
+  BFD_RELOC_PPC_LOCAL24PC,
+  BFD_RELOC_PPC_EMB_NADDR32,
+  BFD_RELOC_PPC_EMB_NADDR16,
+  BFD_RELOC_PPC_EMB_NADDR16_LO,
+  BFD_RELOC_PPC_EMB_NADDR16_HI,
+  BFD_RELOC_PPC_EMB_NADDR16_HA,
+  BFD_RELOC_PPC_EMB_SDAI16,
+  BFD_RELOC_PPC_EMB_SDA2I16,
+  BFD_RELOC_PPC_EMB_SDA2REL,
+  BFD_RELOC_PPC_EMB_SDA21,
+  BFD_RELOC_PPC_EMB_MRKREF,
+  BFD_RELOC_PPC_EMB_RELSEC16,
+  BFD_RELOC_PPC_EMB_RELST_LO,
+  BFD_RELOC_PPC_EMB_RELST_HI,
+  BFD_RELOC_PPC_EMB_RELST_HA,
+  BFD_RELOC_PPC_EMB_BIT_FLD,
+  BFD_RELOC_PPC_EMB_RELSDA,
+  BFD_RELOC_PPC64_HIGHER,
+  BFD_RELOC_PPC64_HIGHER_S,
+  BFD_RELOC_PPC64_HIGHEST,
+  BFD_RELOC_PPC64_HIGHEST_S,
+  BFD_RELOC_PPC64_TOC16_LO,
+  BFD_RELOC_PPC64_TOC16_HI,
+  BFD_RELOC_PPC64_TOC16_HA,
+  BFD_RELOC_PPC64_TOC,
+  BFD_RELOC_PPC64_PLTGOT16,
+  BFD_RELOC_PPC64_PLTGOT16_LO,
+  BFD_RELOC_PPC64_PLTGOT16_HI,
+  BFD_RELOC_PPC64_PLTGOT16_HA,
+  BFD_RELOC_PPC64_ADDR16_DS,
+  BFD_RELOC_PPC64_ADDR16_LO_DS,
+  BFD_RELOC_PPC64_GOT16_DS,
+  BFD_RELOC_PPC64_GOT16_LO_DS,
+  BFD_RELOC_PPC64_PLT16_LO_DS,
+  BFD_RELOC_PPC64_SECTOFF_DS,
+  BFD_RELOC_PPC64_SECTOFF_LO_DS,
+  BFD_RELOC_PPC64_TOC16_DS,
+  BFD_RELOC_PPC64_TOC16_LO_DS,
+  BFD_RELOC_PPC64_PLTGOT16_DS,
+  BFD_RELOC_PPC64_PLTGOT16_LO_DS,
+
+/* IBM 370/390 relocations */
+  BFD_RELOC_I370_D12,
+
+/* The type of reloc used to build a contructor table - at the moment
+probably a 32 bit wide absolute relocation, but the target can choose.
+It generally does map to one of the other relocation types. */
+  BFD_RELOC_CTOR,
+
+/* ARM 26 bit pc-relative branch.  The lowest two bits must be zero and are
+not stored in the instruction. */
+  BFD_RELOC_ARM_PCREL_BRANCH,
+
+/* ARM 26 bit pc-relative branch.  The lowest bit must be zero and is
+not stored in the instruction.  The 2nd lowest bit comes from a 1 bit
+field in the instruction. */
+  BFD_RELOC_ARM_PCREL_BLX,
+
+/* Thumb 22 bit pc-relative branch.  The lowest bit must be zero and is
+not stored in the instruction.  The 2nd lowest bit comes from a 1 bit
+field in the instruction. */
+  BFD_RELOC_THUMB_PCREL_BLX,
+
+/* These relocs are only used within the ARM assembler.  They are not
+(at present) written to any object files. */
+  BFD_RELOC_ARM_IMMEDIATE,
+  BFD_RELOC_ARM_ADRL_IMMEDIATE,
+  BFD_RELOC_ARM_OFFSET_IMM,
+  BFD_RELOC_ARM_SHIFT_IMM,
+  BFD_RELOC_ARM_SWI,
+  BFD_RELOC_ARM_MULTI,
+  BFD_RELOC_ARM_CP_OFF_IMM,
+  BFD_RELOC_ARM_ADR_IMM,
+  BFD_RELOC_ARM_LDR_IMM,
+  BFD_RELOC_ARM_LITERAL,
+  BFD_RELOC_ARM_IN_POOL,
+  BFD_RELOC_ARM_OFFSET_IMM8,
+  BFD_RELOC_ARM_HWLITERAL,
+  BFD_RELOC_ARM_THUMB_ADD,
+  BFD_RELOC_ARM_THUMB_IMM,
+  BFD_RELOC_ARM_THUMB_SHIFT,
+  BFD_RELOC_ARM_THUMB_OFFSET,
+  BFD_RELOC_ARM_GOT12,
+  BFD_RELOC_ARM_GOT32,
+  BFD_RELOC_ARM_JUMP_SLOT,
+  BFD_RELOC_ARM_COPY,
+  BFD_RELOC_ARM_GLOB_DAT,
+  BFD_RELOC_ARM_PLT32,
+  BFD_RELOC_ARM_RELATIVE,
+  BFD_RELOC_ARM_GOTOFF,
+  BFD_RELOC_ARM_GOTPC,
+
+/* Hitachi SH relocs.  Not all of these appear in object files. */
+  BFD_RELOC_SH_PCDISP8BY2,
+  BFD_RELOC_SH_PCDISP12BY2,
+  BFD_RELOC_SH_IMM4,
+  BFD_RELOC_SH_IMM4BY2,
+  BFD_RELOC_SH_IMM4BY4,
+  BFD_RELOC_SH_IMM8,
+  BFD_RELOC_SH_IMM8BY2,
+  BFD_RELOC_SH_IMM8BY4,
+  BFD_RELOC_SH_PCRELIMM8BY2,
+  BFD_RELOC_SH_PCRELIMM8BY4,
+  BFD_RELOC_SH_SWITCH16,
+  BFD_RELOC_SH_SWITCH32,
+  BFD_RELOC_SH_USES,
+  BFD_RELOC_SH_COUNT,
+  BFD_RELOC_SH_ALIGN,
+  BFD_RELOC_SH_CODE,
+  BFD_RELOC_SH_DATA,
+  BFD_RELOC_SH_LABEL,
+  BFD_RELOC_SH_LOOP_START,
+  BFD_RELOC_SH_LOOP_END,
+  BFD_RELOC_SH_COPY,
+  BFD_RELOC_SH_GLOB_DAT,
+  BFD_RELOC_SH_JMP_SLOT,
+  BFD_RELOC_SH_RELATIVE,
+  BFD_RELOC_SH_GOTPC,
+
+/* Thumb 23-, 12- and 9-bit pc-relative branches.  The lowest bit must
+be zero and is not stored in the instruction. */
+  BFD_RELOC_THUMB_PCREL_BRANCH9,
+  BFD_RELOC_THUMB_PCREL_BRANCH12,
+  BFD_RELOC_THUMB_PCREL_BRANCH23,
+
+/* ARC Cores relocs.
+ARC 22 bit pc-relative branch.  The lowest two bits must be zero and are
+not stored in the instruction.  The high 20 bits are installed in bits 26
+through 7 of the instruction. */
+  BFD_RELOC_ARC_B22_PCREL,
+
+/* ARC 26 bit absolute branch.  The lowest two bits must be zero and are not
+stored in the instruction.  The high 24 bits are installed in bits 23
+through 0. */
+  BFD_RELOC_ARC_B26,
+
+/* Mitsubishi D10V relocs.
+This is a 10-bit reloc with the right 2 bits
+assumed to be 0. */
+  BFD_RELOC_D10V_10_PCREL_R,
+
+/* Mitsubishi D10V relocs.
+This is a 10-bit reloc with the right 2 bits
+assumed to be 0.  This is the same as the previous reloc
+except it is in the left container, i.e.,
+shifted left 15 bits. */
+  BFD_RELOC_D10V_10_PCREL_L,
+
+/* This is an 18-bit reloc with the right 2 bits
+assumed to be 0. */
+  BFD_RELOC_D10V_18,
+
+/* This is an 18-bit reloc with the right 2 bits
+assumed to be 0. */
+  BFD_RELOC_D10V_18_PCREL,
+
+/* Mitsubishi D30V relocs.
+This is a 6-bit absolute reloc. */
+  BFD_RELOC_D30V_6,
+
+/* This is a 6-bit pc-relative reloc with
+the right 3 bits assumed to be 0. */
+  BFD_RELOC_D30V_9_PCREL,
+
+/* This is a 6-bit pc-relative reloc with
+the right 3 bits assumed to be 0. Same
+as the previous reloc but on the right side
+of the container. */
+  BFD_RELOC_D30V_9_PCREL_R,
+
+/* This is a 12-bit absolute reloc with the
+right 3 bitsassumed to be 0. */
+  BFD_RELOC_D30V_15,
+
+/* This is a 12-bit pc-relative reloc with
+the right 3 bits assumed to be 0. */
+  BFD_RELOC_D30V_15_PCREL,
+
+/* This is a 12-bit pc-relative reloc with
+the right 3 bits assumed to be 0. Same
+as the previous reloc but on the right side
+of the container. */
+  BFD_RELOC_D30V_15_PCREL_R,
+
+/* This is an 18-bit absolute reloc with
+the right 3 bits assumed to be 0. */
+  BFD_RELOC_D30V_21,
+
+/* This is an 18-bit pc-relative reloc with
+the right 3 bits assumed to be 0. */
+  BFD_RELOC_D30V_21_PCREL,
+
+/* This is an 18-bit pc-relative reloc with
+the right 3 bits assumed to be 0. Same
+as the previous reloc but on the right side
+of the container. */
+  BFD_RELOC_D30V_21_PCREL_R,
+
+/* This is a 32-bit absolute reloc. */
+  BFD_RELOC_D30V_32,
+
+/* This is a 32-bit pc-relative reloc. */
+  BFD_RELOC_D30V_32_PCREL,
+
+/* Mitsubishi M32R relocs.
+This is a 24 bit absolute address. */
+  BFD_RELOC_M32R_24,
+
+/* This is a 10-bit pc-relative reloc with the right 2 bits assumed to be 0. */
+  BFD_RELOC_M32R_10_PCREL,
+
+/* This is an 18-bit reloc with the right 2 bits assumed to be 0. */
+  BFD_RELOC_M32R_18_PCREL,
+
+/* This is a 26-bit reloc with the right 2 bits assumed to be 0. */
+  BFD_RELOC_M32R_26_PCREL,
+
+/* This is a 16-bit reloc containing the high 16 bits of an address
+used when the lower 16 bits are treated as unsigned. */
+  BFD_RELOC_M32R_HI16_ULO,
+
+/* This is a 16-bit reloc containing the high 16 bits of an address
+used when the lower 16 bits are treated as signed. */
+  BFD_RELOC_M32R_HI16_SLO,
+
+/* This is a 16-bit reloc containing the lower 16 bits of an address. */
+  BFD_RELOC_M32R_LO16,
+
+/* This is a 16-bit reloc containing the small data area offset for use in
+add3, load, and store instructions. */
+  BFD_RELOC_M32R_SDA16,
+
+/* This is a 9-bit reloc */
+  BFD_RELOC_V850_9_PCREL,
+
+/* This is a 22-bit reloc */
+  BFD_RELOC_V850_22_PCREL,
+
+/* This is a 16 bit offset from the short data area pointer. */
+  BFD_RELOC_V850_SDA_16_16_OFFSET,
+
+/* This is a 16 bit offset (of which only 15 bits are used) from the
+short data area pointer. */
+  BFD_RELOC_V850_SDA_15_16_OFFSET,
+
+/* This is a 16 bit offset from the zero data area pointer. */
+  BFD_RELOC_V850_ZDA_16_16_OFFSET,
+
+/* This is a 16 bit offset (of which only 15 bits are used) from the
+zero data area pointer. */
+  BFD_RELOC_V850_ZDA_15_16_OFFSET,
+
+/* This is an 8 bit offset (of which only 6 bits are used) from the
+tiny data area pointer. */
+  BFD_RELOC_V850_TDA_6_8_OFFSET,
+
+/* This is an 8bit offset (of which only 7 bits are used) from the tiny
+data area pointer. */
+  BFD_RELOC_V850_TDA_7_8_OFFSET,
+
+/* This is a 7 bit offset from the tiny data area pointer. */
+  BFD_RELOC_V850_TDA_7_7_OFFSET,
+
+/* This is a 16 bit offset from the tiny data area pointer. */
+  BFD_RELOC_V850_TDA_16_16_OFFSET,
+
+/* This is a 5 bit offset (of which only 4 bits are used) from the tiny
+data area pointer. */
+  BFD_RELOC_V850_TDA_4_5_OFFSET,
+
+/* This is a 4 bit offset from the tiny data area pointer. */
+  BFD_RELOC_V850_TDA_4_4_OFFSET,
+
+/* This is a 16 bit offset from the short data area pointer, with the
+bits placed non-contigously in the instruction. */
+  BFD_RELOC_V850_SDA_16_16_SPLIT_OFFSET,
+
+/* This is a 16 bit offset from the zero data area pointer, with the
+bits placed non-contigously in the instruction. */
+  BFD_RELOC_V850_ZDA_16_16_SPLIT_OFFSET,
+
+/* This is a 6 bit offset from the call table base pointer. */
+  BFD_RELOC_V850_CALLT_6_7_OFFSET,
+
+/* This is a 16 bit offset from the call table base pointer. */
+  BFD_RELOC_V850_CALLT_16_16_OFFSET,
+
+
+/* This is a 32bit pcrel reloc for the mn10300, offset by two bytes in the
+instruction. */
+  BFD_RELOC_MN10300_32_PCREL,
+
+/* This is a 16bit pcrel reloc for the mn10300, offset by two bytes in the
+instruction. */
+  BFD_RELOC_MN10300_16_PCREL,
+
+/* This is a 8bit DP reloc for the tms320c30, where the most
+significant 8 bits of a 24 bit word are placed into the least
+significant 8 bits of the opcode. */
+  BFD_RELOC_TIC30_LDP,
+
+/* This is a 7bit reloc for the tms320c54x, where the least
+significant 7 bits of a 16 bit word are placed into the least
+significant 7 bits of the opcode. */
+  BFD_RELOC_TIC54X_PARTLS7,
+
+/* This is a 9bit DP reloc for the tms320c54x, where the most
+significant 9 bits of a 16 bit word are placed into the least
+significant 9 bits of the opcode. */
+  BFD_RELOC_TIC54X_PARTMS9,
+
+/* This is an extended address 23-bit reloc for the tms320c54x. */
+  BFD_RELOC_TIC54X_23,
+
+/* This is a 16-bit reloc for the tms320c54x, where the least
+significant 16 bits of a 23-bit extended address are placed into
+the opcode. */
+  BFD_RELOC_TIC54X_16_OF_23,
+
+/* This is a reloc for the tms320c54x, where the most
+significant 7 bits of a 23-bit extended address are placed into
+the opcode. */
+  BFD_RELOC_TIC54X_MS7_OF_23,
+
+/* This is a 48 bit reloc for the FR30 that stores 32 bits. */
+  BFD_RELOC_FR30_48,
+
+/* This is a 32 bit reloc for the FR30 that stores 20 bits split up into
+two sections. */
+  BFD_RELOC_FR30_20,
+
+/* This is a 16 bit reloc for the FR30 that stores a 6 bit word offset in
+4 bits. */
+  BFD_RELOC_FR30_6_IN_4,
+
+/* This is a 16 bit reloc for the FR30 that stores an 8 bit byte offset
+into 8 bits. */
+  BFD_RELOC_FR30_8_IN_8,
+
+/* This is a 16 bit reloc for the FR30 that stores a 9 bit short offset
+into 8 bits. */
+  BFD_RELOC_FR30_9_IN_8,
+
+/* This is a 16 bit reloc for the FR30 that stores a 10 bit word offset
+into 8 bits. */
+  BFD_RELOC_FR30_10_IN_8,
+
+/* This is a 16 bit reloc for the FR30 that stores a 9 bit pc relative
+short offset into 8 bits. */
+  BFD_RELOC_FR30_9_PCREL,
+
+/* This is a 16 bit reloc for the FR30 that stores a 12 bit pc relative
+short offset into 11 bits. */
+  BFD_RELOC_FR30_12_PCREL,
+
+/* Motorola Mcore relocations. */
+  BFD_RELOC_MCORE_PCREL_IMM8BY4,
+  BFD_RELOC_MCORE_PCREL_IMM11BY2,
+  BFD_RELOC_MCORE_PCREL_IMM4BY2,
+  BFD_RELOC_MCORE_PCREL_32,
+  BFD_RELOC_MCORE_PCREL_JSR_IMM11BY2,
+  BFD_RELOC_MCORE_RVA,
+
+/* This is a 16 bit reloc for the AVR that stores 8 bit pc relative
+short offset into 7 bits. */
+  BFD_RELOC_AVR_7_PCREL,
+
+/* This is a 16 bit reloc for the AVR that stores 13 bit pc relative
+short offset into 12 bits. */
+  BFD_RELOC_AVR_13_PCREL,
+
+/* This is a 16 bit reloc for the AVR that stores 17 bit value (usually
+program memory address) into 16 bits. */
+  BFD_RELOC_AVR_16_PM,
+
+/* This is a 16 bit reloc for the AVR that stores 8 bit value (usually
+data memory address) into 8 bit immediate value of LDI insn. */
+  BFD_RELOC_AVR_LO8_LDI,
+
+/* This is a 16 bit reloc for the AVR that stores 8 bit value (high 8 bit
+of data memory address) into 8 bit immediate value of LDI insn. */
+  BFD_RELOC_AVR_HI8_LDI,
+
+/* This is a 16 bit reloc for the AVR that stores 8 bit value (most high 8 bit
+of program memory address) into 8 bit immediate value of LDI insn. */
+  BFD_RELOC_AVR_HH8_LDI,
+
+/* This is a 16 bit reloc for the AVR that stores negated 8 bit value
+(usually data memory address) into 8 bit immediate value of SUBI insn. */
+  BFD_RELOC_AVR_LO8_LDI_NEG,
+
+/* This is a 16 bit reloc for the AVR that stores negated 8 bit value
+(high 8 bit of data memory address) into 8 bit immediate value of
+SUBI insn. */
+  BFD_RELOC_AVR_HI8_LDI_NEG,
+
+/* This is a 16 bit reloc for the AVR that stores negated 8 bit value
+(most high 8 bit of program memory address) into 8 bit immediate value
+of LDI or SUBI insn. */
+  BFD_RELOC_AVR_HH8_LDI_NEG,
+
+/* This is a 16 bit reloc for the AVR that stores 8 bit value (usually
+command address) into 8 bit immediate value of LDI insn. */
+  BFD_RELOC_AVR_LO8_LDI_PM,
+
+/* This is a 16 bit reloc for the AVR that stores 8 bit value (high 8 bit
+of command address) into 8 bit immediate value of LDI insn. */
+  BFD_RELOC_AVR_HI8_LDI_PM,
+
+/* This is a 16 bit reloc for the AVR that stores 8 bit value (most high 8 bit
+of command address) into 8 bit immediate value of LDI insn. */
+  BFD_RELOC_AVR_HH8_LDI_PM,
+
+/* This is a 16 bit reloc for the AVR that stores negated 8 bit value
+(usually command address) into 8 bit immediate value of SUBI insn. */
+  BFD_RELOC_AVR_LO8_LDI_PM_NEG,
+
+/* This is a 16 bit reloc for the AVR that stores negated 8 bit value
+(high 8 bit of 16 bit command address) into 8 bit immediate value
+of SUBI insn. */
+  BFD_RELOC_AVR_HI8_LDI_PM_NEG,
+
+/* This is a 16 bit reloc for the AVR that stores negated 8 bit value
+(high 6 bit of 22 bit command address) into 8 bit immediate
+value of SUBI insn. */
+  BFD_RELOC_AVR_HH8_LDI_PM_NEG,
+
+/* This is a 32 bit reloc for the AVR that stores 23 bit value
+into 22 bits. */
+  BFD_RELOC_AVR_CALL,
+
+/* Direct 12 bit. */
+  BFD_RELOC_390_12,
+
+/* 12 bit GOT offset. */
+  BFD_RELOC_390_GOT12,
+
+/* 32 bit PC relative PLT address. */
+  BFD_RELOC_390_PLT32,
+
+/* Copy symbol at runtime. */
+  BFD_RELOC_390_COPY,
+
+/* Create GOT entry. */
+  BFD_RELOC_390_GLOB_DAT,
+
+/* Create PLT entry. */
+  BFD_RELOC_390_JMP_SLOT,
+
+/* Adjust by program base. */
+  BFD_RELOC_390_RELATIVE,
+
+/* 32 bit PC relative offset to GOT. */
+  BFD_RELOC_390_GOTPC,
+
+/* 16 bit GOT offset. */
+  BFD_RELOC_390_GOT16,
+
+/* PC relative 16 bit shifted by 1. */
+  BFD_RELOC_390_PC16DBL,
+
+/* 16 bit PC rel. PLT shifted by 1. */
+  BFD_RELOC_390_PLT16DBL,
+
+/* PC relative 32 bit shifted by 1. */
+  BFD_RELOC_390_PC32DBL,
+
+/* 32 bit PC rel. PLT shifted by 1. */
+  BFD_RELOC_390_PLT32DBL,
+
+/* 32 bit PC rel. GOT shifted by 1. */
+  BFD_RELOC_390_GOTPCDBL,
+
+/* 64 bit GOT offset. */
+  BFD_RELOC_390_GOT64,
+
+/* 64 bit PC relative PLT address. */
+  BFD_RELOC_390_PLT64,
+
+/* 32 bit rel. offset to GOT entry. */
+  BFD_RELOC_390_GOTENT,
+
+/* These two relocations are used by the linker to determine which of
+the entries in a C++ virtual function table are actually used.  When
+the --gc-sections option is given, the linker will zero out the entries
+that are not used, so that the code for those functions need not be
+included in the output.
+
+VTABLE_INHERIT is a zero-space relocation used to describe to the
+linker the inheritence tree of a C++ virtual function table.  The
+relocation's symbol should be the parent class' vtable, and the
+relocation should be located at the child vtable.
+
+VTABLE_ENTRY is a zero-space relocation that describes the use of a
+virtual function table entry.  The reloc's symbol should refer to the
+table of the class mentioned in the code.  Off of that base, an offset
+describes the entry that is being used.  For Rela hosts, this offset
+is stored in the reloc's addend.  For Rel hosts, we are forced to put
+this offset in the reloc's section offset. */
+  BFD_RELOC_VTABLE_INHERIT,
+  BFD_RELOC_VTABLE_ENTRY,
+
+/* Intel IA64 Relocations. */
+  BFD_RELOC_IA64_IMM14,
+  BFD_RELOC_IA64_IMM22,
+  BFD_RELOC_IA64_IMM64,
+  BFD_RELOC_IA64_DIR32MSB,
+  BFD_RELOC_IA64_DIR32LSB,
+  BFD_RELOC_IA64_DIR64MSB,
+  BFD_RELOC_IA64_DIR64LSB,
+  BFD_RELOC_IA64_GPREL22,
+  BFD_RELOC_IA64_GPREL64I,
+  BFD_RELOC_IA64_GPREL32MSB,
+  BFD_RELOC_IA64_GPREL32LSB,
+  BFD_RELOC_IA64_GPREL64MSB,
+  BFD_RELOC_IA64_GPREL64LSB,
+  BFD_RELOC_IA64_LTOFF22,
+  BFD_RELOC_IA64_LTOFF64I,
+  BFD_RELOC_IA64_PLTOFF22,
+  BFD_RELOC_IA64_PLTOFF64I,
+  BFD_RELOC_IA64_PLTOFF64MSB,
+  BFD_RELOC_IA64_PLTOFF64LSB,
+  BFD_RELOC_IA64_FPTR64I,
+  BFD_RELOC_IA64_FPTR32MSB,
+  BFD_RELOC_IA64_FPTR32LSB,
+  BFD_RELOC_IA64_FPTR64MSB,
+  BFD_RELOC_IA64_FPTR64LSB,
+  BFD_RELOC_IA64_PCREL21B,
+  BFD_RELOC_IA64_PCREL21BI,
+  BFD_RELOC_IA64_PCREL21M,
+  BFD_RELOC_IA64_PCREL21F,
+  BFD_RELOC_IA64_PCREL22,
+  BFD_RELOC_IA64_PCREL60B,
+  BFD_RELOC_IA64_PCREL64I,
+  BFD_RELOC_IA64_PCREL32MSB,
+  BFD_RELOC_IA64_PCREL32LSB,
+  BFD_RELOC_IA64_PCREL64MSB,
+  BFD_RELOC_IA64_PCREL64LSB,
+  BFD_RELOC_IA64_LTOFF_FPTR22,
+  BFD_RELOC_IA64_LTOFF_FPTR64I,
+  BFD_RELOC_IA64_LTOFF_FPTR32MSB,
+  BFD_RELOC_IA64_LTOFF_FPTR32LSB,
+  BFD_RELOC_IA64_LTOFF_FPTR64MSB,
+  BFD_RELOC_IA64_LTOFF_FPTR64LSB,
+  BFD_RELOC_IA64_SEGREL32MSB,
+  BFD_RELOC_IA64_SEGREL32LSB,
+  BFD_RELOC_IA64_SEGREL64MSB,
+  BFD_RELOC_IA64_SEGREL64LSB,
+  BFD_RELOC_IA64_SECREL32MSB,
+  BFD_RELOC_IA64_SECREL32LSB,
+  BFD_RELOC_IA64_SECREL64MSB,
+  BFD_RELOC_IA64_SECREL64LSB,
+  BFD_RELOC_IA64_REL32MSB,
+  BFD_RELOC_IA64_REL32LSB,
+  BFD_RELOC_IA64_REL64MSB,
+  BFD_RELOC_IA64_REL64LSB,
+  BFD_RELOC_IA64_LTV32MSB,
+  BFD_RELOC_IA64_LTV32LSB,
+  BFD_RELOC_IA64_LTV64MSB,
+  BFD_RELOC_IA64_LTV64LSB,
+  BFD_RELOC_IA64_IPLTMSB,
+  BFD_RELOC_IA64_IPLTLSB,
+  BFD_RELOC_IA64_COPY,
+  BFD_RELOC_IA64_TPREL22,
+  BFD_RELOC_IA64_TPREL64MSB,
+  BFD_RELOC_IA64_TPREL64LSB,
+  BFD_RELOC_IA64_LTOFF_TP22,
+  BFD_RELOC_IA64_LTOFF22X,
+  BFD_RELOC_IA64_LDXMOV,
+
+/* Motorola 68HC11 reloc.
+This is the 8 bits high part of an absolute address. */
+  BFD_RELOC_M68HC11_HI8,
+
+/* Motorola 68HC11 reloc.
+This is the 8 bits low part of an absolute address. */
+  BFD_RELOC_M68HC11_LO8,
+
+/* Motorola 68HC11 reloc.
+This is the 3 bits of a value. */
+  BFD_RELOC_M68HC11_3B,
+
+/* These relocs are only used within the CRIS assembler.  They are not
+(at present) written to any object files. */
+  BFD_RELOC_CRIS_BDISP8,
+  BFD_RELOC_CRIS_UNSIGNED_5,
+  BFD_RELOC_CRIS_SIGNED_6,
+  BFD_RELOC_CRIS_UNSIGNED_6,
+  BFD_RELOC_CRIS_UNSIGNED_4,
+
+/* Relocs used in ELF shared libraries for CRIS. */
+  BFD_RELOC_CRIS_COPY,
+  BFD_RELOC_CRIS_GLOB_DAT,
+  BFD_RELOC_CRIS_JUMP_SLOT,
+  BFD_RELOC_CRIS_RELATIVE,
+
+/* 32-bit offset to symbol-entry within GOT. */
+  BFD_RELOC_CRIS_32_GOT,
+
+/* 16-bit offset to symbol-entry within GOT. */
+  BFD_RELOC_CRIS_16_GOT,
+
+/* 32-bit offset to symbol-entry within GOT, with PLT handling. */
+  BFD_RELOC_CRIS_32_GOTPLT,
+
+/* 16-bit offset to symbol-entry within GOT, with PLT handling. */
+  BFD_RELOC_CRIS_16_GOTPLT,
+
+/* 32-bit offset to symbol, relative to GOT. */
+  BFD_RELOC_CRIS_32_GOTREL,
+
+/* 32-bit offset to symbol with PLT entry, relative to GOT. */
+  BFD_RELOC_CRIS_32_PLT_GOTREL,
+
+/* 32-bit offset to symbol with PLT entry, relative to this relocation. */
+  BFD_RELOC_CRIS_32_PLT_PCREL,
+
+/* Intel i860 Relocations. */
+  BFD_RELOC_860_COPY,
+  BFD_RELOC_860_GLOB_DAT,
+  BFD_RELOC_860_JUMP_SLOT,
+  BFD_RELOC_860_RELATIVE,
+  BFD_RELOC_860_PC26,
+  BFD_RELOC_860_PLT26,
+  BFD_RELOC_860_PC16,
+  BFD_RELOC_860_LOW0,
+  BFD_RELOC_860_SPLIT0,
+  BFD_RELOC_860_LOW1,
+  BFD_RELOC_860_SPLIT1,
+  BFD_RELOC_860_LOW2,
+  BFD_RELOC_860_SPLIT2,
+  BFD_RELOC_860_LOW3,
+  BFD_RELOC_860_LOGOT0,
+  BFD_RELOC_860_SPGOT0,
+  BFD_RELOC_860_LOGOT1,
+  BFD_RELOC_860_SPGOT1,
+  BFD_RELOC_860_LOGOTOFF0,
+  BFD_RELOC_860_SPGOTOFF0,
+  BFD_RELOC_860_LOGOTOFF1,
+  BFD_RELOC_860_SPGOTOFF1,
+  BFD_RELOC_860_LOGOTOFF2,
+  BFD_RELOC_860_LOGOTOFF3,
+  BFD_RELOC_860_LOPC,
+  BFD_RELOC_860_HIGHADJ,
+  BFD_RELOC_860_HAGOT,
+  BFD_RELOC_860_HAGOTOFF,
+  BFD_RELOC_860_HAPC,
+  BFD_RELOC_860_HIGH,
+  BFD_RELOC_860_HIGOT,
+  BFD_RELOC_860_HIGOTOFF,
+
+/* OpenRISC Relocations. */
+  BFD_RELOC_OPENRISC_ABS_26,
+  BFD_RELOC_OPENRISC_REL_26,
+
+/* H8 elf Relocations. */
+  BFD_RELOC_H8_DIR16A8,
+  BFD_RELOC_H8_DIR16R8,
+  BFD_RELOC_H8_DIR24A8,
+  BFD_RELOC_H8_DIR24R8,
+  BFD_RELOC_H8_DIR32A16,
+  BFD_RELOC_UNUSED };
+typedef enum bfd_reloc_code_real bfd_reloc_code_real_type;
+reloc_howto_type *
+bfd_reloc_type_lookup PARAMS ((bfd *abfd, bfd_reloc_code_real_type code));
+
+const char *
+bfd_get_reloc_code_name PARAMS ((bfd_reloc_code_real_type code));
+
+
+typedef struct symbol_cache_entry
+{
+       /* A pointer to the BFD which owns the symbol. This information
+          is necessary so that a back end can work out what additional
+          information (invisible to the application writer) is carried
+          with the symbol.
+
+          This field is *almost* redundant, since you can use section->owner
+          instead, except that some symbols point to the global sections
+          bfd_{abs,com,und}_section.  This could be fixed by making
+          these globals be per-bfd (or per-target-flavor).  FIXME. */
+
+  struct _bfd *the_bfd; /* Use bfd_asymbol_bfd(sym) to access this field. */
+
+       /* The text of the symbol. The name is left alone, and not copied; the
+          application may not alter it. */
+  const char *name;
+
+       /* The value of the symbol.  This really should be a union of a
+          numeric value with a pointer, since some flags indicate that
+          a pointer to another symbol is stored here.  */
+  symvalue value;
+
+       /* Attributes of a symbol: */
+
+#define BSF_NO_FLAGS    0x00
+
+       /* The symbol has local scope; <<static>> in <<C>>. The value
+          is the offset into the section of the data. */
+#define BSF_LOCAL      0x01
+
+       /* The symbol has global scope; initialized data in <<C>>. The
+          value is the offset into the section of the data. */
+#define BSF_GLOBAL     0x02
+
+       /* The symbol has global scope and is exported. The value is
+          the offset into the section of the data. */
+#define BSF_EXPORT     BSF_GLOBAL /* no real difference */
+
+       /* A normal C symbol would be one of:
+          <<BSF_LOCAL>>, <<BSF_FORT_COMM>>,  <<BSF_UNDEFINED>> or
+          <<BSF_GLOBAL>> */
+
+       /* The symbol is a debugging record. The value has an arbitary
+          meaning, unless BSF_DEBUGGING_RELOC is also set.  */
+#define BSF_DEBUGGING  0x08
+
+       /* The symbol denotes a function entry point.  Used in ELF,
+          perhaps others someday.  */
+#define BSF_FUNCTION    0x10
+
+       /* Used by the linker. */
+#define BSF_KEEP        0x20
+#define BSF_KEEP_G      0x40
+
+       /* A weak global symbol, overridable without warnings by
+          a regular global symbol of the same name.  */
+#define BSF_WEAK        0x80
+
+       /* This symbol was created to point to a section, e.g. ELF's
+          STT_SECTION symbols.  */
+#define BSF_SECTION_SYM 0x100
+
+       /* The symbol used to be a common symbol, but now it is
+          allocated. */
+#define BSF_OLD_COMMON  0x200
+
+       /* The default value for common data. */
+#define BFD_FORT_COMM_DEFAULT_VALUE 0
+
+       /* In some files the type of a symbol sometimes alters its
+          location in an output file - ie in coff a <<ISFCN>> symbol
+          which is also <<C_EXT>> symbol appears where it was
+          declared and not at the end of a section.  This bit is set
+          by the target BFD part to convey this information. */
+
+#define BSF_NOT_AT_END    0x400
+
+       /* Signal that the symbol is the label of constructor section. */
+#define BSF_CONSTRUCTOR   0x800
+
+       /* Signal that the symbol is a warning symbol.  The name is a
+          warning.  The name of the next symbol is the one to warn about;
+          if a reference is made to a symbol with the same name as the next
+          symbol, a warning is issued by the linker. */
+#define BSF_WARNING       0x1000
+
+       /* Signal that the symbol is indirect.  This symbol is an indirect
+          pointer to the symbol with the same name as the next symbol. */
+#define BSF_INDIRECT      0x2000
+
+       /* BSF_FILE marks symbols that contain a file name.  This is used
+          for ELF STT_FILE symbols.  */
+#define BSF_FILE          0x4000
+
+       /* Symbol is from dynamic linking information.  */
+#define BSF_DYNAMIC       0x8000
+
+       /* The symbol denotes a data object.  Used in ELF, and perhaps
+          others someday.  */
+#define BSF_OBJECT        0x10000
+
+       /* This symbol is a debugging symbol.  The value is the offset
+          into the section of the data.  BSF_DEBUGGING should be set
+          as well.  */
+#define BSF_DEBUGGING_RELOC 0x20000
+
+  flagword flags;
+
+       /* A pointer to the section to which this symbol is
+          relative.  This will always be non NULL, there are special
+          sections for undefined and absolute symbols.  */
+  struct sec *section;
+
+       /* Back end special data.  */
+  union
+    {
+      PTR p;
+      bfd_vma i;
+    } udata;
+
+} asymbol;
+#define bfd_get_symtab_upper_bound(abfd) \
+     BFD_SEND (abfd, _bfd_get_symtab_upper_bound, (abfd))
+boolean
+bfd_is_local_label PARAMS ((bfd *abfd, asymbol *sym));
+
+boolean
+bfd_is_local_label_name PARAMS ((bfd *abfd, const char *name));
+
+#define bfd_is_local_label_name(abfd, name) \
+     BFD_SEND (abfd, _bfd_is_local_label_name, (abfd, name))
+#define bfd_canonicalize_symtab(abfd, location) \
+     BFD_SEND (abfd, _bfd_canonicalize_symtab,\
+                  (abfd, location))
+boolean
+bfd_set_symtab PARAMS ((bfd *abfd, asymbol **location, unsigned int count));
+
+void
+bfd_print_symbol_vandf PARAMS ((bfd *abfd, PTR file, asymbol *symbol));
+
+#define bfd_make_empty_symbol(abfd) \
+     BFD_SEND (abfd, _bfd_make_empty_symbol, (abfd))
+#define bfd_make_debug_symbol(abfd,ptr,size) \
+        BFD_SEND (abfd, _bfd_make_debug_symbol, (abfd, ptr, size))
+int
+bfd_decode_symclass PARAMS ((asymbol *symbol));
+
+boolean
+bfd_is_undefined_symclass PARAMS ((int symclass));
+
+void
+bfd_symbol_info PARAMS ((asymbol *symbol, symbol_info *ret));
+
+boolean
+bfd_copy_private_symbol_data PARAMS ((bfd *ibfd, asymbol *isym, bfd *obfd, asymbol *osym));
+
+#define bfd_copy_private_symbol_data(ibfd, isymbol, obfd, osymbol) \
+     BFD_SEND (obfd, _bfd_copy_private_symbol_data, \
+               (ibfd, isymbol, obfd, osymbol))
+struct _bfd
+{
+    /* The filename the application opened the BFD with.  */
+    const char *filename;
+
+    /* A pointer to the target jump table.             */
+    const struct bfd_target *xvec;
+
+    /* To avoid dragging too many header files into every file that
+       includes `<<bfd.h>>', IOSTREAM has been declared as a "char
+       *", and MTIME as a "long".  Their correct types, to which they
+       are cast when used, are "FILE *" and "time_t".    The iostream
+       is the result of an fopen on the filename.  However, if the
+       BFD_IN_MEMORY flag is set, then iostream is actually a pointer
+       to a bfd_in_memory struct.  */
+    PTR iostream;
+
+    /* Is the file descriptor being cached?  That is, can it be closed as
+       needed, and re-opened when accessed later?  */
+
+    boolean cacheable;
+
+    /* Marks whether there was a default target specified when the
+       BFD was opened. This is used to select which matching algorithm
+       to use to choose the back end. */
+
+    boolean target_defaulted;
+
+    /* The caching routines use these to maintain a
+       least-recently-used list of BFDs */
+
+    struct _bfd *lru_prev, *lru_next;
+
+    /* When a file is closed by the caching routines, BFD retains
+       state information on the file here: */
+
+    ufile_ptr where;
+
+    /* and here: (``once'' means at least once) */
+
+    boolean opened_once;
+
+    /* Set if we have a locally maintained mtime value, rather than
+       getting it from the file each time: */
+
+    boolean mtime_set;
+
+    /* File modified time, if mtime_set is true: */
+
+    long mtime;
+
+    /* Reserved for an unimplemented file locking extension.*/
+
+    int ifd;
+
+    /* The format which belongs to the BFD. (object, core, etc.) */
+
+    bfd_format format;
+
+    /* The direction the BFD was opened with*/
+
+    enum bfd_direction {no_direction = 0,
+                        read_direction = 1,
+                        write_direction = 2,
+                        both_direction = 3} direction;
+
+    /* Format_specific flags*/
+
+    flagword flags;
+
+    /* Currently my_archive is tested before adding origin to
+       anything. I believe that this can become always an add of
+       origin, with origin set to 0 for non archive files.   */
+
+    ufile_ptr origin;
+
+    /* Remember when output has begun, to stop strange things
+       from happening. */
+    boolean output_has_begun;
+
+    /* Pointer to linked list of sections*/
+    struct sec  *sections;
+
+    /* The number of sections */
+    unsigned int section_count;
+
+    /* Stuff only useful for object files:
+       The start address. */
+    bfd_vma start_address;
+
+    /* Used for input and output*/
+    unsigned int symcount;
+
+    /* Symbol table for output BFD (with symcount entries) */
+    struct symbol_cache_entry  **outsymbols;
+
+    /* Pointer to structure which contains architecture information*/
+    const struct bfd_arch_info *arch_info;
+
+    /* Stuff only useful for archives:*/
+    PTR arelt_data;
+    struct _bfd *my_archive;     /* The containing archive BFD.  */
+    struct _bfd *next;           /* The next BFD in the archive.  */
+    struct _bfd *archive_head;   /* The first BFD in the archive.  */
+    boolean has_armap;
+
+    /* A chain of BFD structures involved in a link.  */
+    struct _bfd *link_next;
+
+    /* A field used by _bfd_generic_link_add_archive_symbols.  This will
+       be used only for archive elements.  */
+    int archive_pass;
+
+    /* Used by the back end to hold private data. */
+
+    union
+      {
+      struct aout_data_struct *aout_data;
+      struct artdata *aout_ar_data;
+      struct _oasys_data *oasys_obj_data;
+      struct _oasys_ar_data *oasys_ar_data;
+      struct coff_tdata *coff_obj_data;
+      struct pe_tdata *pe_obj_data;
+      struct xcoff_tdata *xcoff_obj_data;
+      struct ecoff_tdata *ecoff_obj_data;
+      struct ieee_data_struct *ieee_data;
+      struct ieee_ar_data_struct *ieee_ar_data;
+      struct srec_data_struct *srec_data;
+      struct ihex_data_struct *ihex_data;
+      struct tekhex_data_struct *tekhex_data;
+      struct elf_obj_tdata *elf_obj_data;
+      struct nlm_obj_tdata *nlm_obj_data;
+      struct bout_data_struct *bout_data;
+      struct sun_core_struct *sun_core_data;
+      struct sco5_core_struct *sco5_core_data;
+      struct trad_core_struct *trad_core_data;
+      struct som_data_struct *som_data;
+      struct hpux_core_struct *hpux_core_data;
+      struct hppabsd_core_struct *hppabsd_core_data;
+      struct sgi_core_struct *sgi_core_data;
+      struct lynx_core_struct *lynx_core_data;
+      struct osf_core_struct *osf_core_data;
+      struct cisco_core_struct *cisco_core_data;
+      struct versados_data_struct *versados_data;
+      struct netbsd_core_struct *netbsd_core_data;
+      PTR any;
+      } tdata;
+
+    /* Used by the application to hold private data*/
+    PTR usrdata;
+
+  /* Where all the allocated stuff under this BFD goes.  This is a
+     struct objalloc *, but we use PTR to avoid requiring the inclusion of
+     objalloc.h.  */
+    PTR memory;
+};
+
+typedef enum bfd_error
+{
+  bfd_error_no_error = 0,
+  bfd_error_system_call,
+  bfd_error_invalid_target,
+  bfd_error_wrong_format,
+  bfd_error_wrong_object_format,
+  bfd_error_invalid_operation,
+  bfd_error_no_memory,
+  bfd_error_no_symbols,
+  bfd_error_no_armap,
+  bfd_error_no_more_archived_files,
+  bfd_error_malformed_archive,
+  bfd_error_file_not_recognized,
+  bfd_error_file_ambiguously_recognized,
+  bfd_error_no_contents,
+  bfd_error_nonrepresentable_section,
+  bfd_error_no_debug_section,
+  bfd_error_bad_value,
+  bfd_error_file_truncated,
+  bfd_error_file_too_big,
+  bfd_error_invalid_error_code
+} bfd_error_type;
+
+bfd_error_type
+bfd_get_error PARAMS ((void));
+
+void
+bfd_set_error PARAMS ((bfd_error_type error_tag));
+
+const char *
+bfd_errmsg PARAMS ((bfd_error_type error_tag));
+
+void
+bfd_perror PARAMS ((const char *message));
+
+typedef void (*bfd_error_handler_type) PARAMS ((const char *, ...));
+
+bfd_error_handler_type
+bfd_set_error_handler PARAMS ((bfd_error_handler_type));
+
+void
+bfd_set_error_program_name PARAMS ((const char *));
+
+bfd_error_handler_type
+bfd_get_error_handler PARAMS ((void));
+
+const char *
+bfd_archive_filename PARAMS ((bfd *));
+
+long
+bfd_get_reloc_upper_bound PARAMS ((bfd *abfd, asection *sect));
+
+long
+bfd_canonicalize_reloc PARAMS ((bfd *abfd,
+    asection *sec,
+    arelent **loc,
+    asymbol **syms));
+
+void
+bfd_set_reloc PARAMS ((bfd *abfd, asection *sec, arelent **rel, unsigned int count)
+    
+    );
+
+boolean
+bfd_set_file_flags PARAMS ((bfd *abfd, flagword flags));
+
+int
+bfd_get_arch_size PARAMS ((bfd *abfd));
+
+int
+bfd_get_sign_extend_vma PARAMS ((bfd *abfd));
+
+boolean
+bfd_set_start_address PARAMS ((bfd *abfd, bfd_vma vma));
+
+long
+bfd_get_mtime PARAMS ((bfd *abfd));
+
+long
+bfd_get_size PARAMS ((bfd *abfd));
+
+unsigned int
+bfd_get_gp_size PARAMS ((bfd *abfd));
+
+void
+bfd_set_gp_size PARAMS ((bfd *abfd, unsigned int i));
+
+bfd_vma
+bfd_scan_vma PARAMS ((const char *string, const char **end, int base));
+
+boolean
+bfd_copy_private_bfd_data PARAMS ((bfd *ibfd, bfd *obfd));
+
+#define bfd_copy_private_bfd_data(ibfd, obfd) \
+     BFD_SEND (obfd, _bfd_copy_private_bfd_data, \
+               (ibfd, obfd))
+boolean
+bfd_merge_private_bfd_data PARAMS ((bfd *ibfd, bfd *obfd));
+
+#define bfd_merge_private_bfd_data(ibfd, obfd) \
+     BFD_SEND (obfd, _bfd_merge_private_bfd_data, \
+               (ibfd, obfd))
+boolean
+bfd_set_private_flags PARAMS ((bfd *abfd, flagword flags));
+
+#define bfd_set_private_flags(abfd, flags) \
+     BFD_SEND (abfd, _bfd_set_private_flags, \
+               (abfd, flags))
+#define bfd_sizeof_headers(abfd, reloc) \
+     BFD_SEND (abfd, _bfd_sizeof_headers, (abfd, reloc))
+
+#define bfd_find_nearest_line(abfd, sec, syms, off, file, func, line) \
+     BFD_SEND (abfd, _bfd_find_nearest_line,  (abfd, sec, syms, off, file, func, line))
+
+       /* Do these three do anything useful at all, for any back end?  */
+#define bfd_debug_info_start(abfd) \
+        BFD_SEND (abfd, _bfd_debug_info_start, (abfd))
+
+#define bfd_debug_info_end(abfd) \
+        BFD_SEND (abfd, _bfd_debug_info_end, (abfd))
+
+#define bfd_debug_info_accumulate(abfd, section) \
+        BFD_SEND (abfd, _bfd_debug_info_accumulate, (abfd, section))
+
+
+#define bfd_stat_arch_elt(abfd, stat) \
+        BFD_SEND (abfd, _bfd_stat_arch_elt,(abfd, stat))
+
+#define bfd_update_armap_timestamp(abfd) \
+        BFD_SEND (abfd, _bfd_update_armap_timestamp, (abfd))
+
+#define bfd_set_arch_mach(abfd, arch, mach)\
+        BFD_SEND ( abfd, _bfd_set_arch_mach, (abfd, arch, mach))
+
+#define bfd_relax_section(abfd, section, link_info, again) \
+       BFD_SEND (abfd, _bfd_relax_section, (abfd, section, link_info, again))
+
+#define bfd_gc_sections(abfd, link_info) \
+       BFD_SEND (abfd, _bfd_gc_sections, (abfd, link_info))
+
+#define bfd_merge_sections(abfd, link_info) \
+       BFD_SEND (abfd, _bfd_merge_sections, (abfd, link_info))
+
+#define bfd_link_hash_table_create(abfd) \
+       BFD_SEND (abfd, _bfd_link_hash_table_create, (abfd))
+
+#define bfd_link_add_symbols(abfd, info) \
+       BFD_SEND (abfd, _bfd_link_add_symbols, (abfd, info))
+
+#define bfd_final_link(abfd, info) \
+       BFD_SEND (abfd, _bfd_final_link, (abfd, info))
+
+#define bfd_free_cached_info(abfd) \
+       BFD_SEND (abfd, _bfd_free_cached_info, (abfd))
+
+#define bfd_get_dynamic_symtab_upper_bound(abfd) \
+       BFD_SEND (abfd, _bfd_get_dynamic_symtab_upper_bound, (abfd))
+
+#define bfd_print_private_bfd_data(abfd, file)\
+       BFD_SEND (abfd, _bfd_print_private_bfd_data, (abfd, file))
+
+#define bfd_canonicalize_dynamic_symtab(abfd, asymbols) \
+       BFD_SEND (abfd, _bfd_canonicalize_dynamic_symtab, (abfd, asymbols))
+
+#define bfd_get_dynamic_reloc_upper_bound(abfd) \
+       BFD_SEND (abfd, _bfd_get_dynamic_reloc_upper_bound, (abfd))
+
+#define bfd_canonicalize_dynamic_reloc(abfd, arels, asyms) \
+       BFD_SEND (abfd, _bfd_canonicalize_dynamic_reloc, (abfd, arels, asyms))
+
+extern bfd_byte *bfd_get_relocated_section_contents
+       PARAMS ((bfd *, struct bfd_link_info *,
+                 struct bfd_link_order *, bfd_byte *,
+                 boolean, asymbol **));
+
+boolean
+bfd_alt_mach_code PARAMS ((bfd *abfd, int index));
+
+symindex
+bfd_get_next_mapent PARAMS ((bfd *abfd, symindex previous, carsym **sym));
+
+boolean
+bfd_set_archive_head PARAMS ((bfd *output, bfd *new_head));
+
+bfd *
+bfd_openr_next_archived_file PARAMS ((bfd *archive, bfd *previous));
+
+const char *
+bfd_core_file_failing_command PARAMS ((bfd *abfd));
+
+int
+bfd_core_file_failing_signal PARAMS ((bfd *abfd));
+
+boolean
+core_file_matches_executable_p PARAMS ((bfd *core_bfd, bfd *exec_bfd));
+
+#define BFD_SEND(bfd, message, arglist) \
+               ((*((bfd)->xvec->message)) arglist)
+
+#ifdef DEBUG_BFD_SEND
+#undef BFD_SEND
+#define BFD_SEND(bfd, message, arglist) \
+  (((bfd) && (bfd)->xvec && (bfd)->xvec->message) ? \
+    ((*((bfd)->xvec->message)) arglist) : \
+    (bfd_assert (__FILE__,__LINE__), NULL))
+#endif
+#define BFD_SEND_FMT(bfd, message, arglist) \
+            (((bfd)->xvec->message[(int) ((bfd)->format)]) arglist)
+
+#ifdef DEBUG_BFD_SEND
+#undef BFD_SEND_FMT
+#define BFD_SEND_FMT(bfd, message, arglist) \
+  (((bfd) && (bfd)->xvec && (bfd)->xvec->message) ? \
+   (((bfd)->xvec->message[(int) ((bfd)->format)]) arglist) : \
+   (bfd_assert (__FILE__,__LINE__), NULL))
+#endif
+enum bfd_flavour {
+  bfd_target_unknown_flavour,
+  bfd_target_aout_flavour,
+  bfd_target_coff_flavour,
+  bfd_target_ecoff_flavour,
+  bfd_target_xcoff_flavour,
+  bfd_target_elf_flavour,
+  bfd_target_ieee_flavour,
+  bfd_target_nlm_flavour,
+  bfd_target_oasys_flavour,
+  bfd_target_tekhex_flavour,
+  bfd_target_srec_flavour,
+  bfd_target_ihex_flavour,
+  bfd_target_som_flavour,
+  bfd_target_os9k_flavour,
+  bfd_target_versados_flavour,
+  bfd_target_msdos_flavour,
+  bfd_target_ovax_flavour,
+  bfd_target_evax_flavour
+};
+
+enum bfd_endian { BFD_ENDIAN_BIG, BFD_ENDIAN_LITTLE, BFD_ENDIAN_UNKNOWN };
+
+/* Forward declaration.  */
+typedef struct bfd_link_info _bfd_link_info;
+
+typedef struct bfd_target
+{
+  char *name;
+  enum bfd_flavour flavour;
+  enum bfd_endian byteorder;
+  enum bfd_endian header_byteorder;
+  flagword object_flags;
+  flagword section_flags;
+  char symbol_leading_char;
+  char ar_pad_char;
+  unsigned short ar_max_namelen;
+  bfd_vma        (*bfd_getx64) PARAMS ((const bfd_byte *));
+  bfd_signed_vma (*bfd_getx_signed_64) PARAMS ((const bfd_byte *));
+  void           (*bfd_putx64) PARAMS ((bfd_vma, bfd_byte *));
+  bfd_vma        (*bfd_getx32) PARAMS ((const bfd_byte *));
+  bfd_signed_vma (*bfd_getx_signed_32) PARAMS ((const bfd_byte *));
+  void           (*bfd_putx32) PARAMS ((bfd_vma, bfd_byte *));
+  bfd_vma        (*bfd_getx16) PARAMS ((const bfd_byte *));
+  bfd_signed_vma (*bfd_getx_signed_16) PARAMS ((const bfd_byte *));
+  void           (*bfd_putx16) PARAMS ((bfd_vma, bfd_byte *));
+  bfd_vma        (*bfd_h_getx64) PARAMS ((const bfd_byte *));
+  bfd_signed_vma (*bfd_h_getx_signed_64) PARAMS ((const bfd_byte *));
+  void           (*bfd_h_putx64) PARAMS ((bfd_vma, bfd_byte *));
+  bfd_vma        (*bfd_h_getx32) PARAMS ((const bfd_byte *));
+  bfd_signed_vma (*bfd_h_getx_signed_32) PARAMS ((const bfd_byte *));
+  void           (*bfd_h_putx32) PARAMS ((bfd_vma, bfd_byte *));
+  bfd_vma        (*bfd_h_getx16) PARAMS ((const bfd_byte *));
+  bfd_signed_vma (*bfd_h_getx_signed_16) PARAMS ((const bfd_byte *));
+  void           (*bfd_h_putx16) PARAMS ((bfd_vma, bfd_byte *));
+  const struct bfd_target *(*_bfd_check_format[bfd_type_end]) PARAMS ((bfd *));
+  boolean  (*_bfd_set_format[bfd_type_end]) PARAMS ((bfd *));
+  boolean  (*_bfd_write_contents[bfd_type_end]) PARAMS ((bfd *));
+
+  /* Generic entry points.  */
+#define BFD_JUMP_TABLE_GENERIC(NAME) \
+CONCAT2 (NAME,_close_and_cleanup), \
+CONCAT2 (NAME,_bfd_free_cached_info), \
+CONCAT2 (NAME,_new_section_hook), \
+CONCAT2 (NAME,_get_section_contents), \
+CONCAT2 (NAME,_get_section_contents_in_window)
+
+  /* Called when the BFD is being closed to do any necessary cleanup.  */
+  boolean  (*_close_and_cleanup) PARAMS ((bfd *));
+  /* Ask the BFD to free all cached information.  */
+  boolean  (*_bfd_free_cached_info) PARAMS ((bfd *));
+  /* Called when a new section is created.  */
+  boolean  (*_new_section_hook) PARAMS ((bfd *, sec_ptr));
+  /* Read the contents of a section.  */
+  boolean  (*_bfd_get_section_contents) PARAMS ((bfd *, sec_ptr, PTR,
+                                                 file_ptr, bfd_size_type));
+  boolean  (*_bfd_get_section_contents_in_window)
+    PARAMS ((bfd *, sec_ptr, bfd_window *, file_ptr, bfd_size_type));
+
+  /* Entry points to copy private data.  */
+#define BFD_JUMP_TABLE_COPY(NAME) \
+CONCAT2 (NAME,_bfd_copy_private_bfd_data), \
+CONCAT2 (NAME,_bfd_merge_private_bfd_data), \
+CONCAT2 (NAME,_bfd_copy_private_section_data), \
+CONCAT2 (NAME,_bfd_copy_private_symbol_data), \
+CONCAT2 (NAME,_bfd_set_private_flags), \
+CONCAT2 (NAME,_bfd_print_private_bfd_data) \
+  /* Called to copy BFD general private data from one object file
+     to another.  */
+  boolean  (*_bfd_copy_private_bfd_data) PARAMS ((bfd *, bfd *));
+  /* Called to merge BFD general private data from one object file
+     to a common output file when linking.  */
+  boolean  (*_bfd_merge_private_bfd_data) PARAMS ((bfd *, bfd *));
+  /* Called to copy BFD private section data from one object file
+     to another.  */
+  boolean  (*_bfd_copy_private_section_data) PARAMS ((bfd *, sec_ptr,
+                                                      bfd *, sec_ptr));
+  /* Called to copy BFD private symbol data from one symbol
+     to another.  */
+  boolean  (*_bfd_copy_private_symbol_data) PARAMS ((bfd *, asymbol *,
+                                                     bfd *, asymbol *));
+  /* Called to set private backend flags */
+  boolean  (*_bfd_set_private_flags) PARAMS ((bfd *, flagword));
+
+  /* Called to print private BFD data */
+  boolean  (*_bfd_print_private_bfd_data) PARAMS ((bfd *, PTR));
+
+  /* Core file entry points.  */
+#define BFD_JUMP_TABLE_CORE(NAME) \
+CONCAT2 (NAME,_core_file_failing_command), \
+CONCAT2 (NAME,_core_file_failing_signal), \
+CONCAT2 (NAME,_core_file_matches_executable_p)
+  char *   (*_core_file_failing_command) PARAMS ((bfd *));
+  int      (*_core_file_failing_signal) PARAMS ((bfd *));
+  boolean  (*_core_file_matches_executable_p) PARAMS ((bfd *, bfd *));
+
+  /* Archive entry points.  */
+#define BFD_JUMP_TABLE_ARCHIVE(NAME) \
+CONCAT2 (NAME,_slurp_armap), \
+CONCAT2 (NAME,_slurp_extended_name_table), \
+CONCAT2 (NAME,_construct_extended_name_table), \
+CONCAT2 (NAME,_truncate_arname), \
+CONCAT2 (NAME,_write_armap), \
+CONCAT2 (NAME,_read_ar_hdr), \
+CONCAT2 (NAME,_openr_next_archived_file), \
+CONCAT2 (NAME,_get_elt_at_index), \
+CONCAT2 (NAME,_generic_stat_arch_elt), \
+CONCAT2 (NAME,_update_armap_timestamp)
+  boolean  (*_bfd_slurp_armap) PARAMS ((bfd *));
+  boolean  (*_bfd_slurp_extended_name_table) PARAMS ((bfd *));
+  boolean  (*_bfd_construct_extended_name_table)
+    PARAMS ((bfd *, char **, bfd_size_type *, const char **));
+  void     (*_bfd_truncate_arname) PARAMS ((bfd *, const char *, char *));
+  boolean  (*write_armap)
+    PARAMS ((bfd *, unsigned int, struct orl *, unsigned int, int));
+  PTR      (*_bfd_read_ar_hdr_fn) PARAMS ((bfd *));
+  bfd *    (*openr_next_archived_file) PARAMS ((bfd *, bfd *));
+#define bfd_get_elt_at_index(b,i) BFD_SEND(b, _bfd_get_elt_at_index, (b,i))
+  bfd *    (*_bfd_get_elt_at_index) PARAMS ((bfd *, symindex));
+  int      (*_bfd_stat_arch_elt) PARAMS ((bfd *, struct stat *));
+  boolean  (*_bfd_update_armap_timestamp) PARAMS ((bfd *));
+
+  /* Entry points used for symbols.  */
+#define BFD_JUMP_TABLE_SYMBOLS(NAME) \
+CONCAT2 (NAME,_get_symtab_upper_bound), \
+CONCAT2 (NAME,_get_symtab), \
+CONCAT2 (NAME,_make_empty_symbol), \
+CONCAT2 (NAME,_print_symbol), \
+CONCAT2 (NAME,_get_symbol_info), \
+CONCAT2 (NAME,_bfd_is_local_label_name), \
+CONCAT2 (NAME,_get_lineno), \
+CONCAT2 (NAME,_find_nearest_line), \
+CONCAT2 (NAME,_bfd_make_debug_symbol), \
+CONCAT2 (NAME,_read_minisymbols), \
+CONCAT2 (NAME,_minisymbol_to_symbol)
+  long     (*_bfd_get_symtab_upper_bound) PARAMS ((bfd *));
+  long     (*_bfd_canonicalize_symtab) PARAMS ((bfd *,
+                                                struct symbol_cache_entry **));
+  struct symbol_cache_entry *
+           (*_bfd_make_empty_symbol) PARAMS ((bfd *));
+  void     (*_bfd_print_symbol) PARAMS ((bfd *, PTR,
+                                         struct symbol_cache_entry *,
+                                         bfd_print_symbol_type));
+#define bfd_print_symbol(b,p,s,e) BFD_SEND(b, _bfd_print_symbol, (b,p,s,e))
+  void     (*_bfd_get_symbol_info) PARAMS ((bfd *,
+                                            struct symbol_cache_entry *,
+                                            symbol_info *));
+#define bfd_get_symbol_info(b,p,e) BFD_SEND(b, _bfd_get_symbol_info, (b,p,e))
+  boolean  (*_bfd_is_local_label_name) PARAMS ((bfd *, const char *));
+
+  alent *  (*_get_lineno) PARAMS ((bfd *, struct symbol_cache_entry *));
+  boolean  (*_bfd_find_nearest_line)
+    PARAMS ((bfd *, struct sec *, struct symbol_cache_entry **, bfd_vma,
+             const char **, const char **, unsigned int *));
+ /* Back-door to allow format-aware applications to create debug symbols
+    while using BFD for everything else.  Currently used by the assembler
+    when creating COFF files.  */
+  asymbol *(*_bfd_make_debug_symbol) PARAMS ((bfd *, void *,
+                                              unsigned long size));
+#define bfd_read_minisymbols(b, d, m, s) \
+  BFD_SEND (b, _read_minisymbols, (b, d, m, s))
+  long     (*_read_minisymbols) PARAMS ((bfd *, boolean, PTR *,
+                                         unsigned int *));
+#define bfd_minisymbol_to_symbol(b, d, m, f) \
+  BFD_SEND (b, _minisymbol_to_symbol, (b, d, m, f))
+  asymbol *(*_minisymbol_to_symbol) PARAMS ((bfd *, boolean, const PTR,
+                                             asymbol *));
+
+  /* Routines for relocs.  */
+#define BFD_JUMP_TABLE_RELOCS(NAME) \
+CONCAT2 (NAME,_get_reloc_upper_bound), \
+CONCAT2 (NAME,_canonicalize_reloc), \
+CONCAT2 (NAME,_bfd_reloc_type_lookup)
+  long     (*_get_reloc_upper_bound) PARAMS ((bfd *, sec_ptr));
+  long     (*_bfd_canonicalize_reloc) PARAMS ((bfd *, sec_ptr, arelent **,
+                                               struct symbol_cache_entry **));
+  /* See documentation on reloc types.  */
+  reloc_howto_type *
+           (*reloc_type_lookup) PARAMS ((bfd *, bfd_reloc_code_real_type));
+
+  /* Routines used when writing an object file.  */
+#define BFD_JUMP_TABLE_WRITE(NAME) \
+CONCAT2 (NAME,_set_arch_mach), \
+CONCAT2 (NAME,_set_section_contents)
+  boolean  (*_bfd_set_arch_mach) PARAMS ((bfd *, enum bfd_architecture,
+                                          unsigned long));
+  boolean  (*_bfd_set_section_contents) PARAMS ((bfd *, sec_ptr, PTR,
+                                                 file_ptr, bfd_size_type));
+
+  /* Routines used by the linker.  */
+#define BFD_JUMP_TABLE_LINK(NAME) \
+CONCAT2 (NAME,_sizeof_headers), \
+CONCAT2 (NAME,_bfd_get_relocated_section_contents), \
+CONCAT2 (NAME,_bfd_relax_section), \
+CONCAT2 (NAME,_bfd_link_hash_table_create), \
+CONCAT2 (NAME,_bfd_link_add_symbols), \
+CONCAT2 (NAME,_bfd_final_link), \
+CONCAT2 (NAME,_bfd_link_split_section), \
+CONCAT2 (NAME,_bfd_gc_sections), \
+CONCAT2 (NAME,_bfd_merge_sections)
+  int      (*_bfd_sizeof_headers) PARAMS ((bfd *, boolean));
+  bfd_byte *(*_bfd_get_relocated_section_contents)
+    PARAMS ((bfd *, struct bfd_link_info *, struct bfd_link_order *,
+             bfd_byte *, boolean, struct symbol_cache_entry **));
+
+  boolean  (*_bfd_relax_section)
+    PARAMS ((bfd *, struct sec *, struct bfd_link_info *, boolean *));
+
+  /* Create a hash table for the linker.  Different backends store
+     different information in this table.  */
+  struct bfd_link_hash_table *(*_bfd_link_hash_table_create) PARAMS ((bfd *));
+
+  /* Add symbols from this object file into the hash table.  */
+  boolean  (*_bfd_link_add_symbols) PARAMS ((bfd *, struct bfd_link_info *));
+
+  /* Do a link based on the link_order structures attached to each
+     section of the BFD.  */
+  boolean  (*_bfd_final_link) PARAMS ((bfd *, struct bfd_link_info *));
+
+  /* Should this section be split up into smaller pieces during linking.  */
+  boolean  (*_bfd_link_split_section) PARAMS ((bfd *, struct sec *));
+
+  /* Remove sections that are not referenced from the output.  */
+  boolean  (*_bfd_gc_sections) PARAMS ((bfd *, struct bfd_link_info *));
+
+  /* Attempt to merge SEC_MERGE sections.  */
+  boolean  (*_bfd_merge_sections) PARAMS ((bfd *, struct bfd_link_info *));
+
+  /* Routines to handle dynamic symbols and relocs.  */
+#define BFD_JUMP_TABLE_DYNAMIC(NAME) \
+CONCAT2 (NAME,_get_dynamic_symtab_upper_bound), \
+CONCAT2 (NAME,_canonicalize_dynamic_symtab), \
+CONCAT2 (NAME,_get_dynamic_reloc_upper_bound), \
+CONCAT2 (NAME,_canonicalize_dynamic_reloc)
+  /* Get the amount of memory required to hold the dynamic symbols. */
+  long     (*_bfd_get_dynamic_symtab_upper_bound) PARAMS ((bfd *));
+  /* Read in the dynamic symbols.  */
+  long     (*_bfd_canonicalize_dynamic_symtab)
+    PARAMS ((bfd *, struct symbol_cache_entry **));
+  /* Get the amount of memory required to hold the dynamic relocs.  */
+  long     (*_bfd_get_dynamic_reloc_upper_bound) PARAMS ((bfd *));
+  /* Read in the dynamic relocs.  */
+  long     (*_bfd_canonicalize_dynamic_reloc)
+    PARAMS ((bfd *, arelent **, struct symbol_cache_entry **));
+
+ /* Opposite endian version of this target.  */
+ const struct bfd_target * alternative_target;
+
+ PTR backend_data;
+
+} bfd_target;
+boolean
+bfd_set_default_target PARAMS ((const char *name));
+
+const bfd_target *
+bfd_find_target PARAMS ((const char *target_name, bfd *abfd));
+
+const char **
+bfd_target_list PARAMS ((void));
+
+const bfd_target *
+bfd_search_for_target PARAMS ((int (* search_func) (const bfd_target *, void *), void *));
+
+boolean
+bfd_check_format PARAMS ((bfd *abfd, bfd_format format));
+
+boolean
+bfd_check_format_matches PARAMS ((bfd *abfd, bfd_format format, char ***matching));
+
+boolean
+bfd_set_format PARAMS ((bfd *abfd, bfd_format format));
+
+const char *
+bfd_format_string PARAMS ((bfd_format format));
+
+#ifdef __cplusplus
+}
+#endif
+#endif
diff -purN linux-2.5/arch/ppc64/kdb/kdba_bp.c linuxppc64-2.5/arch/ppc64/kdb/kdba_bp.c
--- linux-2.5/arch/ppc64/kdb/kdba_bp.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/kdba_bp.c	2003-10-13 16:16:57.000000000 +0000
@@ -0,0 +1,789 @@
+/*
+ * Kernel Debugger Architecture Dependent Breakpoint Handling
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *	Scott Lurndal			1999/12/12
+ *		v1.0 restructuring.
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ */
+
+#include <linux/string.h>
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/smp.h>
+#include <linux/ptrace.h>
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+#include "privinst.h"
+
+static char *kdba_rwtypes[] = { "Instruction(Register)", "Data Write",
+			"I/O", "Data Access"};
+
+extern void set_all_DABR(unsigned long val);
+
+/*
+ * Table describing processor architecture hardware
+ * breakpoint registers.
+ */
+
+kdbhard_bp_t	kdb_hardbreaks[KDB_MAXHARDBPT];
+
+/*
+ * kdba_db_trap
+ *
+ * 	Perform breakpoint processing upon entry to the
+ *	processor debugger fault.   Determine and print
+ *	the active breakpoint.
+ *
+ * Parameters:
+ *	ef	Exception frame containing machine register state
+ *	error	Error number passed to kdb.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	KDB_DB_BPT	Standard instruction or data breakpoint encountered
+ *	KDB_DB_SS	Single Step fault ('ss' command or end of 'ssb' command)
+ *	KDB_DB_SSB	Single Step fault, caller should continue ('ssb' command)
+ *	KDB_DB_SSBPT	Single step over breakpoint
+ *	KDB_DB_NOBPT	No existing kdb breakpoint matches this debug exception
+ * Locking:
+ *	None.
+ * Remarks:
+ *	Yup, there be goto's here.
+ *
+ *	If multiple processors receive debug exceptions simultaneously,
+ *	one may be waiting at the kdb fence in kdb() while the user
+ *	issues a 'bc' command to clear the breakpoint the processor
+ *	which is waiting has already encountered.  If this is the case,
+ *	the debug registers will no longer match any entry in the
+ *	breakpoint table, and we'll return the value KDB_DB_NOBPT.
+ *	This can cause a panic in die_if_kernel().  It is safer to
+ *	disable the breakpoint (bd), go until all processors are past
+ *	the breakpoint then clear the breakpoint (bc).  This code
+ *	recognises a breakpoint even when disabled but not when it has
+ *	been cleared.
+ *
+ *	WARNING: This routine clears the debug state.  It should be called
+ *		 once per debug and the result cached.
+ */
+
+kdb_dbtrap_t
+kdba_db_trap(kdb_eframe_t ef, int error_unused)
+{
+	kdb_machreg_t  msr,trap;
+	int rw, reg;
+	int i;
+	kdb_dbtrap_t rv = KDB_DB_BPT;
+	kdb_bp_t *bp;
+	unsigned long primary;
+	unsigned long extended;
+
+	msr = get_msr();
+	trap = ef->trap;
+	if (KDB_DEBUG(BP))
+		kdb_printf("kdb: msr 0x%lx trap 0x%lx\n", msr,trap);
+	if (msr & MSR_SE || ((trap & 0x700) || (trap & 0xd00))) 
+	{
+		if (KDB_STATE(SSBPT)) {
+			if (KDB_DEBUG(BP))
+				kdb_printf("ssbpt\n");
+			KDB_STATE_CLEAR(SSBPT);
+			for(i=0,bp=kdb_breakpoints;
+			    i < KDB_MAXBPT;
+			    i++, bp++) {
+				if (KDB_DEBUG(BP))
+					kdb_printf("bp 0x%p enabled %d delayed %d global %d cpu %d\n",
+						   bp, bp->bp_enabled, bp->bp_delayed, bp->bp_global, bp->bp_cpu);
+				if (!bp->bp_enabled)
+					continue;
+				if (!bp->bp_global && bp->bp_cpu != smp_processor_id())
+					continue;
+				if (KDB_DEBUG(BP))
+					kdb_printf("bp for this cpu\n");
+				if (bp->bp_delayed) {
+					bp->bp_delayed = 0;
+					if (KDB_DEBUG(BP))
+						kdb_printf("kdba_installbp\n");
+					kdba_installbp(ef, bp);
+					if (!KDB_STATE(DOING_SS)) {
+						set_msr(get_msr() & ~MSR_SE);
+						return(KDB_DB_SSBPT);
+					}
+					break;
+				}
+			}
+			if (i == KDB_MAXBPT) {
+				kdb_printf("kdb: Unable to find delayed breakpoint\n");
+			}
+			if (!KDB_STATE(DOING_SS)) {
+				set_msr(get_msr() & ~MSR_SE);
+				return(KDB_DB_NOBPT);
+			}
+			/* FALLTHROUGH */
+		}
+
+		/*
+		 * KDB_STATE_DOING_SS is set when the kernel debugger is using
+		 * the processor trap flag to single-step a processor.  If a
+		 * single step trap occurs and this flag is clear, the SS trap
+		 * will be ignored by KDB and the kernel will be allowed to deal
+		 * with it as necessary (e.g. for ptrace).
+		 */
+		if (!KDB_STATE(DOING_SS))
+			goto unknown;
+
+		/* single step */
+		rv = KDB_DB_SS;		/* Indicate single step */
+		if (KDB_STATE(DOING_SSB)) {
+		    unsigned long instruction;
+
+			kdb_id1(ef->nip);
+			kdb_getarea(instruction,ef->nip);
+			primary=instruction & 0xfc000000;
+			extended=instruction & 0x000007fe;
+			if (kdb_getarea(instruction, ef->nip) ||   /* read failure */
+/* branch conditional */
+			    (primary==16 )||
+/* branch */
+			    (primary==18 )||    
+/* branch conditional to LR, or branch conditional to CR */
+			    (primary==19 && (extended==16 || extended == 528) 
+			     )) {
+				/*
+				 * End the ssb command here.
+				 */
+			    KDB_STATE_CLEAR(DOING_SSB);
+			    KDB_STATE_CLEAR(DOING_SS);
+			    }
+			rv = KDB_DB_SSB; /* Indicate ssb - dismiss immediately */
+		} else {
+			/*
+			 * Print current insn
+			 */
+			kdb_printf("SS trap at ");
+			kdb_symbol_print(ef->nip, NULL, KDB_SP_DEFAULT|KDB_SP_NEWLINE);
+			kdb_printf(" "); /* wms */
+			kdb_id1(ef->nip);
+			KDB_STATE_CLEAR(DOING_SS);
+		}
+
+		if (rv != KDB_DB_SSB)
+			set_msr(get_msr() & ~MSR_SE);
+	}
+	if (rv > 0)
+		goto handled;
+	
+	goto handle;
+
+
+handle:
+
+	/*
+	 * Determine which breakpoint was encountered.
+	 */
+	for(i=0, bp=kdb_breakpoints; i<KDB_MAXBPT; i++, bp++) {
+		if (!(bp->bp_free)
+		 && (bp->bp_global || bp->bp_cpu == smp_processor_id())
+		 && (bp->bp_hard)
+		 && (bp->bp_hard->bph_reg == reg)) {
+			/*
+			 * Hit this breakpoint.
+			 */
+			kdb_printf("%s breakpoint #%d at " kdb_bfd_vma_fmt "\n", 
+				  kdba_rwtypes[rw],
+				  i, (long )bp->bp_addr);
+
+			/*
+			 * For an instruction breakpoint, disassemble
+			 * the current instruction.
+			 */
+			if (rw == 0) {
+				kdb_id1(ef->nip);
+			}
+
+			goto handled;
+		}
+	}
+
+unknown:
+	rv = KDB_DB_NOBPT;	/* Cause kdb() to return */
+
+handled:
+
+
+	return rv;
+}
+
+/*
+ * kdba_bp_trap
+ *
+ * 	Perform breakpoint processing upon entry to the
+ *	processor breakpoint instruction fault.   Determine and print
+ *	the active breakpoint.
+ *
+ * Parameters:
+ *	ef	Exception frame containing machine register state
+ *	error	Error number passed to kdb.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	0	Standard instruction or data breakpoint encountered
+ *	1	Single Step fault ('ss' command)
+ *	2	Single Step fault, caller should continue ('ssb' command)
+ *	3	No existing kdb breakpoint matches this debug exception
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ * 	If multiple processors receive debug exceptions simultaneously,
+ *	one may be waiting at the kdb fence in kdb() while the user
+ *	issues a 'bc' command to clear the breakpoint the processor which
+ * 	is waiting has already encountered.   If this is the case, the
+ *	debug registers will no longer match any entry in the breakpoint
+ *	table, and we'll return the value '3'.  This can cause a panic
+ *	in die_if_kernel().  It is safer to disable the breakpoint (bd),
+ *	'go' until all processors are past the breakpoint then clear the
+ *	breakpoint (bc).  This code recognises a breakpoint even when
+ *	disabled but not when it has been cleared.
+ *
+ *	WARNING: This routine resets the eip.  It should be called
+ *		 once per breakpoint and the result cached.
+ */
+
+kdb_dbtrap_t
+kdba_bp_trap(kdb_eframe_t ef, int error_unused)
+{
+	int i;
+	kdb_dbtrap_t rv;
+	kdb_bp_t *bp;
+
+	/*
+	 * Determine which breakpoint was encountered.
+	 */
+	if (KDB_DEBUG(BP))
+		kdb_printf("kdba_bp_trap: eip=0x%lx (not adjusted) "
+			   "msr=0x%lx trap=0x%lx ef=0x%p esp=0x%lx\n",
+			   ef->nip, ef->msr, ef->trap, ef, ef->gpr[1]);
+
+	rv = KDB_DB_NOBPT;	/* Cause kdb() to return */
+
+	for(i=0, bp=kdb_breakpoints; i<KDB_MAXBPT; i++, bp++) {
+		if (bp->bp_free)
+			continue;
+		if (!bp->bp_global && bp->bp_cpu != smp_processor_id())
+			continue;
+		 if (bp->bp_addr == (ef->nip - bp->bp_adjust)) {
+			/* Hit this breakpoint.  */
+			ef->nip -= bp->bp_adjust;
+			kdb_printf("Instruction(i) breakpoint #%d at 0x%lx (adjusted)\n",
+				  i, ef->nip);
+			kdb_id1(ef->nip);
+			rv = KDB_DB_BPT;
+			bp->bp_delay = 1;
+			break;
+		}
+	}
+
+	return rv;
+}
+
+/*
+ * kdba_handle_bp
+ *
+ *	Handle an instruction-breakpoint trap.  Called when re-installing
+ *	an enabled breakpoint which has has the bp_delay bit set.
+ *
+ * Parameters:
+ * Returns:
+ * Locking:
+ * Remarks:
+ *
+ * Ok, we really need to:
+ *	1) Restore the original instruction byte
+ *	2) Single Step
+ *	3) Restore breakpoint instruction
+ *	4) Continue.
+ *
+ *
+ */
+
+static void
+kdba_handle_bp(kdb_eframe_t ef, kdb_bp_t *bp)
+{
+	if (!ef) {
+		kdb_printf("kdba_handle_bp: ef == NULL\n");
+		return;
+	}
+
+	if (KDB_DEBUG(BP))
+		kdb_printf("ef->eip = 0x%lx\n", ef->nip);
+
+	/*
+	 * Setup single step
+	 */
+	kdba_setsinglestep(ef);
+
+	/* KDB_STATE_SSBPT is set when the kernel debugger must single step
+	 * a task in order to re-establish an instruction breakpoint which
+	 * uses the instruction replacement mechanism. 
+	 */
+	KDB_STATE_SET(SSBPT);
+
+	/*
+	 * Reset delay attribute
+	 */
+	bp->bp_delay = 0;
+	bp->bp_delayed = 1;
+}
+
+
+/*
+ * kdba_bptype
+ *
+ *	Return a string describing type of breakpoint.
+ *
+ * Parameters:
+ *	bph	Pointer to hardware breakpoint description
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Character string.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+char *
+kdba_bptype(kdbhard_bp_t *bph)
+{
+	char *mode;
+
+	mode = kdba_rwtypes[bph->bph_mode];
+
+	return mode;
+}
+
+/*
+ * kdba_printbpreg
+ *
+ *	Print register name assigned to breakpoint
+ *
+ * Parameters:
+ *	bph	Pointer hardware breakpoint structure
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+void
+kdba_printbpreg(kdbhard_bp_t *bph)
+{
+	kdb_printf(" in dr%ld", bph->bph_reg);
+}
+
+/*
+ * kdba_printbp
+ *
+ *	Print string describing hardware breakpoint.
+ *
+ * Parameters:
+ *	bph	Pointer to hardware breakpoint description
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+void
+kdba_printbp(kdb_bp_t *bp)
+{
+	kdb_printf("\n    is enabled");
+	if (bp->bp_hardtype) {
+		kdba_printbpreg(bp->bp_hard);
+		if (bp->bp_hard->bph_mode != 0) {
+			kdb_printf(" for %d bytes",
+				   bp->bp_hard->bph_length+1);
+		}
+	}
+}
+
+/*
+ * kdba_parsebp
+ *
+ *	Parse architecture dependent portion of the
+ *	breakpoint command.
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Zero for success, a kdb diagnostic for failure
+ * Locking:
+ *	None.
+ * Remarks:
+ *	for Ia32 architure, data access, data write and
+ *	I/O breakpoints are supported in addition to instruction
+ * 	breakpoints.
+ *
+ *	{datar|dataw|io|inst} [length]
+ */
+
+int
+kdba_parsebp(int argc, const char **argv, int *nextargp, kdb_bp_t *bp)
+{
+	int		nextarg = *nextargp;
+	int		diag;
+	kdbhard_bp_t 	*bph = &bp->bp_template;
+
+	bph->bph_mode = 0;		/* Default to instruction breakpoint */
+	bph->bph_length = 0;		/* Length must be zero for insn bp */
+	if ((argc + 1) != nextarg) {
+		if (strnicmp(argv[nextarg], "datar", sizeof("datar")) == 0) {
+			bph->bph_mode = 3;
+		} else if (strnicmp(argv[nextarg], "dataw", sizeof("dataw")) == 0) {
+			bph->bph_mode = 1;
+		} else if (strnicmp(argv[nextarg], "io", sizeof("io")) == 0) {
+			bph->bph_mode = 2;
+		} else if (strnicmp(argv[nextarg], "inst", sizeof("inst")) == 0) {
+			bph->bph_mode = 0;
+		} else {
+			return KDB_ARGCOUNT;
+		}
+
+		bph->bph_length = 3;	/* Default to 4 byte */
+
+		nextarg++;
+
+		if ((argc + 1) != nextarg) {
+			unsigned long len;
+
+			diag = kdbgetularg((char *)argv[nextarg],
+					   &len);
+			if (diag)
+				return diag;
+
+
+			if ((len > 4) || (len == 3))
+				return KDB_BADLENGTH;
+
+			bph->bph_length = len;
+			bph->bph_length--; /* Normalize for debug register */
+			nextarg++;
+		}
+
+		if ((argc + 1) != nextarg)
+			return KDB_ARGCOUNT;
+
+		/*
+		 * Indicate to architecture independent level that
+		 * a hardware register assignment is required to enable
+		 * this breakpoint.
+		 */
+
+		bph->bph_free = 0;
+	} else {
+		if (KDB_DEBUG(BP))
+			kdb_printf("kdba_bp: no args, forcehw is %d\n", bp->bp_forcehw);
+		if (bp->bp_forcehw) {
+			/*
+			 * We are forced to use a hardware register for this
+			 * breakpoint because either the bph or bpha
+			 * commands were used to establish this breakpoint.
+			 */
+			bph->bph_free = 0;
+		} else {
+			/*
+			 * Indicate to architecture dependent level that
+			 * the instruction replacement breakpoint technique
+			 * should be used for this breakpoint.
+			 */
+			bph->bph_free = 1;
+			bp->bp_adjust = PPC64_ADJUST_OFFSET;
+		}
+	}
+
+	*nextargp = nextarg;
+	return 0;
+}
+
+/*
+ * kdba_allocbp
+ *
+ *	Associate a hardware register with a breakpoint.
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	A pointer to the allocated register kdbhard_bp_t structure for
+ *	success, Null and a non-zero diagnostic for failure.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+kdbhard_bp_t *
+kdba_allocbp(kdbhard_bp_t *bph, int *diagp)
+{
+	int i;
+	kdbhard_bp_t *newbph;
+
+	for(i=0,newbph=kdb_hardbreaks; i < KDB_MAXHARDBPT; i++, newbph++) {
+		if (newbph->bph_free) {
+			break;
+		}
+	}
+
+	if (i == KDB_MAXHARDBPT) {
+		*diagp = KDB_TOOMANYDBREGS;
+		return NULL;
+	}
+
+	*diagp = 0;
+
+	/*
+	 * Copy data from template.  Can't just copy the entire template
+	 * here because the register number in kdb_hardbreaks must be
+	 * preserved.
+	 */
+	newbph->bph_data = bph->bph_data;
+	newbph->bph_write = bph->bph_write;
+	newbph->bph_mode = bph->bph_mode;
+	newbph->bph_length = bph->bph_length;
+
+	/*
+	 * Mark entry allocated.
+	 */
+	newbph->bph_free = 0;
+
+	return newbph;
+}
+
+/*
+ * kdba_freebp
+ *
+ *	Deallocate a hardware breakpoint
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Zero for success, a kdb diagnostic for failure
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+void
+kdba_freebp(kdbhard_bp_t *bph)
+{
+	bph->bph_free = 1;
+}
+
+/*
+ * kdba_initbp
+ *
+ *	Initialize the breakpoint table for the hardware breakpoint
+ *	register.
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	Zero for success, a kdb diagnostic for failure
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ *	There is one entry per register.  On the ia32 architecture
+ *	all the registers are interchangeable, so no special allocation
+ *	criteria are required.
+ */
+
+void
+kdba_initbp(void)
+{
+	int i;
+	kdbhard_bp_t *bph;
+
+	/*
+	 * Clear the hardware breakpoint table
+	 */
+
+	memset(kdb_hardbreaks, '\0', sizeof(kdb_hardbreaks));
+
+	for(i=0,bph=kdb_hardbreaks; i<KDB_MAXHARDBPT; i++, bph++) {
+		bph->bph_reg = i;
+		bph->bph_free = 1;
+	}
+}
+
+/*
+ * kdba_installbp
+ *
+ *	Install a breakpoint
+ *
+ * Parameters:
+ *	ef	Exception frame
+ *	bp	Breakpoint structure for the breakpoint to be installed
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	For hardware breakpoints, a debug register is allocated
+ *	and assigned to the breakpoint.  If no debug register is
+ *	available, a warning message is printed and the breakpoint
+ *	is disabled.
+ *
+ *	For instruction replacement breakpoints, we must single-step
+ *	over the replaced instruction at this point so we can re-install
+ *	the breakpoint instruction after the single-step.
+ */
+
+int
+kdba_installbp(kdb_eframe_t ef, kdb_bp_t *bp)
+{
+    int rc;
+	/*
+	 * Install the breakpoint, if it is not already installed.
+	 */
+
+	if (KDB_DEBUG(BP)) {
+		kdb_printf("kdba_installbp bp_installed %d\n", bp->bp_installed);
+	}
+	if (!bp->bp_installed) {
+		if (bp->bp_hardtype) {
+			kdba_installdbreg(bp); 
+			bp->bp_installed = 1;
+			if (KDB_DEBUG(BP)) {
+				kdb_printf("kdba_installbp hardware reg %ld at " kdb_bfd_vma_fmt "\n",
+					   (long unsigned int) bp->bp_hard->bph_reg, (long unsigned int) bp->bp_addr);
+			}
+		} else if (bp->bp_delay) {
+			if (KDB_DEBUG(BP))
+				kdb_printf("kdba_installbp delayed bp\n");
+			kdba_handle_bp(ef, bp);
+		} else {
+		    if (KDB_DEBUG(BP))
+			kdb_printf("0x%lx 0x%lx 0x%lx\n",bp->bp_inst,bp->bp_addr,sizeof(bp->bp_addr));
+		    rc = kdb_getword(&bp->bp_inst, bp->bp_addr,sizeof(bp->bp_addr));
+		    kdb_putword(bp->bp_addr, PPC64_BREAKPOINT_INSTRUCTION,sizeof(PPC64_BREAKPOINT_INSTRUCTION));
+		    if (KDB_DEBUG(BP))
+			kdb_printf("kdba_installbp instruction 0x%x at " kdb_bfd_vma_fmt "\n",
+				   PPC64_BREAKPOINT_INSTRUCTION, bp->bp_addr);
+		    bp->bp_installed = 1;
+		}
+	}
+return 0;
+}
+
+/*
+ * kdba_removebp
+ *
+ *	Make a breakpoint ineffective.
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+int
+kdba_removebp(kdb_bp_t *bp)
+{
+	/*
+	 * For hardware breakpoints, remove it from the active register,
+	 * for software breakpoints, restore the instruction stream.
+	 */
+	if (KDB_DEBUG(BP)) {
+		kdb_printf("kdba_removebp bp_installed %d\n", bp->bp_installed);
+	}
+	if (bp->bp_installed) {
+		if (bp->bp_hardtype) {
+			if (KDB_DEBUG(BP)) {
+				kdb_printf("kdb: removing hardware reg %ld at " kdb_bfd_vma_fmt "\n",
+					   bp->bp_hard->bph_reg, bp->bp_addr);
+			}
+			kdba_removedbreg(bp);
+		} else
+		{
+			if (KDB_DEBUG(BP))
+				kdb_printf("kdb: restoring instruction 0x%lx at " kdb_bfd_vma_fmt "\n",
+					   bp->bp_inst, bp->bp_addr);
+			kdb_putword(bp->bp_addr, bp->bp_inst,sizeof(bp->bp_inst));
+		}
+		bp->bp_installed = 0;
+	}
+return 0;
+}
+
+#if 0
+#define systemcfg naca
+#endif
+
+/* install data breakpoint */
+void
+kdba_installdbreg(kdb_bp_t *bp) {
+    if (systemcfg->platform==PLATFORM_PSERIES)
+    {
+    /* set_dabr is the kdb form, using mtspr instructions */
+	set_dabr(bp->bp_addr); 
+    } else if (systemcfg->platform==PLATFORM_PSERIES_LPAR ) {
+	/*set_all_DABR(bp->bp_addr); missing from 2.5? */
+#if 0
+	HvCall_setDABR(bp->bp_addr);
+#endif
+    } else if (systemcfg->platform==PLATFORM_ISERIES_LPAR ) {
+	/* different hcall interface needed here. */
+    }
+}
+
+/* remove data breakpoint-- set it to zero. */
+void
+kdba_removedbreg(kdb_bp_t *bp) {
+    if (systemcfg->platform==PLATFORM_PSERIES)
+    {
+    /* set_dabr is the kdb form, using mtspr instructions */
+	set_dabr(0x0); 
+    } else if (systemcfg->platform==PLATFORM_PSERIES_LPAR ) {
+	/*set_all_DABR(bp->bp_addr); missing from 2.5? */
+#if 0
+	HvCall_setDABR(0x0);
+#endif
+    } else if (systemcfg->platform==PLATFORM_ISERIES_LPAR ) {
+	/* different hcall interface needed here. */
+    }
+}
diff -purN linux-2.5/arch/ppc64/kdb/kdba_bt.c linuxppc64-2.5/arch/ppc64/kdb/kdba_bt.c
--- linux-2.5/arch/ppc64/kdb/kdba_bt.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/kdba_bt.c	2003-11-26 17:14:34.000000000 +0000
@@ -0,0 +1,276 @@
+/*
+ * Minimalist Kernel Debugger - Architecture Dependent Stack Traceback
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *      Srinivasa Thirumalachar
+ *              RSE support for ia64
+ *	Masahiro Adegawa                1999/12/01
+ *		'sr' command, active flag in 'ps'
+ *	Scott Lurndal			1999/12/12
+ *		Significantly restructure for linux2.3
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ *
+ */
+
+#include <linux/ctype.h>
+#include <linux/string.h>
+#include <linux/kernel.h>
+#include <linux/sched.h>
+#include <linux/kallsyms.h>
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+#include <linux/ptrace.h>	/* for STACK_FRAME_OVERHEAD */
+#include <asm/system.h>
+#include "privinst.h"
+
+void systemreset(struct pt_regs *regs)
+{
+	udbg_printf("Oh no!\n");
+	kdb_printf("Oh no!\n");
+	kdb(KDB_REASON_OOPS, 0, (kdb_eframe_t) regs);
+	for (;;);
+}
+
+/* human name vector lookup. */
+static
+const char *getvecname(unsigned long vec)
+{
+	char *ret;
+	switch (vec) {
+	case 0x100:	ret = "(System Reset)"; break; 
+	case 0x200:	ret = "(Machine Check)"; break; 
+	case 0x300:	ret = "(Data Access)"; break; 
+	case 0x400:	ret = "(Instruction Access)"; break; 
+	case 0x500:	ret = "(Hardware Interrupt)"; break; 
+	case 0x600:	ret = "(Alignment)"; break; 
+	case 0x700:	ret = "(Program Check)"; break; 
+	case 0x800:	ret = "(FPU Unavailable)"; break; 
+	case 0x900:	ret = "(Decrementer)"; break; 
+	case 0xc00:	ret = "(System Call)"; break; 
+	case 0xd00:	ret = "(Single Step)"; break; 
+	case 0xf00:	ret = "(Performance Monitor)"; break; 
+	default: ret = "";
+	}
+	return ret;
+}
+
+
+extern unsigned long kdba_getword(unsigned long addr, size_t width);
+
+/* Copy a block of memory using kdba_getword().
+ * This is not efficient.
+ */
+static void kdba_getmem(unsigned long addr, void *p, int size)
+{
+	unsigned char *dst = (unsigned char *)p;
+	while (size > 0) {
+		*dst++ = kdba_getword(addr++, 1);
+		size--;
+	}
+}
+
+
+/*
+ * kdba_bt_stack_ppc
+ *
+ *	kdba_bt_stack with ppc specific parameters.
+ *	Specification as kdba_bt_stack plus :-
+ *
+ * Inputs:
+ *	As kba_bt_stack plus
+ *	regs_esp If 1 get esp from the registers (exception frame), if 0
+ *		 get esp from kdba_getregcontents.
+ */
+
+static int
+kdba_bt_stack_ppc(struct pt_regs *regs, kdb_machreg_t *addr, int argcount,
+		   struct task_struct *p, int regs_esp)
+{
+
+	kdb_machreg_t	esp,eip,ebp,old_esp;
+/*	kdb_symtab_t	symtab, *sym; */
+	kdbtbtable_t	tbtab;
+	/* declare these as raw ptrs so we don't get func descriptors */
+	extern void *ret_from_except, *ret_from_syscall_1;
+/*	int do_bottom_half_ret=0; */
+
+	const char *name;
+	char namebuf[128];
+	unsigned long symsize,symoffset;
+	char *symmodname;
+
+	if (!regs && !addr)
+	{
+		kdb_printf(" invalid regs pointer \n");
+		return 0;
+	}
+
+	/*
+	 * The caller may have supplied an address at which the
+	 * stack traceback operation should begin.  This address
+	 * is assumed by this code to point to a return-address
+	 * on the stack to be traced back.
+	 *
+	 * The end result of this will make it appear as if a function
+	 * entitled '<unknown>' was called from the function which
+	 * contains return-address.
+	 */
+	if (addr) {
+		eip = 0;
+		esp = *addr;
+		ebp=0;
+	} else {
+		ebp=regs->link;
+		eip = regs->nip;
+		if (regs_esp)
+			esp = regs->gpr[1];
+		else
+			kdba_getregcontents("esp", regs, &esp);
+	}
+
+	kdb_printf("          SP(esp)            PC(eip)      Function(args)\n");
+
+	/* (Ref: 64-bit PowerPC ELF ABI Spplement; Ian Lance Taylor, Zembu Labs).
+	 A PPC stack frame looks like this:
+
+	 High Address
+	 Back Chain
+	 FP reg save area
+	 GP reg save area
+	 Local var space
+	 Parameter save area		(SP+48)
+	 TOC save area		(SP+40)
+	 link editor doubleword	(SP+32)
+	 compiler doubleword		(SP+24)
+	 LR save			(SP+16)
+	 CR save			(SP+8)
+	 Back Chain			(SP+0)
+
+	 Note that the LR (ret addr) may not be saved in the *current* frame if
+	 no functions have been called from the current function.
+	 */
+
+	/*
+	 * Run through the activation records and print them.
+	 */
+	while (1) {
+		kdb_printf("0x%016lx  0x%016lx  ", esp, eip);
+		/*		kdbnearsym(eip, &symtab); */
+		kdba_find_tb_table(eip, &tbtab); 
+
+		/*		sym = symtab.sym_name ? &symtab : &tbtab.symtab; *//* use fake symtab if necessary */
+		name = NULL;
+		if (esp >= PAGE_OFFSET) { 
+			/*if ((sym) )*/ 
+			/* if this fails, eip is outside of kernel space, dont trust it. */
+			if (eip > PAGE_OFFSET) {
+				name = kallsyms_lookup(eip, &symsize, &symoffset, &symmodname,
+						       namebuf);
+			}
+			if (name) { 
+				kdb_printf("%s", name);
+			} else {
+				kdb_printf("NO_SYMBOL or Userspace"); 
+			}
+		}
+
+		/* if this fails, eip is outside of kernel space, dont trust data. */
+		if (eip > PAGE_OFFSET) { 
+			if (eip - symoffset > 0) {
+				kdb_printf(" +0x%lx", /*eip -*/ symoffset);
+			}
+		}
+		kdb_printf("\n");
+
+		/* ret_from_except=0xa5e0 ret_from_syscall_1=a378 do_bottom_half_ret=a5e0 */
+		if (esp < PAGE_OFFSET) { /* below kernelspace..   */
+			kdb_printf("<Stack contents outside of kernel space.  %.16lx>\n", esp );
+			break;
+		} else {
+			if (eip == (kdb_machreg_t)ret_from_except ||
+			    eip == (kdb_machreg_t)ret_from_syscall_1 /* ||
+								      eip == (kdb_machreg_t)do_bottom_half_ret */) {
+				/* pull exception regs from the stack */
+				struct pt_regs eregs;
+				kdba_getmem(esp+STACK_FRAME_OVERHEAD, &eregs, sizeof(eregs));
+				kdb_printf("  [exception: %lx:%s regs 0x%lx] nip:[0x%lx] gpr[1]:[0x%x]\n", eregs.trap,getvecname(eregs.trap), esp+STACK_FRAME_OVERHEAD,(unsigned int)eregs.nip,(unsigned int)eregs.gpr[1]);
+				old_esp = esp;
+				esp = kdba_getword(esp, 8);
+				if (!esp)
+					break;
+				eip = kdba_getword(esp+16, 8);	/* saved lr */
+				if (esp < PAGE_OFFSET) {  /* userspace... */
+					if (old_esp > PAGE_OFFSET) {
+						kdb_printf("<Stack drops into userspace here %.16lx>\n",esp);
+						break;
+					}
+				}
+				/* we want to follow exception registers, not into user stack.  ...   */
+				esp = eregs.gpr[1];
+				eip = eregs.nip;
+			} else {
+				esp = kdba_getword(esp, 8);
+				if (!esp)
+					break;
+				eip = kdba_getword(esp+16, 8);	/* saved lr */
+			}
+		}
+	}
+	return 0;
+}
+
+
+/*
+ * kdba_bt_stack
+ *
+ *	This function implements the 'bt' command.  Print a stack
+ *	traceback.
+ *
+ *	bt [<address-expression>]   (addr-exp is for alternate stacks)
+ *	btp <pid>		     (Kernel stack for <pid>)
+ *
+ * 	address expression refers to a return address on the stack.  It
+ *	may be preceeded by a frame pointer.
+ *
+ * Inputs:
+ *	regs	registers at time kdb was entered.
+ *	addr	Pointer to Address provided to 'bt' command, if any.
+ *	argcount
+ *	p	Pointer to task for 'btp' command.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	zero for success, a kdb diagnostic if error
+ * Locking:
+ *	none.
+ * Remarks:
+ *	mds comes in handy when examining the stack to do a manual
+ *	traceback.
+ */
+
+int
+kdba_bt_stack(struct pt_regs *regs, kdb_machreg_t *addr, int argcount,
+	      struct task_struct *p)
+{
+	return(kdba_bt_stack_ppc(regs, addr, argcount, p, 0));
+}
+
+int
+kdba_bt_process(struct task_struct *p, int argcount)
+{
+	return (kdba_bt_stack_ppc(p->thread.regs, (kdb_machreg_t *) p->thread.ksp, argcount, p, 0));
+}
diff -purN linux-2.5/arch/ppc64/kdb/kdba_id.c linuxppc64-2.5/arch/ppc64/kdb/kdba_id.c
--- linux-2.5/arch/ppc64/kdb/kdba_id.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/kdba_id.c	2003-10-13 16:16:57.000000000 +0000
@@ -0,0 +1,278 @@
+/*
+ * Minimalist Kernel Debugger - Architecture Dependent Instruction Disassembly
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *      Srinivasa Thirumalachar
+ *              RSE support for ia64
+ *	Masahiro Adegawa                1999/12/01
+ *		'sr' command, active flag in 'ps'
+ *	Scott Lurndal			1999/12/12
+ *		Significantly restructure for linux2.3
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ *
+ */
+
+#include <stdarg.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/ctype.h>
+#include <linux/string.h>
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+
+/*
+ * kdba_dis_getsym
+ *
+ *	Get a symbol for the disassembler.
+ *
+ * Parameters:
+ *	addr	Address for which to get symbol
+ *	dip	Pointer to disassemble_info
+ * Returns:
+ *	0
+ * Locking:
+ * Remarks:
+ *	Not used for kdb.
+ */
+
+/* ARGSUSED */
+static int
+kdba_dis_getsym(bfd_vma addr, disassemble_info *dip)
+{
+
+	return 0;
+}
+
+/*
+ * kdba_printaddress
+ *
+ *	Print (symbolically) an address.
+ *
+ * Parameters:
+ *	addr	Address for which to get symbol
+ *	dip	Pointer to disassemble_info
+ *	flag	True if a ":<tab>" sequence should follow the address
+ * Returns:
+ *	number of chars printed
+ * Locking:
+ * Remarks:
+ *
+ */
+
+/* ARGSUSED */
+void
+kdba_printaddress(kdb_machreg_t addr, disassemble_info *dip, int flag)
+{
+	kdb_symtab_t symtab;
+
+	/*
+	 * Print a symbol name or address as necessary.
+	 */
+	kdbnearsym(addr, &symtab);
+	if (symtab.sym_name) {
+		/* Do not use kdb_symbol_print here, it always does
+		 * kdb_printf but we want dip->fprintf_func.
+		 */
+		dip->fprintf_func(dip->stream,
+			"0x%0*lx %s",
+			2*sizeof(addr), addr, symtab.sym_name);
+		/* Add offset if needed.  Pad output with blanks to get
+		 * consistent size symbols for disassembly listings.
+		 */
+		if (addr == symtab.sym_start) {
+			if (!flag)
+				dip->fprintf_func(dip->stream, "         ");
+		} else {
+			int len, i;
+			char buf[20];
+			sprintf(buf, "%lx", addr - symtab.sym_start);
+			dip->fprintf_func(dip->stream, "+0x%s", buf);
+			if (!flag) {
+				len = strlen(buf);
+				for (i = len; i < 6; i++)
+					dip->fprintf_func(dip->stream, " ");
+			}
+		}
+
+	} else {
+		dip->fprintf_func(dip->stream, "0x%0*lx", 2*sizeof(addr), addr);
+	}
+
+	if (flag)
+		dip->fprintf_func(dip->stream, ":   ");
+}
+
+/*
+ * kdba_dis_printaddr
+ *
+ *	Print (symbolically) an address.  Called by GNU disassembly
+ *	code via disassemble_info structure.
+ *
+ * Parameters:
+ *	addr	Address for which to get symbol
+ *	dip	Pointer to disassemble_info
+ * Returns:
+ *	number of chars printed.
+ * Locking:
+ * Remarks:
+ *	This function will never append ":<tab>" to the printed
+ *	symbolic address.
+ */
+
+static void
+kdba_dis_printaddr(bfd_vma addr, disassemble_info *dip)
+{
+	return kdba_printaddress(addr, dip, 0);
+}
+
+/*
+ * kdba_dis_getmem
+ *
+ *	Fetch 'length' bytes from 'addr' into 'buf'.
+ *
+ * Parameters:
+ *	addr	Address for which to get symbol
+ *	buf	Address of buffer to fill with bytes from 'addr'
+ *	length	Number of bytes to fetch
+ *	dip	Pointer to disassemble_info
+ * Returns:
+ *	0
+ * Locking:
+ * Remarks:
+ *
+ */
+extern int kdba_getword(unsigned long addr, size_t width);
+
+
+/* ARGSUSED */
+static int
+kdba_dis_getmem(bfd_vma addr, bfd_byte *buf, unsigned int length, disassemble_info *dip)
+{
+	bfd_byte	*bp = buf;
+	int		i;
+
+	/*
+	 * Fill the provided buffer with bytes from
+	 * memory, starting at address 'addr' for 'length bytes.
+	 *
+	 */
+
+	for(i=0; i<length; i++ ){
+		*bp++ = (bfd_byte)kdba_getword(addr++, sizeof(bfd_byte));
+	}
+
+	return 0;
+}
+
+/*
+ * kdba_id_parsemode
+ *
+ * 	Parse IDMODE environment variable string and
+ *	set appropriate value into "disassemble_info" structure.
+ *
+ * Parameters:
+ *	mode	Mode string
+ *	dip	Disassemble_info structure pointer
+ * Returns:
+ * Locking:
+ * Remarks:
+ *	We handle the values 'x86' and '8086' to enable either
+ *	32-bit instruction set or 16-bit legacy instruction set.
+ */
+
+int
+kdba_id_parsemode(const char *mode, disassemble_info *dip)
+{
+
+
+	return 0;
+}
+
+/*
+ * kdba_check_pc
+ *
+ * 	Check that the pc is satisfactory.
+ *
+ * Parameters:
+ *	pc	Program Counter Value.
+ * Returns:
+ *	None
+ * Locking:
+ *	None.
+ * Remarks:
+ *	Can change pc.
+ */
+
+void
+kdba_check_pc(kdb_machreg_t *pc)
+{
+	/* No action */
+}
+
+/*
+ * kdba_id_printinsn
+ *
+ * 	Format and print a single instruction at 'pc'. Return the
+ *	length of the instruction.
+ *
+ * Parameters:
+ *	pc	Program Counter Value.
+ *	dip	Disassemble_info structure pointer
+ * Returns:
+ *	Length of instruction, -1 for error.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	Depends on 'IDMODE' environment variable.
+ */
+
+int
+kdba_id_printinsn(kdb_machreg_t pc, disassemble_info *dip)
+{
+	kdba_dis_printaddr(pc, dip);
+	return print_insn_big_powerpc(pc, dip);
+}
+
+/*
+ * kdba_id_init
+ *
+ * 	Initialize the architecture dependent elements of
+ *	the disassembly information structure
+ *	for the GNU disassembler.
+ *
+ * Parameters:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ */
+
+void __init
+kdba_id_init(disassemble_info *dip)
+{
+	dip->read_memory_func       = kdba_dis_getmem;
+	dip->print_address_func     = kdba_dis_printaddr;
+	dip->symbol_at_address_func = kdba_dis_getsym;
+
+	dip->flavour                = bfd_target_elf_flavour;
+	dip->arch		    = bfd_arch_powerpc;
+	dip->mach		    = bfd_mach_ppc_750;
+	dip->endian	    	    = BFD_ENDIAN_BIG;
+
+	dip->display_endian         = BFD_ENDIAN_BIG;
+}
diff -purN linux-2.5/arch/ppc64/kdb/kdba_io.c linuxppc64-2.5/arch/ppc64/kdb/kdba_io.c
--- linux-2.5/arch/ppc64/kdb/kdba_io.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/kdba_io.c	2003-11-26 17:14:34.000000000 +0000
@@ -0,0 +1,101 @@
+/*
+ * Kernel Debugger Console I/O handler
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *	Chuck Fleckenstein		1999/07/20
+ *		Move kdb_info struct declaration to this file
+ *		for cases where serial support is not compiled into
+ *		the kernel.
+ *
+ *	Masahiro Adegawa		1999/07/20
+ *		Handle some peculiarities of japanese 86/106
+ *		keyboards.
+ *
+ *	marc@mucom.co.il		1999/07/20
+ *		Catch buffer overflow for serial input.
+ *
+ *      Scott Foehner
+ *              Port to ia64
+ *
+ *	Scott Lurndal			2000/01/03
+ *		Restructure for v1.0
+ *
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ *
+ *	Andi Kleen			2000/03/19
+ *		Support simultaneous input from serial line and keyboard.
+ */
+
+#include <linux/kernel.h>
+#include <asm/io.h>
+#include <linux/wait.h>
+#include <linux/delay.h>
+#include <linux/console.h>
+#include <linux/ctype.h>
+#include <linux/keyboard.h>
+#include <linux/serial_reg.h>
+
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+#include <asm/machdep.h>
+#undef FILE
+
+int kdb_port;
+
+struct kdb_serial kdb_serial;
+/*{
+	int io_type;
+	unsigned long iobase;
+	unsigned long ioreg_shift;
+} kdb_serial_t;
+*/
+
+int inchar(void);
+
+
+char *
+kdba_read(char *buffer, size_t bufsize)
+{
+	char	*cp = buffer;
+	char	*bufend = buffer+bufsize-2;	/* Reserve space for newline and null byte */
+
+	for (;;) {
+	    unsigned char key = ppc_md.udbg_getc();
+		/* Echo is done in the low level functions */
+		switch (key) {
+		case '\b': /* backspace */
+		case '\x7f': /* delete */
+			if (cp > buffer) {
+				udbg_puts("\b \b");
+				--cp;
+			}
+			break;
+		case '\n': /* enter */
+		case '\r': /* - the other enter... */
+			ppc_md.udbg_putc('\n');
+			*cp++ = '\n';
+			*cp++ = '\0';
+			return buffer;
+		case '\x00': /* check for NULL from udbg_getc */
+		        break;
+		default:
+			if (cp < bufend)
+			ppc_md.udbg_putc(key);
+				*cp++ = key;
+			break;
+		}
+	}
+}
+
+
+
diff -purN linux-2.5/arch/ppc64/kdb/kdbasupport.c linuxppc64-2.5/arch/ppc64/kdb/kdbasupport.c
--- linux-2.5/arch/ppc64/kdb/kdbasupport.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/kdbasupport.c	2003-11-26 17:14:34.000000000 +0000
@@ -0,0 +1,2052 @@
+/*
+ * Kernel Debugger Architecture Independent Support Functions
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *	Scott Lurndal			1999/12/12
+ *		v1.0 restructuring.
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ */
+
+#include <linux/string.h>
+#include <linux/stddef.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/ptrace.h>
+#include <linux/mm.h>
+#include <linux/sched.h>
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+
+#include <asm/processor.h>
+#include "privinst.h"
+#include <asm/uaccess.h>
+#include <asm/machdep.h>
+
+extern const char *kdb_diemsg;
+unsigned long cpus_in_kdb=0;
+volatile unsigned long kdb_do_reboot=0;
+
+/* prototypes */
+int valid_ppc64_kernel_address(unsigned long addr, unsigned long size);
+int kdba_excprint(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+int kdba_super_regs(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+int kdba_dissect_msr(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+int kdba_halt(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+int kdba_dump_tce_table(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+int kdba_kernelversion(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+int kdba_dmesg(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+int kdba_dump_pci_info(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+int kdba_rd(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+int kdba_bt(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+unsigned long kdba_getword(unsigned long addr, size_t width);
+
+
+extern int kdb_dmesg(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+extern int kdb_ps(int argc, const char **argv, const char **envp, struct pt_regs *regs);
+
+extern int kdb_parse(const char *cmdstr, struct pt_regs *regs);
+
+/* 60secs * 1000*1000 usecs/sec.  HMC interface requires larger amount of time,.. */
+#define KDB_RESET_TIMEOUT 60*1000*1000
+
+/* kdb will use UDBG */
+#define USE_UDBG
+
+#ifdef USE_UDBG
+#include <asm/udbg.h>
+#endif
+
+#include <linux/kbd_kern.h>
+#include <linux/sysrq.h>
+#include <linux/interrupt.h>
+
+#ifdef CONFIG_MAGIC_SYSRQ
+static void
+sysrq_handle_kdb(int key, struct pt_regs *pt_regs, struct kbd_struct *kbd, struct tty_struct *tty) 
+{
+  kdb(KDB_REASON_KEYBOARD,0,pt_regs);
+}
+
+static struct sysrq_key_op sysrq_kdb_op = 
+{
+	handler:	(void*)sysrq_handle_kdb,
+	help_msg:	"(x)kdb",
+	action_msg:	"Entering kdb\n",
+};
+
+void
+kdb_map_scc(void)
+{
+	/* register sysrq 'x' */
+	__sysrq_put_key_op('x', &sysrq_kdb_op);
+}
+#endif
+
+
+/*
+ * kdba_prologue
+ *
+ *	This function analyzes a gcc-generated function prototype
+ *	with or without frame pointers to determine the amount of
+ *	automatic storage and register save storage is used on the
+ *	stack of the target function.  It only counts instructions
+ *	that have been executed up to but excluding the current nip.
+ * Inputs:
+ *	code	Start address of function code to analyze
+ *	pc	Current program counter within function
+ *	sp	Current stack pointer for function
+ *	fp	Current frame pointer for function, may not be valid
+ *	ss	Start of stack for current process.
+ *	caller	1 if looking for data on the caller frame, 0 for callee.
+ * Outputs:
+ *	ar	Activation record, all fields may be set.  fp and oldfp
+ *		are 0 if they cannot be extracted.  return is 0 if the
+ *		code cannot find a valid return address.  args and arg0
+ *		are 0 if the number of arguments cannot be safely
+ *		calculated.
+ * Returns:
+ *	1 if prologue is valid, 0 otherwise.  If pc is 0 treat it as a
+ *	valid prologue to allow bt on wild branches.
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ */
+int
+kdba_prologue(const kdb_symtab_t *symtab, kdb_machreg_t pc, kdb_machreg_t sp,
+	      kdb_machreg_t fp, kdb_machreg_t ss, int caller, kdb_ar_t *ar)
+{
+	/* We don't currently use kdb's generic activation record scanning
+	 * code to handle backtrace.
+	 */
+	return 0;
+}
+
+
+
+/*
+ * kdba_getregcontents
+ *
+ *	Return the contents of the register specified by the
+ *	input string argument.   Return an error if the string
+ *	does not match a machine register.
+ *
+ *	The following pseudo register names are supported:
+ *	   &regs	 - Prints address of exception frame
+ *	   kesp		 - Prints kernel stack pointer at time of fault
+ *	   cesp		 - Prints current kernel stack pointer, inside kdb
+ *	   ceflags	 - Prints current flags, inside kdb
+ *	   %<regname>	 - Uses the value of the registers at the
+ *			   last time the user process entered kernel
+ *			   mode, instead of the registers at the time
+ *			   kdb was entered.
+ *
+ * Parameters:
+ *	regname		Pointer to string naming register
+ *	regs		Pointer to structure containing registers.
+ * Outputs:
+ *	*contents	Pointer to unsigned long to recieve register contents
+ * Returns:
+ *	0		Success
+ *	KDB_BADREG	Invalid register name
+ * Locking:
+ * 	None.
+ * Remarks:
+ * 	If kdb was entered via an interrupt from the kernel itself then
+ *	ss and esp are *not* on the stack.
+ */
+
+static struct kdbregs {
+	char   *reg_name;
+	size_t	reg_offset;
+} kdbreglist[] = {
+	{ "gpr0",	offsetof(struct pt_regs, gpr[0]) },
+	{ "gpr1",	offsetof(struct pt_regs, gpr[1]) },
+	{ "gpr2",	offsetof(struct pt_regs, gpr[2]) },
+	{ "gpr3",	offsetof(struct pt_regs, gpr[3]) },
+	{ "gpr4",	offsetof(struct pt_regs, gpr[4]) },
+	{ "gpr5",	offsetof(struct pt_regs, gpr[5]) },
+	{ "gpr6",	offsetof(struct pt_regs, gpr[6]) },
+	{ "gpr7",	offsetof(struct pt_regs, gpr[7]) },
+	{ "gpr8",	offsetof(struct pt_regs, gpr[8]) },
+	{ "gpr9",	offsetof(struct pt_regs, gpr[9]) },
+	{ "gpr10",	offsetof(struct pt_regs, gpr[10]) },
+	{ "gpr11",	offsetof(struct pt_regs, gpr[11]) },
+	{ "gpr12",	offsetof(struct pt_regs, gpr[12]) },
+	{ "gpr13",	offsetof(struct pt_regs, gpr[13]) },
+	{ "gpr14",	offsetof(struct pt_regs, gpr[14]) },
+	{ "gpr15",	offsetof(struct pt_regs, gpr[15]) },
+	{ "gpr16",	offsetof(struct pt_regs, gpr[16]) },
+	{ "gpr17",	offsetof(struct pt_regs, gpr[17]) },
+	{ "gpr18",	offsetof(struct pt_regs, gpr[18]) },
+	{ "gpr19",	offsetof(struct pt_regs, gpr[19]) },
+	{ "gpr20",	offsetof(struct pt_regs, gpr[20]) },
+	{ "gpr21",	offsetof(struct pt_regs, gpr[21]) },
+	{ "gpr22",	offsetof(struct pt_regs, gpr[22]) },
+	{ "gpr23",	offsetof(struct pt_regs, gpr[23]) },
+	{ "gpr24",	offsetof(struct pt_regs, gpr[24]) },
+	{ "gpr25",	offsetof(struct pt_regs, gpr[25]) },
+	{ "gpr26",	offsetof(struct pt_regs, gpr[26]) },
+	{ "gpr27",	offsetof(struct pt_regs, gpr[27]) },
+	{ "gpr28",	offsetof(struct pt_regs, gpr[28]) },
+	{ "gpr29",	offsetof(struct pt_regs, gpr[29]) },
+	{ "gpr30",	offsetof(struct pt_regs, gpr[30]) },
+	{ "gpr31",	offsetof(struct pt_regs, gpr[31]) },
+	{ "nip",	offsetof(struct pt_regs, nip) },
+	{ "msr",	offsetof(struct pt_regs, msr) },
+	{ "esp",	offsetof(struct pt_regs, gpr[1]) },
+  	{ "orig_gpr3",  offsetof(struct pt_regs, orig_gpr3) },
+	{ "ctr", 	offsetof(struct pt_regs, ctr) },
+	{ "link",	offsetof(struct pt_regs, link) },
+	{ "xer", 	offsetof(struct pt_regs, xer) },
+	{ "ccr",	offsetof(struct pt_regs, ccr) },
+	{ "mq",		offsetof(struct pt_regs, softe) /* mq */ },
+	{ "trap",	offsetof(struct pt_regs, trap) },
+	{ "dar",	offsetof(struct pt_regs, dar)  },
+	{ "dsisr",	offsetof(struct pt_regs, dsisr) },
+	{ "result",	offsetof(struct pt_regs, result) },
+};
+
+static const int nkdbreglist = sizeof(kdbreglist) / sizeof(struct kdbregs);
+
+unsigned long
+getsp(void)
+{
+	unsigned long x;
+	asm("mr %0,1" : "=r" (x):);
+	return x;
+}
+
+int
+kdba_getregcontents(const char *regname,
+		    struct pt_regs *regs,
+		    kdb_machreg_t *contents)
+{
+	int i;
+
+	if (strcmp(regname, "&regs") == 0) {
+		*contents = (unsigned long)regs;
+		return 0;
+	}
+
+	if (strcmp(regname, "kesp") == 0) {
+		*contents = (unsigned long) current->thread.ksp;
+		return 0;
+	}
+
+	if (strcmp(regname, "cesp") == 0) {
+		*contents = getsp();
+		return 0;
+	}
+
+	if (strcmp(regname, "ceflags") == 0) {
+		long flags;
+		local_save_flags(flags);
+		*contents = flags;
+		return 0;
+	}
+
+	if (regname[0] == '%') {
+		/* User registers:  %%e[a-c]x, etc */
+		regname++;
+		regs = (struct pt_regs *)
+			(current->thread.ksp - sizeof(struct pt_regs));
+	}
+
+	for (i=0; i<nkdbreglist; i++) {
+		if (strnicmp(kdbreglist[i].reg_name,
+			     regname,
+			     strlen(regname)) == 0)
+			break;
+	}
+
+	if ((i < nkdbreglist)
+	 && (strlen(kdbreglist[i].reg_name) == strlen(regname))) {
+		*contents = *(unsigned long *)((unsigned long)regs +
+				kdbreglist[i].reg_offset);
+		return(0);
+	}
+
+	return KDB_BADREG;
+}
+
+/*
+ * kdba_setregcontents
+ *
+ *	Set the contents of the register specified by the
+ *	input string argument.   Return an error if the string
+ *	does not match a machine register.
+ *
+ *	Supports modification of user-mode registers via
+ *	%<register-name>
+ *
+ * Parameters:
+ *	regname		Pointer to string naming register
+ *	regs		Pointer to structure containing registers.
+ *	contents	Unsigned long containing new register contents
+ * Outputs:
+ * Returns:
+ *	0		Success
+ *	KDB_BADREG	Invalid register name
+ * Locking:
+ * 	None.
+ * Remarks:
+ */
+
+int
+kdba_setregcontents(const char *regname,
+		  struct pt_regs *regs,
+		  unsigned long contents)
+{
+	int i;
+
+	if (regname[0] == '%') {
+		regname++;
+		regs = (struct pt_regs *)
+			(current->thread.ksp - sizeof(struct pt_regs));
+	}
+
+	for (i=0; i<nkdbreglist; i++) {
+		if (strnicmp(kdbreglist[i].reg_name,
+			     regname,
+			     strlen(regname)) == 0)
+			break;
+	}
+
+	if ((i < nkdbreglist)
+	 && (strlen(kdbreglist[i].reg_name) == strlen(regname))) {
+		*(unsigned long *)((unsigned long)regs
+				   + kdbreglist[i].reg_offset) = contents;
+		return 0;
+	}
+
+	return KDB_BADREG;
+}
+
+/*
+ * kdba_dumpregs
+ *
+ *	Dump the specified register set to the display.
+ *
+ * Parameters:
+ *	regs		Pointer to structure containing registers.
+ *	type		Character string identifying register set to dump
+ *	extra		string further identifying register (optional)
+ * Outputs:
+ * Returns:
+ *	0		Success
+ * Locking:
+ * 	None.
+ * Remarks:
+ *	This function will dump the general register set if the type
+ *	argument is NULL (struct pt_regs).   The alternate register
+ *	set types supported by this function:
+ *
+ *	d 		Debug registers
+ *	c		Control registers
+ *	u		User registers at most recent entry to kernel
+ * Following not yet implemented:
+ *	m		Model Specific Registers (extra defines register #)
+ *	r		Memory Type Range Registers (extra defines register)
+ */
+
+int
+kdba_dumpregs(struct pt_regs *regs,
+	    const char *type,
+	    const char *extra)
+{
+	int i;
+	int count = 0;
+
+	if (type
+	 && (type[0] == 'u')) {
+		type = NULL;
+		regs = (struct pt_regs *)
+			(current->thread.ksp - sizeof(struct pt_regs));
+	}
+
+	if (type == NULL) {
+		struct kdbregs *rlp;
+		kdb_machreg_t contents;
+
+		for (i=0, rlp=kdbreglist; i<nkdbreglist; i++,rlp++) {
+			kdba_getregcontents(rlp->reg_name, regs, &contents);
+			kdb_printf("%-5s = 0x%p%c", rlp->reg_name, (void *)contents, (++count % 2) ? ' ' : '\n');
+		}
+
+		kdb_printf("&regs = 0x%p\n", regs);
+		return 0;
+ 	} else {  /* dump a specific register */
+ 	    kdb_machreg_t contents;
+ 	    if (KDB_BADREG==kdba_getregcontents(type, regs, &contents)) 
+ 		kdb_printf("register %-5s not found \n",type);
+ 	    else
+ 		kdb_printf("%-5s = 0x%p%c", type, (void *)contents, '\n');
+ 	    return 0;
+	}
+
+	switch (type[0]) {
+	case 'm':
+		break;
+	case 'r':
+		break;
+	default:
+		return KDB_BADREG;
+	}
+
+	/* NOTREACHED */
+	return 0;
+}
+
+kdb_machreg_t
+kdba_getpc(kdb_eframe_t ef)
+{
+    return ef ? ef->nip : 0;
+}
+
+int
+kdba_setpc(kdb_eframe_t ef, kdb_machreg_t newpc)
+{
+/* for ppc64, newpc passed in is actually a function descriptor for kdb. */
+    ef->nip =     kdba_getword(newpc+8, 8);
+    KDB_STATE_SET(IP_ADJUSTED);
+    return 0;
+}
+
+/*
+ * kdba_main_loop
+ *
+ *	Do any architecture specific set up before entering the main kdb loop.
+ *	The primary function of this routine is to make all processes look the
+ *	same to kdb, kdb must be able to list a process without worrying if the
+ *	process is running or blocked, so make all process look as though they
+ *	are blocked.
+ *
+ * Inputs:
+ *	reason		The reason KDB was invoked
+ *	error		The hardware-defined error code
+ *	error2		kdb's current reason code.  Initially error but can change
+ *			acording to kdb state.
+ *	db_result	Result from break or debug point.
+ *	ef		The exception frame at time of fault/breakpoint.  If reason
+ *			is KDB_REASON_SILENT then ef is NULL, otherwise it should
+ *			always be valid.
+ * Returns:
+ *	0	KDB was invoked for an event which it wasn't responsible
+ *	1	KDB handled the event for which it was invoked.
+ * Outputs:
+ *	Sets nip and esp in current->thread.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	none.
+ */
+
+int
+kdba_main_loop(kdb_reason_t reason, kdb_reason_t reason2, int error,
+	       kdb_dbtrap_t db_result, kdb_eframe_t ef)
+{
+	int rv;
+	kdb_do_reboot=0;
+
+	/* case where incoming registers are missing */
+	if (ef == NULL)
+	{
+		struct pt_regs regs;
+		asm volatile ("std	0,0(%0)\n\
+                               std	1,8(%0)\n\
+                               std	2,16(%0)\n\
+                               std	3,24(%0)\n\
+                               std	4,32(%0)\n\
+                               std	5,40(%0)\n\
+                               std	6,48(%0)\n\
+                               std	7,56(%0)\n\
+                               std	8,64(%0)\n\
+                               std	9,72(%0)\n\
+                               std	10,80(%0)\n\
+                               std	11,88(%0)\n\
+                               std	12,96(%0)\n\
+                               std	13,104(%0)\n\
+                               std	14,112(%0)\n\
+                               std	15,120(%0)\n\
+                               std	16,128(%0)\n\
+                               std	17,136(%0)\n\
+                               std	18,144(%0)\n\
+                               std	19,152(%0)\n\
+                               std	20,160(%0)\n\
+                               std	21,168(%0)\n\
+                               std	22,176(%0)\n\
+                               std	23,184(%0)\n\
+                               std	24,192(%0)\n\
+                               std	25,200(%0)\n\
+                               std	26,208(%0)\n\
+                               std	27,216(%0)\n\
+                               std	28,224(%0)\n\
+                               std	29,232(%0)\n\
+                               std	30,240(%0)\n\
+                               std	31,248(%0)" : : "b" (&regs));
+                /* one extra step back..  this frame disappears */
+		regs.gpr[1] = kdba_getword(regs.gpr[1], 8);
+		/* Fetch the link reg for this stack frame.
+		 NOTE: the prev kdb_printf fills in the lr. */
+		regs.nip = regs.link = ((unsigned long *)regs.gpr[1])[2];
+		regs.msr = get_msr();
+		regs.ctr = get_ctr();
+		regs.xer = get_xer();
+		regs.ccr = get_cr();
+		regs.trap = 0;
+		/*current->thread.regs = &regs; */
+		ef = &regs;
+	}
+	cpus_in_kdb++;
+	rv = kdb_main_loop(reason, reason2, error, db_result, ef);
+	cpus_in_kdb--;
+	return rv;
+}
+
+void
+kdba_disableint(kdb_intstate_t *state)
+{
+	unsigned long *fp = (unsigned long *)state;
+	unsigned long flags;
+	local_irq_save(flags);
+	*fp = flags;
+}
+
+void
+kdba_restoreint(kdb_intstate_t *state)
+{
+	unsigned long flags = *(unsigned long *)state;
+	local_irq_restore(flags);
+}
+
+void
+kdba_setsinglestep(struct pt_regs *regs)
+{
+	regs->msr |= MSR_SE;
+}
+
+void
+kdba_clearsinglestep(struct pt_regs *regs)
+{
+	
+	regs->msr &= ~MSR_SE;
+}
+
+int
+kdba_getcurrentframe(struct pt_regs *regs)
+{
+	regs->gpr[1] = getsp();
+	/* this stack pointer becomes invalid after we return, so take another step back.  */
+	regs->gpr[1] = kdba_getword(regs->gpr[1], 8);
+	return 0;
+}
+
+#ifdef KDB_HAVE_LONGJMP
+int
+kdba_setjmp(kdb_jmp_buf *buf)
+{
+    asm volatile (
+	"mflr 0; std 0,0(%0)\n\
+	 std	1,8(%0)\n\
+	 std	2,16(%0)\n\
+	 mfcr 0; std 0,24(%0)\n\
+	 std	13,32(%0)\n\
+	 std	14,40(%0)\n\
+	 std	15,48(%0)\n\
+	 std	16,56(%0)\n\
+	 std	17,64(%0)\n\
+	 std	18,72(%0)\n\
+	 std	19,80(%0)\n\
+	 std	20,88(%0)\n\
+	 std	21,96(%0)\n\
+	 std	22,104(%0)\n\
+	 std	23,112(%0)\n\
+	 std	24,120(%0)\n\
+	 std	25,128(%0)\n\
+	 std	26,136(%0)\n\
+	 std	27,144(%0)\n\
+	 std	28,152(%0)\n\
+	 std	29,160(%0)\n\
+	 std	30,168(%0)\n\
+	 std	31,176(%0)\n\
+	 " : : "r" (buf));
+    KDB_STATE_SET(LONGJMP);
+    return 0;
+}
+
+void
+kdba_longjmp(kdb_jmp_buf *buf, int val)
+{
+    if (val == 0)
+	val = 1;
+    asm volatile (
+	"ld	13,32(%0)\n\
+	 ld	14,40(%0)\n\
+	 ld	15,48(%0)\n\
+	 ld	16,56(%0)\n\
+	 ld	17,64(%0)\n\
+	 ld	18,72(%0)\n\
+	 ld	19,80(%0)\n\
+	 ld	20,88(%0)\n\
+	 ld	21,96(%0)\n\
+	 ld	22,104(%0)\n\
+	 ld	23,112(%0)\n\
+	 ld	24,120(%0)\n\
+	 ld	25,128(%0)\n\
+	 ld	26,136(%0)\n\
+	 ld	27,144(%0)\n\
+	 ld	28,152(%0)\n\
+	 ld	29,160(%0)\n\
+	 ld	30,168(%0)\n\
+	 ld	31,176(%0)\n\
+	 ld	0,24(%0)\n\
+	 mtcrf	0x38,0\n\
+	 ld	0,0(%0)\n\
+	 ld	1,8(%0)\n\
+	 ld	2,16(%0)\n\
+	 mtlr	0\n\
+	 mr	3,%1\n\
+	 " : : "r" (buf), "r" (val));
+}
+#endif
+
+/*
+ * kdba_enable_mce
+ *
+ *	This function is called once on each CPU to enable machine
+ *	check exception handling.
+ *
+ * Inputs:
+ *	None.
+ * Outputs:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ */
+
+void
+kdba_enable_mce(void)
+{
+}
+
+/*
+ * kdba_enable_lbr
+ *
+ *	Enable last branch recording.
+ *
+ * Parameters:
+ *	None.
+ * Returns:
+ *	None
+ * Locking:
+ *	None
+ * Remarks:
+ *	None.
+ */
+
+void
+kdba_enable_lbr(void)
+{
+}
+
+/*
+ * kdba_disable_lbr
+ *
+ *	disable last branch recording.
+ *
+ * Parameters:
+ *	None.
+ * Returns:
+ *	None
+ * Locking:
+ *	None
+ * Remarks:
+ *	None.
+ */
+
+void
+kdba_disable_lbr(void)
+{
+}
+
+/*
+ * kdba_print_lbr
+ *
+ *	Print last branch and last exception addresses
+ *
+ * Parameters:
+ *	None.
+ * Returns:
+ *	None
+ * Locking:
+ *	None
+ * Remarks:
+ *	None.
+ */
+
+void
+kdba_print_lbr(void)
+{
+}
+
+/*
+ * kdba_getword
+ *
+ * 	Architecture specific function to access kernel virtual
+ *	address space.
+ *
+ * Parameters:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	None.
+ */
+
+/* 	if (access_ok(VERIFY_READ,__gu_addr,size))			\ */
+ 
+extern inline void sync(void)
+{
+	asm volatile("sync; isync");
+}
+
+extern void (*debugger_fault_handler)(struct pt_regs *);
+extern void longjmp(u_int *, int);
+
+unsigned long
+kdba_getword(unsigned long addr, size_t width)
+{
+	/*
+	 * This function checks the address for validity.  Any address
+	 * in the range PAGE_OFFSET to high_memory is legal, any address
+	 * which maps to a vmalloc region is legal, and any address which
+	 * is a user address, we use get_user() to verify validity.
+	 */
+
+    if (!valid_ppc64_kernel_address(addr, width)) {
+		        /*
+			 * Would appear to be an illegal kernel address;
+			 * Print a message once, and don't print again until
+			 * a legal address is used.
+			 */
+			if (!KDB_STATE(SUPPRESS)) {
+				kdb_printf("    kdb: Not a kernel-space address 0x%lx \n",addr);
+				KDB_STATE_SET(SUPPRESS);
+			}
+			return 0L;
+	}
+
+
+	/*
+	 * A good address.  Reset error flag.
+	 */
+	KDB_STATE_CLEAR(SUPPRESS);
+
+	switch (width) {
+	case 8:
+	{	unsigned long *lp;
+
+		lp = (unsigned long *)(addr);
+		return *lp;
+	}
+	case 4:
+	{	unsigned int *ip;
+
+		ip = (unsigned int *)(addr);
+		return *ip;
+	}
+	case 2:
+	{	unsigned short *sp;
+
+		sp = (unsigned short *)(addr);
+		return *sp;
+	}
+	case 1:
+	{	unsigned char *cp;
+
+		cp = (unsigned char *)(addr);
+		return *cp;
+	}
+	}
+
+	kdb_printf("kdbgetword: Bad width\n");
+	return 0L;
+}
+
+
+
+/*
+ * kdba_putword
+ *
+ * 	Architecture specific function to access kernel virtual
+ *	address space.
+ *
+ * Parameters:
+ *	None.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	None.
+ */
+
+unsigned long
+kdba_putword(unsigned long addr, size_t size, unsigned long contents)
+{
+	/*
+	 * This function checks the address for validity.  Any address
+	 * in the range PAGE_OFFSET to high_memory is legal, any address
+	 * which maps to a vmalloc region is legal, and any address which
+	 * is a user address, we use get_user() to verify validity.
+	 */
+
+	if (addr < PAGE_OFFSET) {
+		/*
+		 * Usermode address.
+		 */
+		unsigned long diag;
+
+		switch (size) {
+		case 4:
+		{	unsigned long *lp;
+
+			lp = (unsigned long *) addr;
+			diag = put_user(contents, lp);
+			break;
+		}
+		case 2:
+		{	unsigned short *sp;
+
+			sp = (unsigned short *) addr;
+			diag = put_user(contents, sp);
+			break;
+		}
+		case 1:
+		{	unsigned char *cp;
+
+			cp = (unsigned char *) addr;
+			diag = put_user(contents, cp);
+			break;
+		}
+		default:
+			kdb_printf("kdba_putword: Bad width\n");
+			return 0;
+		}
+
+		if (diag) {
+			if (!KDB_STATE(SUPPRESS)) {
+				kdb_printf("kdb: Bad user address 0x%lx\n", addr);
+				KDB_STATE_SET(SUPPRESS);
+			}
+			return 0;
+		}
+		KDB_STATE_CLEAR(SUPPRESS);
+		return 0;
+	}
+
+#if 0
+	if (addr > (unsigned long)high_memory) {
+		if (!kdb_vmlist_check(addr, addr+size)) {
+			/*
+			 * Would appear to be an illegal kernel address;
+			 * Print a message once, and don't print again until
+			 * a legal address is used.
+			 */
+			if (!KDB_STATE(SUPPRESS)) {
+				kdb_printf("kdb: xx Bad kernel address 0x%lx\n", addr);
+				KDB_STATE_SET(SUPPRESS);
+			}
+			return 0L;
+		}
+	}
+#endif
+
+	/*
+	 * A good address.  Reset error flag.
+	 */
+	KDB_STATE_CLEAR(SUPPRESS);
+
+	switch (size) {
+	case 4:
+	{	unsigned long *lp;
+
+		lp = (unsigned long *)(addr);
+		*lp = contents;
+		return 0;
+	}
+	case 2:
+	{	unsigned short *sp;
+
+		sp = (unsigned short *)(addr);
+		*sp = (unsigned short) contents;
+		return 0;
+	}
+	case 1:
+	{	unsigned char *cp;
+
+		cp = (unsigned char *)(addr);
+		*cp = (unsigned char) contents;
+		return 0;
+	}
+	}
+
+	kdb_printf("kdba_putword: Bad width 0x%lx\n",size);
+	return 0;
+}
+
+/*
+ * kdba_callback_die
+ *
+ *	Callback function for kernel 'die' function.
+ *
+ * Parameters:
+ *	regs	Register contents at time of trap
+ *	error_code  Trap-specific error code value
+ *	trapno	Trap number
+ *	vp	Pointer to die message
+ * Returns:
+ *	Returns 1 if fault handled by kdb.
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ */
+int
+kdba_callback_die(struct pt_regs *regs, int error_code, long trapno, void *vp)
+{
+	/*
+	 * Save a pointer to the message provided to 'die()'.
+	 */
+	kdb_diemsg = (char *)vp;
+
+	return kdb(KDB_REASON_OOPS, error_code, (kdb_eframe_t) regs);
+}
+
+/*
+ * kdba_callback_bp
+ *
+ *	Callback function for kernel breakpoint trap.
+ *
+ * Parameters:
+ *	regs	Register contents at time of trap
+ *	error_code  Trap-specific error code value
+ *	trapno	Trap number
+ *	vp	Not Used.
+ * Returns:
+ *	Returns 1 if fault handled by kdb.
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ */
+
+int
+kdba_callback_bp(struct pt_regs *regs, int error_code, long trapno, void *vp)
+{
+	int diag;
+
+	if (KDB_DEBUG(BP))
+		kdb_printf("cb_bp: e_c = %d  tn = %ld regs = 0x%p\n", error_code,
+			   trapno, regs);
+
+	diag = kdb(KDB_REASON_BREAK, error_code, (kdb_eframe_t) regs);
+
+	if (KDB_DEBUG(BP))
+		kdb_printf("cb_bp: e_c = %d  tn = %ld regs = 0x%p diag = %d\n", error_code,
+			   trapno, regs, diag);
+	return diag;
+}
+
+/*
+ * kdba_callback_debug
+ *
+ *	Callback function for kernel debug register trap.
+ *
+ * Parameters:
+ *	regs	Register contents at time of trap
+ *	error_code  Trap-specific error code value
+ *	trapno	Trap number
+ *	vp	Not used.
+ * Returns:
+ *	Returns 1 if fault handled by kdb.
+ * Locking:
+ *	None.
+ * Remarks:
+ *
+ */
+
+int
+kdba_callback_debug(struct pt_regs *regs, int error_code, long trapno, void *vp)
+{
+	return kdb(KDB_REASON_DEBUG, error_code, (kdb_eframe_t) regs);
+}
+
+
+
+
+/*
+ * kdba_adjust_ip
+ *
+ * 	Architecture specific adjustment of instruction pointer before leaving
+ *	kdb.
+ *
+ * Parameters:
+ *	reason		The reason KDB was invoked
+ *	error		The hardware-defined error code
+ *	ef		The exception frame at time of fault/breakpoint.  If reason
+ *			is KDB_REASON_SILENT then ef is NULL, otherwise it should
+ *			always be valid.
+ * Returns:
+ *	None.
+ * Locking:
+ *	None.
+ * Remarks:
+ *	noop on ix86.
+ */
+
+void
+kdba_adjust_ip(kdb_reason_t reason, int error, kdb_eframe_t ef)
+{
+	return;
+}
+
+
+
+/*
+ * kdba_find_tb_table
+ *
+ * 	Find the traceback table (defined by the ELF64 ABI) located at
+ *	the end of the function containing pc.
+ *
+ * Parameters:
+ *	nip	starting instruction addr.  does not need to be at the start of the func.
+ *	tab	table to populate if successful
+ * Returns:
+ *	non-zero if successful.  unsuccessful means that a valid tb table was not found
+ * Locking:
+ *	None.
+ * Remarks:
+ *	None.
+ */
+int kdba_find_tb_table(kdb_machreg_t nip, kdbtbtable_t *tab)
+{
+	kdb_machreg_t codeaddr = nip;
+	kdb_machreg_t codeaddr_max;
+	kdb_machreg_t tbtab_start;
+	int instr;
+	int num_parms;
+
+	if (tab == NULL)
+		return 0;
+	memset(tab, 0, sizeof(tab));
+
+	if (nip < PAGE_OFFSET) {  /* this is gonna fail for userspace, at least for now.. */
+	    return 0;
+	}
+
+	/* Scan instructions starting at codeaddr for 128k max */
+	for (codeaddr_max = codeaddr + 128*1024*4;
+	     codeaddr < codeaddr_max;
+	     codeaddr += 4) {
+		instr = kdba_getword(codeaddr, 4);
+		if (instr == 0) {
+			/* table should follow. */
+			int version;
+			unsigned long flags;
+			tbtab_start = codeaddr;	/* save it to compute func start addr */
+			codeaddr += 4;
+			flags = kdba_getword(codeaddr, 8);
+			tab->flags = flags;
+			version = (flags >> 56) & 0xff;
+			if (version != 0)
+				continue;	/* No tb table here. */
+			/* Now, like the version, some of the flags are values
+			 that are more conveniently extracted... */
+			tab->fp_saved = (flags >> 24) & 0x3f;
+			tab->gpr_saved = (flags >> 16) & 0x3f;
+			tab->fixedparms = (flags >> 8) & 0xff;
+			tab->floatparms = (flags >> 1) & 0x7f;
+			codeaddr += 8;
+			num_parms = tab->fixedparms + tab->floatparms;
+			if (num_parms) {
+				unsigned int parminfo;
+				int parm;
+				if (num_parms > 32)
+					return 1;	/* incomplete */
+				parminfo = kdba_getword(codeaddr, 4);
+				/* decode parminfo...32 bits.
+				 A zero means fixed.  A one means float and the
+				 following bit determines single (0) or double (1).
+				 */
+				for (parm = 0; parm < num_parms; parm++) {
+					if (parminfo & 0x80000000) {
+						parminfo <<= 1;
+						if (parminfo & 0x80000000)
+							tab->parminfo[parm] = KDBTBTAB_PARMDFLOAT;
+						else
+							tab->parminfo[parm] = KDBTBTAB_PARMSFLOAT;
+					} else {
+						tab->parminfo[parm] = KDBTBTAB_PARMFIXED;
+					}
+					parminfo <<= 1;
+				}
+				codeaddr += 4;
+			}
+			if (flags & KDBTBTAB_FLAGSHASTBOFF) {
+				tab->tb_offset = kdba_getword(codeaddr, 4);
+				if (tab->tb_offset > 0) {
+					tab->funcstart = tbtab_start - tab->tb_offset;
+				}
+				codeaddr += 4;
+			}
+			/* hand_mask appears to be always be omitted. */
+			if (flags & KDBTBTAB_FLAGSHASCTL) {
+				/* Assume this will never happen for C or asm */
+				return 1;	/* incomplete */
+			}
+			if (flags & KDBTBTAB_FLAGSNAMEPRESENT) {
+				int i;
+				short namlen = kdba_getword(codeaddr, 2);
+				if (namlen >= sizeof(tab->name))
+					namlen = sizeof(tab->name)-1;
+				codeaddr += 2;
+				for (i = 0; i < namlen; i++) {
+					tab->name[i] = kdba_getword(codeaddr++, 1);
+				}
+				tab->name[namlen] = '\0';
+			}
+			/* Fake up a symtab entry in case the caller finds it useful */
+			tab->symtab.value = tab->symtab.sym_start = tab->funcstart;
+			tab->symtab.sym_name = tab->name;
+			tab->symtab.sym_end = tbtab_start;
+			return 1;
+		}
+	}
+	return 0;	/* hit max...sorry. */
+}
+
+int
+kdba_putarea_size(unsigned long to_xxx, void *from, size_t size)
+{
+    char c;
+    c = *((volatile char *)from);
+    c = *((volatile char *)from+size-1);
+    return __copy_to_user((void *)to_xxx,from,size);
+}
+
+
+
+/*
+ * valid_ppc64_kernel_address() returns '1' if the address passed in is
+ * within a valid range.  Function returns 0 if address is outside valid ranges.
+ */
+
+/*
+
+    KERNELBASE    c000000000000000
+        (good range)
+    high_memory   c0000000 20000000
+
+    VMALLOC_START d000000000000000
+        (good range)
+    VMALLOC_END   VMALLOC_START + VALID_EA_BITS  
+
+    IMALLOC_START e000000000000000
+        (good range)
+    IMALLOC_END   IMALLOC_START + VALID_EA_BITS
+
+*/
+
+int
+valid_ppc64_kernel_address(unsigned long addr, unsigned long size)
+{
+	unsigned long i;
+	unsigned long end = (addr + size - 1);	
+
+	int userspace_enabled=0;
+
+/* set USERSPACE=1 to enable userspace memory lookups*/
+	kdbgetintenv("USERSPACE", &userspace_enabled);	
+
+	for (i = addr; i <= end; i = i ++ ) {
+	    if (
+		(!userspace_enabled &&
+		 ((unsigned long)i < (unsigned long)KERNELBASE     ))  || 		
+		(((unsigned long)i > (unsigned long)high_memory) &&
+		 ((unsigned long)i < (unsigned long)VMALLOC_START) )  ||
+		(((unsigned long)i > (unsigned long)VMALLOC_END) &&
+		 ((unsigned long)i < (unsigned long)IMALLOC_START) )  ||
+		( (unsigned long)i > (unsigned long)IMALLOC_END    )       ) {
+		return 0;
+	    }
+	}
+	return 1;
+}
+
+
+int
+kdba_getarea_size(void *to, unsigned long from_xxx, size_t size)
+{
+	int is_valid_kern_addr = valid_ppc64_kernel_address(from_xxx, size);
+	int diag = 0;
+
+	*((volatile char *)to) = '\0';
+	*((volatile char *)to + size - 1) = '\0';
+
+	if (is_valid_kern_addr) {
+		memcpy(to, (void *)from_xxx, size);
+	} else {
+            /*  user space address, just return.  */
+	    diag = -1;
+	}
+
+	return diag;
+}
+
+
+
+/*
+ *  kdba_readarea_size, reads size-lump of memory into to* passed in, returns size.
+ * Making it feel a bit more like mread.. when i'm clearer on kdba end, probally will
+ * remove one of these.
+ */
+int
+kdba_readarea_size(unsigned long from_xxx,void *to, size_t size)
+{
+    int is_valid_kern_addr = valid_ppc64_kernel_address(from_xxx, size);
+
+    *((volatile char *)to) = '\0';
+    *((volatile char *)to + size - 1) = '\0';
+
+    if (is_valid_kern_addr) {
+	memcpy(to, (void *)from_xxx, size);
+	return size;
+    } else {
+	/*  user-space, just return...    */
+	return 0;
+    }
+    /* wont get here */
+    return 0;
+}
+
+
+/* utilities migrated from Xmon or other kernel debug tools. */
+
+/*
+Notes for migrating functions from xmon...
+Add functions to this file.  parmlist for functions must match
+   (int argc, const char **argv, const char **envp, struct pt_regs *fp)
+add function prototype to kdbasupport.c
+add function hook to kdba_init() within kdbasupport.c
+
+Common bits...
+mread() function calls need to be changed to kdba_readarea_size calls.  straightforward change.
+This:
+	nr = mread(codeaddr, &namlen, 2); 
+becomes this:
+	nr = kdba_readarea_size(codeaddr,&namlen,2);
+*/
+
+#define EOF	(-1)
+
+/* for traverse_all_pci_devices */
+#include "../kernel/pci.h"
+/* for NUM_TCE_LEVELS */
+#include <asm/pci_dma.h>
+
+
+/* prototypes */
+int scanhex(unsigned long *);
+int hexdigit(int c);
+/* int kdba_readarea_size(unsigned long from_xxx,void *to,  size_t size); */
+void machine_halt(void); 
+
+
+
+/*
+ A traceback table typically follows each function.
+ The find_tb_table() func will fill in this struct.  Note that the struct
+ is not an exact match with the encoded table defined by the ABI.  It is
+ defined here more for programming convenience.
+ */
+struct tbtable {
+	unsigned long	flags;		/* flags: */
+#define TBTAB_FLAGSGLOBALLINK	(1L<<47)
+#define TBTAB_FLAGSISEPROL	(1L<<46)
+#define TBTAB_FLAGSHASTBOFF	(1L<<45)
+#define TBTAB_FLAGSINTPROC	(1L<<44)
+#define TBTAB_FLAGSHASCTL	(1L<<43)
+#define TBTAB_FLAGSTOCLESS	(1L<<42)
+#define TBTAB_FLAGSFPPRESENT	(1L<<41)
+#define TBTAB_FLAGSNAMEPRESENT	(1L<<38)
+#define TBTAB_FLAGSUSESALLOCA	(1L<<37)
+#define TBTAB_FLAGSSAVESCR	(1L<<33)
+#define TBTAB_FLAGSSAVESLR	(1L<<32)
+#define TBTAB_FLAGSSTORESBC	(1L<<31)
+#define TBTAB_FLAGSFIXUP	(1L<<30)
+#define TBTAB_FLAGSPARMSONSTK	(1L<<0)
+	unsigned char	fp_saved;	/* num fp regs saved f(32-n)..f31 */
+	unsigned char	gpr_saved;	/* num gpr's saved */
+	unsigned char	fixedparms;	/* num fixed point parms */
+	unsigned char	floatparms;	/* num float parms */
+	unsigned char	parminfo[32];	/* types of args.  null terminated */
+#define TBTAB_PARMFIXED 1
+#define TBTAB_PARMSFLOAT 2
+#define TBTAB_PARMDFLOAT 3
+	unsigned int	tb_offset;	/* offset from start of func */
+	unsigned long	funcstart;	/* addr of start of function */
+	char		name[64];	/* name of function (null terminated)*/
+};
+
+
+static int find_tb_table(unsigned long codeaddr, struct tbtable *tab);
+
+
+/* Very cheap human name for vector lookup. */
+static
+const char *getvecname(unsigned long vec)
+{
+	char *ret;
+	switch (vec) {
+	case 0x100:	ret = "(System Reset)"; break; 
+	case 0x200:	ret = "(Machine Check)"; break; 
+	case 0x300:	ret = "(Data Access)"; break; 
+	case 0x400:	ret = "(Instruction Access)"; break; 
+	case 0x500:	ret = "(Hardware Interrupt)"; break; 
+	case 0x600:	ret = "(Alignment)"; break; 
+	case 0x700:	ret = "(Program Check)"; break; 
+	case 0x800:	ret = "(FPU Unavailable)"; break; 
+	case 0x900:	ret = "(Decrementer)"; break; 
+	case 0xc00:	ret = "(System Call)"; break; 
+	case 0xd00:	ret = "(Single Step)"; break; 
+	case 0xf00:	ret = "(Performance Monitor)"; break; 
+	default: ret = "";
+	}
+	return ret;
+}
+
+int
+kdba_halt(int argc, const char **argv, const char **envp, struct pt_regs *fp)
+{
+    kdb_printf("halting machine. ");
+    machine_halt();
+return 0;
+}
+
+
+int
+kdba_excprint(int argc, const char **argv, const char **envp, struct pt_regs *fp)
+{
+	struct task_struct *c;
+	struct tbtable tab;
+
+#ifdef CONFIG_SMP
+	kdb_printf("cpu %d: ", smp_processor_id());
+#endif /* CONFIG_SMP */
+
+	kdb_printf("Vector: %lx %s at  [%p]\n", fp->trap, getvecname(fp->trap), fp);
+	kdb_printf("    pc: %lx", fp->nip);
+	if (find_tb_table(fp->nip, &tab) && tab.name[0]) {
+		/* Got a nice name for it */
+		int delta = fp->nip - tab.funcstart;
+		kdb_printf(" (%s+0x%x)", tab.name, delta);
+	}
+	kdb_printf("\n");
+	kdb_printf("    lr: %lx", fp->link);
+	if (find_tb_table(fp->link, &tab) && tab.name[0]) {
+		/* Got a nice name for it */
+		int delta = fp->link - tab.funcstart;
+		kdb_printf(" (%s+0x%x)", tab.name, delta);
+	}
+	kdb_printf("\n");
+	kdb_printf("    sp: %lx\n", fp->gpr[1]);
+	kdb_printf("   msr: %lx\n", fp->msr);
+
+	if (fp->trap == 0x300 || fp->trap == 0x380 || fp->trap == 0x600) {
+		kdb_printf("   dar: %lx\n", fp->dar);
+		kdb_printf(" dsisr: %lx\n", fp->dsisr);
+	}
+
+	/* XXX: need to copy current or we die.  Why? */
+	c = current;
+	kdb_printf("  current = 0x%p\n", c);
+	kdb_printf("  paca    = 0x%p\n", get_paca());
+	if (c) {
+		kdb_printf("  current = %p, pid = %ld, comm = %s\n",
+		       c, (unsigned long)c->pid, (char *)c->comm);
+	}
+return 0;
+}
+
+
+/* Starting at codeaddr scan forward for a tbtable and fill in the
+ given table.  Return non-zero if successful at doing something.
+ */
+static int
+find_tb_table(unsigned long codeaddr, struct tbtable *tab)
+{
+	unsigned long codeaddr_max;
+	unsigned long tbtab_start;
+	int nr;
+	int instr;
+	int num_parms;
+
+	if (tab == NULL)
+		return 0;
+	memset(tab, 0, sizeof(tab));
+
+	/* Scan instructions starting at codeaddr for 128k max */
+	for (codeaddr_max = codeaddr + 128*1024*4;
+	     codeaddr < codeaddr_max;
+	     codeaddr += 4) {
+	    nr=kdba_readarea_size(codeaddr,&instr,4);
+		if (nr != 4)
+			return 0;	/* Bad read.  Give up promptly. */
+		if (instr == 0) {
+			/* table should follow. */
+			int version;
+			unsigned long flags;
+			tbtab_start = codeaddr;	/* save it to compute func start addr */
+			codeaddr += 4;
+			nr = kdba_readarea_size(codeaddr,&flags,8);
+			if (nr != 8)
+				return 0;	/* Bad read or no tb table. */
+			tab->flags = flags;
+			version = (flags >> 56) & 0xff;
+			if (version != 0)
+				continue;	/* No tb table here. */
+			/* Now, like the version, some of the flags are values
+			 that are more conveniently extracted... */
+			tab->fp_saved = (flags >> 24) & 0x3f;
+			tab->gpr_saved = (flags >> 16) & 0x3f;
+			tab->fixedparms = (flags >> 8) & 0xff;
+			tab->floatparms = (flags >> 1) & 0x7f;
+			codeaddr += 8;
+			num_parms = tab->fixedparms + tab->floatparms;
+			if (num_parms) {
+				unsigned int parminfo;
+				int parm;
+				if (num_parms > 32)
+					return 1;	/* incomplete */
+				nr = kdba_readarea_size(codeaddr,&parminfo,4);
+				if (nr != 4)
+					return 1;	/* incomplete */
+				/* decode parminfo...32 bits.
+				 A zero means fixed.  A one means float and the
+				 following bit determines single (0) or double (1).
+				 */
+				for (parm = 0; parm < num_parms; parm++) {
+					if (parminfo & 0x80000000) {
+						parminfo <<= 1;
+						if (parminfo & 0x80000000)
+							tab->parminfo[parm] = TBTAB_PARMDFLOAT;
+						else
+							tab->parminfo[parm] = TBTAB_PARMSFLOAT;
+					} else {
+						tab->parminfo[parm] = TBTAB_PARMFIXED;
+					}
+					parminfo <<= 1;
+				}
+				codeaddr += 4;
+			}
+			if (flags & TBTAB_FLAGSHASTBOFF) {
+			    nr = kdba_readarea_size(codeaddr,&tab->tb_offset,4);
+				if (nr != 4)
+					return 1;	/* incomplete */
+				if (tab->tb_offset > 0) {
+					tab->funcstart = tbtab_start - tab->tb_offset;
+				}
+				codeaddr += 4;
+			}
+			/* hand_mask appears to be always be omitted. */
+			if (flags & TBTAB_FLAGSHASCTL) {
+				/* Assume this will never happen for C or asm */
+				return 1;	/* incomplete */
+			}
+			if (flags & TBTAB_FLAGSNAMEPRESENT) {
+				short namlen;
+				nr = kdba_readarea_size(codeaddr,&namlen,2);
+				if (nr != 2)
+					return 1;	/* incomplete */
+				if (namlen >= sizeof(tab->name))
+					namlen = sizeof(tab->name)-1;
+				codeaddr += 2;
+				nr = kdba_readarea_size(codeaddr,tab->name,namlen);
+				tab->name[namlen] = '\0';
+				codeaddr += namlen;
+			}
+			return 1;
+		}
+	}
+	return 0;	/* hit max...sorry. */
+}
+
+
+int
+kdba_dissect_msr(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+   long int msr;
+
+   if (argc==0)
+       msr = regs->msr;
+/*       msr = get_msr(); */
+    else 
+	kdbgetularg(argv[1], &msr);
+
+   kdb_printf("msr: %lx (",msr);
+   {
+       if (msr & MSR_SF)   kdb_printf("SF ");
+       if (msr & MSR_ISF)  kdb_printf("ISF ");
+       if (msr & MSR_HV)   kdb_printf("HV ");
+       if (msr & MSR_VEC)  kdb_printf("VEC ");
+       if (msr & MSR_POW)  kdb_printf("POW/");  /* pow/we share */
+       if (msr & MSR_WE)   kdb_printf("WE ");
+       if (msr & MSR_TGPR) kdb_printf("TGPR/"); /* tgpr/ce share */
+       if (msr & MSR_CE)   kdb_printf("CE ");
+       if (msr & MSR_ILE)  kdb_printf("ILE ");
+       if (msr & MSR_EE)   kdb_printf("EE ");
+       if (msr & MSR_PR)   kdb_printf("PR ");
+       if (msr & MSR_FP)   kdb_printf("FP ");
+       if (msr & MSR_ME)   kdb_printf("ME ");
+       if (msr & MSR_FE0)  kdb_printf("FE0 ");
+       if (msr & MSR_SE)   kdb_printf("SE ");
+       if (msr & MSR_BE)   kdb_printf("BE/");   /* be/de share */
+       if (msr & MSR_DE)   kdb_printf("DE ");
+       if (msr & MSR_FE1)  kdb_printf("FE1 ");
+       if (msr & MSR_IP)   kdb_printf("IP ");
+       if (msr & MSR_IR)   kdb_printf("IR ");
+       if (msr & MSR_DR)   kdb_printf("DR ");
+       if (msr & MSR_PE)   kdb_printf("PE ");
+       if (msr & MSR_PX)   kdb_printf("PX ");
+       if (msr & MSR_RI)   kdb_printf("RI ");
+       if (msr & MSR_LE)   kdb_printf("LE ");
+   }
+   kdb_printf(")\n");
+
+   if (msr & MSR_SF)   kdb_printf(" 64 bit mode enabled \n");
+   if (msr & MSR_ISF)  kdb_printf(" Interrupt 64b mode valid on 630 \n");
+   if (msr & MSR_HV)   kdb_printf(" Hypervisor State \n");
+   if (msr & MSR_VEC)  kdb_printf(" Enable Altivec \n");
+   if (msr & MSR_POW)  kdb_printf(" Enable Power Management  \n");
+   if (msr & MSR_WE)   kdb_printf(" Wait State Enable   \n");
+   if (msr & MSR_TGPR) kdb_printf(" TLB Update registers in use   \n");
+   if (msr & MSR_CE)   kdb_printf(" Critical Interrupt Enable   \n");
+   if (msr & MSR_ILE)  kdb_printf(" Interrupt Little Endian   \n");
+   if (msr & MSR_EE)   kdb_printf(" External Interrupt Enable   \n");
+   if (msr & MSR_PR)   kdb_printf(" Problem State / Privilege Level  \n"); 
+   if (msr & MSR_FP)   kdb_printf(" Floating Point enable   \n");
+   if (msr & MSR_ME)   kdb_printf(" Machine Check Enable   \n");
+   if (msr & MSR_FE0)  kdb_printf(" Floating Exception mode 0  \n"); 
+   if (msr & MSR_SE)   kdb_printf(" Single Step   \n");
+   if (msr & MSR_BE)   kdb_printf(" Branch Trace   \n");
+   if (msr & MSR_DE)   kdb_printf(" Debug Exception Enable   \n");
+   if (msr & MSR_FE1)  kdb_printf(" Floating Exception mode 1   \n");
+   if (msr & MSR_IP)   kdb_printf(" Exception prefix 0x000/0xFFF   \n");
+   if (msr & MSR_IR)   kdb_printf(" Instruction Relocate   \n");
+   if (msr & MSR_DR)   kdb_printf(" Data Relocate   \n");
+   if (msr & MSR_PE)   kdb_printf(" Protection Enable   \n");
+   if (msr & MSR_PX)   kdb_printf(" Protection Exclusive Mode   \n");
+   if (msr & MSR_RI)   kdb_printf(" Recoverable Exception   \n");
+   if (msr & MSR_LE)   kdb_printf(" Little Endian   \n");
+   kdb_printf(".\n");
+
+return 0;
+}
+
+int
+kdba_super_regs(int argc, const char **argv, const char **envp, struct pt_regs *regs){
+	int i;
+	struct paca_struct*  ptrPaca = NULL;
+	struct ItLpPaca*  ptrLpPaca = NULL;
+	struct ItLpRegSave*  ptrLpRegSave = NULL;
+
+	{
+	        unsigned long sp, toc;
+		kdb_printf("sr::");
+		asm("mr %0,1" : "=r" (sp) :);
+		asm("mr %0,2" : "=r" (toc) :);
+
+		kdb_printf("msr  = %.16lx  sprg0= %.16lx\n", get_msr(), get_sprg0());
+		kdb_printf("pvr  = %.16lx  sprg1= %.16lx\n", get_pvr(), get_sprg1()); 
+		kdb_printf("dec  = %.16lx  sprg2= %.16lx\n", get_dec(), get_sprg2());
+		kdb_printf("sp   = %.16lx  sprg3= %.16lx\n", sp, get_sprg3());
+		kdb_printf("toc  = %.16lx  dar  = %.16lx\n", toc, get_dar());
+		kdb_printf("srr0 = %.16lx  srr1 = %.16lx\n", get_srr0(), get_srr1());
+		kdb_printf("asr  = %.16lx\n", mfasr());
+		for (i = 0; i < 8; ++i)
+			kdb_printf("sr%.2ld = %.16lx  sr%.2ld = %.16lx\n", (long int)i, (unsigned long)get_sr(i), (long int)(i+8), (long unsigned int) get_sr(i+8));
+
+		// Dump out relevant Paca data areas.
+		kdb_printf("Paca: \n");
+		ptrPaca = (struct paca_struct*)get_sprg3();
+    
+		kdb_printf("  Local Processor Control Area (LpPaca): \n");
+		ptrLpPaca = ptrPaca->xLpPacaPtr;
+		kdb_printf("    Saved Srr0=%.16lx  Saved Srr1=%.16lx \n", ptrLpPaca->xSavedSrr0, ptrLpPaca->xSavedSrr1);
+		kdb_printf("    Saved Gpr3=%.16lx  Saved Gpr4=%.16lx \n", ptrLpPaca->xSavedGpr3, ptrLpPaca->xSavedGpr4);
+		kdb_printf("    Saved Gpr5=%.16lx \n", ptrLpPaca->xSavedGpr5);
+    
+		kdb_printf("  Local Processor Register Save Area (LpRegSave): \n");
+		ptrLpRegSave = ptrPaca->xLpRegSavePtr;
+		kdb_printf("    Saved Sprg0=%.16lx  Saved Sprg1=%.16lx \n", ptrLpRegSave->xSPRG0, ptrLpRegSave->xSPRG0);
+		kdb_printf("    Saved Sprg2=%.16lx  Saved Sprg3=%.16lx \n", ptrLpRegSave->xSPRG2, ptrLpRegSave->xSPRG3);
+		kdb_printf("    Saved Msr  =%.16lx  Saved Nia  =%.16lx \n", ptrLpRegSave->xMSR, ptrLpRegSave->xNIA);
+    
+		return 0;
+	} 
+}
+
+
+
+	
+int
+kdba_dump_tce_table(int argc, const char **argv, const char **envp, struct pt_regs *regs){
+    struct TceTable kt; 
+    long tce_table_address;
+    int nr;
+    int i,j,k;
+    int full,empty;
+    int fulldump=0;
+    u64 mapentry;
+    int totalpages;
+    int levelpages;
+
+    if (argc == 0) {
+	kdb_printf("need address\n");
+	return 0;
+    }
+    else 
+	kdbgetularg(argv[1], &tce_table_address);
+
+    if (argc==2)
+	if (strcmp(argv[2], "full") == 0) 
+	    fulldump=1;
+
+    /* with address, read contents of memory and dump tce table. */
+    /* possibly making some assumptions on the depth and size of table..*/
+
+    nr = kdba_readarea_size(tce_table_address+0 ,&kt.busNumber,8);
+    nr = kdba_readarea_size(tce_table_address+8 ,&kt.size,8);
+    nr = kdba_readarea_size(tce_table_address+16,&kt.startOffset,8);
+    nr = kdba_readarea_size(tce_table_address+24,&kt.base,8);
+    nr = kdba_readarea_size(tce_table_address+32,&kt.index,8);
+    nr = kdba_readarea_size(tce_table_address+40,&kt.tceType,8);
+    nr = kdba_readarea_size(tce_table_address+48,&kt.lock,8);
+
+    kdb_printf("\n");
+    kdb_printf("TceTable at address %s:\n",argv[1]);
+    kdb_printf("BusNumber:   0x%x \n",(uint)kt.busNumber);
+    kdb_printf("size:        0x%x \n",(uint)kt.size);
+    kdb_printf("startOffset: 0x%x \n",(uint)kt.startOffset);
+    kdb_printf("base:        0x%x \n",(uint)kt.base);
+    kdb_printf("index:       0x%x \n",(uint)kt.index);
+    kdb_printf("tceType:     0x%x \n",(uint)kt.tceType);
+#ifdef CONFIG_SMP
+    kdb_printf("lock:        0x%x \n",(uint)kt.lock.lock);
+#endif
+
+    nr = kdba_readarea_size(tce_table_address+56,&kt.mlbm.maxLevel,8);
+    kdb_printf(" maxLevel:        0x%x \n",(uint)kt.mlbm.maxLevel);
+    totalpages=0;
+    for (i=0;i<NUM_TCE_LEVELS;i++) {
+	nr = kdba_readarea_size(tce_table_address+64+i*24,&kt.mlbm.level[i].numBits,8);
+	nr = kdba_readarea_size(tce_table_address+72+i*24,&kt.mlbm.level[i].numBytes,8);
+	nr = kdba_readarea_size(tce_table_address+80+i*24,&kt.mlbm.level[i].map,8);
+	kdb_printf("   level[%d]\n",i);
+	kdb_printf("   numBits:   0x%x\n",(uint)kt.mlbm.level[i].numBits);
+	kdb_printf("   numBytes:  0x%x\n",(uint)kt.mlbm.level[i].numBytes);
+	kdb_printf("   map*:      %p\n",kt.mlbm.level[i].map);
+
+	 /* if these dont match, this might not be a valid tce table, so
+	    dont try to iterate the map entries. */
+	if (kt.mlbm.level[i].numBits == 8*kt.mlbm.level[i].numBytes) {
+	    full=0;empty=0;levelpages=0;
+	    for (j=0;j<kt.mlbm.level[i].numBytes; j++) {
+		mapentry=0;
+		nr = kdba_readarea_size((long int)(kt.mlbm.level[i].map+j),&mapentry,1);
+		if (mapentry)
+		    full++;
+		else
+		    empty++;
+		if (mapentry && fulldump) {
+		    kdb_printf("0x%lx\n",mapentry);
+		}
+		for (k=0;(k<=64) && ((0x1UL<<k) <= mapentry);k++) {
+		    if ((0x1UL<<k) & mapentry) levelpages++;
+		}
+	    }
+	    kdb_printf("      full:0x%x empty:0x%x pages:0x%x\n",full,empty,levelpages);
+	} else {
+	    kdb_printf("      numBits/numBytes mismatch..? \n");
+	}
+	totalpages+=levelpages;
+    }
+    kdb_printf("      Total pages:0x%x\n",totalpages);
+    kdb_printf("\n");
+    return 0;
+}
+
+int
+kdba_kernelversion(int argc, const char **argv, const char **envp, struct pt_regs *regs){
+    extern char *linux_banner;
+
+    kdb_printf("%s\n",linux_banner);
+
+    return 0;
+}
+
+
+static void * 
+kdba_dump_pci(struct device_node *dn, void *data)
+{
+    struct pci_controller *phb;
+    char *device_type;
+    char *status;
+
+    phb = (struct pci_controller *)data;
+    device_type = get_property(dn, "device_type", 0);
+    status = get_property(dn, "status", 0);
+
+    dn->phb = phb;
+    kdb_printf("dn:   %p \n",dn);
+    kdb_printf("    phb      : %p\n",dn->phb);
+    kdb_printf("    name     : %s\n",dn->name);
+    kdb_printf("    full_name: %s\n",dn->full_name);
+    kdb_printf("    busno    : 0x%x\n",dn->busno);
+    kdb_printf("    devfn    : 0x%x\n",dn->devfn);
+    kdb_printf("    tce_table: %p\n",dn->tce_table);
+    return NULL;
+}
+
+int
+kdba_dump_pci_info(int argc, const char **argv, const char **envp, struct pt_regs *regs){
+
+    kdb_printf("kdba_dump_pci_info\n");
+
+/* call this traverse function with my function pointer.. it takes care of traversing, my func just needs to parse the device info.. */
+    traverse_all_pci_devices(kdba_dump_pci);
+    return 0;
+}
+
+
+char *kdb_dumpall_cmds[] = {
+    "excp\n",
+    "bt\n",
+    "rd\n",
+    "dmesg\n",
+    "msr\n",
+    "superreg\n",
+    "pci_info\n",
+    "ps\n",
+    "cpu\n",
+    "set BTAPROMPT=none\n",
+    "bta\n",
+    0
+};
+
+char *kdb_dumpbasic_cmds[] = {
+    "excp\n",
+    "bt\n",
+    "rd\n",
+    "dmesg 25\n",
+    "msr\n",
+    "superreg\n",
+    "ps\n",
+    "cpu\n",
+    0
+};
+
+
+/* dump with "all" parm will dump all.  all other variations dump basic.  See the dump*_cmds defined above */
+int
+kdba_dump(int argc, const char **argv, const char **envp, struct pt_regs *fp)
+{
+    int i, diag;
+    kdb_printf("dump-all\n");
+    if ((argc==1)&& (strcmp(argv[1], "all")==0))	{
+	for (i = 0; kdb_dumpall_cmds[i]; ++i) {
+	    kdb_printf("kdb_cmd[%d]%s: %s",
+		       i, " ", kdb_dumpall_cmds[i]);
+	    diag = kdb_parse(kdb_dumpall_cmds[i], fp);
+	    if (diag)
+		kdb_printf("command failed, kdb diag %d\n", diag);
+	}
+    } else {
+	kdb_printf("dump-basic\n");
+	for (i = 0; kdb_dumpbasic_cmds[i]; ++i) {
+	    kdb_printf("kdb_cmd[%d]%s: %s",
+		       i, " ", kdb_dumpbasic_cmds[i]);
+	    diag = kdb_parse(kdb_dumpbasic_cmds[i], fp);
+	    if (diag)
+		kdb_printf("command failed, kdb diag %d\n", diag);
+	}
+    }
+    return 0;
+}
+
+
+/* Toggle the ppcdbg options.   kdb_parse tokenizes the parms, so need to account for that here.  */
+int
+kdba_ppcdbg(int argc, const char **argv, const char **envp, struct pt_regs *fp) {
+    extern char *trace_names[PPCDBG_NUM_FLAGS];
+
+    int i,j;
+    unsigned long mask;
+    int onoff;
+    if (argc==0)
+	goto ppcdbg_exit;
+
+    for (i=1;i<=argc;i++) {
+	onoff = 1;	/* default */
+	if (argv[i][0] == '+' || argv[i][0] == '-') {
+			/* explicit on or off */
+	    onoff = (argv[i][0] == '+');
+	    argv[i]++;
+	}
+
+	for (j=0;j<PPCDBG_NUM_FLAGS;j++) {
+	    if (trace_names[j] && strcmp(trace_names[j],argv[i])==0) {
+		/* have a match */
+		mask = (1 << j);
+		/* check special case */
+		if (strcmp(argv[i],"all")==0) {
+		    mask = PPCDBG_ALL;
+		}
+		if (mask) {
+		    if (onoff)
+			naca->debug_switch |= mask;
+		    else
+			naca->debug_switch &= ~mask;
+		}
+	    } 
+	}
+    }
+    ppcdbg_exit:
+      kdb_printf("naca->debug_switch 0x%lx\n",naca->debug_switch);
+    return 0;
+}
+
+/* enable or disable surveillance.. based on rtasd.c function.
+  no arguments - display current timeout value.
+  one argument - 'off' or '0' turn off surveillance.
+               - '1-255' set surveillance timeout to argument. */
+int
+kdba_surveillance(int argc, const char **argv, const char **envp, struct pt_regs *fp)
+{
+    unsigned long timeout;
+    int ibm_indicator_token = 9000;
+    int error;
+    unsigned long ret;
+
+    if (argc==0) {
+	goto surveillance_status;
+    } else if (((argc==1)&& (strcmp(argv[1], "off")==0))) {
+	timeout=0;
+    } else {
+	kdbgetularg(argv[1], &timeout);
+    }
+
+    error = rtas_call(rtas_token("set-indicator"), 3, 1, &ret,
+		      ibm_indicator_token, 0, timeout);
+    /*    kdb_printf("Surveillance set-indicator returned value: 0x%x\n",ret); */
+
+    if (error) 
+	kdb_printf("surveillance rtas_call failure 0x%x \n",error);
+
+    surveillance_status:
+      rtas_call(rtas_token("get-sensor-state"), 2, 2, &ret, 
+		ibm_indicator_token, 
+		0/* instance */);
+    kdb_printf("Current surveillance timeout is %ld minutes%s",ret,
+	       ret==0?" (disabled).\n":".\n");
+    return 0;
+}
+
+/* generic debugger() hooks into kdb.  These eliminate the need to add
+  ifdef CONFIG_KDB goop to traps.c and fault.c */
+
+void
+kdb_reset_debugger(struct pt_regs *regs) {
+    int cpu=smp_processor_id();
+    static int reset_cpu = -1;
+    static spinlock_t reset_lock = SPIN_LOCK_UNLOCKED;
+    spin_lock(&reset_lock);
+    if (reset_cpu == -1 || reset_cpu == cpu) {
+	reset_cpu = cpu;
+	spin_unlock(&reset_lock);
+	if (kdb_on) {
+	    ppc64_attention_msg(0x3200+cpu,"KDB Call        ");
+	    kdb(KDB_REASON_ENTER, regs->trap, (kdb_eframe_t) regs);
+	    ppc64_attention_msg(0x3300+cpu,"KDB Done        ");
+	} else {
+	    kdb_on=1;
+	    kdb_do_reboot=1;
+	    ppc64_attention_msg(0x3600+cpu,"KDB Enabled     ");
+	    udelay(KDB_RESET_TIMEOUT);
+	    kdb_on=0;
+	    if (kdb_do_reboot) {
+		ppc64_attention_msg(0x3900+cpu,"Rebooting       ");
+		ppc_md.restart("rebooting...");
+		return;	/* not reached */
+	    } else {
+		ppc64_attention_msg(0x3800+cpu,"KDB skip reboot ");
+		return;
+	    }
+	}
+    } else {
+	spin_unlock(&reset_lock);
+	return;
+    }
+}
+
+void
+kdb_debugger(struct pt_regs *regs) {
+    if (regs)
+	if (regs->trap==0x100) {
+	    kdb_reset_debugger(regs);
+	} else
+	    kdb(KDB_REASON_ENTER,regs->trap,regs);   /* ok */
+    else  /* regs invalid */
+	kdb(KDB_REASON_SILENT,0,regs);
+}
+
+int
+kdb_debugger_bpt(struct pt_regs *regs) {
+    if (regs)
+	return kdb(KDB_REASON_BREAK,regs->trap,regs);
+    else  /* regs invalid */
+	return kdb(KDB_REASON_SILENT,0,regs);
+}
+
+int
+kdb_debugger_sstep(struct pt_regs *regs) {
+    if (regs)
+	return kdb(KDB_REASON_DEBUG,regs->trap,regs); /* ok */
+    else  /* regs invalid */
+	return kdb(KDB_REASON_SILENT,0,regs);
+}
+
+int
+kdb_debugger_iabr_match(struct pt_regs *regs) {
+    if (regs)
+	return kdb(KDB_REASON_BREAK,regs->trap,regs);
+    else  /* regs invalid */
+	return kdb(KDB_REASON_SILENT,0,regs);
+}
+
+int
+kdb_debugger_dabr_match(struct pt_regs *regs) {
+    if (regs)
+	return kdb(KDB_REASON_BREAK,regs->trap,regs);
+    else  /* regs invalid */
+	return kdb(KDB_REASON_SILENT,0,regs);
+}
+
+void
+kdb_debugger_fault_handler(struct pt_regs *regs) {
+    if (regs)
+	kdb(KDB_REASON_FAULT,regs->trap,regs);
+    else  /* regs invalid */
+	kdb(KDB_REASON_SILENT,0,regs);
+    return;
+}
+
+
+
+int
+kdba_state(int argc, const char **argv, const char **envp, struct pt_regs *fp)
+{
+    int i;
+    for (i=0;i<NR_CPUS;i++) {
+	if ( kdb_state[i] != 0 ) {
+	    kdb_printf("kdb_state[%d] = %x" ,i,kdb_state[i]);
+	    kdb_printf(" [");
+	    if KDB_STATE_CPU(KDB,i) kdb_printf("KDB,");
+	    if KDB_STATE_CPU(LEAVING,i) kdb_printf("LEAVING,");
+	    if KDB_STATE_CPU(CMD,i) kdb_printf("CMD,");
+	    if KDB_STATE_CPU(KDB_CONTROL,i) kdb_printf("KDB_CONTROL,");
+	    if KDB_STATE_CPU(HOLD_CPU,i) kdb_printf("HOLD_CPU,");
+	    if KDB_STATE_CPU(DOING_SS,i) kdb_printf("DOING_SS,");
+	    if KDB_STATE_CPU(DOING_SSB,i) kdb_printf("DOING_SSB,");
+	    if KDB_STATE_CPU(SSBPT,i) kdb_printf("SSBPT,");
+	    if KDB_STATE_CPU(REENTRY,i) kdb_printf("REENTRY,");
+	    if KDB_STATE_CPU(SUPPRESS,i) kdb_printf("SUPPRESS,");
+	    if KDB_STATE_CPU(LONGJMP,i) kdb_printf("LONGJMP,");
+	    if KDB_STATE_CPU(PRINTF_LOCK,i) kdb_printf("PRINTF_LOCK,");
+	    if KDB_STATE_CPU(WAIT_IPI,i) kdb_printf("WAIT_IPI,");
+	    if KDB_STATE_CPU(RECURSE,i) kdb_printf("RECURSE,");
+	    if KDB_STATE_CPU(IP_ADJUSTED,i) kdb_printf("IP_ADJUSTED,");
+	    if KDB_STATE_CPU(NO_BP_DELAY,i) kdb_printf("NO_BP_DELAY");
+	    kdb_printf("]\n");
+	}
+    }
+return 0;
+}
+
+
+/*
+ * kdba_init
+ * 	Architecture specific initialization.
+ */
+/*
+kdb_register("commandname",              # name of command user will use to invoke function  
+             function_name,              # name of function within the code 
+             "function example usage",   # sample usage 
+             "function description",     # brief description. 
+             0                           # if i hit enter again, will command repeat itself ?
+Note: functions must take parameters as such:
+functionname(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+*/
+
+void __init
+kdba_init(void)
+{
+#ifdef CONFIG_MAGIC_SYSRQ
+	kdb_map_scc();		/* map sysrq key */
+#endif
+
+	debugger = kdb_debugger;
+	debugger_bpt = kdb_debugger_bpt;
+	debugger_sstep = kdb_debugger_sstep;
+	debugger_iabr_match = kdb_debugger_iabr_match;
+	debugger_dabr_match = kdb_debugger_dabr_match;
+	debugger_fault_handler = NULL; /* this guy is normally off. */
+				    /* = kdb_debugger_fault_handler; */
+
+	kdba_enable_lbr();
+	kdb_register("excp", kdba_excprint, "excp", "print exception info", 0);
+	kdb_register("superreg", kdba_super_regs, "superreg", "display super_regs", 0);
+	kdb_register("msr", kdba_dissect_msr, "msr", "dissect msr", 0);
+	kdb_register("halt", kdba_halt, "halt", "halt machine", 0);
+	kdb_register("tce_table", kdba_dump_tce_table, "tce_table <addr> [full]", "dump the tce table located at <addr>", 0);
+	kdb_register("kernel", kdba_kernelversion, "version", "display running kernel version", 0);
+	kdb_register("pci_info", kdba_dump_pci_info, "dump_pci_info", "dump pci device info", 0);
+	kdb_register("dump", kdba_dump, "dump (all|basic)", "dump all info", 0); 
+	kdb_register("state", kdba_state, "state ", "dump state of all processors", 0); 
+	kdb_register("surv", kdba_surveillance, "surv [off|1-255] ", "disable/change surveillance timeout", 0); 
+	kdb_register("ppcdbg", kdba_ppcdbg, "ppcdbg (a,+b,-c)","toggle PPCDBG options",0);
+	if (!ppc_md.udbg_getc_poll)
+		kdb_on = 0;
+}
diff -purN linux-2.5/arch/ppc64/kdb/opintl.h linuxppc64-2.5/arch/ppc64/kdb/opintl.h
--- linux-2.5/arch/ppc64/kdb/opintl.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/opintl.h	2003-10-13 16:16:57.000000000 +0000
@@ -0,0 +1,42 @@
+/* opintl.h - opcodes specific header for gettext code.
+   Copyright (C) 1998, 1999 Free Software Foundation, Inc.
+
+   Written by Tom Tromey <tromey@cygnus.com>
+
+   This file is part of the opcodes library used by GAS and the GNU binutils.
+
+   You should have received a copy of the GNU General Public License
+   along with GAS; see the file COPYING.  If not, write to the Free
+   Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+   02111-1307, USA. */
+
+#ifdef ENABLE_NLS
+# include <libintl.h>
+/* Note the use of dgetext() and PACKAGE here, rather than gettext().
+   
+   This is because the code in this directory is used to build a library which
+   will be linked with code in other directories to form programs.  We want to
+   maintain a seperate translation file for this directory however, rather
+   than being forced to merge it with that of any program linked to
+   libopcodes.  This is a library, so it cannot depend on the catalog
+   currently loaded.
+
+   In order to do this, we have to make sure that when we extract messages we
+   use the OPCODES domain rather than the domain of the program that included
+   the opcodes library, (eg OBJDUMP).  Hence we use dgettext (PACKAGE, String)
+   and define PACKAGE to be 'opcodes'.  (See the code in configure).  */
+# define _(String) dgettext (PACKAGE, String)
+# ifdef gettext_noop
+#  define N_(String) gettext_noop (String)
+# else
+#  define N_(String) (String)
+# endif
+#else
+# define gettext(Msgid) (Msgid)
+# define dgettext(Domainname, Msgid) (Msgid)
+# define dcgettext(Domainname, Msgid, Category) (Msgid)
+# define textdomain(Domainname) while (0) /* nothing */
+# define bindtextdomain(Domainname, Dirname) while (0) /* nothing */
+# define _(String) (String)
+# define N_(String) (String)
+#endif
diff -purN linux-2.5/arch/ppc64/kdb/ppc-dis.c linuxppc64-2.5/arch/ppc64/kdb/ppc-dis.c
--- linux-2.5/arch/ppc64/kdb/ppc-dis.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/ppc-dis.c	2003-10-13 16:16:57.000000000 +0000
@@ -0,0 +1,281 @@
+/* ppc-dis.c -- Disassemble PowerPC instructions
+   Copyright 1994 Free Software Foundation, Inc.
+   Written by Ian Lance Taylor, Cygnus Support
+
+This file is part of GDB, GAS, and the GNU binutils.
+
+GDB, GAS, and the GNU binutils are free software; you can redistribute
+them and/or modify them under the terms of the GNU General Public
+License as published by the Free Software Foundation; either version
+2, or (at your option) any later version.
+
+GDB, GAS, and the GNU binutils are distributed in the hope that they
+will be useful, but WITHOUT ANY WARRANTY; without even the implied
+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+the GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this file; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  */
+#ifdef __KERNEL__
+#include <linux/kernel.h>
+#include <linux/string.h>
+#include <linux/dis-asm.h>
+#include <linux/kdb.h>
+#include "ppc.h"
+
+#if 0
+#include <setjmp.h>
+#endif
+
+#else
+#include <stdio.h>
+#include "sysdep.h"
+#include "dis-asm.h"
+#include "opcode/ppc.h"
+#endif
+bfd_vma
+bfd_getb32 (addr)
+     register const bfd_byte *addr;
+{
+  unsigned long v;
+
+  v = (unsigned long) addr[0] << 24;
+  v |= (unsigned long) addr[1] << 16;
+  v |= (unsigned long) addr[2] << 8;
+  v |= (unsigned long) addr[3];
+  return (bfd_vma) v;
+}
+
+bfd_vma
+bfd_getl32 (addr)
+     register const bfd_byte *addr;
+{
+  unsigned long v;
+
+  v = (unsigned long) addr[0];
+  v |= (unsigned long) addr[1] << 8;
+  v |= (unsigned long) addr[2] << 16;
+  v |= (unsigned long) addr[3] << 24;
+  return (bfd_vma) v;
+}
+/* This file provides several disassembler functions, all of which use
+   the disassembler interface defined in dis-asm.h.  Several functions
+   are provided because this file handles disassembly for the PowerPC
+   in both big and little endian mode and also for the POWER (RS/6000)
+   chip.  */
+
+static int print_insn_powerpc PARAMS ((bfd_vma, struct disassemble_info *,
+				       int bigendian, int dialect));
+
+/* Print a big endian PowerPC instruction.  For convenience, also
+   disassemble instructions supported by the Motorola PowerPC 601
+   and the Altivec vector unit.  */
+
+int
+print_insn_big_powerpc (memaddr, info)
+     bfd_vma memaddr;
+     struct disassemble_info *info;
+{
+  return print_insn_powerpc (memaddr, info, 1,
+			     PPC_OPCODE_PPC | PPC_OPCODE_601 |
+			     PPC_OPCODE_ALTIVEC);
+}
+
+/* Print a little endian PowerPC instruction.  For convenience, also
+   disassemble instructions supported by the Motorola PowerPC 601
+   and the Altivec vector unit.  */
+
+int
+print_insn_little_powerpc (memaddr, info)
+     bfd_vma memaddr;
+     struct disassemble_info *info;
+{
+  return print_insn_powerpc (memaddr, info, 0,
+			     PPC_OPCODE_PPC | PPC_OPCODE_601 |
+			     PPC_OPCODE_ALTIVEC);
+}
+
+/* Print a POWER (RS/6000) instruction.  */
+
+int
+print_insn_rs6000 (memaddr, info)
+     bfd_vma memaddr;
+     struct disassemble_info *info;
+{
+  return print_insn_powerpc (memaddr, info, 1, PPC_OPCODE_POWER);
+}
+
+/* Print a PowerPC or POWER instruction.  */
+
+static int
+print_insn_powerpc (memaddr, info, bigendian, dialect)
+     bfd_vma memaddr;
+     struct disassemble_info *info;
+     int bigendian;
+     int dialect;
+{
+  bfd_byte buffer[4];
+  int status;
+  unsigned long insn;
+  const struct powerpc_opcode *opcode;
+  const struct powerpc_opcode *opcode_end;
+  unsigned long op;
+
+  (*info->fprintf_func) (info->stream, "  ");
+
+  status = (*info->read_memory_func) (memaddr, buffer, 4, info);
+  if (status != 0)
+    {
+      (*info->memory_error_func) (status, memaddr, info);
+      return -1;
+    }
+
+  if (bigendian)
+    insn = bfd_getb32 (buffer);
+  else
+    insn = bfd_getl32 (buffer);
+
+  /* Get the major opcode of the instruction.  */
+  op = PPC_OP (insn);
+
+  /* Find the first match in the opcode table.  We could speed this up
+     a bit by doing a binary search on the major opcode.  */
+  opcode_end = powerpc_opcodes + powerpc_num_opcodes;
+  for (opcode = powerpc_opcodes; opcode < opcode_end; opcode++)
+    {
+      unsigned long table_op;
+      const unsigned char *opindex;
+      const struct powerpc_operand *operand;
+      int invalid;
+      int need_comma;
+      int need_paren;
+
+      table_op = PPC_OP (opcode->opcode);
+      if (op < table_op)
+	break;
+      if (op > table_op)
+	continue;
+
+      if ((insn & opcode->mask) != opcode->opcode
+	  || (opcode->flags & dialect) == 0)
+	continue;
+
+      /* Make two passes over the operands.  First see if any of them
+	 have extraction functions, and, if they do, make sure the
+	 instruction is valid.  */
+      invalid = 0;
+      for (opindex = opcode->operands; *opindex != 0; opindex++)
+	{
+	  operand = powerpc_operands + *opindex;
+	  if (operand->extract)
+	    (*operand->extract) (insn, &invalid);
+	}
+      if (invalid)
+	continue;
+
+      /* The instruction is valid.  */
+      (*info->fprintf_func) (info->stream, "%-6s", opcode->name);
+      if (opcode->operands[0] != 0)
+	(*info->fprintf_func) (info->stream, "\t");
+
+      /* Now extract and print the operands.  */
+      need_comma = 0;
+      need_paren = 0;
+      for (opindex = opcode->operands; *opindex != 0; opindex++)
+	{
+	  long value;
+
+	  operand = powerpc_operands + *opindex;
+
+	  /* Operands that are marked FAKE are simply ignored.  We
+	     already made sure that the extract function considered
+	     the instruction to be valid.  */
+	  if ((operand->flags & PPC_OPERAND_FAKE) != 0)
+	    continue;
+
+	  /* Extract the value from the instruction.  */
+	  if (operand->extract)
+	    value = (*operand->extract) (insn, (int *) NULL);
+	  else
+	    {
+	      value = (insn >> operand->shift) & ((1 << operand->bits) - 1);
+	      if ((operand->flags & PPC_OPERAND_SIGNED) != 0
+		  && (value & (1 << (operand->bits - 1))) != 0)
+		value -= 1 << operand->bits;
+	    }
+
+	  /* If the operand is optional, and the value is zero, don't
+	     print anything.  */
+	  if ((operand->flags & PPC_OPERAND_OPTIONAL) != 0
+	      && (operand->flags & PPC_OPERAND_NEXT) == 0
+	      && value == 0)
+	    continue;
+
+	  if (need_comma)
+	    {
+	      (*info->fprintf_func) (info->stream, ",");
+	      need_comma = 0;
+	    }
+
+	  /* Print the operand as directed by the flags.  */
+	  if ((operand->flags & PPC_OPERAND_GPR) != 0)
+	    (*info->fprintf_func) (info->stream, "r%ld", value);
+	  else if ((operand->flags & PPC_OPERAND_FPR) != 0)
+	    (*info->fprintf_func) (info->stream, "f%ld", value);
+	  else if ((operand->flags & PPC_OPERAND_VR) != 0)
+	    (*info->fprintf_func) (info->stream, "v%ld", value);
+	  else if ((operand->flags & PPC_OPERAND_RELATIVE) != 0)
+	    (*info->print_address_func) (memaddr + value, info);
+	  else if ((operand->flags & PPC_OPERAND_ABSOLUTE) != 0)
+	    (*info->print_address_func) ((bfd_vma) value & 0xffffffff, info);
+	  else if ((operand->flags & PPC_OPERAND_CR) == 0
+		   || (dialect & PPC_OPCODE_PPC) == 0)
+	    (*info->fprintf_func) (info->stream, "%ld", value);
+	  else
+	    {
+	      if (operand->bits == 3)
+		(*info->fprintf_func) (info->stream, "cr%d", value);
+	      else
+		{
+		  static const char *cbnames[4] = { "lt", "gt", "eq", "so" };
+		  int cr;
+		  int cc;
+
+		  cr = value >> 2;
+		  if (cr != 0)
+		    (*info->fprintf_func) (info->stream, "4*cr%d", cr);
+		  cc = value & 3;
+		  if (cc != 0)
+		    {
+		      if (cr != 0)
+			(*info->fprintf_func) (info->stream, "+");
+		      (*info->fprintf_func) (info->stream, "%s", cbnames[cc]);
+		    }
+		}
+	    }
+
+	  if (need_paren)
+	    {
+	      (*info->fprintf_func) (info->stream, ")");
+	      need_paren = 0;
+	    }
+
+	  if ((operand->flags & PPC_OPERAND_PARENS) == 0)
+	    need_comma = 1;
+	  else
+	    {
+	      (*info->fprintf_func) (info->stream, "(");
+	      need_paren = 1;
+	    }
+	}
+
+      /* We have found and printed an instruction; return.  */
+      return 4;
+    }
+
+  /* We could not find a match.  */
+  (*info->fprintf_func) (info->stream, ".long 0x%lx", insn);
+
+  return 4;
+}
diff -purN linux-2.5/arch/ppc64/kdb/ppc-opc.c linuxppc64-2.5/arch/ppc64/kdb/ppc-opc.c
--- linux-2.5/arch/ppc64/kdb/ppc-opc.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/ppc-opc.c	2003-10-13 16:16:57.000000000 +0000
@@ -0,0 +1,3491 @@
+/* ppc-opc.c -- PowerPC opcode list
+   Copyright (c) 1994, 95, 96, 97, 98, 99, 2000 Free Software Foundation, Inc.
+   Written by Ian Lance Taylor, Cygnus Support
+
+This file is part of GDB, GAS, and the GNU binutils.
+
+GDB, GAS, and the GNU binutils are free software; you can redistribute
+them and/or modify them under the terms of the GNU General Public
+License as published by the Free Software Foundation; either version
+2, or (at your option) any later version.
+
+GDB, GAS, and the GNU binutils are distributed in the hope that they
+will be useful, but WITHOUT ANY WARRANTY; without even the implied
+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+the GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this file; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA
+02111-1307, USA.  */
+#ifndef __KERNEL__
+#include <stdio.h>
+#include "sysdep.h"
+#include "opcode/ppc.h"
+#include "opintl.h"
+#else
+#include <linux/kernel.h>
+#include <linux/string.h>
+#include <linux/dis-asm.h>
+#include <linux/kdb.h>
+#include "ppc.h"
+#include "opintl.h"
+#endif
+/* This file holds the PowerPC opcode table.  The opcode table
+   includes almost all of the extended instruction mnemonics.  This
+   permits the disassembler to use them, and simplifies the assembler
+   logic, at the cost of increasing the table size.  The table is
+   strictly constant data, so the compiler should be able to put it in
+   the .text section.
+
+   This file also holds the operand table.  All knowledge about
+   inserting operands into instructions and vice-versa is kept in this
+   file.  */
+
+/* Local insertion and extraction functions.  */
+
+static unsigned long insert_bat PARAMS ((unsigned long, long, const char **));
+static long extract_bat PARAMS ((unsigned long, int *));
+static unsigned long insert_bba PARAMS ((unsigned long, long, const char **));
+static long extract_bba PARAMS ((unsigned long, int *));
+static unsigned long insert_bd PARAMS ((unsigned long, long, const char **));
+static long extract_bd PARAMS ((unsigned long, int *));
+static unsigned long insert_bdm PARAMS ((unsigned long, long, const char **));
+static long extract_bdm PARAMS ((unsigned long, int *));
+static unsigned long insert_bdp PARAMS ((unsigned long, long, const char **));
+static long extract_bdp PARAMS ((unsigned long, int *));
+static int valid_bo PARAMS ((long));
+static unsigned long insert_bo PARAMS ((unsigned long, long, const char **));
+static long extract_bo PARAMS ((unsigned long, int *));
+static unsigned long insert_boe PARAMS ((unsigned long, long, const char **));
+static long extract_boe PARAMS ((unsigned long, int *));
+static unsigned long insert_ds PARAMS ((unsigned long, long, const char **));
+static long extract_ds PARAMS ((unsigned long, int *));
+static unsigned long insert_li PARAMS ((unsigned long, long, const char **));
+static long extract_li PARAMS ((unsigned long, int *));
+static unsigned long insert_mbe PARAMS ((unsigned long, long, const char **));
+static long extract_mbe PARAMS ((unsigned long, int *));
+static unsigned long insert_mb6 PARAMS ((unsigned long, long, const char **));
+static long extract_mb6 PARAMS ((unsigned long, int *));
+static unsigned long insert_nb PARAMS ((unsigned long, long, const char **));
+static long extract_nb PARAMS ((unsigned long, int *));
+static unsigned long insert_nsi PARAMS ((unsigned long, long, const char **));
+static long extract_nsi PARAMS ((unsigned long, int *));
+static unsigned long insert_ral PARAMS ((unsigned long, long, const char **));
+static unsigned long insert_ram PARAMS ((unsigned long, long, const char **));
+static unsigned long insert_ras PARAMS ((unsigned long, long, const char **));
+static unsigned long insert_rbs PARAMS ((unsigned long, long, const char **));
+static long extract_rbs PARAMS ((unsigned long, int *));
+static unsigned long insert_sh6 PARAMS ((unsigned long, long, const char **));
+static long extract_sh6 PARAMS ((unsigned long, int *));
+static unsigned long insert_spr PARAMS ((unsigned long, long, const char **));
+static long extract_spr PARAMS ((unsigned long, int *));
+static unsigned long insert_tbr PARAMS ((unsigned long, long, const char **));
+static long extract_tbr PARAMS ((unsigned long, int *));
+
+/* The operands table.
+
+   The fields are bits, shift, insert, extract, flags.
+
+   We used to put parens around the various additions, like the one
+   for BA just below.  However, that caused trouble with feeble
+   compilers with a limit on depth of a parenthesized expression, like
+   (reportedly) the compiler in Microsoft Developer Studio 5.  So we
+   omit the parens, since the macros are never used in a context where
+   the addition will be ambiguous.  */
+
+const struct powerpc_operand powerpc_operands[] =
+{
+  /* The zero index is used to indicate the end of the list of
+     operands.  */
+#define UNUSED 0
+  { 0, 0, 0, 0, 0 },
+
+  /* The BA field in an XL form instruction.  */
+#define BA UNUSED + 1
+#define BA_MASK (0x1f << 16)
+  { 5, 16, 0, 0, PPC_OPERAND_CR },
+
+  /* The BA field in an XL form instruction when it must be the same
+     as the BT field in the same instruction.  */
+#define BAT BA + 1
+  { 5, 16, insert_bat, extract_bat, PPC_OPERAND_FAKE },
+
+  /* The BB field in an XL form instruction.  */
+#define BB BAT + 1
+#define BB_MASK (0x1f << 11)
+  { 5, 11, 0, 0, PPC_OPERAND_CR },
+
+  /* The BB field in an XL form instruction when it must be the same
+     as the BA field in the same instruction.  */
+#define BBA BB + 1
+  { 5, 11, insert_bba, extract_bba, PPC_OPERAND_FAKE },
+
+  /* The BD field in a B form instruction.  The lower two bits are
+     forced to zero.  */
+#define BD BBA + 1
+  { 16, 0, insert_bd, extract_bd, PPC_OPERAND_RELATIVE | PPC_OPERAND_SIGNED },
+
+  /* The BD field in a B form instruction when absolute addressing is
+     used.  */
+#define BDA BD + 1
+  { 16, 0, insert_bd, extract_bd, PPC_OPERAND_ABSOLUTE | PPC_OPERAND_SIGNED },
+
+  /* The BD field in a B form instruction when the - modifier is used.
+     This sets the y bit of the BO field appropriately.  */
+#define BDM BDA + 1
+  { 16, 0, insert_bdm, extract_bdm,
+      PPC_OPERAND_RELATIVE | PPC_OPERAND_SIGNED },
+
+  /* The BD field in a B form instruction when the - modifier is used
+     and absolute address is used.  */
+#define BDMA BDM + 1
+  { 16, 0, insert_bdm, extract_bdm,
+      PPC_OPERAND_ABSOLUTE | PPC_OPERAND_SIGNED },
+
+  /* The BD field in a B form instruction when the + modifier is used.
+     This sets the y bit of the BO field appropriately.  */
+#define BDP BDMA + 1
+  { 16, 0, insert_bdp, extract_bdp,
+      PPC_OPERAND_RELATIVE | PPC_OPERAND_SIGNED },
+
+  /* The BD field in a B form instruction when the + modifier is used
+     and absolute addressing is used.  */
+#define BDPA BDP + 1
+  { 16, 0, insert_bdp, extract_bdp,
+      PPC_OPERAND_ABSOLUTE | PPC_OPERAND_SIGNED },
+
+  /* The BF field in an X or XL form instruction.  */
+#define BF BDPA + 1
+  { 3, 23, 0, 0, PPC_OPERAND_CR },
+
+  /* An optional BF field.  This is used for comparison instructions,
+     in which an omitted BF field is taken as zero.  */
+#define OBF BF + 1
+  { 3, 23, 0, 0, PPC_OPERAND_CR | PPC_OPERAND_OPTIONAL },
+
+  /* The BFA field in an X or XL form instruction.  */
+#define BFA OBF + 1
+  { 3, 18, 0, 0, PPC_OPERAND_CR },
+
+  /* The BI field in a B form or XL form instruction.  */
+#define BI BFA + 1
+#define BI_MASK (0x1f << 16)
+  { 5, 16, 0, 0, PPC_OPERAND_CR },
+
+  /* The BO field in a B form instruction.  Certain values are
+     illegal.  */
+#define BO BI + 1
+#define BO_MASK (0x1f << 21)
+  { 5, 21, insert_bo, extract_bo, 0 },
+
+  /* The BO field in a B form instruction when the + or - modifier is
+     used.  This is like the BO field, but it must be even.  */
+#define BOE BO + 1
+  { 5, 21, insert_boe, extract_boe, 0 },
+
+  /* The BT field in an X or XL form instruction.  */
+#define BT BOE + 1
+  { 5, 21, 0, 0, PPC_OPERAND_CR },
+
+  /* The condition register number portion of the BI field in a B form
+     or XL form instruction.  This is used for the extended
+     conditional branch mnemonics, which set the lower two bits of the
+     BI field.  This field is optional.  */
+#define CR BT + 1
+  { 3, 18, 0, 0, PPC_OPERAND_CR | PPC_OPERAND_OPTIONAL },
+
+  /* The D field in a D form instruction.  This is a displacement off
+     a register, and implies that the next operand is a register in
+     parentheses.  */
+#define D CR + 1
+  { 16, 0, 0, 0, PPC_OPERAND_PARENS | PPC_OPERAND_SIGNED },
+
+  /* The DS field in a DS form instruction.  This is like D, but the
+     lower two bits are forced to zero.  */
+#define DS D + 1
+  { 16, 0, insert_ds, extract_ds, PPC_OPERAND_PARENS | PPC_OPERAND_SIGNED },
+
+  /* The E field in a wrteei instruction.  */
+#define E DS + 1
+  { 1, 15, 0, 0, 0 },
+
+  /* The FL1 field in a POWER SC form instruction.  */
+#define FL1 E + 1
+  { 4, 12, 0, 0, 0 },
+
+  /* The FL2 field in a POWER SC form instruction.  */
+#define FL2 FL1 + 1
+  { 3, 2, 0, 0, 0 },
+
+  /* The FLM field in an XFL form instruction.  */
+#define FLM FL2 + 1
+  { 8, 17, 0, 0, 0 },
+
+  /* The FRA field in an X or A form instruction.  */
+#define FRA FLM + 1
+#define FRA_MASK (0x1f << 16)
+  { 5, 16, 0, 0, PPC_OPERAND_FPR },
+
+  /* The FRB field in an X or A form instruction.  */
+#define FRB FRA + 1
+#define FRB_MASK (0x1f << 11)
+  { 5, 11, 0, 0, PPC_OPERAND_FPR },
+
+  /* The FRC field in an A form instruction.  */
+#define FRC FRB + 1
+#define FRC_MASK (0x1f << 6)
+  { 5, 6, 0, 0, PPC_OPERAND_FPR },
+
+  /* The FRS field in an X form instruction or the FRT field in a D, X
+     or A form instruction.  */
+#define FRS FRC + 1
+#define FRT FRS
+  { 5, 21, 0, 0, PPC_OPERAND_FPR },
+
+  /* The FXM field in an XFX instruction.  */
+#define FXM FRS + 1
+#define FXM_MASK (0xff << 12)
+  { 8, 12, 0, 0, 0 },
+
+  /* The L field in a D or X form instruction.  */
+#define L FXM + 1
+  { 1, 21, 0, 0, PPC_OPERAND_OPTIONAL },
+
+  /* The LEV field in a POWER SC form instruction.  */
+#define LEV L + 1
+  { 7, 5, 0, 0, 0 },
+
+  /* The LI field in an I form instruction.  The lower two bits are
+     forced to zero.  */
+#define LI LEV + 1
+  { 26, 0, insert_li, extract_li, PPC_OPERAND_RELATIVE | PPC_OPERAND_SIGNED },
+
+  /* The LI field in an I form instruction when used as an absolute
+     address.  */
+#define LIA LI + 1
+  { 26, 0, insert_li, extract_li, PPC_OPERAND_ABSOLUTE | PPC_OPERAND_SIGNED },
+
+  /* The MB field in an M form instruction.  */
+#define MB LIA + 1
+#define MB_MASK (0x1f << 6)
+  { 5, 6, 0, 0, 0 },
+
+  /* The ME field in an M form instruction.  */
+#define ME MB + 1
+#define ME_MASK (0x1f << 1)
+  { 5, 1, 0, 0, 0 },
+
+  /* The MB and ME fields in an M form instruction expressed a single
+     operand which is a bitmask indicating which bits to select.  This
+     is a two operand form using PPC_OPERAND_NEXT.  See the
+     description in opcode/ppc.h for what this means.  */
+#define MBE ME + 1
+  { 5, 6, 0, 0, PPC_OPERAND_OPTIONAL | PPC_OPERAND_NEXT },
+  { 32, 0, insert_mbe, extract_mbe, 0 },
+
+  /* The MB or ME field in an MD or MDS form instruction.  The high
+     bit is wrapped to the low end.  */
+#define MB6 MBE + 2
+#define ME6 MB6
+#define MB6_MASK (0x3f << 5)
+  { 6, 5, insert_mb6, extract_mb6, 0 },
+
+  /* The NB field in an X form instruction.  The value 32 is stored as
+     0.  */
+#define NB MB6 + 1
+  { 6, 11, insert_nb, extract_nb, 0 },
+
+  /* The NSI field in a D form instruction.  This is the same as the
+     SI field, only negated.  */
+#define NSI NB + 1
+  { 16, 0, insert_nsi, extract_nsi,
+      PPC_OPERAND_NEGATIVE | PPC_OPERAND_SIGNED },
+
+  /* The RA field in an D, DS, X, XO, M, or MDS form instruction.  */
+#define RA NSI + 1
+#define RA_MASK (0x1f << 16)
+  { 5, 16, 0, 0, PPC_OPERAND_GPR },
+
+  /* The RA field in a D or X form instruction which is an updating
+     load, which means that the RA field may not be zero and may not
+     equal the RT field.  */
+#define RAL RA + 1
+  { 5, 16, insert_ral, 0, PPC_OPERAND_GPR },
+
+  /* The RA field in an lmw instruction, which has special value
+     restrictions.  */
+#define RAM RAL + 1
+  { 5, 16, insert_ram, 0, PPC_OPERAND_GPR },
+
+  /* The RA field in a D or X form instruction which is an updating
+     store or an updating floating point load, which means that the RA
+     field may not be zero.  */
+#define RAS RAM + 1
+  { 5, 16, insert_ras, 0, PPC_OPERAND_GPR },
+
+  /* The RB field in an X, XO, M, or MDS form instruction.  */
+#define RB RAS + 1
+#define RB_MASK (0x1f << 11)
+  { 5, 11, 0, 0, PPC_OPERAND_GPR },
+
+  /* The RB field in an X form instruction when it must be the same as
+     the RS field in the instruction.  This is used for extended
+     mnemonics like mr.  */
+#define RBS RB + 1
+  { 5, 1, insert_rbs, extract_rbs, PPC_OPERAND_FAKE },
+
+  /* The RS field in a D, DS, X, XFX, XS, M, MD or MDS form
+     instruction or the RT field in a D, DS, X, XFX or XO form
+     instruction.  */
+#define RS RBS + 1
+#define RT RS
+#define RT_MASK (0x1f << 21)
+  { 5, 21, 0, 0, PPC_OPERAND_GPR },
+
+  /* The SH field in an X or M form instruction.  */
+#define SH RS + 1
+#define SH_MASK (0x1f << 11)
+  { 5, 11, 0, 0, 0 },
+
+  /* The SH field in an MD form instruction.  This is split.  */
+#define SH6 SH + 1
+#define SH6_MASK ((0x1f << 11) | (1 << 1))
+  { 6, 1, insert_sh6, extract_sh6, 0 },
+
+  /* The SI field in a D form instruction.  */
+#define SI SH6 + 1
+  { 16, 0, 0, 0, PPC_OPERAND_SIGNED },
+
+  /* The SI field in a D form instruction when we accept a wide range
+     of positive values.  */
+#define SISIGNOPT SI + 1
+  { 16, 0, 0, 0, PPC_OPERAND_SIGNED | PPC_OPERAND_SIGNOPT },
+
+  /* The SPR field in an XFX form instruction.  This is flipped--the
+     lower 5 bits are stored in the upper 5 and vice- versa.  */
+#define SPR SISIGNOPT + 1
+#define SPR_MASK (0x3ff << 11)
+  { 10, 11, insert_spr, extract_spr, 0 },
+
+  /* The BAT index number in an XFX form m[ft]ibat[lu] instruction.  */
+#define SPRBAT SPR + 1
+#define SPRBAT_MASK (0x3 << 17)
+  { 2, 17, 0, 0, 0 },
+
+  /* The SPRG register number in an XFX form m[ft]sprg instruction.  */
+#define SPRG SPRBAT + 1
+#define SPRG_MASK (0x3 << 16)
+  { 2, 16, 0, 0, 0 },
+
+  /* The SR field in an X form instruction.  */
+#define SR SPRG + 1
+  { 4, 16, 0, 0, 0 },
+
+  /* The SV field in a POWER SC form instruction.  */
+#define SV SR + 1
+  { 14, 2, 0, 0, 0 },
+
+  /* The TBR field in an XFX form instruction.  This is like the SPR
+     field, but it is optional.  */
+#define TBR SV + 1
+  { 10, 11, insert_tbr, extract_tbr, PPC_OPERAND_OPTIONAL },
+
+  /* The TO field in a D or X form instruction.  */
+#define TO TBR + 1
+#define TO_MASK (0x1f << 21)
+  { 5, 21, 0, 0, 0 },
+
+  /* The U field in an X form instruction.  */
+#define U TO + 1
+  { 4, 12, 0, 0, 0 },
+
+  /* The UI field in a D form instruction.  */
+#define UI U + 1
+  { 16, 0, 0, 0, 0 },
+
+  /* The VA field in a VA, VX or VXR form instruction. */
+#define VA UI + 1
+#define VA_MASK	(0x1f << 16)
+  {5, 16, 0, 0, PPC_OPERAND_VR},
+
+  /* The VB field in a VA, VX or VXR form instruction. */
+#define VB VA + 1
+#define VB_MASK (0x1f << 11)
+  {5, 11, 0, 0, PPC_OPERAND_VR}, 
+
+  /* The VC field in a VA form instruction. */
+#define VC VB + 1
+#define VC_MASK (0x1f << 6)
+  {5, 6, 0, 0, PPC_OPERAND_VR},
+
+  /* The VD or VS field in a VA, VX, VXR or X form instruction. */
+#define VD VC + 1
+#define VS VD
+#define VD_MASK (0x1f << 21)
+  {5, 21, 0, 0, PPC_OPERAND_VR},
+
+  /* The SIMM field in a VX form instruction. */
+#define SIMM VD + 1
+  { 5, 16, 0, 0, PPC_OPERAND_SIGNED},
+
+  /* The UIMM field in a VX form instruction. */
+#define UIMM SIMM + 1
+  { 5, 16, 0, 0, 0 },
+
+  /* The SHB field in a VA form instruction. */
+#define SHB UIMM + 1
+  { 4, 6, 0, 0, 0 },
+};
+
+/* The functions used to insert and extract complicated operands.  */
+
+/* The BA field in an XL form instruction when it must be the same as
+   the BT field in the same instruction.  This operand is marked FAKE.
+   The insertion function just copies the BT field into the BA field,
+   and the extraction function just checks that the fields are the
+   same.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_bat (insn, value, errmsg)
+     unsigned long insn;
+     long value ATTRIBUTE_UNUSED;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | (((insn >> 21) & 0x1f) << 16);
+}
+
+static long
+extract_bat (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  if (invalid != (int *) NULL
+      && ((insn >> 21) & 0x1f) != ((insn >> 16) & 0x1f))
+    *invalid = 1;
+  return 0;
+}
+
+/* The BB field in an XL form instruction when it must be the same as
+   the BA field in the same instruction.  This operand is marked FAKE.
+   The insertion function just copies the BA field into the BB field,
+   and the extraction function just checks that the fields are the
+   same.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_bba (insn, value, errmsg)
+     unsigned long insn;
+     long value ATTRIBUTE_UNUSED;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | (((insn >> 16) & 0x1f) << 11);
+}
+
+static long
+extract_bba (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  if (invalid != (int *) NULL
+      && ((insn >> 16) & 0x1f) != ((insn >> 11) & 0x1f))
+    *invalid = 1;
+  return 0;
+}
+
+/* The BD field in a B form instruction.  The lower two bits are
+   forced to zero.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_bd (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | (value & 0xfffc);
+}
+
+/*ARGSUSED*/
+static long
+extract_bd (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  if ((insn & 0x8000) != 0)
+    return (insn & 0xfffc) - 0x10000;
+  else
+    return insn & 0xfffc;
+}
+
+/* The BD field in a B form instruction when the - modifier is used.
+   This modifier means that the branch is not expected to be taken.
+   We must set the y bit of the BO field to 1 if the offset is
+   negative.  When extracting, we require that the y bit be 1 and that
+   the offset be positive, since if the y bit is 0 we just want to
+   print the normal form of the instruction.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_bdm (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  if ((value & 0x8000) != 0)
+    insn |= 1 << 21;
+  return insn | (value & 0xfffc);
+}
+
+static long
+extract_bdm (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  if (invalid != (int *) NULL
+      && ((insn & (1 << 21)) == 0
+	  || (insn & (1 << 15)) == 0))
+    *invalid = 1;
+  if ((insn & 0x8000) != 0)
+    return (insn & 0xfffc) - 0x10000;
+  else
+    return insn & 0xfffc;
+}
+
+/* The BD field in a B form instruction when the + modifier is used.
+   This is like BDM, above, except that the branch is expected to be
+   taken.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_bdp (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  if ((value & 0x8000) == 0)
+    insn |= 1 << 21;
+  return insn | (value & 0xfffc);
+}
+
+static long
+extract_bdp (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  if (invalid != (int *) NULL
+      && ((insn & (1 << 21)) == 0
+	  || (insn & (1 << 15)) != 0))
+    *invalid = 1;
+  if ((insn & 0x8000) != 0)
+    return (insn & 0xfffc) - 0x10000;
+  else
+    return insn & 0xfffc;
+}
+
+/* Check for legal values of a BO field.  */
+
+static int
+valid_bo (value)
+     long value;
+{
+  /* Certain encodings have bits that are required to be zero.  These
+     are (z must be zero, y may be anything):
+         001zy
+	 011zy
+	 1z00y
+	 1z01y
+	 1z1zz
+     */
+  switch (value & 0x14)
+    {
+    default:
+    case 0:
+      return 1;
+    case 0x4:
+      return (value & 0x2) == 0;
+    case 0x10:
+      return (value & 0x8) == 0;
+    case 0x14:
+      return value == 0x14;
+    }
+}
+
+/* The BO field in a B form instruction.  Warn about attempts to set
+   the field to an illegal value.  */
+
+static unsigned long
+insert_bo (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  if (errmsg != (const char **) NULL
+      && ! valid_bo (value))
+    *errmsg = _("invalid conditional option");
+  return insn | ((value & 0x1f) << 21);
+}
+
+static long
+extract_bo (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  long value;
+
+  value = (insn >> 21) & 0x1f;
+  if (invalid != (int *) NULL
+      && ! valid_bo (value))
+    *invalid = 1;
+  return value;
+}
+
+/* The BO field in a B form instruction when the + or - modifier is
+   used.  This is like the BO field, but it must be even.  When
+   extracting it, we force it to be even.  */
+
+static unsigned long
+insert_boe (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  if (errmsg != (const char **) NULL)
+    {
+      if (! valid_bo (value))
+	*errmsg = _("invalid conditional option");
+      else if ((value & 1) != 0)
+	*errmsg = _("attempt to set y bit when using + or - modifier");
+    }
+  return insn | ((value & 0x1f) << 21);
+}
+
+static long
+extract_boe (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  long value;
+
+  value = (insn >> 21) & 0x1f;
+  if (invalid != (int *) NULL
+      && ! valid_bo (value))
+    *invalid = 1;
+  return value & 0x1e;
+}
+
+/* The DS field in a DS form instruction.  This is like D, but the
+   lower two bits are forced to zero.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_ds (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | (value & 0xfffc);
+}
+
+/*ARGSUSED*/
+static long
+extract_ds (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  if ((insn & 0x8000) != 0)
+    return (insn & 0xfffc) - 0x10000;
+  else
+    return insn & 0xfffc;
+}
+
+/* The LI field in an I form instruction.  The lower two bits are
+   forced to zero.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_li (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  if ((value & 3) != 0 && errmsg != (const char **) NULL)
+    *errmsg = _("ignoring least significant bits in branch offset");
+  return insn | (value & 0x3fffffc);
+}
+
+/*ARGSUSED*/
+static long
+extract_li (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  if ((insn & 0x2000000) != 0)
+    return (insn & 0x3fffffc) - 0x4000000;
+  else
+    return insn & 0x3fffffc;
+}
+
+/* The MB and ME fields in an M form instruction expressed as a single
+   operand which is itself a bitmask.  The extraction function always
+   marks it as invalid, since we never want to recognize an
+   instruction which uses a field of this type.  */
+
+static unsigned long
+insert_mbe (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  unsigned long uval, mask;
+  int mb, me, mx, count, last;
+
+  uval = value;
+
+  if (uval == 0)
+    {
+      if (errmsg != (const char **) NULL)
+	*errmsg = _("illegal bitmask");
+      return insn;
+    }
+
+  mb = 0;
+  me = 32;
+  if ((uval & 1) != 0)
+    last = 1;
+  else
+    last = 0;
+  count = 0;
+
+  /* mb: location of last 0->1 transition */
+  /* me: location of last 1->0 transition */
+  /* count: # transitions */
+
+  for (mx = 0, mask = 1 << 31; mx < 32; ++mx, mask >>= 1)
+    {
+      if ((uval & mask) && !last)
+	{
+	  ++count;
+	  mb = mx;
+	  last = 1;
+	}
+      else if (!(uval & mask) && last)
+	{
+	  ++count;
+	  me = mx;
+	  last = 0;
+	}
+    }
+  if (me == 0)
+    me = 32;
+
+  if (count != 2 && (count != 0 || ! last))
+    {
+      if (errmsg != (const char **) NULL)
+	*errmsg = _("illegal bitmask");
+    }
+
+  return insn | (mb << 6) | ((me - 1) << 1);
+}
+
+static long
+extract_mbe (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  long ret;
+  int mb, me;
+  int i;
+
+  if (invalid != (int *) NULL)
+    *invalid = 1;
+
+  mb = (insn >> 6) & 0x1f;
+  me = (insn >> 1) & 0x1f;
+  if (mb < me + 1)
+    {
+      ret = 0;
+      for (i = mb; i <= me; i++)
+	ret |= (long) 1 << (31 - i);
+    }
+  else if (mb == me + 1)
+    ret = ~0;
+  else /* (mb > me + 1) */
+    {
+      ret = ~ (long) 0;
+      for (i = me + 1; i < mb; i++)
+	ret &= ~ ((long) 1 << (31 - i));
+    }
+  return ret;
+}
+
+/* The MB or ME field in an MD or MDS form instruction.  The high bit
+   is wrapped to the low end.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_mb6 (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | ((value & 0x1f) << 6) | (value & 0x20);
+}
+
+/*ARGSUSED*/
+static long
+extract_mb6 (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  return ((insn >> 6) & 0x1f) | (insn & 0x20);
+}
+
+/* The NB field in an X form instruction.  The value 32 is stored as
+   0.  */
+
+static unsigned long
+insert_nb (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  if (value < 0 || value > 32)
+    *errmsg = _("value out of range");
+  if (value == 32)
+    value = 0;
+  return insn | ((value & 0x1f) << 11);
+}
+
+/*ARGSUSED*/
+static long
+extract_nb (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  long ret;
+
+  ret = (insn >> 11) & 0x1f;
+  if (ret == 0)
+    ret = 32;
+  return ret;
+}
+
+/* The NSI field in a D form instruction.  This is the same as the SI
+   field, only negated.  The extraction function always marks it as
+   invalid, since we never want to recognize an instruction which uses
+   a field of this type.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_nsi (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | ((- value) & 0xffff);
+}
+
+static long
+extract_nsi (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  if (invalid != (int *) NULL)
+    *invalid = 1;
+  if ((insn & 0x8000) != 0)
+    return - ((long)(insn & 0xffff) - 0x10000);
+  else
+    return - (long)(insn & 0xffff);
+}
+
+/* The RA field in a D or X form instruction which is an updating
+   load, which means that the RA field may not be zero and may not
+   equal the RT field.  */
+
+static unsigned long
+insert_ral (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  if (value == 0
+      || (unsigned long) value == ((insn >> 21) & 0x1f))
+    *errmsg = "invalid register operand when updating";
+  return insn | ((value & 0x1f) << 16);
+}
+
+/* The RA field in an lmw instruction, which has special value
+   restrictions.  */
+
+static unsigned long
+insert_ram (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  if ((unsigned long) value >= ((insn >> 21) & 0x1f))
+    *errmsg = _("index register in load range");
+  return insn | ((value & 0x1f) << 16);
+}
+
+/* The RA field in a D or X form instruction which is an updating
+   store or an updating floating point load, which means that the RA
+   field may not be zero.  */
+
+static unsigned long
+insert_ras (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg;
+{
+  if (value == 0)
+    *errmsg = _("invalid register operand when updating");
+  return insn | ((value & 0x1f) << 16);
+}
+
+/* The RB field in an X form instruction when it must be the same as
+   the RS field in the instruction.  This is used for extended
+   mnemonics like mr.  This operand is marked FAKE.  The insertion
+   function just copies the BT field into the BA field, and the
+   extraction function just checks that the fields are the same.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_rbs (insn, value, errmsg)
+     unsigned long insn;
+     long value ATTRIBUTE_UNUSED;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | (((insn >> 21) & 0x1f) << 11);
+}
+
+static long
+extract_rbs (insn, invalid)
+     unsigned long insn;
+     int *invalid;
+{
+  if (invalid != (int *) NULL
+      && ((insn >> 21) & 0x1f) != ((insn >> 11) & 0x1f))
+    *invalid = 1;
+  return 0;
+}
+
+/* The SH field in an MD form instruction.  This is split.  */
+
+/*ARGSUSED*/
+static unsigned long
+insert_sh6 (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | ((value & 0x1f) << 11) | ((value & 0x20) >> 4);
+}
+
+/*ARGSUSED*/
+static long
+extract_sh6 (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  return ((insn >> 11) & 0x1f) | ((insn << 4) & 0x20);
+}
+
+/* The SPR field in an XFX form instruction.  This is flipped--the
+   lower 5 bits are stored in the upper 5 and vice- versa.  */
+
+static unsigned long
+insert_spr (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  return insn | ((value & 0x1f) << 16) | ((value & 0x3e0) << 6);
+}
+
+static long
+extract_spr (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  return ((insn >> 16) & 0x1f) | ((insn >> 6) & 0x3e0);
+}
+
+/* The TBR field in an XFX instruction.  This is just like SPR, but it
+   is optional.  When TBR is omitted, it must be inserted as 268 (the
+   magic number of the TB register).  These functions treat 0
+   (indicating an omitted optional operand) as 268.  This means that
+   ``mftb 4,0'' is not handled correctly.  This does not matter very
+   much, since the architecture manual does not define mftb as
+   accepting any values other than 268 or 269.  */
+
+#define TB (268)
+
+static unsigned long
+insert_tbr (insn, value, errmsg)
+     unsigned long insn;
+     long value;
+     const char **errmsg ATTRIBUTE_UNUSED;
+{
+  if (value == 0)
+    value = TB;
+  return insn | ((value & 0x1f) << 16) | ((value & 0x3e0) << 6);
+}
+
+static long
+extract_tbr (insn, invalid)
+     unsigned long insn;
+     int *invalid ATTRIBUTE_UNUSED;
+{
+  long ret;
+
+  ret = ((insn >> 16) & 0x1f) | ((insn >> 6) & 0x3e0);
+  if (ret == TB)
+    ret = 0;
+  return ret;
+}
+
+/* Macros used to form opcodes.  */
+
+/* The main opcode.  */
+#define OP(x) ((((unsigned long)(x)) & 0x3f) << 26)
+#define OP_MASK OP (0x3f)
+
+/* The main opcode combined with a trap code in the TO field of a D
+   form instruction.  Used for extended mnemonics for the trap
+   instructions.  */
+#define OPTO(x,to) (OP (x) | ((((unsigned long)(to)) & 0x1f) << 21))
+#define OPTO_MASK (OP_MASK | TO_MASK)
+
+/* The main opcode combined with a comparison size bit in the L field
+   of a D form or X form instruction.  Used for extended mnemonics for
+   the comparison instructions.  */
+#define OPL(x,l) (OP (x) | ((((unsigned long)(l)) & 1) << 21))
+#define OPL_MASK OPL (0x3f,1)
+
+/* An A form instruction.  */
+#define A(op, xop, rc) (OP (op) | ((((unsigned long)(xop)) & 0x1f) << 1) | (((unsigned long)(rc)) & 1))
+#define A_MASK A (0x3f, 0x1f, 1)
+
+/* An A_MASK with the FRB field fixed.  */
+#define AFRB_MASK (A_MASK | FRB_MASK)
+
+/* An A_MASK with the FRC field fixed.  */
+#define AFRC_MASK (A_MASK | FRC_MASK)
+
+/* An A_MASK with the FRA and FRC fields fixed.  */
+#define AFRAFRC_MASK (A_MASK | FRA_MASK | FRC_MASK)
+
+/* A B form instruction.  */
+#define B(op, aa, lk) (OP (op) | ((((unsigned long)(aa)) & 1) << 1) | ((lk) & 1))
+#define B_MASK B (0x3f, 1, 1)
+
+/* A B form instruction setting the BO field.  */
+#define BBO(op, bo, aa, lk) (B ((op), (aa), (lk)) | ((((unsigned long)(bo)) & 0x1f) << 21))
+#define BBO_MASK BBO (0x3f, 0x1f, 1, 1)
+
+/* A BBO_MASK with the y bit of the BO field removed.  This permits
+   matching a conditional branch regardless of the setting of the y
+   bit.  */
+#define Y_MASK (((unsigned long)1) << 21)
+#define BBOY_MASK (BBO_MASK &~ Y_MASK)
+
+/* A B form instruction setting the BO field and the condition bits of
+   the BI field.  */
+#define BBOCB(op, bo, cb, aa, lk) \
+  (BBO ((op), (bo), (aa), (lk)) | ((((unsigned long)(cb)) & 0x3) << 16))
+#define BBOCB_MASK BBOCB (0x3f, 0x1f, 0x3, 1, 1)
+
+/* A BBOCB_MASK with the y bit of the BO field removed.  */
+#define BBOYCB_MASK (BBOCB_MASK &~ Y_MASK)
+
+/* A BBOYCB_MASK in which the BI field is fixed.  */
+#define BBOYBI_MASK (BBOYCB_MASK | BI_MASK)
+
+/* The main opcode mask with the RA field clear.  */
+#define DRA_MASK (OP_MASK | RA_MASK)
+
+/* A DS form instruction.  */
+#define DSO(op, xop) (OP (op) | ((xop) & 0x3))
+#define DS_MASK DSO (0x3f, 3)
+
+/* An M form instruction.  */
+#define M(op, rc) (OP (op) | ((rc) & 1))
+#define M_MASK M (0x3f, 1)
+
+/* An M form instruction with the ME field specified.  */
+#define MME(op, me, rc) (M ((op), (rc)) | ((((unsigned long)(me)) & 0x1f) << 1))
+
+/* An M_MASK with the MB and ME fields fixed.  */
+#define MMBME_MASK (M_MASK | MB_MASK | ME_MASK)
+
+/* An M_MASK with the SH and ME fields fixed.  */
+#define MSHME_MASK (M_MASK | SH_MASK | ME_MASK)
+
+/* An MD form instruction.  */
+#define MD(op, xop, rc) (OP (op) | ((((unsigned long)(xop)) & 0x7) << 2) | ((rc) & 1))
+#define MD_MASK MD (0x3f, 0x7, 1)
+
+/* An MD_MASK with the MB field fixed.  */
+#define MDMB_MASK (MD_MASK | MB6_MASK)
+
+/* An MD_MASK with the SH field fixed.  */
+#define MDSH_MASK (MD_MASK | SH6_MASK)
+
+/* An MDS form instruction.  */
+#define MDS(op, xop, rc) (OP (op) | ((((unsigned long)(xop)) & 0xf) << 1) | ((rc) & 1))
+#define MDS_MASK MDS (0x3f, 0xf, 1)
+
+/* An MDS_MASK with the MB field fixed.  */
+#define MDSMB_MASK (MDS_MASK | MB6_MASK)
+
+/* An SC form instruction.  */
+#define SC(op, sa, lk) (OP (op) | ((((unsigned long)(sa)) & 1) << 1) | ((lk) & 1))
+#define SC_MASK (OP_MASK | (((unsigned long)0x3ff) << 16) | (((unsigned long)1) << 1) | 1)
+
+/* An VX form instruction. */
+#define VX(op, xop) (OP (op) | (((unsigned long)(xop)) & 0x7ff))
+
+/* The mask for an VX form instruction. */
+#define VX_MASK	VX(0x3f, 0x7ff)
+
+/* An VA form instruction. */
+#define VXA(op, xop) (OP (op) | (((unsigned long)(xop)) & 0x07f))
+
+/* The mask for an VA form instruction. */
+#define VXA_MASK VXA(0x3f, 0x7f)
+
+/* An VXR form instruction. */
+#define VXR(op, xop, rc) (OP (op) | (((rc) & 1) << 10) | (((unsigned long)(xop)) & 0x3ff))
+
+/* The mask for a VXR form instruction. */
+#define VXR_MASK VXR(0x3f, 0x3ff, 1)
+
+/* An X form instruction.  */
+#define X(op, xop) (OP (op) | ((((unsigned long)(xop)) & 0x3ff) << 1))
+
+/* An X form instruction with the RC bit specified.  */
+#define XRC(op, xop, rc) (X ((op), (xop)) | ((rc) & 1))
+
+/* The mask for an X form instruction.  */
+#define X_MASK XRC (0x3f, 0x3ff, 1)
+
+/* An X_MASK with the RA field fixed.  */
+#define XRA_MASK (X_MASK | RA_MASK)
+
+/* An X_MASK with the RB field fixed.  */
+#define XRB_MASK (X_MASK | RB_MASK)
+
+/* An X_MASK with the RT field fixed.  */
+#define XRT_MASK (X_MASK | RT_MASK)
+
+/* An X_MASK with the RA and RB fields fixed.  */
+#define XRARB_MASK (X_MASK | RA_MASK | RB_MASK)
+
+/* An X_MASK with the RT and RA fields fixed.  */
+#define XRTRA_MASK (X_MASK | RT_MASK | RA_MASK)
+
+/* An X form comparison instruction.  */
+#define XCMPL(op, xop, l) (X ((op), (xop)) | ((((unsigned long)(l)) & 1) << 21))
+
+/* The mask for an X form comparison instruction.  */
+#define XCMP_MASK (X_MASK | (((unsigned long)1) << 22))
+
+/* The mask for an X form comparison instruction with the L field
+   fixed.  */
+#define XCMPL_MASK (XCMP_MASK | (((unsigned long)1) << 21))
+
+/* An X form trap instruction with the TO field specified.  */
+#define XTO(op, xop, to) (X ((op), (xop)) | ((((unsigned long)(to)) & 0x1f) << 21))
+#define XTO_MASK (X_MASK | TO_MASK)
+
+/* An X form tlb instruction with the SH field specified.  */
+#define XTLB(op, xop, sh) (X ((op), (xop)) | ((((unsigned long)(sh)) & 0x1f) << 11))
+#define XTLB_MASK (X_MASK | SH_MASK)
+
+/* An XFL form instruction.  */
+#define XFL(op, xop, rc) (OP (op) | ((((unsigned long)(xop)) & 0x3ff) << 1) | (((unsigned long)(rc)) & 1))
+#define XFL_MASK (XFL (0x3f, 0x3ff, 1) | (((unsigned long)1) << 25) | (((unsigned long)1) << 16))
+
+/* An XL form instruction with the LK field set to 0.  */
+#define XL(op, xop) (OP (op) | ((((unsigned long)(xop)) & 0x3ff) << 1))
+
+/* An XL form instruction which uses the LK field.  */
+#define XLLK(op, xop, lk) (XL ((op), (xop)) | ((lk) & 1))
+
+/* The mask for an XL form instruction.  */
+#define XL_MASK XLLK (0x3f, 0x3ff, 1)
+
+/* An XL form instruction which explicitly sets the BO field.  */
+#define XLO(op, bo, xop, lk) \
+  (XLLK ((op), (xop), (lk)) | ((((unsigned long)(bo)) & 0x1f) << 21))
+#define XLO_MASK (XL_MASK | BO_MASK)
+
+/* An XL form instruction which explicitly sets the y bit of the BO
+   field.  */
+#define XLYLK(op, xop, y, lk) (XLLK ((op), (xop), (lk)) | ((((unsigned long)(y)) & 1) << 21))
+#define XLYLK_MASK (XL_MASK | Y_MASK)
+
+/* An XL form instruction which sets the BO field and the condition
+   bits of the BI field.  */
+#define XLOCB(op, bo, cb, xop, lk) \
+  (XLO ((op), (bo), (xop), (lk)) | ((((unsigned long)(cb)) & 3) << 16))
+#define XLOCB_MASK XLOCB (0x3f, 0x1f, 0x3, 0x3ff, 1)
+
+/* An XL_MASK or XLYLK_MASK or XLOCB_MASK with the BB field fixed.  */
+#define XLBB_MASK (XL_MASK | BB_MASK)
+#define XLYBB_MASK (XLYLK_MASK | BB_MASK)
+#define XLBOCBBB_MASK (XLOCB_MASK | BB_MASK)
+
+/* An XL_MASK with the BO and BB fields fixed.  */
+#define XLBOBB_MASK (XL_MASK | BO_MASK | BB_MASK)
+
+/* An XL_MASK with the BO, BI and BB fields fixed.  */
+#define XLBOBIBB_MASK (XL_MASK | BO_MASK | BI_MASK | BB_MASK)
+
+/* An XO form instruction.  */
+#define XO(op, xop, oe, rc) \
+  (OP (op) | ((((unsigned long)(xop)) & 0x1ff) << 1) | ((((unsigned long)(oe)) & 1) << 10) | (((unsigned long)(rc)) & 1))
+#define XO_MASK XO (0x3f, 0x1ff, 1, 1)
+
+/* An XO_MASK with the RB field fixed.  */
+#define XORB_MASK (XO_MASK | RB_MASK)
+
+/* An XS form instruction.  */
+#define XS(op, xop, rc) (OP (op) | ((((unsigned long)(xop)) & 0x1ff) << 2) | (((unsigned long)(rc)) & 1))
+#define XS_MASK XS (0x3f, 0x1ff, 1)
+
+/* A mask for the FXM version of an XFX form instruction.  */
+#define XFXFXM_MASK (X_MASK | (((unsigned long)1) << 20) | (((unsigned long)1) << 11))
+
+/* An XFX form instruction with the FXM field filled in.  */
+#define XFXM(op, xop, fxm) \
+  (X ((op), (xop)) | ((((unsigned long)(fxm)) & 0xff) << 12))
+
+/* An XFX form instruction with the SPR field filled in.  */
+#define XSPR(op, xop, spr) \
+  (X ((op), (xop)) | ((((unsigned long)(spr)) & 0x1f) << 16) | ((((unsigned long)(spr)) & 0x3e0) << 6))
+#define XSPR_MASK (X_MASK | SPR_MASK)
+
+/* An XFX form instruction with the SPR field filled in except for the
+   SPRBAT field.  */
+#define XSPRBAT_MASK (XSPR_MASK &~ SPRBAT_MASK)
+
+/* An XFX form instruction with the SPR field filled in except for the
+   SPRG field.  */
+#define XSPRG_MASK (XSPR_MASK &~ SPRG_MASK)
+
+/* An X form instruction with everything filled in except the E field.  */
+#define XE_MASK (0xffff7fff)
+
+/* The BO encodings used in extended conditional branch mnemonics.  */
+#define BODNZF	(0x0)
+#define BODNZFP	(0x1)
+#define BODZF	(0x2)
+#define BODZFP	(0x3)
+#define BOF	(0x4)
+#define BOFP	(0x5)
+#define BODNZT	(0x8)
+#define BODNZTP	(0x9)
+#define BODZT	(0xa)
+#define BODZTP	(0xb)
+#define BOT	(0xc)
+#define BOTP	(0xd)
+#define BODNZ	(0x10)
+#define BODNZP	(0x11)
+#define BODZ	(0x12)
+#define BODZP	(0x13)
+#define BOU	(0x14)
+
+/* The BI condition bit encodings used in extended conditional branch
+   mnemonics.  */
+#define CBLT	(0)
+#define CBGT	(1)
+#define CBEQ	(2)
+#define CBSO	(3)
+
+/* The TO encodings used in extended trap mnemonics.  */
+#define TOLGT	(0x1)
+#define TOLLT	(0x2)
+#define TOEQ	(0x4)
+#define TOLGE	(0x5)
+#define TOLNL	(0x5)
+#define TOLLE	(0x6)
+#define TOLNG	(0x6)
+#define TOGT	(0x8)
+#define TOGE	(0xc)
+#define TONL	(0xc)
+#define TOLT	(0x10)
+#define TOLE	(0x14)
+#define TONG	(0x14)
+#define TONE	(0x18)
+#define TOU	(0x1f)
+
+/* Smaller names for the flags so each entry in the opcodes table will
+   fit on a single line.  */
+#undef	PPC
+#define PPC     PPC_OPCODE_PPC | PPC_OPCODE_ANY
+#define PPCCOM	PPC_OPCODE_PPC | PPC_OPCODE_COMMON | PPC_OPCODE_ANY
+#define PPC32   PPC_OPCODE_PPC | PPC_OPCODE_32 | PPC_OPCODE_ANY
+#undef PPC64
+#define PPC64   PPC_OPCODE_PPC | PPC_OPCODE_64 | PPC_OPCODE_ANY
+#define PPCONLY	PPC_OPCODE_PPC
+#define PPC403	PPC
+#define PPC405	PPC403
+#define PPC750	PPC
+#define PPC860	PPC
+#define PPCVEC	PPC_OPCODE_ALTIVEC | PPC_OPCODE_ANY
+#define	POWER   PPC_OPCODE_POWER | PPC_OPCODE_ANY
+#define	POWER2	PPC_OPCODE_POWER | PPC_OPCODE_POWER2 | PPC_OPCODE_ANY
+#define PPCPWR2	PPC_OPCODE_PPC | PPC_OPCODE_POWER | PPC_OPCODE_POWER2 | PPC_OPCODE_ANY
+#define	POWER32	PPC_OPCODE_POWER | PPC_OPCODE_ANY | PPC_OPCODE_32
+#define	COM     PPC_OPCODE_POWER | PPC_OPCODE_PPC | PPC_OPCODE_COMMON | PPC_OPCODE_ANY
+#define	COM32   PPC_OPCODE_POWER | PPC_OPCODE_PPC | PPC_OPCODE_COMMON | PPC_OPCODE_ANY | PPC_OPCODE_32
+#define	M601    PPC_OPCODE_POWER | PPC_OPCODE_601 | PPC_OPCODE_ANY
+#define PWRCOM	PPC_OPCODE_POWER | PPC_OPCODE_601 | PPC_OPCODE_COMMON | PPC_OPCODE_ANY
+#define	MFDEC1	PPC_OPCODE_POWER
+#define	MFDEC2	PPC_OPCODE_PPC | PPC_OPCODE_601
+
+/* The opcode table.
+
+   The format of the opcode table is:
+
+   NAME	     OPCODE	MASK		FLAGS		{ OPERANDS }
+
+   NAME is the name of the instruction.
+   OPCODE is the instruction opcode.
+   MASK is the opcode mask; this is used to tell the disassembler
+     which bits in the actual opcode must match OPCODE.
+   FLAGS are flags indicated what processors support the instruction.
+   OPERANDS is the list of operands.
+
+   The disassembler reads the table in order and prints the first
+   instruction which matches, so this table is sorted to put more
+   specific instructions before more general instructions.  It is also
+   sorted by major opcode.  */
+
+const struct powerpc_opcode powerpc_opcodes[] = {
+{ "tdlgti",  OPTO(2,TOLGT), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdllti",  OPTO(2,TOLLT), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdeqi",   OPTO(2,TOEQ), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdlgei",  OPTO(2,TOLGE), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdlnli",  OPTO(2,TOLNL), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdllei",  OPTO(2,TOLLE), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdlngi",  OPTO(2,TOLNG), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdgti",   OPTO(2,TOGT), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdgei",   OPTO(2,TOGE), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdnli",   OPTO(2,TONL), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdlti",   OPTO(2,TOLT), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdlei",   OPTO(2,TOLE), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdngi",   OPTO(2,TONG), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdnei",   OPTO(2,TONE), OPTO_MASK,	PPC64,		{ RA, SI } },
+{ "tdi",     OP(2),	OP_MASK,	PPC64,		{ TO, RA, SI } },
+
+{ "twlgti",  OPTO(3,TOLGT), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tlgti",   OPTO(3,TOLGT), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twllti",  OPTO(3,TOLLT), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tllti",   OPTO(3,TOLLT), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "tweqi",   OPTO(3,TOEQ), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "teqi",    OPTO(3,TOEQ), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twlgei",  OPTO(3,TOLGE), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tlgei",   OPTO(3,TOLGE), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twlnli",  OPTO(3,TOLNL), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tlnli",   OPTO(3,TOLNL), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twllei",  OPTO(3,TOLLE), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tllei",   OPTO(3,TOLLE), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twlngi",  OPTO(3,TOLNG), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tlngi",   OPTO(3,TOLNG), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twgti",   OPTO(3,TOGT), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tgti",    OPTO(3,TOGT), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twgei",   OPTO(3,TOGE), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tgei",    OPTO(3,TOGE), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twnli",   OPTO(3,TONL), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tnli",    OPTO(3,TONL), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twlti",   OPTO(3,TOLT), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tlti",    OPTO(3,TOLT), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twlei",   OPTO(3,TOLE), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tlei",    OPTO(3,TOLE), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twngi",   OPTO(3,TONG), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tngi",    OPTO(3,TONG), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twnei",   OPTO(3,TONE), OPTO_MASK,	PPCCOM,		{ RA, SI } },
+{ "tnei",    OPTO(3,TONE), OPTO_MASK,	PWRCOM,		{ RA, SI } },
+{ "twi",     OP(3),	OP_MASK,	PPCCOM,		{ TO, RA, SI } },
+{ "ti",      OP(3),	OP_MASK,	PWRCOM,		{ TO, RA, SI } },
+
+{ "macchw",	XO(4,172,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchw.",	XO(4,172,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwo",	XO(4,172,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwo.",	XO(4,172,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchws",	XO(4,236,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchws.",	XO(4,236,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwso",	XO(4,236,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwso.",	XO(4,236,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwsu",	XO(4,204,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwsu.",	XO(4,204,0,1), XO_MASK, PPC405,		{ RT, RA, RB } },
+{ "macchwsuo",	XO(4,204,1,0), XO_MASK, PPC405,		{ RT, RA, RB } },
+{ "macchwsuo.",	XO(4,204,1,1), XO_MASK, PPC405,		{ RT, RA, RB } },
+{ "macchwu",	XO(4,140,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwu.",	XO(4,140,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwuo",	XO(4,140,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "macchwuo.",	XO(4,140,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhw",	XO(4,44,0,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhw.",	XO(4,44,0,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwo",	XO(4,44,1,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwo.",	XO(4,44,1,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhws",	XO(4,108,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhws.",	XO(4,108,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwso",	XO(4,108,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwso.",	XO(4,108,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwsu",	XO(4,76,0,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwsu.",	XO(4,76,0,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwsuo",	XO(4,76,1,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwsuo.",	XO(4,76,1,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwu",	XO(4,12,0,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwu.",	XO(4,12,0,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwuo",	XO(4,12,1,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "machhwuo.",	XO(4,12,1,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhw",	XO(4,428,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhw.",	XO(4,428,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwo",	XO(4,428,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwo.",	XO(4,428,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhws",	XO(4,492,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhws.",	XO(4,492,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwso",	XO(4,492,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwso.",	XO(4,492,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwsu",	XO(4,460,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwsu.",	XO(4,460,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwsuo",	XO(4,460,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwsuo.",	XO(4,460,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwu",	XO(4,396,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwu.",	XO(4,396,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwuo",	XO(4,396,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "maclhwuo.",	XO(4,396,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulchw",	XRC(4,168,0),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulchw.",	XRC(4,168,1),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulchwu",	XRC(4,136,0),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulchwu.",	XRC(4,136,1),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulhhw",	XRC(4,40,0),   X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulhhw.",	XRC(4,40,1),   X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulhhwu",	XRC(4,8,0),    X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mulhhwu.",	XRC(4,8,1),    X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mullhw",	XRC(4,424,0),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mullhw.",	XRC(4,424,1),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mullhwu",	XRC(4,392,0),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mullhwu.",	XRC(4,392,1),  X_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchw",	XO(4,174,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchw.",	XO(4,174,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchwo",	XO(4,174,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchwo.",	XO(4,174,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchws",	XO(4,238,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchws.",	XO(4,238,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchwso",	XO(4,238,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmacchwso.",	XO(4,238,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhw",	XO(4,46,0,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhw.",	XO(4,46,0,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhwo",	XO(4,46,1,0),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhwo.",	XO(4,46,1,1),  XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhws",	XO(4,110,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhws.",	XO(4,110,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhwso",	XO(4,110,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmachhwso.",	XO(4,110,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhw",	XO(4,430,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhw.",	XO(4,430,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhwo",	XO(4,430,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhwo.",	XO(4,430,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhws",	XO(4,494,0,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhws.",	XO(4,494,0,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhwso",	XO(4,494,1,0), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "nmaclhwso.",	XO(4,494,1,1), XO_MASK,	PPC405,		{ RT, RA, RB } },
+{ "mfvscr",  VX(4, 1540), VX_MASK,	PPCVEC,		{ VD } },
+{ "mtvscr",  VX(4, 1604), VX_MASK,	PPCVEC,		{ VD } },
+{ "vaddcuw", VX(4,  384), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vaddfp",  VX(4,   10), VX_MASK, 	PPCVEC,		{ VD, VA, VB } },
+{ "vaddsbs", VX(4,  768), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vaddshs", VX(4,  832), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vaddsws", VX(4,  896), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vaddubm", VX(4,    0), VX_MASK, 	PPCVEC,		{ VD, VA, VB } },
+{ "vaddubs", VX(4,  512), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vadduhm", VX(4,   64), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vadduhs", VX(4,  576), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vadduwm", VX(4,  128), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vadduws", VX(4,  640), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vand",    VX(4, 1028), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vandc",   VX(4, 1092), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vavgsb",  VX(4, 1282), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vavgsh",  VX(4, 1346), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vavgsw",  VX(4, 1410), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vavgub",  VX(4, 1026), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vavguh",  VX(4, 1090), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vavguw",  VX(4, 1154), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vcfsx",   VX(4,  842), VX_MASK,	PPCVEC,		{ VD, VB, UIMM } },
+{ "vcfux",   VX(4,  778), VX_MASK,	PPCVEC,		{ VD, VB, UIMM } },
+{ "vcmpbfp",   VXR(4, 966, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpbfp.",  VXR(4, 966, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpeqfp",  VXR(4, 198, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpeqfp.", VXR(4, 198, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpequb",  VXR(4,   6, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpequb.", VXR(4,   6, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpequh",  VXR(4,  70, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpequh.", VXR(4,  70, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpequw",  VXR(4, 134, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpequw.", VXR(4, 134, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgefp",  VXR(4, 454, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgefp.", VXR(4, 454, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtfp",  VXR(4, 710, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtfp.", VXR(4, 710, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtsb",  VXR(4, 774, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtsb.", VXR(4, 774, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtsh",  VXR(4, 838, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtsh.", VXR(4, 838, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtsw",  VXR(4, 902, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtsw.", VXR(4, 902, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtub",  VXR(4, 518, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtub.", VXR(4, 518, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtuh",  VXR(4, 582, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtuh.", VXR(4, 582, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtuw",  VXR(4, 646, 0), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vcmpgtuw.", VXR(4, 646, 1), VXR_MASK, PPCVEC,	{ VD, VA, VB } },
+{ "vctsxs",    VX(4,  970), VX_MASK,	PPCVEC,		{ VD, VB, UIMM } },
+{ "vctuxs",    VX(4,  906), VX_MASK,	PPCVEC,		{ VD, VB, UIMM } },
+{ "vexptefp",  VX(4,  394), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vlogefp",   VX(4,  458), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vmaddfp",   VXA(4,  46), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmaxfp",    VX(4, 1034), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmaxsb",    VX(4,  258), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmaxsh",    VX(4,  322), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmaxsw",    VX(4,  386), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmaxub",    VX(4,    2), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmaxuh",    VX(4,   66), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmaxuw",    VX(4,  130), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmhaddshs", VXA(4,  32), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmhraddshs", VXA(4, 33), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vminfp",    VX(4, 1098), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vminsb",    VX(4,  770), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vminsh",    VX(4,  834), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vminsw",    VX(4,  898), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vminub",    VX(4,  514), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vminuh",    VX(4,  578), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vminuw",    VX(4,  642), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmladduhm", VXA(4,  34), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmrghb",    VX(4,   12), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmrghh",    VX(4,   76), VX_MASK,    PPCVEC,		{ VD, VA, VB } },
+{ "vmrghw",    VX(4,  140), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmrglb",    VX(4,  268), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmrglh",    VX(4,  332), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmrglw",    VX(4,  396), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmsummbm",  VXA(4,  37), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmsumshm",  VXA(4,  40), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmsumshs",  VXA(4,  41), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmsumubm",  VXA(4,  36), VXA_MASK,   PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmsumuhm",  VXA(4,  38), VXA_MASK,   PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmsumuhs",  VXA(4,  39), VXA_MASK,   PPCVEC,		{ VD, VA, VB, VC } },
+{ "vmulesb",   VX(4,  776), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmulesh",   VX(4,  840), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmuleub",   VX(4,  520), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmuleuh",   VX(4,  584), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmulosb",   VX(4,  264), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmulosh",   VX(4,  328), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmuloub",   VX(4,    8), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vmulouh",   VX(4,   72), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vnmsubfp",  VXA(4,  47), VXA_MASK,	PPCVEC,		{ VD, VA, VC, VB } },
+{ "vnor",      VX(4, 1284), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vor",       VX(4, 1156), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vperm",     VXA(4,  43), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vpkpx",     VX(4,  782), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkshss",   VX(4,  398), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkshus",   VX(4,  270), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkswss",   VX(4,  462), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkswus",   VX(4,  334), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkuhum",   VX(4,   14), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkuhus",   VX(4,  142), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkuwum",   VX(4,   78), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vpkuwus",   VX(4,  206), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vrefp",     VX(4,  266), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vrfim",     VX(4,  714), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vrfin",     VX(4,  522), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vrfip",     VX(4,  650), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vrfiz",     VX(4,  586), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vrlb",      VX(4,    4), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vrlh",      VX(4,   68), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vrlw",      VX(4,  132), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vrsqrtefp", VX(4,  330), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vsel",      VXA(4,  42), VXA_MASK,	PPCVEC,		{ VD, VA, VB, VC } },
+{ "vsl",       VX(4,  452), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vslb",      VX(4,  260), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsldoi",    VXA(4,  44), VXA_MASK,	PPCVEC,		{ VD, VA, VB, SHB } },
+{ "vslh",      VX(4,  324), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vslo",      VX(4, 1036), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vslw",      VX(4,  388), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vspltb",    VX(4,  524), VX_MASK,	PPCVEC,		{ VD, VB, UIMM } },
+{ "vsplth",    VX(4,  588), VX_MASK,	PPCVEC,		{ VD, VB, UIMM } },
+{ "vspltisb",  VX(4,  780), VX_MASK,	PPCVEC,		{ VD, SIMM } },
+{ "vspltish",  VX(4,  844), VX_MASK,	PPCVEC,		{ VD, SIMM } },
+{ "vspltisw",  VX(4,  908), VX_MASK,	PPCVEC,		{ VD, SIMM } },
+{ "vspltw",    VX(4,  652), VX_MASK,	PPCVEC,		{ VD, VB, UIMM } },
+{ "vsr",       VX(4,  708), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsrab",     VX(4,  772), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsrah",     VX(4,  836), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsraw",     VX(4,  900), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsrb",      VX(4,  516), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsrh",      VX(4,  580), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsro",      VX(4, 1100), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsrw",      VX(4,  644), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubcuw",   VX(4, 1408), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubfp",    VX(4,   74), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubsbs",   VX(4, 1792), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubshs",   VX(4, 1856), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubsws",   VX(4, 1920), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsububm",   VX(4, 1024), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsububs",   VX(4, 1536), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubuhm",   VX(4, 1088), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubuhs",   VX(4, 1600), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubuwm",   VX(4, 1152), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsubuws",   VX(4, 1664), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsumsws",   VX(4, 1928), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsum2sws",  VX(4, 1672), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsum4sbs",  VX(4, 1800), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsum4shs",  VX(4, 1608), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vsum4ubs",  VX(4, 1544), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+{ "vupkhpx",   VX(4,  846), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vupkhsb",   VX(4,  526), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vupkhsh",   VX(4,  590), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vupklpx",   VX(4,  974), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vupklsb",   VX(4,  654), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vupklsh",   VX(4,  718), VX_MASK,	PPCVEC,		{ VD, VB } },
+{ "vxor",      VX(4, 1220), VX_MASK,	PPCVEC,		{ VD, VA, VB } },
+
+{ "mulli",   OP(7),	OP_MASK,	PPCCOM,		{ RT, RA, SI } },
+{ "muli",    OP(7),	OP_MASK,	PWRCOM,		{ RT, RA, SI } },
+
+{ "subfic",  OP(8),	OP_MASK,	PPCCOM,		{ RT, RA, SI } },
+{ "sfi",     OP(8),	OP_MASK,	PWRCOM,		{ RT, RA, SI } },
+
+{ "dozi",    OP(9),	OP_MASK,	M601,		{ RT, RA, SI } },
+
+{ "cmplwi",  OPL(10,0),	OPL_MASK,	PPCCOM,		{ OBF, RA, UI } },
+{ "cmpldi",  OPL(10,1), OPL_MASK,	PPC64,		{ OBF, RA, UI } },
+{ "cmpli",   OP(10),	OP_MASK,	PPCONLY,	{ BF, L, RA, UI } },
+{ "cmpli",   OP(10),	OP_MASK,	PWRCOM,		{ BF, RA, UI } },
+
+{ "cmpwi",   OPL(11,0),	OPL_MASK,	PPCCOM,		{ OBF, RA, SI } },
+{ "cmpdi",   OPL(11,1),	OPL_MASK,	PPC64,		{ OBF, RA, SI } },
+{ "cmpi",    OP(11),	OP_MASK,	PPCONLY,	{ BF, L, RA, SI } },
+{ "cmpi",    OP(11),	OP_MASK,	PWRCOM,		{ BF, RA, SI } },
+
+{ "addic",   OP(12),	OP_MASK,	PPCCOM,		{ RT, RA, SI } },
+{ "ai",	     OP(12),	OP_MASK,	PWRCOM,		{ RT, RA, SI } },
+{ "subic",   OP(12),	OP_MASK,	PPCCOM,		{ RT, RA, NSI } },
+
+{ "addic.",  OP(13),	OP_MASK,	PPCCOM,		{ RT, RA, SI } },
+{ "ai.",     OP(13),	OP_MASK,	PWRCOM,		{ RT, RA, SI } },
+{ "subic.",  OP(13),	OP_MASK,	PPCCOM,		{ RT, RA, NSI } },
+
+{ "li",	     OP(14),	DRA_MASK,	PPCCOM,		{ RT, SI } },
+{ "lil",     OP(14),	DRA_MASK,	PWRCOM,		{ RT, SI } },
+{ "addi",    OP(14),	OP_MASK,	PPCCOM,		{ RT, RA, SI } },
+{ "cal",     OP(14),	OP_MASK,	PWRCOM,		{ RT, D, RA } },
+{ "subi",    OP(14),	OP_MASK,	PPCCOM,		{ RT, RA, NSI } },
+{ "la",	     OP(14),	OP_MASK,	PPCCOM,		{ RT, D, RA } },
+
+{ "lis",     OP(15),	DRA_MASK,	PPCCOM,		{ RT, SISIGNOPT } },
+{ "liu",     OP(15),	DRA_MASK,	PWRCOM,		{ RT, SISIGNOPT } },
+{ "addis",   OP(15),	OP_MASK,	PPCCOM,		{ RT,RA,SISIGNOPT } },
+{ "cau",     OP(15),	OP_MASK,	PWRCOM,		{ RT,RA,SISIGNOPT } },
+{ "subis",   OP(15),	OP_MASK,	PPCCOM,		{ RT, RA, NSI } },
+
+{ "bdnz-",   BBO(16,BODNZ,0,0), BBOYBI_MASK, PPCCOM,	{ BDM } },
+{ "bdnz+",   BBO(16,BODNZ,0,0), BBOYBI_MASK, PPCCOM,	{ BDP } },
+{ "bdnz",    BBO(16,BODNZ,0,0), BBOYBI_MASK, PPCCOM,	{ BD } },
+{ "bdn",     BBO(16,BODNZ,0,0), BBOYBI_MASK, PWRCOM,	{ BD } },
+{ "bdnzl-",  BBO(16,BODNZ,0,1), BBOYBI_MASK, PPCCOM,	{ BDM } },
+{ "bdnzl+",  BBO(16,BODNZ,0,1), BBOYBI_MASK, PPCCOM,	{ BDP } },
+{ "bdnzl",   BBO(16,BODNZ,0,1), BBOYBI_MASK, PPCCOM,	{ BD } },
+{ "bdnl",    BBO(16,BODNZ,0,1), BBOYBI_MASK, PWRCOM,	{ BD } },
+{ "bdnza-",  BBO(16,BODNZ,1,0), BBOYBI_MASK, PPCCOM,	{ BDMA } },
+{ "bdnza+",  BBO(16,BODNZ,1,0), BBOYBI_MASK, PPCCOM,	{ BDPA } },
+{ "bdnza",   BBO(16,BODNZ,1,0), BBOYBI_MASK, PPCCOM,	{ BDA } },
+{ "bdna",    BBO(16,BODNZ,1,0), BBOYBI_MASK, PWRCOM,	{ BDA } },
+{ "bdnzla-", BBO(16,BODNZ,1,1), BBOYBI_MASK, PPCCOM,	{ BDMA } },
+{ "bdnzla+", BBO(16,BODNZ,1,1), BBOYBI_MASK, PPCCOM,	{ BDPA } },
+{ "bdnzla",  BBO(16,BODNZ,1,1), BBOYBI_MASK, PPCCOM,	{ BDA } },
+{ "bdnla",   BBO(16,BODNZ,1,1), BBOYBI_MASK, PWRCOM,	{ BDA } },
+{ "bdz-",    BBO(16,BODZ,0,0),  BBOYBI_MASK, PPCCOM,	{ BDM } },
+{ "bdz+",    BBO(16,BODZ,0,0),  BBOYBI_MASK, PPCCOM,	{ BDP } },
+{ "bdz",     BBO(16,BODZ,0,0),  BBOYBI_MASK, COM,	{ BD } },
+{ "bdzl-",   BBO(16,BODZ,0,1),  BBOYBI_MASK, PPCCOM,	{ BDM } },
+{ "bdzl+",   BBO(16,BODZ,0,1),  BBOYBI_MASK, PPCCOM,	{ BDP } },
+{ "bdzl",    BBO(16,BODZ,0,1),  BBOYBI_MASK, COM,	{ BD } },
+{ "bdza-",   BBO(16,BODZ,1,0),  BBOYBI_MASK, PPCCOM,	{ BDMA } },
+{ "bdza+",   BBO(16,BODZ,1,0),  BBOYBI_MASK, PPCCOM,	{ BDPA } },
+{ "bdza",    BBO(16,BODZ,1,0),  BBOYBI_MASK, COM,	{ BDA } },
+{ "bdzla-",  BBO(16,BODZ,1,1),  BBOYBI_MASK, PPCCOM,	{ BDMA } },
+{ "bdzla+",  BBO(16,BODZ,1,1),  BBOYBI_MASK, PPCCOM,	{ BDPA } },
+{ "bdzla",   BBO(16,BODZ,1,1),  BBOYBI_MASK, COM,	{ BDA } },
+{ "blt-",    BBOCB(16,BOT,CBLT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "blt+",    BBOCB(16,BOT,CBLT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "blt",     BBOCB(16,BOT,CBLT,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bltl-",   BBOCB(16,BOT,CBLT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bltl+",   BBOCB(16,BOT,CBLT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bltl",    BBOCB(16,BOT,CBLT,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "blta-",   BBOCB(16,BOT,CBLT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "blta+",   BBOCB(16,BOT,CBLT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "blta",    BBOCB(16,BOT,CBLT,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bltla-",  BBOCB(16,BOT,CBLT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bltla+",  BBOCB(16,BOT,CBLT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bltla",   BBOCB(16,BOT,CBLT,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bgt-",    BBOCB(16,BOT,CBGT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bgt+",    BBOCB(16,BOT,CBGT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bgt",     BBOCB(16,BOT,CBGT,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bgtl-",   BBOCB(16,BOT,CBGT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bgtl+",   BBOCB(16,BOT,CBGT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bgtl",    BBOCB(16,BOT,CBGT,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bgta-",   BBOCB(16,BOT,CBGT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bgta+",   BBOCB(16,BOT,CBGT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bgta",    BBOCB(16,BOT,CBGT,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bgtla-",  BBOCB(16,BOT,CBGT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bgtla+",  BBOCB(16,BOT,CBGT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bgtla",   BBOCB(16,BOT,CBGT,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "beq-",    BBOCB(16,BOT,CBEQ,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "beq+",    BBOCB(16,BOT,CBEQ,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "beq",     BBOCB(16,BOT,CBEQ,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "beql-",   BBOCB(16,BOT,CBEQ,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "beql+",   BBOCB(16,BOT,CBEQ,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "beql",    BBOCB(16,BOT,CBEQ,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "beqa-",   BBOCB(16,BOT,CBEQ,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "beqa+",   BBOCB(16,BOT,CBEQ,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "beqa",    BBOCB(16,BOT,CBEQ,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "beqla-",  BBOCB(16,BOT,CBEQ,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "beqla+",  BBOCB(16,BOT,CBEQ,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "beqla",   BBOCB(16,BOT,CBEQ,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bso-",    BBOCB(16,BOT,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bso+",    BBOCB(16,BOT,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bso",     BBOCB(16,BOT,CBSO,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bsol-",   BBOCB(16,BOT,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bsol+",   BBOCB(16,BOT,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bsol",    BBOCB(16,BOT,CBSO,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bsoa-",   BBOCB(16,BOT,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bsoa+",   BBOCB(16,BOT,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bsoa",    BBOCB(16,BOT,CBSO,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bsola-",  BBOCB(16,BOT,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bsola+",  BBOCB(16,BOT,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bsola",   BBOCB(16,BOT,CBSO,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bun-",    BBOCB(16,BOT,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bun+",    BBOCB(16,BOT,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bun",     BBOCB(16,BOT,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BD } },
+{ "bunl-",   BBOCB(16,BOT,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bunl+",   BBOCB(16,BOT,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bunl",    BBOCB(16,BOT,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BD } },
+{ "buna-",   BBOCB(16,BOT,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "buna+",   BBOCB(16,BOT,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "buna",    BBOCB(16,BOT,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDA } },
+{ "bunla-",  BBOCB(16,BOT,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bunla+",  BBOCB(16,BOT,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bunla",   BBOCB(16,BOT,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDA } },
+{ "bge-",    BBOCB(16,BOF,CBLT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bge+",    BBOCB(16,BOF,CBLT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bge",     BBOCB(16,BOF,CBLT,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bgel-",   BBOCB(16,BOF,CBLT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bgel+",   BBOCB(16,BOF,CBLT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bgel",    BBOCB(16,BOF,CBLT,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bgea-",   BBOCB(16,BOF,CBLT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bgea+",   BBOCB(16,BOF,CBLT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bgea",    BBOCB(16,BOF,CBLT,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bgela-",  BBOCB(16,BOF,CBLT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bgela+",  BBOCB(16,BOF,CBLT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bgela",   BBOCB(16,BOF,CBLT,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bnl-",    BBOCB(16,BOF,CBLT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bnl+",    BBOCB(16,BOF,CBLT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bnl",     BBOCB(16,BOF,CBLT,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bnll-",   BBOCB(16,BOF,CBLT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bnll+",   BBOCB(16,BOF,CBLT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bnll",    BBOCB(16,BOF,CBLT,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bnla-",   BBOCB(16,BOF,CBLT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnla+",   BBOCB(16,BOF,CBLT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnla",    BBOCB(16,BOF,CBLT,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bnlla-",  BBOCB(16,BOF,CBLT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnlla+",  BBOCB(16,BOF,CBLT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnlla",   BBOCB(16,BOF,CBLT,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "ble-",    BBOCB(16,BOF,CBGT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "ble+",    BBOCB(16,BOF,CBGT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "ble",     BBOCB(16,BOF,CBGT,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "blel-",   BBOCB(16,BOF,CBGT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "blel+",   BBOCB(16,BOF,CBGT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "blel",    BBOCB(16,BOF,CBGT,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "blea-",   BBOCB(16,BOF,CBGT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "blea+",   BBOCB(16,BOF,CBGT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "blea",    BBOCB(16,BOF,CBGT,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "blela-",  BBOCB(16,BOF,CBGT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "blela+",  BBOCB(16,BOF,CBGT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "blela",   BBOCB(16,BOF,CBGT,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bng-",    BBOCB(16,BOF,CBGT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bng+",    BBOCB(16,BOF,CBGT,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bng",     BBOCB(16,BOF,CBGT,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bngl-",   BBOCB(16,BOF,CBGT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bngl+",   BBOCB(16,BOF,CBGT,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bngl",    BBOCB(16,BOF,CBGT,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bnga-",   BBOCB(16,BOF,CBGT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnga+",   BBOCB(16,BOF,CBGT,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnga",    BBOCB(16,BOF,CBGT,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bngla-",  BBOCB(16,BOF,CBGT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bngla+",  BBOCB(16,BOF,CBGT,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bngla",   BBOCB(16,BOF,CBGT,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bne-",    BBOCB(16,BOF,CBEQ,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bne+",    BBOCB(16,BOF,CBEQ,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bne",     BBOCB(16,BOF,CBEQ,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bnel-",   BBOCB(16,BOF,CBEQ,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bnel+",   BBOCB(16,BOF,CBEQ,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bnel",    BBOCB(16,BOF,CBEQ,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bnea-",   BBOCB(16,BOF,CBEQ,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnea+",   BBOCB(16,BOF,CBEQ,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnea",    BBOCB(16,BOF,CBEQ,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bnela-",  BBOCB(16,BOF,CBEQ,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnela+",  BBOCB(16,BOF,CBEQ,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnela",   BBOCB(16,BOF,CBEQ,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bns-",    BBOCB(16,BOF,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bns+",    BBOCB(16,BOF,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bns",     BBOCB(16,BOF,CBSO,0,0), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bnsl-",   BBOCB(16,BOF,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bnsl+",   BBOCB(16,BOF,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bnsl",    BBOCB(16,BOF,CBSO,0,1), BBOYCB_MASK, COM,		{ CR, BD } },
+{ "bnsa-",   BBOCB(16,BOF,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnsa+",   BBOCB(16,BOF,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnsa",    BBOCB(16,BOF,CBSO,1,0), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bnsla-",  BBOCB(16,BOF,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnsla+",  BBOCB(16,BOF,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnsla",   BBOCB(16,BOF,CBSO,1,1), BBOYCB_MASK, COM,		{ CR, BDA } },
+{ "bnu-",    BBOCB(16,BOF,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bnu+",    BBOCB(16,BOF,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bnu",     BBOCB(16,BOF,CBSO,0,0), BBOYCB_MASK, PPCCOM,	{ CR, BD } },
+{ "bnul-",   BBOCB(16,BOF,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDM } },
+{ "bnul+",   BBOCB(16,BOF,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BDP } },
+{ "bnul",    BBOCB(16,BOF,CBSO,0,1), BBOYCB_MASK, PPCCOM,	{ CR, BD } },
+{ "bnua-",   BBOCB(16,BOF,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnua+",   BBOCB(16,BOF,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnua",    BBOCB(16,BOF,CBSO,1,0), BBOYCB_MASK, PPCCOM,	{ CR, BDA } },
+{ "bnula-",  BBOCB(16,BOF,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDMA } },
+{ "bnula+",  BBOCB(16,BOF,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDPA } },
+{ "bnula",   BBOCB(16,BOF,CBSO,1,1), BBOYCB_MASK, PPCCOM,	{ CR, BDA } },
+{ "bdnzt-",  BBO(16,BODNZT,0,0), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdnzt+",  BBO(16,BODNZT,0,0), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdnzt",   BBO(16,BODNZT,0,0), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdnztl-", BBO(16,BODNZT,0,1), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdnztl+", BBO(16,BODNZT,0,1), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdnztl",  BBO(16,BODNZT,0,1), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdnzta-", BBO(16,BODNZT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdnzta+", BBO(16,BODNZT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdnzta",  BBO(16,BODNZT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bdnztla-",BBO(16,BODNZT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdnztla+",BBO(16,BODNZT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdnztla", BBO(16,BODNZT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bdnzf-",  BBO(16,BODNZF,0,0), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdnzf+",  BBO(16,BODNZF,0,0), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdnzf",   BBO(16,BODNZF,0,0), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdnzfl-", BBO(16,BODNZF,0,1), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdnzfl+", BBO(16,BODNZF,0,1), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdnzfl",  BBO(16,BODNZF,0,1), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdnzfa-", BBO(16,BODNZF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdnzfa+", BBO(16,BODNZF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdnzfa",  BBO(16,BODNZF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bdnzfla-",BBO(16,BODNZF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdnzfla+",BBO(16,BODNZF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdnzfla", BBO(16,BODNZF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bt-",     BBO(16,BOT,0,0), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bt+",     BBO(16,BOT,0,0), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bt",	     BBO(16,BOT,0,0), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bbt",     BBO(16,BOT,0,0), BBOY_MASK, PWRCOM,	{ BI, BD } },
+{ "btl-",    BBO(16,BOT,0,1), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "btl+",    BBO(16,BOT,0,1), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "btl",     BBO(16,BOT,0,1), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bbtl",    BBO(16,BOT,0,1), BBOY_MASK, PWRCOM,	{ BI, BD } },
+{ "bta-",    BBO(16,BOT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bta+",    BBO(16,BOT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bta",     BBO(16,BOT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bbta",    BBO(16,BOT,1,0), BBOY_MASK, PWRCOM,	{ BI, BDA } },
+{ "btla-",   BBO(16,BOT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "btla+",   BBO(16,BOT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "btla",    BBO(16,BOT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bbtla",   BBO(16,BOT,1,1), BBOY_MASK, PWRCOM,	{ BI, BDA } },
+{ "bf-",     BBO(16,BOF,0,0), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bf+",     BBO(16,BOF,0,0), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bf",	     BBO(16,BOF,0,0), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bbf",     BBO(16,BOF,0,0), BBOY_MASK, PWRCOM,	{ BI, BD } },
+{ "bfl-",    BBO(16,BOF,0,1), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bfl+",    BBO(16,BOF,0,1), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bfl",     BBO(16,BOF,0,1), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bbfl",    BBO(16,BOF,0,1), BBOY_MASK, PWRCOM,	{ BI, BD } },
+{ "bfa-",    BBO(16,BOF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bfa+",    BBO(16,BOF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bfa",     BBO(16,BOF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bbfa",    BBO(16,BOF,1,0), BBOY_MASK, PWRCOM,	{ BI, BDA } },
+{ "bfla-",   BBO(16,BOF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bfla+",   BBO(16,BOF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bfla",    BBO(16,BOF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bbfla",   BBO(16,BOF,1,1), BBOY_MASK, PWRCOM,	{ BI, BDA } },
+{ "bdzt-",   BBO(16,BODZT,0,0), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdzt+",   BBO(16,BODZT,0,0), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdzt",    BBO(16,BODZT,0,0), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdztl-",  BBO(16,BODZT,0,1), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdztl+",  BBO(16,BODZT,0,1), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdztl",   BBO(16,BODZT,0,1), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdzta-",  BBO(16,BODZT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdzta+",  BBO(16,BODZT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdzta",   BBO(16,BODZT,1,0), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bdztla-", BBO(16,BODZT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdztla+", BBO(16,BODZT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdztla",  BBO(16,BODZT,1,1), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bdzf-",   BBO(16,BODZF,0,0), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdzf+",   BBO(16,BODZF,0,0), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdzf",    BBO(16,BODZF,0,0), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdzfl-",  BBO(16,BODZF,0,1), BBOY_MASK, PPCCOM,	{ BI, BDM } },
+{ "bdzfl+",  BBO(16,BODZF,0,1), BBOY_MASK, PPCCOM,	{ BI, BDP } },
+{ "bdzfl",   BBO(16,BODZF,0,1), BBOY_MASK, PPCCOM,	{ BI, BD } },
+{ "bdzfa-",  BBO(16,BODZF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdzfa+",  BBO(16,BODZF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdzfa",   BBO(16,BODZF,1,0), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bdzfla-", BBO(16,BODZF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDMA } },
+{ "bdzfla+", BBO(16,BODZF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDPA } },
+{ "bdzfla",  BBO(16,BODZF,1,1), BBOY_MASK, PPCCOM,	{ BI, BDA } },
+{ "bc-",     B(16,0,0),	B_MASK,		PPCCOM,		{ BOE, BI, BDM } },
+{ "bc+",     B(16,0,0),	B_MASK,		PPCCOM,		{ BOE, BI, BDP } },
+{ "bc",	     B(16,0,0),	B_MASK,		COM,		{ BO, BI, BD } },
+{ "bcl-",    B(16,0,1),	B_MASK,		PPCCOM,		{ BOE, BI, BDM } },
+{ "bcl+",    B(16,0,1),	B_MASK,		PPCCOM,		{ BOE, BI, BDP } },
+{ "bcl",     B(16,0,1),	B_MASK,		COM,		{ BO, BI, BD } },
+{ "bca-",    B(16,1,0),	B_MASK,		PPCCOM,		{ BOE, BI, BDMA } },
+{ "bca+",    B(16,1,0),	B_MASK,		PPCCOM,		{ BOE, BI, BDPA } },
+{ "bca",     B(16,1,0),	B_MASK,		COM,		{ BO, BI, BDA } },
+{ "bcla-",   B(16,1,1),	B_MASK,		PPCCOM,		{ BOE, BI, BDMA } },
+{ "bcla+",   B(16,1,1),	B_MASK,		PPCCOM,		{ BOE, BI, BDPA } },
+{ "bcla",    B(16,1,1),	B_MASK,		COM,		{ BO, BI, BDA } },
+
+{ "sc",      SC(17,1,0), 0xffffffff,	PPC,		{ 0 } },
+{ "svc",     SC(17,0,0), SC_MASK,	POWER,		{ LEV, FL1, FL2 } },
+{ "svcl",    SC(17,0,1), SC_MASK,	POWER,		{ LEV, FL1, FL2 } },
+{ "svca",    SC(17,1,0), SC_MASK,	PWRCOM,		{ SV } },
+{ "svcla",   SC(17,1,1), SC_MASK,	POWER,		{ SV } },
+
+{ "b",	     B(18,0,0),	B_MASK,		COM,	{ LI } },
+{ "bl",      B(18,0,1),	B_MASK,		COM,	{ LI } },
+{ "ba",      B(18,1,0),	B_MASK,		COM,	{ LIA } },
+{ "bla",     B(18,1,1),	B_MASK,		COM,	{ LIA } },
+
+{ "mcrf",    XL(19,0),	XLBB_MASK|(3<<21)|(3<<16), COM,	{ BF, BFA } },
+
+{ "blr",     XLO(19,BOU,16,0), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "br",      XLO(19,BOU,16,0), XLBOBIBB_MASK, PWRCOM,	{ 0 } },
+{ "blrl",    XLO(19,BOU,16,1), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "brl",     XLO(19,BOU,16,1), XLBOBIBB_MASK, PWRCOM,	{ 0 } },
+{ "bdnzlr",  XLO(19,BODNZ,16,0), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdnzlr-", XLO(19,BODNZ,16,0), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdnzlr+", XLO(19,BODNZP,16,0), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdnzlrl", XLO(19,BODNZ,16,1), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdnzlrl-",XLO(19,BODNZ,16,1), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdnzlrl+",XLO(19,BODNZP,16,1), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdzlr",   XLO(19,BODZ,16,0), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdzlr-",  XLO(19,BODZ,16,0), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdzlr+",  XLO(19,BODZP,16,0), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdzlrl",  XLO(19,BODZ,16,1), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdzlrl-", XLO(19,BODZ,16,1), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bdzlrl+", XLO(19,BODZP,16,1), XLBOBIBB_MASK, PPCCOM,	{ 0 } },
+{ "bltlr",   XLOCB(19,BOT,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bltlr-",  XLOCB(19,BOT,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bltlr+",  XLOCB(19,BOTP,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bltr",    XLOCB(19,BOT,CBLT,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bltlrl",  XLOCB(19,BOT,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bltlrl-", XLOCB(19,BOT,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bltlrl+", XLOCB(19,BOTP,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bltrl",   XLOCB(19,BOT,CBLT,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bgtlr",   XLOCB(19,BOT,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgtlr-",  XLOCB(19,BOT,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgtlr+",  XLOCB(19,BOTP,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgtr",    XLOCB(19,BOT,CBGT,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bgtlrl",  XLOCB(19,BOT,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgtlrl-", XLOCB(19,BOT,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgtlrl+", XLOCB(19,BOTP,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgtrl",   XLOCB(19,BOT,CBGT,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "beqlr",   XLOCB(19,BOT,CBEQ,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "beqlr-",  XLOCB(19,BOT,CBEQ,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "beqlr+",  XLOCB(19,BOTP,CBEQ,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "beqr",    XLOCB(19,BOT,CBEQ,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "beqlrl",  XLOCB(19,BOT,CBEQ,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "beqlrl-", XLOCB(19,BOT,CBEQ,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "beqlrl+", XLOCB(19,BOTP,CBEQ,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "beqrl",   XLOCB(19,BOT,CBEQ,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bsolr",   XLOCB(19,BOT,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bsolr-",  XLOCB(19,BOT,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bsolr+",  XLOCB(19,BOTP,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bsor",    XLOCB(19,BOT,CBSO,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bsolrl",  XLOCB(19,BOT,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bsolrl-", XLOCB(19,BOT,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bsolrl+", XLOCB(19,BOTP,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bsorl",   XLOCB(19,BOT,CBSO,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bunlr",   XLOCB(19,BOT,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bunlr-",  XLOCB(19,BOT,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bunlr+",  XLOCB(19,BOTP,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bunlrl",  XLOCB(19,BOT,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bunlrl-", XLOCB(19,BOT,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bunlrl+", XLOCB(19,BOTP,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgelr",   XLOCB(19,BOF,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgelr-",  XLOCB(19,BOF,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgelr+",  XLOCB(19,BOFP,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bger",    XLOCB(19,BOF,CBLT,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bgelrl",  XLOCB(19,BOF,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgelrl-", XLOCB(19,BOF,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgelrl+", XLOCB(19,BOFP,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bgerl",   XLOCB(19,BOF,CBLT,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnllr",   XLOCB(19,BOF,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnllr-",  XLOCB(19,BOF,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnllr+",  XLOCB(19,BOFP,CBLT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnlr",    XLOCB(19,BOF,CBLT,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnllrl",  XLOCB(19,BOF,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnllrl-", XLOCB(19,BOF,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnllrl+", XLOCB(19,BOFP,CBLT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnlrl",   XLOCB(19,BOF,CBLT,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "blelr",   XLOCB(19,BOF,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "blelr-",  XLOCB(19,BOF,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "blelr+",  XLOCB(19,BOFP,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bler",    XLOCB(19,BOF,CBGT,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "blelrl",  XLOCB(19,BOF,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "blelrl-", XLOCB(19,BOF,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "blelrl+", XLOCB(19,BOFP,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "blerl",   XLOCB(19,BOF,CBGT,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnglr",   XLOCB(19,BOF,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnglr-",  XLOCB(19,BOF,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnglr+",  XLOCB(19,BOFP,CBGT,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bngr",    XLOCB(19,BOF,CBGT,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnglrl",  XLOCB(19,BOF,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnglrl-", XLOCB(19,BOF,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnglrl+", XLOCB(19,BOFP,CBGT,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bngrl",   XLOCB(19,BOF,CBGT,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnelr",   XLOCB(19,BOF,CBEQ,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnelr-",  XLOCB(19,BOF,CBEQ,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnelr+",  XLOCB(19,BOFP,CBEQ,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bner",    XLOCB(19,BOF,CBEQ,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnelrl",  XLOCB(19,BOF,CBEQ,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnelrl-", XLOCB(19,BOF,CBEQ,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnelrl+", XLOCB(19,BOFP,CBEQ,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnerl",   XLOCB(19,BOF,CBEQ,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnslr",   XLOCB(19,BOF,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnslr-",  XLOCB(19,BOF,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnslr+",  XLOCB(19,BOFP,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnsr",    XLOCB(19,BOF,CBSO,16,0), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnslrl",  XLOCB(19,BOF,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnslrl-", XLOCB(19,BOF,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnslrl+", XLOCB(19,BOFP,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnsrl",   XLOCB(19,BOF,CBSO,16,1), XLBOCBBB_MASK, PWRCOM, { CR } },
+{ "bnulr",   XLOCB(19,BOF,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnulr-",  XLOCB(19,BOF,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnulr+",  XLOCB(19,BOFP,CBSO,16,0), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnulrl",  XLOCB(19,BOF,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnulrl-", XLOCB(19,BOF,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "bnulrl+", XLOCB(19,BOFP,CBSO,16,1), XLBOCBBB_MASK, PPCCOM, { CR } },
+{ "btlr",    XLO(19,BOT,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btlr-",   XLO(19,BOT,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btlr+",   XLO(19,BOTP,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bbtr",    XLO(19,BOT,16,0), XLBOBB_MASK, PWRCOM,	{ BI } },
+{ "btlrl",   XLO(19,BOT,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btlrl-",  XLO(19,BOT,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btlrl+",  XLO(19,BOTP,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bbtrl",   XLO(19,BOT,16,1), XLBOBB_MASK, PWRCOM,	{ BI } },
+{ "bflr",    XLO(19,BOF,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bflr-",   XLO(19,BOF,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bflr+",   XLO(19,BOFP,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bbfr",    XLO(19,BOF,16,0), XLBOBB_MASK, PWRCOM,	{ BI } },
+{ "bflrl",   XLO(19,BOF,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bflrl-",  XLO(19,BOF,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bflrl+",  XLO(19,BOFP,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bbfrl",   XLO(19,BOF,16,1), XLBOBB_MASK, PWRCOM,	{ BI } },
+{ "bdnztlr", XLO(19,BODNZT,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnztlr-",XLO(19,BODNZT,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnztlr+",XLO(19,BODNZTP,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnztlrl",XLO(19,BODNZT,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnztlrl-",XLO(19,BODNZT,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnztlrl+",XLO(19,BODNZTP,16,1), XLBOBB_MASK, PPCCOM,{ BI } },
+{ "bdnzflr", XLO(19,BODNZF,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnzflr-",XLO(19,BODNZF,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnzflr+",XLO(19,BODNZFP,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnzflrl",XLO(19,BODNZF,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnzflrl-",XLO(19,BODNZF,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdnzflrl+",XLO(19,BODNZFP,16,1), XLBOBB_MASK, PPCCOM,{ BI } },
+{ "bdztlr",  XLO(19,BODZT,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdztlr-", XLO(19,BODZT,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdztlr+", XLO(19,BODZTP,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdztlrl", XLO(19,BODZT,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdztlrl-",XLO(19,BODZT,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdztlrl+",XLO(19,BODZTP,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdzflr",  XLO(19,BODZF,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdzflr-", XLO(19,BODZF,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdzflr+", XLO(19,BODZFP,16,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdzflrl", XLO(19,BODZF,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdzflrl-",XLO(19,BODZF,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bdzflrl+",XLO(19,BODZFP,16,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bclr",    XLLK(19,16,0), XLYBB_MASK,	PPCCOM,		{ BO, BI } },
+{ "bclrl",   XLLK(19,16,1), XLYBB_MASK,	PPCCOM,		{ BO, BI } },
+{ "bclr+",   XLYLK(19,16,1,0), XLYBB_MASK, PPCCOM,	{ BOE, BI } },
+{ "bclrl+",  XLYLK(19,16,1,1), XLYBB_MASK, PPCCOM,	{ BOE, BI } },
+{ "bclr-",   XLYLK(19,16,0,0), XLYBB_MASK, PPCCOM,	{ BOE, BI } },
+{ "bclrl-",  XLYLK(19,16,0,1), XLYBB_MASK, PPCCOM,	{ BOE, BI } },
+{ "bcr",     XLLK(19,16,0), XLBB_MASK,	PWRCOM,		{ BO, BI } },
+{ "bcrl",    XLLK(19,16,1), XLBB_MASK,	PWRCOM,		{ BO, BI } },
+
+{ "rfid",    XL(19,18),	0xffffffff,	PPC64,		{ 0 } },
+
+{ "crnot",   XL(19,33), XL_MASK,	PPCCOM,		{ BT, BA, BBA } },
+{ "crnor",   XL(19,33),	XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "rfi",     XL(19,50),	0xffffffff,	COM,		{ 0 } },
+{ "rfci",    XL(19,51),	0xffffffff,	PPC403,		{ 0 } },
+
+{ "rfsvc",   XL(19,82),	0xffffffff,	POWER,		{ 0 } },
+
+{ "crandc",  XL(19,129), XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "isync",   XL(19,150), 0xffffffff,	PPCCOM,		{ 0 } },
+{ "ics",     XL(19,150), 0xffffffff,	PWRCOM,		{ 0 } },
+
+{ "crclr",   XL(19,193), XL_MASK,	PPCCOM,		{ BT, BAT, BBA } },
+{ "crxor",   XL(19,193), XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "crnand",  XL(19,225), XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "crand",   XL(19,257), XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "crset",   XL(19,289), XL_MASK,	PPCCOM,		{ BT, BAT, BBA } },
+{ "creqv",   XL(19,289), XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "crorc",   XL(19,417), XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "crmove",  XL(19,449), XL_MASK,	PPCCOM,		{ BT, BA, BBA } },
+{ "cror",    XL(19,449), XL_MASK,	COM,		{ BT, BA, BB } },
+
+{ "bctr",    XLO(19,BOU,528,0), XLBOBIBB_MASK, COM,	{ 0 } },
+{ "bctrl",   XLO(19,BOU,528,1), XLBOBIBB_MASK, COM,	{ 0 } },
+{ "bltctr",  XLOCB(19,BOT,CBLT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bltctr-", XLOCB(19,BOT,CBLT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bltctr+", XLOCB(19,BOTP,CBLT,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bltctrl", XLOCB(19,BOT,CBLT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bltctrl-",XLOCB(19,BOT,CBLT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bltctrl+",XLOCB(19,BOTP,CBLT,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgtctr",  XLOCB(19,BOT,CBGT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgtctr-", XLOCB(19,BOT,CBGT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgtctr+", XLOCB(19,BOTP,CBGT,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgtctrl", XLOCB(19,BOT,CBGT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgtctrl-",XLOCB(19,BOT,CBGT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgtctrl+",XLOCB(19,BOTP,CBGT,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "beqctr",  XLOCB(19,BOT,CBEQ,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "beqctr-", XLOCB(19,BOT,CBEQ,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "beqctr+", XLOCB(19,BOTP,CBEQ,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "beqctrl", XLOCB(19,BOT,CBEQ,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "beqctrl-",XLOCB(19,BOT,CBEQ,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "beqctrl+",XLOCB(19,BOTP,CBEQ,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bsoctr",  XLOCB(19,BOT,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bsoctr-", XLOCB(19,BOT,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bsoctr+", XLOCB(19,BOTP,CBSO,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bsoctrl", XLOCB(19,BOT,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bsoctrl-",XLOCB(19,BOT,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bsoctrl+",XLOCB(19,BOTP,CBSO,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bunctr",  XLOCB(19,BOT,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bunctr-", XLOCB(19,BOT,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bunctr+", XLOCB(19,BOTP,CBSO,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bunctrl", XLOCB(19,BOT,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bunctrl-",XLOCB(19,BOT,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bunctrl+",XLOCB(19,BOTP,CBSO,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgectr",  XLOCB(19,BOF,CBLT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgectr-", XLOCB(19,BOF,CBLT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgectr+", XLOCB(19,BOFP,CBLT,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgectrl", XLOCB(19,BOF,CBLT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgectrl-",XLOCB(19,BOF,CBLT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bgectrl+",XLOCB(19,BOFP,CBLT,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnlctr",  XLOCB(19,BOF,CBLT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnlctr-", XLOCB(19,BOF,CBLT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnlctr+", XLOCB(19,BOFP,CBLT,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnlctrl", XLOCB(19,BOF,CBLT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnlctrl-",XLOCB(19,BOF,CBLT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnlctrl+",XLOCB(19,BOFP,CBLT,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "blectr",  XLOCB(19,BOF,CBGT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "blectr-", XLOCB(19,BOF,CBGT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "blectr+", XLOCB(19,BOFP,CBGT,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "blectrl", XLOCB(19,BOF,CBGT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "blectrl-",XLOCB(19,BOF,CBGT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "blectrl+",XLOCB(19,BOFP,CBGT,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bngctr",  XLOCB(19,BOF,CBGT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bngctr-", XLOCB(19,BOF,CBGT,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bngctr+", XLOCB(19,BOFP,CBGT,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bngctrl", XLOCB(19,BOF,CBGT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bngctrl-",XLOCB(19,BOF,CBGT,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bngctrl+",XLOCB(19,BOFP,CBGT,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnectr",  XLOCB(19,BOF,CBEQ,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnectr-", XLOCB(19,BOF,CBEQ,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnectr+", XLOCB(19,BOFP,CBEQ,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnectrl", XLOCB(19,BOF,CBEQ,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnectrl-",XLOCB(19,BOF,CBEQ,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnectrl+",XLOCB(19,BOFP,CBEQ,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnsctr",  XLOCB(19,BOF,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnsctr-", XLOCB(19,BOF,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnsctr+", XLOCB(19,BOFP,CBSO,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnsctrl", XLOCB(19,BOF,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnsctrl-",XLOCB(19,BOF,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnsctrl+",XLOCB(19,BOFP,CBSO,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnuctr",  XLOCB(19,BOF,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnuctr-", XLOCB(19,BOF,CBSO,528,0),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnuctr+", XLOCB(19,BOFP,CBSO,528,0), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnuctrl", XLOCB(19,BOF,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnuctrl-",XLOCB(19,BOF,CBSO,528,1),  XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "bnuctrl+",XLOCB(19,BOFP,CBSO,528,1), XLBOCBBB_MASK, PPCCOM,	{ CR } },
+{ "btctr",   XLO(19,BOT,528,0),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btctr-",  XLO(19,BOT,528,0),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btctr+",  XLO(19,BOTP,528,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btctrl",  XLO(19,BOT,528,1),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btctrl-", XLO(19,BOT,528,1),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "btctrl+", XLO(19,BOTP,528,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bfctr",   XLO(19,BOF,528,0),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bfctr-",  XLO(19,BOF,528,0),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bfctr+",  XLO(19,BOFP,528,0), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bfctrl",  XLO(19,BOF,528,1),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bfctrl-", XLO(19,BOF,528,1),  XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bfctrl+", XLO(19,BOFP,528,1), XLBOBB_MASK, PPCCOM,	{ BI } },
+{ "bcctr",   XLLK(19,528,0),     XLYBB_MASK,  PPCCOM,	{ BO, BI } },
+{ "bcctr-",  XLYLK(19,528,0,0),  XLYBB_MASK,  PPCCOM,	{ BOE, BI } },
+{ "bcctr+",  XLYLK(19,528,1,0),  XLYBB_MASK,  PPCCOM,	{ BOE, BI } },
+{ "bcctrl",  XLLK(19,528,1),     XLYBB_MASK,  PPCCOM,	{ BO, BI } },
+{ "bcctrl-", XLYLK(19,528,0,1),  XLYBB_MASK,  PPCCOM,	{ BOE, BI } },
+{ "bcctrl+", XLYLK(19,528,1,1),  XLYBB_MASK,  PPCCOM,	{ BOE, BI } },
+{ "bcc",     XLLK(19,528,0),     XLBB_MASK,   PWRCOM,	{ BO, BI } },
+{ "bccl",    XLLK(19,528,1),     XLBB_MASK,   PWRCOM,	{ BO, BI } },
+
+{ "rlwimi",  M(20,0),	M_MASK,		PPCCOM,		{ RA,RS,SH,MBE,ME } },
+{ "rlimi",   M(20,0),	M_MASK,		PWRCOM,		{ RA,RS,SH,MBE,ME } },
+
+{ "rlwimi.", M(20,1),	M_MASK,		PPCCOM,		{ RA,RS,SH,MBE,ME } },
+{ "rlimi.",  M(20,1),	M_MASK,		PWRCOM,		{ RA,RS,SH,MBE,ME } },
+
+{ "rotlwi",  MME(21,31,0), MMBME_MASK,	PPCCOM,		{ RA, RS, SH } },
+{ "clrlwi",  MME(21,31,0), MSHME_MASK,	PPCCOM,		{ RA, RS, MB } },
+{ "rlwinm",  M(21,0),	M_MASK,		PPCCOM,		{ RA,RS,SH,MBE,ME } },
+{ "rlinm",   M(21,0),	M_MASK,		PWRCOM,		{ RA,RS,SH,MBE,ME } },
+{ "rotlwi.", MME(21,31,1), MMBME_MASK,	PPCCOM,		{ RA,RS,SH } },
+{ "clrlwi.", MME(21,31,1), MSHME_MASK,	PPCCOM,		{ RA, RS, MB } },
+{ "rlwinm.", M(21,1),	M_MASK,		PPCCOM,		{ RA,RS,SH,MBE,ME } },
+{ "rlinm.",  M(21,1),	M_MASK,		PWRCOM,		{ RA,RS,SH,MBE,ME } },
+
+{ "rlmi",    M(22,0),	M_MASK,		M601,		{ RA,RS,RB,MBE,ME } },
+{ "rlmi.",   M(22,1),	M_MASK,		M601,		{ RA,RS,RB,MBE,ME } },
+
+{ "rotlw",   MME(23,31,0), MMBME_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "rlwnm",   M(23,0),	M_MASK,		PPCCOM,		{ RA,RS,RB,MBE,ME } },
+{ "rlnm",    M(23,0),	M_MASK,		PWRCOM,		{ RA,RS,RB,MBE,ME } },
+{ "rotlw.",  MME(23,31,1), MMBME_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "rlwnm.",  M(23,1),	M_MASK,		PPCCOM,		{ RA,RS,RB,MBE,ME } },
+{ "rlnm.",   M(23,1),	M_MASK,		PWRCOM,		{ RA,RS,RB,MBE,ME } },
+
+{ "nop",     OP(24),	0xffffffff,	PPCCOM,		{ 0 } },
+{ "ori",     OP(24),	OP_MASK,	PPCCOM,		{ RA, RS, UI } },
+{ "oril",    OP(24),	OP_MASK,	PWRCOM,		{ RA, RS, UI } },
+
+{ "oris",    OP(25),	OP_MASK,	PPCCOM,		{ RA, RS, UI } },
+{ "oriu",    OP(25),	OP_MASK,	PWRCOM,		{ RA, RS, UI } },
+
+{ "xori",    OP(26),	OP_MASK,	PPCCOM,		{ RA, RS, UI } },
+{ "xoril",   OP(26),	OP_MASK,	PWRCOM,		{ RA, RS, UI } },
+
+{ "xoris",   OP(27),	OP_MASK,	PPCCOM,		{ RA, RS, UI } },
+{ "xoriu",   OP(27),	OP_MASK,	PWRCOM,		{ RA, RS, UI } },
+
+{ "andi.",   OP(28),	OP_MASK,	PPCCOM,		{ RA, RS, UI } },
+{ "andil.",  OP(28),	OP_MASK,	PWRCOM,		{ RA, RS, UI } },
+
+{ "andis.",  OP(29),	OP_MASK,	PPCCOM,		{ RA, RS, UI } },
+{ "andiu.",  OP(29),	OP_MASK,	PWRCOM,		{ RA, RS, UI } },
+
+{ "rotldi",  MD(30,0,0), MDMB_MASK,	PPC64,		{ RA, RS, SH6 } },
+{ "clrldi",  MD(30,0,0), MDSH_MASK,	PPC64,		{ RA, RS, MB6 } },
+{ "rldicl",  MD(30,0,0), MD_MASK,	PPC64,		{ RA, RS, SH6, MB6 } },
+{ "rotldi.", MD(30,0,1), MDMB_MASK,	PPC64,		{ RA, RS, SH6 } },
+{ "clrldi.", MD(30,0,1), MDSH_MASK,	PPC64,		{ RA, RS, MB6 } },
+{ "rldicl.", MD(30,0,1), MD_MASK,	PPC64,		{ RA, RS, SH6, MB6 } },
+
+{ "rldicr",  MD(30,1,0), MD_MASK,	PPC64,		{ RA, RS, SH6, ME6 } },
+{ "rldicr.", MD(30,1,1), MD_MASK,	PPC64,		{ RA, RS, SH6, ME6 } },
+
+{ "rldic",   MD(30,2,0), MD_MASK,	PPC64,		{ RA, RS, SH6, MB6 } },
+{ "rldic.",  MD(30,2,1), MD_MASK,	PPC64,		{ RA, RS, SH6, MB6 } },
+
+{ "rldimi",  MD(30,3,0), MD_MASK,	PPC64,		{ RA, RS, SH6, MB6 } },
+{ "rldimi.", MD(30,3,1), MD_MASK,	PPC64,		{ RA, RS, SH6, MB6 } },
+
+{ "rotld",   MDS(30,8,0), MDSMB_MASK,	PPC64,		{ RA, RS, RB } },
+{ "rldcl",   MDS(30,8,0), MDS_MASK,	PPC64,		{ RA, RS, RB, MB6 } },
+{ "rotld.",  MDS(30,8,1), MDSMB_MASK,	PPC64,		{ RA, RS, RB } },
+{ "rldcl.",  MDS(30,8,1), MDS_MASK,	PPC64,		{ RA, RS, RB, MB6 } },
+
+{ "rldcr",   MDS(30,9,0), MDS_MASK,	PPC64,		{ RA, RS, RB, ME6 } },
+{ "rldcr.",  MDS(30,9,1), MDS_MASK,	PPC64,		{ RA, RS, RB, ME6 } },
+
+{ "cmpw",    XCMPL(31,0,0), XCMPL_MASK, PPCCOM,		{ OBF, RA, RB } },
+{ "cmpd",    XCMPL(31,0,1), XCMPL_MASK, PPC64,		{ OBF, RA, RB } },
+{ "cmp",     X(31,0),	XCMP_MASK,	PPCONLY,	{ BF, L, RA, RB } },
+{ "cmp",     X(31,0),	XCMPL_MASK,	PWRCOM,		{ BF, RA, RB } },
+
+{ "twlgt",   XTO(31,4,TOLGT), XTO_MASK, PPCCOM,		{ RA, RB } },
+{ "tlgt",    XTO(31,4,TOLGT), XTO_MASK, PWRCOM,		{ RA, RB } },
+{ "twllt",   XTO(31,4,TOLLT), XTO_MASK, PPCCOM,		{ RA, RB } },
+{ "tllt",    XTO(31,4,TOLLT), XTO_MASK, PWRCOM,		{ RA, RB } },
+{ "tweq",    XTO(31,4,TOEQ), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "teq",     XTO(31,4,TOEQ), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "twlge",   XTO(31,4,TOLGE), XTO_MASK, PPCCOM,		{ RA, RB } },
+{ "tlge",    XTO(31,4,TOLGE), XTO_MASK, PWRCOM,		{ RA, RB } },
+{ "twlnl",   XTO(31,4,TOLNL), XTO_MASK, PPCCOM,		{ RA, RB } },
+{ "tlnl",    XTO(31,4,TOLNL), XTO_MASK, PWRCOM,		{ RA, RB } },
+{ "twlle",   XTO(31,4,TOLLE), XTO_MASK, PPCCOM,		{ RA, RB } },
+{ "tlle",    XTO(31,4,TOLLE), XTO_MASK, PWRCOM,		{ RA, RB } },
+{ "twlng",   XTO(31,4,TOLNG), XTO_MASK, PPCCOM,		{ RA, RB } },
+{ "tlng",    XTO(31,4,TOLNG), XTO_MASK, PWRCOM,		{ RA, RB } },
+{ "twgt",    XTO(31,4,TOGT), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "tgt",     XTO(31,4,TOGT), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "twge",    XTO(31,4,TOGE), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "tge",     XTO(31,4,TOGE), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "twnl",    XTO(31,4,TONL), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "tnl",     XTO(31,4,TONL), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "twlt",    XTO(31,4,TOLT), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "tlt",     XTO(31,4,TOLT), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "twle",    XTO(31,4,TOLE), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "tle",     XTO(31,4,TOLE), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "twng",    XTO(31,4,TONG), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "tng",     XTO(31,4,TONG), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "twne",    XTO(31,4,TONE), XTO_MASK,	PPCCOM,		{ RA, RB } },
+{ "tne",     XTO(31,4,TONE), XTO_MASK,	PWRCOM,		{ RA, RB } },
+{ "trap",    XTO(31,4,TOU), 0xffffffff,	PPCCOM,		{ 0 } },
+{ "tw",      X(31,4),	X_MASK,		PPCCOM,		{ TO, RA, RB } },
+{ "t",       X(31,4),	X_MASK,		PWRCOM,		{ TO, RA, RB } },
+
+{ "subfc",   XO(31,8,0,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sf",      XO(31,8,0,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "subc",    XO(31,8,0,0), XO_MASK,	PPC,		{ RT, RB, RA } },
+{ "subfc.",  XO(31,8,0,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sf.",     XO(31,8,0,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "subc.",   XO(31,8,0,1), XO_MASK,	PPCCOM,		{ RT, RB, RA } },
+{ "subfco",  XO(31,8,1,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sfo",     XO(31,8,1,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "subco",   XO(31,8,1,0), XO_MASK,	PPC,		{ RT, RB, RA } },
+{ "subfco.", XO(31,8,1,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sfo.",    XO(31,8,1,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "subco.",  XO(31,8,1,1), XO_MASK,	PPC,		{ RT, RB, RA } },
+
+{ "mulhdu",  XO(31,9,0,0), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "mulhdu.", XO(31,9,0,1), XO_MASK,	PPC64,		{ RT, RA, RB } },
+
+{ "addc",    XO(31,10,0,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "a",       XO(31,10,0,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "addc.",   XO(31,10,0,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "a.",      XO(31,10,0,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "addco",   XO(31,10,1,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "ao",      XO(31,10,1,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "addco.",  XO(31,10,1,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "ao.",     XO(31,10,1,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+
+{ "mulhwu",  XO(31,11,0,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "mulhwu.", XO(31,11,0,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+
+{ "mfcr",    X(31,19),	XRARB_MASK,	COM,		{ RT } },
+
+{ "lwarx",   X(31,20),	X_MASK,		PPC,		{ RT, RA, RB } },
+
+{ "ldx",     X(31,21),	X_MASK,		PPC64,		{ RT, RA, RB } },
+
+{ "lwzx",    X(31,23),	X_MASK,		PPCCOM,		{ RT, RA, RB } },
+{ "lx",      X(31,23),	X_MASK,		PWRCOM,		{ RT, RA, RB } },
+
+{ "slw",     XRC(31,24,0), X_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "sl",      XRC(31,24,0), X_MASK,	PWRCOM,		{ RA, RS, RB } },
+{ "slw.",    XRC(31,24,1), X_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "sl.",     XRC(31,24,1), X_MASK,	PWRCOM,		{ RA, RS, RB } },
+
+{ "cntlzw",  XRC(31,26,0), XRB_MASK,	PPCCOM,		{ RA, RS } },
+{ "cntlz",   XRC(31,26,0), XRB_MASK,	PWRCOM,		{ RA, RS } },
+{ "cntlzw.", XRC(31,26,1), XRB_MASK,	PPCCOM,		{ RA, RS } },
+{ "cntlz.",  XRC(31,26,1), XRB_MASK, 	PWRCOM,		{ RA, RS } },
+
+{ "sld",     XRC(31,27,0), X_MASK,	PPC64,		{ RA, RS, RB } },
+{ "sld.",    XRC(31,27,1), X_MASK,	PPC64,		{ RA, RS, RB } },
+
+{ "and",     XRC(31,28,0), X_MASK,	COM,		{ RA, RS, RB } },
+{ "and.",    XRC(31,28,1), X_MASK,	COM,		{ RA, RS, RB } },
+
+{ "maskg",   XRC(31,29,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "maskg.",  XRC(31,29,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "cmplw",   XCMPL(31,32,0), XCMPL_MASK, PPCCOM,	{ OBF, RA, RB } },
+{ "cmpld",   XCMPL(31,32,1), XCMPL_MASK, PPC64,		{ OBF, RA, RB } },
+{ "cmpl",    X(31,32),	XCMP_MASK,	 PPCONLY,	{ BF, L, RA, RB } },
+{ "cmpl",    X(31,32),	XCMPL_MASK,	 PWRCOM,	{ BF, RA, RB } },
+
+{ "subf",    XO(31,40,0,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "sub",     XO(31,40,0,0), XO_MASK,	PPC,		{ RT, RB, RA } },
+{ "subf.",   XO(31,40,0,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "sub.",    XO(31,40,0,1), XO_MASK,	PPC,		{ RT, RB, RA } },
+{ "subfo",   XO(31,40,1,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "subo",    XO(31,40,1,0), XO_MASK,	PPC,		{ RT, RB, RA } },
+{ "subfo.",  XO(31,40,1,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "subo.",   XO(31,40,1,1), XO_MASK,	PPC,		{ RT, RB, RA } },
+
+{ "ldux",    X(31,53),	X_MASK,		PPC64,		{ RT, RAL, RB } },
+
+{ "dcbst",   X(31,54),	XRT_MASK,	PPC,		{ RA, RB } },
+
+{ "lwzux",   X(31,55),	X_MASK,		PPCCOM,		{ RT, RAL, RB } },
+{ "lux",     X(31,55),	X_MASK,		PWRCOM,		{ RT, RA, RB } },
+
+{ "cntlzd",  XRC(31,58,0), XRB_MASK,	PPC64,		{ RA, RS } },
+{ "cntlzd.", XRC(31,58,1), XRB_MASK,	PPC64,		{ RA, RS } },
+
+{ "andc",    XRC(31,60,0), X_MASK,	COM,	{ RA, RS, RB } },
+{ "andc.",   XRC(31,60,1), X_MASK,	COM,	{ RA, RS, RB } },
+
+{ "tdlgt",   XTO(31,68,TOLGT), XTO_MASK, PPC64,		{ RA, RB } },
+{ "tdllt",   XTO(31,68,TOLLT), XTO_MASK, PPC64,		{ RA, RB } },
+{ "tdeq",    XTO(31,68,TOEQ), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "tdlge",   XTO(31,68,TOLGE), XTO_MASK, PPC64,		{ RA, RB } },
+{ "tdlnl",   XTO(31,68,TOLNL), XTO_MASK, PPC64,		{ RA, RB } },
+{ "tdlle",   XTO(31,68,TOLLE), XTO_MASK, PPC64,		{ RA, RB } },
+{ "tdlng",   XTO(31,68,TOLNG), XTO_MASK, PPC64,		{ RA, RB } },
+{ "tdgt",    XTO(31,68,TOGT), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "tdge",    XTO(31,68,TOGE), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "tdnl",    XTO(31,68,TONL), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "tdlt",    XTO(31,68,TOLT), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "tdle",    XTO(31,68,TOLE), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "tdng",    XTO(31,68,TONG), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "tdne",    XTO(31,68,TONE), XTO_MASK,  PPC64,		{ RA, RB } },
+{ "td",	     X(31,68),	X_MASK,		 PPC64,		{ TO, RA, RB } },
+
+{ "mulhd",   XO(31,73,0,0), XO_MASK,	 PPC64,		{ RT, RA, RB } },
+{ "mulhd.",  XO(31,73,0,1), XO_MASK,	 PPC64,		{ RT, RA, RB } },
+
+{ "mulhw",   XO(31,75,0,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "mulhw.",  XO(31,75,0,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+
+{ "mtsrd",   X(31,82),	XRB_MASK|(1<<20), PPC64,	{ SR, RS } },
+
+{ "mfmsr",   X(31,83),	XRARB_MASK,	COM,		{ RT } },
+
+{ "ldarx",   X(31,84),	X_MASK,		PPC64,		{ RT, RA, RB } },
+
+{ "dcbf",    X(31,86),	XRT_MASK,	PPC,		{ RA, RB } },
+
+{ "lbzx",    X(31,87),	X_MASK,		COM,		{ RT, RA, RB } },
+
+{ "neg",     XO(31,104,0,0), XORB_MASK,	COM,		{ RT, RA } },
+{ "neg.",    XO(31,104,0,1), XORB_MASK,	COM,		{ RT, RA } },
+{ "nego",    XO(31,104,1,0), XORB_MASK,	COM,		{ RT, RA } },
+{ "nego.",   XO(31,104,1,1), XORB_MASK,	COM,		{ RT, RA } },
+
+{ "mul",     XO(31,107,0,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "mul.",    XO(31,107,0,1), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "mulo",    XO(31,107,1,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "mulo.",   XO(31,107,1,1), XO_MASK,	M601,		{ RT, RA, RB } },
+
+{ "mtsrdin", X(31,114),	XRA_MASK,	PPC64,		{ RS, RB } },
+
+{ "clf",     X(31,118), XRB_MASK,	POWER,		{ RT, RA } },
+
+{ "lbzux",   X(31,119),	X_MASK,		COM,		{ RT, RAL, RB } },
+
+{ "not",     XRC(31,124,0), X_MASK,	COM,		{ RA, RS, RBS } },
+{ "nor",     XRC(31,124,0), X_MASK,	COM,		{ RA, RS, RB } },
+{ "not.",    XRC(31,124,1), X_MASK,	COM,		{ RA, RS, RBS } },
+{ "nor.",    XRC(31,124,1), X_MASK,	COM,		{ RA, RS, RB } },
+
+{ "wrtee",   X(31,131),	XRARB_MASK,	PPC403,		{ RS } },
+
+{ "subfe",   XO(31,136,0,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sfe",     XO(31,136,0,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "subfe.",  XO(31,136,0,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sfe.",    XO(31,136,0,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "subfeo",  XO(31,136,1,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sfeo",    XO(31,136,1,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "subfeo.", XO(31,136,1,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "sfeo.",   XO(31,136,1,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+
+{ "adde",    XO(31,138,0,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "ae",      XO(31,138,0,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "adde.",   XO(31,138,0,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "ae.",     XO(31,138,0,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "addeo",   XO(31,138,1,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "aeo",     XO(31,138,1,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "addeo.",  XO(31,138,1,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "aeo.",    XO(31,138,1,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+
+{ "mtcr",    XFXM(31,144,0xff), XFXFXM_MASK|FXM_MASK, COM,	{ RS }},
+{ "mtcrf",   X(31,144),	XFXFXM_MASK,	COM,		{ FXM, RS } },
+
+{ "mtmsr",   X(31,146),	XRARB_MASK,	COM,		{ RS } },
+
+{ "stdx",    X(31,149), X_MASK,		PPC64,		{ RS, RA, RB } },
+
+{ "stwcx.",  XRC(31,150,1), X_MASK,	PPC,		{ RS, RA, RB } },
+
+{ "stwx",    X(31,151), X_MASK,		PPCCOM,		{ RS, RA, RB } },
+{ "stx",     X(31,151), X_MASK,		PWRCOM,		{ RS, RA, RB } },
+
+{ "slq",     XRC(31,152,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "slq.",    XRC(31,152,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "sle",     XRC(31,153,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "sle.",    XRC(31,153,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "wrteei",  X(31,163),	XE_MASK,	PPC403,		{ E } },
+
+{ "mtmsrd",  X(31,178),	XRARB_MASK,	PPC64,		{ RS } },
+
+{ "stdux",   X(31,181),	X_MASK,		PPC64,		{ RS, RAS, RB } },
+
+{ "stwux",   X(31,183),	X_MASK,		PPCCOM,		{ RS, RAS, RB } },
+{ "stux",    X(31,183),	X_MASK,		PWRCOM,		{ RS, RA, RB } },
+
+{ "sliq",    XRC(31,184,0), X_MASK,	M601,		{ RA, RS, SH } },
+{ "sliq.",   XRC(31,184,1), X_MASK,	M601,		{ RA, RS, SH } },
+
+{ "subfze",  XO(31,200,0,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfze",    XO(31,200,0,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "subfze.", XO(31,200,0,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfze.",   XO(31,200,0,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "subfzeo", XO(31,200,1,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfzeo",   XO(31,200,1,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "subfzeo.",XO(31,200,1,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfzeo.",  XO(31,200,1,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+
+{ "addze",   XO(31,202,0,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "aze",     XO(31,202,0,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "addze.",  XO(31,202,0,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "aze.",    XO(31,202,0,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "addzeo",  XO(31,202,1,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "azeo",    XO(31,202,1,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "addzeo.", XO(31,202,1,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "azeo.",   XO(31,202,1,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+
+{ "mtsr",    X(31,210),	XRB_MASK|(1<<20), COM32,	{ SR, RS } },
+
+{ "stdcx.",  XRC(31,214,1), X_MASK,	PPC64,		{ RS, RA, RB } },
+
+{ "stbx",    X(31,215),	X_MASK,		COM,	{ RS, RA, RB } },
+
+{ "sllq",    XRC(31,216,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "sllq.",   XRC(31,216,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "sleq",    XRC(31,217,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "sleq.",   XRC(31,217,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "subfme",  XO(31,232,0,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfme",    XO(31,232,0,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "subfme.", XO(31,232,0,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfme.",   XO(31,232,0,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "subfmeo", XO(31,232,1,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfmeo",   XO(31,232,1,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "subfmeo.",XO(31,232,1,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "sfmeo.",  XO(31,232,1,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+
+{ "mulld",   XO(31,233,0,0), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "mulld.",  XO(31,233,0,1), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "mulldo",  XO(31,233,1,0), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "mulldo.", XO(31,233,1,1), XO_MASK,	PPC64,		{ RT, RA, RB } },
+
+{ "addme",   XO(31,234,0,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "ame",     XO(31,234,0,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "addme.",  XO(31,234,0,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "ame.",    XO(31,234,0,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "addmeo",  XO(31,234,1,0), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "ameo",    XO(31,234,1,0), XORB_MASK, PWRCOM,		{ RT, RA } },
+{ "addmeo.", XO(31,234,1,1), XORB_MASK, PPCCOM,		{ RT, RA } },
+{ "ameo.",   XO(31,234,1,1), XORB_MASK, PWRCOM,		{ RT, RA } },
+
+{ "mullw",   XO(31,235,0,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "muls",    XO(31,235,0,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "mullw.",  XO(31,235,0,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "muls.",   XO(31,235,0,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "mullwo",  XO(31,235,1,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "mulso",   XO(31,235,1,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "mullwo.", XO(31,235,1,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "mulso.",  XO(31,235,1,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+
+{ "mtsrin",  X(31,242),	XRA_MASK,	PPC32,		{ RS, RB } },
+{ "mtsri",   X(31,242),	XRA_MASK,	POWER32,	{ RS, RB } },
+
+{ "dcbtst",  X(31,246),	XRT_MASK,	PPC,		{ RA, RB } },
+
+{ "stbux",   X(31,247),	X_MASK,		COM,		{ RS, RAS, RB } },
+
+{ "slliq",   XRC(31,248,0), X_MASK,	M601,		{ RA, RS, SH } },
+{ "slliq.",  XRC(31,248,1), X_MASK,	M601,		{ RA, RS, SH } },
+
+{ "doz",     XO(31,264,0,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "doz.",    XO(31,264,0,1), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "dozo",    XO(31,264,1,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "dozo.",   XO(31,264,1,1), XO_MASK,	M601,		{ RT, RA, RB } },
+
+{ "add",     XO(31,266,0,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "cax",     XO(31,266,0,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "add.",    XO(31,266,0,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "cax.",    XO(31,266,0,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "addo",    XO(31,266,1,0), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "caxo",    XO(31,266,1,0), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+{ "addo.",   XO(31,266,1,1), XO_MASK,	PPCCOM,		{ RT, RA, RB } },
+{ "caxo.",   XO(31,266,1,1), XO_MASK,	PWRCOM,		{ RT, RA, RB } },
+
+{ "lscbx",   XRC(31,277,0), X_MASK,	M601,		{ RT, RA, RB } },
+{ "lscbx.",  XRC(31,277,1), X_MASK,	M601,		{ RT, RA, RB } },
+
+{ "dcbt",    X(31,278),	XRT_MASK,	PPC,		{ RA, RB } },
+
+{ "lhzx",    X(31,279),	X_MASK,		COM,		{ RT, RA, RB } },
+
+{ "icbt",    X(31,262),	XRT_MASK,	PPC403,		{ RA, RB } },
+
+{ "eqv",     XRC(31,284,0), X_MASK,	COM,		{ RA, RS, RB } },
+{ "eqv.",    XRC(31,284,1), X_MASK,	COM,		{ RA, RS, RB } },
+
+{ "tlbie",   X(31,306),	XRTRA_MASK,	PPC,		{ RB } },
+{ "tlbi",    X(31,306),	XRT_MASK,	POWER,		{ RA, RB } },
+
+{ "eciwx",   X(31,310), X_MASK,		PPC,		{ RT, RA, RB } },
+
+{ "lhzux",   X(31,311),	X_MASK,		COM,		{ RT, RAL, RB } },
+
+{ "xor",     XRC(31,316,0), X_MASK,	COM,		{ RA, RS, RB } },
+{ "xor.",    XRC(31,316,1), X_MASK,	COM,		{ RA, RS, RB } },
+
+{ "mfexisr", XSPR(31,323,64), XSPR_MASK, PPC403,	{ RT } },
+{ "mfexier", XSPR(31,323,66), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr0",   XSPR(31,323,128), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr1",   XSPR(31,323,129), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr2",   XSPR(31,323,130), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr3",   XSPR(31,323,131), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr4",   XSPR(31,323,132), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr5",   XSPR(31,323,133), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr6",   XSPR(31,323,134), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbr7",   XSPR(31,323,135), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbear",  XSPR(31,323,144), XSPR_MASK, PPC403,	{ RT } },
+{ "mfbesr",  XSPR(31,323,145), XSPR_MASK, PPC403,	{ RT } },
+{ "mfiocr",  XSPR(31,323,160), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacr0", XSPR(31,323,192), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmact0", XSPR(31,323,193), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmada0", XSPR(31,323,194), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmasa0", XSPR(31,323,195), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacc0", XSPR(31,323,196), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacr1", XSPR(31,323,200), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmact1", XSPR(31,323,201), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmada1", XSPR(31,323,202), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmasa1", XSPR(31,323,203), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacc1", XSPR(31,323,204), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacr2", XSPR(31,323,208), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmact2", XSPR(31,323,209), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmada2", XSPR(31,323,210), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmasa2", XSPR(31,323,211), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacc2", XSPR(31,323,212), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacr3", XSPR(31,323,216), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmact3", XSPR(31,323,217), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmada3", XSPR(31,323,218), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmasa3", XSPR(31,323,219), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmacc3", XSPR(31,323,220), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdmasr", XSPR(31,323,224), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdcr",   X(31,323),	X_MASK,		PPC403,		{ RT, SPR } },
+
+{ "div",     XO(31,331,0,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "div.",    XO(31,331,0,1), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "divo",    XO(31,331,1,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "divo.",   XO(31,331,1,1), XO_MASK,	M601,		{ RT, RA, RB } },
+
+{ "mfmq",     XSPR(31,339,0),   XSPR_MASK, M601,	{ RT } },
+{ "mfxer",    XSPR(31,339,1),   XSPR_MASK, COM,		{ RT } },
+{ "mfrtcu",   XSPR(31,339,4),   XSPR_MASK, COM,		{ RT } },
+{ "mfrtcl",   XSPR(31,339,5),   XSPR_MASK, COM,		{ RT } },
+{ "mfdec",    XSPR(31,339,6),   XSPR_MASK, MFDEC1,	{ RT } },
+{ "mflr",     XSPR(31,339,8),   XSPR_MASK, COM,		{ RT } },
+{ "mfctr",    XSPR(31,339,9),   XSPR_MASK, COM,		{ RT } },
+{ "mftid",    XSPR(31,339,17),  XSPR_MASK, POWER,	{ RT } },
+{ "mfdsisr",  XSPR(31,339,18),  XSPR_MASK, COM,		{ RT } },
+{ "mfdar",    XSPR(31,339,19),  XSPR_MASK, COM,		{ RT } },
+{ "mfdec",    XSPR(31,339,22),  XSPR_MASK, MFDEC2,	{ RT } },
+{ "mfsdr0",   XSPR(31,339,24),  XSPR_MASK, POWER,	{ RT } },
+{ "mfsdr1",   XSPR(31,339,25),  XSPR_MASK, COM,		{ RT } },
+{ "mfsrr0",   XSPR(31,339,26),  XSPR_MASK, COM,		{ RT } },
+{ "mfsrr1",   XSPR(31,339,27),  XSPR_MASK, COM,		{ RT } },
+{ "mfcmpa",   XSPR(31,339,144), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcmpb",   XSPR(31,339,145), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcmpc",   XSPR(31,339,146), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcmpd",   XSPR(31,339,147), XSPR_MASK, PPC860,	{ RT } },
+{ "mficr",    XSPR(31,339,148), XSPR_MASK, PPC860,	{ RT } },
+{ "mfder",    XSPR(31,339,149), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcounta", XSPR(31,339,150), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcountb", XSPR(31,339,151), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcmpe",   XSPR(31,339,152), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcmpf",   XSPR(31,339,153), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcmpg",   XSPR(31,339,154), XSPR_MASK, PPC860,	{ RT } },
+{ "mfcmph",   XSPR(31,339,155), XSPR_MASK, PPC860,	{ RT } },
+{ "mflctrl1", XSPR(31,339,156), XSPR_MASK, PPC860,	{ RT } },
+{ "mflctrl2", XSPR(31,339,157), XSPR_MASK, PPC860,	{ RT } },
+{ "mfictrl",  XSPR(31,339,158), XSPR_MASK, PPC860,	{ RT } },
+{ "mfbar",    XSPR(31,339,159), XSPR_MASK, PPC860,	{ RT } },
+{ "mfsprg4",  XSPR(31,339,260), XSPR_MASK, PPC405,	{ RT } },
+{ "mfsprg5",  XSPR(31,339,261), XSPR_MASK, PPC405,	{ RT } },
+{ "mfsprg6",  XSPR(31,339,262), XSPR_MASK, PPC405,	{ RT } },
+{ "mfsprg7",  XSPR(31,339,263), XSPR_MASK, PPC405,	{ RT } },
+{ "mfsprg",   XSPR(31,339,272), XSPRG_MASK, PPC,	{ RT, SPRG } },
+{ "mfsprg0",  XSPR(31,339,272), XSPR_MASK, PPC,		{ RT } },
+{ "mfsprg1",  XSPR(31,339,273), XSPR_MASK, PPC,		{ RT } },
+{ "mfsprg2",  XSPR(31,339,274), XSPR_MASK, PPC,		{ RT } },
+{ "mfsprg3",  XSPR(31,339,275), XSPR_MASK, PPC,		{ RT } },
+{ "mfasr",    XSPR(31,339,280), XSPR_MASK, PPC64,	{ RT } },
+{ "mfear",    XSPR(31,339,282), XSPR_MASK, PPC,		{ RT } },
+{ "mfpvr",    XSPR(31,339,287), XSPR_MASK, PPC,		{ RT } },
+{ "mfibatu",  XSPR(31,339,528), XSPRBAT_MASK, PPC,	{ RT, SPRBAT } },
+{ "mfibatl",  XSPR(31,339,529), XSPRBAT_MASK, PPC,	{ RT, SPRBAT } },
+{ "mfdbatu",  XSPR(31,339,536), XSPRBAT_MASK, PPC,	{ RT, SPRBAT } },
+{ "mfdbatl",  XSPR(31,339,537), XSPRBAT_MASK, PPC,	{ RT, SPRBAT } },
+{ "mfic_cst", XSPR(31,339,560), XSPR_MASK, PPC860,	{ RT } },
+{ "mfic_adr", XSPR(31,339,561), XSPR_MASK, PPC860,	{ RT } },
+{ "mfic_dat", XSPR(31,339,562), XSPR_MASK, PPC860,	{ RT } },
+{ "mfdc_cst", XSPR(31,339,568), XSPR_MASK, PPC860,	{ RT } },
+{ "mfdc_adr", XSPR(31,339,569), XSPR_MASK, PPC860,	{ RT } },
+{ "mfdc_dat", XSPR(31,339,570), XSPR_MASK, PPC860,	{ RT } },
+{ "mfdpdr",   XSPR(31,339,630), XSPR_MASK, PPC860,	{ RT } },
+{ "mfdpir",   XSPR(31,339,631), XSPR_MASK, PPC860,	{ RT } },
+{ "mfimmr",   XSPR(31,339,638), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_ctr", XSPR(31,339,784), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_ap",  XSPR(31,339,786), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_epn", XSPR(31,339,787), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_twc", XSPR(31,339,789), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_rpn", XSPR(31,339,790), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_ctr", XSPR(31,339,792), XSPR_MASK, PPC860,	{ RT } },
+{ "mfm_casid",XSPR(31,339,793), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_ap",  XSPR(31,339,794), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_epn", XSPR(31,339,795), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_twb", XSPR(31,339,796), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_twc", XSPR(31,339,797), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_rpn", XSPR(31,339,798), XSPR_MASK, PPC860,	{ RT } },
+{ "mfm_tw",   XSPR(31,339,799), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_dbcam",XSPR(31,339,816), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_dbram0",XSPR(31,339,817), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmi_dbram1",XSPR(31,339,818), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_dbcam", XSPR(31,339,824), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_dbram0",XSPR(31,339,825), XSPR_MASK, PPC860,	{ RT } },
+{ "mfmd_dbram1",XSPR(31,339,826), XSPR_MASK, PPC860,	{ RT } },
+{ "mfzpr",   	XSPR(31,339,944), XSPR_MASK, PPC403,	{ RT } },
+{ "mfpid",   	XSPR(31,339,945), XSPR_MASK, PPC403,	{ RT } },
+{ "mfccr0",  	XSPR(31,339,947), XSPR_MASK, PPC405,	{ RT } },
+{ "mficdbdr",	XSPR(31,339,979), XSPR_MASK, PPC403,	{ RT } },
+{ "mfummcr0",	XSPR(31,339,936),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfupmc1",	XSPR(31,339,937),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfupmc2",	XSPR(31,339,938),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfusia",	XSPR(31,339,939),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfummcr1",	XSPR(31,339,940),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfupmc3",	XSPR(31,339,941),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfupmc4",	XSPR(31,339,942),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfiac3",     XSPR(31,339,948),  XSPR_MASK, PPC405,	{ RT } },
+{ "mfiac4",     XSPR(31,339,949),  XSPR_MASK, PPC405,	{ RT } },
+{ "mfdvc1",     XSPR(31,339,950),  XSPR_MASK, PPC405,	{ RT } },
+{ "mfdvc2",     XSPR(31,339,951),  XSPR_MASK, PPC405,	{ RT } },
+{ "mfmmcr0",	XSPR(31,339,952),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfpmc1",	XSPR(31,339,953),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfsgr",	XSPR(31,339,953),  XSPR_MASK, PPC403,	{ RT } },
+{ "mfpmc2",	XSPR(31,339,954),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfdcwr", 	XSPR(31,339,954),  XSPR_MASK, PPC403,	{ RT } },
+{ "mfsia",	XSPR(31,339,955),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfsler",	XSPR(31,339,955),  XSPR_MASK, PPC405,	{ RT } },
+{ "mfmmcr1",	XSPR(31,339,956),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfsu0r",	XSPR(31,339,956),  XSPR_MASK, PPC405,	{ RT } },
+{ "mfpmc3",	XSPR(31,339,957),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfdbcr1", 	XSPR(31,339,957),  XSPR_MASK, PPC405,	{ RT } },
+{ "mfpmc4",	XSPR(31,339,958),  XSPR_MASK, PPC750,	{ RT } },
+{ "mfesr",   XSPR(31,339,980), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdear",  XSPR(31,339,981), XSPR_MASK, PPC403,	{ RT } },
+{ "mfevpr",  XSPR(31,339,982), XSPR_MASK, PPC403,	{ RT } },
+{ "mfcdbcr", XSPR(31,339,983), XSPR_MASK, PPC403,	{ RT } },
+{ "mftsr",   XSPR(31,339,984), XSPR_MASK, PPC403,	{ RT } },
+{ "mftcr",   XSPR(31,339,986), XSPR_MASK, PPC403,	{ RT } },
+{ "mfpit",   XSPR(31,339,987), XSPR_MASK, PPC403,	{ RT } },
+{ "mftbhi",  XSPR(31,339,988), XSPR_MASK, PPC403,	{ RT } },
+{ "mftblo",  XSPR(31,339,989), XSPR_MASK, PPC403,	{ RT } },
+{ "mfsrr2",  XSPR(31,339,990), XSPR_MASK, PPC403,	{ RT } },
+{ "mfsrr3",  XSPR(31,339,991), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdbsr",  XSPR(31,339,1008), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdbcr0", XSPR(31,339,1010), XSPR_MASK, PPC405,	{ RT } },
+{ "mfiac1",  XSPR(31,339,1012), XSPR_MASK, PPC403,	{ RT } },
+{ "mfiac2",  XSPR(31,339,1013), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdac1",  XSPR(31,339,1014), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdac2",  XSPR(31,339,1015), XSPR_MASK, PPC403,	{ RT } },
+{ "mfdccr",  XSPR(31,339,1018), XSPR_MASK, PPC403,	{ RT } },
+{ "mficcr",  XSPR(31,339,1019), XSPR_MASK, PPC403,	{ RT } },
+{ "mfpbl1",  XSPR(31,339,1020), XSPR_MASK, PPC403,	{ RT } },
+{ "mfpbu1",  XSPR(31,339,1021), XSPR_MASK, PPC403,	{ RT } },
+{ "mfpbl2",  XSPR(31,339,1022), XSPR_MASK, PPC403,	{ RT } },
+{ "mfpbu2",  XSPR(31,339,1023), XSPR_MASK, PPC403,	{ RT } },
+{ "mfl2cr",	XSPR(31,339,1017), XSPR_MASK, PPC750,	{ RT } },
+{ "mfictc",	XSPR(31,339,1019), XSPR_MASK, PPC750,	{ RT } },
+{ "mfthrm1",	XSPR(31,339,1020), XSPR_MASK, PPC750,	{ RT } },
+{ "mfthrm2",	XSPR(31,339,1021), XSPR_MASK, PPC750,	{ RT } },
+{ "mfthrm3",	XSPR(31,339,1022), XSPR_MASK, PPC750,	{ RT } },
+{ "mfspr",   X(31,339),	X_MASK,		COM,		{ RT, SPR } },
+
+{ "lwax",    X(31,341),	X_MASK,		PPC64,		{ RT, RA, RB } },
+
+{ "lhax",    X(31,343),	X_MASK,		COM,		{ RT, RA, RB } },
+
+{ "dccci",   X(31,454),	XRT_MASK,	PPC403,		{ RA, RB } },
+
+{ "abs",     XO(31,360,0,0), XORB_MASK, M601,		{ RT, RA } },
+{ "abs.",    XO(31,360,0,1), XORB_MASK, M601,		{ RT, RA } },
+{ "abso",    XO(31,360,1,0), XORB_MASK, M601,		{ RT, RA } },
+{ "abso.",   XO(31,360,1,1), XORB_MASK, M601,		{ RT, RA } },
+
+{ "divs",    XO(31,363,0,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "divs.",   XO(31,363,0,1), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "divso",   XO(31,363,1,0), XO_MASK,	M601,		{ RT, RA, RB } },
+{ "divso.",  XO(31,363,1,1), XO_MASK,	M601,		{ RT, RA, RB } },
+
+{ "tlbia",   X(31,370),	0xffffffff,	PPC,		{ 0 } },
+
+{ "mftbl",   XSPR(31,371,268), XSPR_MASK, PPC,		{ RT } },
+{ "mftbu",   XSPR(31,371,269), XSPR_MASK, PPC,		{ RT } },
+{ "mftb",    X(31,371),	X_MASK,		PPC,		{ RT, TBR } },
+
+{ "lwaux",   X(31,373),	X_MASK,		PPC64,		{ RT, RAL, RB } },
+
+{ "lhaux",   X(31,375),	X_MASK,		COM,		{ RT, RAL, RB } },
+
+{ "sthx",    X(31,407),	X_MASK,		COM,		{ RS, RA, RB } },
+
+{ "lfqx",    X(31,791),	X_MASK,		POWER2,		{ FRT, RA, RB } },
+
+{ "lfqux",   X(31,823),	X_MASK,		POWER2,		{ FRT, RA, RB } },
+
+{ "stfqx",   X(31,919),	X_MASK,		POWER2,		{ FRS, RA, RB } },
+
+{ "stfqux",  X(31,951),	X_MASK,		POWER2,		{ FRS, RA, RB } },
+
+{ "orc",     XRC(31,412,0), X_MASK,	COM,		{ RA, RS, RB } },
+{ "orc.",    XRC(31,412,1), X_MASK,	COM,		{ RA, RS, RB } },
+
+{ "sradi",   XS(31,413,0), XS_MASK,	PPC64,		{ RA, RS, SH6 } },
+{ "sradi.",  XS(31,413,1), XS_MASK,	PPC64,		{ RA, RS, SH6 } },
+
+{ "slbie",   X(31,434),	XRTRA_MASK,	PPC64,		{ RB } },
+
+{ "ecowx",   X(31,438),	X_MASK,		PPC,		{ RT, RA, RB } },
+
+{ "sthux",   X(31,439),	X_MASK,		COM,		{ RS, RAS, RB } },
+
+{ "mr",	     XRC(31,444,0), X_MASK,	COM,		{ RA, RS, RBS } },
+{ "or",      XRC(31,444,0), X_MASK,	COM,		{ RA, RS, RB } },
+{ "mr.",     XRC(31,444,1), X_MASK,	COM,		{ RA, RS, RBS } },
+{ "or.",     XRC(31,444,1), X_MASK,	COM,		{ RA, RS, RB } },
+
+{ "mtexisr", XSPR(31,451,64), XSPR_MASK, PPC403,	{ RT } },
+{ "mtexier", XSPR(31,451,66), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr0",   XSPR(31,451,128), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr1",   XSPR(31,451,129), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr2",   XSPR(31,451,130), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr3",   XSPR(31,451,131), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr4",   XSPR(31,451,132), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr5",   XSPR(31,451,133), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr6",   XSPR(31,451,134), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbr7",   XSPR(31,451,135), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbear",  XSPR(31,451,144), XSPR_MASK, PPC403,	{ RT } },
+{ "mtbesr",  XSPR(31,451,145), XSPR_MASK, PPC403,	{ RT } },
+{ "mtiocr",  XSPR(31,451,160), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacr0", XSPR(31,451,192), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmact0", XSPR(31,451,193), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmada0", XSPR(31,451,194), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmasa0", XSPR(31,451,195), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacc0", XSPR(31,451,196), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacr1", XSPR(31,451,200), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmact1", XSPR(31,451,201), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmada1", XSPR(31,451,202), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmasa1", XSPR(31,451,203), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacc1", XSPR(31,451,204), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacr2", XSPR(31,451,208), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmact2", XSPR(31,451,209), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmada2", XSPR(31,451,210), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmasa2", XSPR(31,451,211), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacc2", XSPR(31,451,212), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacr3", XSPR(31,451,216), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmact3", XSPR(31,451,217), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmada3", XSPR(31,451,218), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmasa3", XSPR(31,451,219), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmacc3", XSPR(31,451,220), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdmasr", XSPR(31,451,224), XSPR_MASK, PPC403,	{ RT } },
+{ "mtummcr0",	XSPR(31,451,936),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtupmc1",	XSPR(31,451,937),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtupmc2",	XSPR(31,451,938),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtusia",	XSPR(31,451,939),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtummcr1",	XSPR(31,451,940),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtupmc3",	XSPR(31,451,941),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtupmc4",	XSPR(31,451,942),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtmmcr0",	XSPR(31,451,952),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtpmc1",	XSPR(31,451,953),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtpmc2",	XSPR(31,451,954),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtsia",	XSPR(31,451,955),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtmmcr1",	XSPR(31,451,956),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtpmc3",	XSPR(31,451,957),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtpmc4",	XSPR(31,451,958),  XSPR_MASK, PPC750,	{ RT } },
+{ "mtl2cr",	XSPR(31,451,1017), XSPR_MASK, PPC750,	{ RT } },
+{ "mtictc",	XSPR(31,451,1019), XSPR_MASK, PPC750,	{ RT } },
+{ "mtthrm1",	XSPR(31,451,1020), XSPR_MASK, PPC750,	{ RT } },
+{ "mtthrm2",	XSPR(31,451,1021), XSPR_MASK, PPC750,	{ RT } },
+{ "mtthrm3",	XSPR(31,451,1022), XSPR_MASK, PPC750,	{ RT } },
+{ "mtdcr",   X(31,451),	X_MASK,		PPC403,		{ SPR, RS } },
+
+{ "divdu",   XO(31,457,0,0), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "divdu.",  XO(31,457,0,1), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "divduo",  XO(31,457,1,0), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "divduo.", XO(31,457,1,1), XO_MASK,	PPC64,		{ RT, RA, RB } },
+
+{ "divwu",   XO(31,459,0,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "divwu.",  XO(31,459,0,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "divwuo",  XO(31,459,1,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "divwuo.", XO(31,459,1,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+
+{ "mtmq",    XSPR(31,467,0),   XSPR_MASK,    M601,	{ RS } },
+{ "mtxer",   XSPR(31,467,1),   XSPR_MASK,    COM,	{ RS } },
+{ "mtlr",    XSPR(31,467,8),   XSPR_MASK,    COM,	{ RS } },
+{ "mtctr",   XSPR(31,467,9),   XSPR_MASK,    COM,	{ RS } },
+{ "mttid",   XSPR(31,467,17),  XSPR_MASK,    POWER,	{ RS } },
+{ "mtdsisr", XSPR(31,467,18),  XSPR_MASK,    COM,	{ RS } },
+{ "mtdar",   XSPR(31,467,19),  XSPR_MASK,    COM,	{ RS } },
+{ "mtrtcu",  XSPR(31,467,20),  XSPR_MASK,    COM,	{ RS } },
+{ "mtrtcl",  XSPR(31,467,21),  XSPR_MASK,    COM,	{ RS } },
+{ "mtdec",   XSPR(31,467,22),  XSPR_MASK,    COM,	{ RS } },
+{ "mtsdr0",  XSPR(31,467,24),  XSPR_MASK,    POWER,	{ RS } },
+{ "mtsdr1",  XSPR(31,467,25),  XSPR_MASK,    COM,	{ RS } },
+{ "mtsrr0",  XSPR(31,467,26),  XSPR_MASK,    COM,	{ RS } },
+{ "mtsrr1",  XSPR(31,467,27),  XSPR_MASK,    COM,	{ RS } },
+{ "mtcmpa",   XSPR(31,467,144), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcmpb",   XSPR(31,467,145), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcmpc",   XSPR(31,467,146), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcmpd",   XSPR(31,467,147), XSPR_MASK, PPC860,	{ RT } },
+{ "mticr",    XSPR(31,467,148), XSPR_MASK, PPC860,	{ RT } },
+{ "mtder",    XSPR(31,467,149), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcounta", XSPR(31,467,150), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcountb", XSPR(31,467,151), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcmpe",   XSPR(31,467,152), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcmpf",   XSPR(31,467,153), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcmpg",   XSPR(31,467,154), XSPR_MASK, PPC860,	{ RT } },
+{ "mtcmph",   XSPR(31,467,155), XSPR_MASK, PPC860,	{ RT } },
+{ "mtlctrl1", XSPR(31,467,156), XSPR_MASK, PPC860,	{ RT } },
+{ "mtlctrl2", XSPR(31,467,157), XSPR_MASK, PPC860,	{ RT } },
+{ "mtictrl",  XSPR(31,467,158), XSPR_MASK, PPC860,	{ RT } },
+{ "mtbar",    XSPR(31,467,159), XSPR_MASK, PPC860,	{ RT } },
+{ "mtsprg",  XSPR(31,467,272), XSPRG_MASK,   PPC,	{ SPRG, RS } },
+{ "mtsprg0", XSPR(31,467,272), XSPR_MASK,    PPC,	{ RT } },
+{ "mtsprg1", XSPR(31,467,273), XSPR_MASK,    PPC,	{ RT } },
+{ "mtsprg2", XSPR(31,467,274), XSPR_MASK,    PPC,	{ RT } },
+{ "mtsprg3", XSPR(31,467,275), XSPR_MASK,    PPC,	{ RT } },
+{ "mtsprg4", XSPR(31,467,276), XSPR_MASK,    PPC405,	{ RT } },
+{ "mtsprg5", XSPR(31,467,277), XSPR_MASK,    PPC405,	{ RT } },
+{ "mtsprg6", XSPR(31,467,278), XSPR_MASK,    PPC405,	{ RT } },
+{ "mtsprg7", XSPR(31,467,279), XSPR_MASK,    PPC405,	{ RT } },
+{ "mtasr",   XSPR(31,467,280), XSPR_MASK,    PPC64,	{ RS } },
+{ "mtear",   XSPR(31,467,282), XSPR_MASK,    PPC,	{ RS } },
+{ "mttbl",   XSPR(31,467,284), XSPR_MASK,    PPC,	{ RS } },
+{ "mttbu",   XSPR(31,467,285), XSPR_MASK,    PPC,	{ RS } },
+{ "mtibatu", XSPR(31,467,528), XSPRBAT_MASK, PPC,	{ SPRBAT, RS } },
+{ "mtibatl", XSPR(31,467,529), XSPRBAT_MASK, PPC,	{ SPRBAT, RS } },
+{ "mtdbatu", XSPR(31,467,536), XSPRBAT_MASK, PPC,	{ SPRBAT, RS } },
+{ "mtdbatl", XSPR(31,467,537), XSPRBAT_MASK, PPC,	{ SPRBAT, RS } },
+{ "mtzpr",   XSPR(31,467,944), XSPR_MASK, PPC403,	{ RT } },
+{ "mtpid",   XSPR(31,467,945), XSPR_MASK, PPC403,	{ RT } },
+{ "mtccr0",  XSPR(31,467,947), XSPR_MASK, PPC405,	{ RT } },
+{ "mtiac3",  XSPR(31,467,948), XSPR_MASK, PPC405,	{ RT } },
+{ "mtiac4",  XSPR(31,467,949), XSPR_MASK, PPC405,	{ RT } },
+{ "mtdvc1",  XSPR(31,467,950), XSPR_MASK, PPC405,	{ RT } },
+{ "mtdvc2",  XSPR(31,467,951), XSPR_MASK, PPC405,	{ RT } },
+{ "mtsgr",   XSPR(31,467,953), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdcwr",  XSPR(31,467,954), XSPR_MASK, PPC403,	{ RT } },
+{ "mtsler",  XSPR(31,467,955), XSPR_MASK, PPC405,	{ RT } },
+{ "mtsu0r",  XSPR(31,467,956), XSPR_MASK, PPC405,	{ RT } },
+{ "mtdbcr1", XSPR(31,467,957), XSPR_MASK, PPC405,	{ RT } },
+{ "mticdbdr",XSPR(31,467,979), XSPR_MASK, PPC403,	{ RT } },
+{ "mtesr",   XSPR(31,467,980), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdear",  XSPR(31,467,981), XSPR_MASK, PPC403,	{ RT } },
+{ "mtevpr",  XSPR(31,467,982), XSPR_MASK, PPC403,	{ RT } },
+{ "mtcdbcr", XSPR(31,467,983), XSPR_MASK, PPC403,	{ RT } },
+{ "mttsr",   XSPR(31,467,984), XSPR_MASK, PPC403,	{ RT } },
+{ "mttcr",   XSPR(31,467,986), XSPR_MASK, PPC403,	{ RT } },
+{ "mtpit",   XSPR(31,467,987), XSPR_MASK, PPC403,	{ RT } },
+{ "mttbhi",  XSPR(31,467,988), XSPR_MASK, PPC403,	{ RT } },
+{ "mttblo",  XSPR(31,467,989), XSPR_MASK, PPC403,	{ RT } },
+{ "mtsrr2",  XSPR(31,467,990), XSPR_MASK, PPC403,	{ RT } },
+{ "mtsrr3",  XSPR(31,467,991), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdbsr",  XSPR(31,467,1008), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdbcr0", XSPR(31,467,1010), XSPR_MASK, PPC405,	{ RT } },
+{ "mtiac1",  XSPR(31,467,1012), XSPR_MASK, PPC403,	{ RT } },
+{ "mtiac2",  XSPR(31,467,1013), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdac1",  XSPR(31,467,1014), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdac2",  XSPR(31,467,1015), XSPR_MASK, PPC403,	{ RT } },
+{ "mtdccr",  XSPR(31,467,1018), XSPR_MASK, PPC403,	{ RT } },
+{ "mticcr",  XSPR(31,467,1019), XSPR_MASK, PPC403,	{ RT } },
+{ "mtpbl1",  XSPR(31,467,1020), XSPR_MASK, PPC403,	{ RT } },
+{ "mtpbu1",  XSPR(31,467,1021), XSPR_MASK, PPC403,	{ RT } },
+{ "mtpbl2",  XSPR(31,467,1022), XSPR_MASK, PPC403,	{ RT } },
+{ "mtpbu2",  XSPR(31,467,1023), XSPR_MASK, PPC403,	{ RT } },
+{ "mtspr",   X(31,467),	       X_MASK,	     COM,	{ SPR, RS } },
+
+{ "dcbi",    X(31,470),	XRT_MASK,	PPC,		{ RA, RB } },
+
+{ "nand",    XRC(31,476,0), X_MASK,	COM,		{ RA, RS, RB } },
+{ "nand.",   XRC(31,476,1), X_MASK,	COM,		{ RA, RS, RB } },
+
+{ "dcread",  X(31,486),	X_MASK,		PPC403,		{ RT, RA, RB }},
+
+{ "nabs",    XO(31,488,0,0), XORB_MASK, M601,		{ RT, RA } },
+{ "nabs.",   XO(31,488,0,1), XORB_MASK, M601,		{ RT, RA } },
+{ "nabso",   XO(31,488,1,0), XORB_MASK, M601,		{ RT, RA } },
+{ "nabso.",  XO(31,488,1,1), XORB_MASK, M601,		{ RT, RA } },
+
+{ "divd",    XO(31,489,0,0), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "divd.",   XO(31,489,0,1), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "divdo",   XO(31,489,1,0), XO_MASK,	PPC64,		{ RT, RA, RB } },
+{ "divdo.",  XO(31,489,1,1), XO_MASK,	PPC64,		{ RT, RA, RB } },
+
+{ "divw",    XO(31,491,0,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "divw.",   XO(31,491,0,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "divwo",   XO(31,491,1,0), XO_MASK,	PPC,		{ RT, RA, RB } },
+{ "divwo.",  XO(31,491,1,1), XO_MASK,	PPC,		{ RT, RA, RB } },
+
+{ "slbia",   X(31,498),	0xffffffff,	PPC64,		{ 0 } },
+
+{ "cli",     X(31,502), XRB_MASK,	POWER,		{ RT, RA } },
+
+{ "mcrxr",   X(31,512),	XRARB_MASK|(3<<21), COM,	{ BF } },
+
+{ "clcs",    X(31,531), XRB_MASK,	M601,		{ RT, RA } },
+
+{ "lswx",    X(31,533),	X_MASK,		PPCCOM,		{ RT, RA, RB } },
+{ "lsx",     X(31,533),	X_MASK,		PWRCOM,		{ RT, RA, RB } },
+
+{ "lwbrx",   X(31,534),	X_MASK,		PPCCOM,		{ RT, RA, RB } },
+{ "lbrx",    X(31,534),	X_MASK,		PWRCOM,		{ RT, RA, RB } },
+
+{ "lfsx",    X(31,535),	X_MASK,		COM,		{ FRT, RA, RB } },
+
+{ "srw",     XRC(31,536,0), X_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "sr",      XRC(31,536,0), X_MASK,	PWRCOM,		{ RA, RS, RB } },
+{ "srw.",    XRC(31,536,1), X_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "sr.",     XRC(31,536,1), X_MASK,	PWRCOM,		{ RA, RS, RB } },
+
+{ "rrib",    XRC(31,537,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "rrib.",   XRC(31,537,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "srd",     XRC(31,539,0), X_MASK,	PPC64,		{ RA, RS, RB } },
+{ "srd.",    XRC(31,539,1), X_MASK,	PPC64,		{ RA, RS, RB } },
+
+{ "maskir",  XRC(31,541,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "maskir.", XRC(31,541,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "tlbsync", X(31,566),	0xffffffff,	PPC,		{ 0 } },
+
+{ "lfsux",   X(31,567),	X_MASK,		COM,		{ FRT, RAS, RB } },
+
+{ "mfsr",    X(31,595),	XRB_MASK|(1<<20), COM32,	{ RT, SR } },
+
+{ "lswi",    X(31,597),	X_MASK,		PPCCOM,		{ RT, RA, NB } },
+{ "lsi",     X(31,597),	X_MASK,		PWRCOM,		{ RT, RA, NB } },
+
+{ "sync",    X(31,598), 0xffffffff,	PPCCOM,		{ 0 } },
+{ "dcs",     X(31,598), 0xffffffff,	PWRCOM,		{ 0 } },
+
+{ "lfdx",    X(31,599), X_MASK,		COM,		{ FRT, RA, RB } },
+
+{ "mfsri",   X(31,627), X_MASK,		PWRCOM,		{ RT, RA, RB } },
+
+{ "dclst",   X(31,630), XRB_MASK,	PWRCOM,		{ RS, RA } },
+
+{ "lfdux",   X(31,631), X_MASK,		COM,		{ FRT, RAS, RB } },
+
+{ "mfsrin",  X(31,659), XRA_MASK,	PPC32,		{ RT, RB } },
+
+{ "stswx",   X(31,661), X_MASK,		PPCCOM,		{ RS, RA, RB } },
+{ "stsx",    X(31,661), X_MASK,		PWRCOM,		{ RS, RA, RB } },
+
+{ "stwbrx",  X(31,662), X_MASK,		PPCCOM,		{ RS, RA, RB } },
+{ "stbrx",   X(31,662), X_MASK,		PWRCOM,		{ RS, RA, RB } },
+
+{ "stfsx",   X(31,663), X_MASK,		COM,		{ FRS, RA, RB } },
+
+{ "srq",     XRC(31,664,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "srq.",    XRC(31,664,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "sre",     XRC(31,665,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "sre.",    XRC(31,665,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "stfsux",  X(31,695),	X_MASK,		COM,		{ FRS, RAS, RB } },
+
+{ "sriq",    XRC(31,696,0), X_MASK,	M601,		{ RA, RS, SH } },
+{ "sriq.",   XRC(31,696,1), X_MASK,	M601,		{ RA, RS, SH } },
+
+{ "stswi",   X(31,725),	X_MASK,		PPCCOM,		{ RS, RA, NB } },
+{ "stsi",    X(31,725),	X_MASK,		PWRCOM,		{ RS, RA, NB } },
+
+{ "stfdx",   X(31,727),	X_MASK,		COM,		{ FRS, RA, RB } },
+
+{ "srlq",    XRC(31,728,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "srlq.",   XRC(31,728,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "sreq",    XRC(31,729,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "sreq.",   XRC(31,729,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "dcba",    X(31,758),	XRT_MASK,	PPC405,		{ RA, RB } },
+
+{ "stfdux",  X(31,759),	X_MASK,		COM,		{ FRS, RAS, RB } },
+
+{ "srliq",   XRC(31,760,0), X_MASK,	M601,		{ RA, RS, SH } },
+{ "srliq.",  XRC(31,760,1), X_MASK,	M601,		{ RA, RS, SH } },
+
+{ "lhbrx",   X(31,790),	X_MASK,		COM,		{ RT, RA, RB } },
+
+{ "sraw",    XRC(31,792,0), X_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "sra",     XRC(31,792,0), X_MASK,	PWRCOM,		{ RA, RS, RB } },
+{ "sraw.",   XRC(31,792,1), X_MASK,	PPCCOM,		{ RA, RS, RB } },
+{ "sra.",    XRC(31,792,1), X_MASK,	PWRCOM,		{ RA, RS, RB } },
+
+{ "srad",    XRC(31,794,0), X_MASK,	PPC64,		{ RA, RS, RB } },
+{ "srad.",   XRC(31,794,1), X_MASK,	PPC64,		{ RA, RS, RB } },
+
+{ "rac",     X(31,818),	X_MASK,		PWRCOM,		{ RT, RA, RB } },
+
+{ "srawi",   XRC(31,824,0), X_MASK,	PPCCOM,		{ RA, RS, SH } },
+{ "srai",    XRC(31,824,0), X_MASK,	PWRCOM,		{ RA, RS, SH } },
+{ "srawi.",  XRC(31,824,1), X_MASK,	PPCCOM,		{ RA, RS, SH } },
+{ "srai.",   XRC(31,824,1), X_MASK,	PWRCOM,		{ RA, RS, SH } },
+
+{ "eieio",   X(31,854),	0xffffffff,	PPC,		{ 0 } },
+
+{ "tlbsx",   XRC(31,914,0), X_MASK, PPC403,	{ RT, RA, RB } },
+{ "tlbsx.",  XRC(31,914,1), X_MASK, PPC403,	{ RT, RA, RB } },
+
+{ "sthbrx",  X(31,918),	X_MASK,		COM,		{ RS, RA, RB } },
+
+{ "sraq",    XRC(31,920,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "sraq.",   XRC(31,920,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "srea",    XRC(31,921,0), X_MASK,	M601,		{ RA, RS, RB } },
+{ "srea.",   XRC(31,921,1), X_MASK,	M601,		{ RA, RS, RB } },
+
+{ "extsh",   XRC(31,922,0), XRB_MASK,	PPCCOM,		{ RA, RS } },
+{ "exts",    XRC(31,922,0), XRB_MASK,	PWRCOM,		{ RA, RS } },
+{ "extsh.",  XRC(31,922,1), XRB_MASK,	PPCCOM,		{ RA, RS } },
+{ "exts.",   XRC(31,922,1), XRB_MASK,	PWRCOM,		{ RA, RS } },
+
+{ "tlbrehi", XTLB(31,946,0), XTLB_MASK,	PPC403,		{ RT, RA } },
+{ "tlbrelo", XTLB(31,946,1), XTLB_MASK,	PPC403,		{ RT, RA } },
+{ "tlbre",   X(31,946),	X_MASK,		PPC403,		{ RT, RA, SH } },
+
+{ "sraiq",   XRC(31,952,0), X_MASK,	M601,		{ RA, RS, SH } },
+{ "sraiq.",  XRC(31,952,1), X_MASK,	M601,		{ RA, RS, SH } },
+
+{ "extsb",   XRC(31,954,0), XRB_MASK,	PPC,		{ RA, RS} },
+{ "extsb.",  XRC(31,954,1), XRB_MASK,	PPC,		{ RA, RS} },
+
+{ "iccci",   X(31,966),	XRT_MASK,	PPC403,		{ RA, RB } },
+
+{ "tlbld",   X(31,978),	XRTRA_MASK,	PPC,		{ RB } },
+
+{ "tlbwehi", XTLB(31,978,0), XTLB_MASK,	PPC403,		{ RT, RA } },
+{ "tlbwelo", XTLB(31,978,1), XTLB_MASK,	PPC403,		{ RT, RA } },
+{ "tlbwe",   X(31,978),	X_MASK,		PPC403,		{ RS, RA, SH } },
+
+{ "icbi",    X(31,982),	XRT_MASK,	PPC,		{ RA, RB } },
+
+{ "stfiwx",  X(31,983),	X_MASK,		PPC,		{ FRS, RA, RB } },
+
+{ "extsw",   XRC(31,986,0), XRB_MASK,	PPC,		{ RA, RS } },
+{ "extsw.",  XRC(31,986,1), XRB_MASK,	PPC,		{ RA, RS } },
+
+{ "icread",  X(31,998),	XRT_MASK,	PPC403,		{ RA, RB } },
+
+{ "tlbli",   X(31,1010), XRTRA_MASK,	PPC,		{ RB } },
+
+{ "dcbz",    X(31,1014), XRT_MASK,	PPC,		{ RA, RB } },
+{ "dclz",    X(31,1014), XRT_MASK,	PPC,		{ RA, RB } },
+
+{ "lvebx",   X(31,   7), X_MASK,	PPCVEC,		{ VD, RA, RB } },
+{ "lvehx",   X(31,  39), X_MASK,	PPCVEC,		{ VD, RA, RB } },
+{ "lvewx",   X(31,  71), X_MASK,	PPCVEC,		{ VD, RA, RB } },
+{ "lvsl",    X(31,   6), X_MASK,	PPCVEC,		{ VD, RA, RB } },
+{ "lvsr",    X(31,  38), X_MASK,	PPCVEC,		{ VD, RA, RB } },
+{ "lvx",     X(31, 103), X_MASK,	PPCVEC,		{ VD, RA, RB } },
+{ "lvxl",    X(31, 359), X_MASK,	PPCVEC,		{ VD, RA, RB } },
+{ "stvebx",  X(31, 135), X_MASK,	PPCVEC,		{ VS, RA, RB } },
+{ "stvehx",  X(31, 167), X_MASK,	PPCVEC,		{ VS, RA, RB } },
+{ "stvewx",  X(31, 199), X_MASK,	PPCVEC,		{ VS, RA, RB } },
+{ "stvx",    X(31, 231), X_MASK,	PPCVEC,		{ VS, RA, RB } },
+{ "stvxl",   X(31, 487), X_MASK,	PPCVEC,		{ VS, RA, RB } },
+
+{ "lwz",     OP(32),	OP_MASK,	PPCCOM,		{ RT, D, RA } },
+{ "l",	     OP(32),	OP_MASK,	PWRCOM,		{ RT, D, RA } },
+
+{ "lwzu",    OP(33),	OP_MASK,	PPCCOM,		{ RT, D, RAL } },
+{ "lu",      OP(33),	OP_MASK,	PWRCOM,		{ RT, D, RA } },
+
+{ "lbz",     OP(34),	OP_MASK,	COM,		{ RT, D, RA } },
+
+{ "lbzu",    OP(35),	OP_MASK,	COM,		{ RT, D, RAL } },
+
+{ "stw",     OP(36),	OP_MASK,	PPCCOM,		{ RS, D, RA } },
+{ "st",      OP(36),	OP_MASK,	PWRCOM,		{ RS, D, RA } },
+
+{ "stwu",    OP(37),	OP_MASK,	PPCCOM,		{ RS, D, RAS } },
+{ "stu",     OP(37),	OP_MASK,	PWRCOM,		{ RS, D, RA } },
+
+{ "stb",     OP(38),	OP_MASK,	COM,		{ RS, D, RA } },
+
+{ "stbu",    OP(39),	OP_MASK,	COM,		{ RS, D, RAS } },
+
+{ "lhz",     OP(40),	OP_MASK,	COM,		{ RT, D, RA } },
+
+{ "lhzu",    OP(41),	OP_MASK,	COM,		{ RT, D, RAL } },
+
+{ "lha",     OP(42),	OP_MASK,	COM,		{ RT, D, RA } },
+
+{ "lhau",    OP(43),	OP_MASK,	COM,		{ RT, D, RAL } },
+
+{ "sth",     OP(44),	OP_MASK,	COM,		{ RS, D, RA } },
+
+{ "sthu",    OP(45),	OP_MASK,	COM,		{ RS, D, RAS } },
+
+{ "lmw",     OP(46),	OP_MASK,	PPCCOM,		{ RT, D, RAM } },
+{ "lm",      OP(46),	OP_MASK,	PWRCOM,		{ RT, D, RA } },
+
+{ "stmw",    OP(47),	OP_MASK,	PPCCOM,		{ RS, D, RA } },
+{ "stm",     OP(47),	OP_MASK,	PWRCOM,		{ RS, D, RA } },
+
+{ "lfs",     OP(48),	OP_MASK,	COM,		{ FRT, D, RA } },
+
+{ "lfsu",    OP(49),	OP_MASK,	COM,		{ FRT, D, RAS } },
+
+{ "lfd",     OP(50),	OP_MASK,	COM,		{ FRT, D, RA } },
+
+{ "lfdu",    OP(51),	OP_MASK,	COM,		{ FRT, D, RAS } },
+
+{ "stfs",    OP(52),	OP_MASK,	COM,		{ FRS, D, RA } },
+
+{ "stfsu",   OP(53),	OP_MASK,	COM,		{ FRS, D, RAS } },
+
+{ "stfd",    OP(54),	OP_MASK,	COM,		{ FRS, D, RA } },
+
+{ "stfdu",   OP(55),	OP_MASK,	COM,		{ FRS, D, RAS } },
+
+{ "lfq",     OP(56),	OP_MASK,	POWER2,		{ FRT, D, RA } },
+
+{ "lfqu",    OP(57),	OP_MASK,	POWER2,		{ FRT, D, RA } },
+
+{ "ld",      DSO(58,0),	DS_MASK,	PPC64,		{ RT, DS, RA } },
+
+{ "ldu",     DSO(58,1), DS_MASK,	PPC64,		{ RT, DS, RAL } },
+
+{ "lwa",     DSO(58,2), DS_MASK,	PPC64,		{ RT, DS, RA } },
+
+{ "fdivs",   A(59,18,0), AFRC_MASK,	PPC,		{ FRT, FRA, FRB } },
+{ "fdivs.",  A(59,18,1), AFRC_MASK,	PPC,		{ FRT, FRA, FRB } },
+
+{ "fsubs",   A(59,20,0), AFRC_MASK,	PPC,		{ FRT, FRA, FRB } },
+{ "fsubs.",  A(59,20,1), AFRC_MASK,	PPC,		{ FRT, FRA, FRB } },
+
+{ "fadds",   A(59,21,0), AFRC_MASK,	PPC,		{ FRT, FRA, FRB } },
+{ "fadds.",  A(59,21,1), AFRC_MASK,	PPC,		{ FRT, FRA, FRB } },
+
+{ "fsqrts",  A(59,22,0), AFRAFRC_MASK,	PPC,		{ FRT, FRB } },
+{ "fsqrts.", A(59,22,1), AFRAFRC_MASK,	PPC,		{ FRT, FRB } },
+
+{ "fres",    A(59,24,0), AFRAFRC_MASK,	PPC,		{ FRT, FRB } },
+{ "fres.",   A(59,24,1), AFRAFRC_MASK,	PPC,		{ FRT, FRB } },
+
+{ "fmuls",   A(59,25,0), AFRB_MASK,	PPC,		{ FRT, FRA, FRC } },
+{ "fmuls.",  A(59,25,1), AFRB_MASK,	PPC,		{ FRT, FRA, FRC } },
+
+{ "fmsubs",  A(59,28,0), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+{ "fmsubs.", A(59,28,1), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+
+{ "fmadds",  A(59,29,0), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+{ "fmadds.", A(59,29,1), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+
+{ "fnmsubs", A(59,30,0), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+{ "fnmsubs.",A(59,30,1), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+
+{ "fnmadds", A(59,31,0), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+{ "fnmadds.",A(59,31,1), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+
+{ "stfq",    OP(60),	OP_MASK,	POWER2,		{ FRS, D, RA } },
+
+{ "stfqu",   OP(61),	OP_MASK,	POWER2,		{ FRS, D, RA } },
+
+{ "std",     DSO(62,0),	DS_MASK,	PPC64,		{ RS, DS, RA } },
+
+{ "stdu",    DSO(62,1),	DS_MASK,	PPC64,		{ RS, DS, RAS } },
+
+{ "fcmpu",   X(63,0),	X_MASK|(3<<21),	COM,		{ BF, FRA, FRB } },
+
+{ "frsp",    XRC(63,12,0), XRA_MASK,	COM,		{ FRT, FRB } },
+{ "frsp.",   XRC(63,12,1), XRA_MASK,	COM,		{ FRT, FRB } },
+
+{ "fctiw",   XRC(63,14,0), XRA_MASK,	PPCCOM,		{ FRT, FRB } },
+{ "fcir",    XRC(63,14,0), XRA_MASK,	POWER2,		{ FRT, FRB } },
+{ "fctiw.",  XRC(63,14,1), XRA_MASK,	PPCCOM,		{ FRT, FRB } },
+{ "fcir.",   XRC(63,14,1), XRA_MASK,	POWER2,		{ FRT, FRB } },
+
+{ "fctiwz",  XRC(63,15,0), XRA_MASK,	PPCCOM,		{ FRT, FRB } },
+{ "fcirz",   XRC(63,15,0), XRA_MASK,	POWER2,		{ FRT, FRB } },
+{ "fctiwz.", XRC(63,15,1), XRA_MASK,	PPCCOM,		{ FRT, FRB } },
+{ "fcirz.",  XRC(63,15,1), XRA_MASK,	POWER2,		{ FRT, FRB } },
+
+{ "fdiv",    A(63,18,0), AFRC_MASK,	PPCCOM,		{ FRT, FRA, FRB } },
+{ "fd",      A(63,18,0), AFRC_MASK,	PWRCOM,		{ FRT, FRA, FRB } },
+{ "fdiv.",   A(63,18,1), AFRC_MASK,	PPCCOM,		{ FRT, FRA, FRB } },
+{ "fd.",     A(63,18,1), AFRC_MASK,	PWRCOM,		{ FRT, FRA, FRB } },
+
+{ "fsub",    A(63,20,0), AFRC_MASK,	PPCCOM,		{ FRT, FRA, FRB } },
+{ "fs",      A(63,20,0), AFRC_MASK,	PWRCOM,		{ FRT, FRA, FRB } },
+{ "fsub.",   A(63,20,1), AFRC_MASK,	PPCCOM,		{ FRT, FRA, FRB } },
+{ "fs.",     A(63,20,1), AFRC_MASK,	PWRCOM,		{ FRT, FRA, FRB } },
+
+{ "fadd",    A(63,21,0), AFRC_MASK,	PPCCOM,		{ FRT, FRA, FRB } },
+{ "fa",      A(63,21,0), AFRC_MASK,	PWRCOM,		{ FRT, FRA, FRB } },
+{ "fadd.",   A(63,21,1), AFRC_MASK,	PPCCOM,		{ FRT, FRA, FRB } },
+{ "fa.",     A(63,21,1), AFRC_MASK,	PWRCOM,		{ FRT, FRA, FRB } },
+
+{ "fsqrt",   A(63,22,0), AFRAFRC_MASK,	PPCPWR2,	{ FRT, FRB } },
+{ "fsqrt.",  A(63,22,1), AFRAFRC_MASK,	PPCPWR2,	{ FRT, FRB } },
+
+{ "fsel",    A(63,23,0), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+{ "fsel.",   A(63,23,1), A_MASK,	PPC,		{ FRT,FRA,FRC,FRB } },
+
+{ "fmul",    A(63,25,0), AFRB_MASK,	PPCCOM,		{ FRT, FRA, FRC } },
+{ "fm",      A(63,25,0), AFRB_MASK,	PWRCOM,		{ FRT, FRA, FRC } },
+{ "fmul.",   A(63,25,1), AFRB_MASK,	PPCCOM,		{ FRT, FRA, FRC } },
+{ "fm.",     A(63,25,1), AFRB_MASK,	PWRCOM,		{ FRT, FRA, FRC } },
+
+{ "frsqrte", A(63,26,0), AFRAFRC_MASK,	PPC,		{ FRT, FRB } },
+{ "frsqrte.",A(63,26,1), AFRAFRC_MASK,	PPC,		{ FRT, FRB } },
+
+{ "fmsub",   A(63,28,0), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fms",     A(63,28,0), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fmsub.",  A(63,28,1), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fms.",    A(63,28,1), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+
+{ "fmadd",   A(63,29,0), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fma",     A(63,29,0), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fmadd.",  A(63,29,1), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fma.",    A(63,29,1), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+
+{ "fnmsub",  A(63,30,0), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fnms",    A(63,30,0), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fnmsub.", A(63,30,1), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fnms.",   A(63,30,1), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+
+{ "fnmadd",  A(63,31,0), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fnma",    A(63,31,0), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fnmadd.", A(63,31,1), A_MASK,	PPCCOM,		{ FRT,FRA,FRC,FRB } },
+{ "fnma.",   A(63,31,1), A_MASK,	PWRCOM,		{ FRT,FRA,FRC,FRB } },
+
+{ "fcmpo",   X(63,32),	X_MASK|(3<<21),	COM,		{ BF, FRA, FRB } },
+
+{ "mtfsb1",  XRC(63,38,0), XRARB_MASK,	COM,		{ BT } },
+{ "mtfsb1.", XRC(63,38,1), XRARB_MASK,	COM,		{ BT } },
+
+{ "fneg",    XRC(63,40,0), XRA_MASK,	COM,		{ FRT, FRB } },
+{ "fneg.",   XRC(63,40,1), XRA_MASK,	COM,		{ FRT, FRB } },
+
+{ "mcrfs",   X(63,64),	XRB_MASK|(3<<21)|(3<<16), COM,	{ BF, BFA } },
+
+{ "mtfsb0",  XRC(63,70,0), XRARB_MASK,	COM,		{ BT } },
+{ "mtfsb0.", XRC(63,70,1), XRARB_MASK,	COM,		{ BT } },
+
+{ "fmr",     XRC(63,72,0), XRA_MASK,	COM,		{ FRT, FRB } },
+{ "fmr.",    XRC(63,72,1), XRA_MASK,	COM,		{ FRT, FRB } },
+
+{ "mtfsfi",  XRC(63,134,0), XRA_MASK|(3<<21)|(1<<11), COM, { BF, U } },
+{ "mtfsfi.", XRC(63,134,1), XRA_MASK|(3<<21)|(1<<11), COM, { BF, U } },
+
+{ "fnabs",   XRC(63,136,0), XRA_MASK,	COM,		{ FRT, FRB } },
+{ "fnabs.",  XRC(63,136,1), XRA_MASK,	COM,		{ FRT, FRB } },
+
+{ "fabs",    XRC(63,264,0), XRA_MASK,	COM,		{ FRT, FRB } },
+{ "fabs.",   XRC(63,264,1), XRA_MASK,	COM,		{ FRT, FRB } },
+
+{ "mffs",    XRC(63,583,0), XRARB_MASK,	COM,		{ FRT } },
+{ "mffs.",   XRC(63,583,1), XRARB_MASK,	COM,		{ FRT } },
+
+{ "mtfsf",   XFL(63,711,0), XFL_MASK,	COM,		{ FLM, FRB } },
+{ "mtfsf.",  XFL(63,711,1), XFL_MASK,	COM,		{ FLM, FRB } },
+
+{ "fctid",   XRC(63,814,0), XRA_MASK,	PPC64,		{ FRT, FRB } },
+{ "fctid.",  XRC(63,814,1), XRA_MASK,	PPC64,		{ FRT, FRB } },
+
+{ "fctidz",  XRC(63,815,0), XRA_MASK,	PPC64,		{ FRT, FRB } },
+{ "fctidz.", XRC(63,815,1), XRA_MASK,	PPC64,		{ FRT, FRB } },
+
+{ "fcfid",   XRC(63,846,0), XRA_MASK,	PPC64,		{ FRT, FRB } },
+{ "fcfid.",  XRC(63,846,1), XRA_MASK,	PPC64,		{ FRT, FRB } },
+
+};
+
+const int powerpc_num_opcodes =
+  sizeof (powerpc_opcodes) / sizeof (powerpc_opcodes[0]);
+
+/* The macro table.  This is only used by the assembler.  */
+
+/* The expressions of the form (-x ! 31) & (x | 31) have the value 0
+   when x=0; 32-x when x is between 1 and 31; are negative if x is
+   negative; and are 32 or more otherwise.  This is what you want
+   when, for instance, you are emulating a right shift by a
+   rotate-left-and-mask, because the underlying instructions support
+   shifts of size 0 but not shifts of size 32.  By comparison, when
+   extracting x bits from some word you want to use just 32-x, because
+   the underlying instructions don't support extracting 0 bits but do
+   support extracting the whole word (32 bits in this case).  */
+
+const struct powerpc_macro powerpc_macros[] = {
+{ "extldi",  4,   PPC64,	"rldicr %0,%1,%3,(%2)-1" },
+{ "extldi.", 4,   PPC64,	"rldicr. %0,%1,%3,(%2)-1" },
+{ "extrdi",  4,   PPC64,	"rldicl %0,%1,(%2)+(%3),64-(%2)" },
+{ "extrdi.", 4,   PPC64,	"rldicl. %0,%1,(%2)+(%3),64-(%2)" },
+{ "insrdi",  4,   PPC64,	"rldimi %0,%1,64-((%2)+(%3)),%3" },
+{ "insrdi.", 4,   PPC64,	"rldimi. %0,%1,64-((%2)+(%3)),%3" },
+{ "rotrdi",  3,   PPC64,	"rldicl %0,%1,(-(%2)!63)&((%2)|63),0" },
+{ "rotrdi.", 3,   PPC64,	"rldicl. %0,%1,(-(%2)!63)&((%2)|63),0" },
+{ "sldi",    3,   PPC64,	"rldicr %0,%1,%2,63-(%2)" },
+{ "sldi.",   3,   PPC64,	"rldicr. %0,%1,%2,63-(%2)" },
+{ "srdi",    3,   PPC64,	"rldicl %0,%1,(-(%2)!63)&((%2)|63),%2" },
+{ "srdi.",   3,   PPC64,	"rldicl. %0,%1,(-(%2)!63)&((%2)|63),%2" },
+{ "clrrdi",  3,   PPC64,	"rldicr %0,%1,0,63-(%2)" },
+{ "clrrdi.", 3,   PPC64,	"rldicr. %0,%1,0,63-(%2)" },
+{ "clrlsldi",4,   PPC64,	"rldic %0,%1,%3,(%2)-(%3)" },
+{ "clrlsldi.",4,  PPC64,	"rldic. %0,%1,%3,(%2)-(%3)" },
+
+{ "extlwi",  4,   PPCCOM,	"rlwinm %0,%1,%3,0,(%2)-1" },
+{ "extlwi.", 4,   PPCCOM,	"rlwinm. %0,%1,%3,0,(%2)-1" },
+{ "extrwi",  4,   PPCCOM,	"rlwinm %0,%1,(%2)+(%3),32-(%2),31" },
+{ "extrwi.", 4,   PPCCOM,	"rlwinm. %0,%1,(%2)+(%3),32-(%2),31" },
+{ "inslwi",  4,   PPCCOM,	"rlwimi %0,%1,(-(%3)!31)&((%3)|31),%3,(%2)+(%3)-1" },
+{ "inslwi.", 4,   PPCCOM,	"rlwimi. %0,%1,(-(%3)!31)&((%3)|31),%3,(%2)+(%3)-1"},
+{ "insrwi",  4,   PPCCOM,	"rlwimi %0,%1,32-((%2)+(%3)),%3,(%2)+(%3)-1" },
+{ "insrwi.", 4,   PPCCOM,	"rlwimi. %0,%1,32-((%2)+(%3)),%3,(%2)+(%3)-1"},
+{ "rotrwi",  3,   PPCCOM,	"rlwinm %0,%1,(-(%2)!31)&((%2)|31),0,31" },
+{ "rotrwi.", 3,   PPCCOM,	"rlwinm. %0,%1,(-(%2)!31)&((%2)|31),0,31" },
+{ "slwi",    3,   PPCCOM,	"rlwinm %0,%1,%2,0,31-(%2)" },
+{ "sli",     3,   PWRCOM,	"rlinm %0,%1,%2,0,31-(%2)" },
+{ "slwi.",   3,   PPCCOM,	"rlwinm. %0,%1,%2,0,31-(%2)" },
+{ "sli.",    3,   PWRCOM,	"rlinm. %0,%1,%2,0,31-(%2)" },
+{ "srwi",    3,   PPCCOM,	"rlwinm %0,%1,(-(%2)!31)&((%2)|31),%2,31" },
+{ "sri",     3,   PWRCOM,	"rlinm %0,%1,(-(%2)!31)&((%2)|31),%2,31" },
+{ "srwi.",   3,   PPCCOM,	"rlwinm. %0,%1,(-(%2)!31)&((%2)|31),%2,31" },
+{ "sri.",    3,   PWRCOM,	"rlinm. %0,%1,(-(%2)!31)&((%2)|31),%2,31" },
+{ "clrrwi",  3,   PPCCOM,	"rlwinm %0,%1,0,0,31-(%2)" },
+{ "clrrwi.", 3,   PPCCOM,	"rlwinm. %0,%1,0,0,31-(%2)" },
+{ "clrlslwi",4,   PPCCOM,	"rlwinm %0,%1,%3,(%2)-(%3),31-(%3)" },
+{ "clrlslwi.",4,  PPCCOM,	"rlwinm. %0,%1,%3,(%2)-(%3),31-(%3)" },
+
+};
+
+const int powerpc_num_macros =
+  sizeof (powerpc_macros) / sizeof (powerpc_macros[0]);
diff -purN linux-2.5/arch/ppc64/kdb/ppc.h linuxppc64-2.5/arch/ppc64/kdb/ppc.h
--- linux-2.5/arch/ppc64/kdb/ppc.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/ppc.h	2003-10-13 16:16:57.000000000 +0000
@@ -0,0 +1,259 @@
+/* ppc.h -- Header file for PowerPC opcode table
+   Copyright 1994, 1995 Free Software Foundation, Inc.
+   Written by Ian Lance Taylor, Cygnus Support
+
+This file is part of GDB, GAS, and the GNU binutils.
+
+GDB, GAS, and the GNU binutils are free software; you can redistribute
+them and/or modify them under the terms of the GNU General Public
+License as published by the Free Software Foundation; either version
+1, or (at your option) any later version.
+
+GDB, GAS, and the GNU binutils are distributed in the hope that they
+will be useful, but WITHOUT ANY WARRANTY; without even the implied
+warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See
+the GNU General Public License for more details.
+
+You should have received a copy of the GNU General Public License
+along with this file; see the file COPYING.  If not, write to the Free
+Software Foundation, 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  */
+
+#ifndef PPC_H
+#define PPC_H
+
+/* The opcode table is an array of struct powerpc_opcode.  */
+
+struct powerpc_opcode
+{
+  /* The opcode name.  */
+  const char *name;
+
+  /* The opcode itself.  Those bits which will be filled in with
+     operands are zeroes.  */
+  unsigned long opcode;
+
+  /* The opcode mask.  This is used by the disassembler.  This is a
+     mask containing ones indicating those bits which must match the
+     opcode field, and zeroes indicating those bits which need not
+     match (and are presumably filled in by operands).  */
+  unsigned long mask;
+
+  /* One bit flags for the opcode.  These are used to indicate which
+     specific processors support the instructions.  The defined values
+     are listed below.  */
+  unsigned long flags;
+
+  /* An array of operand codes.  Each code is an index into the
+     operand table.  They appear in the order which the operands must
+     appear in assembly code, and are terminated by a zero.  */
+  unsigned char operands[8];
+};
+
+/* The table itself is sorted by major opcode number, and is otherwise
+   in the order in which the disassembler should consider
+   instructions.  */
+extern const struct powerpc_opcode powerpc_opcodes[];
+extern const int powerpc_num_opcodes;
+
+/* Values defined for the flags field of a struct powerpc_opcode.  */
+
+/* Opcode is defined for the PowerPC architecture.  */
+#define PPC_OPCODE_PPC (01)
+
+/* Opcode is defined for the POWER (RS/6000) architecture.  */
+#define PPC_OPCODE_POWER (02)
+
+/* Opcode is defined for the POWER2 (Rios 2) architecture.  */
+#define PPC_OPCODE_POWER2 (04)
+
+/* Opcode is only defined on 32 bit architectures.  */
+#define PPC_OPCODE_32 (010)
+
+/* Opcode is only defined on 64 bit architectures.  */
+#define PPC_OPCODE_64 (020)
+
+/* Opcode is supported by the Motorola PowerPC 601 processor.  The 601
+   is assumed to support all PowerPC (PPC_OPCODE_PPC) instructions,
+   but it also supports many additional POWER instructions.  */
+#define PPC_OPCODE_601 (040)
+
+/* Opcode is supported in both the Power and PowerPC architectures
+   (ie, compiler's -mcpu=common or assembler's -mcom).  */
+#define PPC_OPCODE_COMMON (0100)
+
+/* Opcode is supported for any Power or PowerPC platform (this is
+   for the assembler's -many option, and it eliminates duplicates).  */
+#define PPC_OPCODE_ANY (0200)
+
+/* Opcode is supported as part of the 64-bit bridge.  */
+#define PPC_OPCODE_64_BRIDGE (0400)
+
+/* Opcode is supported by Altivec Vector Unit */
+#define PPC_OPCODE_ALTIVEC   (01000)
+
+/* A macro to extract the major opcode from an instruction.  */
+#define PPC_OP(i) (((i) >> 26) & 0x3f)
+
+/* The operands table is an array of struct powerpc_operand.  */
+
+struct powerpc_operand
+{
+  /* The number of bits in the operand.  */
+  int bits;
+
+  /* How far the operand is left shifted in the instruction.  */
+  int shift;
+
+  /* Insertion function.  This is used by the assembler.  To insert an
+     operand value into an instruction, check this field.
+
+     If it is NULL, execute
+         i |= (op & ((1 << o->bits) - 1)) << o->shift;
+     (i is the instruction which we are filling in, o is a pointer to
+     this structure, and op is the opcode value; this assumes twos
+     complement arithmetic).
+
+     If this field is not NULL, then simply call it with the
+     instruction and the operand value.  It will return the new value
+     of the instruction.  If the ERRMSG argument is not NULL, then if
+     the operand value is illegal, *ERRMSG will be set to a warning
+     string (the operand will be inserted in any case).  If the
+     operand value is legal, *ERRMSG will be unchanged (most operands
+     can accept any value).  */
+  unsigned long (*insert) PARAMS ((unsigned long instruction, long op,
+				   const char **errmsg));
+
+  /* Extraction function.  This is used by the disassembler.  To
+     extract this operand type from an instruction, check this field.
+
+     If it is NULL, compute
+         op = ((i) >> o->shift) & ((1 << o->bits) - 1);
+	 if ((o->flags & PPC_OPERAND_SIGNED) != 0
+	     && (op & (1 << (o->bits - 1))) != 0)
+	   op -= 1 << o->bits;
+     (i is the instruction, o is a pointer to this structure, and op
+     is the result; this assumes twos complement arithmetic).
+
+     If this field is not NULL, then simply call it with the
+     instruction value.  It will return the value of the operand.  If
+     the INVALID argument is not NULL, *INVALID will be set to
+     non-zero if this operand type can not actually be extracted from
+     this operand (i.e., the instruction does not match).  If the
+     operand is valid, *INVALID will not be changed.  */
+  long (*extract) PARAMS ((unsigned long instruction, int *invalid));
+
+  /* One bit syntax flags.  */
+  unsigned long flags;
+};
+
+/* Elements in the table are retrieved by indexing with values from
+   the operands field of the powerpc_opcodes table.  */
+
+extern const struct powerpc_operand powerpc_operands[];
+
+/* Values defined for the flags field of a struct powerpc_operand.  */
+
+/* This operand takes signed values.  */
+#define PPC_OPERAND_SIGNED (01)
+
+/* This operand takes signed values, but also accepts a full positive
+   range of values when running in 32 bit mode.  That is, if bits is
+   16, it takes any value from -0x8000 to 0xffff.  In 64 bit mode,
+   this flag is ignored.  */
+#define PPC_OPERAND_SIGNOPT (02)
+
+/* This operand does not actually exist in the assembler input.  This
+   is used to support extended mnemonics such as mr, for which two
+   operands fields are identical.  The assembler should call the
+   insert function with any op value.  The disassembler should call
+   the extract function, ignore the return value, and check the value
+   placed in the valid argument.  */
+#define PPC_OPERAND_FAKE (04)
+
+/* The next operand should be wrapped in parentheses rather than
+   separated from this one by a comma.  This is used for the load and
+   store instructions which want their operands to look like
+       reg,displacement(reg)
+   */
+#define PPC_OPERAND_PARENS (010)
+
+/* This operand may use the symbolic names for the CR fields, which
+   are
+       lt  0	gt  1	eq  2	so  3	un  3
+       cr0 0	cr1 1	cr2 2	cr3 3
+       cr4 4	cr5 5	cr6 6	cr7 7
+   These may be combined arithmetically, as in cr2*4+gt.  These are
+   only supported on the PowerPC, not the POWER.  */
+#define PPC_OPERAND_CR (020)
+
+/* This operand names a register.  The disassembler uses this to print
+   register names with a leading 'r'.  */
+#define PPC_OPERAND_GPR (040)
+
+/* This operand names a floating point register.  The disassembler
+   prints these with a leading 'f'.  */
+#define PPC_OPERAND_FPR (0100)
+
+/* This operand is a relative branch displacement.  The disassembler
+   prints these symbolically if possible.  */
+#define PPC_OPERAND_RELATIVE (0200)
+
+/* This operand is an absolute branch address.  The disassembler
+   prints these symbolically if possible.  */
+#define PPC_OPERAND_ABSOLUTE (0400)
+
+/* This operand is optional, and is zero if omitted.  This is used for
+   the optional BF and L fields in the comparison instructions.  The
+   assembler must count the number of operands remaining on the line,
+   and the number of operands remaining for the opcode, and decide
+   whether this operand is present or not.  The disassembler should
+   print this operand out only if it is not zero.  */
+#define PPC_OPERAND_OPTIONAL (01000)
+
+/* This flag is only used with PPC_OPERAND_OPTIONAL.  If this operand
+   is omitted, then for the next operand use this operand value plus
+   1, ignoring the next operand field for the opcode.  This wretched
+   hack is needed because the Power rotate instructions can take
+   either 4 or 5 operands.  The disassembler should print this operand
+   out regardless of the PPC_OPERAND_OPTIONAL field.  */
+#define PPC_OPERAND_NEXT (02000)
+
+/* This operand should be regarded as a negative number for the
+   purposes of overflow checking (i.e., the normal most negative
+   number is disallowed and one more than the normal most positive
+   number is allowed).  This flag will only be set for a signed
+   operand.  */
+#define PPC_OPERAND_NEGATIVE (04000)
+
+/* This operand names a vector unit register.  The disassembler
+   prints these with a leading 'v'.  */
+#define PPC_OPERAND_VR (010000)
+
+
+/* The POWER and PowerPC assemblers use a few macros.  We keep them
+   with the operands table for simplicity.  The macro table is an
+   array of struct powerpc_macro.  */
+
+struct powerpc_macro
+{
+  /* The macro name.  */
+  const char *name;
+
+  /* The number of operands the macro takes.  */
+  unsigned int operands;
+
+  /* One bit flags for the opcode.  These are used to indicate which
+     specific processors support the instructions.  The values are the
+     same as those for the struct powerpc_opcode flags field.  */
+  unsigned long flags;
+
+  /* A format string to turn the macro into a normal instruction.
+     Each %N in the string is replaced with operand number N (zero
+     based).  */
+  const char *format;
+};
+
+extern const struct powerpc_macro powerpc_macros[];
+extern const int powerpc_num_macros;
+
+#endif /* PPC_H */
diff -purN linux-2.5/arch/ppc64/kdb/privinst.h linuxppc64-2.5/arch/ppc64/kdb/privinst.h
--- linux-2.5/arch/ppc64/kdb/privinst.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/privinst.h	2003-10-13 16:16:57.000000000 +0000
@@ -0,0 +1,95 @@
+/*
+ * Copyright (C) 1996 Paul Mackerras.
+ *
+ *      This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+#include <linux/config.h>
+
+#define GETREG(reg)		\
+    static inline unsigned long get_ ## reg (void)	\
+	{ unsigned long ret; asm volatile ("mf" #reg " %0" : "=r" (ret) :); return ret; }
+
+#define SETREG(reg)		\
+    static inline void set_ ## reg (unsigned long val)	\
+	{ asm volatile ("mt" #reg " %0" : : "r" (val)); }
+
+GETREG(msr)
+SETREG(msr)
+SETREG(msrd)
+GETREG(cr)
+
+#define GSETSPR(n, name)	\
+    static inline long get_ ## name (void) \
+	{ long ret; asm volatile ("mfspr %0," #n : "=r" (ret) : ); return ret; } \
+    static inline void set_ ## name (long val) \
+	{ asm volatile ("mtspr " #n ",%0" : : "r" (val)); }
+
+GSETSPR(0, mq)
+GSETSPR(1, xer)
+GSETSPR(4, rtcu)
+GSETSPR(5, rtcl)
+GSETSPR(8, lr)
+GSETSPR(9, ctr)
+GSETSPR(18, dsisr)
+GSETSPR(19, dar)
+GSETSPR(22, dec)
+GSETSPR(25, sdr1)
+GSETSPR(26, srr0)
+GSETSPR(27, srr1)
+GSETSPR(272, sprg0)
+GSETSPR(273, sprg1)
+GSETSPR(274, sprg2)
+GSETSPR(275, sprg3)
+GSETSPR(282, ear)
+GSETSPR(287, pvr)
+GSETSPR(528, bat0u)
+GSETSPR(529, bat0l)
+GSETSPR(530, bat1u)
+GSETSPR(531, bat1l)
+GSETSPR(532, bat2u)
+GSETSPR(533, bat2l)
+GSETSPR(534, bat3u)
+GSETSPR(535, bat3l)
+GSETSPR(1008, hid0)
+GSETSPR(1009, hid1)
+GSETSPR(1010, iabr)
+GSETSPR(1013, dabr)
+GSETSPR(1023, pir)
+
+static inline int get_sr(int n)
+{
+    int ret;
+
+#if 0
+// DRENG does not assemble 
+    asm (" mfsrin %0,%1" : "=r" (ret) : "r" (n << 28));
+#endif
+    return ret;
+}
+
+static inline void set_sr(int n, int val)
+{
+#if 0
+// DRENG does not assemble 
+    asm ("mtsrin %0,%1" : : "r" (val), "r" (n << 28));
+#endif
+}
+
+static inline void store_inst(void *p)
+{
+    asm volatile ("dcbst 0,%0; sync; icbi 0,%0; isync" : : "r" (p));
+}
+
+static inline void cflush(void *p)
+{
+    asm volatile ("dcbf 0,%0; icbi 0,%0" : : "r" (p));
+}
+
+static inline void cinval(void *p)
+{
+    asm volatile ("dcbi 0,%0; icbi 0,%0" : : "r" (p));
+}
+
diff -purN linux-2.5/arch/ppc64/kdb/symcat.h linuxppc64-2.5/arch/ppc64/kdb/symcat.h
--- linux-2.5/arch/ppc64/kdb/symcat.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kdb/symcat.h	2003-10-13 16:16:57.000000000 +0000
@@ -0,0 +1,49 @@
+/* Symbol concatenation utilities.
+
+   Copyright (C) 1998, 2000 Free Software Foundation, Inc.
+
+   This program is free software; you can redistribute it and/or modify
+   it under the terms of the GNU General Public License as published by
+   the Free Software Foundation; either version 2 of the License, or
+   (at your option) any later version.
+
+   This program is distributed in the hope that it will be useful,
+   but WITHOUT ANY WARRANTY; without even the implied warranty of
+   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+   GNU General Public License for more details.
+ 
+   You should have received a copy of the GNU General Public License along
+   with this program; if not, write to the Free Software Foundation, Inc.,
+   59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.  */
+
+#ifndef SYM_CAT_H
+#define SYM_CAT_H
+
+#if defined (__STDC__) || defined (ALMOST_STDC) || defined (HAVE_STRINGIZE)
+#define CONCAT2(a,b)	 a##b
+#define CONCAT3(a,b,c)	 a##b##c
+#define CONCAT4(a,b,c,d) a##b##c##d
+#define STRINGX(s) #s
+#else
+/* Note one should never pass extra whitespace to the CONCATn macros,
+   e.g. CONCAT2(foo, bar) because traditonal C will keep the space between
+   the two labels instead of concatenating them.  Instead, make sure to
+   write CONCAT2(foo,bar).  */
+#define CONCAT2(a,b)	 a/**/b
+#define CONCAT3(a,b,c)	 a/**/b/**/c
+#define CONCAT4(a,b,c,d) a/**/b/**/c/**/d
+#define STRINGX(s) "s"
+#endif
+
+#define XCONCAT2(a,b)     CONCAT2(a,b)
+#define XCONCAT3(a,b,c)   CONCAT3(a,b,c)
+#define XCONCAT4(a,b,c,d) CONCAT4(a,b,c,d)
+
+/* Note the layer of indirection here is typically used to allow
+   stringification of the expansion of macros.  I.e. "#define foo
+   bar", "XSTRING(foo)", to yield "bar".  Be aware that this only
+   works for __STDC__, not for traditional C which will still resolve
+   to "foo".  */
+#define XSTRING(s) STRINGX(s) 
+
+#endif /* SYM_CAT_H */
diff -purN linux-2.5/arch/ppc64/kernel/HvCall.c linuxppc64-2.5/arch/ppc64/kernel/HvCall.c
--- linux-2.5/arch/ppc64/kernel/HvCall.c	2002-02-14 12:14:35.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/HvCall.c	2003-11-25 05:52:18.000000000 +0000
@@ -8,108 +8,29 @@
  * (at your option) any later version.
  */
 
-#include <linux/stddef.h>
-#include <linux/kernel.h>
-#include <linux/mm.h>
-#include <linux/slab.h>
-#include <asm/system.h>
 #include <asm/page.h>
+#include <asm/abs_addr.h>
 #include <asm/iSeries/HvCall.h>
-#ifndef  _HVCALLSC_H
 #include <asm/iSeries/HvCallSc.h>
-#endif
-#include <asm/iSeries/LparData.h>
-
-#ifndef  _HVTYPES_H
 #include <asm/iSeries/HvTypes.h>
-#endif
 
 
-/*=====================================================================
- * Note that this call takes at MOST one page worth of data
- */
-int HvCall_readLogBuffer(HvLpIndex lpIndex, void *buffer, u64 bufLen)
+void HvCall_writeLogBuffer(const void *buffer, u64 len)
 {
-	struct HvLpBufferList *bufList;
-	u64 bytesLeft = bufLen;
-	u64 leftThisPage;
-	u64 curPtr = virt_to_absolute( (unsigned long) buffer );
-	u64 retVal;
-	int npages;
-	int i;
-
-	npages = 0;
-	while (bytesLeft) {
-		npages++;
-		leftThisPage = ((curPtr & PAGE_MASK) + PAGE_SIZE) - curPtr;
-
-		if (leftThisPage > bytesLeft)
-			bytesLeft = 0;
-		else
-			bytesLeft -= leftThisPage;
-
-		curPtr = (curPtr & PAGE_MASK) + PAGE_SIZE;
+	struct HvLpBufferList hv_buf;
+	u64 left_this_page;
+	u64 cur = virt_to_absolute((unsigned long)buffer);
+
+	while (len) {
+		hv_buf.addr = cur;
+		left_this_page = ((cur & PAGE_MASK) + PAGE_SIZE) - cur;
+		if (left_this_page > len)
+			left_this_page = len;
+		hv_buf.len = left_this_page;
+		len -= left_this_page;
+		HvCall2(HvCallBaseWriteLogBuffer,
+				virt_to_absolute((unsigned long)&hv_buf),
+				left_this_page);
+		cur = (cur & PAGE_MASK) + PAGE_SIZE;
 	}
-
-	if (npages == 0)
-		return 0;
-
-	bufList = (struct HvLpBufferList *)
-		kmalloc(npages * sizeof(struct HvLpBufferList), GFP_ATOMIC);
-	bytesLeft = bufLen;
-	curPtr = virt_to_absolute( (unsigned long) buffer );
-	for(i=0; i<npages; i++) {
-		bufList[i].addr = curPtr;
-      
-		leftThisPage = ((curPtr & PAGE_MASK) + PAGE_SIZE) - curPtr;
-
-		if (leftThisPage > bytesLeft) {
-			bufList[i].len = bytesLeft;
-			bytesLeft = 0;
-		} else {
-			bufList[i].len = leftThisPage;
-			bytesLeft -= leftThisPage;
-		}
-
-		curPtr = (curPtr & PAGE_MASK) + PAGE_SIZE;
-	}
-
-
-	retVal = HvCall3(HvCallBaseReadLogBuffer, lpIndex,
-			 virt_to_absolute((unsigned long)bufList), bufLen);
-
-	kfree(bufList);
-
-	return (int)retVal;
-}
-
-/*=====================================================================
- */
-void HvCall_writeLogBuffer(const void *buffer, u64 bufLen)
-{
-	struct HvLpBufferList bufList;
-	u64 bytesLeft = bufLen;
-	u64 leftThisPage;
-	u64 curPtr = virt_to_absolute( (unsigned long) buffer );
-
-	while (bytesLeft) {
-		bufList.addr = curPtr;
-      
-		leftThisPage = ((curPtr & PAGE_MASK) + PAGE_SIZE) - curPtr;
-
-		if (leftThisPage > bytesLeft) {
-			bufList.len = bytesLeft;
-			bytesLeft = 0;
-		} else {
-			bufList.len = leftThisPage;
-			bytesLeft -= leftThisPage;
-		}
-
-		curPtr = (curPtr & PAGE_MASK) + PAGE_SIZE;
-	}
-
-
-	HvCall2(HvCallBaseWriteLogBuffer,
-		virt_to_absolute((unsigned long)&bufList), bufLen);
-
 }
diff -purN linux-2.5/arch/ppc64/kernel/ItLpQueue.c linuxppc64-2.5/arch/ppc64/kernel/ItLpQueue.c
--- linux-2.5/arch/ppc64/kernel/ItLpQueue.c	2002-04-12 07:45:30.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/ItLpQueue.c	2003-11-25 05:52:18.000000000 +0000
@@ -147,17 +147,14 @@ unsigned ItLpQueue_process( struct ItLpQ
 				printk(KERN_INFO "Unexpected Lp Event type=%d\n", nextLpEvent->xType );
 			
 			ItLpQueue_clearValid( nextLpEvent );
-		}
-		else 	/* No more valid events
-			 * If overflow events are pending
-			 * process them
+		} else if ( lpQueue->xPlicOverflowIntPending )
+			/*
+			 * No more valid events. If overflow events are
+			 * pending process them
 			 */
-			if ( lpQueue->xPlicOverflowIntPending ) {
-				HvCallEvent_getOverflowLpEvents( 
-						lpQueue->xIndex);
-			}
-			else	/* If nothing left then we are done */
-				break;
+			HvCallEvent_getOverflowLpEvents( lpQueue->xIndex);
+		else
+			break;
 	}
 
 	ItLpQueueInProcess = 0;
diff -purN linux-2.5/arch/ppc64/kernel/LparData.c linuxppc64-2.5/arch/ppc64/kernel/LparData.c
--- linux-2.5/arch/ppc64/kernel/LparData.c	2003-03-26 04:30:58.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/LparData.c	2003-10-09 04:34:18.000000000 +0000
@@ -71,7 +71,7 @@ struct HvReleaseData hvReleaseData = {
 	6,		/* TEMP: This allows non-GA driver */
 	4,		/* We are v5r2m0               */
 	3,		/* Min supported PLIC = v5r1m0 */
-	3,		/* Min usuable PLIC   = v5r1m0 */
+	3,		/* Min usable PLIC   = v5r1m0 */
 	{ 0xd3, 0x89, 0x95, 0xa4, /* "Linux 2.4   "*/
 	  0xa7, 0x40, 0xf2, 0x4b,
 	  0xf4, 0x4b, 0xf6, 0xf4 },
@@ -175,7 +175,7 @@ struct ItVpdAreas itVpdAreas = {
 	0, 0,
 	26,		/* # VPD array entries */
 	10,		/* # DMA array entries */
-	MAX_PROCESSORS*2, maxPhysicalProcessors,	/* Max logical, physical procs */
+	NR_CPUS*2, maxPhysicalProcessors,	/* Max logical, physical procs */
 	offsetof(struct ItVpdAreas,xPlicDmaToks),/* offset to DMA toks */
 	offsetof(struct ItVpdAreas,xSlicVpdAdrs),/* offset to VPD addrs */
 	offsetof(struct ItVpdAreas,xPlicDmaLens),/* offset to DMA lens */
diff -purN linux-2.5/arch/ppc64/kernel/Makefile linuxppc64-2.5/arch/ppc64/kernel/Makefile
--- linux-2.5/arch/ppc64/kernel/Makefile	2003-08-05 03:55:20.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/Makefile	2003-12-17 05:08:23.000000000 +0000
@@ -7,24 +7,29 @@ extra-y		:= head.o vmlinux.lds.s
 
 obj-y               :=	setup.o entry.o traps.o irq.o idle.o \
 			time.o process.o signal.o syscalls.o misc.o ptrace.o \
-			align.o semaphore.o bitops.o stab.o htab.o pacaData.o \
+			align.o semaphore.o bitops.o stab.o pacaData.o \
 			udbg.o binfmt_elf32.o sys_ppc32.o ioctl32.o \
 			ptrace32.o signal32.o pmc.o rtc.o init_task.o \
-			lmb.o pci.o pci_dn.o pci_dma.o cputable.o
+			lmb.o cputable.o
 
-obj-$(CONFIG_PPC_ISERIES) += iSeries_pci.o       iSeries_pci_reset.o \
-			     iSeries_IoMmTable.o iSeries_irq.o \
-			     iSeries_VpdInfo.o   XmPciLpEvent.o \
+obj-$(CONFIG_PCI)	+= pci.o pci_dn.o pci_dma.o
+
+ifdef CONFIG_PPC_ISERIES
+obj-$(CONFIG_PCI)	+= iSeries_pci.o iSeries_pci_reset.o \
+			     iSeries_IoMmTable.o 
+endif
+
+obj-$(CONFIG_PPC_ISERIES) += iSeries_irq.o \
+			     iSeries_VpdInfo.o XmPciLpEvent.o \
 			     HvCall.o HvLpConfig.o LparData.o mf_proc.o \
 			     iSeries_setup.o ItLpQueue.o hvCall.o \
-			     mf.o HvLpEvent.o iSeries_proc.o 
+			     mf.o HvLpEvent.o iSeries_proc.o iSeries_htab.o \
+			     proc_pmc.o
 
 obj-$(CONFIG_PPC_PSERIES) += pSeries_pci.o pSeries_lpar.o pSeries_hvCall.o \
-			     eeh.o rtasd.o nvram.o ras.o
-
-# Change this to pSeries only once we've got iSeries up to date
-obj-y			  += open_pic.o xics.o pSeries_htab.o rtas.o \
-			     chrp_setup.o i8259.o prom.o 
+			     eeh.o nvram.o rtasd.o ras.o \
+			     open_pic.o xics.o pSeries_htab.o rtas.o \
+			     chrp_setup.o i8259.o prom.o vio.o
 
 obj-$(CONFIG_PROC_FS)		+= proc_ppc64.o
 obj-$(CONFIG_RTAS_FLASH)	+= rtas_flash.o
@@ -32,5 +37,8 @@ obj-$(CONFIG_SMP)		+= smp.o
 obj-$(CONFIG_MODULES)		+= module.o ppc_ksyms.o
 obj-$(CONFIG_PPC_RTAS)		+= rtas-proc.o
 obj-$(CONFIG_SCANLOG)		+= scanlog.o
+obj-$(CONFIG_VIOPATH)		+= viopath.o
+obj-$(CONFIG_LPARCFG)		+= lparcfg.o
+
 
 CFLAGS_ioctl32.o += -Ifs/
diff -purN linux-2.5/arch/ppc64/kernel/XmPciLpEvent.c linuxppc64-2.5/arch/ppc64/kernel/XmPciLpEvent.c
--- linux-2.5/arch/ppc64/kernel/XmPciLpEvent.c	2003-05-02 17:21:13.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/XmPciLpEvent.c	2003-11-25 05:52:18.000000000 +0000
@@ -1,10 +1,10 @@
 /*
-   * File XmPciLpEvent.h created by Wayne Holm on Mon Jan 15 2001.
-   *
-   * This module handles PCI interrupt events sent by the iSeries Hypervisor.
+ * File XmPciLpEvent.h created by Wayne Holm on Mon Jan 15 2001.
+ *
+ * This module handles PCI interrupt events sent by the iSeries Hypervisor.
 */
 
-
+#include <linux/config.h>
 #include <linux/pci.h>
 #include <linux/init.h>
 #include <linux/threads.h>
@@ -20,13 +20,13 @@
 #include <asm/iSeries/XmPciLpEvent.h>
 #include <asm/ppcdebug.h>
 
-long Pci_Interrupt_Count = 0;
-long Pci_Event_Count     = 0;
+static long Pci_Interrupt_Count;
+static long Pci_Event_Count;
 
 enum XmPciLpEvent_Subtype {
 	XmPciLpEvent_BusCreated	   = 0,		// PHB has been created
 	XmPciLpEvent_BusError	   = 1,		// PHB has failed
-	XmPciLpEvent_BusFailed	   = 2,		// Msg to Seconday, Primary failed bus
+	XmPciLpEvent_BusFailed	   = 2,		// Msg to Secondary, Primary failed bus
 	XmPciLpEvent_NodeFailed	   = 4,		// Multi-adapter bridge has failed
 	XmPciLpEvent_NodeRecovered = 5,		// Multi-adapter bridge has recovered
 	XmPciLpEvent_BusRecovered  = 12,	// PHB has been recovered
@@ -36,14 +36,14 @@ enum XmPciLpEvent_Subtype {
 };
 
 struct XmPciLpEvent_BusInterrupt {
-	HvBusNumber		busNumber;
+	HvBusNumber	busNumber;
 	HvSubBusNumber	subBusNumber;
 };
 
 struct XmPciLpEvent_NodeInterrupt {
-	HvBusNumber		busNumber;
+	HvBusNumber	busNumber;
 	HvSubBusNumber	subBusNumber;
-	HvAgentId		deviceId;
+	HvAgentId	deviceId;
 };
 
 struct XmPciLpEvent {
@@ -53,10 +53,10 @@ struct XmPciLpEvent {
 		u64 alignData;			// Align on an 8-byte boundary
 
 		struct {
-			u32			fisr;
+			u32		fisr;
 			HvBusNumber	busNumber;
 			HvSubBusNumber	subBusNumber;
-			HvAgentId		deviceId;
+			HvAgentId	deviceId;
 		} slotInterrupt;
 
 		struct XmPciLpEvent_BusInterrupt busFailed;
@@ -70,40 +70,52 @@ struct XmPciLpEvent {
 
 };
 
-static void intReceived(struct XmPciLpEvent* eventParm, struct pt_regs* regsParm);
+static void intReceived(struct XmPciLpEvent *eventParm,
+		struct pt_regs *regsParm);
 
-static void XmPciLpEvent_handler( struct HvLpEvent* eventParm, struct pt_regs* regsParm)
+static void XmPciLpEvent_handler(struct HvLpEvent *eventParm,
+		struct pt_regs *regsParm)
 {
-	//PPCDBG(PPCDBG_BUSWALK,"XmPciLpEvent_handler, type 0x%x\n",eventParm->xType );
+#ifdef CONFIG_PCI
+#if 0
+	PPCDBG(PPCDBG_BUSWALK, "XmPciLpEvent_handler, type 0x%x\n",
+			eventParm->xType);
+#endif
 	++Pci_Event_Count;
 
-	if (eventParm && eventParm->xType == HvLpEvent_Type_PciIo) {
-		switch( eventParm->xFlags.xFunction ) {
+	if (eventParm && (eventParm->xType == HvLpEvent_Type_PciIo)) {
+		switch (eventParm->xFlags.xFunction) {
 		case HvLpEvent_Function_Int:
-			intReceived( (struct XmPciLpEvent*)eventParm, regsParm );
+			intReceived((struct XmPciLpEvent *)eventParm, regsParm);
 			break;
 		case HvLpEvent_Function_Ack:
-			printk(KERN_ERR "XmPciLpEvent.c: unexpected ack received\n");
+			printk(KERN_ERR
+				"XmPciLpEvent.c: unexpected ack received\n");
 			break;
 		default:
-			printk(KERN_ERR "XmPciLpEvent.c: unexpected event function %d\n",(int)eventParm->xFlags.xFunction);
+			printk(KERN_ERR
+				"XmPciLpEvent.c: unexpected event function %d\n",
+				(int)eventParm->xFlags.xFunction);
 			break;
 		}
-	}
-	else if (event) {
-		printk(KERN_ERR "XmPciLpEvent.c: Unrecognized PCI event type 0x%x\n",(int)eventParm->xType);
-	}
-	else {
+	} else if (eventParm)
+		printk(KERN_ERR
+			"XmPciLpEvent.c: Unrecognized PCI event type 0x%x\n",
+			(int)eventParm->xType);
+	else
 		printk(KERN_ERR "XmPciLpEvent.c: NULL event received\n");
-	}
+#endif
 }
 
-static void intReceived(struct XmPciLpEvent* eventParm, struct pt_regs* regsParm)
+static void intReceived(struct XmPciLpEvent *eventParm,
+		struct pt_regs *regsParm)
 {
 	int irq;
 
 	++Pci_Interrupt_Count;
-	//PPCDBG(PPCDBG_BUSWALK,"PCI: XmPciLpEvent.c: intReceived\n");
+#if 0
+	PPCDBG(PPCDBG_BUSWALK, "PCI: XmPciLpEvent.c: intReceived\n");
+#endif
 
 	switch (eventParm->hvLpEvent.xSubtype) {
 	case XmPciLpEvent_SlotInterrupt:
@@ -111,33 +123,45 @@ static void intReceived(struct XmPciLpEv
 		/* Dispatch the interrupt handlers for this irq */
 		ppc_irq_dispatch_handler(regsParm, irq);
 		HvCallPci_eoi(eventParm->eventData.slotInterrupt.busNumber,
-			      eventParm->eventData.slotInterrupt.subBusNumber,
-			      eventParm->eventData.slotInterrupt.deviceId);
+			eventParm->eventData.slotInterrupt.subBusNumber,
+			eventParm->eventData.slotInterrupt.deviceId);
 		break;
 		/* Ignore error recovery events for now */
 	case XmPciLpEvent_BusCreated:
-		printk(KERN_INFO "XmPciLpEvent.c: system bus %d created\n", eventParm->eventData.busCreated.busNumber);
+		printk(KERN_INFO "XmPciLpEvent.c: system bus %d created\n",
+			eventParm->eventData.busCreated.busNumber);
 		break;
 	case XmPciLpEvent_BusError:
 	case XmPciLpEvent_BusFailed:
-		printk(KERN_INFO "XmPciLpEvent.c: system bus %d failed\n", eventParm->eventData.busFailed.busNumber);
+		printk(KERN_INFO "XmPciLpEvent.c: system bus %d failed\n",
+			eventParm->eventData.busFailed.busNumber);
 		break;
 	case XmPciLpEvent_BusRecovered:
 	case XmPciLpEvent_UnQuiesceBus:
-		printk(KERN_INFO "XmPciLpEvent.c: system bus %d recovered\n", eventParm->eventData.busRecovered.busNumber);
+		printk(KERN_INFO "XmPciLpEvent.c: system bus %d recovered\n",
+			eventParm->eventData.busRecovered.busNumber);
 		break;
 	case XmPciLpEvent_NodeFailed:
 	case XmPciLpEvent_BridgeError:
-		printk(KERN_INFO "XmPciLpEvent.c: multi-adapter bridge %d/%d/%d failed\n", eventParm->eventData.nodeFailed.busNumber, eventParm->eventData.nodeFailed.subBusNumber, eventParm->eventData.nodeFailed.deviceId);
+		printk(KERN_INFO
+			"XmPciLpEvent.c: multi-adapter bridge %d/%d/%d failed\n",
+			eventParm->eventData.nodeFailed.busNumber,
+			eventParm->eventData.nodeFailed.subBusNumber,
+			eventParm->eventData.nodeFailed.deviceId);
 		break;
 	case XmPciLpEvent_NodeRecovered:
-		printk(KERN_INFO "XmPciLpEvent.c: multi-adapter bridge %d/%d/%d recovered\n", eventParm->eventData.nodeRecovered.busNumber, eventParm->eventData.nodeRecovered.subBusNumber, eventParm->eventData.nodeRecovered.deviceId);
+		printk(KERN_INFO
+			"XmPciLpEvent.c: multi-adapter bridge %d/%d/%d recovered\n",
+			eventParm->eventData.nodeRecovered.busNumber,
+			eventParm->eventData.nodeRecovered.subBusNumber,
+			eventParm->eventData.nodeRecovered.deviceId);
 		break;
 	default:
-		printk(KERN_ERR "XmPciLpEvent.c: unrecognized event subtype 0x%x\n",
-		       eventParm->hvLpEvent.xSubtype);
+		printk(KERN_ERR
+			"XmPciLpEvent.c: unrecognized event subtype 0x%x\n",
+			eventParm->hvLpEvent.xSubtype);
 		break;
-	};
+	}
 }
 
 
@@ -145,18 +169,22 @@ static void intReceived(struct XmPciLpEv
 int XmPciLpEvent_init()
 {
 	int xRc;
-	PPCDBG(PPCDBG_BUSWALK,"XmPciLpEvent_init, Register Event type 0x%04X\n",HvLpEvent_Type_PciIo);
 
-	xRc = HvLpEvent_registerHandler(HvLpEvent_Type_PciIo, &XmPciLpEvent_handler);
+	PPCDBG(PPCDBG_BUSWALK,
+			"XmPciLpEvent_init, Register Event type 0x%04X\n",
+			HvLpEvent_Type_PciIo);
+
+	xRc = HvLpEvent_registerHandler(HvLpEvent_Type_PciIo,
+			&XmPciLpEvent_handler);
 	if (xRc == 0) {
 		xRc = HvLpEvent_openPath(HvLpEvent_Type_PciIo, 0);
-		if (xRc != 0) {
-			printk(KERN_ERR "XmPciLpEvent.c: open event path failed with rc 0x%x\n", xRc);
-		}
-	}
-	else {
-		printk(KERN_ERR "XmPciLpEvent.c: register handler failed with rc 0x%x\n", xRc);
-    	}
-    return xRc;
+		if (xRc != 0)
+			printk(KERN_ERR
+				"XmPciLpEvent.c: open event path failed with rc 0x%x\n",
+				xRc);
+	} else
+		printk(KERN_ERR
+			"XmPciLpEvent.c: register handler failed with rc 0x%x\n",
+			xRc);
+	return xRc;
 }
-
diff -purN linux-2.5/arch/ppc64/kernel/asm-offsets.c linuxppc64-2.5/arch/ppc64/kernel/asm-offsets.c
--- linux-2.5/arch/ppc64/kernel/asm-offsets.c	2003-09-01 23:22:05.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/asm-offsets.c	2003-12-17 04:27:52.000000000 +0000
@@ -56,6 +56,12 @@ int main(void)
 	DEFINE(THREAD_FPSCR, offsetof(struct thread_struct, fpscr));
 	DEFINE(KSP, offsetof(struct thread_struct, ksp));
 
+#ifdef CONFIG_ALTIVEC
+	DEFINE(THREAD_VR0, offsetof(struct thread_struct, vr[0]));
+	DEFINE(THREAD_VRSAVE, offsetof(struct thread_struct, vrsave));
+	DEFINE(THREAD_VSCR, offsetof(struct thread_struct, vscr));
+	DEFINE(THREAD_USED_VR, offsetof(struct thread_struct, used_vr));
+#endif /* CONFIG_ALTIVEC */
 	DEFINE(MM, offsetof(struct task_struct, mm));
 
 	/* naca */
diff -purN linux-2.5/arch/ppc64/kernel/chrp_setup.c linuxppc64-2.5/arch/ppc64/kernel/chrp_setup.c
--- linux-2.5/arch/ppc64/kernel/chrp_setup.c	2003-09-02 06:46:41.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/chrp_setup.c	2003-11-21 06:45:02.000000000 +0000
@@ -57,6 +57,7 @@
 #include <asm/irq.h>
 #include <asm/naca.h>
 #include <asm/time.h>
+#include <asm/nvram.h>
 
 #include "i8259.h"
 #include "open_pic.h"
@@ -64,11 +65,10 @@
 #include <asm/ppcdebug.h>
 #include <asm/cputable.h>
 
-extern volatile unsigned char *chrp_int_ack_special;
-
 void chrp_progress(char *, unsigned short);
 
 extern void openpic_init_IRQ(void);
+extern void openpic_init_irq_desc(irq_desc_t *);
 
 extern void find_and_init_phbs(void);
 
@@ -96,16 +96,19 @@ chrp_get_cpuinfo(struct seq_file *m)
 
 	seq_printf(m, "timebase\t: %lu\n", ppc_tb_freq);
 
-	root = find_path_device("/");
+	root = of_find_node_by_path("/");
 	if (root)
 		model = get_property(root, "model", NULL);
 	seq_printf(m, "machine\t\t: CHRP %s\n", model);
+	of_node_put(root);
 }
 
 #define I8042_DATA_REG 0x60
 
-void __init chrp_request_regions(void) 
+void __init chrp_request_regions(void)
 {
+	struct device_node *i8042;
+
 	request_region(0x20,0x20,"pic1");
 	request_region(0xa0,0x20,"pic2");
 	request_region(0x00,0x20,"dma1");
@@ -118,8 +121,9 @@ void __init chrp_request_regions(void) 
 	 * tree and reserve the region if it does not appear. Later on
 	 * the i8042 code will try and reserve this region and fail.
 	 */
-	if (!find_type_devices("8042"))
+	if (!(i8042 = of_find_node_by_type(NULL, "8042")))
 		request_region(I8042_DATA_REG, 16, "reserved (no i8042)");
+	of_node_put(i8042);
 }
 
 void __init
@@ -158,7 +162,7 @@ chrp_setup_arch(void)
 #endif
 
 	/* Find the Open PIC if present */
-	root = find_path_device("/");
+	root = of_find_node_by_path("/");
 	opprop = (unsigned int *) get_property(root,
 				"platform-open-pic", NULL);
 	if (opprop != 0) {
@@ -170,6 +174,7 @@ chrp_setup_arch(void)
 		printk(KERN_DEBUG "OpenPIC addr: %lx\n", openpic);
 		OpenPIC_Addr = __ioremap(openpic, 0x40000, _PAGE_NO_CACHE);
 	}
+	of_node_put(root);
 
 #ifdef CONFIG_DUMMY_CONSOLE
 	conswitchp = &dummy_con;
@@ -229,6 +234,10 @@ void __init
 chrp_init(unsigned long r3, unsigned long r4, unsigned long r5,
 	  unsigned long r6, unsigned long r7)
 {
+	struct device_node * dn;
+	char * hypertas;
+	unsigned int len;
+
 #if 0 /* PPPBBB remove this later... -Peter */
 #ifdef CONFIG_BLK_DEV_INITRD
 	/* take care of initrd if we have one */
@@ -244,10 +253,12 @@ chrp_init(unsigned long r3, unsigned lon
 	ppc_md.setup_residual = NULL;
 	ppc_md.get_cpuinfo    = chrp_get_cpuinfo;
 	if(naca->interrupt_controller == IC_OPEN_PIC) {
-		ppc_md.init_IRQ       = openpic_init_IRQ; 
+		ppc_md.init_IRQ       = openpic_init_IRQ;
+		ppc_md.init_irq_desc  = openpic_init_irq_desc;
 		ppc_md.get_irq        = openpic_get_irq;
 	} else {
 		ppc_md.init_IRQ       = xics_init_IRQ;
+		ppc_md.init_irq_desc  = xics_init_irq_desc;
 		ppc_md.get_irq        = xics_get_irq;
 	}
 
@@ -262,36 +273,40 @@ chrp_init(unsigned long r3, unsigned lon
 	ppc_md.set_rtc_time   = pSeries_set_rtc_time;
 	ppc_md.calibrate_decr = pSeries_calibrate_decr;
 
-	ppc_md.progress = chrp_progress;
+	ppc_md.progress       = chrp_progress;
+
+	ppc_md.nvram_read     = pSeries_nvram_read;
+	ppc_md.nvram_write    = pSeries_nvram_write;
 
-        /* build up the firmware_features bitmask field
+        /* Build up the firmware_features bitmask field
          * using contents of device-tree/ibm,hypertas-functions.
          * Ultimately this functionality may be moved into prom.c prom_init().
          */
-	struct device_node * dn;
-	char * hypertas;
-	unsigned int len;
-	dn = find_path_device("/rtas");
+	dn = of_find_node_by_path("/rtas");
 	cur_cpu_spec->firmware_features = 0;
 	hypertas = get_property(dn, "ibm,hypertas-functions", &len);
 	if (hypertas) {
-	    while (len > 0){
-		int i;
-		/* check value against table of strings */
-		for(i=0; i < FIRMWARE_MAX_FEATURES ;i++) {
-		    if ((firmware_features_table[i].name) && (strcmp(firmware_features_table[i].name,hypertas))==0) {
-			/* we have a match */
-			cur_cpu_spec->firmware_features |= (1UL << firmware_features_table[i].val);
-			break;
-		    } 
+		while (len > 0){
+			int i, hypertas_len;
+			/* check value against table of strings */
+			for(i=0; i < FIRMWARE_MAX_FEATURES ;i++) {
+				if ((firmware_features_table[i].name) &&
+				    (strcmp(firmware_features_table[i].name,hypertas))==0) {
+					/* we have a match */
+					cur_cpu_spec->firmware_features |= 
+						(firmware_features_table[i].val);
+					break;
+				} 
+			}
+			hypertas_len = strlen(hypertas);
+			len -= hypertas_len +1;
+			hypertas+= hypertas_len +1;
 		}
-		int hypertas_len = strlen(hypertas);
-		len -= hypertas_len +1;
-		hypertas+= hypertas_len +1;
-	    }
 	}
-	udbg_printf("firmware_features bitmask: 0x%x \n",
-		    cur_cpu_spec->firmware_features);
+
+	of_node_put(dn);
+	printk(KERN_INFO "firmware_features = 0x%lx\n", 
+	       cur_cpu_spec->firmware_features);
 }
 
 void
@@ -319,6 +334,13 @@ chrp_progress(char *s, unsigned short he
 		display_character = rtas_token("display-character");
 		set_indicator = rtas_token("set-indicator");
 	}
+	if (display_character == RTAS_UNKNOWN_SERVICE) {
+		/* use hex display */
+		if (set_indicator == RTAS_UNKNOWN_SERVICE)
+			return;
+		rtas_call(set_indicator, 3, 1, NULL, 6, 0, hex);
+		return;
+	}
 
 	if(display_character == RTAS_UNKNOWN_SERVICE) {
 		/* use hex display if available */
@@ -405,11 +427,11 @@ void __init pSeries_calibrate_decr(void)
 
 	/*
 	 * The cpu node should have a timebase-frequency property
-	 * to tell us the rate at which the decrementer counts. 
+	 * to tell us the rate at which the decrementer counts.
 	 */
 	freq = 16666000;        /* hardcoded default */
-	cpu = find_type_devices("cpu");
-	if (cpu != 0) { 
+	cpu = of_find_node_by_type(NULL, "cpu");
+	if (cpu != 0) {
 		fp = (int *) get_property(cpu, "timebase-frequency", NULL);
 		if (fp != 0)
 			freq = *fp;
@@ -422,11 +444,12 @@ void __init pSeries_calibrate_decr(void)
 			processor_freq = *fp;
 	}
 	ppc_proc_freq = processor_freq;
-	
-        printk("time_init: decrementer frequency = %lu.%.6lu MHz\n", 
-	       freq/1000000, freq%1000000 );
+	of_node_put(cpu);
+
+	printk("time_init: decrementer frequency = %lu.%.6lu MHz\n",
+	       freq/1000000, freq%1000000);
 	printk("time_init: processor frequency   = %lu.%.6lu MHz\n",
-		processor_freq/1000000, processor_freq%1000000 );
+	       processor_freq/1000000, processor_freq%1000000);
 
 	tb_ticks_per_jiffy = freq / HZ;
 	tb_ticks_per_sec = tb_ticks_per_jiffy * HZ;
diff -purN linux-2.5/arch/ppc64/kernel/cputable.c linuxppc64-2.5/arch/ppc64/kernel/cputable.c
--- linux-2.5/arch/ppc64/kernel/cputable.c	2003-06-24 04:58:49.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/cputable.c	2003-12-17 04:27:52.000000000 +0000
@@ -21,6 +21,13 @@
 
 struct cpu_spec* cur_cpu_spec = NULL;
 
+/* NOTE:
+ * Unlike ppc32, ppc64 will only call this once for the boot CPU, it's
+ * the responsibility of the appropriate CPU save/restore functions to
+ * eventually copy these settings over. Those save/restore aren't yet
+ * part of the cputable though. That has to be fixed for both ppc32
+ * and ppc64
+ */
 extern void __setup_cpu_power3(unsigned long offset, struct cpu_spec* spec);
 extern void __setup_cpu_power4(unsigned long offset, struct cpu_spec* spec);
 
@@ -30,8 +37,10 @@ extern void __setup_cpu_power4(unsigned 
  */
 #ifdef CONFIG_ALTIVEC
 #define CPU_FTR_ALTIVEC_COMP	CPU_FTR_ALTIVEC
+#define PPC_FEATURE_HAS_ALTIVEC_COMP PPC_FEATURE_HAS_ALTIVEC
 #else
 #define CPU_FTR_ALTIVEC_COMP	0
+#define PPC_FEATURE_HAS_ALTIVEC_COMP    0
 #endif
 
 struct cpu_spec	cpu_specs[] = {
@@ -107,6 +116,24 @@ struct cpu_spec	cpu_specs[] = {
 	    __setup_cpu_power4,
 	    COMMON_PPC64_FW
     },
+    {	/* PPC970 */
+	    0xffff0000, 0x00390000, "PPC970",
+	    CPU_FTR_SPLIT_ID_CACHE | CPU_FTR_USE_TB | CPU_FTR_HPTE_TABLE |
+	    CPU_FTR_PPCAS_ARCH_V2 | CPU_FTR_ALTIVEC_COMP,
+	    COMMON_USER_PPC64 | PPC_FEATURE_HAS_ALTIVEC_COMP,
+	    128, 128,
+	    __setup_cpu_power4,
+	    COMMON_PPC64_FW
+    },
+    {	/* Power5 */
+	    0xffff0000, 0x003a0000, "Power5",
+	    CPU_FTR_SPLIT_ID_CACHE | CPU_FTR_USE_TB | CPU_FTR_HPTE_TABLE |
+	    CPU_FTR_PPCAS_ARCH_V2,
+	    COMMON_USER_PPC64,
+	    128, 128,
+	    __setup_cpu_power4,
+	    COMMON_PPC64_FW
+    },
     {	/* default match */
 	    0x00000000, 0x00000000, "(Power4-Compatible)",
   	    CPU_FTR_SPLIT_ID_CACHE | CPU_FTR_USE_TB | CPU_FTR_HPTE_TABLE |
@@ -130,4 +157,13 @@ firmware_feature_t firmware_features_tab
     {FW_FEATURE_DUMP,		"hcall-dump"},
     {FW_FEATURE_INTERRUPT,	"hcall-interrupt"},
     {FW_FEATURE_MIGRATE,	"hcall-migrate"},
+    {FW_FEATURE_PERFMON,	"hcall-perfmon"},
+    {FW_FEATURE_CRQ,    	"hcall-crq"},
+    {FW_FEATURE_VIO,	        "hcall-vio"},
+    {FW_FEATURE_RDMA,	        "hcall-rdma"},
+    {FW_FEATURE_LLAN,	        "hcall-lLAN"},
+    {FW_FEATURE_BULK,   	"hcall-bulk"},
+    {FW_FEATURE_XDABR,  	"hcall-xdabr"},
+    {FW_FEATURE_MULTITCE,	"hcall-multi-tce"},
+    {FW_FEATURE_SPLPAR,	        "hcall-splpar"},
 };
diff -purN linux-2.5/arch/ppc64/kernel/eeh.c linuxppc64-2.5/arch/ppc64/kernel/eeh.c
--- linux-2.5/arch/ppc64/kernel/eeh.c	2003-08-17 19:52:50.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/eeh.c	2003-09-25 07:00:39.000000000 +0000
@@ -257,7 +257,7 @@ void eeh_init(void)
 
 	/* Enable EEH for all adapters.  Note that eeh requires buid's */
 	info.adapters_enabled = 0;
-	for (phb = find_devices("pci"); phb; phb = phb->next) {
+	for (phb = of_find_node_by_name(NULL, "pci"); phb; phb = of_find_node_by_name(phb, "pci")) {
 		int len;
 		int *buid_vals = (int *) get_property(phb, "ibm,fw-phb-id", &len);
 		if (!buid_vals)
diff -purN linux-2.5/arch/ppc64/kernel/entry.S linuxppc64-2.5/arch/ppc64/kernel/entry.S
--- linux-2.5/arch/ppc64/kernel/entry.S	2003-06-26 00:01:14.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/entry.S	2003-12-17 04:27:52.000000000 +0000
@@ -29,6 +29,7 @@
 #include <asm/thread_info.h>
 #include <asm/ppc_asm.h>
 #include <asm/offsets.h>
+#include <asm/cputable.h>
 
 #ifdef CONFIG_PPC_ISERIES
 #define DO_SOFT_DISABLE
@@ -211,6 +212,15 @@ _GLOBAL(ret_from_syscall_2)
 	.align	2,0
 #endif
 
+	
+_GLOBAL(ppc32_swapcontext)
+	bl	.sys32_swapcontext
+	b	80f
+	
+_GLOBAL(ppc64_swapcontext)
+	bl	.sys_swapcontext
+	b	80f
+
 _GLOBAL(ppc32_sigreturn)
 	bl	.sys32_sigreturn
 	b	80f
@@ -261,10 +271,17 @@ _GLOBAL(_switch)
 	SAVE_10GPRS(22, r1)
 	mflr	r20		/* Return to switch caller */
 	mfmsr	r22
-	andi.	r21, r22, MSR_FP
+	li	r0, MSR_FP
+#ifdef CONFIG_ALTIVEC
+BEGIN_FTR_SECTION
+	oris	r0,r0,MSR_VEC@h	/* Disable altivec */
+	mfspr	r24,SPRN_VRSAVE	/* save vrsave register value */
+	std	r24,THREAD_VRSAVE(r3)
+END_FTR_SECTION_IFSET(CPU_FTR_ALTIVEC)
+#endif /* CONFIG_ALTIVEC */
+	and.	r0,r0,r22
 	beq+	1f
-	li	r6,MSR_FP	/* Disable floating-point */
-	andc	r22,r22,r6
+	andc	r22,r22,r0
 	mtmsrd	r22
 	isync
 1:	std	r20,_NIP(r1)
@@ -275,22 +292,30 @@ _GLOBAL(_switch)
 	addi	r6,r4,-THREAD	/* Convert THREAD to 'current' */
 	std	r6,PACACURRENT(r13)	/* Set new 'current' */
 
-#ifdef CONFIG_PPC_ISERIES
-#error fixme
-	ld	r7,TI_FLAGS(r4)	/* Get run light flag */
-	mfspr	r9,CTRLF
-	srdi	r7,r7,1		/* Align to run light bit in CTRL reg */
-	insrdi	r9,r7,1,63	/* Insert run light into CTRL */
-	mtspr	CTRLT,r9
-#endif
-
 	ld	r1,KSP(r4)	/* Load new stack pointer */
 	ld	r6,_CCR(r1)
 	mtcrf	0xFF,r6
+
+#ifdef CONFIG_ALTIVEC
+BEGIN_FTR_SECTION
+	ld	r0,THREAD_VRSAVE(r4)
+	mtspr	SPRN_VRSAVE,r0		/* if G4, restore VRSAVE reg */
+END_FTR_SECTION_IFSET(CPU_FTR_ALTIVEC)
+#endif /* CONFIG_ALTIVEC */
+
 	/* r3-r13 are destroyed -- Cort */
 	REST_8GPRS(14, r1)
 	REST_10GPRS(22, r1)
 
+#ifdef CONFIG_PPC_ISERIES
+	clrrdi	r7,r1,THREAD_SHIFT	/* get current_thread_info() */
+	ld	r7,TI_FLAGS(r7)		/* Get run light flag */
+	mfspr	r9,CTRLF
+	srdi	r7,r7,TIF_RUN_LIGHT
+	insrdi	r9,r7,1,63		/* Insert run light into CTRL */
+	mtspr	CTRLT,r9
+#endif
+
 	/* convert old thread to its task_struct for return value */
 	addi	r3,r3,-THREAD
 	ld	r7,_NIP(r1)	/* Return to _switch caller in new task */
@@ -308,39 +333,16 @@ _GLOBAL(ret_from_fork)
 	b	.ret_from_except
 
 _GLOBAL(ret_from_except)
-#ifdef CONFIG_PPC_ISERIES
-	ld	r5,SOFTE(r1)
-	cmpdi	0,r5,0
-	beq	4f
-irq_recheck:
-	/* Check for pending interrupts (iSeries) */
-	CHECKANYINT(r3,r4)
-	beq+	4f	/* skip do_IRQ if no interrupts */
-
-#warning FIX ISERIES
-	mfspr	r5,SPRG3
-	li	r3,0
-	stb	r3,PACAPROCENABLED(r5)	/* ensure we are disabled */
-	addi	r3,r1,STACK_FRAME_OVERHEAD
-	bl	.do_IRQ
-	b	irq_recheck	/* loop back and handle more */
-4:
-#endif
 	/*
 	 * Disable interrupts so that current_thread_info()->flags
 	 * can't change between when we test it and when we return
 	 * from the interrupt.
 	 */
-recheck:
 	mfmsr	r10		/* Get current interrupt state */
 	li	r4,0
 	ori	r4,r4,MSR_EE
-	andc	r10,r10,r4	/* clear MSR_EE */
-	mtmsrd	r10,1		/* Update machine state */
-
-#ifdef CONFIG_PPC_ISERIES
-#error fix iSeries soft disable
-#endif
+	andc	r9,r10,r4	/* clear MSR_EE */
+	mtmsrd	r9,1		/* Update machine state */
 
 	ld	r3,_MSR(r1)	/* Returning to user mode? */
 	andi.	r3,r3,MSR_PR
@@ -364,6 +366,28 @@ recheck:
 	REST_GPR(13,r1)
 
 restore:
+#ifdef CONFIG_PPC_ISERIES
+	ld	r5,SOFTE(r1)
+	mfspr	r4,SPRG3		/* get paca address */
+	cmpdi	0,r5,0
+	beq	4f
+	/* Check for pending interrupts (iSeries) */
+	/* this is CHECKANYINT except that we already have the paca address */
+	ld	r3,PACALPPACA+LPPACAANYINT(r4)
+	cmpdi	r3,0
+	beq+	4f			/* skip do_IRQ if no interrupts */
+
+	mfspr	r13,SPRG3		/* get paca pointer back */
+	li	r3,0
+	stb	r3,PACAPROCENABLED(r13)	/* ensure we are soft-disabled */
+	mtmsrd	r10			/* hard-enable again */
+	addi	r3,r1,STACK_FRAME_OVERHEAD
+	bl	.do_IRQ
+	b	.ret_from_except		/* loop back and handle more */
+
+4:	stb	r5,PACAPROCENABLED(r4)
+#endif
+
 	ld	r3,_CTR(r1)
 	ld	r0,_LINK(r1)
 	mtctr	r3
@@ -377,12 +401,6 @@ restore:
 
 	stdcx.	r0,0,r1		/* to clear the reservation */
 
-#ifdef DO_SOFT_DISABLE
-	/* XXX do this in do_work, r13 isnt valid here */
-	ld	r0,SOFTE(r1)
-	stb	r0,PACAPROCENABLED(r13)
-#endif
-
 	mfmsr	r0
 	li	r2, MSR_RI
 	andc	r0,r0,r2
@@ -407,21 +425,21 @@ restore:
 /* Note: this must change if we start using the  TIF_NOTIFY_RESUME bit */
 do_work:
 	/* Enable interrupts */
-	ori	r10,r10,MSR_EE
 	mtmsrd	r10,1
 
 	andi.	r0,r3,_TIF_NEED_RESCHED
 	beq	1f
 	bl	.schedule
-	b	recheck
+	b	.ret_from_except
 
 1:	andi.	r0,r3,_TIF_SIGPENDING
-	beq	recheck
+	beq	.ret_from_except
 	li	r3,0
 	addi	r4,r1,STACK_FRAME_OVERHEAD
 	bl	.do_signal
-	b	recheck
+	b	.ret_from_except
 
+#ifdef CONFIG_PPC_PSERIES
 /*
  * On CHRP, the Run-Time Abstraction Services (RTAS) have to be
  * called with the MMU off.
@@ -498,6 +516,12 @@ _STATIC(rtas_return_loc)
 	mfspr	r4,SPRG3	        /* Get PACA */
 	SET_REG_TO_CONST(r5, KERNELBASE)
         sub     r4,r4,r5                /* RELOC the PACA base pointer */
+
+	mfmsr   r6
+	li	r0,MSR_RI
+	andc	r6,r6,r0
+	sync	
+	mtmsrd  r6
         
         ld	r1,PACAR1(r4)           /* Restore our SP */
 	LOADADDR(r3,.rtas_restore_regs)
@@ -626,3 +650,4 @@ _GLOBAL(enter_prom)
 
 	mtlr    r0
         blr				/* return to caller */
+#endif	/* defined(CONFIG_PPC_PSERIES) */
diff -purN linux-2.5/arch/ppc64/kernel/head.S linuxppc64-2.5/arch/ppc64/kernel/head.S
--- linux-2.5/arch/ppc64/kernel/head.S	2003-10-01 22:41:11.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/head.S	2003-12-17 04:27:52.000000000 +0000
@@ -52,7 +52,7 @@
 /*
  * hcall interface to pSeries LPAR
  */
-#define HSC .long 0x44000022
+#define HVSC .long 0x44000022
 #define H_SET_ASR		0x30
 
 /*
@@ -91,20 +91,26 @@
 	.text
 	.globl  _stext
 _stext:
+#ifdef CONFIG_PPC_PSERIES
 _STATIC(__start)
 	b .__start_initialization_pSeries
+#endif
 #ifdef CONFIG_PPC_ISERIES
-	/* At offset 0x20, there is a pointer to iSeries LPAR data.
-	 * This is required by the hypervisor */
+	/*
+	 * At offset 0x20, there is a pointer to iSeries LPAR data.
+	 * This is required by the hypervisor
+	 */
 	. = 0x20
 	.llong hvReleaseData-KERNELBASE
 
-	/* At offset 0x28 and 0x30 are offsets to the msChunks
+	/*
+	 * At offset 0x28 and 0x30 are offsets to the msChunks
 	 * array (used by the iSeries LPAR debugger to do translation
 	 * between physical addresses and absolute addresses) and
-	 * to the pidhash table (also used by the debugger) */
+	 * to the pidhash table (also used by the debugger)
+	 */
 	.llong msChunks-KERNELBASE
-	.llong pidhash-KERNELBASE
+	.llong 0 /* pidhash-KERNELBASE SFRXXX */
 
 	/* Offset 0x38 - Pointer to start of embedded System.map */
 	.globl	embedded_sysmap_start
@@ -114,7 +120,7 @@ embedded_sysmap_start:
 	.globl	embedded_sysmap_end
 embedded_sysmap_end:
 	.llong	0
-#endif
+#else
 
 	/* Secondary processors spin on this value until it goes to 1. */
 	.globl  __secondary_hold_spinloop
@@ -147,6 +153,7 @@ _GLOBAL(__secondary_hold)
 	/* Relocation is off & we are located at an address less */
 	/* than 0x100, so only need to grab low order offset.    */
 	std     r24,__secondary_hold_acknowledge@l(0)
+	sync
 
 	/* All secondary cpu's wait here until told to start. */
 100:    ld      r4,__secondary_hold_spinloop@l(0)
@@ -163,6 +170,7 @@ _GLOBAL(__secondary_hold)
 	BUG_OPCODE
 #endif
 #endif
+#endif
 
 /*
  * The following macros define the code that appears as
@@ -244,6 +252,14 @@ _GLOBAL(__secondary_hold)
 	std	r22,EX_SRR0(r21);	    /* save SRR0 in exc. frame     */ \
 	ld      r23,LPPACA+LPPACASRR1(r20); /* Get SRR1 from ItLpPaca      */ \
 	std	r23,EX_SRR1(r21);	    /* save SRR1 in exc. frame     */ \
+                                                                         \
+	mfspr   r23,DAR;                /* Save DAR in exc. frame      */ \
+	std	r23,EX_DAR(r21);	                                  \
+	mfspr	r23,DSISR;		/* Save DSISR in exc. frame    */ \
+	stw	r23,EX_DSISR(r21);	                                  \
+	mfspr	r23,SPRG2;		/* Save r20 in exc. frame      */ \
+	std	r23,EX_R20(r21);	                                  \
+                                                                         \
 	mfcr    r23;                        /* save CR in r23              */
 
 /*
@@ -375,9 +391,34 @@ __start_interrupts:
 	STD_EXCEPTION_PSERIES( 0xc00, SystemCall )
 	STD_EXCEPTION_PSERIES( 0xd00, SingleStep )
 	STD_EXCEPTION_PSERIES( 0xe00, Trap_0e )
-	STD_EXCEPTION_PSERIES( 0xf00, PerformanceMonitor )
+
+	/* We need to deal with the Altivec unavailable exception
+	 * here which is at 0xf20, thus in the middle of the
+	 * prolog code of the PerformanceMonitor one. A little
+	 * trickery is thus necessary
+	 */
+	. = 0xf00
+	b	.PerformanceMonitor_Pseries
+	. = 0xf20
+	b	.AltivecUnavailable_Pseries
+
 	STD_EXCEPTION_PSERIES( 0x1300, InstructionBreakpoint )
+	STD_EXCEPTION_PSERIES( 0x1700, AltivecAssist )
 
+	/* Here are the "moved" performance monitor and
+	 * altivec unavailable exceptions
+	 */
+	. = 0x3000
+	.globl PerformanceMonitor_Pseries;
+.PerformanceMonitor_Pseries:
+	EXCEPTION_PROLOG_PSERIES(0xf00, PerformanceMonitor_common)
+	
+	. = 0x3100
+	.globl AltivecUnavailable_Pseries;
+.AltivecUnavailable_Pseries:
+	EXCEPTION_PROLOG_PSERIES(0xf20, AltivecUnavailable_common)
+	
+		
 	/* Space for the naca.  Architected to be located at real address
 	 * NACA_PHYS_ADDR.  Various tools rely on this location being fixed.
 	 * The first dword of the naca is required by iSeries LPAR to
@@ -564,7 +605,11 @@ __end_stab:
 	STD_EXCEPTION_COMMON( 0xe00, Trap_0e, .UnknownException )
 	STD_EXCEPTION_COMMON( 0xf00, PerformanceMonitor, .PerformanceMonitorException )
 	STD_EXCEPTION_COMMON(0x1300, InstructionBreakpoint, .InstructionBreakpointException )
-
+#ifdef CONFIG_ALTIVEC
+	STD_EXCEPTION_COMMON(0x1700, AltivecAssist, .AltivecAssistException )
+#else
+	STD_EXCEPTION_COMMON(0x1700, AltivecAssist, .UnknownException )
+#endif
 /*
  * Return from an exception which is handled without calling
  * save_remaining_regs.  The caller is assumed to have done
@@ -739,6 +784,23 @@ FPUnavailable_common:
 	bl      .KernelFPUnavailableException
 	BUG_OPCODE
 
+	.globl AltivecUnavailable_common
+AltivecUnavailable_common:
+	EXCEPTION_PROLOG_COMMON
+#ifdef CONFIG_ALTIVEC
+	bne	.load_up_altivec		/* if from user, just load it up */
+#endif
+	addi	r3,r1,STACK_FRAME_OVERHEAD
+	DO_COPY_EE()
+	li	r6,0xf20
+	bl      .save_remaining_regs
+#ifdef CONFIG_ALTIVEC
+	bl	.KernelAltivecUnavailableException
+#else
+	bl      .UnknownException
+#endif
+	BUG_OPCODE
+		
 	.globl SystemCall_common
 SystemCall_common:
 	EXCEPTION_PROLOG_COMMON
@@ -1113,7 +1175,6 @@ _GLOBAL(save_remaining_regs)
 	SET_REG_TO_CONST(r22, MSR_KERNEL)
 
 #ifdef DO_SOFT_DISABLE
-#warning FIX ISERIES
 	stb	r20,PACAPROCENABLED(r13) /* possibly soft enable */
 	ori	r22,r22,MSR_EE		/* always hard enable */
 #else
@@ -1219,6 +1280,7 @@ _GLOBAL(__start_initialization_iSeries)
 	b	.start_here_common
 #endif
 
+#ifdef CONFIG_PPC_PSERIES
 _GLOBAL(__start_initialization_pSeries)
 	mr	r31,r3			/* save parameters */
 	mr	r30,r4
@@ -1328,6 +1390,7 @@ _STATIC(__after_prom_start)
 	sub	r5,r5,r27
 	bl	.copy_and_flush		/* copy the rest */
 	b	.start_here_pSeries
+#endif
 
 /*
  * Copy routine used to copy the kernel to start at physical address 0
@@ -1466,6 +1529,126 @@ _GLOBAL(giveup_fpu)
 #endif /* CONFIG_SMP */
 	blr
 
+
+#ifdef CONFIG_ALTIVEC
+		
+/*
+ * load_up_altivec(unused, unused, tsk)
+ * Disable VMX for the task which had it previously,
+ * and save its vector registers in its thread_struct.
+ * Enables the VMX for use in the kernel on return.
+ * On SMP we know the VMX is free, since we give it up every
+ * switch (ie, no lazy save of the vector registers).
+ * On entry: r13 == 'current' && last_task_used_altivec != 'current'
+ */
+_STATIC(load_up_altivec)
+	mfmsr	r5                      /* grab the current MSR */
+	oris	r5,r5,MSR_VEC@h
+	mtmsrd  r5			/* enable use of VMX now */
+	isync
+	
+/*
+ * For SMP, we don't do lazy VMX switching because it just gets too
+ * horrendously complex, especially when a task switches from one CPU
+ * to another.  Instead we call giveup_altvec in switch_to.
+ * VRSAVE isn't dealt with here, that is done in the normal context
+ * switch code. Note that we could rely on vrsave value to eventually
+ * avoid saving all of the VREGs here...
+ */
+#ifndef CONFIG_SMP
+	LOADBASE(r3,last_task_used_altivec)
+	ld	r4,last_task_used_altivec@l(r3)
+	cmpi	0,r4,0
+	beq	1f
+	/* Save VMX state to last_task_used_altivec's THREAD struct */
+	addi	r4,r4,THREAD
+	SAVE_32VRS(0,r5,r4)
+	mfvscr	vr0
+	li	r10,THREAD_VSCR
+	stvx	vr0,r10,r4
+	/* Disable VMX for last_task_used_altivec */
+	ld	r5,PT_REGS(r4)
+	ld	r4,_MSR-STACK_FRAME_OVERHEAD(r5)
+	lis	r20,MSR_VEC@h
+	andc	r4,r4,r20
+	std	r4,_MSR-STACK_FRAME_OVERHEAD(r5)
+1:
+#endif /* CONFIG_SMP */
+	/* Hack: if we get an altivec unavailable trap with VRSAVE
+	 * set to all zeros, we assume this is a broken application
+	 * that fails to set it properly, and thus we switch it to
+	 * all 1's
+	 */
+	mfspr	r4,SPRN_VRSAVE
+	cmpi	0,r4,0
+	bne+	1f
+	li	r4,-1
+	mtspr	SPRN_VRSAVE,r4
+1:
+	/* enable use of VMX after return */
+	ld	r4,PACACURRENT(r13)
+	addi	r5,r4,THREAD		/* Get THREAD */
+	oris	r23,r23,MSR_VEC@h
+	li	r4,1
+	li	r10,THREAD_VSCR
+	stw	r4,THREAD_USED_VR(r5)
+	lvx	vr0,r10,r5
+	REST_32VRS(0,r4,r5)
+#ifndef CONFIG_SMP
+	/* Update last_task_used_math to 'current' */
+	subi	r4,r5,THREAD		/* Back to 'current' */
+	std	r4,last_task_used_altivec@l(r3)
+#endif /* CONFIG_SMP */
+	/* restore registers and return */
+	b	fast_exception_return
+
+/*
+ * disable_kernel_altivec()
+ * Disable the VMX.
+ */
+_GLOBAL(disable_kernel_altivec)
+	mfmsr   r3
+	rldicl  r0,r3,(63-MSR_VEC_LG),1
+	rldicl  r3,r0,(MSR_VEC_LG+1),0
+	mtmsrd  r3			/* disable use of VMX now */
+	isync
+	blr
+
+/*
+ * giveup_altivec(tsk)
+ * Disable VMX for the task given as the argument,
+ * and save the vector registers in its thread_struct.
+ * Enables the VMX for use in the kernel on return.
+ */
+_GLOBAL(giveup_altivec)
+	mfmsr	r5
+	oris	r5,r5,MSR_VEC@h
+	mtmsrd	r5			/* enable use of VMX now */
+	isync
+	cmpi	0,r3,0
+	beqlr-				/* if no previous owner, done */
+	addi	r3,r3,THREAD		/* want THREAD of task */
+	ld	r5,PT_REGS(r3)
+	cmpi	0,r5,0
+	SAVE_32VRS(0,r4,r3)
+	mfvscr	vr0
+	li	r4,THREAD_VSCR
+	stvx	vr0,r4,r3
+	beq	1f
+	ld	r4,_MSR-STACK_FRAME_OVERHEAD(r5)
+	lis	r3,MSR_VEC@h
+	andc	r4,r4,r3		/* disable FP for previous task */
+	std	r4,_MSR-STACK_FRAME_OVERHEAD(r5)
+1:
+#ifndef CONFIG_SMP
+	li	r5,0
+	LOADBASE(r4,last_task_used_altivec)
+	std	r5,last_task_used_altivec@l(r4)
+#endif /* CONFIG_SMP */
+	blr
+
+#endif /* CONFIG_ALTIVEC */
+
 #ifdef CONFIG_SMP
 /*
  * This function is called after the master CPU has released the
@@ -1535,7 +1718,7 @@ _GLOBAL(__secondary_start)
 	cmpwi	r3,0x34         /* Pulsar */
 	bne	98f
 97:	li	r3,H_SET_ASR    /* hcall = H_SET_ASR */
-	HSC     		/* Invoking hcall */
+	HVSC     		/* Invoking hcall */
 	b	99f
 98:                             /* !(rpa hypervisor) || !(star)  */
 	mtasr	r4	        /* set the stab location         */
@@ -1594,6 +1777,7 @@ _GLOBAL(enable_32b_mode)
 	isync
 	blr
 
+#ifdef CONFIG_PPC_PSERIES
 /*
  * This is where the main kernel code starts.
  */
@@ -1703,7 +1887,7 @@ _STATIC(start_here_pSeries)
 	cmpwi	r3,0x34         /* Pulsar */
 	bne	98f
 97:	li	r3,H_SET_ASR    /* hcall = H_SET_ASR */
-	HSC     	        /* Invoking hcall */
+	HVSC     	        /* Invoking hcall */
 	b     	99f
 98:                             /* !(rpa hypervisor) || !(star) */
 	mtasr	r4	        /* set the stab location         */
@@ -1729,6 +1913,7 @@ _STATIC(start_here_pSeries)
 	mtspr	SRR0,r3
 	mtspr	SRR1,r4
 	rfid
+#endif	/* CONFIG_PPC_PSERIES */
 
 	/* This is where all platforms converge execution */
 _STATIC(start_here_common)
@@ -1765,6 +1950,12 @@ _STATIC(start_here_common)
 	addi    r2,r2,0x4000
 	addi    r2,r2,0x4000
 
+	/* Apply the CPUs-specific fixups (nop out sections not relevant
+	 * to this CPU
+	 */
+	li	r3,0
+	bl	.do_cpu_ftr_fixups
+
 	/* setup the systemcfg pointer */
 	LOADADDR(r9,systemcfg)
 	SET_REG_TO_CONST(r8, SYSTEMCFG_VIRT_ADDR)
@@ -1803,10 +1994,8 @@ _STATIC(start_here_common)
 	/* Load up the kernel context */
 5:
 #ifdef DO_SOFT_DISABLE
-#warning FIX ISERIES
-	mfspr	r4,SPRG3
 	li	r5,0
-	stb	r5,PACAPROCENABLED(r4)	/* Soft Disabled */
+	stb	r5,PACAPROCENABLED(r13)	/* Soft Disabled */
 	mfmsr	r5
 	ori	r5,r5,MSR_EE		/* Hard Enabled */
 	mtmsrd	r5
diff -purN linux-2.5/arch/ppc64/kernel/htab.c linuxppc64-2.5/arch/ppc64/kernel/htab.c
--- linux-2.5/arch/ppc64/kernel/htab.c	2003-09-07 01:24:09.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/htab.c	1970-01-01 00:00:00.000000000 +0000
@@ -1,491 +0,0 @@
-/*
- * PowerPC64 port by Mike Corrigan and Dave Engebretsen
- *   {mikejc|engebret}@us.ibm.com
- *
- *    Copyright (c) 2000 Mike Corrigan <mikejc@us.ibm.com>
- *
- * SMP scalability work:
- *    Copyright (C) 2001 Anton Blanchard <anton@au.ibm.com>, IBM
- * 
- *    Module name: htab.c
- *
- *    Description:
- *      PowerPC Hashed Page Table functions
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
- */
-
-#include <linux/spinlock.h>
-#include <linux/errno.h>
-#include <linux/sched.h>
-#include <linux/proc_fs.h>
-#include <linux/stat.h>
-#include <linux/sysctl.h>
-#include <linux/ctype.h>
-#include <linux/cache.h>
-
-#include <asm/ppcdebug.h>
-#include <asm/processor.h>
-#include <asm/pgtable.h>
-#include <asm/mmu.h>
-#include <asm/mmu_context.h>
-#include <asm/page.h>
-#include <asm/types.h>
-#include <asm/system.h>
-#include <asm/uaccess.h>
-#include <asm/naca.h>
-#include <asm/pmc.h>
-#include <asm/machdep.h>
-#include <asm/lmb.h>
-#include <asm/abs_addr.h>
-#include <asm/tlbflush.h>
-#include <asm/io.h>
-#include <asm/eeh.h>
-#include <asm/tlb.h>
-#include <asm/cacheflush.h>
-#include <asm/cputable.h>
-
-/*
- * Note:  pte   --> Linux PTE
- *        HPTE  --> PowerPC Hashed Page Table Entry
- *
- * Execution context:
- *   htab_initialize is called with the MMU off (of course), but
- *   the kernel has been copied down to zero so it can directly
- *   reference global data.  At this point it is very difficult
- *   to print debug info.
- *
- */
-
-HTAB htab_data = {NULL, 0, 0, 0, 0};
-
-extern unsigned long _SDR1;
-
-#define KB (1024)
-#define MB (1024*KB)
-
-static inline void
-loop_forever(void)
-{
-	volatile unsigned long x = 1;
-	for(;x;x|=1)
-		;
-}
-
-static inline void
-create_pte_mapping(unsigned long start, unsigned long end,
-		   unsigned long mode, int large)
-{
-	unsigned long addr;
-	unsigned int step;
-
-	if (large)
-		step = 16*MB;
-	else
-		step = 4*KB;
-
-	for (addr = start; addr < end; addr += step) {
-		unsigned long vpn, hash, hpteg;
-		unsigned long vsid = get_kernel_vsid(addr);
-		unsigned long va = (vsid << 28) | (addr & 0xfffffff);
-		int ret;
-
-		if (large)
-			vpn = va >> LARGE_PAGE_SHIFT;
-		else
-			vpn = va >> PAGE_SHIFT;
-
-		hash = hpt_hash(vpn, large);
-
-		hpteg = ((hash & htab_data.htab_hash_mask)*HPTES_PER_GROUP);
-
-		if (systemcfg->platform == PLATFORM_PSERIES_LPAR)
-			ret = pSeries_lpar_hpte_insert(hpteg, va,
-				(unsigned long)__v2a(addr) >> PAGE_SHIFT,
-				0, mode, 1, large);
-		else
-			ret = pSeries_hpte_insert(hpteg, va,
-				(unsigned long)__v2a(addr) >> PAGE_SHIFT,
-				0, mode, 1, large);
-
-		if (ret == -1) {
-			ppc64_terminate_msg(0x20, "create_pte_mapping");
-			loop_forever();
-		}
-	}
-}
-
-void
-htab_initialize(void)
-{
-	unsigned long table, htab_size_bytes;
-	unsigned long pteg_count;
-	unsigned long mode_rw;
-
-	/*
-	 * Calculate the required size of the htab.  We want the number of
-	 * PTEGs to equal one half the number of real pages.
-	 */ 
-	htab_size_bytes = 1UL << naca->pftSize;
-	pteg_count = htab_size_bytes >> 7;
-
-	/* For debug, make the HTAB 1/8 as big as it normally would be. */
-	ifppcdebug(PPCDBG_HTABSIZE) {
-		pteg_count >>= 3;
-		htab_size_bytes = pteg_count << 7;
-	}
-
-	htab_data.htab_num_ptegs = pteg_count;
-	htab_data.htab_hash_mask = pteg_count - 1;
-
-	if (systemcfg->platform == PLATFORM_PSERIES) {
-		/* Find storage for the HPT.  Must be contiguous in
-		 * the absolute address space.
-		 */
-		table = lmb_alloc(htab_size_bytes, htab_size_bytes);
-		if ( !table ) {
-			ppc64_terminate_msg(0x20, "hpt space");
-			loop_forever();
-		}
-		htab_data.htab = (HPTE *)__a2v(table);
-
-		/* htab absolute addr + encoded htabsize */
-		_SDR1 = table + __ilog2(pteg_count) - 11;
-
-		/* Initialize the HPT with no entries */
-		memset((void *)table, 0, htab_size_bytes);
-	} else {
-		/* Using a hypervisor which owns the htab */
-		htab_data.htab = NULL;
-		_SDR1 = 0; 
-	}
-
-	mode_rw = _PAGE_ACCESSED | _PAGE_COHERENT | PP_RWXX;
-
-	/* XXX we currently map kernel text rw, should fix this */
-	if ((cur_cpu_spec->cpu_features & CPU_FTR_16M_PAGE)
-	    && systemcfg->physicalMemorySize > 256*MB) {
-		create_pte_mapping((unsigned long)KERNELBASE, 
-				   KERNELBASE + 256*MB, mode_rw, 0);
-		create_pte_mapping((unsigned long)KERNELBASE + 256*MB, 
-				   KERNELBASE + (systemcfg->physicalMemorySize), 
-				   mode_rw, 1);
-	} else {
-		create_pte_mapping((unsigned long)KERNELBASE, 
-				   KERNELBASE+(systemcfg->physicalMemorySize), 
-				   mode_rw, 0);
-	}
-}
-#undef KB
-#undef MB
-
-/*
- * find_linux_pte returns the address of a linux pte for a given 
- * effective address and directory.  If not found, it returns zero.
- */
-pte_t *find_linux_pte(pgd_t *pgdir, unsigned long ea)
-{
-	pgd_t *pg;
-	pmd_t *pm;
-	pte_t *pt = NULL;
-	pte_t pte;
-
-	pg = pgdir + pgd_index(ea);
-	if (!pgd_none(*pg)) {
-
-		pm = pmd_offset(pg, ea);
-		if (pmd_present(*pm)) { 
-			pt = pte_offset_kernel(pm, ea);
-			pte = *pt;
-			if (!pte_present(pte))
-				pt = NULL;
-		}
-	}
-
-	return pt;
-}
-
-static inline unsigned long computeHptePP(unsigned long pte)
-{
-	return (pte & _PAGE_USER) |
-		(((pte & _PAGE_USER) >> 1) &
-		 ((~((pte >> 2) &	/* _PAGE_RW */
-		     (pte >> 7))) &	/* _PAGE_DIRTY */
-		  1));
-}
-
-/*
- * Handle a fault by adding an HPTE. If the address can't be determined
- * to be valid via Linux page tables, return 1. If handled return 0
- */
-int __hash_page(unsigned long ea, unsigned long access, unsigned long vsid,
-		pte_t *ptep, unsigned long trap, int local)
-{
-	unsigned long va, vpn;
-	unsigned long newpp, prpn;
-	unsigned long hpteflags;
-	long slot;
-	pte_t old_pte, new_pte;
-
-	/* XXX fix for large ptes */
-	int large = 0;
-
-	/* Search the Linux page table for a match with va */
-	va = (vsid << 28) | (ea & 0x0fffffff);
-
-	if (large)
-		vpn = va >> LARGE_PAGE_SHIFT;
-	else
-		vpn = va >> PAGE_SHIFT;
-
-	/*
-	 * If no pte found or not present, send the problem up to
-	 * do_page_fault
-	 */
-	if (unlikely(!ptep || !pte_present(*ptep)))
-		return 1;
-
-	/* 
-	 * Check the user's access rights to the page.  If access should be
-	 * prevented then send the problem up to do_page_fault.
-	 */
-	access |= _PAGE_PRESENT;
-	if (unlikely(access & ~(pte_val(*ptep))))
-		return 1;
-
-	/*
-	 * At this point, we have a pte (old_pte) which can be used to build
-	 * or update an HPTE. There are 2 cases:
-	 *
-	 * 1. There is a valid (present) pte with no associated HPTE (this is 
-	 *	the most common case)
-	 * 2. There is a valid (present) pte with an associated HPTE. The
-	 *	current values of the pp bits in the HPTE prevent access
-	 *	because we are doing software DIRTY bit management and the
-	 *	page is currently not DIRTY. 
-	 */
-
-	old_pte = *ptep;
-	new_pte = old_pte;
-	/* If the attempted access was a store */
-	if (access & _PAGE_RW)
-		pte_val(new_pte) |= _PAGE_ACCESSED | _PAGE_DIRTY;
-	else
-		pte_val(new_pte) |= _PAGE_ACCESSED;
-
-	newpp = computeHptePP(pte_val(new_pte));
-
-#define PPC64_HWNOEXEC (1 << 2)
-
-	/* We do lazy icache flushing on cpus that support it */
-	if (unlikely((cur_cpu_spec->cpu_features & CPU_FTR_NOEXECUTE)
-		     && pfn_valid(pte_pfn(new_pte)))) {
-		struct page *page = pte_page(new_pte);
-
-		/* page is dirty */
-		if (!PageReserved(page) &&
-		    !test_bit(PG_arch_1, &page->flags)) {
-			if (trap == 0x400) {
-				__flush_dcache_icache(page_address(page));
-				set_bit(PG_arch_1, &page->flags);
-			} else {
-				newpp |= PPC64_HWNOEXEC;
-			}
-		}
-	}
-
-	/* Check if pte already has an hpte (case 2) */
-	if (unlikely(pte_val(old_pte) & _PAGE_HASHPTE)) {
-		/* There MIGHT be an HPTE for this pte */
-		unsigned long hash, slot, secondary;
-
-		hash = hpt_hash(vpn, large);
-		secondary = (pte_val(old_pte) & _PAGE_SECONDARY) >> 15;
-		if (secondary)
-			hash = ~hash;
-		slot = (hash & htab_data.htab_hash_mask) * HPTES_PER_GROUP;
-		slot += (pte_val(old_pte) & _PAGE_GROUP_IX) >> 12;
-
-		if (ppc_md.hpte_updatepp(slot, newpp, va, large, local) == -1)
-			pte_val(old_pte) &= ~_PAGE_HPTEFLAGS;
-		else
-			if (!pte_same(old_pte, new_pte))
-				*ptep = new_pte;
-	}
-
-	if (likely(!(pte_val(old_pte) & _PAGE_HASHPTE))) {
-		unsigned long hash = hpt_hash(vpn, large);
-		unsigned long hpte_group;
-		prpn = pte_val(old_pte) >> PTE_SHIFT;
-
-repeat:
-		hpte_group = ((hash & htab_data.htab_hash_mask) *
-			      HPTES_PER_GROUP) & ~0x7UL;
-
-		/* Update the linux pte with the HPTE slot */
-		pte_val(new_pte) &= ~_PAGE_HPTEFLAGS;
-		pte_val(new_pte) |= _PAGE_HASHPTE;
-
-		/* copy appropriate flags from linux pte */
-		hpteflags = (pte_val(new_pte) & 0x1f8) | newpp;
-
-		slot = ppc_md.hpte_insert(hpte_group, va, prpn, 0,
-					  hpteflags, 0, large);
-
-		/* Primary is full, try the secondary */
-		if (unlikely(slot == -1)) {
-			pte_val(new_pte) |= 1 << 15;
-			hpte_group = ((~hash & htab_data.htab_hash_mask) *
-				      HPTES_PER_GROUP) & ~0x7UL; 
-			slot = ppc_md.hpte_insert(hpte_group, va, prpn,
-						  1, hpteflags, 0, large);
-			if (slot == -1) {
-				if (mftb() & 0x1)
-					hpte_group = ((hash & htab_data.htab_hash_mask) * HPTES_PER_GROUP) & ~0x7UL;
-
-				ppc_md.hpte_remove(hpte_group);
-				goto repeat;
-                        }
-		}
-
-		if (unlikely(slot == -2))
-			panic("hash_page: pte_insert failed\n");
-
-		pte_val(new_pte) |= (slot<<12) & _PAGE_GROUP_IX;
-
-		/* 
-		 * No need to use ldarx/stdcx here because all who
-		 * might be updating the pte will hold the
-		 * page_table_lock or the hash_table_lock
-		 * (we hold both)
-		 */
-		*ptep = new_pte;
-	}
-
-	return 0;
-}
-
-int hash_page(unsigned long ea, unsigned long access, unsigned long trap)
-{
-	void *pgdir;
-	unsigned long vsid;
-	struct mm_struct *mm;
-	pte_t *ptep;
-	int ret;
-	int user_region = 0;
-	int local = 0;
-	cpumask_t tmp;
-
-	/* Check for invalid addresses. */
-	if (!IS_VALID_EA(ea))
-		return 1;
-
- 	switch (REGION_ID(ea)) {
-	case USER_REGION_ID:
-		user_region = 1;
-		mm = current->mm;
-		if (mm == NULL)
-			return 1;
-
-		vsid = get_vsid(mm->context, ea);
-		break;
-	case IO_REGION_ID:
-		mm = &ioremap_mm;
-		vsid = get_kernel_vsid(ea);
-		break;
-	case VMALLOC_REGION_ID:
-		mm = &init_mm;
-		vsid = get_kernel_vsid(ea);
-		break;
-#if 0
-	case EEH_REGION_ID:
-		/*
-		 * Should only be hit if there is an access to MMIO space
-		 * which is protected by EEH.
-		 * Send the problem up to do_page_fault 
-		 */
-	case KERNEL_REGION_ID:
-		/*
-		 * Should never get here - entire 0xC0... region is bolted.
-		 * Send the problem up to do_page_fault 
-		 */
-#endif
-	default:
-		/* Not a valid range
-		 * Send the problem up to do_page_fault 
-		 */
-		return 1;
-		break;
-	}
-
-	pgdir = mm->pgd;
-
-	if (pgdir == NULL)
-		return 1;
-
-	/*
-	 * Lock the Linux page table to prevent mmap and kswapd
-	 * from modifying entries while we search and update
-	 */
-	spin_lock(&mm->page_table_lock);
-
-	tmp = cpumask_of_cpu(smp_processor_id());
-	if (user_region && cpus_equal(mm->cpu_vm_mask, tmp))
-		local = 1;
-
-	ret = hash_huge_page(mm, access, ea, vsid, local);
-	if (ret < 0) {
-		ptep = find_linux_pte(pgdir, ea);
-		ret = __hash_page(ea, access, vsid, ptep, trap, local);
-	}
-
-	spin_unlock(&mm->page_table_lock);
-
-	return ret;
-}
-
-void flush_hash_page(unsigned long context, unsigned long ea, pte_t pte,
-		     int local)
-{
-	unsigned long vsid, vpn, va, hash, secondary, slot;
-
-	/* XXX fix for large ptes */
-	unsigned long large = 0;
-
-	if ((ea >= USER_START) && (ea <= USER_END))
-		vsid = get_vsid(context, ea);
-	else
-		vsid = get_kernel_vsid(ea);
-
-	va = (vsid << 28) | (ea & 0x0fffffff);
-	if (large)
-		vpn = va >> LARGE_PAGE_SHIFT;
-	else
-		vpn = va >> PAGE_SHIFT;
-	hash = hpt_hash(vpn, large);
-	secondary = (pte_val(pte) & _PAGE_SECONDARY) >> 15;
-	if (secondary)
-		hash = ~hash;
-	slot = (hash & htab_data.htab_hash_mask) * HPTES_PER_GROUP;
-	slot += (pte_val(pte) & _PAGE_GROUP_IX) >> 12;
-
-	ppc_md.hpte_invalidate(slot, va, large, local);
-}
-
-void flush_hash_range(unsigned long context, unsigned long number, int local)
-{
-	if (ppc_md.flush_hash_range) {
-		ppc_md.flush_hash_range(context, number, local);
-	} else {
-		int i;
-		struct ppc64_tlb_batch *batch =
-			&ppc64_tlb_batch[smp_processor_id()];
-
-		for (i = 0; i < number; i++)
-			flush_hash_page(context, batch->addr[i], batch->pte[i],
-					local);
-	}
-}
diff -purN linux-2.5/arch/ppc64/kernel/i8259.c linuxppc64-2.5/arch/ppc64/kernel/i8259.c
--- linux-2.5/arch/ppc64/kernel/i8259.c	2003-02-11 11:51:44.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/i8259.c	2003-09-12 19:50:39.000000000 +0000
@@ -124,8 +124,8 @@ static void i8259_unmask_irq(unsigned in
 
 static void i8259_end_irq(unsigned int irq)
 {
-	if (!(irq_desc[irq].status & (IRQ_DISABLED|IRQ_INPROGRESS)) &&
-	    irq_desc[irq].action)
+	if (!(get_irq_desc(irq)->status & (IRQ_DISABLED|IRQ_INPROGRESS)) &&
+	    get_irq_desc(irq)->action)
 		i8259_unmask_irq(irq);
 }
 
diff -purN linux-2.5/arch/ppc64/kernel/iSeries_IoMmTable.c linuxppc64-2.5/arch/ppc64/kernel/iSeries_IoMmTable.c
--- linux-2.5/arch/ppc64/kernel/iSeries_IoMmTable.c	2002-08-28 07:54:04.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/iSeries_IoMmTable.c	2003-12-10 04:13:06.000000000 +0000
@@ -1,3 +1,4 @@
+#define PCIFR(...)
 /************************************************************************/
 /* This module supports the iSeries I/O Address translation mapping     */
 /* Copyright (C) 20yy  <Allan H Trautman> <IBM Corp>                    */
@@ -30,134 +31,139 @@
 #include <linux/pci.h>
 #include <linux/spinlock.h>
 #include <asm/ppcdebug.h>
-#include <asm/flight_recorder.h>
 #include <asm/iSeries/HvCallPci.h>
 #include <asm/iSeries/iSeries_pci.h>
 
 #include "iSeries_IoMmTable.h"
 #include "pci.h"
 
-/*******************************************************************/
-/* Table defines                                                   */
-/* Each Entry size is 4 MB * 1024 Entries = 4GB I/O address space. */
-/*******************************************************************/
+/*
+ * Table defines
+ * Each Entry size is 4 MB * 1024 Entries = 4GB I/O address space.
+ */
 #define Max_Entries 1024
 unsigned long iSeries_IoMmTable_Entry_Size = 0x0000000000400000; 
 unsigned long iSeries_Base_Io_Memory       = 0xE000000000000000;
 unsigned long iSeries_Max_Io_Memory        = 0xE000000000000000;
 static   long iSeries_CurrentIndex         = 0;
 
-/*******************************************************************/
-/* Lookup Tables.                                                  */
-/*******************************************************************/
-struct iSeries_Device_Node** iSeries_IoMmTable;
-u8*                          iSeries_IoBarTable;
-
-/*******************************************************************/
-/* Static and Global variables                                     */
-/*******************************************************************/
-static char*      iSeriesPciIoText     = "iSeries PCI I/O";
+/*
+ * Lookup Tables.
+ */
+struct iSeries_Device_Node **iSeries_IoMmTable;
+u8 *iSeries_IoBarTable;
+
+/*
+ * Static and Global variables
+ */
+static char *iSeriesPciIoText = "iSeries PCI I/O";
 static spinlock_t iSeriesIoMmTableLock = SPIN_LOCK_UNLOCKED;
 
-/*******************************************************************/
-/* iSeries_IoMmTable_Initialize                                    */
-/*******************************************************************/
-/* Allocates and initalizes the Address Translation Table and Bar  */
-/* Tables to get them ready for use.  Must be called before any    */
-/* I/O space is handed out to the device BARs.                     */
-/* A follow up method,iSeries_IoMmTable_Status can be called to    */
-/* adjust the table after the device BARs have been assiged to     */
-/* resize the table.                                               */
-/*******************************************************************/
+/*
+ * iSeries_IoMmTable_Initialize
+ *
+ * Allocates and initalizes the Address Translation Table and Bar
+ * Tables to get them ready for use.  Must be called before any
+ * I/O space is handed out to the device BARs.
+ * A follow up method,iSeries_IoMmTable_Status can be called to
+ * adjust the table after the device BARs have been assiged to
+ * resize the table.
+ */
 void iSeries_IoMmTable_Initialize(void)
 {
 	spin_lock(&iSeriesIoMmTableLock);
-	iSeries_IoMmTable  = kmalloc(sizeof(void*)*Max_Entries,GFP_KERNEL);
-	iSeries_IoBarTable = kmalloc(sizeof(u8)*Max_Entries,   GFP_KERNEL);
+	iSeries_IoMmTable  = kmalloc(sizeof(void *) * Max_Entries, GFP_KERNEL);
+	iSeries_IoBarTable = kmalloc(sizeof(u8) * Max_Entries, GFP_KERNEL);
 	spin_unlock(&iSeriesIoMmTableLock);
-	PCIFR("IoMmTable Initialized 0x%p",  iSeries_IoMmTable);
-	if(iSeries_IoMmTable == NULL || iSeries_IoBarTable == NULL) {
+	PCIFR("IoMmTable Initialized 0x%p", iSeries_IoMmTable);
+	if ((iSeries_IoMmTable == NULL) || (iSeries_IoBarTable == NULL))
 		panic("PCI: I/O tables allocation failed.\n");
-	}
 }
 
-/*******************************************************************/
-/* iSeries_IoMmTable_AllocateEntry                                 */
-/*******************************************************************/
-/* Adds pci_dev entry in address translation table                 */
-/*******************************************************************/
-/* - Allocates the number of entries required in table base on BAR */
-/*   size.                                                         */
-/* - Allocates starting at iSeries_Base_Io_Memory and increases.   */
-/* - The size is round up to be a multiple of entry size.          */
-/* - CurrentIndex is incremented to keep track of the last entry.  */
-/* - Builds the resource entry for allocated BARs.                 */
-/*******************************************************************/
-static void iSeries_IoMmTable_AllocateEntry(struct pci_dev* PciDev, int BarNumber)
+/*
+ * iSeries_IoMmTable_AllocateEntry
+ *
+ * Adds pci_dev entry in address translation table
+ *
+ * - Allocates the number of entries required in table base on BAR
+ *   size.
+ * - Allocates starting at iSeries_Base_Io_Memory and increases.
+ * - The size is round up to be a multiple of entry size.
+ * - CurrentIndex is incremented to keep track of the last entry.
+ * - Builds the resource entry for allocated BARs.
+ */
+static void iSeries_IoMmTable_AllocateEntry(struct pci_dev *PciDev,
+		int BarNumber)
 {
-	struct resource* BarResource = &PciDev->resource[BarNumber];
-	long             BarSize     = pci_resource_len(PciDev,BarNumber);
-	/***********************************************************/
-	/* No space to allocate, quick exit, skip Allocation.      */
-	/***********************************************************/
-	if(BarSize == 0) return;
-	/***********************************************************/
-	/* Set Resource values.                                    */
-	/***********************************************************/
+	struct resource *BarResource = &PciDev->resource[BarNumber];
+	long BarSize = pci_resource_len(PciDev, BarNumber);
+
+	/*
+	 * No space to allocate, quick exit, skip Allocation.
+	 */
+	if (BarSize == 0)
+		return;
+	/*
+	 * Set Resource values.
+	 */
 	spin_lock(&iSeriesIoMmTableLock);
-	BarResource->name  = iSeriesPciIoText;
-	BarResource->start = iSeries_IoMmTable_Entry_Size*iSeries_CurrentIndex;
-	BarResource->start+= iSeries_Base_Io_Memory;
-	BarResource->end   = BarResource->start+BarSize-1;
-	/***********************************************************/
-	/* Allocate the number of table entries needed for BAR.    */
-	/***********************************************************/
+	BarResource->name = iSeriesPciIoText;
+	BarResource->start =
+		iSeries_IoMmTable_Entry_Size * iSeries_CurrentIndex;
+	BarResource->start += iSeries_Base_Io_Memory;
+	BarResource->end = BarResource->start+BarSize-1;
+	/*
+	 * Allocate the number of table entries needed for BAR.
+	 */
 	while (BarSize > 0 ) {
-		*(iSeries_IoMmTable +iSeries_CurrentIndex) = (struct iSeries_Device_Node*)PciDev->sysdata;
-		*(iSeries_IoBarTable+iSeries_CurrentIndex) = BarNumber;
+		*(iSeries_IoMmTable + iSeries_CurrentIndex) =
+			(struct iSeries_Device_Node *)PciDev->sysdata;
+		*(iSeries_IoBarTable + iSeries_CurrentIndex) = BarNumber;
 		BarSize -= iSeries_IoMmTable_Entry_Size;
 		++iSeries_CurrentIndex;
 	}
-	iSeries_Max_Io_Memory = (iSeries_IoMmTable_Entry_Size*iSeries_CurrentIndex)+iSeries_Base_Io_Memory;
+	iSeries_Max_Io_Memory = iSeries_Base_Io_Memory +
+		(iSeries_IoMmTable_Entry_Size * iSeries_CurrentIndex);
 	spin_unlock(&iSeriesIoMmTableLock);
 }
 
-/*******************************************************************/
-/* iSeries_allocateDeviceBars                                      */
-/*******************************************************************/
-/* - Allocates ALL pci_dev BAR's and updates the resources with the*/
-/*   BAR value.  BARS with zero length will have the resources     */
-/*   The HvCallPci_getBarParms is used to get the size of the BAR  */
-/*   space.  It calls iSeries_IoMmTable_AllocateEntry to allocate  */
-/*   each entry.                                                   */
-/* - Loops through The Bar resources(0 - 5) including the ROM      */
-/*   is resource(6).                                               */
-/*******************************************************************/
-void iSeries_allocateDeviceBars(struct pci_dev* PciDev)
+/*
+ * iSeries_allocateDeviceBars
+ *
+ * - Allocates ALL pci_dev BAR's and updates the resources with the
+ *   BAR value.  BARS with zero length will have the resources
+ *   The HvCallPci_getBarParms is used to get the size of the BAR
+ *   space.  It calls iSeries_IoMmTable_AllocateEntry to allocate
+ *   each entry.
+ * - Loops through The Bar resources(0 - 5) including the ROM
+ *   is resource(6).
+ */
+void iSeries_allocateDeviceBars(struct pci_dev *PciDev)
 {
-	struct resource* BarResource;
-	int              BarNumber;
-	for(BarNumber = 0; BarNumber <= PCI_ROM_RESOURCE; ++BarNumber) {
+	struct resource *BarResource;
+	int BarNumber;
+
+	for (BarNumber = 0; BarNumber <= PCI_ROM_RESOURCE; ++BarNumber) {
 		BarResource = &PciDev->resource[BarNumber];
 		iSeries_IoMmTable_AllocateEntry(PciDev, BarNumber);
     	}
 }
 
-/************************************************************************/
-/* Translates the IoAddress to the device that is mapped to IoSpace.    */
-/* This code is inlined, see the iSeries_pci.c file for the replacement.*/
-/************************************************************************/
-struct iSeries_Device_Node* iSeries_xlateIoMmAddress(void* IoAddress)
+/*
+ * Translates the IoAddress to the device that is mapped to IoSpace.
+ * This code is inlined, see the iSeries_pci.c file for the replacement.
+ */
+struct iSeries_Device_Node *iSeries_xlateIoMmAddress(void *IoAddress)
 {
 	return NULL;	   
 }
 
-/************************************************************************
+/*
  * Status hook for IoMmTable
- ************************************************************************/
-void     iSeries_IoMmTable_Status(void)
+ */
+void iSeries_IoMmTable_Status(void)
 {
-	PCIFR("IoMmTable......: 0x%p",iSeries_IoMmTable);
-	PCIFR("IoMmTable Range: 0x%p to 0x%p",iSeries_Base_Io_Memory,iSeries_Max_Io_Memory);
-	return;
+	PCIFR("IoMmTable......: 0x%p", iSeries_IoMmTable);
+	PCIFR("IoMmTable Range: 0x%p to 0x%p", iSeries_Base_Io_Memory,
+			iSeries_Max_Io_Memory);
 }
diff -purN linux-2.5/arch/ppc64/kernel/iSeries_IoMmTable.h linuxppc64-2.5/arch/ppc64/kernel/iSeries_IoMmTable.h
--- linux-2.5/arch/ppc64/kernel/iSeries_IoMmTable.h	2002-02-14 12:14:35.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/iSeries_IoMmTable.h	2003-11-25 05:52:18.000000000 +0000
@@ -31,55 +31,55 @@
 struct pci_dev;
 struct iSeries_Device_Node;
 
-extern struct iSeries_Device_Node** iSeries_IoMmTable;
-extern u8*                          iSeries_IoBarTable;
+extern struct iSeries_Device_Node **iSeries_IoMmTable;
+extern u8 *iSeries_IoBarTable;
 extern unsigned long iSeries_Base_Io_Memory;
 extern unsigned long iSeries_Max_Io_Memory;
 extern unsigned long iSeries_Base_Io_Memory;
 extern unsigned long iSeries_IoMmTable_Entry_Size;
-/************************************************************************/
-/* iSeries_IoMmTable_Initialize                                         */
-/************************************************************************/
-/* - Initalizes the Address Translation Table and get it ready for use. */
-/*   Must be called before any client calls any of the other methods.   */
-/*                                                                      */
-/* Parameters: None.                                                    */
-/*                                                                      */
-/* Return: None.                                                        */  
-/************************************************************************/
-extern  void iSeries_IoMmTable_Initialize(void);
-extern  void iSeries_IoMmTable_Status(void);
+/*
+ * iSeries_IoMmTable_Initialize
+ *
+ * - Initalizes the Address Translation Table and get it ready for use.
+ *   Must be called before any client calls any of the other methods.
+ *
+ * Parameters: None.
+ *
+ * Return: None.
+ */
+extern void iSeries_IoMmTable_Initialize(void);
+extern void iSeries_IoMmTable_Status(void);
 
-/************************************************************************/
-/* iSeries_allocateDeviceBars                                           */
-/************************************************************************/
-/* - Allocates ALL pci_dev BAR's and updates the resources with the BAR */
-/*   value.  BARS with zero length will not have the resources.  The    */
-/*   HvCallPci_getBarParms is used to get the size of the BAR space.    */
-/*   It calls iSeries_IoMmTable_AllocateEntry to allocate each entry.   */
-/*                                                                      */
-/* Parameters:                                                          */
-/* pci_dev = Pointer to pci_dev structure that will be mapped to pseudo */
-/*           I/O Address.                                               */
-/*                                                                      */
-/* Return:                                                              */
-/*   The pci_dev I/O resources updated with pseudo I/O Addresses.       */
-/************************************************************************/
-extern  void iSeries_allocateDeviceBars(struct pci_dev* );
+/*
+ * iSeries_allocateDeviceBars
+ *
+ * - Allocates ALL pci_dev BAR's and updates the resources with the BAR
+ *   value.  BARS with zero length will not have the resources.  The
+ *   HvCallPci_getBarParms is used to get the size of the BAR space.
+ *   It calls iSeries_IoMmTable_AllocateEntry to allocate each entry.
+ *
+ * Parameters:
+ * pci_dev = Pointer to pci_dev structure that will be mapped to pseudo
+ *           I/O Address.
+ *
+ * Return:
+ *   The pci_dev I/O resources updated with pseudo I/O Addresses.
+ */
+extern void iSeries_allocateDeviceBars(struct pci_dev *);
 
-/************************************************************************/
-/* iSeries_xlateIoMmAddress                                             */
-/************************************************************************/
-/* - Translates an I/O Memory address to Device Node that has been the  */
-/*   allocated the psuedo I/O Address.                                  */
-/*                                                                      */
-/* Parameters:                                                          */
-/* IoAddress = I/O Memory Address.                                      */
-/*                                                                      */
-/* Return:                                                              */
-/*   An iSeries_Device_Node to the device mapped to the I/O address. The*/
-/*   BarNumber and BarOffset are valid if the Device Node is returned.  */
-/************************************************************************/
-extern struct iSeries_Device_Node* iSeries_xlateIoMmAddress(void* IoAddress);
+/*
+ * iSeries_xlateIoMmAddress
+ *
+ * - Translates an I/O Memory address to Device Node that has been the
+ *   allocated the psuedo I/O Address.
+ *
+ * Parameters:
+ * IoAddress = I/O Memory Address.
+ *
+ * Return:
+ *   An iSeries_Device_Node to the device mapped to the I/O address. The
+ *   BarNumber and BarOffset are valid if the Device Node is returned.
+ */
+extern struct iSeries_Device_Node *iSeries_xlateIoMmAddress(void *IoAddress);
 
 #endif /* _ISERIES_IOMMTABLE_H */
diff -purN linux-2.5/arch/ppc64/kernel/iSeries_VpdInfo.c linuxppc64-2.5/arch/ppc64/kernel/iSeries_VpdInfo.c
--- linux-2.5/arch/ppc64/kernel/iSeries_VpdInfo.c	2002-09-18 02:00:50.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/iSeries_VpdInfo.c	2003-12-10 04:13:06.000000000 +0000
@@ -38,20 +38,20 @@
 #include <asm/iSeries/iSeries_pci.h>
 #include "pci.h"
 
-/************************************************/
-/* Size of Bus VPD data                         */
-/************************************************/
+/*
+ * Size of Bus VPD data
+ */
 #define BUS_VPDSIZE      1024
-/************************************************/
-/* Bus Vpd Tags                                 */
-/************************************************/
+/*
+ * Bus Vpd Tags
+ */
 #define  VpdEndOfDataTag   0x78
 #define  VpdEndOfAreaTag   0x79
 #define  VpdIdStringTag    0x82
 #define  VpdVendorAreaTag  0x84
-/************************************************/
-/* Mfg Area Tags                                */
-/************************************************/
+/*
+ * Mfg Area Tags
+ */
 #define  VpdFruFlag       0x4647     // "FG"
 #define  VpdFruFrameId    0x4649     // "FI"
 #define  VpdSlotMapFormat 0x4D46     // "MF"
@@ -59,9 +59,9 @@
 #define  VpdFruSerial     0x534E     // "SN"
 #define  VpdSlotMap       0x534D     // "SM"
 
-/************************************************/
-/* Structures of the areas                      */
-/************************************************/
+/*
+ * Structures of the areas
+ */
 struct MfgVpdAreaStruct {
 	u16 Tag;
 	u8  TagLength;
@@ -82,168 +82,172 @@ struct SlotMapStruct {
 typedef struct SlotMapStruct SlotMap;
 #define SLOT_ENTRY_SIZE   16
 
-/****************************************************************
- *                                                              *
- * Bus, Card, Board, FrameId, CardLocation.                     *
- ****************************************************************/
-LocationData* iSeries_GetLocationData(struct pci_dev* PciDev)
+/*
+ * Bus, Card, Board, FrameId, CardLocation.
+ */
+LocationData* iSeries_GetLocationData(struct pci_dev *PciDev)
 {
-	struct iSeries_Device_Node* DevNode = (struct iSeries_Device_Node*)PciDev->sysdata;
-	LocationData* LocationPtr = (LocationData*)kmalloc(LOCATION_DATA_SIZE, GFP_KERNEL);
+	struct iSeries_Device_Node *DevNode =
+		(struct iSeries_Device_Node *)PciDev->sysdata;
+	LocationData *LocationPtr =
+		(LocationData *)kmalloc(LOCATION_DATA_SIZE, GFP_KERNEL);
+
 	if (LocationPtr == NULL) {
 		printk("PCI: LocationData area allocation failed!\n");
 		return NULL;
 	}
-	memset(LocationPtr,0,LOCATION_DATA_SIZE);
-	LocationPtr->Bus              = ISERIES_BUS(DevNode);
-	LocationPtr->Board            = DevNode->Board;
-	LocationPtr->FrameId          = DevNode->FrameId;
-	LocationPtr->Card             = PCI_SLOT(DevNode->DevFn);
-	strcpy(&LocationPtr->CardLocation[0],&DevNode->CardLocation[0]);
+	memset(LocationPtr, 0, LOCATION_DATA_SIZE);
+	LocationPtr->Bus = ISERIES_BUS(DevNode);
+	LocationPtr->Board = DevNode->Board;
+	LocationPtr->FrameId = DevNode->FrameId;
+	LocationPtr->Card = PCI_SLOT(DevNode->DevFn);
+	strcpy(&LocationPtr->CardLocation[0], &DevNode->CardLocation[0]);
 	return LocationPtr;
 }
 
-/************************************************************************/
-/* Formats the device information.                                      */
-/* - Pass in pci_dev* pointer to the device.                            */
-/* - Pass in buffer to place the data.  Danger here is the buffer must  */
-/*   be as big as the client says it is.   Should be at least 128 bytes.*/
-/* Return will the length of the string data put in the buffer.         */
-/* Format:                                                              */
-/* PCI: Bus  0, Device 26, Vendor 0x12AE  Frame  1, Card  C10  Ethernet */
-/* controller                                                           */
-/************************************************************************/
-int  iSeries_Device_Information(struct pci_dev* PciDev,char* Buffer, int BufferSize)
+/*
+ * Formats the device information.
+ * - Pass in pci_dev* pointer to the device.
+ * - Pass in buffer to place the data.  Danger here is the buffer must
+ *   be as big as the client says it is.   Should be at least 128 bytes.
+ * Return will the length of the string data put in the buffer.
+ * Format:
+ * PCI: Bus  0, Device 26, Vendor 0x12AE  Frame  1, Card  C10  Ethernet
+ * controller
+ */
+int iSeries_Device_Information(struct pci_dev *PciDev, char *buffer,
+		int BufferSize)
 {
-	struct iSeries_Device_Node* DevNode = (struct iSeries_Device_Node*)PciDev->sysdata;
-	char*          BufPtr  = Buffer;
-	int            LineLen = 0;
-
-	if (DevNode == NULL) {
-		LineLen = sprintf(BufPtr+LineLen, "PCI: iSeries_Device_Information DevNode is NULL");
-		return LineLen;
-	}
-
-	if (BufferSize >= 128) {
-		LineLen =  sprintf(BufPtr+LineLen,"PCI: Bus%3d, Device%3d, Vendor %04X ",
-				   ISERIES_BUS(DevNode), PCI_SLOT(PciDev->devfn),PciDev->vendor);
-
-		LineLen += sprintf(BufPtr+LineLen,"Frame%3d, Card %4s  ", DevNode->FrameId,DevNode->CardLocation);
-
-		if (pci_class_name(PciDev->class >> 8) == 0) {
-			LineLen += sprintf(BufPtr+LineLen,"0x%04X  ",(int)(PciDev->class >> 8));
-		}
-		else {
-			LineLen += sprintf(BufPtr+LineLen,"%s",pci_class_name(PciDev->class >> 8) );
-		}
-	}
-	return LineLen;
+	struct iSeries_Device_Node *DevNode =
+		(struct iSeries_Device_Node *)PciDev->sysdata;
+	int len;
+
+	if (DevNode == NULL)
+		return sprintf(buffer,
+				"PCI: iSeries_Device_Information DevNode is NULL");
+
+	if (BufferSize < 128)
+		return 0;
+
+	len = sprintf(buffer, "PCI: Bus%3d, Device%3d, Vendor %04X ",
+			ISERIES_BUS(DevNode), PCI_SLOT(PciDev->devfn),
+			PciDev->vendor);
+	len += sprintf(buffer + len, "Frame%3d, Card %4s  ",
+			DevNode->FrameId, DevNode->CardLocation);
+#ifdef CONFIG_PCI
+	if (pci_class_name(PciDev->class >> 8) == 0)
+		len += sprintf(buffer + len, "0x%04X  ",
+				(int)(PciDev->class >> 8));
+	else
+		len += sprintf(buffer + len, "%s",
+				pci_class_name(PciDev->class >> 8));
+#endif
+	return len;
 }
-/************************************************************************/
-/* Build a character string of the device location, Frame  1, Card  C10 */
-/************************************************************************/
-int   device_Location(struct pci_dev* PciDev,char* BufPtr)
+
+/*
+ * Build a character string of the device location, Frame  1, Card  C10
+ */
+int device_Location(struct pci_dev *PciDev, char *BufPtr)
 {
-	struct iSeries_Device_Node* DevNode = (struct iSeries_Device_Node*)PciDev->sysdata;
-	return sprintf(BufPtr,"PCI: Bus%3d, AgentId%3d, Vendor %04X, Location %s",
-		       DevNode->DsaAddr.busNumber,
-		       DevNode->AgentId,
-		       DevNode->Vendor,
-		       DevNode->Location);
+	struct iSeries_Device_Node *DevNode =
+		(struct iSeries_Device_Node *)PciDev->sysdata;
+	return sprintf(BufPtr, "PCI: Bus%3d, AgentId%3d, Vendor %04X, Location %s",
+		       DevNode->DsaAddr.Dsa.busNumber, DevNode->AgentId,
+		       DevNode->Vendor, DevNode->Location);
 }
 
-/*****************************************************************/
-/* Parse the Slot Area                                           */
-/*****************************************************************/
-void  iSeries_Parse_SlotArea(SlotMap* MapPtr,int MapLen, struct iSeries_Device_Node* DevNode)
+/*
+ * Parse the Slot Area
+ */
+void iSeries_Parse_SlotArea(SlotMap *MapPtr, int MapLen,
+		struct iSeries_Device_Node *DevNode)
 {
-	int      SlotMapLen = MapLen;
-	SlotMap* SlotMapPtr = MapPtr;
-	/*************************************************************/
-	/* Parse Slot label until we find the one requrested         */
-	/*************************************************************/
+	int SlotMapLen = MapLen;
+	SlotMap *SlotMapPtr = MapPtr;
+
+	/*
+	 * Parse Slot label until we find the one requrested
+	 */
 	while (SlotMapLen > 0) {
 		if (SlotMapPtr->AgentId == DevNode->AgentId ) {
-			/*******************************************************/
-			/* If Phb wasn't found, grab the entry first one found.*/ 
-			/*******************************************************/
-			if (DevNode->PhbId == 0xff) {
+			/*
+			 * If Phb wasn't found, grab the entry first one found.
+			 */
+			if (DevNode->PhbId == 0xff)
 				DevNode->PhbId = SlotMapPtr->PhbId; 
-			}
-			/**************************************************/
-			/* Found it, extract the data.                    */
-			/**************************************************/
+			/* Found it, extract the data. */
 			if (SlotMapPtr->PhbId == DevNode->PhbId ) {
-	        		memcpy(&DevNode->CardLocation,&SlotMapPtr->CardLocation,3);
+	        		memcpy(&DevNode->CardLocation,
+						&SlotMapPtr->CardLocation, 3);
 				DevNode->CardLocation[3]  = 0;
 				break;
 			}
 		}
-		/*********************************************************/
-		/* Point to the next Slot                                */
-		/*********************************************************/
-		SlotMapPtr = (SlotMap*)((char*)SlotMapPtr+SLOT_ENTRY_SIZE);
+		/* Point to the next Slot */
+		SlotMapPtr = (SlotMap *)((char *)SlotMapPtr + SLOT_ENTRY_SIZE);
 		SlotMapLen -= SLOT_ENTRY_SIZE;
 	}
 }
 
-/*****************************************************************/
-/* Parse the Mfg Area                                            */
-/*****************************************************************/
-static void  iSeries_Parse_MfgArea(u8* AreaData,int AreaLen, struct iSeries_Device_Node* DevNode)
+/*
+ * Parse the Mfg Area
+ */
+static void iSeries_Parse_MfgArea(u8 *AreaData, int AreaLen,
+		struct iSeries_Device_Node *DevNode)
 {
-	MfgArea* MfgAreaPtr = (MfgArea*)AreaData;
-	int      MfgAreaLen = AreaLen;
-	u16      SlotMapFmt = 0;
-
-	/*************************************************************/
-	/* Parse Mfg Data                                            */
-	/*************************************************************/
+	MfgArea *MfgAreaPtr = (MfgArea *)AreaData;
+	int MfgAreaLen = AreaLen;
+	u16 SlotMapFmt = 0;
+
+	/* Parse Mfg Data */
 	while (MfgAreaLen > 0) {
-		int MfgTagLen  = MfgAreaPtr->TagLength;
-		/*******************************************************/
-		/* Frame ID         (FI 4649020310 )                   */
-		/*******************************************************/
-		if (MfgAreaPtr->Tag == VpdFruFrameId) {     /* FI  */
+		int MfgTagLen = MfgAreaPtr->TagLength;
+		/* Frame ID         (FI 4649020310 ) */
+		if (MfgAreaPtr->Tag == VpdFruFrameId)		/* FI  */
 			DevNode->FrameId = MfgAreaPtr->AreaData1;
+		/* Slot Map Format  (MF 4D46020004 ) */
+		else if (MfgAreaPtr->Tag == VpdSlotMapFormat)	/* MF  */
+			SlotMapFmt = (MfgAreaPtr->AreaData1 * 256)
+				+ MfgAreaPtr->AreaData2;
+		/* Slot Map         (SM 534D90 */
+		else if (MfgAreaPtr->Tag == VpdSlotMap)	{	/* SM  */
+			SlotMap *SlotMapPtr;
+
+			if (SlotMapFmt == 0x1004)
+				SlotMapPtr = (SlotMap *)((char *)MfgAreaPtr
+						+ MFG_ENTRY_SIZE + 1);
+ 	    		else
+				SlotMapPtr = (SlotMap *)((char *)MfgAreaPtr
+						+ MFG_ENTRY_SIZE);
+	    		iSeries_Parse_SlotArea(SlotMapPtr, MfgTagLen, DevNode);
 		}
-		/*******************************************************/
-		/* Slot Map Format  (MF 4D46020004 )                   */
-		/*******************************************************/
-		else if (MfgAreaPtr->Tag == VpdSlotMapFormat){   /* MF  */
-			SlotMapFmt = (MfgAreaPtr->AreaData1*256)+(MfgAreaPtr->AreaData2);
-		}
-		/*******************************************************/
-		/* Slot Map         (SM 534D90                         */
-		/*******************************************************/
-		else if (MfgAreaPtr->Tag == VpdSlotMap){         /* SM  */
-			SlotMap*  SlotMapPtr;
-			if (SlotMapFmt == 0x1004) SlotMapPtr = (SlotMap*)((char*)MfgAreaPtr+MFG_ENTRY_SIZE+1);
- 	    		else                     SlotMapPtr = (SlotMap*)((char*)MfgAreaPtr+MFG_ENTRY_SIZE);
-	    		iSeries_Parse_SlotArea(SlotMapPtr,MfgTagLen, DevNode);
-		}
-		/*********************************************************/
-		/* Point to the next Mfg Area                            */
-		/* Use defined size, sizeof give wrong answer            */
-		/*********************************************************/
-		MfgAreaPtr = (MfgArea*)((char*)MfgAreaPtr + MfgTagLen + MFG_ENTRY_SIZE);
+		/*
+		 * Point to the next Mfg Area
+		 * Use defined size, sizeof give wrong answer
+		 */
+		MfgAreaPtr = (MfgArea *)((char *)MfgAreaPtr + MfgTagLen
+				+ MFG_ENTRY_SIZE);
 		MfgAreaLen -= (MfgTagLen + MFG_ENTRY_SIZE); 
 	}	
 }
 
-/*****************************************************************/
-/* Look for "BUS".. Data is not Null terminated.                 */
-/* PHBID of 0xFF indicates PHB was not found in VPD Data.        */	
-/*****************************************************************/
-static int iSeries_Parse_PhbId(u8* AreaPtr,int AreaLength)
+/*
+ * Look for "BUS".. Data is not Null terminated.
+ * PHBID of 0xFF indicates PHB was not found in VPD Data.
+ */
+static int iSeries_Parse_PhbId(u8 *AreaPtr, int AreaLength)
 {
-	u8*  PhbPtr  = AreaPtr;
-	int  DataLen = AreaLength;
-	char PhbId   = 0xFF;                   
+	u8 *PhbPtr = AreaPtr;
+	int DataLen = AreaLength;
+	char PhbId = 0xFF;                   
+
 	while (DataLen > 0) {
-		if (*PhbPtr == 'B' && *(PhbPtr+1) == 'U' && *(PhbPtr+2) == 'S') {
+		if ((*PhbPtr == 'B') && (*(PhbPtr + 1) == 'U')
+				&& (*(PhbPtr + 2) == 'S')) {
 			PhbPtr += 3;
-			while(*PhbPtr == ' ') ++PhbPtr;
+			while (*PhbPtr == ' ')
+				++PhbPtr;
 			PhbId = (*PhbPtr & 0x0F);
 			break;
         	}
@@ -253,64 +257,54 @@ static int iSeries_Parse_PhbId(u8* AreaP
 	return PhbId;
 }
 
-/****************************************************************/
-/* Parse out the VPD Areas                                      */
-/****************************************************************/
-static void  iSeries_Parse_Vpd(u8* VpdData, int VpdDataLen, struct iSeries_Device_Node* DevNode)
+/*
+ * Parse out the VPD Areas
+ */
+static void iSeries_Parse_Vpd(u8 *VpdData, int VpdDataLen,
+		struct iSeries_Device_Node *DevNode)
 {
-	u8*  TagPtr  = VpdData;
-	int  DataLen = VpdDataLen-3;
-	/*************************************************************/
-	/* Parse the Areas                                           */
-	/*************************************************************/
-	while (*TagPtr != VpdEndOfAreaTag && DataLen > 0) {
-		int  AreaLen   = *(TagPtr+1) + (*(TagPtr+2)*256);	
-		u8*  AreaData  = TagPtr+3;
+	u8 *TagPtr = VpdData;
+	int DataLen = VpdDataLen - 3;
 
-		if (*TagPtr == VpdIdStringTag) {
-			DevNode->PhbId = iSeries_Parse_PhbId(AreaData,AreaLen);
-		}
-		else if (*TagPtr == VpdVendorAreaTag) {
-	    		iSeries_Parse_MfgArea(AreaData,AreaLen,DevNode);
-		}
-		/*********************************************************
-		 * Point to next Area.
-		 *********************************************************/
+	while ((*TagPtr != VpdEndOfAreaTag) && (DataLen > 0)) {
+		int AreaLen = *(TagPtr + 1) + (*(TagPtr + 2) * 256);	
+		u8 *AreaData  = TagPtr + 3;
+
+		if (*TagPtr == VpdIdStringTag)
+			DevNode->PhbId = iSeries_Parse_PhbId(AreaData, AreaLen);
+		else if (*TagPtr == VpdVendorAreaTag)
+	    		iSeries_Parse_MfgArea(AreaData, AreaLen, DevNode);
+		/* Point to next Area. */
 		TagPtr  = AreaData + AreaLen;
 		DataLen -= AreaLen;
 	}
 }    
 
-/****************************************************************
- *  iSeries_Get_Location_Code(struct iSeries_Device_Node*)      *
- *
- ****************************************************************/
-void  iSeries_Get_Location_Code(struct iSeries_Device_Node* DevNode)
+void iSeries_Get_Location_Code(struct iSeries_Device_Node *DevNode)
 {
-	int  BusVpdLen = 0;
-	u8*  BusVpdPtr = (u8*)kmalloc(BUS_VPDSIZE, GFP_KERNEL);
+	int BusVpdLen = 0;
+	u8 *BusVpdPtr = (u8 *)kmalloc(BUS_VPDSIZE, GFP_KERNEL);
+
 	if (BusVpdPtr == NULL) {
 		printk("PCI: Bus VPD Buffer allocation failure.\n");
 		return;
 	}
-	BusVpdLen  = HvCallPci_getBusVpd(ISERIES_BUS(DevNode),REALADDR(BusVpdPtr),BUS_VPDSIZE);
+	BusVpdLen = HvCallPci_getBusVpd(ISERIES_BUS(DevNode),
+			REALADDR(BusVpdPtr), BUS_VPDSIZE);
 	if (BusVpdLen == 0) {
 		kfree(BusVpdPtr);
 		printk("PCI: Bus VPD Buffer zero length.\n");
 		return;
 	}
-	//printk("PCI: BusVpdPtr: %p, %d\n",BusVpdPtr, BusVpdLen);
-	/*************************************************************/
-	/* Make sure this is what I think it is                      */
-	/*************************************************************/
-	if (*BusVpdPtr != VpdIdStringTag) {               /*0x82     */
+	/* printk("PCI: BusVpdPtr: %p, %d\n",BusVpdPtr, BusVpdLen); */
+	/* Make sure this is what I think it is */
+	if (*BusVpdPtr != VpdIdStringTag) {	/* 0x82 */
 		printk("PCI: Bus VPD Buffer missing starting tag.\n");
 		kfree(BusVpdPtr);
 		return;
 	}
-	/***************************************************************/
-	/***************************************************************/
 	iSeries_Parse_Vpd(BusVpdPtr,BusVpdLen, DevNode);
-	sprintf(DevNode->Location,"Frame%3d, Card %-4s",DevNode->FrameId,DevNode->CardLocation);
+	sprintf(DevNode->Location, "Frame%3d, Card %-4s", DevNode->FrameId,
+			DevNode->CardLocation);
 	kfree(BusVpdPtr);
 }
diff -purN linux-2.5/arch/ppc64/kernel/iSeries_htab.c linuxppc64-2.5/arch/ppc64/kernel/iSeries_htab.c
--- linux-2.5/arch/ppc64/kernel/iSeries_htab.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/iSeries_htab.c	2003-11-21 05:35:46.000000000 +0000
@@ -0,0 +1,197 @@
+/*
+ * iSeries hashtable management.
+ * 	Derived from pSeries_htab.c
+ *
+ * SMP scalability work:
+ *    Copyright (C) 2001 Anton Blanchard <anton@au.ibm.com>, IBM
+ * 
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+#include <asm/machdep.h>
+#include <asm/pgtable.h>
+#include <asm/mmu.h>
+#include <asm/mmu_context.h>
+#include <asm/iSeries/HvCallHpt.h>
+#include <asm/abs_addr.h>
+
+#if 0
+#include <linux/spinlock.h>
+#include <linux/bitops.h>
+#include <linux/threads.h>
+#include <linux/smp.h>
+
+#include <asm/tlbflush.h>
+#include <asm/tlb.h>
+#include <asm/cputable.h>
+#endif
+
+static long iSeries_hpte_insert(unsigned long hpte_group, unsigned long va,
+			 unsigned long prpn, int secondary,
+			 unsigned long hpteflags, int bolted, int large)
+{
+	long slot;
+	HPTE lhpte;
+
+	/*
+	 * The hypervisor tries both primary and secondary.
+	 * If we are being called to insert in the secondary,
+	 * it means we have already tried both primary and secondary,
+	 * so we return failure immediately.
+	 */
+	if (secondary)
+		return -1;
+
+	slot = HvCallHpt_findValid(&lhpte, va >> PAGE_SHIFT);
+	if (lhpte.dw0.dw0.v)
+		panic("select_hpte_slot found entry already valid\n");
+
+	if (slot == -1)	/* No available entry found in either group */
+		return -1;
+
+	if (slot < 0) {		/* MSB set means secondary group */
+		secondary = 1;
+		slot &= 0x7fffffffffffffff;
+	}
+
+	lhpte.dw1.dword1      = 0;
+	lhpte.dw1.dw1.rpn     = physRpn_to_absRpn(prpn);
+	lhpte.dw1.flags.flags = hpteflags;
+
+	lhpte.dw0.dword0      = 0;
+	lhpte.dw0.dw0.avpn    = va >> 23;
+	lhpte.dw0.dw0.h       = secondary;
+	lhpte.dw0.dw0.bolted  = bolted;
+	lhpte.dw0.dw0.v       = 1;
+
+	/* Now fill in the actual HPTE */
+	HvCallHpt_addValidate(slot, secondary, &lhpte);
+
+	return (secondary << 3) | (slot & 7);
+}
+
+static unsigned long iSeries_hpte_getword0(unsigned long slot)
+{
+	unsigned long dword0;
+	HPTE hpte;
+
+	HvCallHpt_get(&hpte, slot);
+	dword0 = hpte.dw0.dword0;
+
+	return dword0;
+}
+
+static long iSeries_hpte_remove(unsigned long hpte_group)
+{
+	unsigned long slot_offset;
+	int i;
+	HPTE lhpte;
+
+	/* Pick a random slot to start at */
+	slot_offset = mftb() & 0x7;
+
+	for (i = 0; i < HPTES_PER_GROUP; i++) {
+		lhpte.dw0.dword0 = 
+			iSeries_hpte_getword0(hpte_group + slot_offset);
+
+		if (!lhpte.dw0.dw0.bolted) {
+			HvCallHpt_invalidateSetSwBitsGet(hpte_group + 
+							 slot_offset, 0, 0);
+			return i;
+		}
+
+		slot_offset++;
+		slot_offset &= 0x7;
+	}
+
+	return -1;
+}
+
+static long iSeries_hpte_updatepp(unsigned long slot, unsigned long newpp,
+				  unsigned long va, int large, int local)
+{
+	HPTE hpte;
+	unsigned long avpn = va >> 23;
+
+	HvCallHpt_get(&hpte, slot);
+	if ((hpte.dw0.dw0.avpn == avpn) && (hpte.dw0.dw0.v)) {
+		HvCallHpt_setPp(slot, newpp);
+		return 0;
+	}
+	return -1;
+}
+
+/*
+ * Functions used to find the PTE for a particular virtual address. 
+ * Only used during boot when bolting pages.
+ *
+ * Input : vpn      : virtual page number
+ * Output: PTE index within the page table of the entry
+ *         -1 on failure
+ */
+static long iSeries_hpte_find(unsigned long vpn)
+{
+	HPTE hpte;
+	long slot;
+
+	/*
+	 * The HvCallHpt_findValid interface is as follows:
+	 * 0xffffffffffffffff : No entry found.
+	 * 0x00000000xxxxxxxx : Entry found in primary group, slot x
+	 * 0x80000000xxxxxxxx : Entry found in secondary group, slot x
+	 */
+	slot = HvCallHpt_findValid(&hpte, vpn); 
+	if (hpte.dw0.dw0.v) {
+		if (slot < 0) {
+			slot &= 0x7fffffffffffffff;
+			slot = -slot;
+		}
+	} else
+		slot = -1;
+	return slot;
+}
+
+/*
+ * Update the page protection bits. Intended to be used to create
+ * guard pages for kernel data structures on pages which are bolted
+ * in the HPT. Assumes pages being operated on will not be stolen.
+ * Does not work on large pages.
+ *
+ * No need to lock here because we should be the only user.
+ */
+static void iSeries_hpte_updateboltedpp(unsigned long newpp, unsigned long ea)
+{
+	unsigned long vsid,va,vpn;
+	long slot;
+
+	vsid = get_kernel_vsid(ea);
+	va = (vsid << 28) | (ea & 0x0fffffff);
+	vpn = va >> PAGE_SHIFT;
+	slot = iSeries_hpte_find(vpn); 
+	if (slot == -1)
+		panic("updateboltedpp: Could not find page to bolt\n");
+	HvCallHpt_setPp(slot, newpp);
+}
+
+static void iSeries_hpte_invalidate(unsigned long slot, unsigned long va,
+				    int large, int local)
+{
+	HPTE lhpte;
+	unsigned long avpn = va >> 23;
+
+	lhpte.dw0.dword0 = iSeries_hpte_getword0(slot);
+	
+	if ((lhpte.dw0.dw0.avpn == avpn) && lhpte.dw0.dw0.v)
+		HvCallHpt_invalidateSetSwBitsGet(slot, 0, 0);
+}
+
+void hpte_init_iSeries(void)
+{
+	ppc_md.hpte_invalidate	= iSeries_hpte_invalidate;
+	ppc_md.hpte_updatepp	= iSeries_hpte_updatepp;
+	ppc_md.hpte_updateboltedpp = iSeries_hpte_updateboltedpp;
+	ppc_md.hpte_insert	= iSeries_hpte_insert;
+	ppc_md.hpte_remove     	= iSeries_hpte_remove;
+}
diff -purN linux-2.5/arch/ppc64/kernel/iSeries_irq.c linuxppc64-2.5/arch/ppc64/kernel/iSeries_irq.c
--- linux-2.5/arch/ppc64/kernel/iSeries_irq.c	2003-05-02 17:21:54.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/iSeries_irq.c	2003-12-10 04:13:06.000000000 +0000
@@ -40,219 +40,177 @@
 #include <asm/iSeries/iSeries_irq.h>
 #include <asm/iSeries/XmPciLpEvent.h>
 
-
-hw_irq_controller iSeries_IRQ_handler = {
-	"iSeries irq controller",
-	iSeries_startup_IRQ,	/* startup */
-	iSeries_shutdown_IRQ,	/* shutdown */
-	iSeries_enable_IRQ,	/* enable */
-	iSeries_disable_IRQ,	/* disable */
-	NULL,			/* ack  */
-	iSeries_end_IRQ,	/* end  */
-	NULL			/* set_affinity */
-};
-
-
-struct iSeries_irqEntry {
-	u32 dsa;
-	struct iSeries_irqEntry* next;
-};
-
-struct iSeries_irqAnchor {
-	u8  valid : 1;
-	u8  reserved : 7;
-	u16  entryCount;
-	struct iSeries_irqEntry* head;
+static unsigned int iSeries_startup_IRQ(unsigned int irq);
+static void iSeries_shutdown_IRQ(unsigned int irq);
+static void iSeries_enable_IRQ(unsigned int irq);
+static void iSeries_disable_IRQ(unsigned int irq);
+static void iSeries_end_IRQ(unsigned int irq);
+
+static hw_irq_controller iSeries_IRQ_handler = {
+	.typename = "iSeries irq controller",
+	.startup = iSeries_startup_IRQ,
+	.shutdown = iSeries_shutdown_IRQ,
+	.enable = iSeries_enable_IRQ,
+	.disable = iSeries_disable_IRQ,
+	.end = iSeries_end_IRQ
 };
 
-struct iSeries_irqAnchor iSeries_irqMap[NR_IRQS];
-
-void iSeries_init_irqMap(int irq);
+void iSeries_init_irq_desc(irq_desc_t *desc)
+{
+}
 
-/*  This is called by init_IRQ.  set in ppc_md.init_IRQ by iSeries_setup.c */
+/* This is called by init_IRQ.  set in ppc_md.init_IRQ by iSeries_setup.c */
 void __init iSeries_init_IRQ(void)
 {
-	int i;
-	for (i = 0; i < NR_IRQS; i++) {
-		irq_desc[i].handler = &iSeries_IRQ_handler;
-		irq_desc[i].status = 0;
-		irq_desc[i].status |= IRQ_DISABLED;
-		irq_desc[i].depth = 1;
-		iSeries_init_irqMap(i);
-	}
 	/* Register PCI event handler and open an event path */
-	PPCDBG(PPCDBG_BUSWALK,"Register PCI event handler and open an event path\n");
+	PPCDBG(PPCDBG_BUSWALK,
+			"Register PCI event handler and open an event path\n");
 	XmPciLpEvent_init();
 	return;
 }
 
-/**********************************************************************
- *  Called by iSeries_init_IRQ 
- * Prevent IRQs 0 and 255 from being used.  IRQ 0 appears in
- * uninitialized devices.  IRQ 255 appears in the PCI interrupt
- * line register if a PCI error occurs,
- *********************************************************************/
-void __init iSeries_init_irqMap(int irq)
-{
-	iSeries_irqMap[irq].valid = (irq == 0 || irq == 255)? 0 : 1;
-	iSeries_irqMap[irq].entryCount = 0;
-	iSeries_irqMap[irq].head = NULL;
-}
-
-/* This is called out of iSeries_scan_slot to allocate an IRQ for an EADS slot */
-/* It calculates the irq value for the slot.                                   */
-int __init iSeries_allocate_IRQ(HvBusNumber busNumber, HvSubBusNumber subBusNumber, HvAgentId deviceId)
+/*
+ * This is called out of iSeries_scan_slot to allocate an IRQ for an EADS slot
+ * It calculates the irq value for the slot.
+ * Note that subBusNumber is always 0 (at the moment at least).
+ */
+int __init iSeries_allocate_IRQ(HvBusNumber busNumber,
+		HvSubBusNumber subBusNumber, HvAgentId deviceId)
 {
 	u8 idsel = (deviceId >> 4);
-	u8 function = deviceId & 0x0F;
-	int irq = ((((busNumber-1)*16 + (idsel-1)*8 + function)*9/8) % 253) + 2;
-	return irq;
+	u8 function = deviceId & 7;
+
+	return ((busNumber - 1) << 6) + ((idsel - 1) << 3) + function + 1;
 }
 
-/* This is called out of iSeries_scan_slot to assign the EADS slot to its IRQ number */
-int __init iSeries_assign_IRQ(int irq, HvBusNumber busNumber, HvSubBusNumber subBusNumber, HvAgentId deviceId)
+#define IRQ_TO_BUS(irq)		(((((irq) - 1) >> 6) & 0xff) + 1)
+#define IRQ_TO_IDSEL(irq)	(((((irq) - 1) >> 3) & 7) + 1)
+#define IRQ_TO_FUNC(irq)	(((irq) - 1) & 7)
+
+/*
+ * This is called out of iSeries_scan_slot to assign the EADS slot
+ * to its IRQ number
+ */
+int __init iSeries_assign_IRQ(int irq, HvBusNumber busNumber,
+		HvSubBusNumber subBusNumber, HvAgentId deviceId)
 {
-	int rc;
-	u32 dsa = (busNumber << 16) | (subBusNumber << 8) | deviceId;
-	struct iSeries_irqEntry* newEntry;
-	unsigned long flags;
+	irq_desc_t *desc = get_real_irq_desc(irq);
 
-	if (irq < 0 || irq >= NR_IRQS) {
+	if (desc == NULL)
 		return -1;
-	}
-	newEntry = kmalloc(sizeof(*newEntry), GFP_KERNEL);
-	if (newEntry == NULL) {
-		return -ENOMEM;
-	}
-	newEntry->dsa  = dsa;
-	newEntry->next = NULL;
-	/********************************************************************
-	* Probably not necessary to lock the irq since allocation is only 
-	* done during buswalk, but it should not hurt anything except a 
-	* little performance to be smp safe.
-	*******************************************************************/
-	spin_lock_irqsave(&irq_desc[irq].lock, flags);
-
-	if (iSeries_irqMap[irq].valid) {
-		/* Push the new element onto the irq stack */
-		newEntry->next = iSeries_irqMap[irq].head;
-		iSeries_irqMap[irq].head = newEntry;
-		++iSeries_irqMap[irq].entryCount;
-		rc = 0;
-		PPCDBG(PPCDBG_BUSWALK,"iSeries_assign_IRQ   0x%04X.%02X.%02X = 0x%04X\n",busNumber, subBusNumber, deviceId, irq);
-	}
-	else {
-		printk("PCI: Something is wrong with the iSeries_irqMap. \n");
-		kfree(newEntry);
-		rc = -1;
-    }
-	spin_unlock_irqrestore(&irq_desc[irq].lock, flags);
-	return rc;
+	desc->handler = &iSeries_IRQ_handler;
+	return 0;
 }
 
 
 /* This is called by iSeries_activate_IRQs */
-unsigned int iSeries_startup_IRQ(unsigned int irq)
+static unsigned int iSeries_startup_IRQ(unsigned int irq)
 {
-	struct iSeries_irqEntry* entry;
-	u32 bus, subBus, deviceId, function, mask;
-	for(entry=iSeries_irqMap[irq].head; entry!=NULL; entry=entry->next) {
-		bus      = (entry->dsa >> 16) & 0xFFFF;
-		subBus   = (entry->dsa >> 8) & 0xFF;
-		deviceId = entry->dsa & 0xFF;
-		function = deviceId & 0x0F;
-		/* Link the IRQ number to the bridge */
-		HvCallXm_connectBusUnit(bus, subBus, deviceId, irq);
-        	/* Unmask bridge interrupts in the FISR */
-		mask = 0x01010000 << function;
-		HvCallPci_unmaskFisr(bus, subBus, deviceId, mask);
-		PPCDBG(PPCDBG_BUSWALK,"iSeries_activate_IRQ 0x%02X.%02X.%02X  Irq:0x%02X\n",bus,subBus,deviceId,irq);
-	}
+	u32 bus, deviceId, function, mask;
+	const u32 subBus = 0;
+
+	bus = IRQ_TO_BUS(irq);
+	function = IRQ_TO_FUNC(irq);
+	deviceId = (IRQ_TO_IDSEL(irq) << 4) + function;
+
+	/* Link the IRQ number to the bridge */
+	HvCallXm_connectBusUnit(bus, subBus, deviceId, irq);
+
+	/* Unmask bridge interrupts in the FISR */
+	mask = 0x01010000 << function;
+	HvCallPci_unmaskFisr(bus, subBus, deviceId, mask);
+	PPCDBG(PPCDBG_BUSWALK, "iSeries_activate_IRQ 0x%02X.%02X.%02X  Irq:0x%02X\n",
+				bus, subBus, deviceId, irq);
 	return 0;
 }
 
-/* This is called out of iSeries_fixup to activate interrupt
- * generation for usable slots                              */
+/*
+ * This is called out of iSeries_fixup to activate interrupt
+ * generation for usable slots
+ */
 void __init iSeries_activate_IRQs()
 {
 	int irq;
 	unsigned long flags;
-	for (irq=0; irq < NR_IRQS; irq++) {
-		spin_lock_irqsave(&irq_desc[irq].lock, flags);
-		irq_desc[irq].handler->startup(irq);
-		spin_unlock_irqrestore(&irq_desc[irq].lock, flags);
+
+	for_each_irq (irq) {
+		irq_desc_t *desc = get_irq_desc(irq);
+
+		if (desc && desc->handler && desc->handler->startup) {
+			spin_lock_irqsave(&desc->lock, flags);
+			desc->handler->startup(irq);
+			spin_unlock_irqrestore(&desc->lock, flags);
+		}
     	}
 }
 
 /*  this is not called anywhere currently */
-void iSeries_shutdown_IRQ(unsigned int irq) {
-	struct iSeries_irqEntry* entry;
-	u32 bus, subBus, deviceId, function, mask;
+static void iSeries_shutdown_IRQ(unsigned int irq)
+{
+	u32 bus, deviceId, function, mask;
+	const u32 subBus = 0;
 
 	/* irq should be locked by the caller */
+	bus = IRQ_TO_BUS(irq);
+	function = IRQ_TO_FUNC(irq);
+	deviceId = (IRQ_TO_IDSEL(irq) << 4) + function;
 
-	for (entry=iSeries_irqMap[irq].head; entry; entry=entry->next) {
-		bus = (entry->dsa >> 16) & 0xFFFF;
-		subBus = (entry->dsa >> 8) & 0xFF;
-		deviceId = entry->dsa & 0xFF;
-		function = deviceId & 0x0F;
-		/* Invalidate the IRQ number in the bridge */
-		HvCallXm_connectBusUnit(bus, subBus, deviceId, 0);
-		/* Mask bridge interrupts in the FISR */
-		mask = 0x01010000 << function;
-		HvCallPci_maskFisr(bus, subBus, deviceId, mask);
-	}
+	/* Invalidate the IRQ number in the bridge */
+	HvCallXm_connectBusUnit(bus, subBus, deviceId, 0);
 
+	/* Mask bridge interrupts in the FISR */
+	mask = 0x01010000 << function;
+	HvCallPci_maskFisr(bus, subBus, deviceId, mask);
 }
 
-/***********************************************************
+/*
  * This will be called by device drivers (via disable_IRQ)
  * to disable INTA in the bridge interrupt status register.
- ***********************************************************/
-void iSeries_disable_IRQ(unsigned int irq)
+ */
+static void iSeries_disable_IRQ(unsigned int irq)
 {
-	struct iSeries_irqEntry* entry;
-	u32 bus, subBus, deviceId, mask;
+	u32 bus, deviceId, function, mask;
+	const u32 subBus = 0;
 
 	/* The IRQ has already been locked by the caller */
-
-	for (entry=iSeries_irqMap[irq].head; entry; entry=entry->next) {
-		bus      = (entry->dsa >> 16) & 0xFFFF;
-		subBus   = (entry->dsa >> 8) & 0xFF;
-		deviceId = entry->dsa & 0xFF;
-		/* Mask secondary INTA   */
-		mask = 0x80000000;
-		HvCallPci_maskInterrupts(bus, subBus, deviceId, mask);
-		PPCDBG(PPCDBG_BUSWALK,"iSeries_disable_IRQ 0x%02X.%02X.%02X 0x%04X\n",bus,subBus,deviceId,irq);
-    	}
+	bus = IRQ_TO_BUS(irq);
+	function = IRQ_TO_FUNC(irq);
+	deviceId = (IRQ_TO_IDSEL(irq) << 4) + function;
+
+	/* Mask secondary INTA   */
+	mask = 0x80000000;
+	HvCallPci_maskInterrupts(bus, subBus, deviceId, mask);
+	PPCDBG(PPCDBG_BUSWALK, "iSeries_disable_IRQ 0x%02X.%02X.%02X 0x%04X\n",
+	       bus, subBus, deviceId, irq);
 }
 
-/***********************************************************
+/*
  * This will be called by device drivers (via enable_IRQ)
  * to enable INTA in the bridge interrupt status register.
- ***********************************************************/
-void iSeries_enable_IRQ(unsigned int irq)
+ */
+static void iSeries_enable_IRQ(unsigned int irq)
 {
-	struct iSeries_irqEntry* entry;
-	u32 bus, subBus, deviceId, mask;
+	u32 bus, deviceId, function, mask;
+	const u32 subBus = 0;
 
 	/* The IRQ has already been locked by the caller */
-	for (entry=iSeries_irqMap[irq].head; entry; entry=entry->next) {
-		bus      = (entry->dsa >> 16) & 0xFFFF;
-		subBus   = (entry->dsa >> 8) & 0xFF;
-		deviceId = entry->dsa & 0xFF;
-		/* Unmask secondary INTA */
-		mask = 0x80000000;
-		HvCallPci_unmaskInterrupts(bus, subBus, deviceId, mask);
-		PPCDBG(PPCDBG_BUSWALK,"iSeries_enable_IRQ 0x%02X.%02X.%02X 0x%04X\n",bus,subBus,deviceId,irq);
-	}
-}
-
-/* Need to define this so ppc_irq_dispatch_handler will NOT call
-   enable_IRQ at the end of interrupt handling.  However, this
-   does nothing because there is not enough information provided
-   to do the EOI HvCall.  This is done by XmPciLpEvent.c */
-void iSeries_end_IRQ(unsigned int irq)
+	bus = IRQ_TO_BUS(irq);
+	function = IRQ_TO_FUNC(irq);
+	deviceId = (IRQ_TO_IDSEL(irq) << 4) + function;
+
+	/* Unmask secondary INTA */
+	mask = 0x80000000;
+	HvCallPci_unmaskInterrupts(bus, subBus, deviceId, mask);
+	PPCDBG(PPCDBG_BUSWALK, "iSeries_enable_IRQ 0x%02X.%02X.%02X 0x%04X\n",
+	       bus, subBus, deviceId, irq);
+}
+
+/*
+ * Need to define this so ppc_irq_dispatch_handler will NOT call
+ * enable_IRQ at the end of interrupt handling.  However, this does
+ * nothing because there is not enough information provided to do
+ * the EOI HvCall.  This is done by XmPciLpEvent.c
+ */
+static void iSeries_end_IRQ(unsigned int irq)
 {
 }
-
diff -purN linux-2.5/arch/ppc64/kernel/iSeries_pci.c linuxppc64-2.5/arch/ppc64/kernel/iSeries_pci.c
--- linux-2.5/arch/ppc64/kernel/iSeries_pci.c	2003-06-04 14:34:46.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/iSeries_pci.c	2003-12-17 06:00:41.000000000 +0000
@@ -1,3 +1,4 @@
+#define PCIFR(...)
 /*
  * iSeries_pci.c
  *
@@ -35,7 +36,6 @@
 #include <asm/pci-bridge.h>
 #include <asm/ppcdebug.h>
 #include <asm/naca.h>
-#include <asm/flight_recorder.h>
 #include <asm/pci_dma.h>
 
 #include <asm/iSeries/HvCallPci.h>
@@ -49,856 +49,811 @@
 #include "iSeries_IoMmTable.h"
 #include "pci.h"
 
-extern struct pci_controller* hose_head;
-extern struct pci_controller** hose_tail;
-extern int    global_phb_number;
-extern int    panic_timeout;
+extern int panic_timeout;
 
-extern struct device_node *allnodes;
 extern unsigned long iSeries_Base_Io_Memory;    
 
-extern struct pci_ops iSeries_pci_ops;
-extern struct flightRecorder* PciFr;
-extern struct TceTable* tceTables[256];
-
-/*******************************************************************
- * Counters and control flags. 
- *******************************************************************/
-extern long   Pci_Io_Read_Count;
-extern long   Pci_Io_Write_Count;
-extern long   Pci_Cfg_Read_Count;
-extern long   Pci_Cfg_Write_Count;
-extern long   Pci_Error_Count;
-
-extern int    Pci_Retry_Max;	
-extern int    Pci_Error_Flag;
-extern int    Pci_Trace_Flag;
+extern struct TceTable *tceTables[256];
 
 extern void iSeries_MmIoTest(void);
 
-
-/*******************************************************************
+/*
  * Forward declares of prototypes. 
- *******************************************************************/
-struct iSeries_Device_Node* find_Device_Node(struct pci_dev* PciDev);
-struct iSeries_Device_Node* get_Device_Node(struct pci_dev* PciDev);
-
-unsigned long find_and_init_phbs(void);
-struct        pci_controller* alloc_phb(struct device_node *dev, char *model, unsigned int addr_size_words) ;
-
-void  iSeries_Scan_PHBs_Slots(struct pci_controller* Phb);
-void  iSeries_Scan_EADs_Bridge(HvBusNumber Bus, HvSubBusNumber SubBus, int IdSel);
-int   iSeries_Scan_Bridge_Slot(HvBusNumber Bus, struct HvCallPci_BridgeInfo* Info);
-void  list_device_nodes(void);
-
-struct pci_dev;
+ */
+static struct iSeries_Device_Node *find_Device_Node(int bus, int devfn);
+static void iSeries_Scan_PHBs_Slots(struct pci_controller *Phb);
+static void iSeries_Scan_EADs_Bridge(HvBusNumber Bus, HvSubBusNumber SubBus,
+		int IdSel);
+static int iSeries_Scan_Bridge_Slot(HvBusNumber Bus,
+		struct HvCallPci_BridgeInfo *Info);
 
 LIST_HEAD(iSeries_Global_Device_List);
 
-int DeviceCount = 0;
-
+static int DeviceCount;
 
 /* Counters and control flags. */
-static long   Pci_Io_Read_Count  = 0;
-static long   Pci_Io_Write_Count = 0;
-static long   Pci_Cfg_Read_Count = 0;
-static long   Pci_Cfg_Write_Count= 0;
-static long   Pci_Error_Count    = 0;
-
-static int    Pci_Retry_Max      = 3;	/* Only retry 3 times  */	
-static int    Pci_Error_Flag     = 1;	/* Set Retry Error on. */
-static int    Pci_Trace_Flag     = 0;
+static long Pci_Io_Read_Count;
+static long Pci_Io_Write_Count;
+#if 0
+static long Pci_Cfg_Read_Count;
+static long Pci_Cfg_Write_Count;
+#endif
+static long Pci_Error_Count;
+
+static int Pci_Retry_Max = 3;	/* Only retry 3 times  */	
+static int Pci_Error_Flag = 1;	/* Set Retry Error on. */
 
+static struct pci_ops iSeries_pci_ops;
 
-/**********************************************************************************
+/*
  * Log Error infor in Flight Recorder to system Console.
  * Filter out the device not there errors.
  * PCI: EADs Connect Failed 0x18.58.10 Rc: 0x00xx
  * PCI: Read Vendor Failed 0x18.58.10 Rc: 0x00xx
  * PCI: Connect Bus Unit Failed 0x18.58.10 Rc: 0x00xx
- **********************************************************************************/
-void  pci_Log_Error(char* Error_Text, int Bus, int SubBus, int AgentId, int HvRc)
+ */
+static void pci_Log_Error(char *Error_Text, int Bus, int SubBus,
+		int AgentId, int HvRc)
 {
-	if( HvRc != 0x0302) { 
-		char ErrorString[128];
-		sprintf(ErrorString,"%s Failed: 0x%02X.%02X.%02X Rc: 0x%04X",Error_Text,Bus,SubBus,AgentId,HvRc);
-		PCIFR(ErrorString);
-		printk("PCI: %s\n",ErrorString);
-	}
+	if (HvRc == 0x0302)
+		return;
+
+	printk(KERN_ERR "PCI: %s Failed: 0x%02X.%02X.%02X Rc: 0x%04X",
+	       Error_Text, Bus, SubBus, AgentId, HvRc);
 }
 
-/**********************************************************************************
+#if 0
+/*
  * Dump the iSeries Temp Device Node 
- *<4>buswalk [swapper : - DeviceNode: 0xC000000000634300
- *<4>00. Device Node   = 0xC000000000634300
- *<4>    - PciDev      = 0x0000000000000000
- *<4>    - tDevice     = 0x  17:01.00  0x1022 00
- *<4>  4. Device Node = 0xC000000000634480
- *<4>     - PciDev    = 0x0000000000000000
- *<4>     - Device    = 0x  18:38.16 Irq:0xA7 Vendor:0x1014  Flags:0x00
- *<4>     - Devfn     = 0xB0: 22.18
- **********************************************************************************/
-void dumpDevice_Node(struct iSeries_Device_Node* DevNode)
+ * <4>buswalk [swapper : - DeviceNode: 0xC000000000634300
+ * <4>00. Device Node   = 0xC000000000634300
+ * <4>    - PciDev      = 0x0000000000000000
+ * <4>    - tDevice     = 0x  17:01.00  0x1022 00
+ * <4>  4. Device Node = 0xC000000000634480
+ * <4>     - PciDev    = 0x0000000000000000
+ * <4>     - Device    = 0x  18:38.16 Irq:0xA7 Vendor:0x1014  Flags:0x00
+ * <4>     - Devfn     = 0xB0: 22.18
+ */
+void dumpDevice_Node(struct iSeries_Device_Node *DevNode)
 {
-	udbg_printf("Device Node      = 0x%p\n",DevNode);
-	udbg_printf("     - PciDev    = 0x%p\n",DevNode->PciDev);
+	udbg_printf("Device Node      = 0x%p\n", DevNode);
+	udbg_printf("     - PciDev    = 0x%p\n", DevNode->PciDev);
 	udbg_printf("     - Device    = 0x%4X:%02X.%02X (0x%02X)\n",
-		    ISERIES_BUS(DevNode),
-		    ISERIES_SUBBUS(DevNode),
-		    DevNode->AgentId,
-		    DevNode->DevFn);
-	udbg_printf("     - LSlot     = 0x%02X\n",DevNode->LogicalSlot);
-	udbg_printf("     - TceTable  = 0x%p\n  ",DevNode->DevTceTable);
-
-	udbg_printf("     - DSA       = 0x%04X\n",ISERIES_DSA(DevNode)>>32 );
-
+			ISERIES_BUS(DevNode), ISERIES_SUBBUS(DevNode),
+			DevNode->AgentId, DevNode->DevFn);
+	udbg_printf("     - LSlot     = 0x%02X\n", DevNode->LogicalSlot);
+	udbg_printf("     - TceTable  = 0x%p\n  ", DevNode->DevTceTable);
+	udbg_printf("     - DSA       = 0x%04X\n", ISERIES_DSA(DevNode) >> 32);
 	udbg_printf("                 = Irq:0x%02X Vendor:0x%04X  Flags:0x%02X\n",
-		    DevNode->Irq,
-		    DevNode->Vendor,
-		    DevNode->Flags );
-	udbg_printf("     - Location  = %s\n",DevNode->CardLocation);
-
-
+			DevNode->Irq, DevNode->Vendor, DevNode->Flags);
+	udbg_printf("     - Location  = %s\n", DevNode->CardLocation);
 }
-/**********************************************************************************
+
+/*
  * Walk down the device node chain 
- **********************************************************************************/
-void  list_device_nodes(void)
+ */
+static void list_device_nodes(void)
 {
-	struct list_head* Device_Node_Ptr = iSeries_Global_Device_List.next;
-	while(Device_Node_Ptr != &iSeries_Global_Device_List) {
-		dumpDevice_Node( (struct iSeries_Device_Node*)Device_Node_Ptr );
+	struct list_head *Device_Node_Ptr = iSeries_Global_Device_List.next;
+
+	while (Device_Node_Ptr != &iSeries_Global_Device_List) {
+		dumpDevice_Node((struct iSeries_Device_Node*)Device_Node_Ptr);
 		Device_Node_Ptr = Device_Node_Ptr->next;
 	}
 }
-	
+#endif
 
-/***********************************************************************
+/*
  * build_device_node(u16 Bus, int SubBus, u8 DevFn)
- *
- ***********************************************************************/
-struct iSeries_Device_Node* build_device_node(HvBusNumber Bus, HvSubBusNumber  SubBus, int AgentId, int Function)
+ */
+static struct iSeries_Device_Node *build_device_node(HvBusNumber Bus,
+		HvSubBusNumber SubBus, int AgentId, int Function)
 {
-	struct iSeries_Device_Node*  DeviceNode;
-
-	PPCDBG(PPCDBG_BUSWALK,"-build_device_node 0x%02X.%02X.%02X Function: %02X\n",Bus,SubBus,AgentId, Function);
+	struct iSeries_Device_Node *node;
 
-	DeviceNode = kmalloc(sizeof(struct iSeries_Device_Node), GFP_KERNEL);
-	if(DeviceNode == NULL) return NULL;
-
-	memset(DeviceNode,0,sizeof(struct iSeries_Device_Node) );
-	list_add_tail(&DeviceNode->Device_List,&iSeries_Global_Device_List);
-	/*DeviceNode->DsaAddr      = ((u64)Bus<<48)+((u64)SubBus<<40)+((u64)0x10<<32); */
-	ISERIES_BUS(DeviceNode)       = Bus;
-	ISERIES_SUBBUS(DeviceNode)    = SubBus;
-	DeviceNode->DsaAddr.deviceId  = 0x10;
-        DeviceNode->DsaAddr.barNumber = 0;
-	DeviceNode->AgentId           = AgentId;
-	DeviceNode->DevFn             = PCI_DEVFN(ISERIES_ENCODE_DEVICE(AgentId),Function );
-	DeviceNode->IoRetry           = 0;
-	iSeries_Get_Location_Code(DeviceNode);
-	PCIFR("Device 0x%02X.%2X, Node:0x%p ",ISERIES_BUS(DeviceNode),ISERIES_DEVFUN(DeviceNode),DeviceNode);
-	return DeviceNode;
-}
-/****************************************************************************
-* 
-* Allocate pci_controller(phb) initialized common variables. 
-* 
-*****************************************************************************/
-struct pci_controller* pci_alloc_pci_controllerX(char *model, enum phb_types controller_type)
-{
-	struct pci_controller *hose;
-	hose = (struct pci_controller*)kmalloc(sizeof(struct pci_controller), GFP_KERNEL);
-	if(hose == NULL) return NULL;
-
-	memset(hose, 0, sizeof(struct pci_controller));
-	if(strlen(model) < 8) strcpy(hose->what,model);
-	else                  memcpy(hose->what,model,7);
-	hose->type = controller_type;
-	hose->global_number = global_phb_number;
-	global_phb_number++;
-
-	*hose_tail = hose;
-	hose_tail = &hose->next;
-	return hose;
+	PPCDBG(PPCDBG_BUSWALK,
+			"-build_device_node 0x%02X.%02X.%02X Function: %02X\n",
+			Bus, SubBus, AgentId, Function);
+
+	node = kmalloc(sizeof(struct iSeries_Device_Node), GFP_KERNEL);
+	if (node == NULL)
+		return NULL;
+
+	memset(node, 0, sizeof(struct iSeries_Device_Node));
+	list_add_tail(&node->Device_List, &iSeries_Global_Device_List);
+#if 0
+	node->DsaAddr = ((u64)Bus << 48) + ((u64)SubBus << 40) + ((u64)0x10 << 32);
+#endif
+	node->DsaAddr.DsaAddr = 0;
+	node->DsaAddr.Dsa.busNumber = Bus;
+	node->DsaAddr.Dsa.subBusNumber = SubBus;
+	node->DsaAddr.Dsa.deviceId = 0x10;
+	node->AgentId = AgentId;
+	node->DevFn = PCI_DEVFN(ISERIES_ENCODE_DEVICE(AgentId), Function);
+	node->IoRetry = 0;
+	iSeries_Get_Location_Code(node);
+	PCIFR("Device 0x%02X.%2X, Node:0x%p ", ISERIES_BUS(node),
+			ISERIES_DEVFUN(node), node);
+	return node;
 }
 
-/****************************************************************************
- *
- * unsigned int __init find_and_init_phbs(void)
+/*
+ * unsigned long __init find_and_init_phbs(void)
  *
  * Description:
  *   This function checks for all possible system PCI host bridges that connect
  *   PCI buses.  The system hypervisor is queried as to the guest partition
- *   ownership status.  A pci_controller is build for any bus which is partially
+ *   ownership status.  A pci_controller is built for any bus which is partially
  *   owned or fully owned by this guest partition.
- ****************************************************************************/
+ */
 unsigned long __init find_and_init_phbs(void)
 {
-	struct      pci_controller* phb;
-	HvBusNumber BusNumber;
+	struct pci_controller *phb;
+	HvBusNumber bus;
 
-	PPCDBG(PPCDBG_BUSWALK,"find_and_init_phbs Entry\n");
+	PPCDBG(PPCDBG_BUSWALK, "find_and_init_phbs Entry\n");
 
 	/* Check all possible buses. */
-	for (BusNumber = 0; BusNumber < 256; BusNumber++) {
-		int RtnCode = HvCallXm_testBus(BusNumber);
-		if (RtnCode == 0) {
-			phb = pci_alloc_pci_controllerX("PHB HV", phb_type_hypervisor);
-			if(phb == NULL) {
-				printk("PCI: Allocate pci_controller failed.\n");
-				PCIFR(      "Allocate pci_controller failed.");
+	for (bus = 0; bus < 256; bus++) {
+		int ret = HvCallXm_testBus(bus);
+		if (ret == 0) {
+			printk("bus %d appears to exist\n", bus);
+			phb = pci_alloc_pci_controller(phb_type_hypervisor);
+			if (phb == NULL) {
+				PCIFR("Allocate pci_controller failed.");
 				return -1;
 			}
-			phb->pci_mem_offset = phb->local_number = BusNumber;
-			phb->first_busno  = BusNumber;
-			phb->last_busno   = BusNumber;
-			phb->ops          = &iSeries_pci_ops;
-
-			PPCDBG(PPCDBG_BUSWALK, "PCI:Create iSeries pci_controller(%p), Bus: %04X\n",phb,BusNumber);
-			PCIFR("Create iSeries PHB controller: %04X",BusNumber);
-
-			/***************************************************/
-			/* Find and connect the devices.                   */
-			/***************************************************/
+			phb->pci_mem_offset = phb->local_number = bus;
+			phb->first_busno = bus;
+			phb->last_busno = bus;
+			phb->ops = &iSeries_pci_ops;
+
+			PPCDBG(PPCDBG_BUSWALK, "PCI:Create iSeries pci_controller(%p), Bus: %04X\n",
+					phb, bus);
+			PCIFR("Create iSeries PHB controller: %04X", bus);
+
+			/* Find and connect the devices. */
 			iSeries_Scan_PHBs_Slots(phb);
 		}
-		/* Check for Unexpected Return code, a clue that something */
-		/* has gone wrong.                                         */
-		else if(RtnCode != 0x0301) {
-			PCIFR("Unexpected Return on Probe(0x%04X): 0x%04X",BusNumber,RtnCode);
-		}
-
+		/*
+		 * Check for Unexpected Return code, a clue that something
+		 * has gone wrong.
+		 */
+		else if (ret != 0x0301)
+			printk(KERN_ERR "Unexpected Return on Probe(0x%04X): 0x%04X",
+			       bus, ret);
 	}
 	return 0;
 }
-/*********************************************************************** 
+
+/*
  * iSeries_pcibios_init
  *  
  * Chance to initialize and structures or variable before PCI Bus walk.
- *  
- *<4>buswalk [swapper : iSeries_pcibios_init Entry.
- *<4>buswalk [swapper : IoMmTable Initialized 0xC00000000034BD30
- *<4>buswalk [swapper : find_and_init_phbs Entry
- *<4>buswalk [swapper : Create iSeries pci_controller:(0xC00000001F5C7000), Bus 0x0017
- *<4>buswalk [swapper : Connect EADs: 0x17.00.12 = 0x00
- *<4>buswalk [swapper : iSeries_assign_IRQ   0x0017.00.12 = 0x0091
- *<4>buswalk [swapper : - allocate and assign IRQ 0x17.00.12 = 0x91
- *<4>buswalk [swapper : - FoundDevice: 0x17.28.10 = 0x12AE
- *<4>buswalk [swapper : - build_device_node 0x17.28.12
- *<4>buswalk [swapper : iSeries_pcibios_init Exit.
- ***********************************************************************/
+ */
 void iSeries_pcibios_init(void)
 {
-	PPCDBG(PPCDBG_BUSWALK,"iSeries_pcibios_init Entry.\n"); 
-
+	PPCDBG(PPCDBG_BUSWALK, "iSeries_pcibios_init Entry.\n"); 
 	iSeries_IoMmTable_Initialize();
-
 	find_and_init_phbs();
-
-	pci_assign_all_busses = 0;
-	PPCDBG(PPCDBG_BUSWALK,"iSeries_pcibios_init Exit.\n"); 
+	/* pci_assign_all_busses = 0;		SFRXXX*/
+	PPCDBG(PPCDBG_BUSWALK, "iSeries_pcibios_init Exit.\n"); 
 }
-/***********************************************************************
+
+/*
  * pcibios_final_fixup(void)  
- ***********************************************************************/
+ */
 void __init pcibios_final_fixup(void)
 {
-	struct pci_dev* PciDev = NULL;
-	struct iSeries_Device_Node* DeviceNode;
-	char   Buffer[256];
-    	int    DeviceCount = 0;
+	struct pci_dev *pdev = NULL;
+	struct iSeries_Device_Node *node;
+	char Buffer[256];
+    	int DeviceCount = 0;
+
+	PPCDBG(PPCDBG_BUSWALK, "iSeries_pcibios_fixup Entry.\n"); 
 
-	PPCDBG(PPCDBG_BUSWALK,"iSeries_pcibios_fixup Entry.\n"); 
-	/******************************************************/
 	/* Fix up at the device node and pci_dev relationship */
-	/******************************************************/
 	mf_displaySrc(0xC9000100);
 
-	while ((PciDev = pci_find_device(PCI_ANY_ID, PCI_ANY_ID, PciDev)) != NULL) {
-		DeviceNode = find_Device_Node(PciDev);
-		if(DeviceNode != NULL) {
-			++DeviceCount;
-			PciDev->sysdata    = (void*)DeviceNode;
-			DeviceNode->PciDev = PciDev;
-
-			PPCDBG(PPCDBG_BUSWALK,"PciDev 0x%p <==> DevNode 0x%p\n",PciDev,DeviceNode );
-
-			iSeries_allocateDeviceBars(PciDev);
-
-			iSeries_Device_Information(PciDev,Buffer, sizeof(Buffer) );
-			printk("%d. %s\n",DeviceCount,Buffer);
+	printk("pcibios_final_fixup\n");
+	while ((pdev = pci_find_device(PCI_ANY_ID, PCI_ANY_ID, pdev))
+			!= NULL) {
+		node = find_Device_Node(pdev->bus->number, pdev->devfn);
+		printk("pci dev %p (%x.%x), node %p\n", pdev,
+		       pdev->bus->number, pdev->devfn, node);
 
-			create_pci_bus_tce_table((unsigned long)DeviceNode);
-		} else {
-			printk("PCI: Device Tree not found for 0x%016lX\n",(unsigned long)PciDev);
-		}
+		if (node != NULL) {
+			++DeviceCount;
+			pdev->sysdata = (void *)node;
+			node->PciDev = pdev;
+			PPCDBG(PPCDBG_BUSWALK,
+					"pdev 0x%p <==> DevNode 0x%p\n",
+					pdev, node);
+			iSeries_allocateDeviceBars(pdev);
+			iSeries_Device_Information(pdev, Buffer,
+					sizeof(Buffer));
+			printk("%d. %s\n", DeviceCount, Buffer);
+			create_pci_bus_tce_table((unsigned long)node);
+		} else
+			printk("PCI: Device Tree not found for 0x%016lX\n",
+					(unsigned long)pdev);
 	}
 	iSeries_IoMmTable_Status();
-
 	iSeries_activate_IRQs();
-
 	mf_displaySrc(0xC9000200);
 }
 
-void pcibios_fixup_bus(struct pci_bus* PciBus)
+void pcibios_fixup_bus(struct pci_bus *PciBus)
 {
-	PPCDBG(PPCDBG_BUSWALK,"iSeries_pcibios_fixup_bus(0x%04X) Entry.\n",PciBus->number); 
-
+	PPCDBG(PPCDBG_BUSWALK, "iSeries_pcibios_fixup_bus(0x%04X) Entry.\n",
+			PciBus->number); 
 }
 
-
-/***********************************************************************
- * pcibios_fixup_resources(struct pci_dev *dev) 
- *	
- ***********************************************************************/
-void pcibios_fixup_resources(struct pci_dev *PciDev)
+void pcibios_fixup_resources(struct pci_dev *pdev)
 {
-	PPCDBG(PPCDBG_BUSWALK,"pcibios_fixup_resources PciDev %p\n",PciDev);
+	PPCDBG(PPCDBG_BUSWALK, "fixup_resources pdev %p\n", pdev);
 }   
 
+/*
+ * Loop through each node function to find usable EADs bridges.  
+ */
+static void iSeries_Scan_PHBs_Slots(struct pci_controller *Phb)
+{
+	struct HvCallPci_DeviceInfo *DevInfo;
+	HvBusNumber bus = Phb->local_number;	/* System Bus */	
+	const HvSubBusNumber SubBus = 0;	/* EADs is always 0. */
+	int HvRc = 0;
+	int IdSel;	
+	const int MaxAgents = 8;
+
+	DevInfo = (struct HvCallPci_DeviceInfo*)
+		kmalloc(sizeof(struct HvCallPci_DeviceInfo), GFP_KERNEL);
+	if (DevInfo == NULL)
+		return;
 
-/********************************************************************************
-* Loop through each node function to find usable EADs bridges.  
-*********************************************************************************/
-void  iSeries_Scan_PHBs_Slots(struct pci_controller* Phb)
-{
-	struct HvCallPci_DeviceInfo* DevInfo;
-	HvBusNumber    Bus       = Phb->local_number;       /* System Bus        */	
-	HvSubBusNumber SubBus    = 0;                       /* EADs is always 0. */
-	int            HvRc      = 0;
-	int            IdSel     = 1;	
-	int            MaxAgents = 8;
-
-	DevInfo    = (struct HvCallPci_DeviceInfo*)kmalloc(sizeof(struct HvCallPci_DeviceInfo), GFP_KERNEL);
-	if(DevInfo == NULL) return;
-
-	/********************************************************************************
+	/*
 	 * Probe for EADs Bridges      
-	 ********************************************************************************/
-	for (IdSel=1; IdSel < MaxAgents; ++IdSel) {
-    		HvRc = HvCallPci_getDeviceInfo(Bus, SubBus, IdSel,REALADDR(DevInfo), sizeof(struct HvCallPci_DeviceInfo));
+	 */
+	for (IdSel = 1; IdSel < MaxAgents; ++IdSel) {
+    		HvRc = HvCallPci_getDeviceInfo(bus, SubBus, IdSel,
+				REALADDR(DevInfo),
+				sizeof(struct HvCallPci_DeviceInfo));
 		if (HvRc == 0) {
-			if(DevInfo->deviceType == HvCallPci_NodeDevice) {
-				iSeries_Scan_EADs_Bridge(Bus, SubBus, IdSel);
-			}
-			else printk("PCI: Invalid System Configuration(0x%02X.\n",DevInfo->deviceType);
+			if (DevInfo->deviceType == HvCallPci_NodeDevice)
+				iSeries_Scan_EADs_Bridge(bus, SubBus, IdSel);
+			else
+				printk("PCI: Invalid System Configuration(0x%02X)"
+				       " for bus 0x%02x id 0x%02x.\n",
+				       DevInfo->deviceType, bus, IdSel);
 		}
-		else pci_Log_Error("getDeviceInfo",Bus, SubBus, IdSel,HvRc);
+		else
+			pci_Log_Error("getDeviceInfo", bus, SubBus, IdSel, HvRc);
 	}
 	kfree(DevInfo);
 }
 
-/********************************************************************************
-* 
-*********************************************************************************/
-void  iSeries_Scan_EADs_Bridge(HvBusNumber Bus, HvSubBusNumber SubBus, int IdSel)
-{
-	struct HvCallPci_BridgeInfo* BridgeInfo;
-	HvAgentId      AgentId;
-	int            Function;
-	int            HvRc;
-
-	BridgeInfo = (struct HvCallPci_BridgeInfo*)kmalloc(sizeof(struct HvCallPci_BridgeInfo), GFP_KERNEL);
-	if(BridgeInfo == NULL) return;
-
-	/*********************************************************************
-	 * Note: hvSubBus and irq is always be 0 at this level!
-	 *********************************************************************/
-	for (Function=0; Function < 8; ++Function) {
+static void iSeries_Scan_EADs_Bridge(HvBusNumber bus, HvSubBusNumber SubBus,
+		int IdSel)
+{
+	struct HvCallPci_BridgeInfo *BridgeInfo;
+	HvAgentId AgentId;
+	int Function;
+	int HvRc;
+
+	BridgeInfo = (struct HvCallPci_BridgeInfo *)
+		kmalloc(sizeof(struct HvCallPci_BridgeInfo), GFP_KERNEL);
+	if (BridgeInfo == NULL)
+		return;
+
+	/* Note: hvSubBus and irq is always be 0 at this level! */
+	for (Function = 0; Function < 8; ++Function) {
 	  	AgentId = ISERIES_PCI_AGENTID(IdSel, Function);
-		HvRc = HvCallXm_connectBusUnit(Bus, SubBus, AgentId, 0);
+		HvRc = HvCallXm_connectBusUnit(bus, SubBus, AgentId, 0);
  		if (HvRc == 0) {
+			printk("found device at bus %d idsel %d func %d (AgentId %x)\n",
+			       bus, IdSel, Function, AgentId);
   			/*  Connect EADs: 0x18.00.12 = 0x00 */
-			PPCDBG(PPCDBG_BUSWALK,"PCI:Connect EADs: 0x%02X.%02X.%02X\n",Bus, SubBus, AgentId);
-			PCIFR(                    "Connect EADs: 0x%02X.%02X.%02X",  Bus, SubBus, AgentId);
-	    		HvRc = HvCallPci_getBusUnitInfo(Bus, SubBus, AgentId, 
-			                                REALADDR(BridgeInfo), sizeof(struct HvCallPci_BridgeInfo));
+			PPCDBG(PPCDBG_BUSWALK,
+					"PCI:Connect EADs: 0x%02X.%02X.%02X\n",
+					bus, SubBus, AgentId);
+	    		HvRc = HvCallPci_getBusUnitInfo(bus, SubBus, AgentId,
+					REALADDR(BridgeInfo),
+					sizeof(struct HvCallPci_BridgeInfo));
 	 		if (HvRc == 0) {
-				PPCDBG(PPCDBG_BUSWALK,"PCI: BridgeInfo, Type:0x%02X, SubBus:0x%02X, MaxAgents:0x%02X, MaxSubBus: 0x%02X, LSlot: 0x%02X\n",
-				       BridgeInfo->busUnitInfo.deviceType,
-				       BridgeInfo->subBusNumber,
-				       BridgeInfo->maxAgents,
-				       BridgeInfo->maxSubBusNumber,
-				       BridgeInfo->logicalSlotNumber);
-				PCIFR(                     "BridgeInfo, Type:0x%02X, SubBus:0x%02X, MaxAgents:0x%02X, MaxSubBus: 0x%02X, LSlot: 0x%02X",
-				       BridgeInfo->busUnitInfo.deviceType,
-				       BridgeInfo->subBusNumber,
-				       BridgeInfo->maxAgents,
-				       BridgeInfo->maxSubBusNumber,
-				       BridgeInfo->logicalSlotNumber);
+				printk("bridge info: type %x subbus %x maxAgents %x maxsubbus %x logslot %x\n",
+					BridgeInfo->busUnitInfo.deviceType,
+					BridgeInfo->subBusNumber,
+					BridgeInfo->maxAgents,
+					BridgeInfo->maxSubBusNumber,
+					BridgeInfo->logicalSlotNumber);
+				PPCDBG(PPCDBG_BUSWALK,
+					"PCI: BridgeInfo, Type:0x%02X, SubBus:0x%02X, MaxAgents:0x%02X, MaxSubBus: 0x%02X, LSlot: 0x%02X\n",
+					BridgeInfo->busUnitInfo.deviceType,
+					BridgeInfo->subBusNumber,
+					BridgeInfo->maxAgents,
+					BridgeInfo->maxSubBusNumber,
+					BridgeInfo->logicalSlotNumber);
 
-				if (BridgeInfo->busUnitInfo.deviceType == HvCallPci_BridgeDevice)  {
+				if (BridgeInfo->busUnitInfo.deviceType ==
+						HvCallPci_BridgeDevice)  {
 					/* Scan_Bridge_Slot...: 0x18.00.12 */
-					iSeries_Scan_Bridge_Slot(Bus,BridgeInfo);
-				}
-				else printk("PCI: Invalid Bridge Configuration(0x%02X)",BridgeInfo->busUnitInfo.deviceType);
+					iSeries_Scan_Bridge_Slot(bus, BridgeInfo);
+				} else
+					printk("PCI: Invalid Bridge Configuration(0x%02X)",
+						BridgeInfo->busUnitInfo.deviceType);
 			}
-    		}
-		else if(HvRc != 0x000B) pci_Log_Error("EADs Connect",Bus,SubBus,AgentId,HvRc);
+    		} else if (HvRc != 0x000B)
+			pci_Log_Error("EADs Connect",
+					bus, SubBus, AgentId, HvRc);
 	}
 	kfree(BridgeInfo);
 }
 
-/********************************************************************************
-* 
-* This assumes that the node slot is always on the primary bus!
-*
-*********************************************************************************/
-int iSeries_Scan_Bridge_Slot(HvBusNumber Bus, struct HvCallPci_BridgeInfo* BridgeInfo)
+/*
+ * This assumes that the node slot is always on the primary bus!
+ */
+static int iSeries_Scan_Bridge_Slot(HvBusNumber Bus,
+		struct HvCallPci_BridgeInfo *BridgeInfo)
 {
-	struct iSeries_Device_Node* DeviceNode;
+	struct iSeries_Device_Node *node;
 	HvSubBusNumber SubBus = BridgeInfo->subBusNumber;
-	u16       VendorId    = 0;
-	int       HvRc        = 0;
-	u8        Irq         = 0;
-	int       IdSel       = ISERIES_GET_DEVICE_FROM_SUBBUS(SubBus);
-	int       Function    = ISERIES_GET_FUNCTION_FROM_SUBBUS(SubBus);
-	HvAgentId AgentId     = ISERIES_PCI_AGENTID(IdSel, Function);
-	HvAgentId EADsIdSel   = ISERIES_PCI_AGENTID(IdSel, Function);
-	int       FirstSlotId = 0; 	
-
-	/**********************************************************/
-	/* iSeries_allocate_IRQ.: 0x18.00.12(0xA3)                */
-	/**********************************************************/
-  	Irq   = iSeries_allocate_IRQ(Bus, 0, AgentId);
-	iSeries_assign_IRQ(Irq, Bus, 0, AgentId);
-	PPCDBG(PPCDBG_BUSWALK,"PCI:- allocate and assign IRQ 0x%02X.%02X.%02X = 0x%02X\n",Bus, 0, AgentId, Irq );
+	u16 VendorId = 0;
+	int HvRc = 0;
+	u8 Irq = 0;
+	int IdSel = ISERIES_GET_DEVICE_FROM_SUBBUS(SubBus);
+	int Function = ISERIES_GET_FUNCTION_FROM_SUBBUS(SubBus);
+	HvAgentId EADsIdSel = ISERIES_PCI_AGENTID(IdSel, Function);
+
+	/* iSeries_allocate_IRQ.: 0x18.00.12(0xA3) */
+  	Irq = iSeries_allocate_IRQ(Bus, 0, EADsIdSel);
+	iSeries_assign_IRQ(Irq, Bus, 0, EADsIdSel);
+	PPCDBG(PPCDBG_BUSWALK,
+		"PCI:- allocate and assign IRQ 0x%02X.%02X.%02X = 0x%02X\n",
+		Bus, 0, AgentId, Irq);
 
-	/****************************************************************************
+	/*
 	 * Connect all functions of any device found.  
-	 ****************************************************************************/
+	 */
   	for (IdSel = 1; IdSel <= BridgeInfo->maxAgents; ++IdSel) {
     		for (Function = 0; Function < 8; ++Function) {
-			AgentId = ISERIES_PCI_AGENTID(IdSel, Function);
-			HvRc = HvCallXm_connectBusUnit(Bus, SubBus, AgentId, Irq);
-			if( HvRc == 0) {
-				HvRc = HvCallPci_configLoad16(Bus, SubBus, AgentId, PCI_VENDOR_ID, &VendorId);
-				if( HvRc == 0) {
-					/**********************************************************/
-					/* FoundDevice: 0x18.28.10 = 0x12AE                       */
-					/**********************************************************/
-					PPCDBG(PPCDBG_BUSWALK,"PCI:- FoundDevice: 0x%02X.%02X.%02X = 0x%04X\n",
-					                                       Bus, SubBus, AgentId, VendorId);
-
-					HvRc = HvCallPci_configStore8(Bus, SubBus, AgentId, PCI_INTERRUPT_LINE, Irq);  
-					if( HvRc != 0) {
-						pci_Log_Error("PciCfgStore Irq Failed!",Bus,SubBus,AgentId,HvRc);
-					}
-
-					++DeviceCount;
-					DeviceNode = build_device_node(Bus, SubBus, EADsIdSel, Function);
-					DeviceNode->Vendor      = VendorId;
-					DeviceNode->Irq         = Irq;
-					DeviceNode->LogicalSlot = BridgeInfo->logicalSlotNumber;
-					PCIFR("Device(%4d): 0x%02X.%02X.%02X 0x%02X 0x%04X",
-					      DeviceCount,Bus, SubBus, AgentId,
-					      DeviceNode->LogicalSlot,DeviceNode->Vendor);
-
-					/***********************************************************
-					 * On the first device/function, assign irq to slot
-					 ***********************************************************/
-					if(Function == 0) { 
-						FirstSlotId = AgentId;
-						// AHT iSeries_assign_IRQ(Irq, Bus, SubBus, AgentId);
-    					}
-				}
-				else pci_Log_Error("Read Vendor",Bus,SubBus,AgentId,HvRc);
+			HvAgentId AgentId = ISERIES_PCI_AGENTID(IdSel, Function);
+			HvRc = HvCallXm_connectBusUnit(Bus, SubBus,
+					AgentId, Irq);
+			if (HvRc != 0) {
+				pci_Log_Error("Connect Bus Unit",
+					      Bus, SubBus, AgentId, HvRc);
+				continue;
+			}
+			printk("connected bus unit at bus %d subbus 0x%x agentid 0x%x (idsel=%d func=%d)\n",
+			       Bus, SubBus, AgentId, IdSel, Function);
+
+			HvRc = HvCallPci_configLoad16(Bus, SubBus, AgentId,
+						      PCI_VENDOR_ID, &VendorId);
+			if (HvRc != 0) {
+				pci_Log_Error("Read Vendor",
+					      Bus, SubBus, AgentId, HvRc);
+				continue;
 			}
-			else pci_Log_Error("Connect Bus Unit",Bus,SubBus, AgentId,HvRc);
+			printk("read vendor ID: %x\n", VendorId);
+
+			/* FoundDevice: 0x18.28.10 = 0x12AE */
+			PPCDBG(PPCDBG_BUSWALK,
+			       "PCI:- FoundDevice: 0x%02X.%02X.%02X = 0x%04X\n",
+			       Bus, SubBus, AgentId, VendorId);
+			HvRc = HvCallPci_configStore8(Bus, SubBus, AgentId,
+						      PCI_INTERRUPT_LINE, Irq);  
+			if (HvRc != 0)
+				pci_Log_Error("PciCfgStore Irq Failed!",
+					      Bus, SubBus, AgentId, HvRc);
+
+			++DeviceCount;
+			node = build_device_node(Bus, SubBus, EADsIdSel, Function);
+			node->Vendor = VendorId;
+			node->Irq = Irq;
+			node->LogicalSlot = BridgeInfo->logicalSlotNumber;
+
 		} /* for (Function = 0; Function < 8; ++Function) */
 	} /* for (IdSel = 1; IdSel <= MaxAgents; ++IdSel) */
 	return HvRc;
 }
-/************************************************************************/
-/* I/0 Memory copy MUST use mmio commands on iSeries                    */
-/* To do; For performance, include the hv call directly                 */
-/************************************************************************/
-void* iSeries_memset_io(void* dest, char c, size_t Count)
-{
-	u8    ByteValue     = c;
-	long  NumberOfBytes = Count;
-	char* IoBuffer      = dest;
-	while(NumberOfBytes > 0) {
-		iSeries_Write_Byte( ByteValue, (void*)IoBuffer );
+
+/*
+ * I/0 Memory copy MUST use mmio commands on iSeries
+ * To do; For performance, include the hv call directly
+ */
+void *iSeries_memset_io(void *dest, char c, size_t Count)
+{
+	u8 ByteValue = c;
+	long NumberOfBytes = Count;
+	char *IoBuffer = dest;
+
+	while (NumberOfBytes > 0) {
+		iSeries_Write_Byte(ByteValue, (void *)IoBuffer);
 		++IoBuffer;
 		-- NumberOfBytes;
 	}
 	return dest;
-}	
-void* iSeries_memcpy_toio(void *dest, void *source, size_t count)
+}
+
+void *iSeries_memcpy_toio(void *dest, void *source, size_t count)
 {
-	char *dst           = dest;
-	char *src           = source;
-	long  NumberOfBytes = count;
-	while(NumberOfBytes > 0) {
-		iSeries_Write_Byte(*src++, (void*)dst++);
+	char *dst = dest;
+	char *src = source;
+	long NumberOfBytes = count;
+
+	while (NumberOfBytes > 0) {
+		iSeries_Write_Byte(*src++, (void *)dst++);
 		-- NumberOfBytes;
 	}
 	return dest;
 }
-void* iSeries_memcpy_fromio(void *dest, void *source, size_t count)
+
+void *iSeries_memcpy_fromio(void *dest, void *source, size_t count)
 {
 	char *dst = dest;
 	char *src = source;
-	long  NumberOfBytes = count;
-	while(NumberOfBytes > 0) {
-		*dst++ = iSeries_Read_Byte( (void*)src++);
+	long NumberOfBytes = count;
+
+	while (NumberOfBytes > 0) {
+		*dst++ = iSeries_Read_Byte((void *)src++);
 		-- NumberOfBytes;
 	}
 	return dest;
 }
-/**********************************************************************************
+
+/*
  * Look down the chain to find the matching Device Device
- **********************************************************************************/
-struct iSeries_Device_Node* find_Device_Node(struct pci_dev* PciDev)
+ */
+static struct iSeries_Device_Node *find_Device_Node(int bus, int devfn)
 {
-	struct list_head* Device_Node_Ptr = iSeries_Global_Device_List.next;
-	int Bus   = PciDev->bus->number;
-	int DevFn = PciDev->devfn;
-	
-	while(Device_Node_Ptr != &iSeries_Global_Device_List) { 
-		struct iSeries_Device_Node* DevNode = (struct iSeries_Device_Node*)Device_Node_Ptr;
-		if(Bus == ISERIES_BUS(DevNode) && DevFn == DevNode->DevFn) {
-			return DevNode;
-		}
-		Device_Node_Ptr = Device_Node_Ptr->next;
+	struct list_head *pos;
+
+	list_for_each(pos, &iSeries_Global_Device_List) {
+		struct iSeries_Device_Node *node =
+			list_entry(pos, struct iSeries_Device_Node, Device_List);
+
+		if ((bus == ISERIES_BUS(node)) && (devfn == node->DevFn))
+			return node;
 	}
 	return NULL;
 }
-/******************************************************************/
-/* Returns the device node for the passed pci_dev                 */
-/* Sanity Check Node PciDev to passed pci_dev                     */
-/* If none is found, returns a NULL which the client must handle. */
-/******************************************************************/
-struct iSeries_Device_Node* get_Device_Node(struct pci_dev* PciDev)
-{
-	struct iSeries_Device_Node* Node;
-	Node = (struct iSeries_Device_Node*)PciDev->sysdata;
-	if(Node == NULL ) {
-		Node = find_Device_Node(PciDev);
-	}
-	else if(Node->PciDev != PciDev) { 
-		Node = find_Device_Node(PciDev);
+
+#if 0
+/*
+ * Returns the device node for the passed pci_dev
+ * Sanity Check Node PciDev to passed pci_dev
+ * If none is found, returns a NULL which the client must handle.
+ */
+static struct iSeries_Device_Node *get_Device_Node(struct pci_dev *pdev)
+{
+	struct iSeries_Device_Node *node;
+
+	node = pdev->sysdata;
+	if (node == NULL || node->PciDev != pdev)
+		node = find_Device_Node(pdev->bus->number, pdev->devfn);
+	return node;
+}
+#endif
+
+/*
+ * Config space read and write functions.
+ * For now at least, we look for the device node for the bus and devfn
+ * that we are asked to access.  It may be possible to translate the devfn
+ * to a subbus and deviceid more directly.
+ */
+static u64 hv_cfg_read_func[4]  = {
+	HvCallPciConfigLoad8, HvCallPciConfigLoad16,
+	HvCallPciConfigLoad32, HvCallPciConfigLoad32
+};
+
+static u64 hv_cfg_write_func[4] = {
+	HvCallPciConfigStore8, HvCallPciConfigStore16,
+	HvCallPciConfigStore32, HvCallPciConfigStore32
+};
+
+/*
+ * Read PCI config space
+ */
+static int iSeries_pci_read_config(struct pci_bus *bus, unsigned int devfn,
+		int offset, int size, u32 *val)
+{
+	struct iSeries_Device_Node *node = find_Device_Node(bus->number, devfn);
+	u64 fn;
+	struct HvCallPci_LoadReturn ret;
+
+	if (node == NULL)
+		return PCIBIOS_DEVICE_NOT_FOUND;
+
+	fn = hv_cfg_read_func[(size - 1) & 3];
+	HvCall3Ret16(fn, &ret, node->DsaAddr.DsaAddr, offset, 0);
+
+	if (ret.rc != 0) {
+		*val = ~0;
+		return PCIBIOS_DEVICE_NOT_FOUND;	/* or something */
 	}
-	return Node;
+
+	*val = ret.value;
+	return 0;
 }
-/**********************************************************************************
- *
- * Read PCI Config Space Code 
- *
- **********************************************************************************/
-/** BYTE  *************************************************************************/
-int iSeries_Node_read_config_byte(struct iSeries_Device_Node* DevNode, int Offset, u8* ReadValue)
-{
-	u8  ReadData; 
-	if(DevNode == NULL) { return 0x301; } 
-	++Pci_Cfg_Read_Count;
-	DevNode->ReturnCode = HvCallPci_configLoad8(ISERIES_BUS(DevNode),ISERIES_SUBBUS(DevNode),0x10,
-	                                                Offset,&ReadData);
-	if(Pci_Trace_Flag == 1) {
-		PCIFR("RCB: 0x%04X.%02X 0x%04X = 0x%02X",ISERIES_BUS(DevNode),DevNode->DevFn,Offset,ReadData);
-	}
-	if(DevNode->ReturnCode != 0 ) { 
-		printk("PCI: RCB: 0x%04X.%02X  Error: 0x%04X\n",ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
-		PCIFR(      "RCB: 0x%04X.%02X  Error: 0x%04X",  ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
-	}
-	*ReadValue = ReadData; 
- 	return DevNode->ReturnCode;
-}
-/** WORD  *************************************************************************/
-int iSeries_Node_read_config_word(struct iSeries_Device_Node* DevNode, int Offset, u16* ReadValue)
-{
-	u16  ReadData; 
-	if(DevNode == NULL) { return 0x301; } 
-	++Pci_Cfg_Read_Count;
-	DevNode->ReturnCode = HvCallPci_configLoad16(ISERIES_BUS(DevNode),ISERIES_SUBBUS(DevNode),0x10,
-	                                                Offset,&ReadData);
-	if(Pci_Trace_Flag == 1) {
-		PCIFR("RCW: 0x%04X.%02X 0x%04X = 0x%04X",ISERIES_BUS(DevNode),DevNode->DevFn,Offset,ReadData);
-	}
-	if(DevNode->ReturnCode != 0 ) { 
-		printk("PCI: RCW: 0x%04X.%02X  Error: 0x%04X\n",ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
-		PCIFR(      "RCW: 0x%04X.%02X  Error: 0x%04X",  ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
-
-	}
-	*ReadValue = ReadData; 
- 	return DevNode->ReturnCode;
-}
-/** DWORD *************************************************************************/
-int iSeries_Node_read_config_dword(struct iSeries_Device_Node* DevNode, int Offset, u32* ReadValue)
-{
- 	u32  ReadData; 
-	if(DevNode == NULL) { return 0x301; } 
-	++Pci_Cfg_Read_Count;
-	DevNode->ReturnCode = HvCallPci_configLoad32(ISERIES_BUS(DevNode),ISERIES_SUBBUS(DevNode),0x10,
-	                                                Offset,&ReadData);
-	if(Pci_Trace_Flag == 1) {
-		PCIFR("RCL: 0x%04X.%02X 0x%04X = 0x%08X",ISERIES_BUS(DevNode),DevNode->DevFn,Offset,ReadData);
-	}
-	if(DevNode->ReturnCode != 0 ) { 
-		printk("PCI: RCL: 0x%04X.%02X  Error: 0x%04X\n",ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
-		PCIFR(      "RCL: 0x%04X.%02X  Error: 0x%04X",  ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
-	}
-	*ReadValue = ReadData; 
- 	return DevNode->ReturnCode;
-}
-int iSeries_pci_read_config_byte(struct pci_dev* PciDev, int Offset, u8* ReadValue) { 
-	struct iSeries_Device_Node* DevNode = get_Device_Node(PciDev);
-	if(DevNode == NULL) return 0x0301;
-	return iSeries_Node_read_config_byte( DevNode ,Offset,ReadValue);
-}
-int iSeries_pci_read_config_word(struct pci_dev* PciDev, int Offset, u16* ReadValue) { 
-	struct iSeries_Device_Node* DevNode = get_Device_Node(PciDev);
-	if(DevNode == NULL) return 0x0301;
-	return iSeries_Node_read_config_word( DevNode ,Offset,ReadValue );
-}
-int iSeries_pci_read_config_dword(struct pci_dev* PciDev, int Offset, u32* ReadValue) { 
-	struct iSeries_Device_Node* DevNode = get_Device_Node(PciDev);
-	if(DevNode == NULL) return 0x0301;
-	return iSeries_Node_read_config_dword(DevNode ,Offset,ReadValue  );
-}
-/**********************************************************************************/
-/*                                                                                */
-/* Write PCI Config Space                                                         */
-/*                                                                                */
-/** BYTE  *************************************************************************/
-int iSeries_Node_write_config_byte(struct iSeries_Device_Node* DevNode, int Offset, u8 WriteData)
-{
-	++Pci_Cfg_Write_Count;
-	DevNode->ReturnCode = HvCallPci_configStore8(ISERIES_BUS(DevNode),ISERIES_SUBBUS(DevNode),0x10,
-	                                                  Offset,WriteData);
-	if(Pci_Trace_Flag == 1) {
-		PCIFR("WCB: 0x%04X.%02X 0x%04X = 0x%02X",ISERIES_BUS(DevNode),DevNode->DevFn,Offset,WriteData);
-	}
-	if(DevNode->ReturnCode != 0 ) { 
-		printk("PCI: WCB: 0x%04X.%02X  Error: 0x%04X\n",ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
-		PCIFR(      "WCB: 0x%04X.%02X  Error: 0x%04X",  ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
-	}
- 	return DevNode->ReturnCode;
-}
-/** WORD  *************************************************************************/
-int iSeries_Node_write_config_word(struct iSeries_Device_Node* DevNode, int Offset, u16 WriteData)
-{
-	++Pci_Cfg_Write_Count;
-	DevNode->ReturnCode = HvCallPci_configStore16(ISERIES_BUS(DevNode),ISERIES_SUBBUS(DevNode),0x10,
-	                                                  Offset,WriteData);
-	if(Pci_Trace_Flag == 1) {
-		PCIFR("WCW: 0x%04X.%02X 0x%04X = 0x%04X",ISERIES_BUS(DevNode),DevNode->DevFn,Offset,WriteData);
-	}
-	if(DevNode->ReturnCode != 0 ) { 
-		printk("PCI: WCW: 0x%04X.%02X  Error: 0x%04X\n",ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
-		PCIFR(      "WCW: 0x%04X.%02X  Error: 0x%04X",  ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
-	}
- 	return DevNode->ReturnCode;
-}
-/** DWORD *************************************************************************/
-int iSeries_Node_write_config_dword(struct iSeries_Device_Node* DevNode, int Offset, u32 WriteData)
-{
-	++Pci_Cfg_Write_Count;
-	DevNode->ReturnCode = HvCallPci_configStore32(ISERIES_BUS(DevNode),ISERIES_SUBBUS(DevNode),0x10,
-	                                                  Offset,WriteData);
-	if(Pci_Trace_Flag == 1) {
-		PCIFR("WCL: 0x%04X.%02X 0x%04X = 0x%08X",ISERIES_BUS(DevNode),DevNode->DevFn,Offset,WriteData);
-	}
-	if(DevNode->ReturnCode != 0 ) { 
-		printk("PCI: WCL: 0x%04X.%02X  Error: 0x%04X\n",ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
-		PCIFR(      "WCL: 0x%04X.%02X  Error: 0x%04X",  ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->ReturnCode);
-	}
-	return DevNode->ReturnCode;
-}
-int iSeries_pci_write_config_byte( struct pci_dev* PciDev,int Offset, u8 WriteValue)
-{
-	struct iSeries_Device_Node* DevNode = get_Device_Node(PciDev);
-	if(DevNode == NULL) return 0x0301;
-	return iSeries_Node_write_config_byte( DevNode,Offset,WriteValue);
-}
-int iSeries_pci_write_config_word( struct pci_dev* PciDev,int Offset,u16 WriteValue)
-{
-	struct iSeries_Device_Node* DevNode = get_Device_Node(PciDev);
-	if(DevNode == NULL) return 0x0301;
-	return iSeries_Node_write_config_word( DevNode,Offset,WriteValue);
-}
-int iSeries_pci_write_config_dword(struct pci_dev* PciDev,int Offset,u32 WriteValue)
-{
-	struct iSeries_Device_Node* DevNode = get_Device_Node(PciDev);
-	if(DevNode == NULL) return 0x0301;
-	return iSeries_Node_write_config_dword(DevNode,Offset,WriteValue);
-}
-
-/************************************************************************/
-/* Branch Table                                                         */
-/************************************************************************/
-struct pci_ops iSeries_pci_ops = {
-	iSeries_pci_read_config_byte,
-	iSeries_pci_read_config_word,
-	iSeries_pci_read_config_dword,
-	iSeries_pci_write_config_byte,
-	iSeries_pci_write_config_word,
-	iSeries_pci_write_config_dword 
+
+/*
+ * Write PCI config space
+ */
+
+static int iSeries_pci_write_config(struct pci_bus *bus, unsigned int devfn,
+		int offset, int size, u32 val)
+{
+	struct iSeries_Device_Node *node = find_Device_Node(bus->number, devfn);
+	u64 fn;
+	u64 ret;
+
+	if (node == NULL)
+		return PCIBIOS_DEVICE_NOT_FOUND;
+
+	fn = hv_cfg_write_func[(size - 1) & 3];
+	ret = HvCall4(fn, node->DsaAddr.DsaAddr, offset, val, 0);
+
+	if (ret != 0)
+		return PCIBIOS_DEVICE_NOT_FOUND;
+
+	return 0;
+}
+
+static struct pci_ops iSeries_pci_ops = {
+	.read = iSeries_pci_read_config,
+	.write = iSeries_pci_write_config
 };
 
-/************************************************************************
+/*
  * Check Return Code
  * -> On Failure, print and log information.
  *    Increment Retry Count, if exceeds max, panic partition.
  * -> If in retry, print and log success 
- ************************************************************************
+ *
  * PCI: Device 23.90 ReadL I/O Error( 0): 0x1234
  * PCI: Device 23.90 ReadL Retry( 1)
  * PCI: Device 23.90 ReadL Retry Successful(1)
- ************************************************************************/
-int  CheckReturnCode(char* TextHdr, struct iSeries_Device_Node* DevNode, u64 RtnCode)
+ */
+static int CheckReturnCode(char *TextHdr, struct iSeries_Device_Node *DevNode,
+		u64 ret)
 {
-	if(RtnCode != 0)  {
+	if (ret != 0)  {
 		++Pci_Error_Count;
 		++DevNode->IoRetry;
-		PCIFR(      "%s: Device 0x%04X:%02X  I/O Error(%2d): 0x%04X",
-			    TextHdr,ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->IoRetry,(int)RtnCode);
 		printk("PCI: %s: Device 0x%04X:%02X  I/O Error(%2d): 0x%04X\n",
-		            TextHdr,ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->IoRetry,(int)RtnCode);
-		/*******************************************************/
-		/* Bump the retry and check for retry count exceeded.  */
-		/* If, Exceeded, panic the system.                     */           
-		/*******************************************************/
-		if(DevNode->IoRetry > Pci_Retry_Max && Pci_Error_Flag > 0 ) {
+				TextHdr, DevNode->DsaAddr.Dsa.busNumber, DevNode->DevFn,
+				DevNode->IoRetry, (int)ret);
+		/*
+		 * Bump the retry and check for retry count exceeded.
+		 * If, Exceeded, panic the system.
+		 */
+		if ((DevNode->IoRetry > Pci_Retry_Max) &&
+				(Pci_Error_Flag > 0)) {
 			mf_displaySrc(0xB6000103);
 			panic_timeout = 0; 
-			panic("PCI: Hardware I/O Error, SRC B6000103, Automatic Reboot Disabled.\n");
+			panic("PCI: Hardware I/O Error, SRC B6000103, "
+					"Automatic Reboot Disabled.\n");
 		}
 		return -1;	/* Retry Try */
 	}
-	/********************************************************************
-	* If retry was in progress, log success and rest retry count        *
-	*********************************************************************/
-	else if(DevNode->IoRetry > 0) {
+	/* If retry was in progress, log success and rest retry count */
+	if (DevNode->IoRetry > 0) {
 		PCIFR("%s: Device 0x%04X:%02X Retry Successful(%2d).",
-		      TextHdr,ISERIES_BUS(DevNode),DevNode->DevFn,DevNode->IoRetry);
+				TextHdr, DevNode->DsaAddr.Dsa.busNumber, DevNode->DevFn,
+				DevNode->IoRetry);
 		DevNode->IoRetry = 0;
-		return 0; 
 	}
 	return 0; 
 }
-/************************************************************************/
-/* Translate the I/O Address into a device node, bar, and bar offset.   */
-/* Note: Make sure the passed variable end up on the stack to avoid     */
-/* the exposure of being device global.                                 */
-/************************************************************************/
-static inline struct iSeries_Device_Node* xlateIoMmAddress(void* IoAddress,
-							    union HvDsaMap* DsaPtr,
-							   u64* BarOffsetPtr) {
-
-	unsigned long BaseIoAddr = (unsigned long)IoAddress-iSeries_Base_Io_Memory;
-	long          TableIndex = BaseIoAddr/iSeries_IoMmTable_Entry_Size;
-	struct iSeries_Device_Node* DevNode = *(iSeries_IoMmTable +TableIndex);
-	if(DevNode != NULL) {
-		DsaPtr->DsaAddr       = ISERIES_DSA(DevNode);
-		DsaPtr->Dsa.barNumber = *(iSeries_IoBarTable+TableIndex);
-		*BarOffsetPtr         = BaseIoAddr % iSeries_IoMmTable_Entry_Size;
-	}
-	else {
+
+/*
+ * Translate the I/O Address into a device node, bar, and bar offset.
+ * Note: Make sure the passed variable end up on the stack to avoid
+ * the exposure of being device global.
+ */
+static inline struct iSeries_Device_Node *xlateIoMmAddress(void *IoAddress,
+		 u64 *dsaptr, u64 *BarOffsetPtr)
+{
+	unsigned long BaseIoAddr;
+	unsigned long TableIndex;
+	struct iSeries_Device_Node *DevNode;
+
+	if (((unsigned long)IoAddress < iSeries_Base_Io_Memory) ||
+			((unsigned long)IoAddress >= iSeries_Max_Io_Memory))
+		return NULL;
+	BaseIoAddr = (unsigned long)IoAddress - iSeries_Base_Io_Memory;
+	TableIndex = BaseIoAddr / iSeries_IoMmTable_Entry_Size;
+	DevNode = iSeries_IoMmTable[TableIndex];
+
+	if (DevNode != NULL) {
+		int barnum = iSeries_IoBarTable[TableIndex];
+		*dsaptr = DevNode->DsaAddr.DsaAddr | (barnum << 24);
+		*BarOffsetPtr = BaseIoAddr % iSeries_IoMmTable_Entry_Size;
+	} else
 		panic("PCI: Invalid PCI IoAddress detected!\n");
-	}
 	return DevNode;
 }
 
-/************************************************************************/
-/* Read MM I/O Instructions for the iSeries                             */
-/* On MM I/O error, all ones are returned and iSeries_pci_IoError is cal*/
-/* else, data is returned in big Endian format.                         */
-/************************************************************************/
-/* iSeries_Read_Byte = Read Byte  ( 8 bit)                              */
-/* iSeries_Read_Word = Read Word  (16 bit)                              */
-/* iSeries_Read_Long = Read Long  (32 bit)                              */
-/************************************************************************/
-u8  iSeries_Read_Byte(void* IoAddress)
-{
-	u64    BarOffset;
-	union  HvDsaMap DsaData;
-	struct HvCallPci_LoadReturn Return;
-	struct iSeries_Device_Node* DevNode = xlateIoMmAddress(IoAddress,&DsaData,&BarOffset);
-
+/*
+ * Read MM I/O Instructions for the iSeries
+ * On MM I/O error, all ones are returned and iSeries_pci_IoError is cal
+ * else, data is returned in big Endian format.
+ *
+ * iSeries_Read_Byte = Read Byte  ( 8 bit)
+ * iSeries_Read_Word = Read Word  (16 bit)
+ * iSeries_Read_Long = Read Long  (32 bit)
+ */
+u8 iSeries_Read_Byte(void *IoAddress)
+{
+	u64 BarOffset;
+	u64 dsa;
+	struct HvCallPci_LoadReturn ret;
+	struct iSeries_Device_Node *DevNode =
+		xlateIoMmAddress(IoAddress, &dsa, &BarOffset);
+
+	if (DevNode == NULL) {
+		static unsigned long last_jiffies;
+		static int num_printed;
+
+		if ((jiffies - last_jiffies) > 60 * HZ) {
+			last_jiffies = jiffies;
+			num_printed = 0;
+		}
+		if (num_printed++ < 10)
+			printk(KERN_ERR "iSeries_Read_Byte: invalid access at IO address %p\n", IoAddress);
+		return 0xff;
+	}
 	do {
 		++Pci_Io_Read_Count;
-		HvCall3Ret16(HvCallPciBarLoad8, &Return, DsaData.DsaAddr,BarOffset, 0);
-	} while (CheckReturnCode("RDB",DevNode, Return.rc) != 0);
+		HvCall3Ret16(HvCallPciBarLoad8, &ret, dsa, BarOffset, 0);
+	} while (CheckReturnCode("RDB", DevNode, ret.rc) != 0);
 
-	if(Pci_Trace_Flag == 1)	PCIFR("RDB: IoAddress 0x%p = 0x%02X",IoAddress, (u8)Return.value); 
-	return (u8)Return.value;
+	return (u8)ret.value;
 }
-u16  iSeries_Read_Word(void* IoAddress)
+
+u16 iSeries_Read_Word(void *IoAddress)
 {
-	u64    BarOffset;
-	union  HvDsaMap DsaData;
-	struct HvCallPci_LoadReturn Return;
-	struct iSeries_Device_Node* DevNode = xlateIoMmAddress(IoAddress,&DsaData,&BarOffset);
+	u64 BarOffset;
+	u64 dsa;
+	struct HvCallPci_LoadReturn ret;
+	struct iSeries_Device_Node *DevNode =
+		xlateIoMmAddress(IoAddress, &dsa, &BarOffset);
+
+	if (DevNode == NULL) {
+		static unsigned long last_jiffies;
+		static int num_printed;
 
+		if ((jiffies - last_jiffies) > 60 * HZ) {
+			last_jiffies = jiffies;
+			num_printed = 0;
+		}
+		if (num_printed++ < 10)
+			printk(KERN_ERR "iSeries_Read_Word: invalid access at IO address %p\n", IoAddress);
+		return 0xffff;
+	}
 	do {
 		++Pci_Io_Read_Count;
-		HvCall3Ret16(HvCallPciBarLoad16,&Return, DsaData.DsaAddr,BarOffset, 0);
-	} while (CheckReturnCode("RDW",DevNode, Return.rc) != 0);
+		HvCall3Ret16(HvCallPciBarLoad16, &ret, dsa,
+				BarOffset, 0);
+	} while (CheckReturnCode("RDW", DevNode, ret.rc) != 0);
 
-	if(Pci_Trace_Flag == 1) PCIFR("RDW: IoAddress 0x%p = 0x%04X",IoAddress, swab16((u16)Return.value));
-	return swab16((u16)Return.value);
+	return swab16((u16)ret.value);
 }
-u32  iSeries_Read_Long(void* IoAddress)
+
+u32 iSeries_Read_Long(void *IoAddress)
 {
-	u64    BarOffset;
-	union  HvDsaMap DsaData;
-	struct HvCallPci_LoadReturn Return;
-	struct iSeries_Device_Node* DevNode = xlateIoMmAddress(IoAddress,&DsaData,&BarOffset);
+	u64 BarOffset;
+	u64 dsa;
+	struct HvCallPci_LoadReturn ret;
+	struct iSeries_Device_Node *DevNode =
+		xlateIoMmAddress(IoAddress, &dsa, &BarOffset);
+
+	if (DevNode == NULL) {
+		static unsigned long last_jiffies;
+		static int num_printed;
 
+		if ((jiffies - last_jiffies) > 60 * HZ) {
+			last_jiffies = jiffies;
+			num_printed = 0;
+		}
+		if (num_printed++ < 10)
+			printk(KERN_ERR "iSeries_Read_Long: invalid access at IO address %p\n", IoAddress);
+		return 0xffffffff;
+	}
 	do {
 		++Pci_Io_Read_Count;
-		HvCall3Ret16(HvCallPciBarLoad32,&Return, DsaData.DsaAddr,BarOffset, 0);
-	} while (CheckReturnCode("RDL",DevNode, Return.rc) != 0);
+		HvCall3Ret16(HvCallPciBarLoad32, &ret, dsa,
+				BarOffset, 0);
+	} while (CheckReturnCode("RDL", DevNode, ret.rc) != 0);
 
-	if(Pci_Trace_Flag == 1) PCIFR("RDL: IoAddress 0x%p = 0x%04X",IoAddress, swab32((u32)Return.value));
-	return swab32((u32)Return.value);
+	return swab32((u32)ret.value);
 }
-/************************************************************************/
-/* Write MM I/O Instructions for the iSeries                            */
-/************************************************************************/
-/* iSeries_Write_Byte = Write Byte (8 bit)                              */
-/* iSeries_Write_Word = Write Word(16 bit)                              */
-/* iSeries_Write_Long = Write Long(32 bit)                              */
-/************************************************************************/
-void iSeries_Write_Byte(u8 Data, void* IoAddress)
-{
-	u64    BarOffset;
-	union  HvDsaMap DsaData;
-	struct HvCallPci_LoadReturn Return;
-	struct iSeries_Device_Node* DevNode = xlateIoMmAddress(IoAddress,&DsaData,&BarOffset);
 
+/*
+ * Write MM I/O Instructions for the iSeries
+ *
+ * iSeries_Write_Byte = Write Byte (8 bit)
+ * iSeries_Write_Word = Write Word(16 bit)
+ * iSeries_Write_Long = Write Long(32 bit)
+ */
+void iSeries_Write_Byte(u8 data, void *IoAddress)
+{
+	u64 BarOffset;
+	u64 dsa;
+	u64 rc;
+	struct iSeries_Device_Node *DevNode =
+		xlateIoMmAddress(IoAddress, &dsa, &BarOffset);
+
+	if (DevNode == NULL) {
+		static unsigned long last_jiffies;
+		static int num_printed;
+
+		if ((jiffies - last_jiffies) > 60 * HZ) {
+			last_jiffies = jiffies;
+			num_printed = 0;
+		}
+		if (num_printed++ < 10)
+			printk(KERN_ERR "iSeries_Write_Byte: invalid access at IO address %p\n", IoAddress);
+		return;
+	}
 	do {
 		++Pci_Io_Write_Count;
-		Return.rc = HvCall4(HvCallPciBarStore8, DsaData.DsaAddr,BarOffset, Data, 0);
-	} while (CheckReturnCode("WWB",DevNode, Return.rc) != 0);
-	if(Pci_Trace_Flag == 1) PCIFR("WWB: IoAddress 0x%p = 0x%02X",IoAddress,Data);
-}
-void iSeries_Write_Word(u16 Data, void* IoAddress)
-{
-	u64    BarOffset;
-	union  HvDsaMap DsaData;
-	struct HvCallPci_LoadReturn Return;
-	struct iSeries_Device_Node* DevNode = xlateIoMmAddress(IoAddress,&DsaData,&BarOffset);
+		rc = HvCall4(HvCallPciBarStore8, dsa, BarOffset, data, 0);
+	} while (CheckReturnCode("WWB", DevNode, rc) != 0);
+}
+
+void iSeries_Write_Word(u16 data, void *IoAddress)
+{
+	u64 BarOffset;
+	u64 dsa;
+	u64 rc;
+	struct iSeries_Device_Node *DevNode =
+		xlateIoMmAddress(IoAddress, &dsa, &BarOffset);
 
+	if (DevNode == NULL) {
+		static unsigned long last_jiffies;
+		static int num_printed;
+
+		if ((jiffies - last_jiffies) > 60 * HZ) {
+			last_jiffies = jiffies;
+			num_printed = 0;
+		}
+		if (num_printed++ < 10)
+			printk(KERN_ERR "iSeries_Write_Word: invalid access at IO address %p\n", IoAddress);
+		return;
+	}
 	do {
 		++Pci_Io_Write_Count;
-		Return.rc = HvCall4(HvCallPciBarStore16,DsaData.DsaAddr,BarOffset, swab16(Data), 0);
-	} while (CheckReturnCode("WWW",DevNode, Return.rc) != 0);
-	if(Pci_Trace_Flag == 1) PCIFR("WWW: IoAddress 0x%p = 0x%04X",IoAddress,Data);
-}
-void iSeries_Write_Long(u32 Data, void* IoAddress)
-{
-	u64    BarOffset;
-	union  HvDsaMap DsaData;
-	struct HvCallPci_LoadReturn Return;
-	struct iSeries_Device_Node* DevNode = xlateIoMmAddress(IoAddress,&DsaData,&BarOffset);
+		rc = HvCall4(HvCallPciBarStore16, dsa, BarOffset, swab16(data), 0);
+	} while (CheckReturnCode("WWW", DevNode, rc) != 0);
+}
+
+void iSeries_Write_Long(u32 data, void *IoAddress)
+{
+	u64 BarOffset;
+	u64 dsa;
+	u64 rc;
+	struct iSeries_Device_Node *DevNode =
+		xlateIoMmAddress(IoAddress, &dsa, &BarOffset);
+
+	if (DevNode == NULL) {
+		static unsigned long last_jiffies;
+		static int num_printed;
 
+		if ((jiffies - last_jiffies) > 60 * HZ) {
+			last_jiffies = jiffies;
+			num_printed = 0;
+		}
+		if (num_printed++ < 10)
+			printk(KERN_ERR "iSeries_Write_Long: invalid access at IO address %p\n", IoAddress);
+		return;
+	}
 	do {
 		++Pci_Io_Write_Count;
-		Return.rc = HvCall4(HvCallPciBarStore32,DsaData.DsaAddr,BarOffset, swab32(Data), 0);
-	} while (CheckReturnCode("WWL",DevNode, Return.rc) != 0);
-	if(Pci_Trace_Flag == 1) PCIFR("WWL: IoAddress 0x%p = 0x%08X",IoAddress, Data);
+		rc = HvCall4(HvCallPciBarStore32, dsa, BarOffset, swab32(data), 0);
+	} while (CheckReturnCode("WWL", DevNode, rc) != 0);
+}
+
+void pcibios_name_device(struct pci_dev *dev)
+{
 }
diff -purN linux-2.5/arch/ppc64/kernel/iSeries_pci_reset.c linuxppc64-2.5/arch/ppc64/kernel/iSeries_pci_reset.c
--- linux-2.5/arch/ppc64/kernel/iSeries_pci_reset.c	2002-06-10 03:44:23.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/iSeries_pci_reset.c	2003-12-10 04:13:06.000000000 +0000
@@ -1,3 +1,4 @@
+#define PCIFR(...)
 /************************************************************************/
 /* File iSeries_pci_reset.c created by Allan Trautman on Mar 21 2001.   */
 /************************************************************************/
@@ -35,53 +36,67 @@
 #include <asm/iSeries/HvCallPci.h>
 #include <asm/iSeries/HvTypes.h>
 #include <asm/iSeries/mf.h>
-#include <asm/flight_recorder.h>
 #include <asm/pci.h>
 
 #include <asm/iSeries/iSeries_pci.h>
 #include "pci.h"
 
-/************************************************************************/
-/* Interface to toggle the reset line                                   */
-/* Time is in .1 seconds, need for seconds.                             */
-/************************************************************************/
-int  iSeries_Device_ToggleReset(struct pci_dev* PciDev, int AssertTime, int DelayTime)
+/*
+ * Interface to toggle the reset line
+ * Time is in .1 seconds, need for seconds.
+ */
+int iSeries_Device_ToggleReset(struct pci_dev *PciDev, int AssertTime,
+		int DelayTime)
 {
 	unsigned long AssertDelay, WaitDelay;
-	struct iSeries_Device_Node* DeviceNode = (struct iSeries_Device_Node*)PciDev->sysdata;
+	struct iSeries_Device_Node *DeviceNode =
+		(struct iSeries_Device_Node *)PciDev->sysdata;
+
  	if (DeviceNode == NULL) { 
-		printk("PCI: Pci Reset Failed, Device Node not found for pci_dev %p\n",PciDev);
+		printk("PCI: Pci Reset Failed, Device Node not found for pci_dev %p\n",
+				PciDev);
 		return -1;
 	}
-	/********************************************************************
+	/*
 	 * Set defaults, Assert is .5 second, Wait is 3 seconds.
-	 ********************************************************************/
-	if (AssertTime == 0) AssertDelay = ( 5 * HZ)/10;
-	else                 AssertDelay = (AssertTime*HZ)/10;
-	if (WaitDelay == 0)  WaitDelay   = (30 * HZ)/10;
-	else                 WaitDelay   = (DelayTime* HZ)/10;
+	 */
+	if (AssertTime == 0)
+		AssertDelay = (5 * HZ) / 10;
+	else
+		AssertDelay = (AssertTime * HZ) / 10;
+	if (WaitDelay == 0)
+		WaitDelay = (30 * HZ) / 10;
+	else
+		WaitDelay = (DelayTime * HZ) / 10;
 
-	/********************************************************************
+	/*
 	 * Assert reset
-	 ********************************************************************/
-	DeviceNode->ReturnCode = HvCallPci_setSlotReset(ISERIES_BUS(DeviceNode),0x00,DeviceNode->AgentId,1);
+	 */
+	DeviceNode->ReturnCode = HvCallPci_setSlotReset(ISERIES_BUS(DeviceNode),
+			0x00, DeviceNode->AgentId, 1);
 	if (DeviceNode->ReturnCode == 0) {
 		set_current_state(TASK_UNINTERRUPTIBLE);
-		schedule_timeout(AssertDelay);       /* Sleep for the time     */
-		DeviceNode->ReturnCode = HvCallPci_setSlotReset(ISERIES_BUS(DeviceNode),0x00,DeviceNode->AgentId, 0);
+		schedule_timeout(AssertDelay);       /* Sleep for the time */
+		DeviceNode->ReturnCode =
+			HvCallPci_setSlotReset(ISERIES_BUS(DeviceNode),
+					0x00, DeviceNode->AgentId, 0);
 
-		/***************************************************************
+		/*
    		 * Wait for device to reset
-		 ***************************************************************/
+		 */
 		set_current_state(TASK_UNINTERRUPTIBLE);  
 		schedule_timeout(WaitDelay);
 	}
-	if (DeviceNode->ReturnCode == 0) {
-		PCIFR("Slot 0x%04X.%02 Reset\n",ISERIES_BUS(DeviceNode),DeviceNode->AgentId );
-	} 
+	if (DeviceNode->ReturnCode == 0)
+		PCIFR("Slot 0x%04X.%02 Reset\n", ISERIES_BUS(DeviceNode),
+				DeviceNode->AgentId);
 	else {
-		printk("PCI: Slot 0x%04X.%02X Reset Failed, RCode: %04X\n",ISERIES_BUS(DeviceNode),DeviceNode->AgentId,DeviceNode->ReturnCode);
-		PCIFR(      "Slot 0x%04X.%02X Reset Failed, RCode: %04X\n",ISERIES_BUS(DeviceNode),DeviceNode->AgentId,DeviceNode->ReturnCode);
+		printk("PCI: Slot 0x%04X.%02X Reset Failed, RCode: %04X\n",
+				ISERIES_BUS(DeviceNode), DeviceNode->AgentId,
+				DeviceNode->ReturnCode);
+		PCIFR("Slot 0x%04X.%02X Reset Failed, RCode: %04X\n",
+				ISERIES_BUS(DeviceNode), DeviceNode->AgentId,
+				DeviceNode->ReturnCode);
 	}
 	return DeviceNode->ReturnCode;
 }
diff -purN linux-2.5/arch/ppc64/kernel/iSeries_proc.c linuxppc64-2.5/arch/ppc64/kernel/iSeries_proc.c
--- linux-2.5/arch/ppc64/kernel/iSeries_proc.c	2002-02-14 12:14:36.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/iSeries_proc.c	2003-12-17 05:46:52.000000000 +0000
@@ -16,30 +16,22 @@
   * along with this program; if not, write to the Free Software
   * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
   */
-
-
-/* Change Activity: */
-/* End Change Activity */
-
 #include <linux/proc_fs.h>
 #include <linux/spinlock.h>
-#ifndef _ISERIES_PROC_H
+#include <linux/init.h>
 #include <asm/iSeries/iSeries_proc.h>
-#endif
 
-
-static struct proc_dir_entry * iSeries_proc_root = NULL;
-static int iSeries_proc_initializationDone = 0;
+static struct proc_dir_entry *iSeries_proc_root;
+static int iSeries_proc_initializationDone;
 static spinlock_t iSeries_proc_lock;
 
-struct iSeries_proc_registration
-{
+struct iSeries_proc_registration {
 	struct iSeries_proc_registration *next;
 	iSeriesProcFunction functionMember;
 };
 
-
 struct iSeries_proc_registration preallocated[16];
+
 #define MYQUEUETYPE(T) struct MYQueue##T
 #define MYQUEUE(T) \
 MYQUEUETYPE(T) \
@@ -69,74 +61,71 @@ do { \
 	if ((q)->tail == NULL) \
 		(q)->head = NULL; \
 } while(0)
+
 MYQUEUE(iSeries_proc_registration);
 typedef MYQUEUETYPE(iSeries_proc_registration) aQueue;
 
-
-aQueue iSeries_free;
-aQueue iSeries_queued;
+static aQueue iSeries_free;
+static aQueue iSeries_queued;
 
 void iSeries_proc_early_init(void)
 {
 	int i = 0;
 	unsigned long flags;
+
 	iSeries_proc_initializationDone = 0;
 	spin_lock_init(&iSeries_proc_lock);
 	MYQUEUECTOR(&iSeries_free);
 	MYQUEUECTOR(&iSeries_queued);
 
 	spin_lock_irqsave(&iSeries_proc_lock, flags);
-	for (i = 0; i < 16; ++i) {
-		MYQUEUEENQ(&iSeries_free, preallocated+i);
-	}
+	for (i = 0; i < 16; ++i)
+		MYQUEUEENQ(&iSeries_free, preallocated + i);
 	spin_unlock_irqrestore(&iSeries_proc_lock, flags);
 }
 
-void iSeries_proc_create(void)
+static int iSeries_proc_create(void)
 {
 	unsigned long flags;
-	struct iSeries_proc_registration *reg = NULL;
-	spin_lock_irqsave(&iSeries_proc_lock, flags);
+	struct iSeries_proc_registration *reg;
+
 	printk("iSeries_proc: Creating /proc/iSeries\n");
 
+	spin_lock_irqsave(&iSeries_proc_lock, flags);
 	iSeries_proc_root = proc_mkdir("iSeries", 0);
-	if (!iSeries_proc_root) return;
+	if (!iSeries_proc_root)
+		goto out;
 
 	MYQUEUEDEQ(&iSeries_queued, reg);
-
 	while (reg != NULL) {
 		(*(reg->functionMember))(iSeries_proc_root);
-
 		MYQUEUEDEQ(&iSeries_queued, reg);
 	}
 
 	iSeries_proc_initializationDone = 1;
+out:
 	spin_unlock_irqrestore(&iSeries_proc_lock, flags);
+	return 0;
 }
 
+arch_initcall(iSeries_proc_create);
+
 void iSeries_proc_callback(iSeriesProcFunction initFunction)
 {
 	unsigned long flags;
-	spin_lock_irqsave(&iSeries_proc_lock, flags);
 
-	if (iSeries_proc_initializationDone) {
+	spin_lock_irqsave(&iSeries_proc_lock, flags);
+	if (iSeries_proc_initializationDone)
 		(*initFunction)(iSeries_proc_root);
-	} else {
+	else {
 		struct iSeries_proc_registration *reg = NULL;
 
 		MYQUEUEDEQ(&iSeries_free, reg);
-
 		if (reg != NULL) {
-			/* printk("Registering %p in reg %p\n", initFunction, reg); */
 			reg->functionMember = initFunction;
-
 			MYQUEUEENQ(&iSeries_queued, reg);
-		} else {
+		} else
 			printk("Couldn't get a queue entry\n");
-		}
 	}
-
 	spin_unlock_irqrestore(&iSeries_proc_lock, flags);
 }
-
-
diff -purN linux-2.5/arch/ppc64/kernel/iSeries_setup.c linuxppc64-2.5/arch/ppc64/kernel/iSeries_setup.c
--- linux-2.5/arch/ppc64/kernel/iSeries_setup.c	2003-07-28 04:00:17.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/iSeries_setup.c	2003-11-21 06:45:02.000000000 +0000
@@ -25,6 +25,8 @@
 #include <linux/bootmem.h>
 #include <linux/initrd.h>
 #include <linux/seq_file.h>
+#include <linux/kdev_t.h>
+#include <linux/major.h>
 #include <linux/root_dev.h>
 
 #include <asm/processor.h>
@@ -53,34 +55,34 @@
 #include <asm/iSeries/mf.h>
 
 /* Function Prototypes */
-
 extern void abort(void);
-#ifdef CONFIG_PPC_ISERIES
-static void build_iSeries_Memory_Map( void );
-static void setup_iSeries_cache_sizes( void );
-static void iSeries_bolt_kernel(unsigned long saddr, unsigned long eaddr);
-#endif
-void build_valid_hpte( unsigned long vsid, unsigned long ea, unsigned long pa,
-		       pte_t * ptep, unsigned hpteflags, unsigned bolted );
 extern void ppcdbg_initialize(void);
 extern void iSeries_pcibios_init(void);
+
+static void build_iSeries_Memory_Map(void);
+static void setup_iSeries_cache_sizes(void);
+static void iSeries_bolt_kernel(unsigned long saddr, unsigned long eaddr);
+void build_valid_hpte(unsigned long vsid, unsigned long ea, unsigned long pa,
+		pte_t *ptep, unsigned hpteflags, unsigned bolted);
 static void iSeries_setup_dprofile(void);
+void iSeries_setup_arch(void);
 
 /* Global Variables */
+static unsigned long procFreqHz;
+static unsigned long procFreqMhz;
+static unsigned long procFreqMhzHundreths;
+
+static unsigned long tbFreqHz;
+static unsigned long tbFreqMhz;
+static unsigned long tbFreqMhzHundreths;
 
-static unsigned long procFreqHz = 0;
-static unsigned long procFreqMhz = 0;
-static unsigned long procFreqMhzHundreths = 0;
-
-static unsigned long tbFreqHz = 0;
-static unsigned long tbFreqMhz = 0;
-static unsigned long tbFreqMhzHundreths = 0;
-
-unsigned long dprof_shift = 0;
-unsigned long dprof_len = 0;
-unsigned int * dprof_buffer = NULL;
+unsigned long dprof_shift;
+unsigned long dprof_len;
+unsigned int *dprof_buffer;
 
-int piranha_simulator = 0;
+int piranha_simulator;
+
+int boot_cpuid;
 
 extern char _end[];
 
@@ -92,7 +94,7 @@ extern unsigned long embedded_sysmap_end
 extern unsigned long iSeries_recal_tb;
 extern unsigned long iSeries_recal_titan;
 
-static int mf_initialized = 0;
+static int mf_initialized;
 
 struct MemoryBlock {
 	unsigned long absStart;
@@ -106,30 +108,30 @@ struct MemoryBlock {
  * and return the number of physical blocks and fill in the array of
  * block data.
  */
-
-unsigned long iSeries_process_Condor_mainstore_vpd( struct MemoryBlock *mb_array, unsigned long max_entries )
+unsigned long iSeries_process_Condor_mainstore_vpd(struct MemoryBlock *mb_array,
+		unsigned long max_entries)
 {
-	/* Determine if absolute memory has any
-	 * holes so that we can interpret the
-	 * access map we get back from the hypervisor
-	 * correctly.
-	 */
-	
 	unsigned long holeFirstChunk, holeSizeChunks;
 	unsigned long numMemoryBlocks = 1;
-	struct IoHriMainStoreSegment4 * msVpd = (struct IoHriMainStoreSegment4 *)xMsVpd;
+	struct IoHriMainStoreSegment4 *msVpd =
+		(struct IoHriMainStoreSegment4 *)xMsVpd;
 	unsigned long holeStart = msVpd->nonInterleavedBlocksStartAdr;
-	unsigned long holeEnd   = msVpd->nonInterleavedBlocksEndAdr;
+	unsigned long holeEnd = msVpd->nonInterleavedBlocksEndAdr;
 	unsigned long holeSize = holeEnd - holeStart;
 
 	printk("Mainstore_VPD: Condor\n");
-
+	/*
+	 * Determine if absolute memory has any
+	 * holes so that we can interpret the
+	 * access map we get back from the hypervisor
+	 * correctly.
+	 */
 	mb_array[0].logicalStart = 0;
-	mb_array[0].logicalEnd   = 0x100000000;
-	mb_array[0].absStart     = 0;
-	mb_array[0].absEnd       = 0x100000000;
+	mb_array[0].logicalEnd = 0x100000000;
+	mb_array[0].absStart = 0;
+	mb_array[0].absEnd = 0x100000000;
 
-	if ( holeSize ) {
+	if (holeSize) {
 		numMemoryBlocks = 2;
 		holeStart = holeStart & 0x000fffffffffffff;
 		holeStart = addr_to_chunk(holeStart);
@@ -138,275 +140,264 @@ unsigned long iSeries_process_Condor_mai
 		holeSizeChunks = holeSize;
 		printk( "Main store hole: start chunk = %0lx, size = %0lx chunks\n",
 				holeFirstChunk, holeSizeChunks );
-		mb_array[0].logicalEnd   = holeFirstChunk;
-		mb_array[0].absEnd       = holeFirstChunk;
+		mb_array[0].logicalEnd = holeFirstChunk;
+		mb_array[0].absEnd = holeFirstChunk;
 		mb_array[1].logicalStart = holeFirstChunk;
-		mb_array[1].logicalEnd   = 0x100000000 - holeSizeChunks;
-		mb_array[1].absStart     = holeFirstChunk + holeSizeChunks;
-		mb_array[1].absEnd       = 0x100000000;
+		mb_array[1].logicalEnd = 0x100000000 - holeSizeChunks;
+		mb_array[1].absStart = holeFirstChunk + holeSizeChunks;
+		mb_array[1].absEnd = 0x100000000;
 	}
-
-	
 	return numMemoryBlocks;
 }
 
-#define MaxSegmentAreas 32
-#define MaxSegmentAdrRangeBlocks 128
-#define MaxAreaRangeBlocks 4
-unsigned long iSeries_process_Regatta_mainstore_vpd( struct MemoryBlock *mb_array, unsigned long max_entries )
+#define MaxSegmentAreas			32
+#define MaxSegmentAdrRangeBlocks	128
+#define MaxAreaRangeBlocks		4
+
+unsigned long iSeries_process_Regatta_mainstore_vpd(
+		struct MemoryBlock *mb_array, unsigned long max_entries)
 {
-	struct IoHriMainStoreSegment5 * msVpdP = (struct IoHriMainStoreSegment5 *)xMsVpd;
+	struct IoHriMainStoreSegment5 *msVpdP =
+		(struct IoHriMainStoreSegment5 *)xMsVpd;
 	unsigned long numSegmentBlocks = 0;
 	u32 existsBits = msVpdP->msAreaExists;
 	unsigned long area_num;
 
 	printk("Mainstore_VPD: Regatta\n");
 
-	for ( area_num = 0; area_num < MaxSegmentAreas; ++area_num ) {
+	for (area_num = 0; area_num < MaxSegmentAreas; ++area_num ) {
 		unsigned long numAreaBlocks;
-		struct IoHriMainStoreArea4 * currentArea;
+		struct IoHriMainStoreArea4 *currentArea;
 
-		if ( existsBits & 0x80000000 ) {
+		if (existsBits & 0x80000000) {
 			unsigned long block_num;
 
 			currentArea = &msVpdP->msAreaArray[area_num];
 			numAreaBlocks = currentArea->numAdrRangeBlocks;
-
-			printk("ms_vpd: processing area %2ld  blocks=%ld", area_num, numAreaBlocks);
-
-			for ( block_num = 0; block_num < numAreaBlocks; ++block_num ) {
+			printk("ms_vpd: processing area %2ld  blocks=%ld",
+					area_num, numAreaBlocks);
+			for (block_num = 0; block_num < numAreaBlocks;
+					++block_num ) {
 				/* Process an address range block */
 				struct MemoryBlock tempBlock;
 				unsigned long i;
 
-				tempBlock.absStart = (unsigned long)currentArea->xAdrRangeBlock[block_num].blockStart;
-				tempBlock.absEnd   = (unsigned long)currentArea->xAdrRangeBlock[block_num].blockEnd;
+				tempBlock.absStart =
+					(unsigned long)currentArea->xAdrRangeBlock[block_num].blockStart;
+				tempBlock.absEnd =
+					(unsigned long)currentArea->xAdrRangeBlock[block_num].blockEnd;
 				tempBlock.logicalStart = 0;
 				tempBlock.logicalEnd   = 0;
-
-				printk("\n          block %ld absStart=%016lx absEnd=%016lx", block_num,
-							tempBlock.absStart, tempBlock.absEnd);
-
-				for ( i=0; i<numSegmentBlocks; ++i ) {
-					if ( mb_array[i].absStart == tempBlock.absStart )
+				printk("\n          block %ld absStart=%016lx absEnd=%016lx",
+						block_num, tempBlock.absStart,
+						tempBlock.absEnd);
+
+				for (i = 0; i < numSegmentBlocks; ++i) {
+					if (mb_array[i].absStart ==
+							tempBlock.absStart)
 						break;
 				}
-				if ( i == numSegmentBlocks ) {
-					if ( numSegmentBlocks == max_entries ) {
+				if (i == numSegmentBlocks) {
+					if (numSegmentBlocks == max_entries)
 						panic("iSeries_process_mainstore_vpd: too many memory blocks");
-					}
 					mb_array[numSegmentBlocks] = tempBlock;
 					++numSegmentBlocks;
-				}
-				else {
+				} else
 					printk(" (duplicate)");
-				}
 			}
 			printk("\n");
 		}
 		existsBits <<= 1;
 	}
 	/* Now sort the blocks found into ascending sequence */
-	if ( numSegmentBlocks > 1 ) {
+	if (numSegmentBlocks > 1) {
 		unsigned long m, n;
-		for ( m=0; m<numSegmentBlocks-1; ++m ) {
-			for ( n=numSegmentBlocks-1; m<n; --n ) {
-				if ( mb_array[n].absStart < mb_array[n-1].absStart ) {
+
+		for (m = 0; m < numSegmentBlocks - 1; ++m) {
+			for (n = numSegmentBlocks - 1; m < n; --n) {
+				if (mb_array[n].absStart <
+						mb_array[n-1].absStart) {
 					struct MemoryBlock tempBlock;
+
 					tempBlock = mb_array[n];
 					mb_array[n] = mb_array[n-1];
 					mb_array[n-1] = tempBlock;
 				}
-				
 			}
 		}
 	}
-	/* Assign "logical" addresses to each block.  These
+	/*
+	 * Assign "logical" addresses to each block.  These
 	 * addresses correspond to the hypervisor "bitmap" space.
 	 * Convert all addresses into units of 256K chunks.
 	 */
 	{
 	unsigned long i, nextBitmapAddress;
+
 	printk("ms_vpd: %ld sorted memory blocks\n", numSegmentBlocks);
 	nextBitmapAddress = 0;
-	for ( i=0; i<numSegmentBlocks; ++i ) {
-		unsigned long length = mb_array[i].absEnd - mb_array[i].absStart;
+	for (i = 0; i < numSegmentBlocks; ++i) {
+		unsigned long length = mb_array[i].absEnd -
+			mb_array[i].absStart;
+
 		mb_array[i].logicalStart = nextBitmapAddress;
 		mb_array[i].logicalEnd = nextBitmapAddress + length;
 		nextBitmapAddress += length;
 		printk("          Bitmap range: %016lx - %016lx\n"
-		       "        Absolute range: %016lx - %016lx\n",
-				mb_array[i].logicalStart, mb_array[i].logicalEnd, 
+				"        Absolute range: %016lx - %016lx\n",
+				mb_array[i].logicalStart,
+				mb_array[i].logicalEnd, 
 				mb_array[i].absStart, mb_array[i].absEnd);
-		mb_array[i].absStart     = addr_to_chunk( mb_array[i].absStart & 0x000fffffffffffff );
-		mb_array[i].absEnd       = addr_to_chunk( mb_array[i].absEnd & 0x000fffffffffffff );
-		mb_array[i].logicalStart = addr_to_chunk( mb_array[i].logicalStart );
-		mb_array[i].logicalEnd   = addr_to_chunk( mb_array[i].logicalEnd );
+		mb_array[i].absStart = addr_to_chunk(mb_array[i].absStart &
+				0x000fffffffffffff);
+		mb_array[i].absEnd = addr_to_chunk(mb_array[i].absEnd &
+				0x000fffffffffffff);
+		mb_array[i].logicalStart =
+			addr_to_chunk(mb_array[i].logicalStart);
+		mb_array[i].logicalEnd = addr_to_chunk(mb_array[i].logicalEnd);
 	}
 	}
 
 	return numSegmentBlocks;
-
 }
 
-unsigned long iSeries_process_mainstore_vpd( struct MemoryBlock *mb_array, unsigned long max_entries )
+unsigned long iSeries_process_mainstore_vpd(struct MemoryBlock *mb_array,
+		unsigned long max_entries)
 {
 	unsigned long i;
 	unsigned long mem_blocks = 0;
+
 	if (cur_cpu_spec->cpu_features & CPU_FTR_SLB)
-		mem_blocks = iSeries_process_Regatta_mainstore_vpd( mb_array, max_entries );
+		mem_blocks = iSeries_process_Regatta_mainstore_vpd(mb_array,
+				max_entries);
 	else
-		mem_blocks = iSeries_process_Condor_mainstore_vpd( mb_array, max_entries );
+		mem_blocks = iSeries_process_Condor_mainstore_vpd(mb_array,
+				max_entries);
 
 	printk("Mainstore_VPD: numMemoryBlocks = %ld \n", mem_blocks);
-	for ( i=0; i<mem_blocks; ++i ) {
+	for (i = 0; i < mem_blocks; ++i) {
 		printk("Mainstore_VPD: block %3ld logical chunks %016lx - %016lx\n"
 		       "                             abs chunks %016lx - %016lx\n",
 			i, mb_array[i].logicalStart, mb_array[i].logicalEnd,
 			mb_array[i].absStart, mb_array[i].absEnd);
 	}
-
 	return mem_blocks;
 }
 
-/*
- * void __init iSeries_init_early()
- */
-
-
-
-void __init
-iSeries_init_early(void)
+void __init iSeries_init_early(void)
 {
-#ifdef CONFIG_PPC_ISERIES
 	ppcdbg_initialize();
-	
+
 #if defined(CONFIG_BLK_DEV_INITRD)
 	/*
 	 * If the init RAM disk has been configured and there is
 	 * a non-zero starting address for it, set it up
 	 */
-
-	if ( naca->xRamDisk ) {
+	if (naca->xRamDisk) {
 		initrd_start = (unsigned long)__va(naca->xRamDisk);
-		initrd_end   = initrd_start + naca->xRamDiskSize * PAGE_SIZE;
+		initrd_end = initrd_start + naca->xRamDiskSize * PAGE_SIZE;
 		initrd_below_start_ok = 1;	// ramdisk in kernel space
 		ROOT_DEV = Root_RAM0;
-
-		if ( ((rd_size*1024)/PAGE_SIZE) < naca->xRamDiskSize )
-			rd_size = (naca->xRamDiskSize*PAGE_SIZE)/1024;
+		if (((rd_size * 1024) / PAGE_SIZE) < naca->xRamDiskSize)
+			rd_size = (naca->xRamDiskSize * PAGE_SIZE) / 1024;
 	} else
-	
 #endif /* CONFIG_BLK_DEV_INITRD */
-	  {
-                
-	    /*		ROOT_DEV = MKDEV( VIODASD_MAJOR, 1 ); */
-	  }
+	{
+	    /* ROOT_DEV = MKDEV(VIODASD_MAJOR, 1); */
+	}
 
 	iSeries_recal_tb = get_tb();
 	iSeries_recal_titan = HvCallXm_loadTod();
 
-	ppc_md.setup_arch	 	= iSeries_setup_arch;
-	ppc_md.setup_residual	 	= iSeries_setup_residual;
-	ppc_md.get_cpuinfo	 	= iSeries_get_cpuinfo;
-	ppc_md.init_IRQ		 	= iSeries_init_IRQ;
-	ppc_md.get_irq		 	= iSeries_get_irq;
-	ppc_md.init		 	= NULL;
-
-	ppc_md.restart		 	= iSeries_restart;
-	ppc_md.power_off	 	= iSeries_power_off;
-	ppc_md.halt		 	= iSeries_halt;
-
-	ppc_md.get_boot_time		= iSeries_get_boot_time;
-	ppc_md.set_rtc_time	 	= iSeries_set_rtc_time;
-	ppc_md.get_rtc_time	 	= iSeries_get_rtc_time;
-	ppc_md.calibrate_decr	 	= iSeries_calibrate_decr;
-	ppc_md.progress			= iSeries_progress;
+	ppc_md.setup_arch = iSeries_setup_arch;
+	ppc_md.setup_residual = iSeries_setup_residual;
+	ppc_md.get_cpuinfo = iSeries_get_cpuinfo;
+	ppc_md.init_IRQ = iSeries_init_IRQ;
+	ppc_md.init_irq_desc = iSeries_init_irq_desc;
+	ppc_md.get_irq = iSeries_get_irq;
+	ppc_md.init = NULL;
+
+	ppc_md.restart = iSeries_restart;
+	ppc_md.power_off = iSeries_power_off;
+	ppc_md.halt = iSeries_halt;
+
+	ppc_md.get_boot_time = iSeries_get_boot_time;
+	ppc_md.set_rtc_time = iSeries_set_rtc_time;
+	ppc_md.get_rtc_time = iSeries_get_rtc_time;
+	ppc_md.calibrate_decr = iSeries_calibrate_decr;
+	ppc_md.progress = iSeries_progress;
 
 	hpte_init_iSeries();
 	tce_init_iSeries();
 
-	/* Initialize the table which translate Linux physical addresses to
+	/*
+	 * Initialize the table which translate Linux physical addresses to
 	 * AS/400 absolute addresses
 	 */
-
 	build_iSeries_Memory_Map();
-
 	setup_iSeries_cache_sizes();
-
 	/* Initialize machine-dependency vectors */
-
-
 #ifdef CONFIG_SMP
 	smp_init_iSeries();
 #endif
-
-	if ( itLpNaca.xPirEnvironMode == 0 ) 
+	if (itLpNaca.xPirEnvironMode == 0) 
 		piranha_simulator = 1;
-#endif
 }
 
-/*
- * void __init iSeries_init()
- */
-
-void __init
-iSeries_init(unsigned long r3, unsigned long r4, unsigned long r5, 
+void __init iSeries_init(unsigned long r3, unsigned long r4, unsigned long r5, 
 	   unsigned long r6, unsigned long r7)
 {
+	char *p, *q;
+
 	/* Associate Lp Event Queue 0 with processor 0 */
-	HvCallEvent_setLpEventQueueInterruptProc( 0, 0 );
+	HvCallEvent_setLpEventQueueInterruptProc(0, 0);
 
-	{
-		/* copy the command line parameter from the primary VSP  */
-		char *p, *q;
-		HvCallEvent_dmaToSp( cmd_line,
-				     2*64*1024,
-				     256,
-				     HvLpDma_Direction_RemoteToLocal );
-
-		p = q = cmd_line + 255;
-		while( p > cmd_line ) {
-			if ((*p == 0) || (*p == ' ') || (*p == '\n'))
-				--p;
-			else
-				break;
-		}
-		if ( p < q )
-			*(p+1) = 0;
+	/* copy the command line parameter from the primary VSP  */
+	HvCallEvent_dmaToSp(cmd_line, 2 * 64* 1024, 256,
+			HvLpDma_Direction_RemoteToLocal);
+
+	p = q = cmd_line + 255;
+	while (p > cmd_line) {
+		if ((*p == 0) || (*p == ' ') || (*p == '\n'))
+			--p;
+		else
+			break;
 	}
+	if (p < q)
+		*(p + 1) = 0;
 
         if (strstr(cmd_line, "dprofile=")) {
-                char *p, *q;
-
                 for (q = cmd_line; (p = strstr(q, "dprofile=")) != 0; ) {
 			unsigned long size, new_klimit;
+
                         q = p + 9;
-                        if (p > cmd_line && p[-1] != ' ')
+                        if ((p > cmd_line) && (p[-1] != ' '))
                                 continue;
                         dprof_shift = simple_strtoul(q, &q, 0);
-			dprof_len = (unsigned long)_etext - (unsigned long)_stext;
+			dprof_len = (unsigned long)_etext -
+				(unsigned long)_stext;
 			dprof_len >>= dprof_shift;
-			size = ((dprof_len * sizeof(unsigned int)) + (PAGE_SIZE-1)) & PAGE_MASK;
-			dprof_buffer = (unsigned int *)((klimit + (PAGE_SIZE-1)) & PAGE_MASK);
+			size = ((dprof_len * sizeof(unsigned int)) +
+					(PAGE_SIZE-1)) & PAGE_MASK;
+			dprof_buffer = (unsigned int *)((klimit +
+						(PAGE_SIZE-1)) & PAGE_MASK);
 			new_klimit = ((unsigned long)dprof_buffer) + size;
-			lmb_reserve( __pa(klimit), (new_klimit-klimit));
+			lmb_reserve(__pa(klimit), (new_klimit-klimit));
 			klimit = new_klimit;
-			memset( dprof_buffer, 0, size );
+			memset(dprof_buffer, 0, size);
                 }
         }
 
 	iSeries_setup_dprofile();
 
-	iSeries_proc_early_init();	
+	iSeries_proc_early_init();
 	mf_init();
 	mf_initialized = 1;
 	mb();
 
-	iSeries_proc_callback( &pmc_proc_init );
+	iSeries_proc_callback(&pmc_proc_init);
 }
 
-#ifdef CONFIG_PPC_ISERIES
 /*
  * The iSeries may have very large memories ( > 128 GB ) and a partition
  * may get memory in "chunks" that may be anywhere in the 2**52 real
@@ -444,9 +435,10 @@ static void __init build_iSeries_Memory_
 
 	/* Chunk size on iSeries is 256K bytes */
 	totalChunks = (u32)HvLpConfig_getMsChunks();
-	klimit = msChunks_alloc(klimit, totalChunks, 1UL<<18);
+	klimit = msChunks_alloc(klimit, totalChunks, 1UL << 18);
 
-	/* Get absolute address of our load area
+	/*
+	 * Get absolute address of our load area
 	 * and map it to physical address 0
 	 * This guarantees that the loadarea ends up at physical 0
 	 * otherwise, it might not be returned by PLIC as the first
@@ -456,63 +448,68 @@ static void __init build_iSeries_Memory_
 	loadAreaFirstChunk = (u32)addr_to_chunk(itLpNaca.xLoadAreaAddr);
 	loadAreaSize =  itLpNaca.xLoadAreaChunks;
 
-	/* Only add the pages already mapped here.  
+	/*
+	 * Only add the pages already mapped here.  
 	 * Otherwise we might add the hpt pages 
 	 * The rest of the pages of the load area
 	 * aren't in the HPT yet and can still
 	 * be assigned an arbitrary physical address
 	 */
-	if ( (loadAreaSize * 64) > HvPagesToMap )
+	if ((loadAreaSize * 64) > HvPagesToMap)
 		loadAreaSize = HvPagesToMap / 64;
 
 	loadAreaLastChunk = loadAreaFirstChunk + loadAreaSize - 1;
 
-	/* TODO Do we need to do something if the HPT is in the 64MB load area?
+	/*
+	 * TODO Do we need to do something if the HPT is in the 64MB load area?
 	 * This would be required if the itLpNaca.xLoadAreaChunks includes 
 	 * the HPT size
 	 */
 
-	printk( "Mapping load area - physical addr = 0000000000000000\n"
-                "                    absolute addr = %016lx\n", 
-			chunk_to_addr(loadAreaFirstChunk) );
-	printk( "Load area size %dK\n", loadAreaSize*256 );
+	printk("Mapping load area - physical addr = 0000000000000000\n"
+		"                    absolute addr = %016lx\n",
+		chunk_to_addr(loadAreaFirstChunk));
+	printk("Load area size %dK\n", loadAreaSize * 256);
+	
+	for (nextPhysChunk = 0; nextPhysChunk < loadAreaSize; ++nextPhysChunk)
+		msChunks.abs[nextPhysChunk] =
+			loadAreaFirstChunk + nextPhysChunk;
 	
-	for (	nextPhysChunk = 0; 
-		nextPhysChunk < loadAreaSize; 
-		++nextPhysChunk ) {
-		msChunks.abs[nextPhysChunk] = loadAreaFirstChunk+nextPhysChunk;
-	}
-	
-	/* Get absolute address of our HPT and remember it so
+	/*
+	 * Get absolute address of our HPT and remember it so
 	 * we won't map it to any physical address
 	 */
-
 	hptFirstChunk = (u32)addr_to_chunk(HvCallHpt_getHptAddress());
-	hptSizePages =  (u32)(HvCallHpt_getHptPages());
-	hptSizeChunks = hptSizePages >> (msChunks.chunk_shift-PAGE_SHIFT);
+	hptSizePages = (u32)HvCallHpt_getHptPages();
+	hptSizeChunks = hptSizePages >> (msChunks.chunk_shift - PAGE_SHIFT);
 	hptLastChunk = hptFirstChunk + hptSizeChunks - 1;
-	
-	printk( "HPT absolute addr = %016lx, size = %dK\n",
-			chunk_to_addr(hptFirstChunk), hptSizeChunks*256 );
+
+	printk("HPT absolute addr = %016lx, size = %dK\n",
+			chunk_to_addr(hptFirstChunk), hptSizeChunks * 256);
 
 	/* Fill in the htab_data structure */
-	
 	/* Fill in size of hashed page table */
-	num_ptegs = hptSizePages * (PAGE_SIZE/(sizeof(HPTE)*HPTES_PER_GROUP));
+	num_ptegs = hptSizePages *
+		(PAGE_SIZE / (sizeof(HPTE) * HPTES_PER_GROUP));
 	htab_data.htab_num_ptegs = num_ptegs;
 	htab_data.htab_hash_mask = num_ptegs - 1;
 	
-	/* The actual hashed page table is in the hypervisor, we have no direct access */
+	/*
+	 * The actual hashed page table is in the hypervisor,
+	 * we have no direct access
+	 */
 	htab_data.htab = NULL;
 
-	/* Determine if absolute memory has any
+	/*
+	 * Determine if absolute memory has any
 	 * holes so that we can interpret the
 	 * access map we get back from the hypervisor
 	 * correctly.
 	 */
-	numMemoryBlocks = iSeries_process_mainstore_vpd( mb, 32 );
+	numMemoryBlocks = iSeries_process_mainstore_vpd(mb, 32);
 
-	/* Process the main store access map from the hypervisor
+	/*
+	 * Process the main store access map from the hypervisor
 	 * to build up our physical -> absolute translation table
 	 */
 	curBlock = 0;
@@ -520,30 +517,29 @@ static void __init build_iSeries_Memory_
 	currDword = 0;
 	moreChunks = totalChunks;
 
-	while ( moreChunks ) {
-		map = HvCallSm_get64BitsOfAccessMap( itLpNaca.xLpIndex,
-						     currDword );
+	while (moreChunks) {
+		map = HvCallSm_get64BitsOfAccessMap(itLpNaca.xLpIndex,
+				currDword);
 		thisChunk = currChunk;
-		while ( map ) {
+		while (map) {
 			chunkBit = map >> 63;
 			map <<= 1;
-			if ( chunkBit ) {
+			if (chunkBit) {
 				--moreChunks;
-
-				while ( thisChunk >= mb[curBlock].logicalEnd ) {
+				while (thisChunk >= mb[curBlock].logicalEnd) {
 					++curBlock;
-					if ( curBlock >= numMemoryBlocks )
+					if (curBlock >= numMemoryBlocks)
 						panic("out of memory blocks");
 				}
-				if ( thisChunk < mb[curBlock].logicalStart )
+				if (thisChunk < mb[curBlock].logicalStart)
 					panic("memory block error");
 
-				absChunk = mb[curBlock].absStart + ( thisChunk - mb[curBlock].logicalStart );
-
-				if ( ( ( absChunk < hptFirstChunk ) ||
-				       ( absChunk > hptLastChunk ) ) &&
-				     ( ( absChunk < loadAreaFirstChunk ) ||
-				       ( absChunk > loadAreaLastChunk ) ) ) {
+				absChunk = mb[curBlock].absStart +
+					(thisChunk - mb[curBlock].logicalStart);
+				if (((absChunk < hptFirstChunk) ||
+				     (absChunk > hptLastChunk)) &&
+				    ((absChunk < loadAreaFirstChunk) ||
+				     (absChunk > loadAreaLastChunk))) {
 					msChunks.abs[nextPhysChunk] = absChunk;
 					++nextPhysChunk;
 				}
@@ -553,8 +549,9 @@ static void __init build_iSeries_Memory_
 		++currDword;
 		currChunk += 64;
 	}
-					
-	/* main store size (in chunks) is 
+
+	/*
+	 * main store size (in chunks) is 
 	 *   totalChunks - hptSizeChunks
 	 * which should be equal to 
 	 *   nextPhysChunk
@@ -562,12 +559,12 @@ static void __init build_iSeries_Memory_
 	systemcfg->physicalMemorySize = chunk_to_addr(nextPhysChunk);
 
 	/* Bolt kernel mappings for all of memory */
-	iSeries_bolt_kernel( 0, systemcfg->physicalMemorySize );
+	iSeries_bolt_kernel(0, systemcfg->physicalMemorySize);
 
 	lmb_init();
-	lmb_add( 0, systemcfg->physicalMemorySize );
+	lmb_add(0, systemcfg->physicalMemorySize);
 	lmb_analyze();	/* ?? */
-	lmb_reserve( 0, __pa(klimit));
+	lmb_reserve(0, __pa(klimit));
 
 	/* 
 	 * Hardcode to GP size.  I am not sure where to get this info. DRENG
@@ -579,59 +576,94 @@ static void __init build_iSeries_Memory_
  * Set up the variables that describe the cache line sizes
  * for this machine.
  */
-
 static void __init setup_iSeries_cache_sizes(void)
 {
 	unsigned int i, n;
 	unsigned int procIx = get_paca()->xLpPaca.xDynHvPhysicalProcIndex;
 
-	systemcfg->iCacheL1Size = xIoHriProcessorVpd[procIx].xInstCacheSize * 1024;
-	systemcfg->iCacheL1LineSize = xIoHriProcessorVpd[procIx].xInstCacheOperandSize;
-	systemcfg->dCacheL1Size = xIoHriProcessorVpd[procIx].xDataL1CacheSizeKB * 1024;
-	systemcfg->dCacheL1LineSize = xIoHriProcessorVpd[procIx].xDataCacheOperandSize;
+	systemcfg->iCacheL1Size =
+		xIoHriProcessorVpd[procIx].xInstCacheSize * 1024;
+	systemcfg->iCacheL1LineSize =
+		xIoHriProcessorVpd[procIx].xInstCacheOperandSize;
+	systemcfg->dCacheL1Size =
+		xIoHriProcessorVpd[procIx].xDataL1CacheSizeKB * 1024;
+	systemcfg->dCacheL1LineSize =
+		xIoHriProcessorVpd[procIx].xDataCacheOperandSize;
 	naca->iCacheL1LinesPerPage = PAGE_SIZE / systemcfg->iCacheL1LineSize;
 	naca->dCacheL1LinesPerPage = PAGE_SIZE / systemcfg->dCacheL1LineSize;
 
 	i = systemcfg->iCacheL1LineSize;
 	n = 0;
-	while ((i=(i/2))) ++n;
+	while ((i = (i / 2)))
+		++n;
 	naca->iCacheL1LogLineSize = n;
 
 	i = systemcfg->dCacheL1LineSize;
 	n = 0;
-	while ((i=(i/2))) ++n;
+	while ((i = (i / 2)))
+		++n;
 	naca->dCacheL1LogLineSize = n;
 
-	printk( "D-cache line size = %d\n", (unsigned int)systemcfg->dCacheL1LineSize);
-	printk( "I-cache line size = %d\n", (unsigned int)systemcfg->iCacheL1LineSize);
+	printk("D-cache line size = %d\n",
+			(unsigned int)systemcfg->dCacheL1LineSize);
+	printk("I-cache line size = %d\n",
+			(unsigned int)systemcfg->iCacheL1LineSize);
 }
 
 /*
- * Bolt the kernel addr space into the HPT
+ * Create a pte. Used during initialization only.
  */
+static void iSeries_make_pte(unsigned long va, unsigned long pa,
+			     int mode)
+{
+	HPTE local_hpte, rhpte;
+	unsigned long hash, vpn;
+	long slot;
+
+	vpn = va >> PAGE_SHIFT;
+	hash = hpt_hash(vpn, 0);
+
+	local_hpte.dw1.dword1 = pa | mode;
+	local_hpte.dw0.dword0 = 0;
+	local_hpte.dw0.dw0.avpn = va >> 23;
+	local_hpte.dw0.dw0.bolted = 1;		/* bolted */
+	local_hpte.dw0.dw0.v = 1;
+
+	slot = HvCallHpt_findValid(&rhpte, vpn);
+	if (slot < 0) {
+		/* Must find space in primary group */
+		panic("hash_page: hpte already exists\n");
+	}
+	HvCallHpt_addValidate(slot, 0, (HPTE *)&local_hpte );
+}
 
+/*
+ * Bolt the kernel addr space into the HPT
+ */
 static void __init iSeries_bolt_kernel(unsigned long saddr, unsigned long eaddr)
 {
 	unsigned long pa;
 	unsigned long mode_rw = _PAGE_ACCESSED | _PAGE_COHERENT | PP_RWXX;
 	HPTE hpte;
 
-	for (pa=saddr; pa < eaddr ;pa+=PAGE_SIZE) {
+	for (pa = saddr; pa < eaddr ;pa += PAGE_SIZE) {
 		unsigned long ea = (unsigned long)__va(pa);
-		unsigned long vsid = get_kernel_vsid( ea );
-		unsigned long va = ( vsid << 28 ) | ( pa & 0xfffffff );
+		unsigned long vsid = get_kernel_vsid(ea);
+		unsigned long va = (vsid << 28) | (pa & 0xfffffff);
 		unsigned long vpn = va >> PAGE_SHIFT;
-		unsigned long slot = HvCallHpt_findValid( &hpte, vpn );
-		if ( hpte.dw0.dw0.v ) {
+		unsigned long slot = HvCallHpt_findValid(&hpte, vpn);
+
+		if (hpte.dw0.dw0.v) {
 			/* HPTE exists, so just bolt it */
-			HvCallHpt_setSwBits( slot, 0x10, 0 );
-		} else {
+			HvCallHpt_setSwBits(slot, 0x10, 0);
+			/* And make sure the pp bits are correct */
+			HvCallHpt_setPp(slot, PP_RWXX);
+		} else
 			/* No HPTE exists, so create a new bolted one */
-			build_valid_hpte(vsid, ea, pa, NULL, mode_rw, 1);
-		}
+			iSeries_make_pte(va, (unsigned long)__v2a(ea),
+					mode_rw);
 	}
 }
-#endif /* CONFIG_PPC_ISERIES */
 
 extern unsigned long ppc_proc_freq;
 extern unsigned long ppc_tb_freq;
@@ -639,10 +671,9 @@ extern unsigned long ppc_tb_freq;
 /*
  * Document me.
  */
-void __init
-iSeries_setup_arch(void)
+void __init iSeries_setup_arch(void)
 {
-	void *	eventStack;
+	void *eventStack;
 	unsigned procIx = get_paca()->xLpPaca.xDynHvPhysicalProcIndex;
 
 	/* Add an eye catcher and the systemcfg layout version number */
@@ -657,50 +688,43 @@ iSeries_setup_arch(void)
 	 * we subtract out the KERNELBASE and add in the
 	 * absolute real address of the kernel load area
 	 */
-	
-	eventStack = alloc_bootmem_pages( LpEventStackSize );
-	
-	memset( eventStack, 0, LpEventStackSize );
+	eventStack = alloc_bootmem_pages(LpEventStackSize);
+	memset(eventStack, 0, LpEventStackSize);
 	
 	/* Invoke the hypervisor to initialize the event stack */
-	
-	HvCallEvent_setLpEventStack( 0, eventStack, LpEventStackSize );
-	
+	HvCallEvent_setLpEventStack(0, eventStack, LpEventStackSize);
+
 	/* Initialize fields in our Lp Event Queue */
-	
 	xItLpQueue.xSlicEventStackPtr = (char *)eventStack;
 	xItLpQueue.xSlicCurEventPtr = (char *)eventStack;
 	xItLpQueue.xSlicLastValidEventPtr = (char *)eventStack + 
 					(LpEventStackSize - LpEventMaxSize);
 	xItLpQueue.xIndex = 0;
-	
+
 	/* Compute processor frequency */
-	procFreqHz = (((1UL<<34) * 1000000) / xIoHriProcessorVpd[procIx].xProcFreq );
+	procFreqHz = ((1UL << 34) * 1000000) /
+			xIoHriProcessorVpd[procIx].xProcFreq;
 	procFreqMhz = procFreqHz / 1000000;
-	procFreqMhzHundreths = (procFreqHz/10000) - (procFreqMhz*100);
-
+	procFreqMhzHundreths = (procFreqHz / 10000) - (procFreqMhz * 100);
 	ppc_proc_freq = procFreqHz;
 
 	/* Compute time base frequency */
-	tbFreqHz = (((1UL<<32) * 1000000) / xIoHriProcessorVpd[procIx].xTimeBaseFreq );
+	tbFreqHz = ((1UL << 32) * 1000000) /
+		xIoHriProcessorVpd[procIx].xTimeBaseFreq;
 	tbFreqMhz = tbFreqHz / 1000000;
-	tbFreqMhzHundreths = (tbFreqHz/10000) - (tbFreqMhz*100);
-
+	tbFreqMhzHundreths = (tbFreqHz / 10000) - (tbFreqMhz * 100);
 	ppc_tb_freq = tbFreqHz;
 
 	printk("Max  logical processors = %d\n", 
-			itVpdAreas.xSlicMaxLogicalProcs );
+			itVpdAreas.xSlicMaxLogicalProcs);
 	printk("Max physical processors = %d\n",
-			itVpdAreas.xSlicMaxPhysicalProcs );
-	printk("Processor frequency = %lu.%02lu\n",
-			procFreqMhz, 
-			procFreqMhzHundreths );
-	printk("Time base frequency = %lu.%02lu\n",
-			tbFreqMhz,
-			tbFreqMhzHundreths );
+			itVpdAreas.xSlicMaxPhysicalProcs);
+	printk("Processor frequency = %lu.%02lu\n", procFreqMhz,
+			procFreqMhzHundreths);
+	printk("Time base frequency = %lu.%02lu\n", tbFreqMhz,
+			tbFreqMhzHundreths);
 	systemcfg->processor = xIoHriProcessorVpd[procIx].xPVR;
 	printk("Processor version = %x\n", systemcfg->processor);
-
 }
 
 /*
@@ -715,38 +739,27 @@ iSeries_setup_arch(void)
  *
  * Output(s):
  *  *buffer - Buffer with CPU data.
- *
- * Returns:
- *   The number of bytes copied into 'buffer' if OK, otherwise zero or less
- *   on error.
  */
-void iSeries_setup_residual(struct seq_file *m)
+void iSeries_setup_residual(struct seq_file *m, int cpu_id)
 {
-	
-	seq_printf(m,"clock\t\t: %lu.%02luMhz\n",
-		procFreqMhz, procFreqMhzHundreths );
-	seq_printf(m,"time base\t: %lu.%02luMHz\n",
-		tbFreqMhz, tbFreqMhzHundreths );
-	seq_printf(m,"i-cache\t\t: %d\n",
-		systemcfg->iCacheL1LineSize);
-	seq_printf(m,"d-cache\t\t: %d\n",
-		systemcfg->dCacheL1LineSize);
-
+	seq_printf(m, "clock\t\t: %lu.%02luMhz\n", procFreqMhz,
+			procFreqMhzHundreths);
+	seq_printf(m, "time base\t: %lu.%02luMHz\n", tbFreqMhz,
+			tbFreqMhzHundreths);
+	seq_printf(m, "i-cache\t\t: %d\n", systemcfg->iCacheL1LineSize);
+	seq_printf(m, "d-cache\t\t: %d\n", systemcfg->dCacheL1LineSize);
 }
 
 void iSeries_get_cpuinfo(struct seq_file *m)
 {
-
-	seq_printf(m,"machine\t\t: 64-bit iSeries Logical Partition\n");
-
+	seq_printf(m, "machine\t\t: 64-bit iSeries Logical Partition\n");
 }
 
 /*
  * Document me.
  * and Implement me.
  */
-int
-iSeries_get_irq(struct pt_regs *regs)
+int iSeries_get_irq(struct pt_regs *regs)
 {
 	/* -2 means ignore this interrupt */
 	return -2;
@@ -755,8 +768,7 @@ iSeries_get_irq(struct pt_regs *regs)
 /*
  * Document me.
  */
-void
-iSeries_restart(char *cmd)
+void iSeries_restart(char *cmd)
 {
 	mf_reboot();
 }
@@ -764,8 +776,7 @@ iSeries_restart(char *cmd)
 /*
  * Document me.
  */
-void
-iSeries_power_off(void)
+void iSeries_power_off(void)
 {
 	mf_powerOff();
 }
@@ -773,8 +784,7 @@ iSeries_power_off(void)
 /*
  * Document me.
  */
-void
-iSeries_halt(void)
+void iSeries_halt(void)
 {
 	mf_powerOff();
 }
@@ -792,24 +802,19 @@ extern void setup_default_decr(void);
  *   and sets up the kernel timer decrementer based on that value.
  *
  */
-void __init
-iSeries_calibrate_decr(void)
+void __init iSeries_calibrate_decr(void)
 {
 	unsigned long	cyclesPerUsec;
-
 	struct div_result divres;
 	
-	/* Compute decrementer (and TB) frequency 
-	 * in cycles/sec 
-	 */
+	/* Compute decrementer (and TB) frequency in cycles/sec */
+	cyclesPerUsec = ppc_tb_freq / 1000000;
 
-	cyclesPerUsec = ppc_tb_freq / 1000000;	/* cycles / usec */
-
-	/* Set the amount to refresh the decrementer by.  This
+	/*
+	 * Set the amount to refresh the decrementer by.  This
 	 * is the number of decrementer ticks it takes for 
 	 * 1/HZ seconds.
 	 */
-
 	tb_ticks_per_jiffy = ppc_tb_freq / HZ;
 
 #if 0
@@ -824,47 +829,54 @@ iSeries_calibrate_decr(void)
 	 * that jiffies (and xtime) will match the time returned
 	 * by do_gettimeofday.
 	 */
-	tb_ticks_per_sec   = tb_ticks_per_jiffy * HZ;
+	tb_ticks_per_sec = tb_ticks_per_jiffy * HZ;
 	tb_ticks_per_usec = cyclesPerUsec;
 	tb_to_us = mulhwu_scale_factor(ppc_tb_freq, 1000000);
-	div128_by_32( 1024*1024, 0, tb_ticks_per_sec, &divres );
+	div128_by_32(1024 * 1024, 0, tb_ticks_per_sec, &divres);
 	tb_to_xs = divres.result_low;
 	setup_default_decr();
 }
 
-void __init
-iSeries_progress( char * st, unsigned short code )
+void __init iSeries_progress(char * st, unsigned short code)
 {
-	printk( "Progress: [%04x] - %s\n", (unsigned)code, st );
-	if ( !piranha_simulator && mf_initialized ) {
-	    if (code != 0xffff)
-		mf_displayProgress( code );
-	    else
-		mf_clearSrc();
+	printk("Progress: [%04x] - %s\n", (unsigned)code, st);
+	if (!piranha_simulator && mf_initialized) {
+		if (code != 0xffff)
+			mf_displayProgress(code);
+		else
+			mf_clearSrc();
 	}
 }
 
-
 void iSeries_fixup_klimit(void)
 {
-	/* Change klimit to take into account any ram disk that may be included */
+	/*
+	 * Change klimit to take into account any ram disk
+	 * that may be included
+	 */
 	if (naca->xRamDisk)
-		klimit = KERNELBASE + (u64)naca->xRamDisk + (naca->xRamDiskSize * PAGE_SIZE);
+		klimit = KERNELBASE + (u64)naca->xRamDisk +
+			(naca->xRamDiskSize * PAGE_SIZE);
 	else {
-		/* No ram disk was included - check and see if there was an embedded system map */
-		/* Change klimit to take into account any embedded system map */
+		/*
+		 * No ram disk was included - check and see if there
+		 * was an embedded system map.  Change klimit to take
+		 * into account any embedded system map
+		 */
 		if (embedded_sysmap_end)
-			klimit = KERNELBASE + ((embedded_sysmap_end+4095) & 0xfffffffffffff000);
+			klimit = KERNELBASE + ((embedded_sysmap_end + 4095) &
+					0xfffffffffffff000);
 	}
 }
 
 static void iSeries_setup_dprofile(void)
 {
-	if ( dprof_buffer ) {
+	if (dprof_buffer) {
 		unsigned i;
-		for (i=0; i<NR_CPUS; ++i) {
+
+		for (i = 0; i < NR_CPUS; ++i) {
 			paca[i].prof_shift = dprof_shift;
-			paca[i].prof_len = dprof_len-1;
+			paca[i].prof_len = dprof_len - 1;
 			paca[i].prof_buffer = dprof_buffer;
 			paca[i].prof_stext = (unsigned *)_stext;
 			mb();
diff -purN linux-2.5/arch/ppc64/kernel/iSeries_setup.h linuxppc64-2.5/arch/ppc64/kernel/iSeries_setup.h
--- linux-2.5/arch/ppc64/kernel/iSeries_setup.h	2002-03-26 07:32:20.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/iSeries_setup.h	2003-11-21 06:45:02.000000000 +0000
@@ -19,25 +19,24 @@
 #ifndef	__ISERIES_SETUP_H__
 #define	__ISERIES_SETUP_H__
 
-extern void		 iSeries_init_early(void);
-extern void		 iSeries_init(unsigned long r3,
-			            unsigned long ird_start,
-				    unsigned long ird_end,
-				    unsigned long cline_start,
-				    unsigned long cline_end);
-extern void		 iSeries_setup_arch(void);
-extern void		 iSeries_setup_residual(struct seq_file *m);
-extern void		 iSeries_get_cpuinfo(struct seq_file *m);
-extern void		 iSeries_init_IRQ(void);
-extern int		 iSeries_get_irq(struct pt_regs *regs);
-extern void		 iSeries_restart(char *cmd);
-extern void		 iSeries_power_off(void);
-extern void		 iSeries_halt(void);
-extern void		 iSeries_time_init(void);
-extern void      iSeries_get_boot_time(struct rtc_time *tm);
-extern int		 iSeries_set_rtc_time(unsigned long now);
-extern unsigned long	 iSeries_get_rtc_time(void);
-extern void		 iSeries_calibrate_decr(void);
-extern void 	 iSeries_progress( char *, unsigned short );
+extern void iSeries_init_early(void);
+extern void iSeries_init(unsigned long r3, unsigned long ird_start,
+		unsigned long ird_end, unsigned long cline_start,
+		unsigned long cline_end);
+extern void iSeries_setup_arch(void);
+extern void iSeries_setup_residual(struct seq_file *m, int cpu_id);
+extern void iSeries_get_cpuinfo(struct seq_file *m);
+extern void iSeries_init_IRQ(void);
+extern void iSeries_init_irq_desc(irq_desc_t *);
+extern int iSeries_get_irq(struct pt_regs *regs);
+extern void iSeries_restart(char *cmd);
+extern void iSeries_power_off(void);
+extern void iSeries_halt(void);
+extern void iSeries_time_init(void);
+extern void iSeries_get_boot_time(struct rtc_time *tm);
+extern int iSeries_set_rtc_time(struct rtc_time *tm);
+extern void iSeries_get_rtc_time(struct rtc_time *tm);
+extern void iSeries_calibrate_decr(void);
+extern void iSeries_progress( char *, unsigned short );
 
 #endif /* __ISERIES_SETUP_H__ */
diff -purN linux-2.5/arch/ppc64/kernel/idle.c linuxppc64-2.5/arch/ppc64/kernel/idle.c
--- linux-2.5/arch/ppc64/kernel/idle.c	2003-09-11 04:07:58.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/idle.c	2003-11-24 04:24:35.000000000 +0000
@@ -1,5 +1,13 @@
 /*
- * idle.c
+ * Idle daemon for PowerPC.  Idle daemon will handle any action
+ * that needs to be taken when the system becomes idle.
+ *
+ * Originally Written by Cort Dougan (cort@cs.nmt.edu)
+ *
+ * iSeries supported added by Mike Corrigan <mikejc@us.ibm.com>
+ *
+ * Additional shared processor, SMT, and firmware support
+ *    Copyright (c) 2003 Dave Engebretsen <engebret@us.ibm.com>
  *
  * This program is free software; you can redistribute it and/or
  * modify it under the terms of the GNU General Public License
@@ -26,14 +34,18 @@
 #include <asm/processor.h>
 #include <asm/mmu.h>
 #include <asm/cache.h>
+#include <asm/cputable.h>
 #include <asm/time.h>
-
-#ifdef CONFIG_PPC_ISERIES
-
 #include <asm/iSeries/LparData.h>
 #include <asm/iSeries/HvCall.h>
 #include <asm/iSeries/ItLpQueue.h>
 
+extern long cede_processor(void);
+extern long poll_pending(void);
+
+int (*idle_loop)(void);
+
+#ifdef CONFIG_PPC_ISERIES
 unsigned long maxYieldTime = 0;
 unsigned long minYieldTime = 0xffffffffffffffffUL;
 
@@ -66,24 +78,24 @@ static void yield_shared_processor(void)
 	process_iSeries_events();
 }
 
-int cpu_idle(void)
+int iSeries_idle(void)
 {
 	struct paca_struct *lpaca;
 	long oldval;
 	unsigned long CTRL;
 
-#warning fix iseries run light
-#if 0
 	/* ensure iSeries run light will be out when idle */
-	current->thread.flags &= ~PPC_FLAG_RUN_LIGHT;
+	clear_thread_flag(TIF_RUN_LIGHT);
 	CTRL = mfspr(CTRLF);
 	CTRL &= ~RUNLATCH;
 	mtspr(CTRLT, CTRL);
+#if 0
+	init_idle();	
 #endif
 
 	lpaca = get_paca();
 
-	while (1) {
+	for (;;) {
 		if (lpaca->xLpPaca.xSharedProc) {
 			if (ItLpQueue_isLpIntPending(lpaca->lpQueuePtr))
 				process_iSeries_events();
@@ -109,16 +121,13 @@ int cpu_idle(void)
 			}
 		}
 
-		if (need_resched())
-			schedule();
+		schedule();
 	}
-
 	return 0;
 }
+#endif
 
-#else /* CONFIG_PPC_ISERIES */
-
-int cpu_idle(void)
+int default_idle(void)
 {
 	long oldval;
 
@@ -145,9 +154,155 @@ int cpu_idle(void)
 	return 0;
 }
 
-#endif /* CONFIG_PPC_ISERIES */
+#ifdef CONFIG_PPC_PSERIES
+int dedicated_idle(void)
+{
+	long oldval;
+	struct paca_struct *lpaca = get_paca(), *ppaca;
+	unsigned long start_snooze;
+
+	ppaca = &paca[(lpaca->xPacaIndex) ^ 1];
+
+	while (1) {
+		/* Indicate to the HV that we are idle.  Now would be
+		 * a good time to find other work to dispatch. */
+		lpaca->xLpPaca.xIdle = 1;
+
+		oldval = test_and_clear_thread_flag(TIF_NEED_RESCHED);
+		if (!oldval) {
+			set_thread_flag(TIF_POLLING_NRFLAG);
+			start_snooze = __get_tb();
+			while (!need_resched()) {
+				/* need_resched could be 1 or 0 at this 
+				 * point.  If it is 0, set it to 0, so
+				 * an IPI/Prod is sent.  If it is 1, keep
+				 * it that way & schedule work.
+				 */
+				if (__get_tb() < 
+				    (start_snooze + 
+				     naca->smt_snooze_delay*tb_ticks_per_usec)) {  
+					HMT_low(); /* Low thread priority */
+					continue;
+				}
+
+				HMT_very_low(); /* Low power mode */
+
+				/* If the SMT mode is system controlled & the 
+				 * partner thread is doing work, switch into
+				 * ST mode.
+				 */
+				if((naca->smt_state == SMT_DYNAMIC) &&
+				   (!(ppaca->xLpPaca.xIdle))) {
+					/* Indicate we are no longer polling for
+					 * work, and then clear need_resched.  If
+					 * need_resched was 1, set it back to 1
+					 * and schedule work
+					 */
+					clear_thread_flag(TIF_POLLING_NRFLAG);
+					oldval = test_and_clear_thread_flag(TIF_NEED_RESCHED);
+					if(oldval == 1) {
+						set_need_resched();
+						break;
+					}
+
+					/* DRENG: Go HMT_medium here ? */
+					local_irq_disable(); 
+					lpaca->yielded = 1;
+
+					/* SMT dynamic mode.  Cede will result 
+					 * in this thread going dormant, if the
+					 * partner thread is still doing work.
+					 * Thread wakes up if partner goes idle,
+					 * an interrupt is presented, or a prod
+					 * occurs.  Returning from the cede
+					 * enables external interrupts.
+					 */
+					cede_processor();
+
+					lpaca->yielded = 0;
+				} else {
+					/* Give the HV an opportunity at the
+					 * processor, since we are not doing
+					 * any work.
+					 */
+					poll_pending();
+				}
+			}
+		} else {
+			set_need_resched();
+		}
+
+		HMT_medium();
+		lpaca->xLpPaca.xIdle = 0;
+		schedule();
+	}
+	return 0;
+}
+
+int shared_idle(void)
+{
+	struct paca_struct *lpaca = get_paca();
+
+	while (1) {
+		/* Indicate to the HV that we are idle.  Now would be
+		 * a good time to find other work to dispatch. */
+		lpaca->xLpPaca.xIdle = 1;
+
+		if (!need_resched()) {
+			local_irq_disable(); 
+			lpaca->yielded = 1;
+			
+			/* 
+			 * Yield the processor to the hypervisor.  We return if
+			 * an external interrupt occurs (which are driven prior
+			 * to returning here) or if a prod occurs from another 
+			 * processor.  When returning here, external interrupts 
+			 * are enabled.
+			 */
+			cede_processor();
+			
+			lpaca->yielded = 0;
+		}
+
+		HMT_medium();
+		lpaca->xLpPaca.xIdle = 0;
+		schedule();
+	}
+
+	return 0;
+}
+#endif
+
+int cpu_idle(void)
+{
+	idle_loop();
+	return 0; 
+}
 
-void default_idle(void)
+int idle_setup(void)
 {
-	barrier();
+#ifdef CONFIG_PPC_ISERIES
+	idle_loop = iSeries_idle;
+#else
+	if (systemcfg->platform & PLATFORM_PSERIES) {
+		if (cur_cpu_spec->firmware_features & FW_FEATURE_SPLPAR) {
+			if(get_paca()->xLpPaca.xSharedProc) {
+				printk("idle = shared_idle\n");
+				idle_loop = shared_idle;
+			} else {
+				printk("idle = dedicated_idle\n");
+				idle_loop = dedicated_idle;
+			}
+		} else {
+			printk("idle = default_idle\n");
+			idle_loop = default_idle;
+		}
+	} else {
+		printk("idle_setup: unknown platform, use default_idle\n");
+		idle_loop = default_idle;
+	}
+#endif
+
+	return 1;
 }
+
diff -purN linux-2.5/arch/ppc64/kernel/irq.c linuxppc64-2.5/arch/ppc64/kernel/irq.c
--- linux-2.5/arch/ppc64/kernel/irq.c	2003-10-16 01:43:36.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/irq.c	2003-12-10 04:13:06.000000000 +0000
@@ -40,6 +40,7 @@
 #include <linux/irq.h>
 #include <linux/proc_fs.h>
 #include <linux/random.h>
+#include <linux/bootmem.h>
 #include <linux/kallsyms.h>
 
 #include <asm/uaccess.h>
@@ -55,30 +56,297 @@
 #include <asm/machdep.h>
 #include <asm/paca.h>
 
+/*
+ * Because the name space for interrupts is so large on ppc64 systems we
+ * avoid declaring a single array of "NR_IRQ" interrupts and instead build
+ * a three level tree leading to the irq_desc_t (similar to page tables).
+ *
+ * Currently we cover 24-bit irq values:
+ *    10-bits:  the "base" dir (2-pages)
+ *     9-bits:  the "middle" dir (1-page)
+ *     5-bits:  the "bottom" page (1-page) holding 128byte irq_desc's.
+ *
+ * We pack a hw_irq_stat struct directly after the irq_desc in the otherwise
+ * wasted space of the cacheline.
+ *
+ * MAX_IRQS is the max this implementation will support.
+ * It is much larger than NR_IRQS which is bogus on this arch and often used
+ * to declare arrays.
+ *
+ * Note that all "undefined" mid table and bottom table pointers will point
+ * to dummy tables.  Therefore, we don't need to check for NULL on spurious
+ * interrupts.
+ */
+
+#define IRQ_BASE_INDEX_SIZE  10
+#define IRQ_MID_INDEX_SIZE  9
+#define IRQ_BOT_DESC_SIZE 5
+
+#define IRQ_BASE_PTRS	(1 << IRQ_BASE_INDEX_SIZE)
+#define IRQ_MID_PTRS	(1 << IRQ_MID_INDEX_SIZE)
+#define IRQ_BOT_DESCS (1 << IRQ_BOT_DESC_SIZE)
+
+#define IRQ_BASE_IDX_SHIFT (IRQ_MID_INDEX_SIZE + IRQ_BOT_DESC_SIZE)
+#define IRQ_MID_IDX_SHIFT (IRQ_BOT_DESC_SIZE)
+
+#define IRQ_MID_IDX_MASK  ((1 << IRQ_MID_INDEX_SIZE) - 1)
+#define IRQ_BOT_IDX_MASK  ((1 << IRQ_BOT_DESC_SIZE) - 1)
+
+
+/* The hw_irq_stat struct is stored directly after the irq_desc_t
+ * in the same cacheline.  We need to use care to make sure we don't
+ * overrun the size of the cacheline.
+ *
+ * Currently sizeof(irq_desc_t) is 32 bytes or less and this hw_irq_stat
+ * fills the rest of the cache line.
+ *
+ * The irqs_per_cpu field is an optimization for systems with 4 cpus or less
+ * to avoid allocating space for irq-per-cpu statistics (and hitting another
+ * cacheline to do the counting).  This field could be discarded.
+ */
+struct hw_irq_stat {
+	unsigned long irqs;		/* statistic per irq */
+	unsigned long *per_cpu_stats;
+	struct proc_dir_entry *irq_dir, *smp_affinity;
+	cpumask_t irq_affinity;
+	unsigned long irqs_per_cpu[4];
+};
+static inline struct hw_irq_stat *get_irq_stat(irq_desc_t *desc)
+{
+	/* WARNING: this assume lock is the last field! */
+	return (struct hw_irq_stat *)(&desc->lock+1);
+}
+static inline unsigned long *get_irq_per_cpu(struct hw_irq_stat *hw)
+{
+	return hw->per_cpu_stats;
+}
+
 #ifdef CONFIG_SMP
 extern void iSeries_smp_message_recv( struct pt_regs * );
 #endif
 
-volatile unsigned char *chrp_int_ack_special;
 static void register_irq_proc (unsigned int irq);
-
-irq_desc_t irq_desc[NR_IRQS] __cacheline_aligned = {
-	[0 ... NR_IRQS-1] = {
-		.lock = SPIN_LOCK_UNLOCKED
-	}
-};
+static irq_desc_t *add_irq_desc(unsigned int irq);
 	
 int ppc_spurious_interrupts = 0;
 unsigned long lpEvent_count = 0;
 
+extern int mem_init_done;
+
+irq_desc_t **irq_desc_base_dir[IRQ_BASE_PTRS] __page_aligned = {0};
+irq_desc_t **irq_desc_mid_null;
+irq_desc_t *irq_desc_bot_null;
+
+static inline irq_desc_t **get_irq_mid_table(unsigned int irq)
+{
+	/* Assume irq < MAX_IRQS so we won't index off the end. */
+	return irq_desc_base_dir[irq >> IRQ_BASE_IDX_SHIFT];
+}
+
+static inline irq_desc_t *get_irq_bot_table(unsigned int irq,
+					    irq_desc_t **mid_ptr)
+{
+	return mid_ptr[(irq >> IRQ_MID_IDX_SHIFT) & IRQ_MID_IDX_MASK];
+}
+
+
+/* This should be inline. */
+void *_get_irq_desc(unsigned int irq)
+{
+	irq_desc_t **mid_table, *bot_table, *desc;
+
+	mid_table = get_irq_mid_table(irq);
+	bot_table = get_irq_bot_table(irq, mid_table);
+
+	desc = bot_table + (irq & IRQ_BOT_IDX_MASK);
+	return desc;
+}
+
+/* This is used by the for_each_irq(i) macro to iterate quickly over
+ * all interrupt.  It optimizes by skipping over ptrs to the null tables
+ * when possible, but it may produce false positives.
+ */
+unsigned int _next_irq(unsigned int irq)
+{
+	irq_desc_t **mid_table, *bot_table;
+
+	irq++;
+	/* Easy case first...staying on the current bot_table. */
+	if (irq & IRQ_BOT_IDX_MASK)
+		return irq;
+
+	/* Now skip empty mid tables */
+	while (irq < MAX_IRQS &&
+	       (mid_table = get_irq_mid_table(irq)) == irq_desc_mid_null) {
+		/* index to the next base index (i.e. the next mid table) */
+		irq = (irq & ~(IRQ_BASE_IDX_SHIFT-1)) + IRQ_BASE_IDX_SHIFT;
+	}
+	/* And skip empty bot tables */
+	while (irq < MAX_IRQS &&
+	       (bot_table = get_irq_bot_table(irq, mid_table)) == irq_desc_bot_null) {
+		/* index to the next mid index (i.e. the next bot table) */
+		irq = (irq & ~(IRQ_MID_IDX_SHIFT-1)) + IRQ_MID_IDX_SHIFT;
+	}
+	return irq;
+}
+
+
+/* Same as get_irq_desc(irq) except it will "fault in" a real desc as needed
+ * rather than return the null entry.
+ * This is used by code that is actually defining the irq.
+ *
+ * NULL may be returned on memory allocation failure.  In general, init code
+ * doesn't look for this, but setup_irq does.  In this failure case the desc
+ * is left pointing at the null pages so callers of get_irq_desc() should
+ * always return something.
+ */
+void *_get_real_irq_desc(unsigned int irq)
+{
+	irq_desc_t *desc = get_irq_desc(irq);
+	if (((unsigned long)desc & PAGE_MASK) ==
+	    (unsigned long)irq_desc_bot_null) {
+		desc = add_irq_desc(irq);
+	}
+	return desc;
+}
+
+/* Allocate an irq middle page and init entries to null page. */
+static irq_desc_t **alloc_irq_mid_page(void)
+{
+	irq_desc_t **m, **ent;
+
+	if (mem_init_done)
+		m = (irq_desc_t **)__get_free_page(GFP_KERNEL);
+	else
+		m = (irq_desc_t **)alloc_bootmem_pages(PAGE_SIZE);
+	if (m) {
+		for (ent = m; ent < m + IRQ_MID_PTRS; ent++) {
+			*ent = irq_desc_bot_null;
+		}
+	}
+	return m;
+}
+
+/* Allocate an irq bottom page and init the entries. */
+static irq_desc_t *alloc_irq_bot_page(void)
+{
+	irq_desc_t *b, *ent;
+	if (mem_init_done)
+		b = (irq_desc_t *)get_zeroed_page(GFP_KERNEL);
+	else
+		b = (irq_desc_t *)alloc_bootmem_pages(PAGE_SIZE);
+	if (b) {
+		for (ent = b; ent < b + IRQ_BOT_DESCS; ent++) {
+			ent->lock = SPIN_LOCK_UNLOCKED;
+		}
+	}
+	return b;
+}
+
+/*
+ * The universe of interrupt numbers ranges from 0 to 2^24.
+ * Use a sparsely populated tree to map from the irq to the handler.
+ * Top level is 2 contiguous pages, covering the 10 most significant 
+ * bits.  Mid level is 1 page, covering 9 bits.  Last page covering
+ * 5 bits is the irq_desc, each of which is 128B.
+ */
+static void irq_desc_init(void) {
+	irq_desc_t ***entry_p;
+
+	/* 
+	 * Now initialize the tables to point though the NULL tables for
+	 * the default case of no interrupt handler (spurious).
+	 */
+	irq_desc_bot_null = alloc_irq_bot_page();
+	irq_desc_mid_null = alloc_irq_mid_page();
+	if (!irq_desc_bot_null || !irq_desc_mid_null)
+		panic("irq_desc_init: could not allocate pages\n");
+	for(entry_p = irq_desc_base_dir;
+	    entry_p < irq_desc_base_dir + IRQ_BASE_PTRS;
+	    entry_p++) {
+		*entry_p = irq_desc_mid_null;
+	}
+}
+
+/*
+ * Add a new irq desc for the given irq if needed.
+ * This breaks any ptr to the "null" middle or "bottom" irq desc page.
+ * Note that we don't ever coalesce pages as the interrupts are released.
+ * This isn't worth the effort.  We add the cpu stats info when the
+ * interrupt is actually requested.
+ *
+ * May return NULL if memory could not be allocated.
+ */
+static irq_desc_t *add_irq_desc(unsigned int irq)
+{
+	irq_desc_t **mid_table_p, *bot_table_p;
+
+	mid_table_p = get_irq_mid_table(irq); 
+	if(mid_table_p == irq_desc_mid_null) {
+		/* No mid table for this IRQ - create it */
+		mid_table_p = alloc_irq_mid_page();
+		if (!mid_table_p) return NULL;
+		irq_desc_base_dir[irq >> IRQ_BASE_IDX_SHIFT] = mid_table_p;
+	}
+
+	bot_table_p = (irq_desc_t *)(*(mid_table_p + ((irq >> 5) & 0x1ff)));
+
+	if(bot_table_p == irq_desc_bot_null) {
+		/* No bot table for this IRQ - create it */
+		bot_table_p = alloc_irq_bot_page();
+		if (!bot_table_p) return NULL;
+		mid_table_p[(irq >> IRQ_MID_IDX_SHIFT) & IRQ_MID_IDX_MASK] = bot_table_p;
+	}
+
+	return bot_table_p + (irq & IRQ_BOT_IDX_MASK);
+}
+
+void allocate_per_cpu_stats(struct hw_irq_stat *hwstat)
+{
+	unsigned long *p;
+
+	if (naca->interrupt_controller == IC_OPEN_PIC) {
+		/* Cheap optimization -- assume max cpus on open pic is 4
+		 * and so they will fit in cacheline after desc.
+		 * ToDo: verify max cpus.  Assume no hot plug?
+		 */
+		hwstat->per_cpu_stats = hwstat->irqs_per_cpu;
+		return;
+	}
+	if (mem_init_done) {
+		p = (unsigned long *)kmalloc(sizeof(long)*NR_CPUS, GFP_KERNEL);
+		if (p) memset(p, 0, sizeof(long)*NR_CPUS);
+	} else
+		p = (unsigned long *)alloc_bootmem(sizeof(long)*NR_CPUS);
+	hwstat->per_cpu_stats = p;
+}
+
 int
 setup_irq(unsigned int irq, struct irqaction * new)
 {
 	int shared = 0;
 	unsigned long flags;
 	struct irqaction *old, **p;
-	irq_desc_t *desc = irq_desc + irq;
+	irq_desc_t *desc = get_real_irq_desc(irq);
+	struct hw_irq_stat *hwstat;
+
+	if (!desc)
+		return -ENOMEM;
+
+	ppc_md.init_irq_desc(desc);
 
+	hwstat = get_irq_stat(desc);
+
+#ifdef CONFIG_IRQ_ALL_CPUS
+	hwstat->irq_affinity = CPU_MASK_ALL;
+#else
+	hwstat->irq_affinity = CPU_MASK_NONE;
+#endif
+
+	/* Now is the time to add per-cpu kstat data to the desc
+	 * since it appears we are actually going to use the irq.
+	 */
+	allocate_per_cpu_stats(hwstat);
 	/*
 	 * Some drivers like serial.c use request_irq() heavily,
 	 * so we have to be careful not to interfere with a
@@ -133,7 +401,7 @@ setup_irq(unsigned int irq, struct irqac
 
 inline void synchronize_irq(unsigned int irq)
 {
-	while (irq_desc[irq].status & IRQ_INPROGRESS)
+	while (get_irq_desc(irq)->status & IRQ_INPROGRESS)
 		cpu_relax();
 }
 
@@ -147,11 +415,10 @@ EXPORT_SYMBOL(synchronize_irq);
 static int
 do_free_irq(int irq, void* dev_id)
 {
-	irq_desc_t *desc;
+	irq_desc_t *desc = get_irq_desc(irq);
 	struct irqaction **p;
 	unsigned long flags;
 
-	desc = irq_desc + irq;
 	spin_lock_irqsave(&desc->lock,flags);
 	p = &desc->action;
 	for (;;) {
@@ -182,41 +449,6 @@ do_free_irq(int irq, void* dev_id)
 	return -ENOENT;
 }
 
-int request_irq(unsigned int irq,
-	irqreturn_t (*handler)(int, void *, struct pt_regs *),
-	unsigned long irqflags, const char * devname, void *dev_id)
-{
-	struct irqaction *action;
-	int retval;
-
-	if (irq >= NR_IRQS)
-		return -EINVAL;
-	if (!handler)
-		/* We could implement really free_irq() instead of that... */
-		return do_free_irq(irq, dev_id);
-	
-	action = (struct irqaction *)
-		kmalloc(sizeof(struct irqaction), GFP_KERNEL);
-	if (!action) {
-		printk(KERN_ERR "kmalloc() failed for irq %d !\n", irq);
-		return -ENOMEM;
-	}
-	
-	action->handler = handler;
-	action->flags = irqflags;					
-	action->mask = 0;
-	action->name = devname;
-	action->dev_id = dev_id;
-	action->next = NULL;
-	
-	retval = setup_irq(irq, action);
-	if (retval)
-		kfree(action);
-		
-	return 0;
-}
-
-EXPORT_SYMBOL(request_irq);
 
 void free_irq(unsigned int irq, void *dev_id)
 {
@@ -245,7 +477,7 @@ EXPORT_SYMBOL(free_irq);
  
 inline void disable_irq_nosync(unsigned int irq)
 {
-	irq_desc_t *desc = irq_desc + irq;
+	irq_desc_t *desc = get_irq_desc(irq);
 	unsigned long flags;
 
 	spin_lock_irqsave(&desc->lock, flags);
@@ -274,7 +506,7 @@ EXPORT_SYMBOL(disable_irq_nosync);
  
 void disable_irq(unsigned int irq)
 {
-	irq_desc_t *desc = irq_desc + irq;
+	irq_desc_t *desc = get_irq_desc(irq);
 	disable_irq_nosync(irq);
 	if (desc->action)
 		synchronize_irq(irq);
@@ -294,13 +526,13 @@ EXPORT_SYMBOL(disable_irq);
  
 void enable_irq(unsigned int irq)
 {
-	irq_desc_t *desc = irq_desc + irq;
+	irq_desc_t *desc = get_irq_desc(irq);
 	unsigned long flags;
 
 	spin_lock_irqsave(&desc->lock, flags);
 	switch (desc->depth) {
 	case 1: {
-		unsigned int status = desc->status & ~(IRQ_DISABLED | IRQ_INPROGRESS);
+		unsigned int status = desc->status & ~IRQ_DISABLED;
 		desc->status = status;
 		if ((status & (IRQ_PENDING | IRQ_REPLAY)) == IRQ_PENDING) {
 			desc->status = status | IRQ_REPLAY;
@@ -321,10 +553,49 @@ void enable_irq(unsigned int irq)
 
 EXPORT_SYMBOL(enable_irq);
 
+int request_irq(unsigned int irq,
+	irqreturn_t (*handler)(int, void *, struct pt_regs *),
+	unsigned long irqflags, const char * devname, void *dev_id)
+{
+	struct irqaction *action;
+	int retval;
+
+	if (irq >= MAX_IRQS)
+		return -EINVAL;
+	if (!handler)
+		/* We could implement really free_irq() instead of that... */
+		return do_free_irq(irq, dev_id);
+	
+	action = (struct irqaction *)
+		kmalloc(sizeof(struct irqaction), GFP_KERNEL);
+	if (!action) {
+		printk(KERN_ERR "kmalloc() failed for irq %d !\n", irq);
+		return -ENOMEM;
+	}
+	
+	action->handler = handler;
+	action->flags = irqflags;					
+	action->mask = 0;
+	action->name = devname;
+	action->dev_id = dev_id;
+	action->next = NULL;
+	
+	retval = setup_irq(irq, action);
+	if (retval)
+		kfree(action);
+		
+	return 0;
+}
+
+EXPORT_SYMBOL(request_irq);
+
 int show_interrupts(struct seq_file *p, void *v)
 {
 	int i, j;
 	struct irqaction * action;
+	irq_desc_t *desc;
+	struct hw_irq_stat *hwstat;
+	unsigned long *per_cpus;
 	unsigned long flags;
 
 	seq_printf(p, "           ");
@@ -334,31 +605,35 @@ int show_interrupts(struct seq_file *p, 
 	}
 	seq_putc(p, '\n');
 
-	for (i = 0 ; i < NR_IRQS ; i++) {
-		spin_lock_irqsave(&irq_desc[i].lock, flags);
-		action = irq_desc[i].action;
+	for_each_irq(i) {
+		desc = get_irq_desc(i);
+		spin_lock_irqsave(&desc->lock, flags);
+		action = desc->action;
+
 		if (!action || !action->handler)
 			goto skip;
-		seq_printf(p, "%3d: ", i);		
-#ifdef CONFIG_SMP
-		for (j = 0; j < NR_CPUS; j++) {
-			if (cpu_online(j))
-				seq_printf(p, "%10u ", kstat_cpu(j).irqs[i]);
+		seq_printf(p, "%3d: ", i);
+		hwstat = get_irq_stat(desc);
+		per_cpus = get_irq_per_cpu(hwstat);
+		if (per_cpus) {
+			for (j = 0; j < NR_CPUS; j++) {
+				if (cpu_online(j))
+					seq_printf(p, "%10lu ", per_cpus[j]);
+			}
+		} else {
+			seq_printf(p, "%10lu ", hwstat->irqs);
 		}
-#else		
-		seq_printf(p, "%10u ", kstat_irqs(i));
-#endif /* CONFIG_SMP */
-		if (irq_desc[i].handler)		
-			seq_printf(p, " %s ", irq_desc[i].handler->typename );
+		if (get_irq_desc(i)->handler)		
+			seq_printf(p, " %s ", get_irq_desc(i)->handler->typename );
 		else
 			seq_printf(p, "  None      ");
-		seq_printf(p, "%s", (irq_desc[i].status & IRQ_LEVEL) ? "Level " : "Edge  ");
+		seq_printf(p, "%s", (get_irq_desc(i)->status & IRQ_LEVEL) ? "Level " : "Edge  ");
 		seq_printf(p, "    %s",action->name);
 		for (action=action->next; action; action = action->next)
 			seq_printf(p, ", %s", action->name);
 		seq_putc(p, '\n');
 skip:
-		spin_unlock_irqrestore(&irq_desc[i].lock, flags);
+		spin_unlock_irqrestore(&desc->lock, flags);
 	}
 	seq_printf(p, "BAD: %10u\n", ppc_spurious_interrupts);
 	return 0;
@@ -474,9 +749,17 @@ void ppc_irq_dispatch_handler(struct pt_
 	int status;
 	struct irqaction *action;
 	int cpu = smp_processor_id();
-	irq_desc_t *desc = irq_desc + irq;
+	irq_desc_t *desc = get_irq_desc(irq);
+	struct hw_irq_stat *hwstat;
+	unsigned long *per_cpus;
+
+	/* Statistics. */
+	hwstat = get_irq_stat(desc);	/* same cache line as desc */
+	hwstat->irqs++;
+	per_cpus = get_irq_per_cpu(hwstat); /* same cache line for < 8 cpus */
+	if (per_cpus)
+		per_cpus[cpu]++;
 
-	kstat_cpu(cpu).irqs[irq]++;
 	spin_lock(&desc->lock);
 	ack_irq(irq);	
 	/*
@@ -550,26 +833,23 @@ out:
 	 * The ->end() handler has to deal with interrupts which got
 	 * disabled while the handler was running.
 	 */
-	if (irq_desc[irq].handler) {
-		if (irq_desc[irq].handler->end)
-			irq_desc[irq].handler->end(irq);
-		else if (irq_desc[irq].handler->enable)
-			irq_desc[irq].handler->enable(irq);
+	if (desc->handler) {
+		if (desc->handler->end)
+			desc->handler->end(irq);
+		else if (desc->handler->enable)
+			desc->handler->enable(irq);
 	}
 	spin_unlock(&desc->lock);
 }
 
+#ifdef CONFIG_PPC_ISERIES
 int do_IRQ(struct pt_regs *regs)
 {
-	int irq, first = 1;
-#ifdef CONFIG_PPC_ISERIES
 	struct paca_struct *lpaca;
 	struct ItLpQueue *lpq;
-#endif
 
 	irq_enter();
 
-#ifdef CONFIG_PPC_ISERIES
 	lpaca = get_paca();
 #ifdef CONFIG_SMP
 	if (lpaca->xLpPaca.xIntDword.xFields.xIpiCnt) {
@@ -580,7 +860,26 @@ int do_IRQ(struct pt_regs *regs)
 	lpq = lpaca->lpQueuePtr;
 	if (lpq && ItLpQueue_isLpIntPending(lpq))
 		lpEvent_count += ItLpQueue_process(lpq, regs);
-#else
+
+	irq_exit();
+
+	if (lpaca->xLpPaca.xIntDword.xFields.xDecrInt) {
+		lpaca->xLpPaca.xIntDword.xFields.xDecrInt = 0;
+		/* Signal a fake decrementer interrupt */
+		timer_interrupt(regs);
+	}
+
+	return 1; /* lets ret_from_int know we can do checks */
+}
+
+#else	/* CONFIG_PPC_ISERIES */
+
+int do_IRQ(struct pt_regs *regs)
+{
+	int irq, first = 1;
+
+	irq_enter();
+
 	/*
 	 * Every arch is required to implement ppc_md.get_irq.
 	 * This function will either return an irq number or -1 to
@@ -596,20 +895,12 @@ int do_IRQ(struct pt_regs *regs)
 	if (irq != -2 && first)
 		/* That's not SMP safe ... but who cares ? */
 		ppc_spurious_interrupts++;
-#endif
 
 	irq_exit();
 
-#ifdef CONFIG_PPC_ISERIES
-	if (lpaca->xLpPaca.xIntDword.xFields.xDecrInt) {
-		lpaca->xLpPaca.xIntDword.xFields.xDecrInt = 0;
-		/* Signal a fake decrementer interrupt */
-		timer_interrupt(regs);
-	}
-#endif
-
 	return 1; /* lets ret_from_int know we can do checks */
 }
+#endif	/* CONFIG_PPC_ISERIES */
 
 unsigned long probe_irq_on (void)
 {
@@ -634,43 +925,43 @@ void __init init_IRQ(void)
 {
 	static int once = 0;
 
-	if ( once )
+	if (once)
 		return;
-	else
-		once++;
-	
+
+	once++;
+
+	/* Initialize the irq tree */
+	irq_desc_init();
+
 	ppc_md.init_IRQ();
 }
 
 static struct proc_dir_entry * root_irq_dir;
-static struct proc_dir_entry * irq_dir [NR_IRQS];
-static struct proc_dir_entry * smp_affinity_entry [NR_IRQS];
 
-#ifdef CONFIG_IRQ_ALL_CPUS
-cpumask_t irq_affinity [NR_IRQS] = { [0 ... NR_IRQS-1] = CPU_MASK_ALL };
-#else  /* CONFIG_IRQ_ALL_CPUS */
-cpumask_t irq_affinity [NR_IRQS] = { [0 ... NR_IRQS-1] = CPU_MASK_NONE };
-#endif /* CONFIG_IRQ_ALL_CPUS */
+/* XXX Fix this when we clean up large irq support */
+cpumask_t get_irq_affinity(unsigned int irq)
+{
+	irq_desc_t *desc = get_irq_desc(irq);
+	struct hw_irq_stat *hwstat = get_irq_stat(desc);
+
+	return hwstat->irq_affinity;
+}
 
 #define HEX_DIGITS (2*sizeof(cpumask_t))
 
 static int irq_affinity_read_proc (char *page, char **start, off_t off,
 			int count, int *eof, void *data)
 {
-	int k, len;
-	cpumask_t tmp = irq_affinity[(long)data];
+	int len;
+	unsigned int irq = (long)data;
 
 	if (count < HEX_DIGITS+1)
 		return -EINVAL;
 
-	for (k = 0; k < sizeof(cpumask_t) / sizeof(u16); ++k) {
-		int j = sprintf(page, "%04hx", (u16)cpus_coerce(tmp));
-		len += j;
-		page += j;
-		cpus_shift_right(tmp, tmp, 16);
-	}
-	len += sprintf(page, "\n");
-	return len;
+	len = format_cpumask(page, get_irq_affinity(irq));
+	page += len;
+ 	len += sprintf(page, "\n");
+ 	return len;
 }
 
 static unsigned int parse_hex_value (const char *buffer,
@@ -693,7 +984,7 @@ static unsigned int parse_hex_value (con
 	 */
 
 	for (i = 0; i < count; i++) {
-		unsigned int c = hexnum[i];
+		unsigned long c = hexnum[i];
 		int k;
 
 		switch (c) {
@@ -705,7 +996,7 @@ static unsigned int parse_hex_value (con
 		}
 		cpus_shift_left(value, value, 4);
 		for (k = 0; k < 4; ++k)
-			if (test_bit(k, (unsigned long *)&c))
+			if (test_bit(k, &c))
 				cpu_set(k, value);
 	}
 out:
@@ -716,16 +1007,28 @@ out:
 static int irq_affinity_write_proc (struct file *file, const char *buffer,
 					unsigned long count, void *data)
 {
-	int irq = (long)data, full_count = count, err;
+	unsigned int irq = (long)data;
+	irq_desc_t *desc = get_irq_desc(irq);
+	struct hw_irq_stat *hwstat = get_irq_stat(desc);
+	int full_count = count, err;
 	cpumask_t new_value, tmp;
+	cpumask_t allcpus = CPU_MASK_ALL;
 
-	if (!irq_desc[irq].handler->set_affinity)
+	if (!desc->handler->set_affinity)
 		return -EIO;
 
 	err = parse_hex_value(buffer, count, &new_value);
 	if (err)
 		return err;
 
+	/* 
+	 * We check for CPU_MASK_ALL in xics to send irqs to all cpus.
+	 * In some cases CPU_MASK_ALL is smaller than the cpumask (eg
+	 * NR_CPUS == 32 and cpumask is a long), so we mask it here to
+	 * be consistent.
+	 */
+	cpus_and(new_value, new_value, allcpus);
+
 	/*
 	 * Do not allow disabling IRQs completely - it's a too easy
 	 * way to make the system unusable accidentally :-) At least
@@ -735,19 +1038,24 @@ static int irq_affinity_write_proc (stru
 	if (cpus_empty(tmp))
 		return -EINVAL;
 
-	irq_affinity[irq] = new_value;
-	irq_desc[irq].handler->set_affinity(irq, new_value);
-
+	hwstat->irq_affinity = new_value;
+	desc->handler->set_affinity(irq, new_value);
 	return full_count;
 }
 
 static int prof_cpu_mask_read_proc (char *page, char **start, off_t off,
 			int count, int *eof, void *data)
 {
-	unsigned long *mask = (unsigned long *) data;
+	int len;
+	cpumask_t *mask = (cpumask_t *) data;
+
 	if (count < HEX_DIGITS+1)
 		return -EINVAL;
-	return sprintf (page, "%08lx\n", *mask);
+
+	len = format_cpumask(page, *mask);
+	page += len;
+	len += sprintf (page, "\n");
+	return len;
 }
 
 static int prof_cpu_mask_write_proc (struct file *file, const char __user *buffer,
@@ -785,18 +1093,24 @@ static void register_irq_proc (unsigned 
 {
 	struct proc_dir_entry *entry;
 	char name [MAX_NAMELEN];
+	irq_desc_t *desc;
+	struct hw_irq_stat *hwstat;
 
-	if (!root_irq_dir || (irq_desc[irq].handler == NULL) || irq_dir[irq])
+	desc = get_real_irq_desc(irq);
+	if (!root_irq_dir || !desc || !desc->handler)
+		return;
+	hwstat = get_irq_stat(desc);
+	if (hwstat->irq_dir)
 		return;
 
 	memset(name, 0, MAX_NAMELEN);
 	sprintf(name, "%d", irq);
 
 	/* create /proc/irq/1234 */
-	irq_dir[irq] = proc_mkdir(name, root_irq_dir);
+	hwstat->irq_dir = proc_mkdir(name, root_irq_dir);	
 
 	/* create /proc/irq/1234/smp_affinity */
-	entry = create_proc_entry("smp_affinity", 0600, irq_dir[irq]);
+	entry = create_proc_entry("smp_affinity", 0600, hwstat->irq_dir);
 
 	if (entry) {
 		entry->nlink = 1;
@@ -804,8 +1118,7 @@ static void register_irq_proc (unsigned 
 		entry->read_proc = irq_affinity_read_proc;
 		entry->write_proc = irq_affinity_write_proc;
 	}
-
-	smp_affinity_entry[irq] = entry;
+	hwstat->smp_affinity = entry;
 }
 
 unsigned long prof_cpu_mask = -1;
@@ -832,8 +1145,8 @@ void init_irq_proc (void)
 	/*
 	 * Create entries for all existing IRQs.
 	 */
-	for (i = 0; i < NR_IRQS; i++) {
-		if (irq_desc[i].handler == NULL)
+	for_each_irq(i) {
+		if (get_irq_desc(i)->handler == NULL)
 			continue;
 		register_irq_proc(i);
 	}
diff -purN linux-2.5/arch/ppc64/kernel/lparcfg.c linuxppc64-2.5/arch/ppc64/kernel/lparcfg.c
--- linux-2.5/arch/ppc64/kernel/lparcfg.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/lparcfg.c	2003-12-11 21:53:44.000000000 +0000
@@ -0,0 +1,478 @@
+/*
+ * PowerPC64 LPAR Configuration Information Driver
+ *
+ * Dave Engebretsen engebret@us.ibm.com
+ *    Copyright (c) 2003 Dave Engebretsen
+ * Will Schmidt willschm@us.ibm.com
+ *    SPLPAR updates, Copyright (c) 2003 Will Schmidt IBM Corporation.
+ *
+ *      This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ *
+ * This driver creates a proc file at /proc/ppc64/lparcfg which contains
+ * keyword - value pairs that specify the configuration of the partition.
+ */
+
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <linux/proc_fs.h>
+#include <linux/init.h>
+#include <asm/uaccess.h>
+#include <asm/iSeries/HvLpConfig.h>
+#include <asm/iSeries/ItLpPaca.h>
+#include <asm/hvcall.h>
+#include <asm/cputable.h>
+
+#define MODULE_VERSION "1.0"
+#define MODULE_NAME "lparcfg"
+
+static struct proc_dir_entry *proc_ppc64_lparcfg;
+#define LPARCFG_BUFF_SIZE 4096
+
+#ifdef CONFIG_PPC_ISERIES
+static unsigned char e2a(unsigned char x)
+{
+        switch (x) {
+        case 0xF0:
+                return '0';
+        case 0xF1:
+                return '1';
+        case 0xF2:
+                return '2';
+        case 0xF3:
+                return '3';
+        case 0xF4:
+                return '4';
+        case 0xF5:
+                return '5';
+        case 0xF6:
+                return '6';
+        case 0xF7:
+                return '7';
+        case 0xF8:
+                return '8';
+        case 0xF9:
+                return '9';
+        case 0xC1:
+                return 'A';
+        case 0xC2:
+                return 'B';
+        case 0xC3:
+                return 'C';
+        case 0xC4:
+                return 'D';
+        case 0xC5:
+                return 'E';
+        case 0xC6:
+                return 'F';
+        case 0xC7:
+                return 'G';
+        case 0xC8:
+                return 'H';
+        case 0xC9:
+                return 'I';
+        case 0xD1:
+                return 'J';
+        case 0xD2:
+                return 'K';
+        case 0xD3:
+                return 'L';
+        case 0xD4:
+                return 'M';
+        case 0xD5:
+                return 'N';
+        case 0xD6:
+                return 'O';
+        case 0xD7:
+                return 'P';
+        case 0xD8:
+                return 'Q';
+        case 0xD9:
+                return 'R';
+        case 0xE2:
+                return 'S';
+        case 0xE3:
+                return 'T';
+        case 0xE4:
+                return 'U';
+        case 0xE5:
+                return 'V';
+        case 0xE6:
+                return 'W';
+        case 0xE7:
+                return 'X';
+        case 0xE8:
+                return 'Y';
+        case 0xE9:
+                return 'Z';
+        }
+        return ' ';
+}
+
+/* 
+ * Methods used to fetch LPAR data when running on an iSeries platform.
+ */
+static int lparcfg_data(unsigned char *buf, unsigned long size)
+{
+	unsigned long n = 0, pool_id, lp_index; 
+	int shared, entitled_capacity, max_entitled_capacity;
+	int processors, max_processors;
+	struct paca_struct *lpaca = get_paca();
+
+	if((buf == NULL) || (size > LPARCFG_BUFF_SIZE)) {
+		return -EFAULT;
+	}
+	memset(buf, 0, size); 
+
+	shared = (int)(lpaca->xLpPacaPtr->xSharedProc);
+	n += snprintf(buf, LPARCFG_BUFF_SIZE - n,
+		      "serial_number=%c%c%c%c%c%c%c\n", 
+		      e2a(xItExtVpdPanel.mfgID[2]),
+		      e2a(xItExtVpdPanel.mfgID[3]),
+		      e2a(xItExtVpdPanel.systemSerial[1]),
+		      e2a(xItExtVpdPanel.systemSerial[2]),
+		      e2a(xItExtVpdPanel.systemSerial[3]),
+		      e2a(xItExtVpdPanel.systemSerial[4]),
+		      e2a(xItExtVpdPanel.systemSerial[5])); 
+
+	n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+		      "system_type=%c%c%c%c\n",
+		      e2a(xItExtVpdPanel.machineType[0]),
+		      e2a(xItExtVpdPanel.machineType[1]),
+		      e2a(xItExtVpdPanel.machineType[2]),
+		      e2a(xItExtVpdPanel.machineType[3])); 
+
+	lp_index = HvLpConfig_getLpIndex(); 
+	n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+		      "partition_id=%d\n", (int)lp_index); 
+
+	n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+		      "system_active_processors=%d\n", 
+		      (int)HvLpConfig_getSystemPhysicalProcessors()); 
+
+	n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+		      "system_potential_processors=%d\n", 
+		      (int)HvLpConfig_getSystemPhysicalProcessors()); 
+
+	processors = (int)HvLpConfig_getPhysicalProcessors(); 
+	n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+		      "partition_active_processors=%d\n", processors);  
+
+	max_processors = (int)HvLpConfig_getMaxPhysicalProcessors(); 
+	n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+		      "partition_potential_processors=%d\n", max_processors);  
+
+	if(shared) {
+		entitled_capacity = HvLpConfig_getSharedProcUnits(); 
+		max_entitled_capacity = HvLpConfig_getMaxSharedProcUnits(); 
+	} else {
+		entitled_capacity = processors * 100; 
+		max_entitled_capacity = max_processors * 100; 
+	}
+	n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+		      "partition_entitled_capacity=%d\n", entitled_capacity);
+
+	n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+		      "partition_max_entitled_capacity=%d\n", 
+		      max_entitled_capacity);
+
+	if(shared) {
+		pool_id = HvLpConfig_getSharedPoolIndex(); 
+		n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, "pool=%d\n", 
+			      (int)pool_id); 
+		n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "pool_capacity=%d\n", (int)(HvLpConfig_getNumProcsInSharedPool(pool_id)*100)); 
+	}
+
+	n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+		      "shared_processor_mode=%d\n", shared);
+
+	return 0;
+}
+#endif /* CONFIG_PPC_ISERIES */
+
+#ifdef CONFIG_PPC_PSERIES
+/* 
+ * Methods used to fetch LPAR data when running on a pSeries platform.
+ */
+
+/*
+ * H_GET_PPP hcall returns info in 4 parms.
+ *  entitled_capacity,unallocated_capacity,
+ *  aggregation, resource_capability).
+ *
+ *  R4 = Entitled Processor Capacity Percentage. 
+ *  R5 = Unallocated Processor Capacity Percentage.
+ *  R6 (AABBCCDDEEFFGGHH).
+ *      XXXX - reserved (0)
+ *          XXXX - reserved (0)
+ *              XXXX - Group Number
+ *                  XXXX - Pool Number.
+ *  R7 (PPOONNMMLLKKJJII)
+ *      XX - reserved. (0)
+ *        XX - bit 0-6 reserved (0).   bit 7 is Capped indicator.
+ *          XX - variable processor Capacity Weight
+ *            XX - Unallocated Variable Processor Capacity Weight.
+ *              XXXX - Active processors in Physical Processor Pool.
+ *                  XXXX  - Processors active on platform. 
+ */
+unsigned int h_get_ppp(unsigned long *entitled,unsigned long  *unallocated,unsigned long *aggregation,unsigned long *resource)
+{
+	unsigned long rc;
+	rc = plpar_hcall_4out(H_GET_PPP,0,0,0,0,entitled,unallocated,aggregation,resource);
+	return 0;
+}
+
+/*
+ * get_splpar_potential_characteristics().
+ * Retrieve the potential_processors and max_entitled_capacity values
+ * through the get-system-parameter rtas call.
+ */
+#define SPLPAR_CHARACTERISTICS_TOKEN 20
+#define SPLPAR_MAXLENGTH 1026*(sizeof(char))
+unsigned int get_splpar_potential_characteristics()
+{
+	/* return 0 for now.  Underlying rtas functionality is not yet complete. 12/01/2003*/
+	return 0; 
+#if 0 
+	long call_status;
+	unsigned long ret[2];
+
+	char * buffer = kmalloc(SPLPAR_MAXLENGTH, GFP_KERNEL);
+
+	printk("token for ibm,get-system-parameter (0x%x)\n",rtas_token("ibm,get-system-parameter"));
+
+	call_status = rtas_call(rtas_token("ibm,get-system-parameter"), 3, 1,
+				NULL,
+				SPLPAR_CHARACTERISTICS_TOKEN,
+				&buffer,
+				SPLPAR_MAXLENGTH,
+				(void *)&ret);
+
+	if (call_status!=0) {
+		printk("Error calling get-system-parameter (0x%lx)\n",call_status);
+		kfree(buffer);
+		return -1;
+	} else {
+		printk("get-system-parameter (%s)\n",buffer);
+		kfree(buffer);
+		/* TODO: Add code here to parse out value for system_potential_processors and partition_max_entitled_capacity */
+		return 1;
+	}
+#endif
+}
+
+static int lparcfg_data(unsigned char *buf, unsigned long size)
+{
+	unsigned long n = 0;
+	int shared, max_entitled_capacity;
+	int processors, system_active_processors, system_potential_processors;
+	struct device_node *root;
+	const char *model = "";
+	const char *system_id = "";
+	unsigned int *lp_index_ptr, lp_index = 0;
+	struct device_node *rtas_node;
+	int *ip;
+	unsigned long h_entitled,h_unallocated,h_aggregation,h_resource;
+
+	if((buf == NULL) || (size > LPARCFG_BUFF_SIZE)) {
+		return -EFAULT;
+	}
+	memset(buf, 0, size); 
+
+	root = find_path_device("/");
+	if (root) {
+		model = get_property(root, "model", NULL);
+		system_id = get_property(root, "system-id", NULL);
+		lp_index_ptr = (unsigned int *)get_property(root, "ibm,partition-no", NULL);
+		if(lp_index_ptr) lp_index = *lp_index_ptr;
+	}
+
+	n  = snprintf(buf, LPARCFG_BUFF_SIZE - n,
+		      "serial_number=%s\n", system_id); 
+
+	n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+		      "system_type=%s\n", model); 
+
+	n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+		      "partition_id=%d\n", (int)lp_index); 
+
+	rtas_node = find_path_device("/rtas");
+	ip = (int *)get_property(rtas_node, "ibm,lrdr-capacity", NULL);
+	if (ip == NULL) {
+		system_active_processors = systemcfg->processorCount; 
+	} else {
+		system_active_processors = *(ip + 4);
+	}
+
+	if (cur_cpu_spec->firmware_features & FW_FEATURE_SPLPAR) {
+		h_get_ppp(&h_entitled,&h_unallocated,&h_aggregation,&h_resource);
+#ifdef DEBUG
+		n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "R4=0x%lx\n", h_entitled);
+		n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "R5=0x%lx\n", h_unallocated);
+		n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "R6=0x%lx\n", h_aggregation);
+		n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "R7=0x%lx\n", h_resource);
+#endif /* DEBUG */
+	}
+
+	if (cur_cpu_spec->firmware_features & FW_FEATURE_SPLPAR) {
+		system_potential_processors =  get_splpar_potential_characteristics();
+		n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "system_active_processors=%d\n", 
+			      (h_resource >> 2*8) && 0xffff);
+		n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "system_potential_processors=%d\n", 
+			      system_potential_processors);
+	} else {
+		system_potential_processors = system_active_processors;
+		n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "system_active_processors=%d\n", 
+			      system_active_processors);
+		n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "system_potential_processors=%d\n", 
+			      system_potential_processors);
+	}
+
+	processors = systemcfg->processorCount;
+	n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+		      "partition_active_processors=%d\n", processors);  
+	n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+		      "partition_potential_processors=%d\n",
+		      system_active_processors);
+
+	/* max_entitled_capacity will come out of get_splpar_potential_characteristics() when that function is complete */
+	max_entitled_capacity = system_active_processors * 100; 
+	if (cur_cpu_spec->firmware_features & FW_FEATURE_SPLPAR) {
+		n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "partition_entitled_capacity=%ld\n", h_entitled);
+	} else {
+		n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "partition_entitled_capacity=%d\n", system_active_processors*100);
+	}
+
+	n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+		      "partition_max_entitled_capacity=%d\n", 
+		      max_entitled_capacity);
+
+	shared = 0;
+	n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+		      "shared_processor_mode=%d\n", shared);
+
+	if (cur_cpu_spec->firmware_features & FW_FEATURE_SPLPAR) {
+		n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "pool=%d\n", (h_aggregation >> 0*8)&&0xffff);
+
+		n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "pool_capacity=%d\n", (h_resource >> 3*8) &&0xffff);
+
+		n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "group=%d\n", (h_aggregation >> 2*8)&&0xffff);
+
+		n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "capped=%d\n", (h_resource >> 6*8)&&0x40);
+
+		n += snprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "capacity_weight=%d\n", (int)(h_resource>>5*8)&0xFF);
+	}
+	return 0;
+}
+#endif /* CONFIG_PPC_PSERIES */
+
+
+static ssize_t lparcfg_read(struct file *file, char *buf,
+			    size_t count, loff_t *ppos)
+{
+	struct proc_dir_entry *dp = PDE(file->f_dentry->d_inode);
+	unsigned long *data = (unsigned long *)dp->data;
+	unsigned long p;
+	ssize_t read;
+	char * pnt;
+
+	if (!data) {
+		printk(KERN_ERR "lparcfg: read failed no data\n");
+		return -EIO;
+	}
+
+	if(ppos) {
+		p = *ppos;
+	} else {
+		return -EFAULT;
+	}
+
+	if (p >= LPARCFG_BUFF_SIZE) return 0;
+
+	lparcfg_data((unsigned char *)data, LPARCFG_BUFF_SIZE); 
+	if (count > (strlen((char *)data) - p))
+		count = (strlen((char *)data)) - p;
+	read = 0;
+
+	pnt = (char *)(data) + p;
+	copy_to_user(buf, (void *)pnt, count);
+	read += count;
+	*ppos += read;
+	return read;
+}
+
+static int lparcfg_open(struct inode * inode, struct file * file)
+{
+	struct proc_dir_entry *dp = PDE(file->f_dentry->d_inode);
+	unsigned int *data = (unsigned int *)dp->data;
+
+	if (!data) {
+		printk(KERN_ERR "lparcfg: open failed no data\n");
+		return -EIO;
+	}
+
+	return 0;
+}
+
+struct file_operations lparcfg_fops = {
+	owner:		THIS_MODULE,
+	read:		lparcfg_read,
+	open:		lparcfg_open,
+};
+
+int __init lparcfg_init(void)
+{
+	struct proc_dir_entry *ent;
+
+	ent = create_proc_entry("ppc64/lparcfg", S_IRUSR, NULL);
+	if (ent) {
+		ent->proc_fops = &lparcfg_fops;
+		ent->data = kmalloc(LPARCFG_BUFF_SIZE, GFP_KERNEL);
+		if (!ent->data) {
+			printk(KERN_ERR "Failed to allocate buffer for lparcfg\n");
+			remove_proc_entry("lparcfg", ent->parent);
+			return -ENOMEM;
+		}
+	} else {
+		printk(KERN_ERR "Failed to create ppc64/lparcfg\n");
+		return -EIO;
+	}
+
+	proc_ppc64_lparcfg = ent;
+	return 0;
+}
+
+void __exit lparcfg_cleanup(void)
+{
+	if (proc_ppc64_lparcfg) {
+		if (proc_ppc64_lparcfg->data) {
+		    kfree(proc_ppc64_lparcfg->data);
+		}
+		remove_proc_entry("lparcfg", proc_ppc64_lparcfg->parent);
+	}
+}
+
+module_init(lparcfg_init);
+module_exit(lparcfg_cleanup);
+MODULE_DESCRIPTION("Interface for LPAR configuration data");
+MODULE_AUTHOR("Dave Engebretsen");
+MODULE_LICENSE("GPL");
diff -purN linux-2.5/arch/ppc64/kernel/mf.c linuxppc64-2.5/arch/ppc64/kernel/mf.c
--- linux-2.5/arch/ppc64/kernel/mf.c	2002-12-30 12:29:15.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/mf.c	2003-12-17 05:50:27.000000000 +0000
@@ -36,166 +36,125 @@
 #include <asm/nvram.h>
 #include <asm/time.h>
 #include <asm/iSeries/ItSpCommArea.h>
-#include <asm/iSeries/mf_proc.h>
 #include <asm/iSeries/iSeries_proc.h>
 #include <asm/uaccess.h>
 #include <linux/pci.h>
 #include <linux/bcd.h>
 
-extern struct pci_dev * iSeries_vio_dev;
+extern struct pci_dev *iSeries_vio_dev;
 
 /*
  * This is the structure layout for the Machine Facilites LPAR event
  * flows.
  */
-struct VspCmdData;
-struct CeMsgData;
-union SafeCast
-{
-	u64 ptrAsU64;
+union safe_cast {
+	u64 ptr_as_u64;
 	void *ptr;
 };
 
+struct VspCmdData {
+	union safe_cast token;
+	u16 cmd;
+	HvLpIndex lp_index;
+	u8 result_code;
+	u32 reserved;
+	union {
+		u64 state;	/* GetStateOut */
+		u64 ipl_type;	/* GetIplTypeOut, Function02SelectIplTypeIn */
+		u64 ipl_mode;	/* GetIplModeOut, Function02SelectIplModeIn */
+		u64 page[4];	/* GetSrcHistoryIn */
+		u64 flag;	/* GetAutoIplWhenPrimaryIplsOut,
+				   SetAutoIplWhenPrimaryIplsIn,
+				   WhiteButtonPowerOffIn,
+				   Function08FastPowerOffIn,
+				   IsSpcnRackPowerIncompleteOut */
+		struct {
+			u64 token;
+			u64 address_type;
+			u64 side;
+			u32 length;
+			u32 offset;
+		} kern;		/* SetKernelImageIn, GetKernelImageIn,
+				   SetKernelCmdLineIn, GetKernelCmdLineIn */
+		u32 length_out;	/* GetKernelImageOut, GetKernelCmdLineOut */
+		u8 reserved[80];
+	} sub_data;
+};
 
-typedef void (*CeMsgCompleteHandler)( void *token, struct CeMsgData *vspCmdRsp );
-
-struct CeMsgCompleteData
-{
-	CeMsgCompleteHandler xHdlr;
-	void *xToken;
+struct VspRspData {
+	struct semaphore *sem;
+	struct VspCmdData *response;
 };
 
-struct VspRspData
-{
-	struct semaphore *xSemaphore;
-	struct VspCmdData *xResponse;
+struct AllocData {
+	u16 size;
+	u16 type;
+	u32 count;
+	u16 reserved1;
+	u8 reserved2;
+	HvLpIndex target_lp;
 };
 
-struct IoMFLpEvent
-{
-	struct HvLpEvent xHvLpEvent;
+struct CeMsgData;
 
-	u16 xSubtypeRc;
-	u16 xRsvd1;
-	u32 xRsvd2;
-
-	union
-	{
-
-		struct AllocData
-		{
-			u16 xSize;
-			u16 xType;
-			u32 xCount;
-			u16 xRsvd3;
-			u8 xRsvd4;
-			HvLpIndex xTargetLp;
-		} xAllocData;
-
-		struct CeMsgData
-		{
-			u8 xCEMsg[12];
-			char xReserved[4];
-			struct CeMsgCompleteData *xToken;
-		} xCEMsgData;
-
-		struct VspCmdData
-		{
-			union SafeCast xTokenUnion;
-			u16 xCmd;
-			HvLpIndex xLpIndex;
-			u8 xRc;
-			u32 xReserved1;
+typedef void (*CeMsgCompleteHandler)(void *token, struct CeMsgData *vspCmdRsp);
 
-			union VspCmdSubData
-			{
-				struct
-				{
-					u64 xState;
-				} xGetStateOut;
-
-				struct
-				{
-					u64 xIplType;
-				} xGetIplTypeOut, xFunction02SelectIplTypeIn;
-
-				struct
-				{
-					u64 xIplMode;
-				} xGetIplModeOut, xFunction02SelectIplModeIn;
-
-				struct
-				{
-					u64 xPage[4];
-				} xGetSrcHistoryIn;
-
-				struct
-				{
-					u64 xFlag;
-				} xGetAutoIplWhenPrimaryIplsOut,
-					xSetAutoIplWhenPrimaryIplsIn,
-					xWhiteButtonPowerOffIn,
-					xFunction08FastPowerOffIn,
-					xIsSpcnRackPowerIncompleteOut;
-
-				struct
-				{
-					u64 xToken;
-					u64 xAddressType;
-					u64 xSide;
-					u32 xTransferLength;
-					u32 xOffset;
-				} xSetKernelImageIn,
-					xGetKernelImageIn,
-					xSetKernelCmdLineIn,
-					xGetKernelCmdLineIn;
-
-				struct
-				{
-					u32 xTransferLength;
-				} xGetKernelImageOut,xGetKernelCmdLineOut;
-
-
-				u8 xReserved2[80];
-
-			} xSubData;
-		} xVspCmd;
-	} xUnion;
+struct CeMsgCompleteData {
+	CeMsgCompleteHandler handler;
+	void *token;
 };
 
+struct CeMsgData {
+	u8 ce_msg[12];
+	char reserved[4];
+	struct CeMsgCompleteData *completion;
+};
+
+struct IoMFLpEvent {
+	struct HvLpEvent hp_lp_event;
+	u16 subtype_result_code;
+	u16 reserved1;
+	u32 reserved2;
+	union {
+		struct AllocData alloc;
+		struct CeMsgData ce_msg;
+		struct VspCmdData vsp_cmd;
+	} data;
+};
+
+#define subtype_data(a, b, c, d)	\
+		(((a) << 24) + ((b) << 16) + ((c) << 8) + (d))
 
 /*
  * All outgoing event traffic is kept on a FIFO queue.  The first
  * pointer points to the one that is outstanding, and all new
  * requests get stuck on the end.  Also, we keep a certain number of
- * preallocated stack elements so that we can operate very early in
+ * preallocated pending events so that we can operate very early in
  * the boot up sequence (before kmalloc is ready).
  */
-struct StackElement
-{
-	struct StackElement * next;
+struct pending_event {
+	struct pending_event *next;
 	struct IoMFLpEvent event;
 	MFCompleteHandler hdlr;
-	char dmaData[72];
-	unsigned dmaDataLength;
-	unsigned remoteAddress;
+	char dma_data[72];
+	unsigned dma_data_length;
+	unsigned remote_address;
 };
-static spinlock_t spinlock;
-static struct StackElement * head = NULL;
-static struct StackElement * tail = NULL;
-static struct StackElement * avail = NULL;
-static struct StackElement prealloc[16];
+static spinlock_t pending_event_spinlock;
+static struct pending_event *pending_event_head;
+static struct pending_event *pending_event_tail;
+static struct pending_event *pending_event_avail;
+static struct pending_event pending_event_prealloc[16];
 
 /*
- * Put a stack element onto the available queue, so it can get reused.
- * Attention! You must have the spinlock before calling!
+ * Put a pending event onto the available queue, so it can get reused.
+ * Attention! You must have the pending_event_spinlock before calling!
  */
-void free( struct StackElement * element )
+static void free_pending_event(struct pending_event *ev)
 {
-	if ( element != NULL )
-	{
-		element->next = avail;
-		avail = element;
+	if (ev != NULL) {
+		ev->next = pending_event_avail;
+		pending_event_avail = ev;
 	}
 }
 
@@ -203,68 +162,68 @@ void free( struct StackElement * element
  * Enqueue the outbound event onto the stack.  If the queue was
  * empty to begin with, we must also issue it via the Hypervisor
  * interface.  There is a section of code below that will touch
- * the first stack pointer without the protection of the spinlock.
+ * the first stack pointer without the protection of the pending_event_spinlock.
  * This is OK, because we know that nobody else will be modifying
  * the first pointer when we do this.
  */
-static int signalEvent( struct StackElement * newElement )
+static int signal_event(struct pending_event *ev)
 {
 	int rc = 0;
 	unsigned long flags;
 	int go = 1;
-	struct StackElement * element;
+	struct pending_event *ev1;
 	HvLpEvent_Rc hvRc;
 
 	/* enqueue the event */
-	if ( newElement != NULL )
-	{
-		spin_lock_irqsave( &spinlock, flags );
-		if ( head == NULL )
-			head = newElement;
+	if (ev != NULL) {
+		ev->next = NULL;
+		spin_lock_irqsave(&pending_event_spinlock, flags);
+		if (pending_event_head == NULL)
+			pending_event_head = ev;
 		else {
 			go = 0;
-			tail->next = newElement;
+			pending_event_tail->next = ev;
 		}
-		newElement->next = NULL;
-		tail = newElement;
-		spin_unlock_irqrestore( &spinlock, flags );
+		pending_event_tail = ev;
+		spin_unlock_irqrestore(&pending_event_spinlock, flags);
 	}
 
 	/* send the event */
-	while ( go )
-	{
+	while (go) {
 		go = 0;
 
 		/* any DMA data to send beforehand? */
-		if ( head->dmaDataLength > 0 )
-			HvCallEvent_dmaToSp( head->dmaData, head->remoteAddress, head->dmaDataLength, HvLpDma_Direction_LocalToRemote );
-
-		hvRc = HvCallEvent_signalLpEvent(&head->event.xHvLpEvent);
-		if ( hvRc != HvLpEvent_Rc_Good )
-		{
-			printk( KERN_ERR "mf.c: HvCallEvent_signalLpEvent() failed with %d\n", (int)hvRc );
-
-			spin_lock_irqsave( &spinlock, flags );
-			element = head;
-			head = head->next;
-			if ( head != NULL )
+		if (pending_event_head->dma_data_length > 0)
+			HvCallEvent_dmaToSp(pending_event_head->dma_data,
+					pending_event_head->remote_address,
+					pending_event_head->dma_data_length,
+					HvLpDma_Direction_LocalToRemote);
+
+		hvRc = HvCallEvent_signalLpEvent(
+				&pending_event_head->event.hp_lp_event);
+		if (hvRc != HvLpEvent_Rc_Good) {
+			printk(KERN_ERR "mf.c: HvCallEvent_signalLpEvent() failed with %d\n",
+					(int)hvRc);
+
+			spin_lock_irqsave(&pending_event_spinlock, flags);
+			ev1 = pending_event_head;
+			pending_event_head = pending_event_head->next;
+			if (pending_event_head != NULL)
 				go = 1;
-			spin_unlock_irqrestore( &spinlock, flags );
+			spin_unlock_irqrestore(&pending_event_spinlock, flags);
 
-			if ( element == newElement )
+			if (ev1 == ev)
 				rc = -EIO;
-			else {
-				if ( element->hdlr != NULL )
-				{
-					union SafeCast mySafeCast;
-					mySafeCast.ptrAsU64 = element->event.xHvLpEvent.xCorrelationToken;
-					(*element->hdlr)( mySafeCast.ptr, -EIO );
-				}
+			else if (ev1->hdlr != NULL) {
+				union safe_cast mySafeCast;
+
+				mySafeCast.ptr_as_u64 = ev1->event.hp_lp_event.xCorrelationToken;
+				(*ev1->hdlr)(mySafeCast.ptr, -EIO);
 			}
 
-			spin_lock_irqsave( &spinlock, flags );
-			free( element );
-			spin_unlock_irqrestore( &spinlock, flags );
+			spin_lock_irqsave(&pending_event_spinlock, flags);
+			free_pending_event(ev1);
+			spin_unlock_irqrestore(&pending_event_spinlock, flags);
 		}
 	}
 
@@ -272,80 +231,74 @@ static int signalEvent( struct StackElem
 }
 
 /*
- * Allocate a new StackElement structure, and initialize it.
+ * Allocate a new pending_event structure, and initialize it.
  */
-static struct StackElement * newStackElement( void )
+static struct pending_event *new_pending_event(void)
 {
-	struct StackElement * newElement = NULL;
+	struct pending_event *ev = NULL;
 	HvLpIndex primaryLp = HvLpConfig_getPrimaryLpIndex();
 	unsigned long flags;
+	struct HvLpEvent *hev;
 
-	if ( newElement == NULL )
-	{
-		spin_lock_irqsave( &spinlock, flags );
-		if ( avail != NULL )
-		{
-			newElement = avail;
-			avail = avail->next;
-		}
-		spin_unlock_irqrestore( &spinlock, flags );
-	}
-
-	if ( newElement == NULL )
-		newElement = kmalloc(sizeof(struct StackElement),GFP_ATOMIC);
-
-	if ( newElement == NULL )
-	{
-		printk( KERN_ERR "mf.c: unable to kmalloc %ld bytes\n", sizeof(struct StackElement) );
+	spin_lock_irqsave(&pending_event_spinlock, flags);
+	if (pending_event_avail != NULL) {
+		ev = pending_event_avail;
+		pending_event_avail = pending_event_avail->next;
+	}
+	spin_unlock_irqrestore(&pending_event_spinlock, flags);
+	if (ev == NULL)
+		ev = kmalloc(sizeof(struct pending_event),GFP_ATOMIC);
+	if (ev == NULL) {
+		printk(KERN_ERR "mf.c: unable to kmalloc %ld bytes\n",
+				sizeof(struct pending_event));
 		return NULL;
 	}
+	memset(ev, 0, sizeof(struct pending_event));
+	hev = &ev->event.hp_lp_event;
+	hev->xFlags.xValid = 1;
+	hev->xFlags.xAckType = HvLpEvent_AckType_ImmediateAck;
+	hev->xFlags.xAckInd = HvLpEvent_AckInd_DoAck;
+	hev->xFlags.xFunction = HvLpEvent_Function_Int;
+	hev->xType = HvLpEvent_Type_MachineFac;
+	hev->xSourceLp = HvLpConfig_getLpIndex();
+	hev->xTargetLp = primaryLp;
+	hev->xSizeMinus1 = sizeof(ev->event)-1;
+	hev->xRc = HvLpEvent_Rc_Good;
+	hev->xSourceInstanceId = HvCallEvent_getSourceLpInstanceId(primaryLp,
+			HvLpEvent_Type_MachineFac);
+	hev->xTargetInstanceId = HvCallEvent_getTargetLpInstanceId(primaryLp,
+			HvLpEvent_Type_MachineFac);
 
-	memset( newElement, 0, sizeof(struct StackElement) );
-	newElement->event.xHvLpEvent.xFlags.xValid = 1;
-	newElement->event.xHvLpEvent.xFlags.xAckType = HvLpEvent_AckType_ImmediateAck;
-	newElement->event.xHvLpEvent.xFlags.xAckInd = HvLpEvent_AckInd_DoAck;
-	newElement->event.xHvLpEvent.xFlags.xFunction = HvLpEvent_Function_Int;
-	newElement->event.xHvLpEvent.xType = HvLpEvent_Type_MachineFac;
-	newElement->event.xHvLpEvent.xSourceLp = HvLpConfig_getLpIndex();
-	newElement->event.xHvLpEvent.xTargetLp = primaryLp;
-	newElement->event.xHvLpEvent.xSizeMinus1 = sizeof(newElement->event)-1;
-	newElement->event.xHvLpEvent.xRc = HvLpEvent_Rc_Good;
-	newElement->event.xHvLpEvent.xSourceInstanceId = HvCallEvent_getSourceLpInstanceId(primaryLp,HvLpEvent_Type_MachineFac);
-	newElement->event.xHvLpEvent.xTargetInstanceId = HvCallEvent_getTargetLpInstanceId(primaryLp,HvLpEvent_Type_MachineFac);
-
-	return newElement;
+	return ev;
 }
 
-static int signalVspInstruction( struct VspCmdData *vspCmd )
+static int signal_vsp_instruction(struct VspCmdData *vspCmd)
 {
-	struct StackElement * newElement = newStackElement();
-	int rc = 0;
+	struct pending_event *ev = new_pending_event();
+	int rc;
 	struct VspRspData response;
 	DECLARE_MUTEX_LOCKED(Semaphore);
-	response.xSemaphore = &Semaphore;
-	response.xResponse = vspCmd;
 
-	if ( newElement == NULL )
-		rc = -ENOMEM;
-	else {
-		newElement->event.xHvLpEvent.xSubtype = 6;
-		newElement->event.xHvLpEvent.x.xSubtypeData = ('M'<<24)+('F'<<16)+('V'<<8)+('I'<<0);
-		newElement->event.xUnion.xVspCmd.xTokenUnion.ptr = &response;
-		newElement->event.xUnion.xVspCmd.xCmd = vspCmd->xCmd;
-		newElement->event.xUnion.xVspCmd.xLpIndex = HvLpConfig_getLpIndex();
-		newElement->event.xUnion.xVspCmd.xRc = 0xFF;
-		newElement->event.xUnion.xVspCmd.xReserved1 = 0;
-		memcpy(&(newElement->event.xUnion.xVspCmd.xSubData),&(vspCmd->xSubData), sizeof(vspCmd->xSubData));
-		mb();
+	if (ev == NULL)
+		return -ENOMEM;
 
-		rc = signalEvent(newElement);
-	}
+	response.sem = &Semaphore;
+	response.response = vspCmd;
+	ev->event.hp_lp_event.xSubtype = 6;
+	ev->event.hp_lp_event.x.xSubtypeData =
+		subtype_data('M', 'F',  'V',  'I');
+	ev->event.data.vsp_cmd.token.ptr = &response;
+	ev->event.data.vsp_cmd.cmd = vspCmd->cmd;
+	ev->event.data.vsp_cmd.lp_index = HvLpConfig_getLpIndex();
+	ev->event.data.vsp_cmd.result_code = 0xFF;
+	ev->event.data.vsp_cmd.reserved = 0;
+	memcpy(&(ev->event.data.vsp_cmd.sub_data),
+			&(vspCmd->sub_data), sizeof(vspCmd->sub_data));
+	mb();
 
+	rc = signal_event(ev);
 	if (rc == 0)
-	{
 		down(&Semaphore);
-	}
-
 	return rc;
 }
 
@@ -353,46 +306,42 @@ static int signalVspInstruction( struct 
 /*
  * Send a 12-byte CE message to the primary partition VSP object
  */
-static int signalCEMsg( char * ceMsg, void * token )
+static int signal_ce_msg(char *ce_msg, struct CeMsgCompleteData *completion)
 {
-	struct StackElement * newElement = newStackElement();
-	int rc = 0;
+	struct pending_event *ev = new_pending_event();
 
-	if ( newElement == NULL )
-		rc = -ENOMEM;
-	else {
-		newElement->event.xHvLpEvent.xSubtype = 0;
-		newElement->event.xHvLpEvent.x.xSubtypeData = ('M'<<24)+('F'<<16)+('C'<<8)+('E'<<0);
-		memcpy( newElement->event.xUnion.xCEMsgData.xCEMsg, ceMsg, 12 );
-		newElement->event.xUnion.xCEMsgData.xToken = token;
-		rc = signalEvent(newElement);
-	}
+	if (ev == NULL)
+		return -ENOMEM;
 
-	return rc;
+	ev->event.hp_lp_event.xSubtype = 0;
+	ev->event.hp_lp_event.x.xSubtypeData =
+		subtype_data('M',  'F',  'C',  'E');
+	memcpy(ev->event.data.ce_msg.ce_msg, ce_msg, 12);
+	ev->event.data.ce_msg.completion = completion;
+	return signal_event(ev);
 }
 
 /*
  * Send a 12-byte CE message and DMA data to the primary partition VSP object
  */
-static int dmaAndSignalCEMsg( char * ceMsg, void * token, void * dmaData, unsigned dmaDataLength, unsigned remoteAddress )
+static int dma_and_signal_ce_msg(char *ce_msg,
+		struct CeMsgCompleteData *completion, void *dma_data,
+		unsigned dma_data_length, unsigned remote_address)
 {
-	struct StackElement * newElement = newStackElement();
-	int rc = 0;
+	struct pending_event *ev = new_pending_event();
 
-	if ( newElement == NULL )
-		rc = -ENOMEM;
-	else {
-		newElement->event.xHvLpEvent.xSubtype = 0;
-		newElement->event.xHvLpEvent.x.xSubtypeData = ('M'<<24)+('F'<<16)+('C'<<8)+('E'<<0);
-		memcpy( newElement->event.xUnion.xCEMsgData.xCEMsg, ceMsg, 12 );
-		newElement->event.xUnion.xCEMsgData.xToken = token;
-		memcpy( newElement->dmaData, dmaData, dmaDataLength );
-		newElement->dmaDataLength = dmaDataLength;
-		newElement->remoteAddress = remoteAddress;
-		rc = signalEvent(newElement);
-	}
+	if (ev == NULL)
+		return -ENOMEM;
 
-	return rc;
+	ev->event.hp_lp_event.xSubtype = 0;
+	ev->event.hp_lp_event.x.xSubtypeData =
+		subtype_data('M', 'F', 'C', 'E');
+	memcpy(ev->event.data.ce_msg.ce_msg, ce_msg, 12);
+	ev->event.data.ce_msg.completion = completion;
+	memcpy(ev->dma_data, dma_data, dma_data_length);
+	ev->dma_data_length = dma_data_length;
+	ev->remote_address = remote_address;
+	return signal_event(ev);
 }
 
 /*
@@ -401,18 +350,17 @@ static int dmaAndSignalCEMsg( char * ceM
  * this fails (why?), we'll simply force it off in a not-so-nice
  * manner.
  */
-static int shutdown( void )
+static int shutdown(void)
 {
-	int rc = kill_proc(1,SIGINT,1);
+	int rc = kill_proc(1, SIGINT, 1);
 
-	if ( rc )
-	{
-		printk( KERN_ALERT "mf.c: SIGINT to init failed (%d), hard shutdown commencing\n", rc );
+	if (rc) {
+		printk(KERN_ALERT "mf.c: SIGINT to init failed (%d), "
+				"hard shutdown commencing\n", rc);
 		mf_powerOff();
-	}
-	else
-		printk( KERN_INFO "mf.c: init has been successfully notified to proceed with shutdown\n" );
-
+	} else
+		printk(KERN_INFO "mf.c: init has been successfully notified "
+				"to proceed with shutdown\n");
 	return rc;
 }
 
@@ -420,67 +368,64 @@ static int shutdown( void )
  * The primary partition VSP object is sending us a new
  * event flow.  Handle it...
  */
-static void intReceived( struct IoMFLpEvent * event )
+static void intReceived(struct IoMFLpEvent *event)
 {
 	int freeIt = 0;
-	struct StackElement * two = NULL;
+	struct pending_event *two = NULL;
+
 	/* ack the interrupt */
-	event->xHvLpEvent.xRc = HvLpEvent_Rc_Good;
-	HvCallEvent_ackLpEvent( &event->xHvLpEvent );
+	event->hp_lp_event.xRc = HvLpEvent_Rc_Good;
+	HvCallEvent_ackLpEvent(&event->hp_lp_event);
 
-    /* process interrupt */
-	switch( event->xHvLpEvent.xSubtype )
-	{
+	/* process interrupt */
+	switch (event->hp_lp_event.xSubtype) {
 	case 0:	/* CE message */
-		switch( event->xUnion.xCEMsgData.xCEMsg[3] )
-		{
+		switch (event->data.ce_msg.ce_msg[3]) {
 		case 0x5B:	/* power control notification */
-			if ( (event->xUnion.xCEMsgData.xCEMsg[5]&0x20) != 0 )
-			{
-				printk( KERN_INFO "mf.c: Commencing partition shutdown\n" );
-				if ( shutdown() == 0 )
-					signalCEMsg( "\x00\x00\x00\xDB\x00\x00\x00\x00\x00\x00\x00\x00", NULL );
+			if ((event->data.ce_msg.ce_msg[5] & 0x20) != 0) {
+				printk(KERN_INFO "mf.c: Commencing partition shutdown\n");
+				if (shutdown() == 0)
+					signal_ce_msg("\x00\x00\x00\xDB\x00\x00\x00\x00\x00\x00\x00\x00", NULL);
 			}
 			break;
 		case 0xC0:	/* get time */
-			{
-				if ( (head != NULL) && ( head->event.xUnion.xCEMsgData.xCEMsg[3] == 0x40 ) )
-				{
-					freeIt = 1;
-					if ( head->event.xUnion.xCEMsgData.xToken != 0 )
-					{
-						CeMsgCompleteHandler xHdlr = head->event.xUnion.xCEMsgData.xToken->xHdlr;
-						void * token = head->event.xUnion.xCEMsgData.xToken->xToken;
+			if ((pending_event_head == NULL) ||
+			    (pending_event_head->event.data.ce_msg.ce_msg[3]
+			     != 0x40))
+				break;
+			freeIt = 1;
+			if (pending_event_head->event.data.ce_msg.completion != 0) {
+				CeMsgCompleteHandler handler = pending_event_head->event.data.ce_msg.completion->handler;
+				void *token = pending_event_head->event.data.ce_msg.completion->token;
 
-						if (xHdlr != NULL)
-							(*xHdlr)( token, &(event->xUnion.xCEMsgData) );
-					}
-				}
+				if (handler != NULL)
+					(*handler)(token, &(event->data.ce_msg));
 			}
 			break;
 		}
 
 		/* remove from queue */
-		if ( freeIt == 1 )
-		{
+		if (freeIt == 1) {
 			unsigned long flags;
-			spin_lock_irqsave( &spinlock, flags );
-			if ( head != NULL )
-			{
-				struct StackElement *oldHead = head;
-				head = head->next;
-				two = head;
-				free( oldHead );
+
+			spin_lock_irqsave(&pending_event_spinlock, flags);
+			if (pending_event_head != NULL) {
+				struct pending_event *oldHead =
+					pending_event_head;
+
+				pending_event_head = pending_event_head->next;
+				two = pending_event_head;
+				free_pending_event(oldHead);
 			}
-			spin_unlock_irqrestore( &spinlock, flags );
+			spin_unlock_irqrestore(&pending_event_spinlock, flags);
 		}
 
 		/* send next waiting event */
-		if ( two != NULL )
-			signalEvent( NULL );
+		if (two != NULL)
+			signal_event(NULL);
 		break;
 	case 1:	/* IT sys shutdown */
-		printk( KERN_INFO "mf.c: Commencing system shutdown\n" );
+		printk(KERN_INFO "mf.c: Commencing system shutdown\n");
 		shutdown();
 		break;
 	}
@@ -491,81 +436,74 @@ static void intReceived( struct IoMFLpEv
  * of a flow we sent to them.  If there are other flows queued
  * up, we must send another one now...
  */
-static void ackReceived( struct IoMFLpEvent * event )
+static void ackReceived(struct IoMFLpEvent *event)
 {
 	unsigned long flags;
-	struct StackElement * two = NULL;
+	struct pending_event * two = NULL;
 	unsigned long freeIt = 0;
 
-    /* handle current event */
-	if ( head != NULL )
-	{
-		switch( event->xHvLpEvent.xSubtype )
-		{
+	/* handle current event */
+	if (pending_event_head != NULL) {
+		switch (event->hp_lp_event.xSubtype) {
 		case 0:     /* CE msg */
-			if ( event->xUnion.xCEMsgData.xCEMsg[3] == 0x40 )
-			{
-				if ( event->xUnion.xCEMsgData.xCEMsg[2] != 0 )
-				{
+			if (event->data.ce_msg.ce_msg[3] == 0x40) {
+				if (event->data.ce_msg.ce_msg[2] != 0) {
 					freeIt = 1;
-					if ( head->event.xUnion.xCEMsgData.xToken != 0 )
-					{
-						CeMsgCompleteHandler xHdlr = head->event.xUnion.xCEMsgData.xToken->xHdlr;
-						void * token = head->event.xUnion.xCEMsgData.xToken->xToken;
+					if (pending_event_head->event.data.ce_msg.completion
+							!= 0) {
+						CeMsgCompleteHandler handler = pending_event_head->event.data.ce_msg.completion->handler;
+						void *token = pending_event_head->event.data.ce_msg.completion->token;
 
-						if (xHdlr != NULL)
-							(*xHdlr)( token, &(event->xUnion.xCEMsgData) );
+						if (handler != NULL)
+							(*handler)(token, &(event->data.ce_msg));
 					}
 				}
-			} else {
+			} else
 				freeIt = 1;
-			}
 			break;
 		case 4:	/* allocate */
 		case 5:	/* deallocate */
-			if ( head->hdlr != NULL )
-			{
-				union SafeCast mySafeCast;
-				mySafeCast.ptrAsU64 = event->xHvLpEvent.xCorrelationToken;
-				(*head->hdlr)( mySafeCast.ptr, event->xUnion.xAllocData.xCount );
+			if (pending_event_head->hdlr != NULL) {
+				union safe_cast mySafeCast;
+
+				mySafeCast.ptr_as_u64 = event->hp_lp_event.xCorrelationToken;
+				(*pending_event_head->hdlr)(mySafeCast.ptr, event->data.alloc.count);
 			}
 			freeIt = 1;
 			break;
 		case 6:
 			{
-				struct VspRspData *rsp = (struct VspRspData *)event->xUnion.xVspCmd.xTokenUnion.ptr;
+				struct VspRspData *rsp = (struct VspRspData *)event->data.vsp_cmd.token.ptr;
 
-				if (rsp != NULL)
-				{
-					if (rsp->xResponse != NULL)
-						memcpy(rsp->xResponse, &(event->xUnion.xVspCmd), sizeof(event->xUnion.xVspCmd));
-					if (rsp->xSemaphore != NULL)
-						up(rsp->xSemaphore);
-				} else {
-					printk( KERN_ERR "mf.c: no rsp\n");
-				}
+				if (rsp != NULL) {
+					if (rsp->response != NULL)
+						memcpy(rsp->response, &(event->data.vsp_cmd), sizeof(event->data.vsp_cmd));
+					if (rsp->sem != NULL)
+						up(rsp->sem);
+				} else
+					printk(KERN_ERR "mf.c: no rsp\n");
 				freeIt = 1;
 			}
 			break;
 		}
 	}
 	else
-		printk( KERN_ERR "mf.c: stack empty for receiving ack\n" );
+		printk(KERN_ERR "mf.c: stack empty for receiving ack\n");
 
-    /* remove from queue */
-	spin_lock_irqsave( &spinlock, flags );
-	if (( head != NULL ) && ( freeIt == 1 ))
-	{
-		struct StackElement *oldHead = head;
-		head = head->next;
-		two = head;
-		free( oldHead );
+	/* remove from queue */
+	spin_lock_irqsave(&pending_event_spinlock, flags);
+	if ((pending_event_head != NULL) && (freeIt == 1)) {
+		struct pending_event *oldHead = pending_event_head;
+
+		pending_event_head = pending_event_head->next;
+		two = pending_event_head;
+		free_pending_event(oldHead);
 	} 
-	spin_unlock_irqrestore( &spinlock, flags );
+	spin_unlock_irqrestore(&pending_event_spinlock, flags);
 
-    /* send next waiting event */
-	if ( two != NULL )
-		signalEvent( NULL );
+	/* send next waiting event */
+	if (two != NULL)
+		signal_event(NULL);
 }
 
 /*
@@ -574,101 +512,94 @@ static void ackReceived( struct IoMFLpEv
  * parse it enough to know if it is an interrupt or an
  * acknowledge.
  */
-static void hvHandler( struct HvLpEvent * event, struct pt_regs * regs )
+static void hvHandler(struct HvLpEvent *event, struct pt_regs *regs)
 {
-	if ( (event != NULL) && (event->xType == HvLpEvent_Type_MachineFac) )
-	{
-		switch( event->xFlags.xFunction )
-		{
+	if ((event != NULL) && (event->xType == HvLpEvent_Type_MachineFac)) {
+		switch(event->xFlags.xFunction) {
 		case HvLpEvent_Function_Ack:
-			ackReceived( (struct IoMFLpEvent *)event );
+			ackReceived((struct IoMFLpEvent *)event);
 			break;
 		case HvLpEvent_Function_Int:
-			intReceived( (struct IoMFLpEvent *)event );
+			intReceived((struct IoMFLpEvent *)event);
 			break;
 		default:
-			printk( KERN_ERR "mf.c: non ack/int event received\n" );
+			printk(KERN_ERR "mf.c: non ack/int event received\n");
 			break;
 		}
-	}
-	else
-		printk( KERN_ERR "mf.c: alien event received\n" );
+	} else
+		printk(KERN_ERR "mf.c: alien event received\n");
 }
 
 /*
  * Global kernel interface to allocate and seed events into the
  * Hypervisor.
  */
-void mf_allocateLpEvents( HvLpIndex targetLp,
-			  HvLpEvent_Type type,
-			  unsigned size,
-			  unsigned count,
-			  MFCompleteHandler hdlr,
-			  void * userToken )
+void mf_allocateLpEvents(HvLpIndex targetLp, HvLpEvent_Type type,
+		unsigned size, unsigned count, MFCompleteHandler hdlr,
+		void *userToken)
 {
-	struct StackElement * newElement = newStackElement();
-	int rc = 0;
+	struct pending_event *ev = new_pending_event();
+	int rc;
 
-	if ( newElement == NULL )
+	if (ev == NULL) {
 		rc = -ENOMEM;
-	else {
-		union SafeCast mine;
+	} else {
+		union safe_cast mine;
+
 		mine.ptr = userToken;
-		newElement->event.xHvLpEvent.xSubtype = 4;
-		newElement->event.xHvLpEvent.xCorrelationToken = mine.ptrAsU64;
-		newElement->event.xHvLpEvent.x.xSubtypeData = ('M'<<24)+('F'<<16)+('M'<<8)+('A'<<0);
-		newElement->event.xUnion.xAllocData.xTargetLp = targetLp;
-		newElement->event.xUnion.xAllocData.xType = type;
-		newElement->event.xUnion.xAllocData.xSize = size;
-		newElement->event.xUnion.xAllocData.xCount = count;
-		newElement->hdlr = hdlr;
-		rc = signalEvent(newElement);
+		ev->event.hp_lp_event.xSubtype = 4;
+		ev->event.hp_lp_event.xCorrelationToken = mine.ptr_as_u64;
+		ev->event.hp_lp_event.x.xSubtypeData =
+			subtype_data('M', 'F', 'M', 'A');
+		ev->event.data.alloc.target_lp = targetLp;
+		ev->event.data.alloc.type = type;
+		ev->event.data.alloc.size = size;
+		ev->event.data.alloc.count = count;
+		ev->hdlr = hdlr;
+		rc = signal_event(ev);
 	}
-
-	if ( (rc != 0) && (hdlr != NULL) )
-		(*hdlr)( userToken, rc );
+	if ((rc != 0) && (hdlr != NULL))
+		(*hdlr)(userToken, rc);
 }
 
 /*
  * Global kernel interface to unseed and deallocate events already in
  * Hypervisor.
  */
-void mf_deallocateLpEvents( HvLpIndex targetLp,
-			    HvLpEvent_Type type,
-			    unsigned count,
-			    MFCompleteHandler hdlr,
-			    void * userToken )
+void mf_deallocateLpEvents(HvLpIndex targetLp, HvLpEvent_Type type,
+		unsigned count, MFCompleteHandler hdlr, void *userToken)
 {
-	struct StackElement * newElement = newStackElement();
-	int rc = 0;
+	struct pending_event *ev = new_pending_event();
+	int rc;
 
-	if ( newElement == NULL )
+	if (ev == NULL)
 		rc = -ENOMEM;
 	else {
-		union SafeCast mine;
+		union safe_cast mine;
+
 		mine.ptr = userToken;
-		newElement->event.xHvLpEvent.xSubtype = 5;
-		newElement->event.xHvLpEvent.xCorrelationToken = mine.ptrAsU64;
-		newElement->event.xHvLpEvent.x.xSubtypeData = ('M'<<24)+('F'<<16)+('M'<<8)+('D'<<0);
-		newElement->event.xUnion.xAllocData.xTargetLp = targetLp;
-		newElement->event.xUnion.xAllocData.xType = type;
-		newElement->event.xUnion.xAllocData.xCount = count;
-		newElement->hdlr = hdlr;
-		rc = signalEvent(newElement);
+		ev->event.hp_lp_event.xSubtype = 5;
+		ev->event.hp_lp_event.xCorrelationToken = mine.ptr_as_u64;
+		ev->event.hp_lp_event.x.xSubtypeData =
+			subtype_data('M', 'F', 'M', 'D');
+		ev->event.data.alloc.target_lp = targetLp;
+		ev->event.data.alloc.type = type;
+		ev->event.data.alloc.count = count;
+		ev->hdlr = hdlr;
+		rc = signal_event(ev);
 	}
-
-	if ( (rc != 0) && (hdlr != NULL) )
-		(*hdlr)( userToken, rc );
+	if ((rc != 0) && (hdlr != NULL))
+		(*hdlr)(userToken, rc);
 }
 
 /*
  * Global kernel interface to tell the VSP object in the primary
  * partition to power this partition off.
  */
-void mf_powerOff( void )
+void mf_powerOff(void)
 {
-	printk( KERN_INFO "mf.c: Down it goes...\n" );
-	signalCEMsg( "\x00\x00\x00\x4D\x00\x00\x00\x00\x00\x00\x00\x00", NULL );
+	printk(KERN_INFO "mf.c: Down it goes...\n");
+	signal_ce_msg("\x00\x00\x00\x4D\x00\x00\x00\x00\x00\x00\x00\x00", NULL);
 	for (;;);
 }
 
@@ -676,111 +607,104 @@ void mf_powerOff( void )
  * Global kernel interface to tell the VSP object in the primary
  * partition to reboot this partition.
  */
-void mf_reboot( void )
+void mf_reboot(void)
 {
-	printk( KERN_INFO "mf.c: Preparing to bounce...\n" );
-	signalCEMsg( "\x00\x00\x00\x4E\x00\x00\x00\x00\x00\x00\x00\x00", NULL );
+	printk(KERN_INFO "mf.c: Preparing to bounce...\n");
+	signal_ce_msg("\x00\x00\x00\x4E\x00\x00\x00\x00\x00\x00\x00\x00", NULL);
 	for (;;);
 }
 
 /*
  * Display a single word SRC onto the VSP control panel.
  */
-void mf_displaySrc( u32 word )
+void mf_displaySrc(u32 word)
 {
 	u8 ce[12];
 
-	memcpy( ce, "\x00\x00\x00\x4A\x00\x00\x00\x01\x00\x00\x00\x00", 12 );
-	ce[8] = word>>24;
-	ce[9] = word>>16;
-	ce[10] = word>>8;
+	memcpy(ce, "\x00\x00\x00\x4A\x00\x00\x00\x01\x00\x00\x00\x00", 12);
+	ce[8] = word >> 24;
+	ce[9] = word >> 16;
+	ce[10] = word >> 8;
 	ce[11] = word;
-	signalCEMsg( ce, NULL );
+	signal_ce_msg(ce, NULL);
 }
 
 /*
  * Display a single word SRC of the form "PROGXXXX" on the VSP control panel.
  */
-void mf_displayProgress( u16 value )
+void mf_displayProgress(u16 value)
 {
 	u8 ce[12];
 	u8 src[72];
 
-	memcpy( ce, "\x00\x00\x04\x4A\x00\x00\x00\x48\x00\x00\x00\x00", 12 );
-	memcpy( src,
-		"\x01\x00\x00\x01"
-		"\x00\x00\x00\x00"
-		"\x00\x00\x00\x00"
-		"\x00\x00\x00\x00"
-		"\x00\x00\x00\x00"
-		"\x00\x00\x00\x00"
-		"\x00\x00\x00\x00"
-		"\x00\x00\x00\x00"
-		"\x00\x00\x00\x00"
-		"\x00\x00\x00\x00"
-		"PROGxxxx"
-		"                        ",
-		72 );
-	src[6] = value>>8;
-	src[7] = value&255;
-	src[44] = "0123456789ABCDEF"[(value>>12)&15];
-	src[45] = "0123456789ABCDEF"[(value>>8)&15];
-	src[46] = "0123456789ABCDEF"[(value>>4)&15];
-	src[47] = "0123456789ABCDEF"[value&15];
-	dmaAndSignalCEMsg( ce, NULL, src, sizeof(src), 9*64*1024 );
+	memcpy(ce, "\x00\x00\x04\x4A\x00\x00\x00\x48\x00\x00\x00\x00", 12);
+	memcpy(src, "\x01\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x00"
+		"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00"
+		"\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00"
+		"\x00\x00\x00\x00PROGxxxx                        ",
+		72);
+	src[6] = value >> 8;
+	src[7] = value & 255;
+	src[44] = "0123456789ABCDEF"[(value >> 12) & 15];
+	src[45] = "0123456789ABCDEF"[(value >> 8) & 15];
+	src[46] = "0123456789ABCDEF"[(value >> 4) & 15];
+	src[47] = "0123456789ABCDEF"[value & 15];
+	dma_and_signal_ce_msg(ce, NULL, src, sizeof(src), 9 * 64 * 1024);
 }
 
 /*
  * Clear the VSP control panel.  Used to "erase" an SRC that was
  * previously displayed.
  */
-void mf_clearSrc( void )
+void mf_clearSrc(void)
 {
-	signalCEMsg( "\x00\x00\x00\x4B\x00\x00\x00\x00\x00\x00\x00\x00", NULL );
+	signal_ce_msg("\x00\x00\x00\x4B\x00\x00\x00\x00\x00\x00\x00\x00", NULL);
 }
 
 /*
  * Initialization code here.
  */
-void mf_init( void )
+void mf_init(void)
 {
 	int i;
 
-    /* initialize */
-	spin_lock_init( &spinlock );
-	for ( i = 0; i < sizeof(prealloc)/sizeof(*prealloc); ++i )
-		free( &prealloc[i] );
-	HvLpEvent_registerHandler( HvLpEvent_Type_MachineFac, &hvHandler );
+	/* initialize */
+	spin_lock_init(&pending_event_spinlock);
+	for (i = 0;
+	     i < sizeof(pending_event_prealloc) / sizeof(*pending_event_prealloc);
+	     ++i)
+		free_pending_event(&pending_event_prealloc[i]);
+	HvLpEvent_registerHandler(HvLpEvent_Type_MachineFac, &hvHandler);
 
 	/* virtual continue ack */
-	signalCEMsg( "\x00\x00\x00\x57\x00\x00\x00\x00\x00\x00\x00\x00", NULL );
+	signal_ce_msg("\x00\x00\x00\x57\x00\x00\x00\x00\x00\x00\x00\x00", NULL);
 
 	/* initialization complete */
-	printk( KERN_NOTICE "mf.c: iSeries Linux LPAR Machine Facilities initialized\n" );
+	printk(KERN_NOTICE "mf.c: iSeries Linux LPAR Machine Facilities initialized\n");
 
 	iSeries_proc_callback(&mf_proc_init);
 }
 
 void mf_setSide(char side)
 {
-	int rc = 0;
-	u64 newSide = 0;
+	u64 newSide;
 	struct VspCmdData myVspCmd;
 
 	memset(&myVspCmd, 0, sizeof(myVspCmd));
-	if (side == 'A')
-		newSide = 0;
-	else if (side == 'B')
-		newSide = 1;
-	else if (side == 'C')
-		newSide = 2; 
-	else
-		newSide = 3;
-
-	myVspCmd.xSubData.xFunction02SelectIplTypeIn.xIplType = newSide;
-	myVspCmd.xCmd = 10;
+	switch (side) {
+	case 'A':	newSide = 0;
+			break;
+	case 'B':	newSide = 1;
+			break;
+	case 'C':	newSide = 2; 
+			break;
+	default:	newSide = 3;
+			break;
+	}
+	myVspCmd.sub_data.ipl_type = newSide;
+	myVspCmd.cmd = 10;
 
-	rc = signalVspInstruction(&myVspCmd);
+	(void)signal_vsp_instruction(&myVspCmd);
 }
 
 char mf_getSide(void)
@@ -790,91 +714,82 @@ char mf_getSide(void)
 	struct VspCmdData myVspCmd;
 
 	memset(&myVspCmd, 0, sizeof(myVspCmd));
-	myVspCmd.xCmd = 2;
-	myVspCmd.xSubData.xFunction02SelectIplTypeIn.xIplType = 0;
+	myVspCmd.cmd = 2;
+	myVspCmd.sub_data.ipl_type = 0;
 	mb();
-	rc = signalVspInstruction(&myVspCmd);
+	rc = signal_vsp_instruction(&myVspCmd);
 
 	if (rc != 0)
-	{
 		return returnValue;
-	} else {
-		if (myVspCmd.xRc == 0)
-		{
-			if (myVspCmd.xSubData.xGetIplTypeOut.xIplType == 0)
-				returnValue = 'A';
-			else if (myVspCmd.xSubData.xGetIplTypeOut.xIplType == 1)
-				returnValue = 'B';
-			else if (myVspCmd.xSubData.xGetIplTypeOut.xIplType == 2)
-				returnValue = 'C';
-			else
-				returnValue = 'D';
+
+	if (myVspCmd.result_code == 0) {
+		switch (myVspCmd.sub_data.ipl_type) {
+		case 0:	returnValue = 'A';
+			break;
+		case 1:	returnValue = 'B';
+			break;
+		case 2:	returnValue = 'C';
+			break;
+		default:	returnValue = 'D';
+			break;
 		}
 	}
-
 	return returnValue;
 }
 
 void mf_getSrcHistory(char *buffer, int size)
 {
-    /*    struct IplTypeReturnStuff returnStuff;
-     struct StackElement * newElement = newStackElement();
-     int rc = 0;
-     char *pages[4];
-
-     pages[0] = kmalloc(4096, GFP_ATOMIC);
-     pages[1] = kmalloc(4096, GFP_ATOMIC);
-     pages[2] = kmalloc(4096, GFP_ATOMIC);
-     pages[3] = kmalloc(4096, GFP_ATOMIC);
-     if (( newElement == NULL ) || (pages[0] == NULL) || (pages[1] == NULL) || (pages[2] == NULL) || (pages[3] == NULL))
-     rc = -ENOMEM;
-     else
-     {
-     returnStuff.xType = 0;
-     returnStuff.xRc = 0;
-     returnStuff.xDone = 0;
-     newElement->event.xHvLpEvent.xSubtype = 6;
-     newElement->event.xHvLpEvent.x.xSubtypeData = ('M'<<24)+('F'<<16)+('V'<<8)+('I'<<0);
-     newElement->event.xUnion.xVspCmd.xEvent = &returnStuff;
-     newElement->event.xUnion.xVspCmd.xCmd = 4;
-     newElement->event.xUnion.xVspCmd.xLpIndex = HvLpConfig_getLpIndex();
-     newElement->event.xUnion.xVspCmd.xRc = 0xFF;
-     newElement->event.xUnion.xVspCmd.xReserved1 = 0;
-     newElement->event.xUnion.xVspCmd.xSubData.xGetSrcHistoryIn.xPage[0] = (0x8000000000000000ULL | virt_to_absolute((unsigned long)pages[0]));
-     newElement->event.xUnion.xVspCmd.xSubData.xGetSrcHistoryIn.xPage[1] = (0x8000000000000000ULL | virt_to_absolute((unsigned long)pages[1]));
-     newElement->event.xUnion.xVspCmd.xSubData.xGetSrcHistoryIn.xPage[2] = (0x8000000000000000ULL | virt_to_absolute((unsigned long)pages[2]));
-     newElement->event.xUnion.xVspCmd.xSubData.xGetSrcHistoryIn.xPage[3] = (0x8000000000000000ULL | virt_to_absolute((unsigned long)pages[3]));
-     mb();
-     rc = signalEvent(newElement);
-     }
-
-     if (rc != 0)
-     {
-     return;
-     }
-     else
-     {
-     while (returnStuff.xDone != 1)
-     {
-     udelay(10);
-     }
-
-     if (returnStuff.xRc == 0)
-     {
-     memcpy(buffer, pages[0], size);
-     }
-     }
-
-     kfree(pages[0]);
-     kfree(pages[1]);
-     kfree(pages[2]);
-     kfree(pages[3]);*/
+#if 0
+	struct IplTypeReturnStuff returnStuff;
+	struct pending_event *ev = new_pending_event();
+	int rc = 0;
+	char *pages[4];
+
+	pages[0] = kmalloc(4096, GFP_ATOMIC);
+	pages[1] = kmalloc(4096, GFP_ATOMIC);
+	pages[2] = kmalloc(4096, GFP_ATOMIC);
+	pages[3] = kmalloc(4096, GFP_ATOMIC);
+	if ((ev == NULL) || (pages[0] == NULL) || (pages[1] == NULL)
+			 || (pages[2] == NULL) || (pages[3] == NULL))
+		return -ENOMEM;
+
+	returnStuff.xType = 0;
+	returnStuff.xRc = 0;
+	returnStuff.xDone = 0;
+	ev->event.hp_lp_event.xSubtype = 6;
+	ev->event.hp_lp_event.x.xSubtypeData =
+		subtype_data('M', 'F', 'V', 'I');
+	ev->event.data.vsp_cmd.xEvent = &returnStuff;
+	ev->event.data.vsp_cmd.cmd = 4;
+	ev->event.data.vsp_cmd.lp_index = HvLpConfig_getLpIndex();
+	ev->event.data.vsp_cmd.result_code = 0xFF;
+	ev->event.data.vsp_cmd.reserved = 0;
+	ev->event.data.vsp_cmd.sub_data.page[0] =
+		(0x8000000000000000ULL | virt_to_absolute((unsigned long)pages[0]));
+	ev->event.data.vsp_cmd.sub_data.page[1] =
+		(0x8000000000000000ULL | virt_to_absolute((unsigned long)pages[1]));
+	ev->event.data.vsp_cmd.sub_data.page[2] =
+		(0x8000000000000000ULL | virt_to_absolute((unsigned long)pages[2]));
+	ev->event.data.vsp_cmd.sub_data.page[3] =
+		(0x8000000000000000ULL | virt_to_absolute((unsigned long)pages[3]));
+	mb();
+	if (signal_event(ev) != 0)
+		return;
+
+ 	while (returnStuff.xDone != 1)
+ 		udelay(10);
+ 	if (returnStuff.xRc == 0)
+ 		memcpy(buffer, pages[0], size);
+	kfree(pages[0]);
+	kfree(pages[1]);
+	kfree(pages[2]);
+	kfree(pages[3]);
+#endif
 }
 
 void mf_setCmdLine(const char *cmdline, int size, u64 side)
 {
 	struct VspCmdData myVspCmd;
-	int rc = 0;
 	dma_addr_t dma_addr = 0;
 	char *page = pci_alloc_consistent(iSeries_vio_dev, size, &dma_addr);
 
@@ -886,13 +801,13 @@ void mf_setCmdLine(const char *cmdline, 
 	copy_from_user(page, cmdline, size);
 
 	memset(&myVspCmd, 0, sizeof(myVspCmd));
-	myVspCmd.xCmd = 31;
-	myVspCmd.xSubData.xSetKernelCmdLineIn.xToken = dma_addr;
-	myVspCmd.xSubData.xSetKernelCmdLineIn.xAddressType = HvLpDma_AddressType_TceIndex;
-	myVspCmd.xSubData.xSetKernelCmdLineIn.xSide = side;
-	myVspCmd.xSubData.xSetKernelCmdLineIn.xTransferLength = size;
+	myVspCmd.cmd = 31;
+	myVspCmd.sub_data.kern.token = dma_addr;
+	myVspCmd.sub_data.kern.address_type = HvLpDma_AddressType_TceIndex;
+	myVspCmd.sub_data.kern.side = side;
+	myVspCmd.sub_data.kern.length = size;
 	mb();
-	rc = signalVspInstruction(&myVspCmd);
+	(void)signal_vsp_instruction(&myVspCmd);
 
 	pci_free_consistent(iSeries_vio_dev, size, page, dma_addr);
 }
@@ -900,31 +815,29 @@ void mf_setCmdLine(const char *cmdline, 
 int mf_getCmdLine(char *cmdline, int *size, u64 side)
 {
 	struct VspCmdData myVspCmd;
-	int rc = 0;
+	int rc;
 	int len = *size;
-	dma_addr_t dma_addr = pci_map_single(iSeries_vio_dev, cmdline, *size, PCI_DMA_FROMDEVICE);
+	dma_addr_t dma_addr;
 
-	memset(cmdline, 0, *size);
+	dma_addr = pci_map_single(iSeries_vio_dev, cmdline, len,
+			PCI_DMA_FROMDEVICE);
+	memset(cmdline, 0, len);
 	memset(&myVspCmd, 0, sizeof(myVspCmd));
-	myVspCmd.xCmd = 33;
-	myVspCmd.xSubData.xGetKernelCmdLineIn.xToken = dma_addr;
-	myVspCmd.xSubData.xGetKernelCmdLineIn.xAddressType = HvLpDma_AddressType_TceIndex;
-	myVspCmd.xSubData.xGetKernelCmdLineIn.xSide = side;
-	myVspCmd.xSubData.xGetKernelCmdLineIn.xTransferLength = *size;
+	myVspCmd.cmd = 33;
+	myVspCmd.sub_data.kern.token = dma_addr;
+	myVspCmd.sub_data.kern.address_type = HvLpDma_AddressType_TceIndex;
+	myVspCmd.sub_data.kern.side = side;
+	myVspCmd.sub_data.kern.length = len;
 	mb();
-	rc = signalVspInstruction(&myVspCmd);
-
-	if ( ! rc ) {
+	rc = signal_vsp_instruction(&myVspCmd);
 
-		if (myVspCmd.xRc == 0)
-		{
-			len = myVspCmd.xSubData.xGetKernelCmdLineOut.xTransferLength;
-		}
-		/* else
-			{
+	if (rc == 0) {
+		if (myVspCmd.result_code == 0)
+			len = myVspCmd.sub_data.length_out;
+#if 0
+		else
 			memcpy(cmdline, "Bad cmdline", 11);
-			}
-		*/
+#endif
 	}
 
 	pci_unmap_single(iSeries_vio_dev, dma_addr, *size, PCI_DMA_FROMDEVICE);
@@ -936,10 +849,8 @@ int mf_getCmdLine(char *cmdline, int *si
 int mf_setVmlinuxChunk(const char *buffer, int size, int offset, u64 side)
 {
 	struct VspCmdData myVspCmd;
-	int rc = 0;
-
+	int rc;
 	dma_addr_t dma_addr = 0;
-
 	char *page = pci_alloc_consistent(iSeries_vio_dev, size, &dma_addr);
 
 	if (page == NULL) {
@@ -950,23 +861,19 @@ int mf_setVmlinuxChunk(const char *buffe
 	copy_from_user(page, buffer, size);
 	memset(&myVspCmd, 0, sizeof(myVspCmd));
 
-	myVspCmd.xCmd = 30;
-	myVspCmd.xSubData.xGetKernelImageIn.xToken = dma_addr;
-	myVspCmd.xSubData.xGetKernelImageIn.xAddressType = HvLpDma_AddressType_TceIndex;
-	myVspCmd.xSubData.xGetKernelImageIn.xSide = side;
-	myVspCmd.xSubData.xGetKernelImageIn.xOffset = offset;
-	myVspCmd.xSubData.xGetKernelImageIn.xTransferLength = size;
+	myVspCmd.cmd = 30;
+	myVspCmd.sub_data.kern.token = dma_addr;
+	myVspCmd.sub_data.kern.address_type = HvLpDma_AddressType_TceIndex;
+	myVspCmd.sub_data.kern.side = side;
+	myVspCmd.sub_data.kern.offset = offset;
+	myVspCmd.sub_data.kern.length = size;
 	mb();
-	rc = signalVspInstruction(&myVspCmd);
-
-	if (rc == 0)
-	{
-		if (myVspCmd.xRc == 0)
-		{
+	rc = signal_vsp_instruction(&myVspCmd);
+	if (rc == 0) {
+		if (myVspCmd.result_code == 0)
 			rc = 0;
-		} else {
+		else
 			rc = -ENOMEM;
-		}
 	}
 
 	pci_free_consistent(iSeries_vio_dev, size, page, dma_addr);
@@ -977,31 +884,27 @@ int mf_setVmlinuxChunk(const char *buffe
 int mf_getVmlinuxChunk(char *buffer, int *size, int offset, u64 side)
 {
 	struct VspCmdData myVspCmd;
-	int rc = 0;
+	int rc;
 	int len = *size;
+	dma_addr_t dma_addr;
 
-	dma_addr_t dma_addr = pci_map_single(iSeries_vio_dev, buffer, *size, PCI_DMA_FROMDEVICE);
-
+	dma_addr = pci_map_single(iSeries_vio_dev, buffer, len,
+			PCI_DMA_FROMDEVICE);
 	memset(buffer, 0, len);
-
 	memset(&myVspCmd, 0, sizeof(myVspCmd));
-	myVspCmd.xCmd = 32;
-	myVspCmd.xSubData.xGetKernelImageIn.xToken = dma_addr;
-	myVspCmd.xSubData.xGetKernelImageIn.xAddressType = HvLpDma_AddressType_TceIndex;
-	myVspCmd.xSubData.xGetKernelImageIn.xSide = side;
-	myVspCmd.xSubData.xGetKernelImageIn.xOffset = offset;
-	myVspCmd.xSubData.xGetKernelImageIn.xTransferLength = len;
+	myVspCmd.cmd = 32;
+	myVspCmd.sub_data.kern.token = dma_addr;
+	myVspCmd.sub_data.kern.address_type = HvLpDma_AddressType_TceIndex;
+	myVspCmd.sub_data.kern.side = side;
+	myVspCmd.sub_data.kern.offset = offset;
+	myVspCmd.sub_data.kern.length = len;
 	mb();
-	rc = signalVspInstruction(&myVspCmd);
-
-	if (rc == 0)
-	{
-		if (myVspCmd.xRc == 0)
-		{
-			*size = myVspCmd.xSubData.xGetKernelImageOut.xTransferLength;
-		} else {
+	rc = signal_vsp_instruction(&myVspCmd);
+	if (rc == 0) {
+		if (myVspCmd.result_code == 0)
+			*size = myVspCmd.sub_data.length_out;
+		else
 			rc = -ENOMEM;
-		}
 	}
 
 	pci_unmap_single(iSeries_vio_dev, dma_addr, len, PCI_DMA_FROMDEVICE);
@@ -1015,12 +918,11 @@ int mf_setRtcTime(unsigned long time)
 
 	to_tm(time, &tm);
 
-	return mf_setRtc( &tm );
+	return mf_setRtc(&tm);
 }
 
-struct RtcTimeData
-{
-	struct semaphore *xSemaphore;
+struct RtcTimeData {
+	struct semaphore *sem;
 	struct CeMsgData xCeMsg;
 	int xRc;
 };
@@ -1030,26 +932,23 @@ void getRtcTimeComplete(void * token, st
 	struct RtcTimeData *rtc = (struct RtcTimeData *)token;
 
 	memcpy(&(rtc->xCeMsg), ceMsg, sizeof(rtc->xCeMsg));
-
 	rtc->xRc = 0;
-	up(rtc->xSemaphore);
+	up(rtc->sem);
 }
 
 static unsigned long lastsec = 1;
 
 int mf_getRtcTime(unsigned long *time)
 {
-/*    unsigned long usec, tsec; */
-	
 	u32 dataWord1 = *((u32 *)(&xSpCommArea.xBcdTimeAtIplStart));
 	u32 dataWord2 = *(((u32 *)&(xSpCommArea.xBcdTimeAtIplStart)) + 1);
 	int year = 1970;
-	int year1 = ( dataWord1 >> 24 ) & 0x000000FF;
-	int year2 = ( dataWord1 >> 16 ) & 0x000000FF;
-	int sec = ( dataWord1 >> 8 ) & 0x000000FF;
+	int year1 = (dataWord1 >> 24) & 0x000000FF;
+	int year2 = (dataWord1 >> 16) & 0x000000FF;
+	int sec = (dataWord1 >> 8) & 0x000000FF;
 	int min = dataWord1 & 0x000000FF;
-	int hour = ( dataWord2 >> 24 ) & 0x000000FF;
-	int day = ( dataWord2 >> 8 ) & 0x000000FF;
+	int hour = (dataWord2 >> 24) & 0x000000FF;
+	int day = (dataWord2 >> 8) & 0x000000FF;
 	int mon = dataWord2 & 0x000000FF;
 
 	BCD_TO_BIN(sec);
@@ -1062,49 +961,41 @@ int mf_getRtcTime(unsigned long *time)
 	year = year1 * 100 + year2;
 
 	*time = mktime(year, mon, day, hour, min, sec);
-
-	*time += ( jiffies / HZ );
+	*time += (jiffies / HZ);
     
-	/* Now THIS is a nasty hack!
+	/*
+	 * Now THIS is a nasty hack!
 	 * It ensures that the first two calls to mf_getRtcTime get different
 	 * answers.  That way the loop in init_time (time.c) will not think
 	 * the clock is stuck.
 	 */
-	if ( lastsec ) {
+	if (lastsec) {
 		*time -= lastsec;
 		--lastsec;
 	}
-    
 	return 0;
-
 }
 
-int mf_getRtc( struct rtc_time * tm )
+int mf_getRtc(struct rtc_time *tm)
 {
-
 	struct CeMsgCompleteData ceComplete;
 	struct RtcTimeData rtcData;
-	int rc = 0;
+	int rc;
 	DECLARE_MUTEX_LOCKED(Semaphore);
 
 	memset(&ceComplete, 0, sizeof(ceComplete));
 	memset(&rtcData, 0, sizeof(rtcData));
-
-	rtcData.xSemaphore = &Semaphore;
-
-	ceComplete.xHdlr = &getRtcTimeComplete;
-	ceComplete.xToken = (void *)&rtcData;
-
-	rc = signalCEMsg( "\x00\x00\x00\x40\x00\x00\x00\x00\x00\x00\x00\x00", &ceComplete );
-
-	if ( rc == 0 )
-	{
+	rtcData.sem = &Semaphore;
+	ceComplete.handler = &getRtcTimeComplete;
+	ceComplete.token = (void *)&rtcData;
+	rc = signal_ce_msg("\x00\x00\x00\x40\x00\x00\x00\x00\x00\x00\x00\x00",
+			&ceComplete);
+	if (rc == 0) {
 		down(&Semaphore);
 
-		if ( rtcData.xRc == 0)
-		{
-			if ( ( rtcData.xCeMsg.xCEMsg[2] == 0xa9 ) ||
-			     ( rtcData.xCeMsg.xCEMsg[2] == 0xaf ) ) {
+		if (rtcData.xRc == 0) {
+			if ((rtcData.xCeMsg.ce_msg[2] == 0xa9) ||
+			    (rtcData.xCeMsg.ce_msg[2] == 0xaf)) {
 				/* TOD clock is not set */
 				tm->tm_sec = 1;
 				tm->tm_min = 1;
@@ -1112,16 +1003,16 @@ int mf_getRtc( struct rtc_time * tm )
 				tm->tm_mday = 10;
 				tm->tm_mon = 8;
 				tm->tm_year = 71;
-				mf_setRtc( tm );
+				mf_setRtc(tm);
 			}
 			{
-				u32 dataWord1 = *((u32 *)(rtcData.xCeMsg.xCEMsg+4));
-				u32 dataWord2 = *((u32 *)(rtcData.xCeMsg.xCEMsg+8));
-				u8 year = (dataWord1 >> 16 ) & 0x000000FF;
-				u8 sec = ( dataWord1 >> 8 ) & 0x000000FF;
+				u32 dataWord1 = *((u32 *)(rtcData.xCeMsg.ce_msg+4));
+				u32 dataWord2 = *((u32 *)(rtcData.xCeMsg.ce_msg+8));
+				u8 year = (dataWord1 >> 16) & 0x000000FF;
+				u8 sec = (dataWord1 >> 8) & 0x000000FF;
 				u8 min = dataWord1 & 0x000000FF;
-				u8 hour = ( dataWord2 >> 24 ) & 0x000000FF;
-				u8 day = ( dataWord2 >> 8 ) & 0x000000FF;
+				u8 hour = (dataWord2 >> 24) & 0x000000FF;
+				u8 day = (dataWord2 >> 8) & 0x000000FF;
 				u8 mon = dataWord2 & 0x000000FF;
 
 				BCD_TO_BIN(sec);
@@ -1131,7 +1022,7 @@ int mf_getRtc( struct rtc_time * tm )
 				BCD_TO_BIN(mon);
 				BCD_TO_BIN(year);
 
-				if ( year <= 69 )
+				if (year <= 69)
 					year += 100;
 	    
 				tm->tm_sec = sec;
@@ -1154,17 +1045,14 @@ int mf_getRtc( struct rtc_time * tm )
 		tm->tm_wday = 0;
 		tm->tm_yday = 0;
 		tm->tm_isdst = 0;
-
 	}
 
 	return rc;
-
 }
 
 int mf_setRtc(struct rtc_time * tm)
 {
 	char ceTime[12] = "\x00\x00\x00\x41\x00\x00\x00\x00\x00\x00\x00\x00";
-	int rc = 0;
 	u8 day, mon, hour, min, sec, y1, y2;
 	unsigned year;
     
@@ -1194,10 +1082,5 @@ int mf_setRtc(struct rtc_time * tm)
 	ceTime[10] = day;
 	ceTime[11] = mon;
    
-	rc = signalCEMsg( ceTime, NULL );
-
-	return rc;
+	return signal_ce_msg(ceTime, NULL);
 }
-
-
-
diff -purN linux-2.5/arch/ppc64/kernel/mf_proc.c linuxppc64-2.5/arch/ppc64/kernel/mf_proc.c
--- linux-2.5/arch/ppc64/kernel/mf_proc.c	2002-04-25 04:46:36.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/mf_proc.c	2003-12-17 05:50:27.000000000 +0000
@@ -16,151 +16,27 @@
  * along with this program; if not, write to the Free Software
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
-
-
-/* Change Activity: */
-/* End Change Activity */
-
-#ifndef _MF_PROC_H
-#include <asm/iSeries/mf_proc.h>
-#endif
-#ifndef MF_H_INCLUDED
-#include <asm/iSeries/mf.h>
-#endif
 #include <asm/uaccess.h>
+#include <asm/iSeries/mf.h>
 
 static struct proc_dir_entry *mf_proc_root = NULL;
 
-int proc_mf_dump_cmdline
-(char *page, char **start, off_t off, int count, int *eof, void *data);
-
-int proc_mf_dump_vmlinux
-(char *page, char **start, off_t off, int count, int *eof, void *data);
-
-int proc_mf_dump_side
-(char *page, char **start, off_t off, int count, int *eof, void *data);
-
-int proc_mf_change_side
-(struct file *file, const char *buffer, unsigned long count, void *data);
-
-int proc_mf_dump_src
-(char *page, char **start, off_t off, int count, int *eof, void *data);
-int proc_mf_change_src (struct file *file, const char *buffer, unsigned long count, void *data);
-int proc_mf_change_cmdline(struct file *file, const char *buffer, unsigned long count, void *data);
-int proc_mf_change_vmlinux(struct file *file, const char *buffer, unsigned long count, void *data);
-
-
-void mf_proc_init(struct proc_dir_entry *iSeries_proc)
-{
-	struct proc_dir_entry *ent = NULL;
-	struct proc_dir_entry *mf_a = NULL;
-	struct proc_dir_entry *mf_b = NULL;
-	struct proc_dir_entry *mf_c = NULL;
-	struct proc_dir_entry *mf_d = NULL;
-
-	mf_proc_root = proc_mkdir("mf", iSeries_proc);
-	if (!mf_proc_root) return;
-
-	mf_a = proc_mkdir("A", mf_proc_root);
-	if (!mf_a) return;
-
-	ent = create_proc_entry("cmdline", S_IFREG|S_IRUSR|S_IWUSR, mf_a);
-	if (!ent) return;
-	ent->nlink = 1;
-	ent->data = (void *)0;
-	ent->read_proc = proc_mf_dump_cmdline;
-	ent->write_proc = proc_mf_change_cmdline;
-
-	ent = create_proc_entry("vmlinux", S_IFREG|S_IWUSR, mf_a);
-	if (!ent) return;
-	ent->nlink = 1;
-	ent->data = (void *)0;
-	ent->write_proc = proc_mf_change_vmlinux;
-	ent->read_proc = NULL;
-
-	mf_b = proc_mkdir("B", mf_proc_root);
-	if (!mf_b) return;
-
-	ent = create_proc_entry("cmdline", S_IFREG|S_IRUSR|S_IWUSR, mf_b);
-	if (!ent) return;
-	ent->nlink = 1;
-	ent->data = (void *)1;
-	ent->read_proc = proc_mf_dump_cmdline;
-	ent->write_proc = proc_mf_change_cmdline;
-
-	ent = create_proc_entry("vmlinux", S_IFREG|S_IWUSR, mf_b);
-	if (!ent) return;
-	ent->nlink = 1;
-	ent->data = (void *)1;
-	ent->write_proc = proc_mf_change_vmlinux;
-	ent->read_proc = NULL;
-
-	mf_c = proc_mkdir("C", mf_proc_root);
-	if (!mf_c) return;
-
-	ent = create_proc_entry("cmdline", S_IFREG|S_IRUSR|S_IWUSR, mf_c);
-	if (!ent) return;
-	ent->nlink = 1;
-	ent->data = (void *)2;
-	ent->read_proc = proc_mf_dump_cmdline;
-	ent->write_proc = proc_mf_change_cmdline;
-
-	ent = create_proc_entry("vmlinux", S_IFREG|S_IWUSR, mf_c);
-	if (!ent) return;
-	ent->nlink = 1;
-	ent->data = (void *)2;
-	ent->write_proc = proc_mf_change_vmlinux;
-	ent->read_proc = NULL;
-
-	mf_d = proc_mkdir("D", mf_proc_root);
-	if (!mf_d) return;
-
-
-	ent = create_proc_entry("cmdline", S_IFREG|S_IRUSR|S_IWUSR, mf_d);
-	if (!ent) return;
-	ent->nlink = 1;
-	ent->data = (void *)3;
-	ent->read_proc = proc_mf_dump_cmdline;
-	ent->write_proc = proc_mf_change_cmdline;
-#if 0
-	ent = create_proc_entry("vmlinux", S_IFREG|S_IRUSR, mf_d);
-	if (!ent) return;
-	ent->nlink = 1;
-	ent->data = (void *)3;
-	ent->read_proc = proc_mf_dump_vmlinux;
-	ent->write_proc = NULL;
-#endif
-	ent = create_proc_entry("side", S_IFREG|S_IRUSR|S_IWUSR, mf_proc_root);
-	if (!ent) return;
-	ent->nlink = 1;
-	ent->data = (void *)0;
-	ent->read_proc = proc_mf_dump_side;
-	ent->write_proc = proc_mf_change_side;
-
-	ent = create_proc_entry("src", S_IFREG|S_IRUSR|S_IWUSR, mf_proc_root);
-	if (!ent) return;
-	ent->nlink = 1;
-	ent->data = (void *)0;
-	ent->read_proc = proc_mf_dump_src;
-	ent->write_proc = proc_mf_change_src;
-}
-
-int proc_mf_dump_cmdline
-(char *page, char **start, off_t off, int count, int *eof, void *data)
+static int proc_mf_dump_cmdline(char *page, char **start, off_t off,
+		int count, int *eof, void *data)
 {
-	int		len = count;
+	int len = count;
 	char *p;
     
 	len = mf_getCmdLine(page, &len, (u64)data);
    
 	p = page + len - 1;
-	while ( p > page ) {
-		if ( (*p == 0) || (*p == ' ') )
+	while (p > page) {
+		if ((*p == 0) || (*p == ' '))
 			--p;
 		else
 			break;
 	}
-	if ( *p != '\n' ) {
+	if (*p != '\n') {
 		++p;
 		*p = '\n';
 	}
@@ -179,68 +55,76 @@ int proc_mf_dump_cmdline
 	return len;			
 }
 
-int proc_mf_dump_vmlinux
-(char *page, char **start, off_t off, int count, int *eof, void *data)
+#if 0
+static int proc_mf_dump_vmlinux(char *page, char **start, off_t off,
+		int count, int *eof, void *data)
 {
 	int sizeToGet = count;
+
 	if (!capable(CAP_SYS_ADMIN))
 		return -EACCES;
 
-	if (mf_getVmlinuxChunk(page, &sizeToGet, off, (u64)data) == 0)
-	{
-		if (sizeToGet != 0)
-		{
+	if (mf_getVmlinuxChunk(page, &sizeToGet, off, (u64)data) == 0) {
+		if (sizeToGet != 0) {
 			*start = page + off;
 			return sizeToGet;
-		} else {
-			*eof = 1;
-			return 0;
 		}
-	} else {
 		*eof = 1;
 		return 0;
 	}
+	*eof = 1;
+	return 0;
 }
+#endif
 
-int proc_mf_dump_side
-(char *page, char **start, off_t off, int count, int *eof, void *data)
+static int proc_mf_dump_side(char *page, char **start, off_t off,
+		int count, int *eof, void *data)
 {
-	int		len = 0;
-
+	int len;
 	char mf_current_side = mf_getSide();
+
 	len = sprintf(page, "%c\n", mf_current_side);
 
-	if (len <= off+count) *eof = 1;
+	if (len <= (off + count))
+		*eof = 1;
 	*start = page + off;
 	len -= off;
-	if (len>count) len = count;
-	if (len<0) len = 0;
+	if (len > count)
+		len = count;
+	if (len < 0)
+		len = 0;
 	return len;			
 }
 
-int proc_mf_change_side(struct file *file, const char *buffer, unsigned long count, void *data)
+static int proc_mf_change_side(struct file *file, const char __user *buffer,
+		unsigned long count, void *data)
 {
+	char stkbuf[10];
+
 	if (!capable(CAP_SYS_ADMIN))
 		return -EACCES;
 
-	if ((*buffer != 'A') &&
-	    (*buffer != 'B') &&
-	    (*buffer != 'C') &&
-	    (*buffer != 'D'))
-	{
+	if (count > (sizeof(stkbuf) - 1))
+		count = sizeof(stkbuf) - 1;
+	if (copy_from_user(stkbuf, buffer, count))
+		return -EFAULT;
+	stkbuf[count] = 0;
+	if ((*stkbuf != 'A') && (*stkbuf != 'B') &&
+	    (*stkbuf != 'C') && (*stkbuf != 'D')) {
 		printk(KERN_ERR "mf_proc.c: proc_mf_change_side: invalid side\n");
 		return -EINVAL;
 	}
 
-	mf_setSide(*buffer);
+	mf_setSide(*stkbuf);
 
-	return count;			
+	return count;
 }
 
-int proc_mf_dump_src
-(char *page, char **start, off_t off, int count, int *eof, void *data)
+static int proc_mf_dump_src(char *page, char **start, off_t off,
+		int count, int *eof, void *data)
 {
-	int		len = 0;
+	int len;
+
 	mf_getSrcHistory(page, count);
 	len = count;
 	len -= off;			
@@ -254,28 +138,34 @@ int proc_mf_dump_src
 	return len;			
 }
 
-int proc_mf_change_src(struct file *file, const char *buffer, unsigned long count, void *data)
+static int proc_mf_change_src(struct file *file, const char __user *buffer,
+		unsigned long count, void *data)
 {
+	char stkbuf[10];
+
 	if (!capable(CAP_SYS_ADMIN))
 		return -EACCES;
 
-	if ((count < 4) && (count != 1))
-	{
+	if ((count < 4) && (count != 1)) {
 		printk(KERN_ERR "mf_proc: invalid src\n");
 		return -EINVAL;
 	}
 
-	if ((count == 1) && ((*buffer) == '\0'))
-	{
+	if (count > (sizeof(stkbuf) - 1))
+		count = sizeof(stkbuf) - 1;
+	if (copy_from_user(stkbuf, buffer, count))
+		return -EFAULT;
+
+	if ((count == 1) && (*stkbuf == '\0'))
 		mf_clearSrc();
-	} else {
-		mf_displaySrc(*(u32 *)buffer);
-	}
+	else
+		mf_displaySrc(*(u32 *)stkbuf);
 
 	return count;			
 }
 
-int proc_mf_change_cmdline(struct file *file, const char *buffer, unsigned long count, void *data)
+static int proc_mf_change_cmdline(struct file *file, const char *buffer,
+		unsigned long count, void *data)
 {
 	if (!capable(CAP_SYS_ADMIN))
 		return -EACCES;
@@ -285,7 +175,8 @@ int proc_mf_change_cmdline(struct file *
 	return count;			
 }
 
-int proc_mf_change_vmlinux(struct file *file, const char *buffer, unsigned long count, void *data)
+static int proc_mf_change_vmlinux(struct file *file, const char *buffer,
+		unsigned long count, void *data)
 {
 	if (!capable(CAP_SYS_ADMIN))
 		return -EACCES;
@@ -295,3 +186,70 @@ int proc_mf_change_vmlinux(struct file *
 
 	return count;			
 }
+
+void mf_proc_init(struct proc_dir_entry *iSeries_proc)
+{
+	struct proc_dir_entry *ent;
+	struct proc_dir_entry *mf;
+	char name[2];
+	int i;
+
+	mf_proc_root = proc_mkdir("mf", iSeries_proc);
+	if (!mf_proc_root)
+		return;
+
+	name[1] = '\0';
+	for (i = 0; i < 4; i++) {
+		name[0] = 'A' + i;
+		mf = proc_mkdir(name, mf_proc_root);
+		if (!mf)
+			return;
+
+		ent = create_proc_entry("cmdline", S_IFREG|S_IRUSR|S_IWUSR, mf);
+		if (!ent)
+			return;
+		ent->nlink = 1;
+		ent->data = (void *)(long)i;
+		ent->read_proc = proc_mf_dump_cmdline;
+		ent->write_proc = proc_mf_change_cmdline;
+
+		if (i == 3)	/* no vmlinux entry for 'D' */
+			continue;
+
+		ent = create_proc_entry("vmlinux", S_IFREG|S_IWUSR, mf);
+		if (!ent)
+			return;
+		ent->nlink = 1;
+		ent->data = (void *)(long)i;
+#if 0
+		if (i == 3) {
+			/*
+			 * if we had a 'D' vmlinux entry, it would only
+			 * be readable.
+			 */
+			ent->read_proc = proc_mf_dump_vmlinux;
+			ent->write_proc = NULL;
+		} else
+#endif
+		{
+			ent->write_proc = proc_mf_change_vmlinux;
+			ent->read_proc = NULL;
+		}
+	}
+
+	ent = create_proc_entry("side", S_IFREG|S_IRUSR|S_IWUSR, mf_proc_root);
+	if (!ent)
+		return;
+	ent->nlink = 1;
+	ent->data = (void *)0;
+	ent->read_proc = proc_mf_dump_side;
+	ent->write_proc = proc_mf_change_side;
+
+	ent = create_proc_entry("src", S_IFREG|S_IRUSR|S_IWUSR, mf_proc_root);
+	if (!ent)
+		return;
+	ent->nlink = 1;
+	ent->data = (void *)0;
+	ent->read_proc = proc_mf_dump_src;
+	ent->write_proc = proc_mf_change_src;
+}
diff -purN linux-2.5/arch/ppc64/kernel/misc.S linuxppc64-2.5/arch/ppc64/kernel/misc.S
--- linux-2.5/arch/ppc64/kernel/misc.S	2003-10-16 13:54:39.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/misc.S	2003-12-17 04:27:52.000000000 +0000
@@ -66,32 +66,31 @@ _GLOBAL(get_sp)
 	blr
 		
 #ifdef CONFIG_PPC_ISERIES
-/* unsigned long __no_use_save_flags(void) */
-_GLOBAL(__no_use_save_flags)
-#warning FIX ISERIES
-	mfspr	r4,SPRG3
-	lbz	r3,PACAPROCENABLED(r4)
+/* unsigned long local_save_flags(void) */
+_GLOBAL(local_get_flags)
+	lbz	r3,PACAPROCENABLED(r13)
 	blr
 
-/* void __no_use_restore_flags(unsigned long flags) */	
-_GLOBAL(__no_use_restore_flags)
-/*
- * Just set/clear the MSR_EE bit through restore/flags but do not
- * change anything else.  This is needed by the RT system and makes
- * sense anyway.
- *    -- Cort
- */
-#warning FIX ISERIES
-	mfspr	r6,SPRG3
-	lbz	r5,PACAPROCENABLED(r6)
+/* unsigned long local_irq_disable(void) */
+_GLOBAL(local_irq_disable)
+	lbz	r3,PACAPROCENABLED(r13)
+	li	r4,0
+	stb	r4,PACAPROCENABLED(r13)
+	blr			/* Done */
+
+/* void local_irq_restore(unsigned long flags) */	
+_GLOBAL(local_irq_restore)
+	lbz	r5,PACAPROCENABLED(r13)
 	 /* Check if things are setup the way we want _already_. */
 	cmpw	0,r3,r5
 	beqlr
 	/* are we enabling interrupts? */
 	cmpi	0,r3,0
-	stb	r3,PACAPROCENABLED(r6)
+	stb	r3,PACAPROCENABLED(r13)
 	beqlr
 	/* Check pending interrupts */
+	/*   A decrementer, IPI or PMC interrupt may have occurred
+	 *   while we were in the hypervisor (which enables) */
 	CHECKANYINT(r4,r5)
 	beqlr
 
@@ -101,35 +100,8 @@ _GLOBAL(__no_use_restore_flags)
 	li	r0,0x5555
 	sc
 	blr
+#endif /* CONFIG_PPC_ISERIES */
 
-_GLOBAL(__no_use_cli)
-#warning FIX ISERIES
-	mfspr	r5,SPRG3
-	lbz	r3,PACAPROCENABLED(r5)
-	li	r4,0
-	stb	r4,PACAPROCENABLED(r5)
-	blr			/* Done */
-
-_GLOBAL(__no_use_sti)
-#warning FIX ISERIES
-	mfspr	r6,SPRG3
-	li	r3,1
-	stb	r3,PACAPROCENABLED(r6)
-
-	/* Check for pending interrupts
-	 *   A decrementer, IPI or PMC interrupt may have occurred
-	 *   while we were in the hypervisor (which enables)
-	 */
-	CHECKANYINT(r4,r5)
-	beqlr
-
-	/* 
-	 * Handle pending interrupts in interrupt context
-	 */
-	li	r0,0x5555
-	sc	
-	blr
-#endif
 /*
  * Flush instruction cache.
  */
@@ -446,7 +418,7 @@ _GLOBAL(cvt_df)
 	blr
 
 /*
- * identify_cpu,
+ * identify_cpu and calls setup_cpu
  * In:	r3 = base of the cpu_specs array
  *	r4 = address of cur_cpu_spec
  *	r5 = relocation offset
@@ -462,9 +434,17 @@ _GLOBAL(identify_cpu)
 	addi	r3,r3,CPU_SPEC_ENTRY_SIZE
 	b	1b
 1:
-	add	r3,r3,r5
-	std	r3,0(r4)
-	blr
+	add	r0,r3,r5
+	std	r0,0(r4)
+	ld	r4,CPU_SPEC_SETUP(r3)
+	sub	r4,r4,r5
+	ld	r4,0(r4)
+	sub	r4,r4,r5
+	mtctr	r4
+	/* Calling convention for cpu setup is r3=offset, r4=cur_cpu_spec */
+	mr	r4,r3
+	mr	r3,r5
+	bctr
 
 /*
  * do_cpu_ftr_fixups - goes through the list of CPU feature fixups
@@ -514,25 +494,6 @@ _GLOBAL(do_cpu_ftr_fixups)
 	isync
 	b	1b
 
-/*
- * call_setup_cpu - call the setup_cpu function for this cpu
- * r3 = data offset
- *
- * Setup function is called with:
- *   r3 = data offset
- *   r4 = ptr to CPU spec (relocated)
- */
-_GLOBAL(call_setup_cpu)
-	LOADADDR(r4, cur_cpu_spec)
-	sub	r4,r4,r3
-	lwz	r4,0(r4)		# load pointer to cpu_spec
-	sub	r4,r4,r3		# relocate
-	lwz	r6,CPU_SPEC_SETUP(r4)	# load function pointer
-	sub	r6,r6,r3
-	mtctr	r6
-	bctr
-
-
 
 /*
  * Create a kernel thread
@@ -595,6 +556,10 @@ SYSCALL(dup)
 SYSCALL(execve)
 SYSCALL(waitpid)
 
+#ifdef CONFIG_PPC_ISERIES	/* hack hack hack */
+#define ppc_rtas	sys_ni_syscall
+#endif
+
 /* Why isn't this a) automatic, b) written in 'C'? */	
 	.balign 8
 _GLOBAL(sys_call_table32)
@@ -838,20 +803,22 @@ _GLOBAL(sys_call_table32)
 	.llong .sys_epoll_ctl
 	.llong .sys_epoll_wait
 	.llong .sys_remap_file_pages
-	.llong .sys_ni_syscall		/* 240 */
-	.llong .sys_ni_syscall
-	.llong .sys_ni_syscall
-	.llong .sys_ni_syscall
-	.llong .sys_ni_syscall
-	.llong .sys_ni_syscall		/* 245 */
-	.llong .sys_ni_syscall
-	.llong .sys_ni_syscall
-	.llong .sys_ni_syscall
-	.llong .sys_ni_syscall
+	.llong .ppc32_timer_create	/* 240 */
+	.llong .compat_timer_settime
+	.llong .compat_timer_gettime
+	.llong .sys_timer_getoverrun
+	.llong .sys_timer_delete
+	.llong .compat_clock_settime	/* 245 */
+	.llong .compat_clock_gettime
+	.llong .compat_clock_getres
+	.llong .compat_clock_nanosleep
+	.llong .ppc32_swapcontext
 	.llong .sys32_tgkill		/* 250 */
 	.llong .sys32_utimes
-	.llong .sys_statfs64
-	.llong .sys_fstatfs64
+	.llong .compat_statfs64
+	.llong .compat_fstatfs64
+	.llong .ppc32_fadvise64_64	/* 32bit only fadvise64_64 */
+	.llong .ppc_rtas		/* 255 */
 
 	.balign 8
 _GLOBAL(sys_call_table)
@@ -1104,8 +1071,10 @@ _GLOBAL(sys_call_table)
 	.llong .sys_clock_gettime
 	.llong .sys_clock_getres
 	.llong .sys_clock_nanosleep
-	.llong .sys_ni_syscall
+	.llong .ppc64_swapcontext
 	.llong .sys_tgkill		/* 250 */
 	.llong .sys_utimes
 	.llong .sys_statfs64
 	.llong .sys_fstatfs64
+	.llong .sys_ni_syscall		/* 32bit only fadvise64_64 */
+	.llong .ppc_rtas		/* 255 */
diff -purN linux-2.5/arch/ppc64/kernel/nvram.c linuxppc64-2.5/arch/ppc64/kernel/nvram.c
--- linux-2.5/arch/ppc64/kernel/nvram.c	2002-11-08 08:42:53.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/nvram.c	2003-11-21 04:51:10.000000000 +0000
@@ -20,23 +20,50 @@
 #include <linux/fcntl.h>
 #include <linux/nvram.h>
 #include <linux/init.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
 #include <asm/uaccess.h>
 #include <asm/nvram.h>
 #include <asm/rtas.h>
 #include <asm/prom.h>
+#include <asm/machdep.h>
 
-static unsigned int rtas_nvram_size;
+#define DEBUG_NVRAM
+
+static int nvram_scan_partitions(void);
+static int nvram_setup_partition(void);
+static int nvram_create_os_partition(void);
+static int nvram_remove_os_partition(void);
+static unsigned char nvram_checksum(struct nvram_header *p);
+static int nvram_write_header(struct nvram_partition * part);
+
+static unsigned int nvram_size;
 static unsigned int nvram_fetch, nvram_store;
-static char nvram_buf[4];	/* assume this is in the first 4GB */
+static char nvram_buf[NVRW_CNT];	/* assume this is in the first 4GB */
+static struct nvram_partition * nvram_part;
+static long nvram_error_log_index = -1;
+static long nvram_error_log_size = 0;
+static spinlock_t nvram_lock = SPIN_LOCK_UNLOCKED;
+
+volatile int no_more_logging = 1; /* Until we initialize everything,
+				   * make sure we don't try logging
+				   * anything */
+
+extern volatile int error_log_cnt;
+
+struct err_log_info {
+	int error_type;
+	unsigned int seq_num;
+};
 
-static loff_t nvram_llseek(struct file *file, loff_t offset, int origin)
+static loff_t dev_nvram_llseek(struct file *file, loff_t offset, int origin)
 {
 	switch (origin) {
 	case 1:
 		offset += file->f_pos;
 		break;
 	case 2:
-		offset += rtas_nvram_size;
+		offset += nvram_size;
 		break;
 	}
 	if (offset < 0)
@@ -46,53 +73,76 @@ static loff_t nvram_llseek(struct file *
 }
 
 
-static ssize_t read_nvram(struct file *file, char *buf,
+static ssize_t dev_nvram_read(struct file *file, char *buf,
 			  size_t count, loff_t *ppos)
 {
-	unsigned int i;
-	unsigned long len;
-	char *p = buf;
+	ssize_t len;
+	char *tmp_buffer;
 
 	if (verify_area(VERIFY_WRITE, buf, count))
 		return -EFAULT;
-	if (*ppos >= rtas_nvram_size)
+	if (*ppos >= nvram_size)
 		return 0;
-	for (i = *ppos; count > 0 && i < rtas_nvram_size; ++i, ++p, --count) {
-		if ((rtas_call(nvram_fetch, 3, 2, &len, i, __pa(nvram_buf), 1) != 0) ||
-		    len != 1)
-			return -EIO;
-		if (__put_user(nvram_buf[0], p))
-			return -EFAULT;
+	if (count > nvram_size) 
+		count = nvram_size;
+
+	tmp_buffer = (char *) kmalloc(count, GFP_KERNEL);
+	if (!tmp_buffer) {
+		printk(KERN_ERR "dev_read_nvram: kmalloc failed\n");
+		return -ENOMEM;
 	}
-	*ppos = i;
-	return p - buf;
+
+	len = ppc_md.nvram_read(tmp_buffer, count, ppos);
+	if ((long)len <= 0) {
+		kfree(tmp_buffer);
+		return len;
+	}
+
+	if (copy_to_user(buf, tmp_buffer, len)) {
+		kfree(tmp_buffer);
+		return -EFAULT;
+	}
+
+	kfree(tmp_buffer);
+	return len;
+
 }
 
-static ssize_t write_nvram(struct file *file, const char *buf,
+static ssize_t dev_nvram_write(struct file *file, const char *buf,
 			   size_t count, loff_t *ppos)
 {
-	unsigned int i;
-	unsigned long len;
-	const char *p = buf;
-	char c;
+	ssize_t len;
+	char * tmp_buffer;
 
 	if (verify_area(VERIFY_READ, buf, count))
 		return -EFAULT;
-	if (*ppos >= rtas_nvram_size)
+	if (*ppos >= nvram_size)
 		return 0;
-	for (i = *ppos; count > 0 && i < rtas_nvram_size; ++i, ++p, --count) {
-		if (__get_user(c, p))
-			return -EFAULT;
-		nvram_buf[0] = c;
-		if ((rtas_call(nvram_store, 3, 2, &len, i, __pa(nvram_buf), 1) != 0) ||
-		    len != 1)
-			return -EIO;
+	if (count > nvram_size)
+		count = nvram_size;
+
+	tmp_buffer = (char *) kmalloc(count, GFP_KERNEL);
+	if (!tmp_buffer) {
+		printk(KERN_ERR "dev_nvram_write: kmalloc failed\n");
+		return -ENOMEM;
 	}
-	*ppos = i;
-	return p - buf;
+	
+	if (copy_from_user(tmp_buffer, buf, count)) {
+		kfree(tmp_buffer);
+		return -EFAULT;
+	}
+
+	len = ppc_md.nvram_write(tmp_buffer, count, ppos);
+	if ((long)len <= 0) {
+		kfree(tmp_buffer);
+		return len;
+	}
+
+	kfree(tmp_buffer);
+	return len;
 }
 
-static int nvram_ioctl(struct inode *inode, struct file *file,
+static int dev_nvram_ioctl(struct inode *inode, struct file *file,
 	unsigned int cmd, unsigned long arg)
 {
 	return -EINVAL;
@@ -100,10 +150,10 @@ static int nvram_ioctl(struct inode *ino
 
 struct file_operations nvram_fops = {
 	.owner =	THIS_MODULE,
-	.llseek =	nvram_llseek,
-	.read =		read_nvram,
-	.write =	write_nvram,
-	.ioctl =	nvram_ioctl,
+	.llseek =	dev_nvram_llseek,
+	.read =		dev_nvram_read,
+	.write =	dev_nvram_write,
+	.ioctl =	dev_nvram_ioctl,
 };
 
 static struct miscdevice nvram_dev = {
@@ -112,21 +162,135 @@ static struct miscdevice nvram_dev = {
 	&nvram_fops
 };
 
+ssize_t pSeries_nvram_read(char *buf, size_t count, loff_t *index)
+{
+	unsigned int i;
+	unsigned long len, done;
+	unsigned long flags;
+	char *p = buf;
+
+	if (*index >= nvram_size)
+		return 0;
+
+	i = *index;
+	if (i + count > nvram_size)
+		count = nvram_size - i;
+
+	spin_lock_irqsave(&nvram_lock, flags);
+
+	for (; count != 0; count -= len) {
+		len = count;
+		if (len > NVRW_CNT)
+			len = NVRW_CNT;
+		
+		if ((rtas_call(nvram_fetch, 3, 2, &done, i, __pa(nvram_buf),
+			       len) != 0) || len != done) {
+			spin_unlock_irqrestore(&nvram_lock, flags);
+			return -EIO;
+		}
+		
+		memcpy(p, nvram_buf, len);
+
+		p += len;
+		i += len;
+	}
+
+	spin_unlock_irqrestore(&nvram_lock, flags);
+	
+	*index = i;
+	return p - buf;
+}
+
+ssize_t pSeries_nvram_write(char *buf, size_t count, loff_t *index)
+{
+	unsigned int i;
+	unsigned long len, done;
+	unsigned long flags;
+	const char *p = buf;
+
+	if (*index >= nvram_size)
+		return 0;
+
+	i = *index;
+	if (i + count > nvram_size)
+		count = nvram_size - i;
+
+	spin_lock_irqsave(&nvram_lock, flags);
+
+	for (; count != 0; count -= len) {
+		len = count;
+		if (len > NVRW_CNT)
+			len = NVRW_CNT;
+
+		memcpy(nvram_buf, p, len);
+
+		if ((rtas_call(nvram_store, 3, 2, &done, i, __pa(nvram_buf),
+			       len) != 0) || len != done) {
+			spin_unlock_irqrestore(&nvram_lock, flags);
+			return -EIO;
+		}
+		
+		p += len;
+		i += len;
+	}
+	spin_unlock_irqrestore(&nvram_lock, flags);
+	
+	*index = i;
+	return p - buf;
+}
+ 
 int __init nvram_init(void)
 {
 	struct device_node *nvram;
 	unsigned int *nbytes_p, proplen;
-	if ((nvram = find_type_devices("nvram")) != NULL) {
+	int error;
+	int rc;
+	
+	if ((nvram = of_find_node_by_type(NULL, "nvram")) != NULL) {
 		nbytes_p = (unsigned int *)get_property(nvram, "#bytes", &proplen);
 		if (nbytes_p && proplen == sizeof(unsigned int)) {
-			rtas_nvram_size = *nbytes_p;
+			nvram_size = *nbytes_p;
+		} else {
+			return -EIO;
 		}
 	}
 	nvram_fetch = rtas_token("nvram-fetch");
 	nvram_store = rtas_token("nvram-store");
-	printk(KERN_INFO "PPC64 nvram contains %d bytes\n", rtas_nvram_size);
+	printk(KERN_INFO "PPC64 nvram contains %d bytes\n", nvram_size);
+	of_node_put(nvram);
+
+  	rc = misc_register(&nvram_dev);
+  
+  	/* If we don't know how big NVRAM is then we shouldn't touch
+  	   the nvram partitions */
+  	if (nvram == NULL) {
+  		return rc;
+  	}
+  	
+  	/* initialize our anchor for the nvram partition list */
+  	nvram_part = (struct nvram_partition *) kmalloc(sizeof(struct nvram_partition), GFP_KERNEL);
+  	if (!nvram_part) {
+  		printk(KERN_ERR "nvram_init: Failed kmalloc\n");
+  		return -ENOMEM;
+  	}
+  	INIT_LIST_HEAD(&nvram_part->partition);
+  
+  	/* Get all the NVRAM partitions */
+  	error = nvram_scan_partitions();
+  	if (error) {
+  		printk(KERN_ERR "nvram_init: Failed nvram_scan_partitions\n");
+  		return error;
+  	}
+  		
+  	if(nvram_setup_partition()) 
+  		printk(KERN_WARNING "nvram_init: Could not find nvram partition"
+  		       " for nvram buffered error logging.\n");
+  
+#ifdef DEBUG_NVRAM
+	nvram_print_partitions("NVRAM Partitions");
+#endif
 
-	return misc_register(&nvram_dev);
+  	return rc;
 }
 
 void __exit nvram_cleanup(void)
@@ -134,6 +298,444 @@ void __exit nvram_cleanup(void)
         misc_deregister( &nvram_dev );
 }
 
+static int nvram_scan_partitions(void)
+{
+	loff_t cur_index = 0;
+	struct nvram_header phead;
+	struct nvram_partition * tmp_part;
+	unsigned char c_sum;
+	char * header;
+	long size;
+	
+	header = (char *) kmalloc(NVRAM_HEADER_LEN, GFP_KERNEL);
+	if (!header) {
+		printk(KERN_ERR "nvram_scan_partitions: Failed kmalloc\n");
+		return -ENOMEM;
+	}
+
+	while (cur_index < nvram_size) {
+
+		size = ppc_md.nvram_read(header, NVRAM_HEADER_LEN, &cur_index);
+		if (size != NVRAM_HEADER_LEN) {
+			printk(KERN_ERR "nvram_scan_partitions: Error parsing "
+			       "nvram partitions\n");
+			kfree(header);
+			return size;
+		}
+
+		cur_index -= NVRAM_HEADER_LEN; /* nvram_read will advance us */
+
+		memcpy(&phead, header, NVRAM_HEADER_LEN);
+
+		c_sum = nvram_checksum(&phead);
+		if (c_sum != phead.checksum)
+			printk(KERN_WARNING "WARNING: nvram partition checksum "
+			       "was %02x, should be %02x!\n", phead.checksum, c_sum);
+		
+		tmp_part = (struct nvram_partition *)
+			kmalloc(sizeof(struct nvram_partition), GFP_KERNEL);
+		if (!tmp_part) {
+			printk(KERN_ERR "nvram_scan_partitions: kmalloc failed\n");
+			kfree(header);
+			return -ENOMEM;
+		}
+		
+		memcpy(&tmp_part->header, &phead, NVRAM_HEADER_LEN);
+		tmp_part->index = cur_index;
+		list_add_tail(&tmp_part->partition, &nvram_part->partition);
+		
+		cur_index += phead.length * NVRAM_BLOCK_LEN;
+	}
+
+	kfree(header);
+	return 0;
+}
+
+/* nvram_setup_partition
+ *
+ * This will setup the partition we need for buffering the
+ * error logs and cleanup partitions if needed.
+ *
+ * The general strategy is the following:
+ * 1.) If there is ppc64,linux partition large enough then use it.
+ * 2.) If there is not a ppc64,linux partition large enough, search
+ * for a free partition that is large enough.
+ * 3.) If there is not a free partition large enough remove 
+ * _all_ OS partitions and consolidate the space.
+ * 4.) Will first try getting a chunk that will satisfy the maximum
+ * error log size (NVRAM_MAX_REQ).
+ * 5.) If the max chunk cannot be allocated then try finding a chunk
+ * that will satisfy the minum needed (NVRAM_MIN_REQ).
+ */
+static int nvram_setup_partition(void)
+{
+	struct list_head * p;
+	struct nvram_partition * part;
+	int rc;
+
+	/* see if we have an OS partition that meets our needs.
+	   will try getting the max we need.  If not we'll delete
+	   partitions and try again. */
+	list_for_each(p, &nvram_part->partition) {
+		part = list_entry(p, struct nvram_partition, partition);
+		if (part->header.signature != NVRAM_SIG_OS)
+			continue;
+
+		if (strcmp(part->header.name, "ppc64,linux"))
+			continue;
+
+		if (part->header.length >= NVRAM_MIN_REQ) {
+			/* found our partition */
+			nvram_error_log_index = part->index + NVRAM_HEADER_LEN;
+			nvram_error_log_size = ((part->header.length - 1) *
+						NVRAM_BLOCK_LEN) - sizeof(struct err_log_info);
+			return 0;
+		}
+	}
+	
+	/* try creating a partition with the free space we have */
+	rc = nvram_create_os_partition();
+	if (!rc) {
+		return 0;
+	}
+		
+	/* need to free up some space */
+	rc = nvram_remove_os_partition();
+	if (rc) {
+		return rc;
+	}
+	
+	/* create a partition in this new space */
+	rc = nvram_create_os_partition();
+	if (rc) {
+		printk(KERN_ERR "nvram_create_os_partition: Could not find a "
+		       "NVRAM partition large enough\n");
+		return rc;
+	}
+	
+	return 0;
+}
+
+static int nvram_remove_os_partition(void)
+{
+	struct list_head *i;
+	struct list_head *j;
+	struct nvram_partition * part;
+	struct nvram_partition * cur_part;
+	int rc;
+
+	list_for_each(i, &nvram_part->partition) {
+		part = list_entry(i, struct nvram_partition, partition);
+		if (part->header.signature != NVRAM_SIG_OS)
+			continue;
+		
+		/* Make os partition a free partition */
+		part->header.signature = NVRAM_SIG_FREE;
+		sprintf(part->header.name, "wwwwwwwwwwww");
+		part->header.checksum = nvram_checksum(&part->header);
+
+		/* Merge contiguous free partitions backwards */
+		list_for_each_prev(j, &part->partition) {
+			cur_part = list_entry(j, struct nvram_partition, partition);
+			if (cur_part == nvram_part || cur_part->header.signature != NVRAM_SIG_FREE) {
+				break;
+			}
+			
+			part->header.length += cur_part->header.length;
+			part->header.checksum = nvram_checksum(&part->header);
+			part->index = cur_part->index;
+
+			list_del(&cur_part->partition);
+			kfree(cur_part);
+			j = &part->partition; /* fixup our loop */
+		}
+		
+		/* Merge contiguous free partitions forwards */
+		list_for_each(j, &part->partition) {
+			cur_part = list_entry(j, struct nvram_partition, partition);
+			if (cur_part == nvram_part || cur_part->header.signature != NVRAM_SIG_FREE) {
+				break;
+			}
+
+			part->header.length += cur_part->header.length;
+			part->header.checksum = nvram_checksum(&part->header);
+
+			list_del(&cur_part->partition);
+			kfree(cur_part);
+			j = &part->partition; /* fixup our loop */
+		}
+		
+		rc = nvram_write_header(part);
+		if (rc <= 0) {
+			printk(KERN_ERR "nvram_remove_os_partition: nvram_write failed (%d)\n", rc);
+			return rc;
+		}
+
+	}
+	
+	return 0;
+}
+
+/* nvram_create_os_partition
+ *
+ * Create a OS linux partition to buffer error logs.
+ * Will create a partition starting at the first free
+ * space found if space has enough room.
+ */
+static int nvram_create_os_partition(void)
+{
+	struct list_head * p;
+	struct nvram_partition * part;
+	struct nvram_partition * new_part = NULL;
+	struct nvram_partition * free_part;
+	int seq_init[2] = { 0, 0 };
+	loff_t tmp_index;
+	long size = 0;
+	int rc;
+	
+	/* Find a free partition that will give us the maximum needed size 
+	   If can't find one that will give us the minimum size needed */
+	list_for_each(p, &nvram_part->partition) {
+		part = list_entry(p, struct nvram_partition, partition);
+		if (part->header.signature != NVRAM_SIG_FREE)
+			continue;
+
+		if (part->header.length >= NVRAM_MAX_REQ) {
+			size = NVRAM_MAX_REQ;
+			free_part = part;
+			break;
+		}
+		if (!size && part->header.length >= NVRAM_MIN_REQ) {
+			size = NVRAM_MIN_REQ;
+			free_part = part;
+		}
+	}
+	if (!size) {
+		return -ENOSPC;
+	}
+	
+	/* Create our OS partition */
+	new_part = (struct nvram_partition *)
+		kmalloc(sizeof(struct nvram_partition), GFP_KERNEL);
+	if (!new_part) {
+		printk(KERN_ERR "nvram_create_os_partition: kmalloc failed\n");
+		return -ENOMEM;
+	}
+
+	new_part->index = free_part->index;
+	new_part->header.signature = NVRAM_SIG_OS;
+	new_part->header.length = size;
+	sprintf(new_part->header.name, "ppc64,linux");
+	new_part->header.checksum = nvram_checksum(&new_part->header);
+
+	rc = nvram_write_header(new_part);
+	if (rc <= 0) {
+		printk(KERN_ERR "nvram_create_os_partition: nvram_write_header \
+				failed (%d)\n", rc);
+		return rc;
+	}
+
+	/* make sure and initialize to zero the sequence number and the error
+	   type logged */
+	tmp_index = new_part->index + NVRAM_HEADER_LEN;
+	rc = ppc_md.nvram_write((char *)&seq_init, sizeof(seq_init), &tmp_index);
+	if (rc <= 0) {
+		printk(KERN_ERR "nvram_create_os_partition: nvram_write failed (%d)\n", rc);
+		return rc;
+	}
+	
+	nvram_error_log_index = new_part->index + NVRAM_HEADER_LEN;
+	nvram_error_log_size = ((part->header.length - 1) *
+				NVRAM_BLOCK_LEN) - sizeof(struct err_log_info);
+	
+	list_add_tail(&new_part->partition, &free_part->partition);
+
+	if (free_part->header.length <= size) {
+		list_del(&free_part->partition);
+		kfree(free_part);
+		return 0;
+	} 
+
+	/* Adjust the partition we stole the space from */
+	free_part->index += size * NVRAM_BLOCK_LEN;
+	free_part->header.length -= size;
+	free_part->header.checksum = nvram_checksum(&free_part->header);
+	
+	rc = nvram_write_header(free_part);
+	if (rc <= 0) {
+		printk(KERN_ERR "nvram_create_os_partition: nvram_write_header "
+		       "failed (%d)\n", rc);
+		return rc;
+	}
+
+	return 0;
+}
+
+
+void nvram_print_partitions(char * label)
+{
+	struct list_head * p;
+	struct nvram_partition * tmp_part;
+	
+	printk(KERN_WARNING "--------%s---------\n", label);
+	printk(KERN_WARNING "indx\t\tsig\tchks\tlen\tname\n");
+	list_for_each(p, &nvram_part->partition) {
+		tmp_part = list_entry(p, struct nvram_partition, partition);
+		printk(KERN_WARNING "%d    \t%02x\t%02x\t%d\t%s\n",
+		       tmp_part->index, tmp_part->header.signature,
+		       tmp_part->header.checksum, tmp_part->header.length,
+		       tmp_part->header.name);
+	}
+}
+
+/* nvram_write_error_log
+ *
+ * We need to buffer the error logs into nvram to ensure that we have
+ * the failure information to decode.  If we have a severe error there
+ * is no way to guarantee that the OS or the machine is in a state to
+ * get back to user land and write the error to disk.  For example if
+ * the SCSI device driver causes a Machine Check by writing to a bad
+ * IO address, there is no way of guaranteeing that the device driver
+ * is in any state that is would also be able to write the error data
+ * captured to disk, thus we buffer it in NVRAM for analysis on the
+ * next boot.
+ *
+ * In NVRAM the partition containing the error log buffer will looks like:
+ * Header (in bytes):
+ * +-----------+----------+--------+------------+------------------+
+ * | signature | checksum | length | name       | data             |
+ * |0          |1         |2      3|4         15|16        length-1|
+ * +-----------+----------+--------+------------+------------------+
+ *
+ * The 'data' section would look like (in bytes):
+ * +--------------+------------+-----------------------------------+
+ * | event_logged | sequence # | error log                         |
+ * |0            3|4          7|8            nvram_error_log_size-1|
+ * +--------------+------------+-----------------------------------+
+ *
+ * event_logged: 0 if event has not been logged to syslog, 1 if it has
+ * sequence #: The unique sequence # for each event. (until it wraps)
+ * error log: The error log from event_scan
+ */
+int nvram_write_error_log(char * buff, int length, unsigned int err_type)
+{
+	int rc;
+	loff_t tmp_index;
+	struct err_log_info info;
+	
+	if (no_more_logging) {
+		return -EPERM;
+	}
+
+	if (nvram_error_log_index == -1) {
+		return -ESPIPE;
+	}
+
+	if (length > nvram_error_log_size) {
+		length = nvram_error_log_size;
+	}
+
+	info.error_type = err_type;
+	info.seq_num = error_log_cnt;
+
+	tmp_index = nvram_error_log_index;
+
+	rc = ppc_md.nvram_write((char *)&info, sizeof(struct err_log_info), &tmp_index);
+	if (rc <= 0) {
+		printk(KERN_ERR "nvram_write_error_log: Failed nvram_write (%d)\n", rc);
+		return rc;
+	}
+
+	rc = ppc_md.nvram_write(buff, length, &tmp_index);
+	if (rc <= 0) {
+		printk(KERN_ERR "nvram_write_error_log: Failed nvram_write (%d)\n", rc);
+		return rc;
+	}
+	
+	return 0;
+}
+
+/* nvram_read_error_log
+ *
+ * Reads nvram for error log for at most 'length'
+ */
+int nvram_read_error_log(char * buff, int length, unsigned int * err_type)
+{
+	int rc;
+	loff_t tmp_index;
+	struct err_log_info info;
+	
+	if (nvram_error_log_index == -1)
+		return -1;
+
+	if (length > nvram_error_log_size)
+		length = nvram_error_log_size;
+
+	tmp_index = nvram_error_log_index;
+
+	rc = ppc_md.nvram_read((char *)&info, sizeof(struct err_log_info), &tmp_index);
+	if (rc <= 0) {
+		printk(KERN_ERR "nvram_read_error_log: Failed nvram_read (%d)\n", rc);
+		return rc;
+	}
+
+	rc = ppc_md.nvram_read(buff, length, &tmp_index);
+	if (rc <= 0) {
+		printk(KERN_ERR "nvram_read_error_log: Failed nvram_read (%d)\n", rc);
+		return rc;
+	}
+
+	error_log_cnt = info.seq_num;
+	*err_type = info.error_type;
+
+	return 0;
+}
+
+/* This doesn't actually zero anything, but it sets the event_logged
+ * word to tell that this event is safely in syslog.
+ */
+int nvram_clear_error_log()
+{
+	loff_t tmp_index;
+	int clear_word = ERR_FLAG_ALREADY_LOGGED;
+	int rc;
+
+	tmp_index = nvram_error_log_index;
+	
+	rc = ppc_md.nvram_write((char *)&clear_word, sizeof(int), &tmp_index);
+	if (rc <= 0) {
+		printk(KERN_ERR "nvram_clear_error_log: Failed nvram_write (%d)\n", rc);
+		return rc;
+	}
+
+	return 0;
+}
+
+static int nvram_write_header(struct nvram_partition * part)
+{
+	loff_t tmp_index;
+	int rc;
+	
+	tmp_index = part->index;
+	rc = ppc_md.nvram_write((char *)&part->header, NVRAM_HEADER_LEN, &tmp_index); 
+
+	return rc;
+}
+
+static unsigned char nvram_checksum(struct nvram_header *p)
+{
+	unsigned int c_sum, c_sum2;
+	unsigned short *sp = (unsigned short *)p->name; /* assume 6 shorts */
+	c_sum = p->signature + p->length + sp[0] + sp[1] + sp[2] + sp[3] + sp[4] + sp[5];
+
+	/* The sum may have spilled into the 3rd byte.  Fold it back. */
+	c_sum = ((c_sum & 0xffff) + (c_sum >> 16)) & 0xffff;
+	/* The sum cannot exceed 2 bytes.  Fold it into a checksum */
+	c_sum2 = (c_sum >> 8) + (c_sum << 8);
+	c_sum = ((c_sum + c_sum2) >> 8) & 0xff;
+	return c_sum;
+}
+
 module_init(nvram_init);
 module_exit(nvram_cleanup);
 MODULE_LICENSE("GPL");
diff -purN linux-2.5/arch/ppc64/kernel/open_pic.c linuxppc64-2.5/arch/ppc64/kernel/open_pic.c
--- linux-2.5/arch/ppc64/kernel/open_pic.c	2003-08-19 02:46:23.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/open_pic.c	2003-11-07 22:47:21.000000000 +0000
@@ -85,10 +85,10 @@ unsigned int openpic_vec_spurious;
  */
 #ifdef CONFIG_SMP
 #define THIS_CPU		Processor[cpu]
-#define DECL_THIS_CPU		int cpu = smp_processor_id()
+#define DECL_THIS_CPU		int cpu = hard_smp_processor_id()
 #define CHECK_THIS_CPU		check_arg_cpu(cpu)
 #else
-#define THIS_CPU		Processor[smp_processor_id()]
+#define THIS_CPU		Processor[hard_smp_processor_id()]
 #define DECL_THIS_CPU
 #define CHECK_THIS_CPU
 #endif /* CONFIG_SMP */
@@ -130,19 +130,29 @@ unsigned int openpic_vec_spurious;
 
 #define GET_ISU(source)	ISU[(source) >> 4][(source) & 0xf]
 
+void
+openpic_init_irq_desc(irq_desc_t *desc)
+{
+	/* Don't mess with the handler if already set.
+	 * This leaves the setup of isa/ipi handlers undisturbed.
+	 */
+	if (!desc->handler)
+		desc->handler = &open_pic;
+}
+
 void __init openpic_init_IRQ(void)
 {
         struct device_node *np;
         int i;
         unsigned int *addrp;
         unsigned char* chrp_int_ack_special = 0;
-        unsigned char init_senses[NR_IRQS - NUM_8259_INTERRUPTS];
+        unsigned char init_senses[NR_IRQS - NUM_ISA_INTERRUPTS];
         int nmi_irq = -1;
 #if defined(CONFIG_VT) && defined(CONFIG_ADB_KEYBOARD) && defined(XMON)
         struct device_node *kbd;
 #endif
 
-        if (!(np = find_devices("pci"))
+        if (!(np = of_find_node_by_name(NULL, "pci"))
             || !(addrp = (unsigned int *)
                  get_property(np, "8259-interrupt-acknowledge", NULL)))
                 printk(KERN_ERR "Cannot find pci to get ack address\n");
@@ -151,13 +161,14 @@ void __init openpic_init_IRQ(void)
 			__ioremap(addrp[prom_n_addr_cells(np)-1], 1, _PAGE_NO_CACHE);
         /* hydra still sets OpenPIC_InitSenses to a static set of values */
         if (OpenPIC_InitSenses == NULL) {
-                prom_get_irq_senses(init_senses, NUM_8259_INTERRUPTS, NR_IRQS);
+                prom_get_irq_senses(init_senses, NUM_ISA_INTERRUPTS, NR_IRQS);
                 OpenPIC_InitSenses = init_senses;
-                OpenPIC_NumInitSenses = NR_IRQS - NUM_8259_INTERRUPTS;
+                OpenPIC_NumInitSenses = NR_IRQS - NUM_ISA_INTERRUPTS;
         }
-        openpic_init(1, NUM_8259_INTERRUPTS, chrp_int_ack_special, nmi_irq);
-        for ( i = 0 ; i < NUM_8259_INTERRUPTS  ; i++ )
-                irq_desc[i].handler = &i8259_pic;
+        openpic_init(1, NUM_ISA_INTERRUPTS, chrp_int_ack_special, nmi_irq);
+        for ( i = 0 ; i < NUM_ISA_INTERRUPTS  ; i++ )
+                get_real_irq_desc(i)->handler = &i8259_pic;
+	of_node_put(np);
 }
 
 static inline u_int openpic_read(volatile u_int *addr)
@@ -341,8 +352,8 @@ void __init openpic_init(int main_pic, i
 		/* Disabled, Priority 10..13 */
 		openpic_initipi(i, 10+i, openpic_vec_ipi+i);
 		/* IPIs are per-CPU */
-		irq_desc[openpic_vec_ipi+i].status |= IRQ_PER_CPU;
-		irq_desc[openpic_vec_ipi+i].handler = &open_pic_ipi;
+		get_real_irq_desc(openpic_vec_ipi+i)->status |= IRQ_PER_CPU;
+		get_real_irq_desc(openpic_vec_ipi+i)->handler = &open_pic_ipi;
 	}
 #endif
 
@@ -354,7 +365,7 @@ void __init openpic_init(int main_pic, i
 	/* SIOint (8259 cascade) is special */
 	if (offset) {
 		openpic_initirq(0, 8, offset, 1, 1);
-		openpic_mapirq(0, 1 << boot_cpuid);
+		openpic_mapirq(0, 1 << get_hard_smp_processor_id(boot_cpuid));
 	}
 
 	/* Init all external sources */
@@ -367,18 +378,14 @@ void __init openpic_init(int main_pic, i
 		pri = (i == programmer_switch_irq)? 9: 8;
 		sense = (i < OpenPIC_NumInitSenses)? OpenPIC_InitSenses[i]: 1;
 		if (sense)
-			irq_desc[i+offset].status = IRQ_LEVEL;
+			get_real_irq_desc(i+offset)->status = IRQ_LEVEL;
 
 		/* Enabled, Priority 8 or 9 */
 		openpic_initirq(i, pri, i+offset, !sense, sense);
 		/* Processor 0 */
-		openpic_mapirq(i, 1 << boot_cpuid);
+		openpic_mapirq(i, 1 << get_hard_smp_processor_id(boot_cpuid));
 	}
 
-	/* Init descriptors */
-	for (i = offset; i < NumSources + offset; i++)
-		irq_desc[i].handler = &open_pic;
-
 	/* Initialize the spurious interrupt */
 	ppc64_boot_msg(0x24, "OpenPic Spurious");
 	openpic_set_spurious(openpic_vec_spurious);
@@ -397,7 +404,7 @@ static int __init openpic_setup_i8259(vo
 {
 	if (naca->interrupt_controller == IC_OPEN_PIC) {
 		/* Initialize the cascade */
-		if (request_irq(NUM_8259_INTERRUPTS, no_action, SA_INTERRUPT,
+		if (request_irq(NUM_ISA_INTERRUPTS, no_action, SA_INTERRUPT,
 				"82c59 cascade", NULL))
 			printk(KERN_ERR "Unable to get OpenPIC IRQ 0 for cascade\n");
 		i8259_init();
@@ -513,10 +520,23 @@ static void openpic_set_spurious(u_int v
 			   vec);
 }
 
+/*
+ * Convert a cpu mask from logical to physical cpu numbers.
+ */
+static inline u32 physmask(u32 cpumask)
+{
+	int i;
+	u32 mask = 0;
+
+	for (i = 0; i < NR_CPUS; ++i, cpumask >>= 1)
+		mask |= (cpumask & 1) << get_hard_smp_processor_id(i);
+	return mask;
+}
+
 void openpic_init_processor(u_int cpumask)
 {
 	openpic_write(&OpenPIC->Global.Processor_Initialization,
-		      cpumask & cpus_coerce(cpu_online_map));
+		      physmask(cpumask & cpus_coerce(cpu_online_map)));
 }
 
 #ifdef CONFIG_SMP
@@ -550,7 +570,7 @@ void openpic_cause_IPI(u_int ipi, u_int 
 	CHECK_THIS_CPU;
 	check_arg_ipi(ipi);
 	openpic_write(&OpenPIC->THIS_CPU.IPI_Dispatch(ipi),
-		      cpumask & cpus_coerce(cpu_online_map));
+		      physmask(cpumask & cpus_coerce(cpu_online_map)));
 }
 
 void openpic_request_IPIs(void)
@@ -591,7 +611,7 @@ void __devinit do_openpic_setup_cpu(void
 {
 #ifdef CONFIG_IRQ_ALL_CPUS
  	int i;
-	u32 msk = 1 << smp_processor_id();
+	u32 msk = 1 << hard_smp_processor_id();
 #endif
 
 	spin_lock(&openpic_setup_lock);
@@ -636,7 +656,7 @@ static void __init openpic_maptimer(u_in
 {
 	check_arg_timer(timer);
 	openpic_write(&OpenPIC->Global.Timer[timer].Destination,
-		      cpumask & cpus_coerce(cpu_online_map));
+		      physmask(cpumask & cpus_coerce(cpu_online_map)));
 }
 
 
@@ -753,7 +773,7 @@ static inline void openpic_set_sense(u_i
 
 static void openpic_end_irq(unsigned int irq_nr)
 {
-	if ((irq_desc[irq_nr].status & IRQ_LEVEL) != 0)
+	if ((get_irq_desc(irq_nr)->status & IRQ_LEVEL) != 0)
 		openpic_eoi();
 }
 
@@ -762,7 +782,7 @@ static void openpic_set_affinity(unsigne
 	cpumask_t tmp;
 
 	cpus_and(tmp, cpumask, cpu_online_map);
-	openpic_mapirq(irq_nr - open_pic_irq_offset, cpus_coerce(tmp));
+	openpic_mapirq(irq_nr - open_pic_irq_offset, physmask(cpus_coerce(tmp)));
 }
 
 #ifdef CONFIG_SMP
diff -purN linux-2.5/arch/ppc64/kernel/open_pic.h linuxppc64-2.5/arch/ppc64/kernel/open_pic.h
--- linux-2.5/arch/ppc64/kernel/open_pic.h	2003-08-19 02:46:23.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/open_pic.h	2003-09-12 11:01:38.000000000 +0000
@@ -38,9 +38,4 @@ extern void openpic_init_processor(u_int
 extern void openpic_setup_ISU(int isu_num, unsigned long addr);
 extern void openpic_cause_IPI(u_int ipi, u_int cpumask);
 
-extern inline int openpic_to_irq(int irq)
-{
-	return irq += NUM_8259_INTERRUPTS;
-}
-/*extern int open_pic_irq_offset;*/
 #endif /* _PPC64_KERNEL_OPEN_PIC_H */
diff -purN linux-2.5/arch/ppc64/kernel/pSeries_htab.c linuxppc64-2.5/arch/ppc64/kernel/pSeries_htab.c
--- linux-2.5/arch/ppc64/kernel/pSeries_htab.c	2003-06-07 01:19:27.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/pSeries_htab.c	2003-12-04 23:00:40.000000000 +0000
@@ -219,10 +219,10 @@ static long pSeries_hpte_updatepp(unsign
 
 	/* Ensure it is out of the tlb too */
 	if ((cur_cpu_spec->cpu_features & CPU_FTR_TLBIEL) && !large && local) {
-		_tlbiel(va);
+		tlbiel(va);
 	} else {
 		spin_lock_irqsave(&pSeries_tlbie_lock, flags);
-		_tlbie(va, large);
+		tlbie(va, large);
 		spin_unlock_irqrestore(&pSeries_tlbie_lock, flags);
 	}
 
@@ -256,7 +256,7 @@ static void pSeries_hpte_updateboltedpp(
 
 	/* Ensure it is out of the tlb too */
 	spin_lock_irqsave(&pSeries_tlbie_lock, flags);
-	_tlbie(va, 0);
+	tlbie(va, 0);
 	spin_unlock_irqrestore(&pSeries_tlbie_lock, flags);
 }
 
@@ -285,10 +285,10 @@ static void pSeries_hpte_invalidate(unsi
 
 	/* Invalidate the tlb */
 	if ((cur_cpu_spec->cpu_features & CPU_FTR_TLBIEL) && !large && local) {
-		_tlbiel(va);
+		tlbiel(va);
 	} else {
 		spin_lock_irqsave(&pSeries_tlbie_lock, flags);
-		_tlbie(va, large);
+		tlbie(va, large);
 		spin_unlock_irqrestore(&pSeries_tlbie_lock, flags);
 	}
 }
@@ -350,12 +350,8 @@ static void pSeries_flush_hash_range(uns
 	if ((cur_cpu_spec->cpu_features & CPU_FTR_TLBIEL) && !large && local) {
 		asm volatile("ptesync":::"memory");
 
-		for (i = 0; i < j; i++) {
-			asm volatile("\n\
-			clrldi  %0,%0,16\n\
-			tlbiel   %0"
-			: : "r" (batch->vaddr[i]) : "memory" );
-		}
+		for (i = 0; i < j; i++)
+			__tlbiel(batch->vaddr[i]);
 
 		asm volatile("ptesync":::"memory");
 	} else {
@@ -364,12 +360,8 @@ static void pSeries_flush_hash_range(uns
 
 		asm volatile("ptesync":::"memory");
 
-		for (i = 0; i < j; i++) {
-			asm volatile("\n\
-			clrldi  %0,%0,16\n\
-			tlbie   %0"
-			: : "r" (batch->vaddr[i]) : "memory" );
-		}
+		for (i = 0; i < j; i++)
+			__tlbie(batch->vaddr[i], 0);
 
 		asm volatile("eieio; tlbsync; ptesync":::"memory");
 
@@ -389,10 +381,11 @@ void hpte_init_pSeries(void)
 	ppc_md.hpte_remove     	= pSeries_hpte_remove;
 
 	/* Disable TLB batching on nighthawk */
-	root = find_path_device("/");
+	root = of_find_node_by_path("/");
 	if (root) {
 		model = get_property(root, "model", NULL);
 		if (strcmp(model, "CHRP IBM,9076-N81"))
 			ppc_md.flush_hash_range = pSeries_flush_hash_range;
+		of_node_put(root);
 	}
 }
diff -purN linux-2.5/arch/ppc64/kernel/pSeries_hvCall.S linuxppc64-2.5/arch/ppc64/kernel/pSeries_hvCall.S
--- linux-2.5/arch/ppc64/kernel/pSeries_hvCall.S	2002-09-17 23:32:53.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/pSeries_hvCall.S	2003-12-10 21:43:51.000000000 +0000
@@ -22,7 +22,7 @@
 /*
  * hcall interface to pSeries LPAR
  */
-#define HSC .long 0x44000022
+#define HVSC .long 0x44000022
 
 /* long plpar_hcall(unsigned long opcode,	 R3 
 		 unsigned long arg1,		 R4 
@@ -44,7 +44,7 @@ _GLOBAL(plpar_hcall)
         std     r9,-16(r1)
         std     r10,-24(r1)
 	
-	HSC                     /* invoke the hypervisor */
+	HVSC                    /* invoke the hypervisor */
 
         ld      r10,-8(r1)      /* Fetch r4-r7 ret args. */
         std     r4,0(r10)
@@ -63,7 +63,81 @@ _GLOBAL(plpar_hcall)
 _GLOBAL(plpar_hcall_norets)
 	mfcr	r0
 	std	r0,-8(r1)
-	HSC                     /* invoke the hypervisor */
+	HVSC                    /* invoke the hypervisor */
+	ld	r0,-8(r1)
+	mtcrf	0xff,r0
+	blr                     /* return r3 = status */
+
+
+/* long plpar_hcall_8arg_2ret(unsigned long opcode,		 R3 
+			     unsigned long arg1,		 R4 
+		 	     unsigned long arg2,		 R5 
+			     unsigned long arg3,		 R6 
+	 		     unsigned long arg4,		 R7 
+	 		     unsigned long arg5,		 R8 
+			     unsigned long arg6,		 R9 
+	 		     unsigned long arg7,		 R10 
+	 		     unsigned long arg8,		 112(R1)
+	 		     unsigned long *out1);		 120(R1)
+
+ */
+
+	.text
+_GLOBAL(plpar_hcall_8arg_2ret)
+	mfcr	r0
+
+	ld              r11, 112(r1) /* put arg8 and out1 in R11 and R12 */
+	ld              r12, 120(r1)
+
+	std	r0,-8(r1)
+	stdu	r1,-32(r1)
+
+        std     r12,-8(r1)      /* Save out ptr */
+	
+	HVSC                    /* invoke the hypervisor */
+
+        ld      r10,-8(r1)      /* Fetch r4 ret arg */
+        std     r4,0(r10)
+
+	ld	r1,0(r1)
+	ld	r0,-8(r1)
+	mtcrf	0xff,r0
+	blr                     /* return r3 = status */
+
+
+/* long plpar_hcall_4out(unsigned long opcode,	 R3 
+		 unsigned long arg1,		 R4 
+		 unsigned long arg2,		 R5 
+		 unsigned long arg3,		 R6 
+		 unsigned long arg4,		 R7 
+		 unsigned long *out1,	(r4)	 R8 
+		 unsigned long *out2,	(r5)	 R9
+		 unsigned long *out3,   (r6)     R10
+		 unsigned long *out4);	(r7)	 112(R1). From Parameter save area. 
+ */
+_GLOBAL(plpar_hcall_4out)
+	mfcr	r0
+	std	r0,-8(r1)
+	ld      r14,112(r1)
+	stdu	r1,-48(r1) 
+
+	std     r8,32(r1)       /* Save out ptrs. */
+	std     r9,24(r1)
+	std     r10,16(r1)
+	std     r14,8(r1)
+	
+	HVSC                    /* invoke the hypervisor */
+
+	ld      r14,32(r1)      /* Fetch r4-r7 ret args. */
+	std     r4,0(r14)
+	ld      r14,24(r1)
+	std     r5,0(r14)
+	ld      r14,16(r1)
+	std     r6,0(r14)
+	ld      r14,8(r1)
+	std     r7,0(r14)
+
+	ld	r1,0(r1) 
 	ld	r0,-8(r1)
 	mtcrf	0xff,r0
 	blr                     /* return r3 = status */
diff -purN linux-2.5/arch/ppc64/kernel/pSeries_lpar.c linuxppc64-2.5/arch/ppc64/kernel/pSeries_lpar.c
--- linux-2.5/arch/ppc64/kernel/pSeries_lpar.c	2003-09-07 01:40:37.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/pSeries_lpar.c	2003-11-12 23:23:27.000000000 +0000
@@ -35,6 +35,32 @@
 #include <asm/tlbflush.h>
 #include <asm/tlb.h>
 #include <asm/hvcall.h>
+#include <asm/prom.h>
+
+long poll_pending(void)
+{
+	unsigned long dummy;
+	return plpar_hcall(H_POLL_PENDING, 0, 0, 0, 0,
+			   &dummy, &dummy, &dummy);
+}
+
+long prod_processor(void)
+{
+	plpar_hcall_norets(H_PROD);
+	return(0); 
+}
+
+long cede_processor(void)
+{
+	plpar_hcall_norets(H_CEDE);
+	return(0); 
+}
+
+long register_vpa(unsigned long flags, unsigned long proc, unsigned long vpa)
+{
+	plpar_hcall_norets(H_REGISTER_VPA, flags, proc, vpa);
+	return(0); 
+}
 
 long plpar_pte_remove(unsigned long flags,
 		      unsigned long ptex,
@@ -206,6 +232,61 @@ static unsigned char udbg_getcLP(void)
 	}
 }
 
+/* returns 0 if couldn't find or use /chosen/stdout as console */
+static int find_udbg_vterm(void)
+{
+	struct device_node *stdout_node;
+	u32 *termno;
+	char *name;
+	int found = 0;
+
+	/* find the boot console from /chosen/stdout */
+	if (!of_stdout_device) {
+		printk(KERN_WARNING "couldn't get path from /chosen/stdout!\n");
+		return found;
+	}
+	stdout_node = of_find_node_by_path(of_stdout_device);
+	if (!stdout_node) {
+		printk(KERN_WARNING "couldn't find node from /chosen/stdout\n");
+		return found;
+	}
+
+	/* now we have the stdout node; figure out what type of device it is. */
+	name = (char *)get_property(stdout_node, "name", 0);
+	if (!name) {
+		printk(KERN_WARNING "stdout node missing 'name' property!\n");
+		goto out;
+	}
+
+	if (strncmp(name, "vty", 3) == 0) {
+		if (device_is_compatible(stdout_node, "hvterm1")) {
+			termno = (u32 *)get_property(stdout_node, "reg", 0);
+			if (termno) {
+				vtermno = termno[0];
+				ppc_md.udbg_putc = udbg_putcLP;
+				ppc_md.udbg_getc = udbg_getcLP;
+				ppc_md.udbg_getc_poll = udbg_getc_pollLP;
+				found = 1;
+			}
+		} else {
+			/* XXX implement udbg_putcLP_vtty for hvterm-protocol1 case */
+			printk(KERN_WARNING "%s doesn't speak hvterm1; "
+					"can't print udbg messages\n", of_stdout_device);
+		}
+	} else if (strncmp(name, "serial", 6)) {
+		/* XXX fix ISA serial console */
+		printk(KERN_WARNING "serial stdout on LPAR ('%s')! "
+				"can't print udbg messages\n", of_stdout_device);
+	} else {
+		printk(KERN_WARNING "don't know how to print to stdout '%s'\n",
+				of_stdout_device);
+	}
+
+out:
+	of_node_put(stdout_node);
+	return found;
+}
+
 void pSeries_lpar_mm_init(void);
 
 /* This is called early in setup.c.
@@ -213,8 +294,6 @@ void pSeries_lpar_mm_init(void);
  */
 void pSeriesLP_init_early(void)
 {
-	struct device_node *np;
-
 	pSeries_lpar_mm_init();
 
 	ppc_md.tce_build	 = tce_build_pSeriesLP;
@@ -225,24 +304,13 @@ void pSeriesLP_init_early(void)
 #endif
 
 	/* The keyboard is not useful in the LPAR environment.
-	 * Leave all the interfaces NULL.
+	 * Leave all the ppc_md keyboard interfaces NULL.
 	 */
 
-	/* lookup the first virtual terminal number in case we don't have a
-	 * com port. Zero is probably correct in case someone calls udbg
-	 * before the init. The property is a pair of numbers.  The first
-	 * is the starting termno (the one we use) and the second is the
-	 * number of terminals.
-	 */
-	np = find_path_device("/rtas");
-	if (np) {
-		u32 *termno = (u32 *)get_property(np, "ibm,termno", 0);
-		if (termno)
-			vtermno = termno[0];
-	}
-	ppc_md.udbg_putc = udbg_putcLP;
-	ppc_md.udbg_getc = udbg_getcLP;
-	ppc_md.udbg_getc_poll = udbg_getc_pollLP;
+	if (0 == find_udbg_vterm()) {
+		printk(KERN_WARNING
+			"can't use stdout; can't print early debug messages.\n");
+	}
 }
 
 int hvc_get_chars(int index, char *buf, int count)
@@ -285,23 +353,28 @@ int hvc_put_chars(int index, const char 
 	return -1;
 }
 
+/* return the number of client vterms present */
+/* XXX this requires an interface change to handle multiple discontiguous
+ * vterms */
 int hvc_count(int *start_termno)
 {
-	u32 *termno;
-	struct device_node *dn;
+	struct device_node *vty;
+	int num_found = 0;
 
-	if ((dn = find_path_device("/rtas")) != NULL) {
-		if ((termno = (u32 *)get_property(dn, "ibm,termno", 0)) != NULL) {
-			if (start_termno)
-				*start_termno = termno[0];
-			return termno[1];
-		}
+	/* consider only the first vty node.
+	 * we should _always_ be able to find one. */
+	vty = of_find_node_by_name(NULL, "vty");
+	if (vty && device_is_compatible(vty, "hvterm1")) {
+		u32 *termno = (u32 *)get_property(vty, "reg", 0);
+
+		if (termno && start_termno)
+			*start_termno = *termno;
+		num_found = 1;
+		of_node_put(vty);
 	}
-	return 0;
-}
-
-
 
+	return num_found;
+}
 
 long pSeries_lpar_hpte_insert(unsigned long hpte_group,
 			      unsigned long va, unsigned long prpn,
diff -purN linux-2.5/arch/ppc64/kernel/pSeries_pci.c linuxppc64-2.5/arch/ppc64/kernel/pSeries_pci.c
--- linux-2.5/arch/ppc64/kernel/pSeries_pci.c	2003-08-17 19:52:50.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/pSeries_pci.c	2003-11-19 17:10:13.000000000 +0000
@@ -176,6 +176,55 @@ int pci_read_irq_line(struct pci_dev *pc
 	       pci_name(pci_dev), pci_dev->irq);
 	return 0;
 }
+EXPORT_SYMBOL(pci_read_irq_line);
+
+#define ISA_SPACE_MASK 0x1
+#define ISA_SPACE_IO 0x1
+
+static void pci_process_ISA_OF_ranges(struct device_node *isa_node,
+		                      unsigned long phb_io_base_phys,
+				      void * phb_io_base_virt)
+{
+	struct isa_range *range;
+	unsigned long pci_addr;
+	unsigned int isa_addr;
+	unsigned int size;
+	int rlen = 0;
+
+	range = (struct isa_range *) get_property(isa_node, "ranges", &rlen);
+	if (rlen < sizeof(struct isa_range)) {
+		printk(KERN_ERR "unexpected isa range size: %s\n", 
+				__FUNCTION__);
+		return;	
+	}
+	
+	/* From "ISA Binding to 1275"
+	 * The ranges property is laid out as an array of elements,
+	 * each of which comprises:
+	 *   cells 0 - 1:	an ISA address
+	 *   cells 2 - 4:	a PCI address 
+	 *			(size depending on dev->n_addr_cells)
+	 *   cell 5:		the size of the range
+	 */
+	if ((range->isa_addr.a_hi && ISA_SPACE_MASK) == ISA_SPACE_IO) {
+		isa_addr = range->isa_addr.a_lo;
+		pci_addr = (unsigned long) range->pci_addr.a_mid << 32 | 
+			range->pci_addr.a_lo;
+
+		/* Assume these are both zero */
+		if ((pci_addr != 0) || (isa_addr != 0)) {
+			printk(KERN_ERR "unexpected isa to pci mapping: %s\n",
+					__FUNCTION__);
+			return;
+		}
+		
+		size = PAGE_ALIGN(range->size);
+
+		__ioremap_explicit(phb_io_base_phys, 
+				   (unsigned long) phb_io_base_virt, 
+				   size, _PAGE_NO_CACHE);
+	}
+}
 
 static void __init pci_process_bridge_OF_ranges(struct pci_controller *hose,
 						struct device_node *dev,
@@ -188,10 +237,11 @@ static void __init pci_process_bridge_OF
 	struct resource *res;
 	int np, na = prom_n_addr_cells(dev);
 	unsigned long pci_addr, cpu_phys_addr;
+	struct device_node *isa_dn;
 
 	np = na + 5;
 
-	/*
+	/* From "PCI Binding to 1275"
 	 * The ranges property is laid out as an array of elements,
 	 * each of which comprises:
 	 *   cells 0 - 2:	a PCI address
@@ -215,12 +265,22 @@ static void __init pci_process_bridge_OF
 		switch (ranges[0] >> 24) {
 		case 1:		/* I/O space */
 			hose->io_base_phys = cpu_phys_addr;
-			hose->io_base_virt = __ioremap(hose->io_base_phys,
-						       size, _PAGE_NO_CACHE);
+			hose->io_base_virt = reserve_phb_iospace(size);
+			PPCDBG(PPCDBG_PHBINIT, 
+			       "phb%d io_base_phys 0x%lx io_base_virt 0x%lx\n", 
+			       hose->global_number, hose->io_base_phys, 
+			       (unsigned long) hose->io_base_virt);
+
 			if (primary) {
 				pci_io_base = (unsigned long)hose->io_base_virt;
-				if (find_type_devices("isa"))
+				isa_dn = of_find_node_by_type(NULL, "isa");
+				if (isa_dn) {
 					isa_io_base = pci_io_base;
+					of_node_put(isa_dn);
+					pci_process_ISA_OF_ranges(isa_dn,
+						hose->io_base_phys,
+						hose->io_base_virt);
+				}
 			}
 
 			res = &hose->io_resource;
@@ -386,7 +446,7 @@ unsigned long __init find_and_init_phbs(
 	unsigned int root_size_cells = 0;
 	unsigned int index;
 	unsigned int *opprop;
-	struct device_node *root = find_path_device("/");
+	struct device_node *root = of_find_node_by_path("/");
 
 	read_pci_config = rtas_token("read-pci-config");
 	write_pci_config = rtas_token("write-pci-config");
@@ -402,7 +462,9 @@ unsigned long __init find_and_init_phbs(
 
 	index = 0;
 
-	for (node = root->child; node != NULL; node = node->sibling) {
+	for (node = of_get_next_child(root, NULL);
+	     node != NULL;
+	     node = of_get_next_child(root, node)) {
 		if (node->type == NULL || strcmp(node->type, "pci") != 0)
 			continue;
 
@@ -420,6 +482,7 @@ unsigned long __init find_and_init_phbs(
 		index++;
 	}
 
+	of_node_put(root);
 	pci_devs_phb_init();
 
 	return 0;
@@ -450,7 +513,7 @@ void pcibios_name_device(struct pci_dev 
 #endif
 }   
 
-void __init pcibios_fixup_device_resources(struct pci_dev *dev,
+void __devinit pcibios_fixup_device_resources(struct pci_dev *dev,
 					   struct pci_bus *bus)
 {
 	/* Update device resources.  */
@@ -469,8 +532,9 @@ void __init pcibios_fixup_device_resourc
 		}
         }
 }
+EXPORT_SYMBOL(pcibios_fixup_device_resources);
 
-void __init pcibios_fixup_bus(struct pci_bus *bus)
+void __devinit pcibios_fixup_bus(struct pci_bus *bus)
 {
 	struct pci_controller *hose = PCI_GET_PHB_PTR(bus);
 	struct list_head *ln;
@@ -519,18 +583,106 @@ void __init pcibios_fixup_bus(struct pci
 			pcibios_fixup_device_resources(dev, bus);
 	}
 }
+EXPORT_SYMBOL(pcibios_fixup_bus);
 
 static void check_s7a(void)
 {
 	struct device_node *root;
 	char *model;
 
-	root = find_path_device("/");
+	root = of_find_node_by_path("/");
 	if (root) {
 		model = get_property(root, "model", NULL);
 		if (model && !strcmp(model, "IBM,7013-S7A"))
 			s7a_workaround = 1;
+		of_node_put(root);
+	}
+}
+
+static int get_bus_io_range(struct pci_bus *bus, unsigned long *start_phys,
+				unsigned long *start_virt, unsigned long *size)
+{
+	struct pci_controller *hose = PCI_GET_PHB_PTR(bus);
+	struct pci_bus_region region;
+	struct resource *res;
+
+	if (bus->self) {
+		res = bus->resource[0];
+		pcibios_resource_to_bus(bus->self, &region, res);
+		*start_phys = hose->io_base_phys + region.start;
+		*start_virt = (unsigned long) hose->io_base_virt + 
+				region.start;
+		if (region.end > region.start) 
+			*size = region.end - region.start + 1;
+		else {
+			printk("%s(): unexpected region 0x%lx->0x%lx\n", 
+					__FUNCTION__, region.start, region.end);
+			return 1;
+		}
+		
+	} else {
+		/* Root Bus */
+		res = &hose->io_resource;
+		*start_phys = hose->io_base_phys;
+		*start_virt = (unsigned long) hose->io_base_virt;
+		if (res->end > res->start)
+			*size = res->end - res->start + 1;
+		else {
+			printk("%s(): unexpected region 0x%lx->0x%lx\n", 
+					__FUNCTION__, res->start, res->end);
+			return 1;
+		}
 	}
+
+	return 0;
+}
+
+int unmap_bus_range(struct pci_bus *bus)
+{
+	unsigned long start_phys;
+	unsigned long start_virt;
+	unsigned long size;
+
+	if (!bus) {
+		printk(KERN_ERR "%s() expected bus\n", __FUNCTION__);
+		return 1;
+	}
+	
+	if (get_bus_io_range(bus, &start_phys, &start_virt, &size))
+		return 1;
+	if (iounmap_explicit((void *) start_virt, size))
+		return 1;
+
+	return 0;
+}
+EXPORT_SYMBOL(unmap_bus_range);
+
+int remap_bus_range(struct pci_bus *bus)
+{
+	unsigned long start_phys;
+	unsigned long start_virt;
+	unsigned long size;
+
+	if (!bus) {
+		printk(KERN_ERR "%s() expected bus\n", __FUNCTION__);
+		return 1;
+	}
+	
+	if (get_bus_io_range(bus, &start_phys, &start_virt, &size))
+		return 1;
+	if (__ioremap_explicit(start_phys, start_virt, size, _PAGE_NO_CACHE))
+		return 1;
+
+	return 0;
+}
+EXPORT_SYMBOL(remap_bus_range);
+
+static void phbs_fixup_io(void)
+{
+	struct pci_controller *hose;
+
+	for (hose=hose_head;hose;hose=hose->next) 
+		remap_bus_range(hose->bus);
 }
 
 extern void chrp_request_regions(void);
@@ -544,6 +696,7 @@ void __init pcibios_final_fixup(void)
 	while ((dev = pci_find_device(PCI_ANY_ID, PCI_ANY_ID, dev)) != NULL)
 		pci_read_irq_line(dev);
 
+	phbs_fixup_io();
 	chrp_request_regions();
 	pci_fix_bus_sysdata();
 	create_tce_tables();
diff -purN linux-2.5/arch/ppc64/kernel/pci.c linuxppc64-2.5/arch/ppc64/kernel/pci.c
--- linux-2.5/arch/ppc64/kernel/pci.c	2003-07-31 23:47:19.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/pci.c	2003-12-10 04:13:06.000000000 +0000
@@ -126,6 +126,7 @@ struct pci_dev *pci_find_dev_by_addr(uns
 	while ((dev = pci_find_device(PCI_ANY_ID, PCI_ANY_ID, dev)) != NULL) {
 		if ((dev->class >> 16) == PCI_BASE_CLASS_BRIDGE)
 			continue;
+		
 		for (i = 0; i < DEVICE_COUNT_RESOURCE; i++) {
 			unsigned long start = pci_resource_start(dev,i);
 			unsigned long end = pci_resource_end(dev,i);
@@ -144,7 +145,7 @@ struct pci_dev *pci_find_dev_by_addr(uns
 	return NULL;
 }
 
-void __devinit
+void 
 pcibios_resource_to_bus(struct pci_dev *dev, struct pci_bus_region *region,
 			struct resource *res)
 {
@@ -224,7 +225,11 @@ pci_alloc_pci_controller(enum phb_types 
         struct pci_controller *hose;
 	char *model;
 
+#ifdef CONFIG_PPC_ISERIES
+        hose = (struct pci_controller *)kmalloc(sizeof(struct pci_controller), GFP_KERNEL);
+#else
         hose = (struct pci_controller *)alloc_bootmem(sizeof(struct pci_controller));
+#endif
         if(hose == NULL) {
                 printk(KERN_ERR "PCI: Allocate pci_controller failed.\n");
                 return NULL;
@@ -232,6 +237,11 @@ pci_alloc_pci_controller(enum phb_types 
         memset(hose, 0, sizeof(struct pci_controller));
 
 	switch(controller_type) {
+#ifdef CONFIG_PPC_ISERIES
+	case phb_type_hypervisor:
+		model = "PHB HV";
+		break;
+#endif
 	case phb_type_python:
 		model = "PHB PY";
 		break;
@@ -311,6 +321,7 @@ static int __init pcibios_init(void)
 		hose->last_busno = bus->subordinate;
 	}
 
+#ifndef CONFIG_PPC_ISERIES
 	if (pci_probe_only)
 		pcibios_claim_of_setup();
 	else
@@ -318,6 +329,7 @@ static int __init pcibios_init(void)
 		   pci_assign_unassigned_resources() is able to work
 		   correctly with [partially] allocated PCI tree. */
 		pci_assign_unassigned_resources();
+#endif
 
 	/* Call machine dependent fixup */
 	pcibios_final_fixup();
@@ -375,19 +387,27 @@ int pcibios_enable_device(struct pci_dev
  */
 int pci_domain_nr(struct pci_bus *bus)
 {
+#ifdef CONFIG_PPC_ISERIES
+	return 0;
+#else
 	struct pci_controller *hose = PCI_GET_PHB_PTR(bus);
 
 	return hose->global_number;
+#endif
 }
 
+EXPORT_SYMBOL(pci_domain_nr);
+
 /* Set the name of the bus as it appears in /proc/bus/pci */
 int pci_name_bus(char *name, struct pci_bus *bus)
 {
+#ifndef CONFIG_PPC_ISERIES
 	struct pci_controller *hose = PCI_GET_PHB_PTR(bus);
 
 	if (hose->buid)
 		sprintf(name, "%04x:%02x", pci_domain_nr(bus), bus->number);
 	else
+#endif
 		sprintf(name, "%02x", bus->number);
 
 	return 0;
diff -purN linux-2.5/arch/ppc64/kernel/pci_dma.c linuxppc64-2.5/arch/ppc64/kernel/pci_dma.c
--- linux-2.5/arch/ppc64/kernel/pci_dma.c	2003-10-01 22:32:13.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/pci_dma.c	2003-12-04 16:50:24.000000000 +0000
@@ -48,11 +48,13 @@
 /* #define MONITOR_TCE 1 */ /* Turn on to sanity check TCE generation. */
 
 
+#ifdef CONFIG_PPC_PSERIES
 /* Initialize so this guy does not end up in the BSS section.
  * Only used to pass OF initialization data set in prom.c into the main 
  * kernel code -- data ultimately copied into tceTables[].
  */
 extern struct _of_tce_table of_tce_table[];
+#endif
 
 extern struct pci_controller* hose_head;
 extern struct pci_controller** hose_tail;
@@ -553,7 +555,7 @@ inline dma_addr_t get_tces( struct TceTa
 }
 
 #ifdef CONFIG_PPC_ISERIES
-static void tce_free_one_iSeries( struct TceTable *tbl, long tcenum )
+void tce_free_one_iSeries( struct TceTable *tbl, long tcenum )
 {
 	u64 set_tce_rc;
 	union Tce tce;
@@ -701,6 +703,7 @@ void create_tce_tables_for_buses(struct 
 	}
 }
 
+#ifdef CONFIG_PPC_PSERIES
 void create_tce_tables_for_busesLP(struct list_head *bus_list)
 {
 	struct list_head *ln;
@@ -722,15 +725,19 @@ void create_tce_tables_for_busesLP(struc
 		create_tce_tables_for_busesLP(&bus->children);
 	}
 }
+#endif
 
 void create_tce_tables(void) {
 	struct pci_dev *dev = NULL;
 	struct device_node *dn, *mydn;
 
+#ifdef CONFIG_PPC_PSERIES
 	if (systemcfg->platform == PLATFORM_PSERIES_LPAR) {
 		create_tce_tables_for_busesLP(&pci_root_buses);
 	}
-	else {
+	else
+#endif
+	{
 		create_tce_tables_for_buses(&pci_root_buses);
 	}
 	/* Now copy the tce_table ptr from the bus devices down to every
@@ -884,6 +891,7 @@ static void getTceTableParmsiSeries(stru
 static void getTceTableParmsPSeries(struct pci_controller *phb,
 				    struct device_node *dn,
 				    struct TceTable *newTceTable ) {
+#ifdef CONFIG_PPC_PSERIES
 	phandle node;
 	unsigned long i;
 
@@ -953,6 +961,7 @@ static void getTceTableParmsPSeries(stru
 		}
 		i++;
 	}
+#endif
 }
 
 /*
@@ -970,6 +979,7 @@ static void getTceTableParmsPSeries(stru
 static void getTceTableParmsPSeriesLP(struct pci_controller *phb,
 				    struct device_node *dn,
 				    struct TceTable *newTceTable ) {
+#ifdef CONFIG_PPC_PSERIES
 	u32 *dma_window = (u32 *)get_property(dn, "ibm,dma-window", 0);
 	if (!dma_window) {
 		panic("PCI_DMA: getTceTableParmsPSeriesLP: device %s has no ibm,dma-window property!\n", dn->full_name);
@@ -985,6 +995,7 @@ static void getTceTableParmsPSeriesLP(st
 	PPCDBG(PPCDBG_TCEINIT, "\tnewTceTable->index       = 0x%lx\n", newTceTable->index);
 	PPCDBG(PPCDBG_TCEINIT, "\tnewTceTable->startOffset = 0x%lx\n", newTceTable->startOffset);
 	PPCDBG(PPCDBG_TCEINIT, "\tnewTceTable->size        = 0x%lx\n", newTceTable->size);
+#endif
 }
 
 /* Allocates a contiguous real buffer and creates TCEs over it.
diff -purN linux-2.5/arch/ppc64/kernel/pci_dn.c linuxppc64-2.5/arch/ppc64/kernel/pci_dn.c
--- linux-2.5/arch/ppc64/kernel/pci_dn.c	2003-06-24 04:03:30.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/pci_dn.c	2003-11-21 06:45:02.000000000 +0000
@@ -46,6 +46,7 @@
 static void * __init
 update_dn_pci_info(struct device_node *dn, void *data)
 {
+#ifdef CONFIG_PPC_PSERIES
 	struct pci_controller *phb = (struct pci_controller *)data;
 	u32 *regs;
 	char *device_type = get_property(dn, "device_type", 0);
@@ -64,6 +65,7 @@ update_dn_pci_info(struct device_node *d
 			dn->devfn = (regs[0] >> 8) & 0xff;
 		}
 	}
+#endif
 	return NULL;
 }
 
@@ -97,6 +99,7 @@ void *traverse_pci_devices(struct device
 		return ret;
 	for (dn = start->child; dn; dn = nextdn) {
 		nextdn = NULL;
+#ifdef CONFIG_PPC_PSERIES
 		if (get_property(dn, "class-code", 0)) {
 			if (pre && (ret = pre(dn, data)) != NULL)
 				return ret;
@@ -112,6 +115,7 @@ void *traverse_pci_devices(struct device
 					post(dn, data);
 			}
 		}
+#endif
 		if (!nextdn) {
 			/* Walk up to next valid sibling. */
 			do {
diff -purN linux-2.5/arch/ppc64/kernel/ppc_ksyms.c linuxppc64-2.5/arch/ppc64/kernel/ppc_ksyms.c
--- linux-2.5/arch/ppc64/kernel/ppc_ksyms.c	2003-10-16 01:43:36.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/ppc_ksyms.c	2003-12-17 04:27:52.000000000 +0000
@@ -39,6 +39,7 @@
 #include <asm/hw_irq.h>
 #include <asm/abs_addr.h>
 #include <asm/cacheflush.h>
+#include <asm/proc_fs.h>
 #ifdef CONFIG_PPC_ISERIES
 #include <asm/iSeries/iSeries_pci.h>
 #include <asm/iSeries/iSeries_proc.h>
@@ -163,21 +164,23 @@ EXPORT_SYMBOL(kernel_thread);
 EXPORT_SYMBOL(flush_instruction_cache);
 EXPORT_SYMBOL(_get_PVR);
 EXPORT_SYMBOL(giveup_fpu);
-EXPORT_SYMBOL(enable_kernel_fp);
+#ifdef CONFIG_ALTIVEC
+EXPORT_SYMBOL(giveup_altivec);
+#endif
 EXPORT_SYMBOL(flush_icache_range);
 EXPORT_SYMBOL(flush_icache_user_range);
 EXPORT_SYMBOL(flush_dcache_page);
 #ifdef CONFIG_SMP
 #ifdef CONFIG_PPC_ISERIES
-EXPORT_SYMBOL(__no_use_restore_flags);
-EXPORT_SYMBOL(__no_use_save_flags);
-EXPORT_SYMBOL(__no_use_sti);
-EXPORT_SYMBOL(__no_use_cli);
+EXPORT_SYMBOL(local_get_flags);
+EXPORT_SYMBOL(local_irq_disable);
+EXPORT_SYMBOL(local_irq_restore);
 #endif
 #endif
 
 EXPORT_SYMBOL(ppc_md);
 
+#ifdef CONFIG_PPC_PSERIES
 EXPORT_SYMBOL(find_devices);
 EXPORT_SYMBOL(find_type_devices);
 EXPORT_SYMBOL(find_compatible_devices);
@@ -186,6 +189,7 @@ EXPORT_SYMBOL(device_is_compatible);
 EXPORT_SYMBOL(machine_is_compatible);
 EXPORT_SYMBOL(find_all_nodes);
 EXPORT_SYMBOL(get_property);
+#endif
 
 
 EXPORT_SYMBOL_NOVERS(memcpy);
@@ -197,7 +201,6 @@ EXPORT_SYMBOL_NOVERS(memcmp);
 EXPORT_SYMBOL(abs);
 
 EXPORT_SYMBOL(timer_interrupt);
-EXPORT_SYMBOL(irq_desc);
 EXPORT_SYMBOL(get_wchan);
 EXPORT_SYMBOL(console_drivers);
 #ifdef CONFIG_XMON
@@ -222,3 +225,4 @@ EXPORT_SYMBOL(debugger_fault_handler);
 
 EXPORT_SYMBOL(tb_ticks_per_usec);
 EXPORT_SYMBOL(paca);
+EXPORT_SYMBOL(proc_ppc64);
diff -purN linux-2.5/arch/ppc64/kernel/proc_pmc.c linuxppc64-2.5/arch/ppc64/kernel/proc_pmc.c
--- linux-2.5/arch/ppc64/kernel/proc_pmc.c	2002-09-18 02:00:50.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/proc_pmc.c	2003-12-11 05:14:49.000000000 +0000
@@ -44,8 +44,6 @@
 
 static int proc_pmc_control_mode = 0;
 
-static struct proc_dir_entry *proc_ppc64_root = NULL;
-static struct proc_dir_entry *proc_ppc64_pmc_root = NULL;
 static struct proc_dir_entry *proc_ppc64_pmc_system_root = NULL;
 static struct proc_dir_entry *proc_ppc64_pmc_cpu_root[NR_CPUS] = {NULL, };
 
@@ -82,8 +80,8 @@ int proc_pmc_set_pmc6(  struct file *fil
 int proc_pmc_set_pmc7(  struct file *file, const char *buffer, unsigned long count, void *data);
 int proc_pmc_set_pmc8(  struct file *file, const char *buffer, unsigned long count, void *data);
 
-
-void proc_ppc64_init(void)
+#if 0
+int proc_ppc64_init(void)
 {
 	unsigned long i;
 	struct proc_dir_entry *ent = NULL;
@@ -97,15 +95,21 @@ void proc_ppc64_init(void)
 	 *   /proc/ppc64/pmc/cpu0 
 	 */
 	spin_lock(&proc_ppc64_lock);
-	proc_ppc64_root = proc_mkdir("ppc64", 0);
-	if (!proc_ppc64_root) return;
+	if (proc_ppc64.root == NULL) {
+		proc_ppc64_init();
+		if (!proc_ppc64.root) {
+			spin_unlock(&proc_ppc64_lock);
+			return;
+		}
+	}
 	spin_unlock(&proc_ppc64_lock);
 
 	/* Placeholder for rtas interfaces. */
-	rtas_proc_dir = proc_mkdir("rtas", proc_ppc64_root);
-
+	if (proc_ppc64.rtas == NULL) {
+		return;
+	}
 
-	proc_ppc64_pmc_root = proc_mkdir("pmc", proc_ppc64_root);
+	proc_ppc64_pmc_root = proc_mkdir("pmc", proc_ppc64.root);
 
 	proc_ppc64_pmc_system_root = proc_mkdir("system", proc_ppc64_pmc_root);
 	for (i = 0; i < NR_CPUS; i++) {
@@ -116,7 +120,6 @@ void proc_ppc64_init(void)
 		}
 	}
 
-
 	/* Create directories for the software counters. */
 	for (i = 0; i < NR_CPUS; i++) {
 		if (!cpu_online(i))
@@ -127,7 +130,7 @@ void proc_ppc64_init(void)
 			ent->nlink = 1;
 			ent->data = (void *)proc_ppc64_pmc_cpu_root[i];
 			ent->read_proc = (void *)proc_ppc64_pmc_stab_read;
-			ent->write_proc = (void *)proc_ppc64_pmc_stab_read;
+			ent->write_proc = NULL;
 		}
 
 		ent = create_proc_entry("htab", S_IRUGO | S_IWUSR, 
@@ -136,7 +139,7 @@ void proc_ppc64_init(void)
 			ent->nlink = 1;
 			ent->data = (void *)proc_ppc64_pmc_cpu_root[i];
 			ent->read_proc = (void *)proc_ppc64_pmc_htab_read;
-			ent->write_proc = (void *)proc_ppc64_pmc_htab_read;
+			ent->write_proc = NULL;
 		}
 	}
 
@@ -146,7 +149,7 @@ void proc_ppc64_init(void)
 		ent->nlink = 1;
 		ent->data = (void *)proc_ppc64_pmc_system_root;
 		ent->read_proc = (void *)proc_ppc64_pmc_stab_read;
-		ent->write_proc = (void *)proc_ppc64_pmc_stab_read;
+		ent->write_proc = NULL;
 	}
 
 	ent = create_proc_entry("htab", S_IRUGO | S_IWUSR, 
@@ -155,7 +158,7 @@ void proc_ppc64_init(void)
 		ent->nlink = 1;
 		ent->data = (void *)proc_ppc64_pmc_system_root;
 		ent->read_proc = (void *)proc_ppc64_pmc_htab_read;
-		ent->write_proc = (void *)proc_ppc64_pmc_htab_read;
+		ent->write_proc = NULL;
 	}
 
 	/* Create directories for the hardware counters. */
@@ -168,7 +171,7 @@ void proc_ppc64_init(void)
 			ent->nlink = 1;
 			ent->data = (void *)proc_ppc64_pmc_cpu_root[i];
 			ent->read_proc = (void *)proc_ppc64_pmc_hw_read;
-			ent->write_proc = (void *)proc_ppc64_pmc_hw_read;
+			ent->write_proc = NULL;
 		}
 	}
 
@@ -178,9 +181,10 @@ void proc_ppc64_init(void)
 		ent->nlink = 1;
 		ent->data = (void *)proc_ppc64_pmc_system_root;
 		ent->read_proc = (void *)proc_ppc64_pmc_hw_read;
-		ent->write_proc = (void *)proc_ppc64_pmc_hw_read;
+		ent->write_proc = NULL;
 	}
 }
+#endif
 
 /*
  * Find the requested 'file' given a proc token.
@@ -676,15 +680,22 @@ static inline void proc_pmc_tlb(void)
 
 int proc_pmc_set_control( struct file *file, const char *buffer, unsigned long count, void *data )
 {
-	if      ( ! strncmp( buffer, "stop", 4 ) )
+	char stkbuf[10];
+	if (count > 9) count = 9;
+	if (copy_from_user (stkbuf, buffer, count)) {
+		return -EFAULT;
+	}
+	stkbuf[count] = 0;
+
+	if      ( ! strncmp( stkbuf, "stop", 4 ) )
 		proc_pmc_stop();
-	else if ( ! strncmp( buffer, "start", 5 ) )
+	else if ( ! strncmp( stkbuf, "start", 5 ) )
 		proc_pmc_start();
-	else if ( ! strncmp( buffer, "reset", 5 ) )
+	else if ( ! strncmp( stkbuf, "reset", 5 ) )
 		proc_pmc_reset();
-	else if ( ! strncmp( buffer, "cpi", 3 ) )
+	else if ( ! strncmp( stkbuf, "cpi", 3 ) )
 		proc_pmc_cpi();
-	else if ( ! strncmp( buffer, "tlb", 3 ) )
+	else if ( ! strncmp( stkbuf, "tlb", 3 ) )
 		proc_pmc_tlb();
 	
 	/* IMPLEMENT ME */
diff -purN linux-2.5/arch/ppc64/kernel/proc_ppc64.c linuxppc64-2.5/arch/ppc64/kernel/proc_ppc64.c
--- linux-2.5/arch/ppc64/kernel/proc_ppc64.c	2003-08-31 23:14:08.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/proc_ppc64.c	2003-11-24 21:41:12.000000000 +0000
@@ -30,6 +30,8 @@
 #include <linux/init.h>
 #include <linux/mm.h>
 #include <linux/proc_fs.h>
+#include <linux/slab.h>
+#include <linux/kernel.h>
 
 #include <asm/proc_fs.h>
 #include <asm/naca.h>
@@ -37,10 +39,12 @@
 #include <asm/systemcfg.h>
 #include <asm/rtas.h>
 #include <asm/uaccess.h>
+#include <asm/prom.h>
 
 struct proc_ppc64_t proc_ppc64;
 
 void proc_ppc64_create_paca(int num);
+void proc_ppc64_create_smt(void);
 
 static loff_t  page_map_seek( struct file *file, loff_t off, int whence);
 static ssize_t page_map_read( struct file *file, char *buf, size_t nbytes, loff_t *ppos);
@@ -52,15 +56,32 @@ static struct file_operations page_map_f
 	.mmap	= page_map_mmap
 };
 
+#ifdef CONFIG_PPC_PSERIES
+/* routines for /proc/ppc64/ofdt */
+static ssize_t ofdt_write(struct file *, const char __user *, size_t, loff_t *);
+static void proc_ppc64_create_ofdt(struct proc_dir_entry *);
+static int do_remove_node(char *);
+static int do_add_node(char *, size_t);
+static void release_prop_list(const struct property *);
+static struct property *new_property(const char *, const int, const unsigned char *, struct property *);
+static char * parse_next_property(char *, char *, char **, int *, unsigned char**);
+static struct file_operations ofdt_fops = {
+	.write = ofdt_write
+};
+#endif
 
-static int __init proc_ppc64_init(void)
+int __init proc_ppc64_init(void)
 {
 
-	printk(KERN_INFO "proc_ppc64: Creating /proc/ppc64/\n");
 
-	proc_ppc64.root = proc_mkdir("ppc64", 0);
-	if (!proc_ppc64.root)
+	if (proc_ppc64.root == NULL) {
+		printk(KERN_INFO "proc_ppc64: Creating /proc/ppc64/\n");
+		proc_ppc64.root = proc_mkdir("ppc64", 0);
+		if (!proc_ppc64.root)
+			return 0;
+	} else {
 		return 0;
+	}
 
 	proc_ppc64.naca = create_proc_entry("naca", S_IRUSR, proc_ppc64.root);
 	if ( proc_ppc64.naca ) {
@@ -90,8 +111,18 @@ static int __init proc_ppc64_init(void)
 		}
 	}
 
+#ifdef CONFIG_PPC_PSERIES
 	/* Placeholder for rtas interfaces. */
-	proc_ppc64.rtas = proc_mkdir("rtas", proc_ppc64.root);
+	if (proc_ppc64.rtas == NULL)
+		proc_ppc64.rtas = proc_mkdir("rtas", proc_ppc64.root);
+
+	if (proc_ppc64.rtas)
+		proc_symlink("rtas", 0, "ppc64/rtas");
+
+	proc_ppc64_create_smt();
+
+	proc_ppc64_create_ofdt(proc_ppc64.root);
+#endif
 
 	return 0;
 }
@@ -173,5 +204,310 @@ static int page_map_mmap( struct file *f
 	return 0;
 }
 
-fs_initcall(proc_ppc64_init);
+#ifdef CONFIG_PPC_PSERIES
+/* create /proc/ppc64/ofdt write-only by root */
+static void proc_ppc64_create_ofdt(struct proc_dir_entry *parent)
+{
+	struct proc_dir_entry *ent;
+
+	ent = create_proc_entry("ofdt", S_IWUSR, parent);
+	if (ent) {
+		ent->nlink = 1;
+		ent->data = NULL;
+		ent->size = 0;
+		ent->proc_fops = &ofdt_fops;
+	}
+}
+
+/**
+ * ofdt_write - perform operations on the Open Firmware device tree
+ *
+ * @file: not used
+ * @buf: command and arguments
+ * @count: size of the command buffer
+ * @off: not used
+ *
+ * Operations supported at this time are addition and removal of
+ * whole nodes along with their properties.  Operations on individual
+ * properties are not implemented (yet).
+ */
+static ssize_t ofdt_write(struct file *file, const char __user *buf, size_t count, loff_t *off)
+{
+	int rv = 0;
+	char *kbuf;
+	char *tmp;
+
+	if (!(kbuf = kmalloc(count + 1, GFP_KERNEL))) {
+		rv = -ENOMEM;
+		goto out;
+	}
+	if (copy_from_user(kbuf, buf, count)) {
+		rv = -EFAULT;
+		goto out;
+	}
+
+	kbuf[count] = '\0';
+
+	tmp = strchr(kbuf, ' ');
+	if (!tmp) {
+		rv = -EINVAL;
+		goto out;
+	}
+	*tmp = '\0';
+	tmp++;
+
+	if (!strcmp(kbuf, "add_node"))
+		rv = do_add_node(tmp, count - (tmp - kbuf));
+	else if (!strcmp(kbuf, "remove_node"))
+		rv = do_remove_node(tmp);
+	else
+		rv = -EINVAL;
+out:
+	kfree(kbuf);
+	return rv ? rv : count;
+}
+
+static int do_remove_node(char *buf)
+{
+	struct device_node *node;
+	int rv = 0;
+
+	if ((node = of_find_node_by_path(buf)))
+		of_remove_node(node);
+	else
+		rv = -ENODEV;
+
+	of_node_put(node);
+	return rv;
+}
+
+static int do_add_node(char *buf, size_t bufsize)
+{
+	char *path, *end, *name;
+	struct device_node *np;
+	struct property *prop = NULL;
+	unsigned char* value;
+	int length, rv = 0;
+
+	end = buf + bufsize;
+	path = buf;
+	buf = strchr(buf, ' ');
+	if (!buf)
+		return -EINVAL;
+	*buf = '\0';
+	buf++;
+
+	if ((np = of_find_node_by_path(path))) {
+		of_node_put(np);
+		return -EINVAL;
+	}
+
+	/* rv = build_prop_list(tmp, bufsize - (tmp - buf), &proplist); */
+	while (buf < end &&
+	       (buf = parse_next_property(buf, end, &name, &length, &value))) {
+		struct property *last = prop;
+
+		prop = new_property(name, length, value, last);
+		if (!prop) {
+			rv = -ENOMEM;
+			prop = last;
+			goto out;
+		}
+	}
+	if (!buf) {
+		rv = -EINVAL;
+		goto out;
+	}
+
+	rv = of_add_node(path, prop);
+
+out:
+	if (rv)
+		release_prop_list(prop);
+	return rv;
+}
+
+static struct property *new_property(const char *name, const int length, const unsigned char *value, struct property *last)
+{
+	struct property *new = kmalloc(sizeof(*new), GFP_KERNEL);
+
+	if (!new)
+		return NULL;
+	memset(new, 0, sizeof(*new));
+
+	if (!(new->name = kmalloc(strlen(name) + 1, GFP_KERNEL)))
+		goto cleanup;
+	if (!(new->value = kmalloc(length + 1, GFP_KERNEL)))
+		goto cleanup;
+
+	strcpy(new->name, name);
+	memcpy(new->value, value, length);
+	*(((char *)new->value) + length) = 0;
+	new->length = length;
+	new->next = last;
+	return new;
+
+cleanup:
+	if (new->name)
+		kfree(new->name);
+	if (new->value)
+		kfree(new->value);
+	kfree(new);
+	return NULL;
+}
+
+/**
+ * parse_next_property - process the next property from raw input buffer
+ * @buf: input buffer, must be nul-terminated
+ * @end: end of the input buffer + 1, for validation
+ * @name: return value; set to property name in buf
+ * @length: return value; set to length of value
+ * @value: return value; set to the property value in buf
+ *
+ * Note that the caller must make copies of the name and value returned,
+ * this function does no allocation or copying of the data.  Return value
+ * is set to the next name in buf, or NULL on error.
+ */
+static char * parse_next_property(char *buf, char *end, char **name, int *length, unsigned char **value)
+{
+	char *tmp;
+
+	*name = buf;
+
+	tmp = strchr(buf, ' ');
+	if (!tmp) {
+		printk(KERN_ERR "property parse failed in %s at line %d\n", __FUNCTION__, __LINE__);
+		return NULL;
+	}
+	*tmp = '\0';
+
+	if (++tmp >= end) {
+		printk(KERN_ERR "property parse failed in %s at line %d\n", __FUNCTION__, __LINE__);
+		return NULL;
+	}
+
+	/* now we're on the length */
+	*length = -1;
+	*length = simple_strtoul(tmp, &tmp, 10);
+	if (*length == -1) {
+		printk(KERN_ERR "property parse failed in %s at line %d\n", __FUNCTION__, __LINE__);
+		return NULL;
+	}
+	if (*tmp != ' ' || ++tmp >= end) {
+		printk(KERN_ERR "property parse failed in %s at line %d\n", __FUNCTION__, __LINE__);
+		return NULL;
+	}
+
+	/* now we're on the value */
+	*value = tmp;
+	tmp += *length;
+	if (tmp > end) {
+		printk(KERN_ERR "property parse failed in %s at line %d\n", __FUNCTION__, __LINE__);
+		return NULL;
+	}
+	else if (tmp < end && *tmp != ' ' && *tmp != '\0') {
+		printk(KERN_ERR "property parse failed in %s at line %d\n", __FUNCTION__, __LINE__);
+		return NULL;
+	}
+	tmp++;
+
+	/* and now we should be on the next name, or the end */
+	return tmp;
+}
+
+static void release_prop_list(const struct property *prop)
+{
+	struct property *next;
+	for (; prop; prop = next) {
+		next = prop->next;
+		kfree(prop->name);
+		kfree(prop->value);
+		kfree(prop);
+	}
+
+}
+#endif	/* defined(CONFIG_PPC_PSERIES) */
+
+static int proc_ppc64_smt_snooze_read(char *page, char **start, off_t off,
+				      int count, int *eof, void *data)
+{
+	if (naca->smt_snooze_delay)
+		return sprintf(page, "%lu\n", naca->smt_snooze_delay);
+	else 
+		return sprintf(page, "disabled\n");
+}
+ 
+static int proc_ppc64_smt_snooze_write(struct file* file, const char *buffer,
+				       unsigned long count, void *data)
+{
+	unsigned long val;
+	char val_string[22];
 
+	if (!capable(CAP_SYS_ADMIN))
+		return -EACCES;
+
+	if (count > sizeof(val_string) - 1)
+		return -EINVAL;
+
+	if (copy_from_user(val_string, buffer, count))
+		return -EFAULT;
+
+	val_string[count] = '\0';
+
+	if (val_string[0] == '0' && (val_string[1] == '\n' || val_string[1] == '\0')) {
+		naca->smt_snooze_delay = 0;
+		return count;
+	}
+ 
+	val = simple_strtoul(val_string, NULL, 10);
+	if (val != 0) 
+		naca->smt_snooze_delay = val;
+	else
+		return -EINVAL;
+
+	return count;
+}
+ 
+static int proc_ppc64_smt_state_read(char *page, char **start, off_t off,
+				      int count, int *eof, void *data)
+{
+	switch(naca->smt_state) {
+	case SMT_OFF:
+		return sprintf(page, "off\n");
+		break;
+	case SMT_ON:
+		return sprintf(page, "on\n");
+		break;
+	case SMT_DYNAMIC:
+		return sprintf(page, "dynamic\n");
+		break;
+	default:
+		return sprintf(page, "unknown\n");
+		break;
+	}
+}
+ 
+void proc_ppc64_create_smt(void)
+{
+	struct proc_dir_entry *ent_snooze = 
+		create_proc_entry("smt-snooze-delay", S_IRUGO | S_IWUSR, 
+				  proc_ppc64.root);
+	struct proc_dir_entry *ent_enabled = 
+		create_proc_entry("smt-enabled", S_IRUGO | S_IWUSR, 
+				  proc_ppc64.root);
+	if (ent_snooze) {
+		ent_snooze->nlink = 1;
+		ent_snooze->data = NULL;
+		ent_snooze->read_proc = (void *)proc_ppc64_smt_snooze_read;
+		ent_snooze->write_proc = (void *)proc_ppc64_smt_snooze_write;
+	}
+
+	if (ent_enabled) {
+		ent_enabled->nlink = 1;
+		ent_enabled->data = NULL;
+		ent_enabled->read_proc = (void *)proc_ppc64_smt_state_read;
+		ent_enabled->write_proc = NULL;
+	}
+}
+
+fs_initcall(proc_ppc64_init);
diff -purN linux-2.5/arch/ppc64/kernel/process.c linuxppc64-2.5/arch/ppc64/kernel/process.c
--- linux-2.5/arch/ppc64/kernel/process.c	2003-10-08 02:53:40.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/process.c	2003-12-17 04:27:52.000000000 +0000
@@ -50,7 +50,10 @@
 #include <asm/cputable.h>
 #include <asm/sections.h>
 
+#ifndef CONFIG_SMP
 struct task_struct *last_task_used_math = NULL;
+struct task_struct *last_task_used_altivec = NULL;
+#endif
 
 struct mm_struct ioremap_mm = { pgd             : ioremap_dir  
                                ,page_table_lock : SPIN_LOCK_UNLOCKED };
@@ -58,8 +61,7 @@ struct mm_struct ioremap_mm = { pgd     
 char *sysmap = NULL;
 unsigned long sysmap_size = 0;
 
-void
-enable_kernel_fp(void)
+void enable_kernel_fp(void)
 {
 #ifdef CONFIG_SMP
 	if (current->thread.regs && (current->thread.regs->msr & MSR_FP))
@@ -70,6 +72,7 @@ enable_kernel_fp(void)
 	giveup_fpu(last_task_used_math);
 #endif /* CONFIG_SMP */
 }
+EXPORT_SYMBOL(enable_kernel_fp);
 
 int dump_task_fpu(struct task_struct *tsk, elf_fpregset_t *fpregs)
 {
@@ -85,6 +88,31 @@ int dump_task_fpu(struct task_struct *ts
 	return 1;
 }
 
+#ifdef CONFIG_ALTIVEC
+
+void enable_kernel_altivec(void)
+{
+#ifdef CONFIG_SMP
+	if (current->thread.regs && (current->thread.regs->msr & MSR_VEC))
+		giveup_altivec(current);
+	else
+		giveup_altivec(NULL);	/* just enables FP for kernel */
+#else
+	giveup_altivec(last_task_used_altivec);
+#endif /* CONFIG_SMP */
+}
+EXPORT_SYMBOL(enable_kernel_altivec);
+
+int dump_task_altivec(struct pt_regs *regs, elf_vrregset_t *vrregs)
+{
+	if (regs->msr & MSR_VEC)
+		giveup_altivec(current);
+	memcpy(vrregs, &current->thread.vr[0], sizeof(*vrregs));
+	return 1;
+}
+
+#endif /* CONFIG_ALTIVEC */
+
 struct task_struct *__switch_to(struct task_struct *prev,
 				struct task_struct *new)
 {
@@ -104,8 +132,20 @@ struct task_struct *__switch_to(struct t
 	 */
 	if (prev->thread.regs && (prev->thread.regs->msr & MSR_FP))
 		giveup_fpu(prev);
+#ifdef CONFIG_ALTIVEC
+	if (prev->thread.regs && (prev->thread.regs->msr & MSR_VEC))
+		giveup_altivec(prev);
+#endif /* CONFIG_ALTIVEC */
 #endif /* CONFIG_SMP */
 
+#if defined(CONFIG_ALTIVEC) && !defined(CONFIG_SMP)
+	/* Avoid the trap.  On smp this this never happens since
+	 * we don't set last_task_used_altivec -- Cort
+	 */
+	if (new->thread.regs && last_task_used_altivec == new)
+		new->thread.regs->msr |= MSR_VEC;
+#endif /* CONFIG_ALTIVEC */
+
 	new_thread = &new->thread;
 	old_thread = &current->thread;
 
@@ -158,8 +198,14 @@ void show_regs(struct pt_regs * regs)
 
 void exit_thread(void)
 {
+#ifndef CONFIG_SMP
 	if (last_task_used_math == current)
 		last_task_used_math = NULL;
+#ifdef CONFIG_ALTIVEC
+	if (last_task_used_altivec == current)
+		last_task_used_altivec = NULL;
+#endif /* CONFIG_ALTIVEC */
+#endif /* CONFIG_SMP */
 }
 
 void flush_thread(void)
@@ -169,8 +215,14 @@ void flush_thread(void)
 	if (t->flags & _TIF_ABI_PENDING)
 		t->flags ^= (_TIF_ABI_PENDING | _TIF_32BIT);
 
+#ifndef CONFIG_SMP
 	if (last_task_used_math == current)
 		last_task_used_math = NULL;
+#ifdef CONFIG_ALTIVEC
+	if (last_task_used_altivec == current)
+		last_task_used_altivec = NULL;
+#endif /* CONFIG_ALTIVEC */
+#endif /* CONFIG_SMP */
 }
 
 void
@@ -178,6 +230,25 @@ release_thread(struct task_struct *t)
 {
 }
 
+
+/*
+ * This gets called before we allocate a new thread and copy
+ * the current task into it.
+ */
+void prepare_to_copy(struct task_struct *tsk)
+{
+	struct pt_regs *regs = tsk->thread.regs;
+
+	if (regs == NULL)
+		return;
+	if (regs->msr & MSR_FP)
+		giveup_fpu(current);
+#ifdef CONFIG_ALTIVEC
+	if (regs->msr & MSR_VEC)
+		giveup_altivec(current);
+#endif /* CONFIG_ALTIVEC */
+}
+
 /*
  * Copy a thread..
  */
@@ -268,9 +339,25 @@ void start_thread(struct pt_regs *regs, 
 	regs->gpr[1] = sp;
 	regs->gpr[2] = toc;
 	regs->msr = MSR_USER64;
+#ifndef CONFIG_SMP
 	if (last_task_used_math == current)
 		last_task_used_math = 0;
+#endif /* CONFIG_SMP */
+	memset(current->thread.fpr, 0, sizeof(current->thread.fpr));
 	current->thread.fpscr = 0;
+#ifdef CONFIG_ALTIVEC
+#ifndef CONFIG_SMP
+	if (last_task_used_altivec == current)
+		last_task_used_altivec = 0;
+#endif /* CONFIG_SMP */
+	memset(current->thread.vr, 0, sizeof(current->thread.vr));
+	current->thread.vscr.u[0] = 0;
+	current->thread.vscr.u[1] = 0;
+	current->thread.vscr.u[2] = 0;
+	current->thread.vscr.u[3] = 0x00010000; /* Java mode disabled */
+	current->thread.vrsave = 0;
+	current->thread.used_vr = 0;
+#endif /* CONFIG_ALTIVEC */
 }
 
 int set_fpexc_mode(struct task_struct *tsk, unsigned int val)
@@ -314,9 +401,6 @@ int sys_clone(unsigned long clone_flags,
 		}
 	}
 
-	if (regs->msr & MSR_FP)
-		giveup_fpu(current);
-
 	return do_fork(clone_flags & ~CLONE_IDLETASK, p2, regs, 0,
 		    (int *)parent_tidptr, (int *)child_tidptr);
 }
@@ -325,9 +409,6 @@ int sys_fork(unsigned long p1, unsigned 
 	     unsigned long p4, unsigned long p5, unsigned long p6,
 	     struct pt_regs *regs)
 {
-	if (regs->msr & MSR_FP)
-		giveup_fpu(current);
-
 	return do_fork(SIGCHLD, regs->gpr[1], regs, 0, NULL, NULL);
 }
 
@@ -335,9 +416,6 @@ int sys_vfork(unsigned long p1, unsigned
 	      unsigned long p4, unsigned long p5, unsigned long p6,
 	      struct pt_regs *regs)
 {
-	if (regs->msr & MSR_FP)
-		giveup_fpu(current);
-
 	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, regs->gpr[1], regs, 0,
 	            NULL, NULL);
 }
@@ -355,7 +433,10 @@ int sys_execve(unsigned long a0, unsigne
 		goto out;
 	if (regs->msr & MSR_FP)
 		giveup_fpu(current);
-  
+#ifdef CONFIG_ALTIVEC
+	if (regs->msr & MSR_VEC)
+		giveup_altivec(current);
+#endif /* CONFIG_ALTIVEC */
 	error = do_execve(filename, (char **) a1, (char **) a2, regs);
   
 	if (error == 0)
diff -purN linux-2.5/arch/ppc64/kernel/prom.c linuxppc64-2.5/arch/ppc64/kernel/prom.c
--- linux-2.5/arch/ppc64/kernel/prom.c	2003-08-19 02:46:23.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/prom.c	2003-12-09 16:45:05.000000000 +0000
@@ -29,6 +29,7 @@
 #include <linux/spinlock.h>
 #include <linux/types.h>
 #include <linux/pci.h>
+#include <linux/proc_fs.h>
 #include <asm/prom.h>
 #include <asm/rtas.h>
 #include <asm/lmb.h>
@@ -44,6 +45,7 @@
 #include <asm/bitops.h>
 #include <asm/naca.h>
 #include <asm/pci.h>
+#include <asm/pci_dma.h>
 #include <asm/bootinfo.h>
 #include <asm/ppcdebug.h>
 #include "open_pic.h"
@@ -147,16 +149,16 @@ char *bootpath = 0;
 char *bootdevice = 0;
 
 int boot_cpuid = 0;
+#define MAX_CPU_THREADS 2
 
 struct device_node *allnodes = 0;
-
-#define UNDEFINED_IRQ 0xffff
-unsigned short real_irq_to_virt_map[NR_HW_IRQS];
-unsigned short virt_irq_to_real_map[NR_IRQS];
-int last_virt_irq = 2;	/* index of last virt_irq.  Skip through IPI */
+/* use when traversing tree through the allnext, child, sibling,
+ * or parent members of struct device_node.
+ */
+static rwlock_t devtree_lock = RW_LOCK_UNLOCKED;
 
 static unsigned long call_prom(const char *service, int nargs, int nret, ...);
-static void prom_exit(void);
+static void prom_panic(const char *reason);
 static unsigned long copy_device_tree(unsigned long);
 static unsigned long inspect_node(phandle, struct device_node *, unsigned long,
 				  unsigned long, struct device_node ***);
@@ -168,6 +170,11 @@ static int prom_next_node(phandle *);
 static struct bi_record * prom_bi_rec_verify(struct bi_record *);
 static unsigned long prom_bi_rec_reserve(unsigned long);
 static struct device_node *find_phandle(phandle);
+static void of_node_cleanup(struct device_node *);
+static struct device_node *derive_parent(const char *);
+static void add_node_proc_entries(struct device_node *);
+static void remove_node_proc_entries(struct device_node *);
+static int of_finish_dynamic_node(struct device_node *);
 
 #ifdef DEBUG_PROM
 void prom_dump_lmb(void);
@@ -223,10 +230,12 @@ call_prom(const char *service, int nargs
 
 
 static void __init
-prom_exit()
+prom_panic(const char *reason)
 {
 	unsigned long offset = reloc_offset();
 
+	prom_print(reason);
+	/* ToDo: should put up an SRC here */
 	call_prom(RELOC("exit"), 0, 0);
 
 	for (;;)			/* should never get here */
@@ -602,6 +611,9 @@ prom_instantiate_rtas(void)
 						      _rtas->base) >= 0) {
 				_rtas->entry = (long)_prom->args.rets[1];
 			}
+			RELOC(rtas_rmo_buf)
+				= lmb_alloc_base(RTAS_RMOBUF_MAX, PAGE_SIZE,
+							rtas_region);
 		}
 
 		if (_rtas->entry <= 0) {
@@ -785,8 +797,7 @@ prom_initialize_tce_table(void)
 		base = lmb_alloc(minsize, align);
 
 		if ( !base ) {
-			prom_print(RELOC("ERROR, cannot find space for TCE table.\n"));
-			prom_exit();
+			prom_panic(RELOC("ERROR, cannot find space for TCE table.\n"));
 		}
 
 		vbase = absolute_to_virt(base);
@@ -884,16 +895,21 @@ static void
 prom_hold_cpus(unsigned long mem)
 {
 	unsigned long i;
-	unsigned int cpuid;
+	unsigned int reg;
 	phandle node;
 	unsigned long offset = reloc_offset();
 	char type[64], *path;
+	int cpuid = 0;
+	unsigned int interrupt_server[MAX_CPU_THREADS];
+	unsigned int cpu_threads, hw_cpu_num;
+	int propsize;
 	extern void __secondary_hold(void);
         extern unsigned long __secondary_hold_spinloop;
         extern unsigned long __secondary_hold_acknowledge;
         unsigned long *spinloop     = __v2a(&__secondary_hold_spinloop);
         unsigned long *acknowledge  = __v2a(&__secondary_hold_acknowledge);
         unsigned long secondary_hold = (unsigned long)__v2a(*PTRRELOC((unsigned long *)__secondary_hold));
+        struct naca_struct *_naca = RELOC(naca);
         struct systemcfg *_systemcfg = RELOC(systemcfg);
 	struct paca_struct *_xPaca = PTRRELOC(&paca[0]);
 	struct prom_t *_prom = PTRRELOC(&prom);
@@ -946,13 +962,9 @@ prom_hold_cpus(unsigned long mem)
 		if (strcmp(type, RELOC("okay")) != 0)
 			continue;
 
-                cpuid = -1;
+                reg = -1;
 		call_prom(RELOC("getprop"), 4, 1, node, RELOC("reg"),
-			  &cpuid, sizeof(cpuid));
-
-		/* Only need to start secondary procs, not ourself. */
-		if ( cpuid == _prom->cpu )
-			continue;
+			  &reg, sizeof(reg));
 
 		path = (char *) mem;
 		memset(path, 0, 256);
@@ -962,12 +974,14 @@ prom_hold_cpus(unsigned long mem)
 
 #ifdef DEBUG_PROM
 		prom_print_nl();
-		prom_print(RELOC("cpu hw idx   = 0x"));
+		prom_print(RELOC("cpuid        = 0x"));
 		prom_print_hex(cpuid);
 		prom_print_nl();
+		prom_print(RELOC("cpu hw idx   = 0x"));
+		prom_print_hex(reg);
+		prom_print_nl();
 #endif
-		prom_print(RELOC("starting cpu "));
-		prom_print(path);
+		_xPaca[cpuid].xHwProcNum = reg;
 
 		/* Init the acknowledge var which will be reset by
 		 * the secondary cpu when it awakens from its OF
@@ -975,45 +989,85 @@ prom_hold_cpus(unsigned long mem)
 		 */
 		*acknowledge = (unsigned long)-1;
 
-#ifdef DEBUG_PROM
-		prom_print(RELOC("    3) spinloop       = 0x"));
-		prom_print_hex(spinloop);
-		prom_print_nl();
-		prom_print(RELOC("    3) *spinloop      = 0x"));
-		prom_print_hex(*spinloop);
-		prom_print_nl();
-		prom_print(RELOC("    3) acknowledge    = 0x"));
-		prom_print_hex(acknowledge);
-		prom_print_nl();
-		prom_print(RELOC("    3) *acknowledge   = 0x"));
-		prom_print_hex(*acknowledge);
-		prom_print_nl();
-		prom_print(RELOC("    3) secondary_hold = 0x"));
-		prom_print_hex(secondary_hold);
-		prom_print_nl();
-#endif
-		call_prom(RELOC("start-cpu"), 3, 0, node, secondary_hold, cpuid);
-		prom_print(RELOC("..."));
-		for ( i = 0 ; (i < 100000000) && 
-			      (*acknowledge == ((unsigned long)-1)); i++ ) ;
-#ifdef DEBUG_PROM
-		{
-			unsigned long *p = 0x0;
-			prom_print(RELOC("    4) 0x0 = 0x"));
-			prom_print_hex(*p);
-			prom_print_nl();
+		propsize = call_prom(RELOC("getprop"), 4, 1, node,
+				     RELOC("ibm,ppc-interrupt-server#s"), 
+				     &interrupt_server, 
+				     sizeof(interrupt_server));
+		if (propsize < 0) {
+			/* no property.  old hardware has no SMT */
+			cpu_threads = 1;
+			interrupt_server[0] = reg; /* fake it with phys id */
+		} else {
+			/* We have a threaded processor */
+			cpu_threads = propsize / sizeof(u32);
+			if (cpu_threads > MAX_CPU_THREADS) {
+				prom_print(RELOC("SMT: too many threads!\nSMT: found "));
+				prom_print_hex(cpu_threads);
+				prom_print(RELOC(", max is "));
+				prom_print_hex(MAX_CPU_THREADS);
+				prom_print_nl();
+				cpu_threads = 1; /* ToDo: panic? */
+			}
 		}
+
+		hw_cpu_num = interrupt_server[0];
+		if (hw_cpu_num != _prom->cpu) {
+			/* Primary Thread of non-boot cpu */
+			prom_print_hex(cpuid);
+			prom_print(RELOC(" : starting cpu "));
+			prom_print(path);
+			prom_print(RELOC("..."));
+			call_prom(RELOC("start-cpu"), 3, 0, node, 
+				  secondary_hold, cpuid);
+
+			for ( i = 0 ; (i < 100000000) && 
+			      (*acknowledge == ((unsigned long)-1)); i++ ) ;
+
+			if (*acknowledge == cpuid) {
+				prom_print(RELOC("ok\n"));
+#ifdef CONFIG_SMP
+				/* Set the number of active processors. */
+				_systemcfg->processorCount++;
+				cpu_set(cpuid, RELOC(cpu_available_map));
+				cpu_set(cpuid, RELOC(cpu_possible_map));
+				cpu_set(cpuid, RELOC(cpu_present_at_boot));
 #endif
-		if (*acknowledge == cpuid) {
-			prom_print(RELOC("ok\n"));
-			/* Set the number of active processors. */
-			_systemcfg->processorCount++;
-			_xPaca[cpuid].active = 1;
-		} else {
-			prom_print(RELOC("failed: "));
-			prom_print_hex(*acknowledge);
+			} else {
+				prom_print(RELOC("failed: "));
+				prom_print_hex(*acknowledge);
+				prom_print_nl();
+				/* prom_panic(RELOC("cpu failed to start")); */
+			}
+		}
+#ifdef CONFIG_SMP
+		else {
+			prom_print_hex(cpuid);
+			prom_print(RELOC(" : booting  cpu "));
+			prom_print(path);
+			prom_print_nl();
+			cpu_set(cpuid, RELOC(cpu_available_map));
+			cpu_set(cpuid, RELOC(cpu_possible_map));
+			cpu_set(cpuid, RELOC(cpu_online_map));
+			cpu_set(cpuid, RELOC(cpu_present_at_boot));
+		}
+
+		/* Init paca for secondary threads.   They start later. */
+		for (i=1; i < cpu_threads; i++) {
+			cpuid++;
+			_xPaca[cpuid].xHwProcNum = interrupt_server[i];
+			prom_print_hex(interrupt_server[i]);
+			prom_print(RELOC(" : preparing thread ... "));
+			if (_naca->smt_state) {
+				cpu_set(cpuid, RELOC(cpu_available_map));
+				cpu_set(cpuid, RELOC(cpu_present_at_boot));
+				prom_print(RELOC("available"));
+			} else {
+				prom_print(RELOC("not available"));
+			}
 			prom_print_nl();
 		}
+#endif
+		cpuid++;
 	}
 #ifdef CONFIG_HMT
 	/* Only enable HMT on processors that provide support. */
@@ -1023,10 +1077,10 @@ prom_hold_cpus(unsigned long mem)
 		prom_print(RELOC("    starting secondary threads\n"));
 
 		for (i = 0; i < NR_CPUS; i += 2) {
-			if (!_xPaca[i].active)
+			if (!cpu_online(i))
 				continue;
 
-			if (i == boot_cpuid) {
+			if (i == 0) {
 				unsigned long pir = _get_PIR();
 				if (__is_processor(PV_PULSAR)) {
 					RELOC(hmt_thread_data)[i].pir = 
@@ -1036,7 +1090,8 @@ prom_hold_cpus(unsigned long mem)
 						pir & 0x3ff;
 				}
 			}
-			_xPaca[i+1].active = 1;
+/* 			cpu_set(i+1, cpu_online_map); */
+			cpu_set(i+1, RELOC(cpu_possible_map));
 		}
 		_systemcfg->processorCount *= 2;
 	} else {
@@ -1049,6 +1104,105 @@ prom_hold_cpus(unsigned long mem)
 #endif
 }
 
+static void
+smt_setup(void)
+{
+	char *p, *q;
+	char my_smt_enabled = SMT_DYNAMIC;
+	unsigned long my_smt_snooze_delay; 
+	ihandle prom_options = NULL;
+	char option[9];
+	unsigned long offset = reloc_offset();
+        struct naca_struct *_naca = RELOC(naca);
+	char found = 0;
+
+	if (strstr(RELOC(cmd_line), RELOC("smt-enabled="))) {
+		for (q = RELOC(cmd_line); (p = strstr(q, RELOC("smt-enabled="))) != 0; ) {
+			q = p + 12;
+			if (p > RELOC(cmd_line) && p[-1] != ' ')
+				continue;
+			found = 1;
+			if (q[0] == 'o' && q[1] == 'f' && 
+			    q[2] == 'f' && (q[3] == ' ' || q[3] == '\0')) {
+				my_smt_enabled = SMT_OFF;
+			} else if (q[0]=='o' && q[1] == 'n' && 
+				   (q[2] == ' ' || q[2] == '\0')) {
+				my_smt_enabled = SMT_ON;
+			} else {
+				my_smt_enabled = SMT_DYNAMIC;
+			} 
+		}
+	}
+	if (!found) {
+		prom_options = (ihandle)call_prom(RELOC("finddevice"), 1, 1, RELOC("/options"));
+		if (prom_options != (ihandle) -1) {
+			call_prom(RELOC("getprop"), 
+				4, 1, prom_options,
+				RELOC("ibm,smt-enabled"), 
+				option, 
+				sizeof(option));
+			if (option[0] != 0) {
+				found = 1;
+				if (!strcmp(option, "off"))	
+					my_smt_enabled = SMT_OFF;
+				else if (!strcmp(option, "on"))	
+					my_smt_enabled = SMT_ON;
+				else
+					my_smt_enabled = SMT_DYNAMIC;
+			}
+		}
+	}
+
+	if (!found )
+		my_smt_enabled = SMT_DYNAMIC; /* default to on */
+
+	found = 0;
+	if (my_smt_enabled) {
+		if (strstr(RELOC(cmd_line), RELOC("smt-snooze-delay="))) {
+			for (q = RELOC(cmd_line); (p = strstr(q, RELOC("smt-snooze-delay="))) != 0; ) {
+				q = p + 17;
+				if (p > RELOC(cmd_line) && p[-1] != ' ')
+					continue;
+				found = 1;
+				/* Don't use simple_strtoul() because _ctype & others aren't RELOC'd */
+				my_smt_snooze_delay = 0;
+				while (*q >= '0' && *q <= '9') {
+					my_smt_snooze_delay = my_smt_snooze_delay * 10 + *q - '0';
+					q++;
+				}
+			}
+		}
+
+		if (!found) {
+			prom_options = (ihandle)call_prom(RELOC("finddevice"), 1, 1, RELOC("/options"));
+			if (prom_options != (ihandle) -1) {
+				call_prom(RELOC("getprop"), 
+					4, 1, prom_options,
+					RELOC("ibm,smt-snooze-delay"), 
+					option, 
+					sizeof(option));
+				if (option[0] != 0) {
+					found = 1;
+					/* Don't use simple_strtoul() because _ctype & others aren't RELOC'd */
+					my_smt_snooze_delay = 0;
+					q = option;
+					while (*q >= '0' && *q <= '9') {
+						my_smt_snooze_delay = my_smt_snooze_delay * 10 + *q - '0';
+						q++;
+					}
+				}
+			}
+		}
+
+		if (!found) {
+			my_smt_snooze_delay = 30000; /* default value */
+		}
+	} else {
+		my_smt_snooze_delay = 0; /* default value */
+	}
+	_naca->smt_snooze_delay = my_smt_snooze_delay;
+	_naca->smt_state = my_smt_enabled;
+}
 
 /*
  * We enter here early on, when the Open Firmware prom is still
@@ -1060,7 +1214,7 @@ prom_init(unsigned long r3, unsigned lon
 	  unsigned long r6, unsigned long r7)
 {
 	unsigned long mem;
-	ihandle prom_root, prom_cpu;
+	ihandle prom_mmu,prom_root, prom_cpu;
 	phandle cpu_pkg;
 	unsigned long offset = reloc_offset();
 	long l;
@@ -1092,12 +1246,12 @@ prom_init(unsigned long r3, unsigned lon
 				       RELOC("/chosen"));
 
 	if ((long)_prom->chosen <= 0)
-		prom_exit();
+		prom_panic(RELOC("cannot find chosen")); /* msg won't be printed :( */
 
         if ((long)call_prom(RELOC("getprop"), 4, 1, _prom->chosen,
 			    RELOC("stdout"), &getprop_rval,
 			    sizeof(getprop_rval)) <= 0)
-                prom_exit();
+                prom_panic(RELOC("cannot find stdout"));
 
         _prom->stdout = (ihandle)(unsigned long)getprop_rval;
 
@@ -1123,7 +1277,7 @@ prom_init(unsigned long r3, unsigned lon
         if ((long)call_prom(RELOC("getprop"), 4, 1, _prom->chosen,
 			    RELOC("cpu"), &getprop_rval,
 			    sizeof(getprop_rval)) <= 0)
-                prom_exit();
+                prom_panic(RELOC("cannot find boot cpu"));
 
 	prom_cpu = (ihandle)(unsigned long)getprop_rval;
 	cpu_pkg = call_prom(RELOC("instance-to-package"), 1, 1, prom_cpu);
@@ -1131,11 +1285,9 @@ prom_init(unsigned long r3, unsigned lon
 		cpu_pkg, RELOC("reg"),
 		&getprop_rval, sizeof(getprop_rval));
 	_prom->cpu = (int)(unsigned long)getprop_rval;
-	_xPaca[_prom->cpu].active = 1;
-#ifdef CONFIG_SMP
-	cpu_set(_prom->cpu, RELOC(cpu_online_map));
-#endif
-	RELOC(boot_cpuid) = _prom->cpu;
+	_xPaca[0].xHwProcNum = _prom->cpu;
+
+	RELOC(boot_cpuid) = 0;
 
 #ifdef DEBUG_PROM
   	prom_print(RELOC("Booting CPU hw index = 0x"));
@@ -1158,6 +1310,15 @@ prom_init(unsigned long r3, unsigned lon
 		mem = DOUBLEWORD_ALIGN(mem + strlen(d) + 1);
 	}
 
+	RELOC(cmd_line[0]) = 0;
+	if ((long)_prom->chosen > 0) {
+		call_prom(RELOC("getprop"), 4, 1, _prom->chosen, 
+			  RELOC("bootargs"), p, sizeof(cmd_line));
+		if (p != NULL && p[0] != 0)
+			strncpy(RELOC(cmd_line), p, sizeof(cmd_line));
+	}
+	RELOC(cmd_line[sizeof(cmd_line) - 1]) = 0;
+
 	mem = prom_initialize_lmb(mem);
 
 	mem = prom_bi_rec_reserve(mem);
@@ -1169,11 +1330,12 @@ prom_init(unsigned long r3, unsigned lon
         /* Initialize some system info into the Naca early... */
         mem = prom_initialize_naca(mem);
 
+	smt_setup();
+	
         /* If we are on an SMP machine, then we *MUST* do the
          * following, regardless of whether we have an SMP
          * kernel or not.
          */
-        if (_systemcfg->processorCount > 1)
 	        prom_hold_cpus(mem);
 
 #ifdef DEBUG_PROM
@@ -1188,9 +1350,34 @@ prom_init(unsigned long r3, unsigned lon
 	if (_systemcfg->platform == PLATFORM_PSERIES)
 		prom_initialize_tce_table();
 
-	prom_print(RELOC("Calling quiesce ...\n"));
-	call_prom(RELOC("quiesce"), 0, 0);
-	phys = KERNELBASE - offset;
+ 	if ((long) call_prom(RELOC("getprop"), 4, 1,
+				_prom->chosen,
+				RELOC("mmu"),
+				&getprop_rval,
+				sizeof(getprop_rval)) <= 0) {	
+                prom_panic(RELOC(" no MMU found\n"));
+	}
+
+	/* We assume the phys. address size is 3 cells */
+	RELOC(prom_mmu) = (ihandle)(unsigned long)getprop_rval;
+
+	if ((long)call_prom(RELOC("call-method"), 4, 4,
+				RELOC("translate"),
+				prom_mmu,
+				(void *)(KERNELBASE - offset),
+				(void *)1) != 0) {
+		prom_print(RELOC(" (translate failed) "));
+	} else {
+		prom_print(RELOC(" (translate ok) "));
+		phys = (unsigned long)_prom->args.rets[3];
+	}
+
+	/* If OpenFirmware version >= 3, then use quiesce call */
+	if (_prom->version >= 3) {
+		prom_print(RELOC("Calling quiesce ...\n"));
+		call_prom(RELOC("quiesce"), 0, 0);
+		phys = KERNELBASE - offset;
+	}
 
 	prom_print(RELOC("returning from prom_init\n"));
 	return phys;
@@ -1308,46 +1495,6 @@ check_display(unsigned long mem)
 	return DOUBLEWORD_ALIGN(mem);
 }
 
-void
-virt_irq_init(void)
-{
-	int i;
-	for (i = 0; i < NR_IRQS; i++)
-		virt_irq_to_real_map[i] = UNDEFINED_IRQ;
-	for (i = 0; i < NR_HW_IRQS; i++)
-		real_irq_to_virt_map[i] = UNDEFINED_IRQ;
-}
-
-/* Create a mapping for a real_irq if it doesn't already exist.
- * Return the virtual irq as a convenience.
- */
-unsigned long
-virt_irq_create_mapping(unsigned long real_irq)
-{
-	unsigned long virq;
-	if (naca->interrupt_controller == IC_OPEN_PIC)
-		return real_irq;	/* no mapping for openpic (for now) */
-	virq = real_irq_to_virt(real_irq);
-	if (virq == UNDEFINED_IRQ) {
-		/* Assign a virtual IRQ number */
-		if (real_irq < NR_IRQS && virt_irq_to_real(real_irq) == UNDEFINED_IRQ) {
-			/* A 1-1 mapping will work. */
-			virq = real_irq;
-		} else {
-			while (last_virt_irq < NR_IRQS &&
-			       virt_irq_to_real(++last_virt_irq) != UNDEFINED_IRQ)
-				/* skip irq's in use */;
-			if (last_virt_irq >= NR_IRQS)
-				panic("Too many IRQs are required on this system.  NR_IRQS=%d\n", NR_IRQS);
-			virq = last_virt_irq;
-		}
-		virt_irq_to_real_map[virq] = real_irq;
-		real_irq_to_virt_map[real_irq] = virq;
-	}
-	return virq;
-}
-
-
 static int __init
 prom_next_node(phandle *nodep)
 {
@@ -1381,8 +1528,7 @@ copy_device_tree(unsigned long mem_start
 
 	root = call_prom(RELOC("peer"), 1, 1, (phandle)0);
 	if (root == (phandle)0) {
-		prom_print(RELOC("couldn't get device tree root\n"));
-		prom_exit();
+		prom_panic(RELOC("couldn't get device tree root\n"));
 	}
 	allnextp = &RELOC(allnodes);
 	mem_start = DOUBLEWORD_ALIGN(mem_start);
@@ -1444,6 +1590,23 @@ inspect_node(phandle node, struct device
 		*prev_propp = PTRUNRELOC(pp);
 		prev_propp = &pp->next;
 	}
+
+	/* Add a "linux_phandle" value */
+        if (np->node) {
+		u32 ibm_phandle = 0;
+		int len;
+
+                /* First see if "ibm,phandle" exists and use its value */
+                len = (int)
+                        call_prom(RELOC("getprop"), 4, 1, node, RELOC("ibm,phandle"),
+                                  &ibm_phandle, sizeof(ibm_phandle));
+                if (len < 0) {
+                        np->linux_phandle = np->node;
+                } else {
+                        np->linux_phandle = ibm_phandle;
+		}
+	}
+
 	*prev_propp = 0;
 
 	/* get the node's full name */
@@ -1477,8 +1640,6 @@ finish_device_tree(void)
 {
 	unsigned long mem = klimit;
 
-	virt_irq_init();
-
 	mem = finish_node(allnodes, mem, NULL, 0, 0);
 	dev_tree_size = mem - (unsigned long) allnodes;
 
@@ -1487,7 +1648,7 @@ finish_device_tree(void)
 
 	klimit = mem;
 
-	rtas.dev = find_devices("rtas");
+	rtas.dev = of_find_node_by_name(NULL, "rtas");
 }
 
 static unsigned long __init
@@ -1540,7 +1701,7 @@ finish_node(struct device_node *np, unsi
 /*
  * Find the interrupt parent of a node.
  */
-static struct device_node * __init
+static struct device_node * __devinit
 intr_parent(struct device_node *p)
 {
 	phandle *parp;
@@ -1555,7 +1716,7 @@ intr_parent(struct device_node *p)
  * Find out the size of each entry of the interrupts property
  * for a node.
  */
-static int __init
+static int __devinit
 prom_n_intr_cells(struct device_node *np)
 {
 	struct device_node *p;
@@ -1583,7 +1744,7 @@ prom_n_intr_cells(struct device_node *np
  * Map an interrupt from a device up to the platform interrupt
  * descriptor.
  */
-static int __init
+static int __devinit
 map_interrupt(unsigned int **irq, struct device_node **ictrler,
 	      struct device_node *np, unsigned int *ints, int nintrc)
 {
@@ -1704,7 +1865,7 @@ finish_node_interrupts(struct device_nod
 		n = map_interrupt(&irq, &ic, np, ints, intrcells);
 		if (n <= 0)
 			continue;
-		np->intrs[i].line = openpic_to_irq(virt_irq_create_mapping(irq[0]));
+		np->intrs[i].line = irq_offset_up(irq[0]);
 		if (n > 1)
 			np->intrs[i].sense = irq[1];
 		if (n > 2) {
@@ -1939,11 +2100,14 @@ int
 machine_is_compatible(const char *compat)
 {
 	struct device_node *root;
-	
-	root = find_path_device("/");
-	if (root == 0)
-		return 0;
-	return device_is_compatible(root, compat);
+	int rc = 0;
+  
+	root = of_find_node_by_path("/");
+	if (root) {
+		rc = device_is_compatible(root, compat);
+		of_node_put(root);
+	}
+	return rc;
 }
 
 /*
@@ -1983,16 +2147,560 @@ find_path_device(const char *path)
 	return NULL;
 }
 
+/*******
+ *
+ * New implementation of the OF "find" APIs, return a refcounted
+ * object, call of_node_put() when done.  The device tree and list
+ * are protected by a rw_lock.
+ *
+ * Note that property management will need some locking as well,
+ * this isn't dealt with yet.
+ *
+ *******/
+
+/**
+ *	of_find_node_by_name - Find a node by its "name" property
+ *	@from:	The node to start searching from or NULL, the node
+ *		you pass will not be searched, only the next one
+ *		will; typically, you pass what the previous call
+ *		returned. of_node_put() will be called on it
+ *	@name:	The name string to match against
+ *
+ *	Returns a node pointer with refcount incremented, use
+ *	of_node_put() on it when done.
+ */
+struct device_node *of_find_node_by_name(struct device_node *from,
+	const char *name)
+{
+	struct device_node *np;
+
+	read_lock(&devtree_lock);
+	np = from ? from->allnext : allnodes;
+	for (; np != 0; np = np->allnext)
+		if (np->name != 0 && strcasecmp(np->name, name) == 0
+		    && of_node_get(np))
+			break;
+	if (from)
+		of_node_put(from);
+	read_unlock(&devtree_lock);
+	return np;
+}
+
+/**
+ *	of_find_node_by_type - Find a node by its "device_type" property
+ *	@from:	The node to start searching from or NULL, the node
+ *		you pass will not be searched, only the next one
+ *		will; typically, you pass what the previous call
+ *		returned. of_node_put() will be called on it
+ *	@name:	The type string to match against
+ *
+ *	Returns a node pointer with refcount incremented, use
+ *	of_node_put() on it when done.
+ */
+struct device_node *of_find_node_by_type(struct device_node *from,
+	const char *type)
+{
+	struct device_node *np;
+
+	read_lock(&devtree_lock);
+	np = from ? from->allnext : allnodes;
+	for (; np != 0; np = np->allnext)
+		if (np->type != 0 && strcasecmp(np->type, type) == 0
+		    && of_node_get(np))
+			break;
+	if (from)
+		of_node_put(from);
+	read_unlock(&devtree_lock);
+	return np;
+}
+EXPORT_SYMBOL(of_find_node_by_type);
+
+/**
+ *	of_find_compatible_node - Find a node based on type and one of the
+ *                                tokens in its "compatible" property
+ *	@from:		The node to start searching from or NULL, the node
+ *			you pass will not be searched, only the next one
+ *			will; typically, you pass what the previous call
+ *			returned. of_node_put() will be called on it
+ *	@type:		The type string to match "device_type" or NULL to ignore
+ *	@compatible:	The string to match to one of the tokens in the device
+ *			"compatible" list.
+ *
+ *	Returns a node pointer with refcount incremented, use
+ *	of_node_put() on it when done.
+ */
+struct device_node *of_find_compatible_node(struct device_node *from,
+	const char *type, const char *compatible)
+{
+	struct device_node *np;
+
+	read_lock(&devtree_lock);
+	np = from ? from->allnext : allnodes;
+	for (; np != 0; np = np->allnext) {
+		if (type != NULL
+		    && !(np->type != 0 && strcasecmp(np->type, type) == 0))
+			continue;
+		if (device_is_compatible(np, compatible) && of_node_get(np))
+			break;
+	}
+	if (from)
+		of_node_put(from);
+	read_unlock(&devtree_lock);
+	return np;
+}
+
+/**
+ *	of_find_node_by_path - Find a node matching a full OF path
+ *	@path:	The full path to match
+ *
+ *	Returns a node pointer with refcount incremented, use
+ *	of_node_put() on it when done.
+ */
+struct device_node *of_find_node_by_path(const char *path)
+{
+	struct device_node *np = allnodes;
+
+	read_lock(&devtree_lock);
+	for (; np != 0; np = np->allnext)
+		if (np->full_name != 0 && strcasecmp(np->full_name, path) == 0
+		    && of_node_get(np))
+			break;
+	read_unlock(&devtree_lock);
+	return np;
+}
+
+/**
+ *	of_find_all_nodes - Get next node in global list
+ *	@prev:	Previous node or NULL to start iteration
+ *		of_node_put() will be called on it
+ *
+ *	Returns a node pointer with refcount incremented, use
+ *	of_node_put() on it when done.
+ */
+struct device_node *of_find_all_nodes(struct device_node *prev)
+{
+	struct device_node *np;
+
+	read_lock(&devtree_lock);
+	np = prev ? prev->allnext : allnodes;
+	for (; np != 0; np = np->allnext)
+		if (of_node_get(np))
+			break;
+	if (prev)
+		of_node_put(prev);
+	read_unlock(&devtree_lock);
+	return np;
+}
+
+/**
+ *	of_get_parent - Get a node's parent if any
+ *	@node:	Node to get parent
+ *
+ *	Returns a node pointer with refcount incremented, use
+ *	of_node_put() on it when done.
+ */
+struct device_node *of_get_parent(const struct device_node *node)
+{
+	struct device_node *np;
+
+	if (!node)
+		return NULL;
+
+	read_lock(&devtree_lock);
+	np = of_node_get(node->parent);
+	read_unlock(&devtree_lock);
+	return np;
+}
+
+/**
+ *	of_get_next_child - Iterate a node childs
+ *	@node:	parent node
+ *	@prev:	previous child of the parent node, or NULL to get first
+ *
+ *	Returns a node pointer with refcount incremented, use
+ *	of_node_put() on it when done.
+ */
+struct device_node *of_get_next_child(const struct device_node *node,
+	struct device_node *prev)
+{
+	struct device_node *next;
+
+	read_lock(&devtree_lock);
+	next = prev ? prev->sibling : node->child;
+	for (; next != 0; next = next->sibling)
+		if (of_node_get(next))
+			break;
+	if (prev)
+		of_node_put(prev);
+	read_unlock(&devtree_lock);
+	return next;
+}
+
+/**
+ *	of_node_get - Increment refcount of a node
+ *	@node:	Node to inc refcount, NULL is supported to
+ *		simplify writing of callers
+ *
+ *	Returns the node itself or NULL if gone.
+ */
+struct device_node *of_node_get(struct device_node *node)
+{
+	if (node && !OF_IS_STALE(node)) {
+		atomic_inc(&node->_users);
+		return node;
+	}
+	return NULL;
+}
+
+/**
+ *	of_node_put - Decrement refcount of a node
+ *	@node:	Node to dec refcount, NULL is supported to
+ *		simplify writing of callers
+ *
+ */
+void of_node_put(struct device_node *node)
+{
+	if (!node)
+		return;
+
+	WARN_ON(0 == atomic_read(&node->_users));
+
+	if (OF_IS_STALE(node)) {
+		if (atomic_dec_and_test(&node->_users)) {
+			of_node_cleanup(node);
+			return;
+		}
+	}
+	else
+		atomic_dec(&node->_users);
+}
+
+/**
+ *	of_node_cleanup - release a dynamically allocated node
+ *	@arg:  Node to be released
+ */
+static void of_node_cleanup(struct device_node *node)
+{
+	struct property *prop = node->properties;
+
+	if (!OF_IS_DYNAMIC(node))
+		return;
+	while (prop) {
+		struct property *next = prop->next;
+		kfree(prop->name);
+		kfree(prop->value);
+		kfree(prop);
+		prop = next;
+	}
+	kfree(node->intrs);
+	kfree(node->addrs);
+	kfree(node->full_name);
+	kfree(node);
+}
+
+/**
+ *	derive_parent - basically like dirname(1)
+ *	@path:  the full_name of a node to be added to the tree
+ *
+ *	Returns the node which should be the parent of the node
+ *	described by path.  E.g., for path = "/foo/bar", returns
+ *	the node with full_name = "/foo".
+ */
+static struct device_node *derive_parent(const char *path)
+{
+	struct device_node *parent = NULL;
+	char *parent_path = "/";
+	size_t parent_path_len = strrchr(path, '/') - path + 1;
+
+	/* reject if path is "/" */
+	if (!strcmp(path, "/"))
+		return NULL;
+
+	if (strrchr(path, '/') != path) {
+		parent_path = kmalloc(parent_path_len, GFP_KERNEL);
+		if (!parent_path)
+			return NULL;
+		strlcpy(parent_path, path, parent_path_len);
+	}
+	parent = of_find_node_by_path(parent_path);
+	if (strcmp(parent_path, "/"))
+		kfree(parent_path);
+	return parent;
+}
+
+/*
+ * Routines for "runtime" addition and removal of device tree nodes.
+ */
+
+/*
+ * Given a path and a property list, construct an OF device node, add
+ * it to the device tree and global list, and place it in
+ * /proc/device-tree.  This function may sleep.
+ */
+int of_add_node(const char *path, struct property *proplist)
+{
+	struct device_node *np;
+	int err = 0;
+
+	np = kmalloc(sizeof(struct device_node), GFP_KERNEL);
+	if (!np)
+		return -ENOMEM;
+
+	memset(np, 0, sizeof(*np));
+
+	np->full_name = kmalloc(strlen(path) + 1, GFP_KERNEL);
+	if (!np->full_name) {
+		kfree(np);
+		return -ENOMEM;
+	}
+	strcpy(np->full_name, path);
+
+	np->properties = proplist;
+	OF_MARK_DYNAMIC(np);
+	of_node_get(np);
+	np->parent = derive_parent(path);
+	if (!np->parent) {
+		kfree(np);
+		return -EINVAL; /* could also be ENOMEM, though */
+	}
+
+	if (0 != (err = of_finish_dynamic_node(np))) {
+		kfree(np);
+		return err;
+	}
+
+	write_lock(&devtree_lock);
+	np->sibling = np->parent->child;
+	np->allnext = allnodes;
+	np->parent->child = np;
+	allnodes = np;
+	write_unlock(&devtree_lock);
+
+	add_node_proc_entries(np);
+
+	of_node_put(np->parent);
+	of_node_put(np);
+	return 0;
+}
+
+/*
+ * Remove an OF device node from the system.
+ */
+int of_remove_node(struct device_node *np)
+{
+	struct device_node *parent, *child;
+
+	parent = of_get_parent(np);
+	child = of_get_next_child(np, NULL);
+	if (child && !child->child && !child->sibling) {
+		/* For now, we will allow removal of a
+		 * node with one and only one child, so
+		 * that we can support removing a slot with
+		 * an IOA in it.  More general support for
+		 * subtree removal to be implemented later, if
+		 * necessary.
+		 */
+		of_remove_node(child);
+	}
+	else if (child) {
+		of_node_put(child);
+		of_node_put(parent);
+		return -EINVAL;
+	}
+	of_node_put(child);
+
+	write_lock(&devtree_lock);
+	OF_MARK_STALE(np);
+	remove_node_proc_entries(np);
+	if (allnodes == np)
+		allnodes = np->allnext;
+	else {
+		struct device_node *prev;
+		for (prev = allnodes;
+		     prev->allnext != np;
+		     prev = prev->allnext)
+			;
+		prev->allnext = np->allnext;
+	}
+
+	if (np->parent->child == np)
+		np->parent->child = np->sibling;
+	else {
+		struct device_node *prevsib;
+		for (prevsib = np->parent->child;
+		     prevsib->sibling != np;
+		     prevsib = prevsib->sibling)
+			;
+		prevsib->sibling = np->sibling;
+	}
+	write_unlock(&devtree_lock);
+	of_node_put(parent);
+	return 0;
+}
+
+/*
+ * Add a node to /proc/device-tree.
+ */
+static void add_node_proc_entries(struct device_node *np)
+{
+	struct proc_dir_entry *ent;
+
+	ent = proc_mkdir(strrchr(np->full_name, '/') + 1, np->parent->pde);
+	if (ent)
+		proc_device_tree_add_node(np, ent);
+}
+
+static void remove_node_proc_entries(struct device_node *np)
+{
+	struct property *pp = np->properties;
+	struct device_node *parent = np->parent;
+
+	while (pp) {
+		remove_proc_entry(pp->name, np->pde);
+		pp = pp->next;
+	}
+
+	/* Assuming that symlinks have the same parent directory as
+	 * np->pde.
+	 */
+	if (np->name_link)
+		remove_proc_entry(np->name_link->name, parent->pde);
+	if (np->addr_link)
+		remove_proc_entry(np->addr_link->name, parent->pde);
+	if (np->pde)
+		remove_proc_entry(np->pde->name, parent->pde);
+}
+
+/*
+ * Fix up the uninitialized fields in a new device node:
+ * name, type, n_addrs, addrs, n_intrs, intrs, and pci-specific fields
+ *
+ * A lot of boot-time code is duplicated here, because functions such
+ * as finish_node_interrupts, interpret_pci_props, etc. cannot use the
+ * slab allocator.
+ *
+ * This should probably be split up into smaller chunks.
+ */
+
+static int of_finish_dynamic_node(struct device_node *node)
+{
+	struct device_node *parent = of_get_parent(node);
+	u32 *regs;
+	unsigned int *ints;
+	int intlen, intrcells;
+	int i, j, n, err = 0;
+	unsigned int *irq;
+	struct device_node *ic;
+ 
+	node->name = get_property(node, "name", 0);
+	node->type = get_property(node, "device_type", 0);
+
+	if (!parent) {
+		err = -ENODEV;
+		goto out;
+	}
+
+	/* do the work of interpret_pci_props */
+	if (parent->type && !strcmp(parent->type, "pci")) {
+		struct address_range *adr;
+		struct pci_reg_property *pci_addrs;
+		int i, l;
+
+		pci_addrs = (struct pci_reg_property *)
+			get_property(node, "assigned-addresses", &l);
+		if (pci_addrs != 0 && l >= sizeof(struct pci_reg_property)) {
+			i = 0;
+			adr = kmalloc(sizeof(struct address_range) * 
+				      (l / sizeof(struct pci_reg_property)),
+				      GFP_KERNEL);
+			if (!adr) {
+				err = -ENOMEM;
+				goto out;
+			}
+			while ((l -= sizeof(struct pci_reg_property)) >= 0) {
+				adr[i].space = pci_addrs[i].addr.a_hi;
+				adr[i].address = pci_addrs[i].addr.a_lo;
+				adr[i].size = pci_addrs[i].size_lo;
+				++i;
+			}
+			node->addrs = adr;
+			node->n_addrs = i;
+		}
+	}
+
+	/* now do the work of finish_node_interrupts */
+
+	ints = (unsigned int *) get_property(node, "interrupts", &intlen);
+	if (!ints)
+		goto out;
+
+	intrcells = prom_n_intr_cells(node);
+	intlen /= intrcells * sizeof(unsigned int);
+	node->n_intrs = intlen;
+	node->intrs = kmalloc(sizeof(struct interrupt_info) * intlen,
+			      GFP_KERNEL);
+	if (!node->intrs) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	for (i = 0; i < intlen; ++i) {
+		node->intrs[i].line = 0;
+		node->intrs[i].sense = 1;
+		n = map_interrupt(&irq, &ic, node, ints, intrcells);
+		if (n <= 0)
+			continue;
+		node->intrs[i].line = irq_offset_up(irq[0]);
+		if (n > 1)
+			node->intrs[i].sense = irq[1];
+		if (n > 2) {
+			printk(KERN_DEBUG "hmmm, got %d intr cells for %s:", n,
+			       node->full_name);
+			for (j = 0; j < n; ++j)
+				printk(" %d", irq[j]);
+			printk("\n");
+		}
+		ints += intrcells;
+	}
+
+       /* now do the rough equivalent of update_dn_pci_info, this
+        * probably is not correct for phb's, but should work for
+	* IOAs and slots.
+        */
+
+       node->phb = parent->phb;
+
+       regs = (u32 *)get_property(node, "reg", 0);
+       if (regs) {
+               node->busno = (regs[0] >> 16) & 0xff;
+               node->devfn = (regs[0] >> 8) & 0xff;
+       }
+
+	/* fixing up tce_table */
+
+	if(strcmp(node->name, "pci") == 0 &&
+                get_property(node, "ibm,dma-window", NULL)) {
+                node->bussubno = node->busno;
+                create_pci_bus_tce_table((unsigned long)node);
+        }
+	else
+		node->tce_table = parent->tce_table;
+
+out:
+	of_node_put(parent);
+	return err;
+}
+
 /*
  * Find the device_node with a given phandle.
  */
-static struct device_node * __init
+static struct device_node * __devinit
 find_phandle(phandle ph)
 {
 	struct device_node *np;
 
 	for (np = allnodes; np != 0; np = np->allnext)
-		if (np->node == ph)
+		if (np->linux_phandle == ph)
 			return np;
 	return NULL;
 }
@@ -2082,17 +2790,6 @@ print_properties(struct device_node *np)
 #endif
 
 
-void __init
-abort()
-{
-#ifdef CONFIG_XMON
-	xmon(NULL);
-#endif
-	for (;;)
-		prom_exit();
-}
-
-
 /* Verify bi_recs are good */
 static struct bi_record *
 prom_bi_rec_verify(struct bi_record *bi_recs)
diff -purN linux-2.5/arch/ppc64/kernel/ras.c linuxppc64-2.5/arch/ppc64/kernel/ras.c
--- linux-2.5/arch/ppc64/kernel/ras.c	2003-06-07 01:59:39.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/ras.c	2003-11-21 04:51:10.000000000 +0000
@@ -1,4 +1,3 @@
-
 /*
  * ras.c
  * Copyright (C) 2001 Dave Engebretsen IBM Corporation
@@ -70,27 +69,29 @@ static int __init init_ras_IRQ(void)
 	struct device_node *np;
 	unsigned int *ireg, len, i;
 
-	if((np = find_path_device("/event-sources/internal-errors")) &&
-	   (ireg = (unsigned int *)get_property(np, "open-pic-interrupt", 
-						&len))) {
-		for(i=0; i<(len / sizeof(*ireg)); i++) {
-			request_irq(virt_irq_create_mapping(*(ireg)) + NUM_8259_INTERRUPTS, 
+	if ((np = of_find_node_by_path("/event-sources/internal-errors")) &&
+	    (ireg = (unsigned int *)get_property(np, "open-pic-interrupt",
+						 &len))) {
+		for (i=0; i<(len / sizeof(*ireg)); i++) {
+			request_irq(irq_offset_up(*(ireg)), 
 				    ras_error_interrupt, 0, 
 				    "RAS_ERROR", NULL);
 			ireg++;
 		}
 	}
+	of_node_put(np);
 
-	if((np = find_path_device("/event-sources/epow-events")) &&
-	   (ireg = (unsigned int *)get_property(np, "open-pic-interrupt", 
-						&len))) {
-		for(i=0; i<(len / sizeof(*ireg)); i++) {
-			request_irq(virt_irq_create_mapping(*(ireg)) + NUM_8259_INTERRUPTS, 
+	if ((np = of_find_node_by_path("/event-sources/epow-events")) &&
+	    (ireg = (unsigned int *)get_property(np, "open-pic-interrupt",
+						 &len))) {
+		for (i=0; i<(len / sizeof(*ireg)); i++) {
+			request_irq(irq_offset_up(*(ireg)),
 				    ras_epow_interrupt, 0, 
 				    "RAS_EPOW", NULL);
 			ireg++;
 		}
 	}
+	of_node_put(np);
 
 	return 1;
 }
@@ -112,7 +113,7 @@ ras_epow_interrupt(int irq, void *dev_id
 
 	status = rtas_call(rtas_token("check-exception"), 6, 1, NULL, 
 			   0x500, irq, 
-			   EPOW_WARNING | POWERMGM_EVENTS, 
+			   RTAS_EPOW_WARNING | RTAS_POWERMGM_EVENTS, 
 			   1,  /* Time Critical */
 			   __pa(&log_entry), size);
 
@@ -120,6 +121,10 @@ ras_epow_interrupt(int irq, void *dev_id
 		    *((unsigned long *)&log_entry), status); 
 	printk(KERN_WARNING 
 		"EPOW <0x%lx 0x%lx>\n",*((unsigned long *)&log_entry), status);
+
+	/* format and print the extended information */
+	log_error((char *)&log_entry, ERR_TYPE_RTAS_LOG, 0);
+	
 	return IRQ_HANDLED;
 }
 
@@ -137,15 +142,23 @@ ras_error_interrupt(int irq, void *dev_i
 	struct rtas_error_log log_entry;
 	unsigned int size = sizeof(log_entry);
 	long status = 0xdeadbeef;
+	int fatal;
 
 	status = rtas_call(rtas_token("check-exception"), 6, 1, NULL, 
 			   0x500, irq, 
-			   INTERNAL_ERROR, 
+			   RTAS_INTERNAL_ERROR, 
 			   1, /* Time Critical */
 			   __pa(&log_entry), size);
 
-	if((status != 1) && 
-	   (log_entry.severity >= SEVERITY_ERROR_SYNC)) {
+	if ((status == 0) && (log_entry.severity >= SEVERITY_ERROR_SYNC)) 
+		fatal = 1;
+	else
+		fatal = 0;
+
+	/* format and print the extended information */
+	log_error((char *)&log_entry, ERR_TYPE_RTAS_LOG, fatal); 
+
+	if (fatal) {
 		udbg_printf("HW Error <0x%lx 0x%lx>\n",
 			    *((unsigned long *)&log_entry), status);
 		printk(KERN_EMERG 
@@ -155,6 +168,7 @@ ras_error_interrupt(int irq, void *dev_i
 #ifndef DEBUG
 		/* Don't actually power off when debugging so we can test
 		 * without actually failing while injecting errors.
+		 * Error data will not be logged to syslog.
 		 */
 		ppc_md.power_off();
 #endif
diff -purN linux-2.5/arch/ppc64/kernel/rtas-proc.c linuxppc64-2.5/arch/ppc64/kernel/rtas-proc.c
--- linux-2.5/arch/ppc64/kernel/rtas-proc.c	2003-04-21 19:42:07.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/rtas-proc.c	2003-12-11 04:12:26.000000000 +0000
@@ -20,6 +20,7 @@
 #include <linux/ctype.h>
 #include <linux/time.h>
 #include <linux/string.h>
+#include <linux/init.h>
 
 #include <asm/uaccess.h>
 #include <asm/bitops.h>
@@ -27,6 +28,7 @@
 #include <asm/io.h>
 #include <asm/prom.h>
 #include <asm/rtas.h>
+#include <asm/proc_fs.h>
 #include <asm/machdep.h> /* for ppc_md */
 #include <asm/time.h>
 
@@ -161,6 +163,8 @@ static ssize_t ppc_rtas_tone_volume_writ
 		size_t count, loff_t *ppos);
 static ssize_t ppc_rtas_tone_volume_read(struct file * file, char * buf,
 		size_t count, loff_t *ppos);
+static ssize_t ppc_rtas_rmo_buf_read(struct file *file, char *buf,
+				    size_t count, loff_t *ppos);
 
 struct file_operations ppc_rtas_poweron_operations = {
 	.read =		ppc_rtas_poweron_read,
@@ -185,6 +189,10 @@ struct file_operations ppc_rtas_tone_vol
 	.write =	ppc_rtas_tone_volume_write
 };
 
+static struct file_operations ppc_rtas_rmo_buf_ops = {
+	.read =		ppc_rtas_rmo_buf_read,
+};
+
 int ppc_rtas_find_all_sensors (void);
 int ppc_rtas_process_sensor(struct individual_sensor s, int state, 
 		int error, char * buf);
@@ -200,39 +208,42 @@ void proc_rtas_init(void)
 {
 	struct proc_dir_entry *entry;
 
-	rtas_node = find_devices("rtas");
+	rtas_node = of_find_node_by_name(NULL, "rtas");
 	if ((rtas_node == NULL) || (systemcfg->platform == PLATFORM_ISERIES_LPAR)) {
 		return;
 	}
 	
-	if (proc_rtas == NULL) {
-		proc_rtas = proc_mkdir("rtas", 0);
+	if (proc_ppc64.rtas == NULL) {
+		proc_ppc64_init();
 	}
 
-	if (proc_rtas == NULL) {
+	if (proc_ppc64.rtas == NULL) {
 		printk(KERN_ERR "Failed to create /proc/rtas in proc_rtas_init\n");
 		return;
 	}
 
 	/* /proc/rtas entries */
 
-	entry = create_proc_entry("progress", S_IRUGO|S_IWUSR, proc_rtas);
+	entry = create_proc_entry("progress", S_IRUGO|S_IWUSR, proc_ppc64.rtas);
 	if (entry) entry->proc_fops = &ppc_rtas_progress_operations;
 
-	entry = create_proc_entry("clock", S_IRUGO|S_IWUSR, proc_rtas); 
+	entry = create_proc_entry("clock", S_IRUGO|S_IWUSR, proc_ppc64.rtas); 
 	if (entry) entry->proc_fops = &ppc_rtas_clock_operations;
 
-	entry = create_proc_entry("poweron", S_IWUSR|S_IRUGO, proc_rtas); 
+	entry = create_proc_entry("poweron", S_IWUSR|S_IRUGO, proc_ppc64.rtas); 
 	if (entry) entry->proc_fops = &ppc_rtas_poweron_operations;
 
-	create_proc_read_entry("sensors", S_IRUGO, proc_rtas, 
+	create_proc_read_entry("sensors", S_IRUGO, proc_ppc64.rtas, 
 			ppc_rtas_sensor_read, NULL);
 	
-	entry = create_proc_entry("frequency", S_IWUSR|S_IRUGO, proc_rtas); 
+	entry = create_proc_entry("frequency", S_IWUSR|S_IRUGO, proc_ppc64.rtas); 
 	if (entry) entry->proc_fops = &ppc_rtas_tone_freq_operations;
 
-	entry = create_proc_entry("volume", S_IWUSR|S_IRUGO, proc_rtas); 
+	entry = create_proc_entry("volume", S_IWUSR|S_IRUGO, proc_ppc64.rtas); 
 	if (entry) entry->proc_fops = &ppc_rtas_tone_volume_operations;
+
+	entry = create_proc_entry("rmo_buffer", S_IRUSR, proc_ppc64.rtas);
+	if (entry) entry->proc_fops = &ppc_rtas_rmo_buf_ops;
 }
 
 /* ****************************************************************** */
@@ -241,12 +252,18 @@ void proc_rtas_init(void)
 static ssize_t ppc_rtas_poweron_write(struct file * file, const char * buf,
 		size_t count, loff_t *ppos)
 {
+	char stkbuf[40];  /* its small, its on stack */
 	struct rtc_time tm;
 	unsigned long nowtime;
 	char *dest;
 	int error;
 
-	nowtime = simple_strtoul(buf, &dest, 10);
+	if (39 < count) count = 39;
+	if (copy_from_user (stkbuf, buf, count)) {
+		return -EFAULT;
+	}
+	stkbuf[count] = 0;
+	nowtime = simple_strtoul(stkbuf, &dest, 10);
 	if (*dest != '\0' && *dest != '\n') {
 		printk("ppc_rtas_poweron_write: Invalid time\n");
 		return count;
@@ -267,18 +284,23 @@ static ssize_t ppc_rtas_poweron_write(st
 static ssize_t ppc_rtas_poweron_read(struct file * file, char * buf,
 		size_t count, loff_t *ppos)
 {
+	char stkbuf[40];  /* its small, its on stack */
 	int n;
 	if (power_on_time == 0)
-		n = sprintf(buf, "Power on time not set\n");
+		n = snprintf(stkbuf, 40, "Power on time not set\n");
 	else
-		n = sprintf(buf, "%lu\n", power_on_time);
+		n = snprintf(stkbuf, 40, "%lu\n", power_on_time);
 
-	if (*ppos >= strlen(buf))
+	int sn = strlen (stkbuf) +1;
+	if (*ppos >= sn)
 		return 0;
-	if (n > strlen(buf) - *ppos)
-		n = strlen(buf) - *ppos;
+	if (n > sn - *ppos)
+		n = sn - *ppos;
 	if (n > count)
 		n = count;
+	if (copy_to_user (buf, stkbuf + (*ppos), n)) {
+		return -EFAULT;
+	}
 	*ppos += n;
 	return n;
 }
@@ -291,11 +313,16 @@ static ssize_t ppc_rtas_progress_write(s
 {
 	unsigned long hex;
 
-	strcpy(progress_led, buf); /* save the string */
+	if (count >= MAX_LINELENGTH) count = MAX_LINELENGTH -1;
+	if (copy_from_user (progress_led, buf, count)) { /* save the string */
+		return -EFAULT;
+	}
+	progress_led[count] = 0;
+
 	/* Lets see if the user passed hexdigits */
-	hex = simple_strtoul(buf, NULL, 10);
-	
-	ppc_md.progress ((char *)buf, hex);
+	hex = simple_strtoul(progress_led, NULL, 10);
+
+	ppc_md.progress ((char *)progress_led, hex);
 	return count;
 
 	/* clear the line */ /* ppc_md.progress("                   ", 0xffff);*/
@@ -305,14 +332,30 @@ static ssize_t ppc_rtas_progress_read(st
 		size_t count, loff_t *ppos)
 {
 	int n = 0;
-	if (progress_led != NULL)
-		n = sprintf (buf, "%s\n", progress_led);
-	if (*ppos >= strlen(buf))
+	
+	if (progress_led == NULL) return 0;
+
+	char * tmpbuf = kmalloc (MAX_LINELENGTH, GFP_KERNEL);
+	if (!tmpbuf) {
+		printk(KERN_ERR "error: kmalloc failed\n");
+		return -ENOMEM;
+	}
+	n = sprintf (tmpbuf, "%s\n", progress_led);
+
+	int sn = strlen (tmpbuf) +1;
+	if (*ppos >= sn) {
+		kfree (tmpbuf);
 		return 0;
-	if (n > strlen(buf) - *ppos)
-		n = strlen(buf) - *ppos;
+	}
+	if (n > sn - *ppos)
+		n = sn - *ppos;
 	if (n > count)
 		n = count;
+	if (copy_to_user (buf, tmpbuf + (*ppos), n)) {
+		kfree (tmpbuf);
+		return -EFAULT;
+	}
+	kfree (tmpbuf);
 	*ppos += n;
 	return n;
 }
@@ -323,12 +366,18 @@ static ssize_t ppc_rtas_progress_read(st
 static ssize_t ppc_rtas_clock_write(struct file * file, const char * buf, 
 		size_t count, loff_t *ppos)
 {
+	char stkbuf[40];  /* its small, its on stack */
 	struct rtc_time tm;
 	unsigned long nowtime;
 	char *dest;
 	int error;
 
-	nowtime = simple_strtoul(buf, &dest, 10);
+	if (39 < count) count = 39;
+	if (copy_from_user (stkbuf, buf, count)) {
+		return -EFAULT;
+	}
+	stkbuf[count] = 0;
+	nowtime = simple_strtoul(stkbuf, &dest, 10);
 	if (*dest != '\0' && *dest != '\n') {
 		printk("ppc_rtas_clock_write: Invalid time\n");
 		return count;
@@ -356,21 +405,27 @@ static ssize_t ppc_rtas_clock_read(struc
 	year = ret[0]; mon  = ret[1]; day  = ret[2];
 	hour = ret[3]; min  = ret[4]; sec  = ret[5];
 
+	char stkbuf[40];  /* its small, its on stack */
+
 	if (error != 0){
 		printk(KERN_WARNING "error: reading the clock returned: %s\n", 
 				ppc_rtas_process_error(error));
-		n = sprintf (buf, "0");
+		n = snprintf (stkbuf, 40, "0");
 	} else { 
-		n = sprintf (buf, "%lu\n", mktime(year, mon, day, hour, min, sec));
+		n = snprintf (stkbuf, 40, "%lu\n", mktime(year, mon, day, hour, min, sec));
 	}
 	kfree(ret);
 
-	if (*ppos >= strlen(buf))
+	int sn = strlen (stkbuf) +1;
+	if (*ppos >= sn)
 		return 0;
-	if (n > strlen(buf) - *ppos)
-		n = strlen(buf) - *ppos;
+	if (n > sn - *ppos)
+		n = sn - *ppos;
 	if (n > count)
 		n = count;
+	if (copy_to_user (buf, stkbuf + (*ppos), n)) {
+		return -EFAULT;
+	}
 	*ppos += n;
 	return n;
 }
@@ -764,7 +819,7 @@ int get_location_code(struct individual_
 		n += check_location_string(ret, buffer + n);
 		n += sprintf ( buffer+n, " ");
 		/* see how many characters we have printed */
-		sprintf ( t, "%s ", ret);
+		snprintf ( t, 50, "%s ", ret);
 
 		pos += strlen(t);
 		if (pos >= llen) pos=0;
@@ -777,10 +832,17 @@ int get_location_code(struct individual_
 static ssize_t ppc_rtas_tone_freq_write(struct file * file, const char * buf,
 		size_t count, loff_t *ppos)
 {
+	char stkbuf[40];  /* its small, its on stack */
 	unsigned long freq;
 	char *dest;
 	int error;
-	freq = simple_strtoul(buf, &dest, 10);
+
+	if (39 < count) count = 39;
+	if (copy_from_user (stkbuf, buf, count)) {
+		return -EFAULT;
+	}
+	stkbuf[count] = 0;
+	freq = simple_strtoul(stkbuf, &dest, 10);
 	if (*dest != '\0' && *dest != '\n') {
 		printk("ppc_rtas_tone_freq_write: Invalid tone freqency\n");
 		return count;
@@ -799,14 +861,19 @@ static ssize_t ppc_rtas_tone_freq_read(s
 		size_t count, loff_t *ppos)
 {
 	int n;
-	n = sprintf(buf, "%lu\n", rtas_tone_frequency);
+	char stkbuf[40];  /* its small, its on stack */
+	n = snprintf(stkbuf, 40, "%lu\n", rtas_tone_frequency);
 
-	if (*ppos >= strlen(buf))
+	int sn = strlen (stkbuf) +1;
+	if (*ppos >= sn)
 		return 0;
-	if (n > strlen(buf) - *ppos)
-		n = strlen(buf) - *ppos;
+	if (n > sn - *ppos)
+		n = sn - *ppos;
 	if (n > count)
 		n = count;
+	if (copy_to_user (buf, stkbuf + (*ppos), n)) {
+		return -EFAULT;
+	}
 	*ppos += n;
 	return n;
 }
@@ -816,10 +883,17 @@ static ssize_t ppc_rtas_tone_freq_read(s
 static ssize_t ppc_rtas_tone_volume_write(struct file * file, const char * buf,
 		size_t count, loff_t *ppos)
 {
+	char stkbuf[40];  /* its small, its on stack */
 	unsigned long volume;
 	char *dest;
 	int error;
-	volume = simple_strtoul(buf, &dest, 10);
+
+	if (39 < count) count = 39;
+	if (copy_from_user (stkbuf, buf, count)) {
+		return -EFAULT;
+	}
+	stkbuf[count] = 0;
+	volume = simple_strtoul(stkbuf, &dest, 10);
 	if (*dest != '\0' && *dest != '\n') {
 		printk("ppc_rtas_tone_volume_write: Invalid tone volume\n");
 		return count;
@@ -840,14 +914,44 @@ static ssize_t ppc_rtas_tone_volume_read
 		size_t count, loff_t *ppos)
 {
 	int n;
-	n = sprintf(buf, "%lu\n", rtas_tone_volume);
+	char stkbuf[40];  /* its small, its on stack */
+	n = snprintf(stkbuf, 40, "%lu\n", rtas_tone_volume);
 
-	if (*ppos >= strlen(buf))
+	int sn = strlen (stkbuf) +1;
+	if (*ppos >= sn)
 		return 0;
-	if (n > strlen(buf) - *ppos)
-		n = strlen(buf) - *ppos;
+	if (n > sn - *ppos)
+		n = sn - *ppos;
 	if (n > count)
 		n = count;
+	if (copy_to_user (buf, stkbuf + (*ppos), n)) {
+		return -EFAULT;
+	}
 	*ppos += n;
 	return n;
 }
+
+#define RMO_READ_BUF_MAX 30
+
+/* RTAS Userspace access */
+static ssize_t ppc_rtas_rmo_buf_read(struct file *file, char __user *buf,
+				    size_t count, loff_t *ppos)
+{
+	char kbuf[RMO_READ_BUF_MAX];
+	int n;
+
+	n = sprintf(kbuf, "%016lx %x\n", rtas_rmo_buf, RTAS_RMOBUF_MAX);
+	if (n > count)
+		n = count;
+
+	if (ppos && *ppos != 0)
+		return 0;
+
+	if (copy_to_user(buf, kbuf, n))
+		return -EFAULT;
+
+	if (ppos)
+		*ppos = n;
+	
+	return n;
+}
diff -purN linux-2.5/arch/ppc64/kernel/rtas.c linuxppc64-2.5/arch/ppc64/kernel/rtas.c
--- linux-2.5/arch/ppc64/kernel/rtas.c	2003-09-02 06:46:42.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/rtas.c	2003-11-21 04:51:10.000000000 +0000
@@ -16,6 +16,7 @@
 #include <linux/types.h>
 #include <linux/spinlock.h>
 #include <linux/module.h>
+#include <linux/init.h>
 
 #include <asm/prom.h>
 #include <asm/proc_fs.h>
@@ -28,6 +29,8 @@
 #include <asm/system.h>
 #include <asm/abs_addr.h>
 #include <asm/udbg.h>
+#include <asm/delay.h>
+#include <asm/uaccess.h>
 
 struct flash_block_list_header rtas_firmware_flash_list = {0, 0};
 
@@ -59,7 +62,7 @@ struct rtas_t rtas = { 
 extern unsigned long reloc_offset(void);
 
 spinlock_t rtas_data_buf_lock = SPIN_LOCK_UNLOCKED;
-char rtas_data_buf[RTAS_DATA_BUF_SIZE];
+char rtas_data_buf[RTAS_DATA_BUF_SIZE]__page_aligned;
 
 void
 phys_call_rtas(int token, int nargs, int nret, ...)
@@ -180,18 +183,18 @@ rtas_call(int token, int nargs, int nret
 }
 
 /* Given an RTAS status code of 990n compute the hinted delay of 10^n
- * (last digit) milliseconds.  For now we bound at n=3 (1 sec).
+ * (last digit) milliseconds.  For now we bound at n=5 (100 sec).
  */
 unsigned int
 rtas_extended_busy_delay_time(int status)
 {
 	int order = status - 9900;
-	unsigned int ms;
+	unsigned long ms;
 
 	if (order < 0)
 		order = 0;	/* RTC depends on this for -2 clock busy */
-	else if (order > 3)
-		order = 3;	/* bound */
+	else if (order > 5)
+		order = 5;	/* bound */
 
 	/* Use microseconds for reasonable accuracy */
 	for (ms = 1000; order > 0; order--)
@@ -199,6 +202,80 @@ rtas_extended_busy_delay_time(int status
 	return ms / (1000000/HZ); /* round down is fine */
 }
 
+int
+rtas_get_power_level(int powerdomain, int *level)
+{
+	int token = rtas_token("get-power-level");
+	long powerlevel;
+	int rc;
+
+	if (token == RTAS_UNKNOWN_SERVICE)
+		return RTAS_UNKNOWN_OP;
+
+	while(1) {
+		rc = (int) rtas_call(token, 1, 2, &powerlevel, powerdomain);
+		if (rc == RTAS_BUSY)
+			udelay(1);
+		else
+			break;
+	}
+	*level = (int) powerlevel;
+	return rc;
+}
+
+int
+rtas_get_sensor(int sensor, int index, int *state)
+{
+	int token = rtas_token("get-sensor-state");
+	unsigned int wait_time;
+	long returned_state;
+	int rc;
+
+	if (token == RTAS_UNKNOWN_SERVICE)
+		return RTAS_UNKNOWN_OP;
+
+	while (1) {
+		rc = (int) rtas_call(token, 2, 2, &returned_state, sensor,
+					index);
+		if (rc == RTAS_BUSY)
+			udelay(1);
+		else if (rtas_is_extended_busy(rc)) {
+			wait_time = rtas_extended_busy_delay_time(rc);
+			udelay(wait_time * 1000);
+		}
+		else
+			break;
+	}
+	*state = (int) returned_state;
+	return rc;
+}
+
+int
+rtas_set_indicator(int indicator, int index, int new_value)
+{
+	int token = rtas_token("set-indicator");
+	unsigned int wait_time;
+	int rc;
+
+	if (token == RTAS_UNKNOWN_SERVICE)
+		return RTAS_UNKNOWN_OP;
+
+	while (1) {
+		rc = (int) rtas_call(token, 3, 1, NULL, indicator, index,
+					new_value);
+		if (rc == RTAS_BUSY)
+			udelay(1);
+		else if (rtas_is_extended_busy(rc)) {
+			wait_time = rtas_extended_busy_delay_time(rc);
+			udelay(wait_time * 1000);
+		}
+		else
+			break;
+	}
+
+	return rc;
+}
+
 #define FLASH_BLOCK_LIST_VERSION (1UL)
 static void
 rtas_flash_firmware(void)
@@ -308,9 +385,51 @@ rtas_halt(void)
         rtas_power_off();
 }
 
-EXPORT_SYMBOL(proc_ppc64);
+unsigned long rtas_rmo_buf = 0;
+
+asmlinkage int ppc_rtas(struct rtas_args __user *uargs)
+{
+	struct rtas_args args;
+	unsigned long flags;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	if (copy_from_user(&args, uargs, 3 * sizeof(u32)) != 0)
+		return -EFAULT;
+
+	if (args.nargs > ARRAY_SIZE(args.args)
+	    || args.nret > ARRAY_SIZE(args.args)
+	    || args.nargs + args.nret > ARRAY_SIZE(args.args))
+		return -EINVAL;
+
+	/* Copy in args. */
+	if (copy_from_user(args.args, uargs->args,
+			   args.nargs * sizeof(rtas_arg_t)) != 0)
+		return -EFAULT;
+
+	spin_lock_irqsave(&rtas.lock, flags);
+	get_paca()->xRtas = args;
+	enter_rtas((void *)__pa((unsigned long)&get_paca()->xRtas));
+	args = get_paca()->xRtas;
+	spin_unlock_irqrestore(&rtas.lock, flags);
+
+	/* Copy out args. */
+	if (copy_to_user(uargs->args + args.nargs,
+			 args.args + args.nargs,
+			 args.nret * sizeof(rtas_arg_t)) != 0)
+		return -EFAULT;
+
+	return 0;
+}
+
+
 EXPORT_SYMBOL(rtas_firmware_flash_list);
 EXPORT_SYMBOL(rtas_token);
 EXPORT_SYMBOL(rtas_call);
 EXPORT_SYMBOL(rtas_data_buf);
 EXPORT_SYMBOL(rtas_data_buf_lock);
+EXPORT_SYMBOL(rtas_extended_busy_delay_time);
+EXPORT_SYMBOL(rtas_get_sensor);
+EXPORT_SYMBOL(rtas_get_power_level);
+EXPORT_SYMBOL(rtas_set_indicator);
diff -purN linux-2.5/arch/ppc64/kernel/rtas_flash.c linuxppc64-2.5/arch/ppc64/kernel/rtas_flash.c
--- linux-2.5/arch/ppc64/kernel/rtas_flash.c	2003-03-26 04:30:59.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/rtas_flash.c	2003-12-11 04:12:26.000000000 +0000
@@ -14,15 +14,64 @@
  */
 
 #include <linux/module.h>
-#include <linux/proc_fs.h>
+
+#include <linux/config.h>
 #include <linux/init.h>
+#include <asm/proc_fs.h>
+#include <asm/delay.h>
 #include <asm/uaccess.h>
 #include <asm/rtas.h>
 
 #define MODULE_VERSION "1.0"
 #define MODULE_NAME "rtas_flash"
 
-#define FIRMWARE_FLASH_NAME "firmware_flash"
+#define FIRMWARE_FLASH_NAME "firmware_flash"   
+#define FIRMWARE_UPDATE_NAME "firmware_update"
+#define MANAGE_FLASH_NAME "manage_flash"
+#define VALIDATE_FLASH_NAME "validate_flash"
+
+/* General RTAS Status Codes */
+#define RTAS_RC_SUCCESS  0
+#define RTAS_RC_HW_ERR	-1
+#define RTAS_RC_BUSY	-2
+
+/* Flash image status values */
+#define FLASH_AUTH           -9002 /* RTAS Not Service Authority Partition */
+#define FLASH_NO_OP          -1099 /* No operation initiated by user */	
+#define FLASH_IMG_SHORT	     -1005 /* Flash image shorter than expected */
+#define FLASH_IMG_BAD_LEN    -1004 /* Bad length value in flash list block */
+#define FLASH_IMG_NULL_DATA  -1003 /* Bad data value in flash list block */
+#define FLASH_IMG_READY      0     /* Firmware img ready for flash on reboot */
+
+/* Manage image status values */
+#define MANAGE_AUTH          -9002 /* RTAS Not Service Authority Partition */
+#define MANAGE_ACTIVE_ERR    -9001 /* RTAS Cannot Overwrite Active Img */
+#define MANAGE_NO_OP         -1099 /* No operation initiated by user */
+#define MANAGE_PARAM_ERR     -3    /* RTAS Parameter Error */
+#define MANAGE_HW_ERR        -1    /* RTAS Hardware Error */
+
+/* Validate image status values */
+#define VALIDATE_AUTH          -9002 /* RTAS Not Service Authority Partition */
+#define VALIDATE_NO_OP         -1099 /* No operation initiated by the user */
+#define VALIDATE_INCOMPLETE    -1002 /* User copied < VALIDATE_BUF_SIZE */
+#define VALIDATE_READY	       -1001 /* Firmware image ready for validation */
+#define VALIDATE_PARAM_ERR     -3    /* RTAS Parameter Error */
+#define VALIDATE_HW_ERR        -1    /* RTAS Hardware Error */
+#define VALIDATE_TMP_UPDATE    0     /* Validate Return Status */
+#define VALIDATE_FLASH_AUTH    1     /* Validate Return Status */
+#define VALIDATE_INVALID_IMG   2     /* Validate Return Status */
+#define VALIDATE_CUR_UNKNOWN   3     /* Validate Return Status */
+#define VALIDATE_TMP_COMMIT_DL 4     /* Validate Return Status */
+#define VALIDATE_TMP_COMMIT    5     /* Validate Return Status */
+#define VALIDATE_TMP_UPDATE_DL 6     /* Validate Return Status */
+
+/* ibm,manage-flash-image operation tokens */
+#define RTAS_REJECT_TMP_IMG   0
+#define RTAS_COMMIT_TMP_IMG   1
+
+/* Array sizes */
+#define VALIDATE_BUF_SIZE 4096    
+#define RTAS_MSG_MAXLEN   64
 
 /* Local copy of the flash block list.
  * We only allow one open of the flash proc file and create this
@@ -34,21 +83,35 @@
  * is treated as the number of entries currently in the block
  * (i.e. not a byte count).  This is all fixed on release.
  */
-static struct flash_block_list *flist;
-static char *flash_msg;
-static int flash_possible;
-
-static int rtas_flash_open(struct inode *inode, struct file *file)
-{
-	if ((file->f_mode & FMODE_WRITE) && flash_possible) {
-		if (flist)
-			return -EBUSY;
-		flist = (struct flash_block_list *)get_zeroed_page(GFP_KERNEL);
-		if (!flist)
-			return -ENOMEM;
-	}
-	return 0;
-}
+
+/* Status int must be first member of struct */
+struct rtas_update_flash_t
+{
+	int status;			/* Flash update status */
+	struct flash_block_list *flist; /* Local copy of flash block list */
+};
+
+/* Status int must be first member of struct */
+struct rtas_manage_flash_t
+{
+	int status;			/* Returned status */
+	unsigned int op;		/* Reject or commit image */
+};
+
+/* Status int must be first member of struct */
+struct rtas_validate_flash_t
+{
+	int status;		 	/* Returned status */	
+	char buf[VALIDATE_BUF_SIZE]; 	/* Candidate image buffer */
+	unsigned int buf_size;		/* Size of image buf */
+	unsigned int update_results;	/* Update results token */
+};
+
+static spinlock_t flash_file_open_lock = SPIN_LOCK_UNLOCKED;
+static struct proc_dir_entry *firmware_flash_pde = NULL;
+static struct proc_dir_entry *firmware_update_pde = NULL;
+static struct proc_dir_entry *validate_pde = NULL;
+static struct proc_dir_entry *manage_pde = NULL;
 
 /* Do simple sanity checks on the flash image. */
 static int flash_list_valid(struct flash_block_list *flist)
@@ -57,32 +120,29 @@ static int flash_list_valid(struct flash
 	int i;
 	unsigned long block_size, image_size;
 
-	flash_msg = NULL;
 	/* Paranoid self test here.  We also collect the image size. */
 	image_size = 0;
 	for (f = flist; f; f = f->next) {
 		for (i = 0; i < f->num_blocks; i++) {
 			if (f->blocks[i].data == NULL) {
-				flash_msg = "error: internal error null data\n";
-				return 0;
+				return FLASH_IMG_NULL_DATA;
 			}
 			block_size = f->blocks[i].length;
 			if (block_size <= 0 || block_size > PAGE_SIZE) {
-				flash_msg = "error: internal error bad length\n";
-				return 0;
+				return FLASH_IMG_BAD_LEN;
 			}
 			image_size += block_size;
 		}
 	}
+
 	if (image_size < (256 << 10)) {
-		if (image_size < 2)
-			flash_msg = NULL;	/* allow "clear" of image */
-		else
-			flash_msg = "error: flash image short\n";
-		return 0;
+		if (image_size < 2) 
+			return FLASH_NO_OP;
 	}
+
 	printk(KERN_INFO "FLASH: flash image with %ld bytes stored for hardware flash on reboot\n", image_size);
-	return 1;
+
+	return FLASH_IMG_READY;
 }
 
 static void free_flash_list(struct flash_block_list *f)
@@ -101,40 +161,80 @@ static void free_flash_list(struct flash
 
 static int rtas_flash_release(struct inode *inode, struct file *file)
 {
-	if (flist) {
-		/* Always clear saved list on a new attempt. */
+	struct proc_dir_entry *dp = PDE(file->f_dentry->d_inode);
+	struct rtas_update_flash_t *uf;
+	
+	uf = (struct rtas_update_flash_t *) dp->data;
+	if (uf->flist) {    
+		/* File was opened in write mode for a new flash attempt */
+		/* Clear saved list */
 		if (rtas_firmware_flash_list.next) {
 			free_flash_list(rtas_firmware_flash_list.next);
 			rtas_firmware_flash_list.next = NULL;
 		}
 
-		if (flash_list_valid(flist))
-			rtas_firmware_flash_list.next = flist;
+		if (uf->status != FLASH_AUTH)  
+			uf->status = flash_list_valid(uf->flist);
+
+		if (uf->status == FLASH_IMG_READY) 
+			rtas_firmware_flash_list.next = uf->flist;
 		else
-			free_flash_list(flist);
-		flist = NULL;
+			free_flash_list(uf->flist);
+
+		uf->flist = NULL;
 	}
+
+	atomic_dec(&dp->count);
 	return 0;
 }
 
+static void get_flash_status_msg(int status, char *buf)
+{
+	char *msg;
+
+	switch (status) {
+	case FLASH_AUTH:
+		msg = "error: this partition does not have service authority\n";
+		break;
+	case FLASH_NO_OP:
+		msg = "info: no firmware image for flash\n";
+		break;
+	case FLASH_IMG_SHORT:
+		msg = "error: flash image short\n";
+		break;
+	case FLASH_IMG_BAD_LEN:
+		msg = "error: internal error bad length\n";
+		break;
+	case FLASH_IMG_NULL_DATA:
+		msg = "error: internal error null data\n";
+		break;
+	case FLASH_IMG_READY:
+		msg = "ready: firmware image ready for flash on reboot\n";
+		break;
+	default:
+		sprintf(buf, "error: unexpected status value %d\n", status);
+		return;
+	}
+
+	strcpy(buf, msg);	
+}
+
 /* Reading the proc file will show status (not the firmware contents) */
 static ssize_t rtas_flash_read(struct file *file, char *buf,
 			       size_t count, loff_t *ppos)
 {
+	struct proc_dir_entry *dp = PDE(file->f_dentry->d_inode);
+	struct rtas_update_flash_t *uf;
+	char msg[RTAS_MSG_MAXLEN];
 	int error;
-	char *msg;
 	int msglen;
 
-	if (!flash_possible) {
-		msg = "error: this partition does not have service authority\n";
-	} else if (flist) {
-		msg = "info: this file is busy for write by some process\n";
-	} else if (flash_msg) {
-		msg = flash_msg;	/* message from last flash attempt */
-	} else if (rtas_firmware_flash_list.next) {
-		msg = "ready: firmware image ready for flash on reboot\n";
-	} else {
-		msg = "info: no firmware image for flash\n";
+	uf = (struct rtas_update_flash_t *) dp->data;
+
+	if (!strcmp(dp->name, FIRMWARE_FLASH_NAME)) {
+		get_flash_status_msg(uf->status, msg);
+	} else {	   /* FIRMWARE_UPDATE_NAME */
+		sprintf(msg, "%d\n", uf->status);
 	}
 	msglen = strlen(msg);
 	if (msglen > count)
@@ -147,7 +247,8 @@ static ssize_t rtas_flash_read(struct fi
 	if (error)
 		return -EINVAL;
 
-	copy_to_user(buf, msg, msglen);
+	if (copy_to_user(buf, msg, msglen))
+		return -EFAULT;
 
 	if (ppos)
 		*ppos = msglen;
@@ -162,14 +263,28 @@ static ssize_t rtas_flash_read(struct fi
 static ssize_t rtas_flash_write(struct file *file, const char *buffer,
 				size_t count, loff_t *off)
 {
-	size_t len = count;
+	struct proc_dir_entry *dp = PDE(file->f_dentry->d_inode);
+	struct rtas_update_flash_t *uf;
 	char *p;
 	int next_free;
-	struct flash_block_list *fl = flist;
+	struct flash_block_list *fl;
+
+	uf = (struct rtas_update_flash_t *) dp->data;
 
-	if (!flash_possible || len == 0)
-		return len;	/* discard data */
+	if (uf->status == FLASH_AUTH || count == 0)
+		return count;	/* discard data */
 
+	/* In the case that the image is not ready for flashing, the memory
+	 * allocated for the block list will be freed upon the release of the 
+	 * proc file
+	 */
+	if (uf->flist == NULL) {
+		uf->flist = (struct flash_block_list *) get_zeroed_page(GFP_KERNEL);
+		if (!uf->flist)
+			return -ENOMEM;
+	}
+
+	fl = uf->flist;
 	while (fl->next)
 		fl = fl->next; /* seek to last block_list for append */
 	next_free = fl->num_blocks;
@@ -182,47 +297,392 @@ static ssize_t rtas_flash_write(struct f
 		next_free = 0;
 	}
 
-	if (len > PAGE_SIZE)
-		len = PAGE_SIZE;
+	if (count > PAGE_SIZE)
+		count = PAGE_SIZE;
 	p = (char *)get_zeroed_page(GFP_KERNEL);
 	if (!p)
 		return -ENOMEM;
-	if(copy_from_user(p, buffer, len)) {
+	
+	if(copy_from_user(p, buffer, count)) {
 		free_page((unsigned long)p);
 		return -EFAULT;
 	}
 	fl->blocks[next_free].data = p;
-	fl->blocks[next_free].length = len;
+	fl->blocks[next_free].length = count;
 	fl->num_blocks++;
 
-	return len;
+	return count;
+}
+
+static int rtas_excl_open(struct inode *inode, struct file *file)
+{
+	struct proc_dir_entry *dp = PDE(inode);
+
+	/* Enforce exclusive open with use count of PDE */
+	spin_lock(&flash_file_open_lock);
+	if (atomic_read(&dp->count) > 1) {
+		spin_unlock(&flash_file_open_lock);
+		return -EBUSY;
+	}
+
+	atomic_inc(&dp->count);
+	spin_unlock(&flash_file_open_lock);
+	
+	return 0;
+}
+
+static int rtas_excl_release(struct inode *inode, struct file *file)
+{
+	struct proc_dir_entry *dp = PDE(inode);
+
+	atomic_dec(&dp->count);
+
+	return 0;
+}
+
+static void manage_flash(struct rtas_manage_flash_t *args_buf)
+{
+	unsigned int wait_time;
+	s32 rc;
+
+	while (1) {
+		rc = (s32) rtas_call(rtas_token("ibm,manage-flash-image"), 1, 
+				1, NULL, (long) args_buf->op);
+		if (rc == RTAS_RC_BUSY)
+			udelay(1);
+		else if (rtas_is_extended_busy(rc)) {
+			wait_time = rtas_extended_busy_delay_time(rc);
+			udelay(wait_time * 1000);
+		} else
+			break;
+	}
+
+	args_buf->status = rc;
+}
+
+static ssize_t manage_flash_read(struct file *file, char *buf,
+			       size_t count, loff_t *ppos)
+{
+	struct proc_dir_entry *dp = PDE(file->f_dentry->d_inode);
+	struct rtas_manage_flash_t *args_buf;
+	char msg[RTAS_MSG_MAXLEN];
+	int msglen;
+	int error;
+
+	args_buf = (struct rtas_manage_flash_t *) dp->data;
+	if (args_buf == NULL)
+		return 0;
+
+	msglen = sprintf(msg, "%d\n", args_buf->status);
+	if (msglen > count)
+		msglen = count;
+
+	if (ppos && *ppos != 0)
+		return 0;	/* be cheap */
+
+	error = verify_area(VERIFY_WRITE, buf, msglen);
+	if (error)
+		return -EINVAL;
+
+	if (copy_to_user(buf, msg, msglen))
+		return -EFAULT;
+
+	if (ppos)
+		*ppos = msglen;
+	return msglen;
+}
+
+static ssize_t manage_flash_write(struct file *file, const char *buf,
+				size_t count, loff_t *off)
+{
+	struct proc_dir_entry *dp = PDE(file->f_dentry->d_inode);
+	struct rtas_manage_flash_t *args_buf;
+	const char reject_str[] = "0";
+	const char commit_str[] = "1";
+	char stkbuf[10];
+	int op;
+
+	args_buf = (struct rtas_manage_flash_t *) dp->data;
+	if ((args_buf->status == MANAGE_AUTH) || (count == 0))
+		return count;
+		
+	op = -1;
+	if (buf) {
+		if (count > 9) count = 9;
+		if (copy_from_user (stkbuf, buf, count)) {
+			return -EFAULT;
+		}
+		if (strncmp(stkbuf, reject_str, strlen(reject_str)) == 0) 
+			op = RTAS_REJECT_TMP_IMG;
+		else if (strncmp(stkbuf, commit_str, strlen(commit_str)) == 0) 
+			op = RTAS_COMMIT_TMP_IMG;
+	}
+	
+	if (op == -1)   /* buf is empty, or contains invalid string */
+		return -EINVAL;
+
+	args_buf->op = op;
+	manage_flash(args_buf);
+
+	return count;
+}
+
+static void validate_flash(struct rtas_validate_flash_t *args_buf)
+{
+	int token = rtas_token("ibm,validate-flash-image");
+	unsigned int wait_time;
+	long update_results;
+	s32 rc;	
+
+	rc = 0;
+	while(1) {
+		spin_lock(&rtas_data_buf_lock);
+		memcpy(rtas_data_buf, args_buf->buf, VALIDATE_BUF_SIZE);
+		rc = (s32) rtas_call(token, 2, 2, &update_results, 
+				     __pa(rtas_data_buf), args_buf->buf_size);
+		memcpy(args_buf->buf, rtas_data_buf, VALIDATE_BUF_SIZE);
+		spin_unlock(&rtas_data_buf_lock);
+			
+		if (rc == RTAS_RC_BUSY)
+			udelay(1);
+		else if (rtas_is_extended_busy(rc)) {
+			wait_time = rtas_extended_busy_delay_time(rc);
+			udelay(wait_time * 1000);
+		} else
+			break;
+	}
+
+	args_buf->status = rc;
+	args_buf->update_results = (u32) update_results;
+}
+
+static int get_validate_flash_msg(struct rtas_validate_flash_t *args_buf, 
+		                   char *msg)
+{
+	int n;
+
+	if (args_buf->status >= VALIDATE_TMP_UPDATE) { 
+		n = sprintf(msg, "%d\n", args_buf->update_results);
+		if ((args_buf->update_results >= VALIDATE_CUR_UNKNOWN) ||
+		    (args_buf->update_results == VALIDATE_TMP_UPDATE))
+			n += sprintf(msg + n, "%s\n", args_buf->buf);
+	} else {
+		n = sprintf(msg, "%d\n", args_buf->status);
+	}
+	return n;
+}
+
+static ssize_t validate_flash_read(struct file *file, char *buf,
+			       size_t count, loff_t *ppos)
+{
+	struct proc_dir_entry *dp = PDE(file->f_dentry->d_inode);
+	struct rtas_validate_flash_t *args_buf;
+	char msg[RTAS_MSG_MAXLEN];
+	int msglen;
+	int error;
+
+	args_buf = (struct rtas_validate_flash_t *) dp->data;
+
+	if (ppos && *ppos != 0)
+		return 0;	/* be cheap */
+	
+	msglen = get_validate_flash_msg(args_buf, msg);
+	if (msglen > count)
+		msglen = count;
+
+	error = verify_area(VERIFY_WRITE, buf, msglen);
+	if (error)
+		return -EINVAL;
+
+	if (copy_to_user(buf, msg, msglen))
+		return -EFAULT;
+
+	if (ppos)
+		*ppos = msglen;
+	return msglen;
+}
+
+static ssize_t validate_flash_write(struct file *file, const char *buf,
+				size_t count, loff_t *off)
+{
+	struct proc_dir_entry *dp = PDE(file->f_dentry->d_inode);
+	struct rtas_validate_flash_t *args_buf;
+	int rc;
+
+	args_buf = (struct rtas_validate_flash_t *) dp->data;
+
+	if (dp->data == NULL) {
+		dp->data = kmalloc(sizeof(struct rtas_validate_flash_t), 
+				GFP_KERNEL);
+		if (dp->data == NULL) 
+			return -ENOMEM;
+	}
+
+	/* We are only interested in the first 4K of the
+	 * candidate image */
+	if ((*off >= VALIDATE_BUF_SIZE) || 
+		(args_buf->status == VALIDATE_AUTH)) {
+		*off += count;
+		return count;
+	}
+
+	if (*off + count >= VALIDATE_BUF_SIZE)  {
+		count = VALIDATE_BUF_SIZE - *off;
+		args_buf->status = VALIDATE_READY;	
+	} else {
+		args_buf->status = VALIDATE_INCOMPLETE;
+	}
+
+	if (verify_area(VERIFY_READ, buf, count)) {
+		rc = -EFAULT;
+		goto done;
+	}
+	if (copy_from_user(args_buf->buf + *off, buf, count)) {
+		rc = -EFAULT;
+		goto done;
+	}
+
+	*off += count;
+	rc = count;
+done:
+	if (rc < 0) {
+		kfree(dp->data);
+		dp->data = NULL;
+	}
+	return rc;
+}
+
+static int validate_flash_release(struct inode *inode, struct file *file)
+{
+	struct proc_dir_entry *dp = PDE(file->f_dentry->d_inode);
+	struct rtas_validate_flash_t *args_buf;
+
+	args_buf = (struct rtas_validate_flash_t *) dp->data;
+
+	if (args_buf->status == VALIDATE_READY) {
+		args_buf->buf_size = VALIDATE_BUF_SIZE;
+		validate_flash(args_buf);
+	}
+
+	atomic_dec(&dp->count);
+
+	return 0;
+}
+
+static inline void remove_flash_pde(struct proc_dir_entry *dp)
+{
+	if (dp) {
+		if (dp->data != NULL)
+			kfree(dp->data);
+		remove_proc_entry(dp->name, proc_ppc64.rtas);
+	}
+}
+
+static inline int initialize_flash_pde_data(const char *rtas_call_name, 
+		                            size_t buf_size,
+					    struct proc_dir_entry *dp)
+{
+	int *status;
+	int token;
+
+	dp->data = kmalloc(buf_size, GFP_KERNEL);
+	if (dp->data == NULL) {
+		remove_flash_pde(dp);
+		return -ENOMEM;
+	}
+
+	memset(dp->data, 0, buf_size);
+
+	/* This code assumes that the status int is the first member of the
+	 * struct 
+	 */
+	status = (int *) dp->data;
+	token = rtas_token(rtas_call_name);
+	if (token == RTAS_UNKNOWN_SERVICE)
+		*status = FLASH_AUTH;
+	else
+		*status = FLASH_NO_OP;
+
+	return 0;
+}
+
+static inline struct proc_dir_entry * create_flash_pde(const char *filename, 
+					struct file_operations *fops)
+{
+	struct proc_dir_entry *ent = NULL;
+
+	ent = create_proc_entry(filename, S_IRUSR | S_IWUSR, proc_ppc64.rtas);
+	if (ent != NULL) {
+		ent->nlink = 1;
+		ent->proc_fops = fops;
+		ent->owner = THIS_MODULE;
+	}
+
+	return ent;
 }
 
 static struct file_operations rtas_flash_operations = {
 	.read		= rtas_flash_read,
 	.write		= rtas_flash_write,
-	.open		= rtas_flash_open,
+	.open		= rtas_excl_open,
 	.release	= rtas_flash_release,
 };
 
+static struct file_operations manage_flash_operations = {
+	.read		= manage_flash_read,
+	.write		= manage_flash_write,
+	.open		= rtas_excl_open,
+	.release	= rtas_excl_release,
+};
+
+static struct file_operations validate_flash_operations = {
+	.read		= validate_flash_read,
+	.write		= validate_flash_write,
+	.open		= rtas_excl_open,
+	.release	= validate_flash_release,
+};
 
 int __init rtas_flash_init(void)
 {
-	struct proc_dir_entry *ent = NULL;
+	int rc;
 
 	if (!proc_ppc64.rtas) {
 		printk(KERN_WARNING "rtas proc dir does not already exist");
 		return -ENOENT;
 	}
 
-	if (rtas_token("ibm,update-flash-64-and-reboot") != RTAS_UNKNOWN_SERVICE)
-		flash_possible = 1;
-
-	if ((ent = create_proc_entry(FIRMWARE_FLASH_NAME, S_IRUSR | S_IWUSR, proc_ppc64.rtas)) != NULL) {
-		ent->nlink = 1;
-		ent->proc_fops = &rtas_flash_operations;
-		ent->owner = THIS_MODULE;
-	}
+	firmware_flash_pde = create_flash_pde(FIRMWARE_FLASH_NAME, 
+					      &rtas_flash_operations);
+	rc = initialize_flash_pde_data("ibm,update-flash-64-and-reboot",
+			 	       sizeof(struct rtas_update_flash_t), 
+				       firmware_flash_pde);
+	if (rc != 0)
+		return rc;
+	
+	firmware_update_pde = create_flash_pde(FIRMWARE_UPDATE_NAME, 
+					       &rtas_flash_operations);
+	rc = initialize_flash_pde_data("ibm,update-flash-64-and-reboot",
+			 	       sizeof(struct rtas_update_flash_t), 
+				       firmware_update_pde);
+	if (rc != 0)
+		return rc;
+	
+	validate_pde = create_flash_pde(VALIDATE_FLASH_NAME, 
+			      		&validate_flash_operations);
+	rc = initialize_flash_pde_data("ibm,validate-flash-image",
+		                       sizeof(struct rtas_validate_flash_t), 
+				       validate_pde);
+	if (rc != 0)
+		return rc;
+	
+	manage_pde = create_flash_pde(MANAGE_FLASH_NAME, 
+				      &manage_flash_operations);
+	rc = initialize_flash_pde_data("ibm,manage-flash-image",
+			               sizeof(struct rtas_manage_flash_t),
+				       manage_pde);
+	if (rc != 0)
+		return rc;
+	
 	return 0;
 }
 
@@ -230,7 +690,10 @@ void __exit rtas_flash_cleanup(void)
 {
 	if (!proc_ppc64.rtas)
 		return;
-	remove_proc_entry(FIRMWARE_FLASH_NAME, proc_ppc64.rtas);
+	remove_flash_pde(firmware_flash_pde);
+	remove_flash_pde(firmware_update_pde);
+	remove_flash_pde(validate_pde);
+	remove_flash_pde(manage_pde);
 }
 
 module_init(rtas_flash_init);
diff -purN linux-2.5/arch/ppc64/kernel/rtasd.c linuxppc64-2.5/arch/ppc64/kernel/rtasd.c
--- linux-2.5/arch/ppc64/kernel/rtasd.c	2003-08-19 02:46:23.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/rtasd.c	2003-12-12 16:13:55.000000000 +0000
@@ -17,11 +17,15 @@
 #include <linux/proc_fs.h>
 #include <linux/init.h>
 #include <linux/vmalloc.h>
+#include <linux/spinlock.h>
 
 #include <asm/uaccess.h>
 #include <asm/io.h>
 #include <asm/rtas.h>
 #include <asm/prom.h>
+#include <asm/nvram.h>
+#include <asm/atomic.h>
+#include <asm/proc_fs.h>
 
 #if 0
 #define DEBUG(A...)	printk(KERN_ERR A)
@@ -29,36 +33,184 @@
 #define DEBUG(A...)
 #endif
 
-static spinlock_t rtas_log_lock = SPIN_LOCK_UNLOCKED;
+static spinlock_t log_lock = SPIN_LOCK_UNLOCKED;
 
 DECLARE_WAIT_QUEUE_HEAD(rtas_log_wait);
 
-#define LOG_NUMBER		64		/* must be a power of two */
-#define LOG_NUMBER_MASK		(LOG_NUMBER-1)
-
 static char *rtas_log_buf;
 static unsigned long rtas_log_start;
 static unsigned long rtas_log_size;
 
-static int surveillance_requested;
+static int surveillance_timeout = -1;
 static unsigned int rtas_event_scan_rate;
 static unsigned int rtas_error_log_max;
+static unsigned int rtas_error_log_buffer_max;
 
-#define EVENT_SCAN_ALL_EVENTS	0xf0000000
-#define SURVEILLANCE_TOKEN	9000
-#define SURVEILLANCE_TIMEOUT	1
-#define SURVEILLANCE_SCANRATE	1
+extern spinlock_t proc_ppc64_lock;
+extern volatile int no_more_logging;
 
-struct proc_dir_entry *proc_rtas;
+volatile int error_log_cnt = 0;
 
 /*
  * Since we use 32 bit RTAS, the physical address of this must be below
  * 4G or else bad things happen. Allocate this in the kernel data and
  * make it big enough.
  */
-#define RTAS_ERROR_LOG_MAX 1024
 static unsigned char logdata[RTAS_ERROR_LOG_MAX];
 
+/* To see this info, grep RTAS /var/log/messages and each entry
+ * will be collected together with obvious begin/end.
+ * There will be a unique identifier on the begin and end lines.
+ * This will persist across reboots.
+ *
+ * format of error logs returned from RTAS:
+ * bytes	(size)	: contents
+ * --------------------------------------------------------
+ * 0-7		(8)	: rtas_error_log
+ * 8-47		(40)	: extended info
+ * 48-51	(4)	: vendor id
+ * 52-1023 (vendor specific) : location code and debug data
+ */
+static void printk_log_rtas(char *buf, int len)
+{
+
+	int i,j,n;
+	int perline = 16;
+	char buffer[64];
+	char * str = "RTAS event";
+
+	printk(RTAS_ERR "%d -------- %s begin --------\n", error_log_cnt, str);
+
+	/*
+	 * Print perline bytes on each line, each line will start
+	 * with RTAS and a changing number, so syslogd will
+	 * print lines that are otherwise the same.  Separate every
+	 * 4 bytes with a space.
+	 */
+	for (i=0; i < len; i++) {
+		j = i % perline;
+		if (j == 0) {
+			memset(buffer, 0, sizeof(buffer));
+			n = sprintf(buffer, "RTAS %d:", i/perline);
+		}
+
+		if ((i % 4) == 0)
+			n += sprintf(buffer+n, " ");
+
+		n += sprintf(buffer+n, "%02x", (unsigned char)buf[i]);
+
+		if (j == (perline-1))
+			printk(KERN_ERR "%s\n", buffer);
+	}
+	if ((i % perline) != 0)
+		printk(KERN_ERR "%s\n", buffer);
+
+	printk(RTAS_ERR "%d -------- %s end ----------\n", error_log_cnt, str);
+}
+
+static int log_rtas_len(char * buf)
+{
+	int len;
+	struct rtas_error_log *err;
+
+	/* rtas fixed header */
+	len = 8;
+	err = (struct rtas_error_log *)buf;
+	if (err->extended_log_length) {
+
+		/* extended header */
+		len += err->extended_log_length;
+
+		if (len > RTAS_ERROR_LOG_MAX)
+			len = RTAS_ERROR_LOG_MAX;
+	}
+	return len;
+}
+
+/*
+ * First write to nvram, if fatal error, that is the only
+ * place we log the info.  The error will be picked up
+ * on the next reboot by rtasd.  If not fatal, run the
+ * method for the type of error.  Currently, only RTAS
+ * errors have methods implemented, but in the future
+ * there might be a need to store data in nvram before a
+ * call to panic().
+ *
+ * XXX We write to nvram periodically, to indicate error has
+ * been written and sync'd, but there is a possibility
+ * that if we don't shutdown correctly, a duplicate error
+ * record will be created on next reboot.
+ */
+void pSeries_log_error(char *buf, unsigned int err_type, int fatal)
+{
+	unsigned long offset;
+	unsigned long s;
+	int len = 0;
+
+	DEBUG("logging event\n");
+
+	if (buf == NULL)
+		return;
+
+	spin_lock_irqsave(&log_lock, s);
+
+	/* get length and increase count */
+	switch (err_type & ERR_TYPE_MASK) {
+	case ERR_TYPE_RTAS_LOG:
+		len = log_rtas_len(buf);
+		if (!(err_type & ERR_FLAG_BOOT))
+			error_log_cnt++;
+		break;
+	case ERR_TYPE_KERNEL_PANIC:
+	default:
+		spin_unlock_irqrestore(&log_lock, s);
+		return;
+	}
+
+	/* Write error to NVRAM */
+	if (!no_more_logging && !(err_type & ERR_FLAG_BOOT))
+		nvram_write_error_log(buf, len, err_type);
+
+	/* Check to see if we need to or have stopped logging */
+	if (fatal || no_more_logging) {
+		no_more_logging = 1;
+		spin_unlock_irqrestore(&log_lock, s);
+		return;
+	}
+
+	/* call type specific method for error */
+	switch (err_type & ERR_TYPE_MASK) {
+	case ERR_TYPE_RTAS_LOG:
+		/* put into syslog and error_log file */
+		printk_log_rtas(buf, len);
+
+		offset = rtas_error_log_buffer_max *
+			((rtas_log_start+rtas_log_size) & LOG_NUMBER_MASK);
+
+		/* First copy over sequence number */
+		memcpy(&rtas_log_buf[offset], (void *) &error_log_cnt, sizeof(int));
+
+		/* Second copy over error log data */
+		offset += sizeof(int);
+		memcpy(&rtas_log_buf[offset], buf, len);
+
+		if (rtas_log_size < LOG_NUMBER)
+			rtas_log_size += 1;
+		else
+			rtas_log_start += 1;
+
+		spin_unlock_irqrestore(&log_lock, s);
+		wake_up_interruptible(&rtas_log_wait);
+		break;
+	case ERR_TYPE_KERNEL_PANIC:
+	default:
+		spin_unlock_irqrestore(&log_lock, s);
+		return;
+	}
+
+}
+
+
 static int rtas_log_open(struct inode * inode, struct file * file)
 {
 	return 0;
@@ -69,36 +221,50 @@ static int rtas_log_release(struct inode
 	return 0;
 }
 
+/* This will check if all events are logged, if they are then, we
+ * know that we can safely clear the events in NVRAM.
+ * Next we'll sit and wait for something else to log.
+ */
 static ssize_t rtas_log_read(struct file * file, char * buf,
 			 size_t count, loff_t *ppos)
 {
 	int error;
 	char *tmp;
+	unsigned long s;
 	unsigned long offset;
 
-	if (!buf || count < rtas_error_log_max)
+	if (!buf || count < rtas_error_log_buffer_max)
 		return -EINVAL;
 
-	count = rtas_error_log_max;
+	count = rtas_error_log_buffer_max;
 
 	error = verify_area(VERIFY_WRITE, buf, count);
 	if (error)
-		return -EINVAL;
+		return -EFAULT;
 
-	tmp = kmalloc(rtas_error_log_max, GFP_KERNEL);
+	tmp = kmalloc(count, GFP_KERNEL);
 	if (!tmp)
 		return -ENOMEM;
 
+
+	spin_lock_irqsave(&log_lock, s);
+	/* if it's 0, then we know we got the last one (the one in NVRAM) */
+	if (rtas_log_size == 0 && !no_more_logging)
+		nvram_clear_error_log();
+	spin_unlock_irqrestore(&log_lock, s);
+
+
 	error = wait_event_interruptible(rtas_log_wait, rtas_log_size);
 	if (error)
 		goto out;
 
-	spin_lock(&rtas_log_lock);
-	offset = rtas_error_log_max * (rtas_log_start & LOG_NUMBER_MASK);
+	spin_lock_irqsave(&log_lock, s);
+	offset = rtas_error_log_buffer_max * (rtas_log_start & LOG_NUMBER_MASK);
 	memcpy(tmp, &rtas_log_buf[offset], count);
+
 	rtas_log_start += 1;
 	rtas_log_size -= 1;
-	spin_unlock(&rtas_log_lock);
+	spin_unlock_irqrestore(&log_lock, s);
 
 	error = copy_to_user(buf, tmp, count) ? -EFAULT : count;
 out:
@@ -121,42 +287,18 @@ struct file_operations proc_rtas_log_ope
 	.release =	rtas_log_release,
 };
 
-static void log_rtas(char *buf)
-{
-	unsigned long offset;
-
-	DEBUG("logging rtas event\n");
-
-	spin_lock(&rtas_log_lock);
-
-	offset = rtas_error_log_max *
-			((rtas_log_start+rtas_log_size) & LOG_NUMBER_MASK);
-
-	memcpy(&rtas_log_buf[offset], buf, rtas_error_log_max);
-
-	if (rtas_log_size < LOG_NUMBER)
-		rtas_log_size += 1;
-	else
-		rtas_log_start += 1;
-
-	spin_unlock(&rtas_log_lock);
-	wake_up_interruptible(&rtas_log_wait);
-}
-
-static int enable_surveillance(void)
+static int enable_surveillance(int timeout)
 {
 	int error;
 
 	error = rtas_call(rtas_token("set-indicator"), 3, 1, NULL,
-			  SURVEILLANCE_TOKEN, 0, SURVEILLANCE_TIMEOUT);
+			  SURVEILLANCE_TOKEN, 0, timeout);
 
 	if (error) {
 		printk(KERN_ERR "rtasd: could not enable surveillance\n");
 		return -1;
 	}
 
-	rtas_event_scan_rate = SURVEILLANCE_SCANRATE;
-
 	return 0;
 }
 
@@ -165,11 +307,12 @@ static int get_eventscan_parms(void)
 	struct device_node *node;
 	int *ip;
 
-	node = find_path_device("/rtas");
+	node = of_find_node_by_path("/rtas");
 
 	ip = (int *)get_property(node, "rtas-event-scan-rate", NULL);
 	if (ip == NULL) {
 		printk(KERN_ERR "rtasd: no rtas-event-scan-rate\n");
+		of_node_put(node);
 		return -1;
 	}
 	rtas_event_scan_rate = *ip;
@@ -178,6 +321,7 @@ static int get_eventscan_parms(void)
 	ip = (int *)get_property(node, "rtas-error-log-max", NULL);
 	if (ip == NULL) {
 		printk(KERN_ERR "rtasd: no rtas-error-log-max\n");
+		of_node_put(node);
 		return -1;
 	}
 	rtas_error_log_max = *ip;
@@ -187,6 +331,7 @@ static int get_eventscan_parms(void)
 		printk(KERN_ERR "rtasd: truncated error log from %d to %d bytes\n", rtas_error_log_max, RTAS_ERROR_LOG_MAX);
 		rtas_error_log_max = RTAS_ERROR_LOG_MAX;
 	}
+	of_node_put(node);
 
 	return 0;
 }
@@ -195,10 +340,12 @@ extern long sys_sched_get_priority_max(i
 
 static int rtasd(void *unused)
 {
+	unsigned int err_type;
 	int cpu = 0;
 	int error;
 	int first_pass = 1;
 	int event_scan = rtas_token("event-scan");
+	int rc;
 
 	if (event_scan == RTAS_UNKNOWN_SERVICE || get_eventscan_parms() == -1)
 		goto error;
@@ -209,6 +356,9 @@ static int rtasd(void *unused)
 		goto error;
 	}
 
+	/* We can use rtas_log_buf now */
+	no_more_logging = 0;
+
 	DEBUG("will sleep for %d jiffies\n", (HZ*60/rtas_event_scan_rate) / 2);
 
 	daemonize("rtasd");
@@ -219,6 +369,16 @@ static int rtasd(void *unused)
 	current->nice = sys_sched_get_priority_max(SCHED_FIFO) + 1;
 #endif
 
+	/* See if we have any error stored in NVRAM */
+	memset(logdata, 0, rtas_error_log_max);
+
+	rc = nvram_read_error_log(logdata, rtas_error_log_max, &err_type);
+	if (!rc) {
+		if (err_type != ERR_FLAG_ALREADY_LOGGED) {
+			pSeries_log_error(logdata, err_type | ERR_FLAG_BOOT, 0);
+		}
+	}
+
 repeat:
 	for (cpu = 0; cpu < NR_CPUS; cpu++) {
 		if (!cpu_online(cpu))
@@ -231,7 +391,7 @@ repeat:
 		do {
 			memset(logdata, 0, rtas_error_log_max);
 			error = rtas_call(event_scan, 4, 1, NULL,
-					EVENT_SCAN_ALL_EVENTS, 0,
+					RTAS_EVENT_SCAN_ALL_EVENTS, 0,
 					__pa(logdata), rtas_error_log_max);
 			if (error == -1) {
 				printk(KERN_ERR "event-scan failed\n");
@@ -239,7 +399,7 @@ repeat:
 			}
 
 			if (error == 0)
-				log_rtas(logdata);
+				pSeries_log_error(logdata, ERR_TYPE_RTAS_LOG, 0);
 
 		} while(error == 0);
 
@@ -252,9 +412,9 @@ repeat:
 		schedule_timeout(first_pass ? HZ : (HZ*60/rtas_event_scan_rate) / 2);
 	}
 
-	if (first_pass && surveillance_requested) {
+	if (first_pass && (surveillance_timeout != -1)) {
 		DEBUG("enabling surveillance\n");
-		if (enable_surveillance())
+		if (enable_surveillance(surveillance_timeout))
 			goto error_vfree;
 		DEBUG("surveillance enabled\n");
 	}
@@ -273,25 +433,29 @@ static int __init rtas_init(void)
 {
 	struct proc_dir_entry *entry;
 
-	if (proc_rtas == NULL) {
-		proc_rtas = proc_mkdir("rtas", 0);
+	if (proc_ppc64.rtas == NULL) {
+		proc_ppc64_init();
 	}
 
-	if (proc_rtas == NULL) {
-		printk(KERN_ERR "Failed to create /proc/rtas in rtas_init\n");
-	} else {
-		entry = create_proc_entry("error_log", S_IRUSR, proc_rtas);
-		if (entry)
-			entry->proc_fops = &proc_rtas_log_operations;
-		else
-			printk(KERN_ERR "Failed to create rtas/error_log proc entry\n");
+	if (proc_ppc64.rtas == NULL) {
+		printk(KERN_ERR "rtas_init: /proc/ppc64/rtas does not exist.");
+		return -EIO;
 	}
 
+	entry = create_proc_entry("error_log", S_IRUSR, proc_ppc64.rtas);
+	if (entry)
+		entry->proc_fops = &proc_rtas_log_operations;
+	else
+		printk(KERN_ERR "Failed to create rtas/error_log proc entry\n");
+
 	if (kernel_thread(rtasd, 0, CLONE_FS) < 0)
 		printk(KERN_ERR "Failed to start RTAS daemon\n");
 
 	printk(KERN_ERR "RTAS daemon started\n");
 
+	/* Make room for the sequence number */
+	rtas_error_log_buffer_max = rtas_error_log_max + sizeof(int);
+
 	return 0;
 }
 
@@ -300,8 +464,8 @@ static int __init surveillance_setup(cha
 	int i;
 
 	if (get_option(&str,&i)) {
-		if (i == 1)
-			surveillance_requested = 1;
+		if (i >= 0 && i <= 255)
+			surveillance_timeout = i;
 	}
 
 	return 1;
diff -purN linux-2.5/arch/ppc64/kernel/scanlog.c linuxppc64-2.5/arch/ppc64/kernel/scanlog.c
--- linux-2.5/arch/ppc64/kernel/scanlog.c	2003-08-31 23:14:08.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/scanlog.c	2003-12-11 04:12:26.000000000 +0000
@@ -28,6 +28,7 @@
 #include <asm/uaccess.h>
 #include <asm/rtas.h>
 #include <asm/prom.h>
+#include <asm/proc_fs.h>
 
 #define MODULE_VERSION "1.0"
 #define MODULE_NAME "scanlog"
@@ -43,9 +44,6 @@ static int scanlog_debug;
 static unsigned int ibm_scan_log_dump;			/* RTAS token */
 static struct proc_dir_entry *proc_ppc64_scan_log_dump;	/* The proc file */
 
-extern struct proc_dir_entry *proc_rtas;
-
-
 static ssize_t scanlog_read(struct file *file, char *buf,
 			    size_t count, loff_t *ppos)
 {
@@ -135,17 +133,24 @@ static ssize_t scanlog_read(struct file 
 static ssize_t scanlog_write(struct file * file, const char * buf,
 			     size_t count, loff_t *ppos)
 {
+	char stkbuf[20];
 	unsigned long status;
 
+	if (count > 19) count = 19;
+	if (copy_from_user (stkbuf, buf, count)) {
+		return -EFAULT;
+	}
+	stkbuf[count] = 0;
+
 	if (buf) {
-		if (strncmp(buf, "reset", 5) == 0) {
+		if (strncmp(stkbuf, "reset", 5) == 0) {
 			DEBUG("reset scanlog\n");
 			status = rtas_call(ibm_scan_log_dump, 2, 1, NULL, NULL, 0);
 			DEBUG("rtas returns %ld\n", status);
-		} else if (strncmp(buf, "debugon", 7) == 0) {
+		} else if (strncmp(stkbuf, "debugon", 7) == 0) {
 			printk(KERN_ERR "scanlog: debug on\n");
 			scanlog_debug = 1;
-		} else if (strncmp(buf, "debugoff", 8) == 0) {
+		} else if (strncmp(stkbuf, "debugoff", 8) == 0) {
 			printk(KERN_ERR "scanlog: debug off\n");
 			scanlog_debug = 0;
 		}
@@ -207,15 +212,16 @@ int __init scanlog_init(void)
 		return -EIO;
 	}
 
-	if (proc_rtas == NULL)
-                proc_rtas = proc_mkdir("rtas", 0);
+	if (proc_ppc64.rtas == NULL) {
+		proc_ppc64_init();
+	}
 
-	if (proc_rtas == NULL) {
+	if (proc_ppc64.rtas == NULL) {
 		printk(KERN_ERR "Failed to create /proc/rtas in scanlog_init\n");
 		return -EIO;
 	}
 
-        ent = create_proc_entry("scan-log-dump",  S_IRUSR, proc_rtas);
+        ent = create_proc_entry("scan-log-dump",  S_IRUSR, proc_ppc64.rtas);
 	if (ent) {
 		ent->proc_fops = &scanlog_fops;
 		/* Ideally we could allocate a buffer < 4G */
diff -purN linux-2.5/arch/ppc64/kernel/setup.c linuxppc64-2.5/arch/ppc64/kernel/setup.c
--- linux-2.5/arch/ppc64/kernel/setup.c	2003-10-08 02:53:40.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/setup.c	2003-12-17 05:08:23.000000000 +0000
@@ -38,6 +38,7 @@
 #include <asm/paca.h>
 #include <asm/ppcdebug.h>
 #include <asm/time.h>
+#include <asm/cputable.h>
 #include <asm/sections.h>
 
 extern unsigned long klimit;
@@ -58,6 +59,9 @@ extern void iSeries_init_early( void );
 extern void pSeries_init_early( void );
 extern void pSeriesLP_init_early(void);
 extern void mm_init_ppc64( void ); 
+extern void pseries_secondary_smp_init(unsigned long); 
+extern int  idle_setup(void);
+extern void vpa_init(int cpu);
 
 unsigned long decr_overclock = 1;
 unsigned long decr_overclock_proc0 = 1;
@@ -137,12 +141,16 @@ void __init disable_early_printk(void)
 }
 
 /*
- * Do some initial setup of the system.  The paramters are those which 
+ * Do some initial setup of the system.  The parameters are those which 
  * were passed in from the bootloader.
  */
 void setup_system(unsigned long r3, unsigned long r4, unsigned long r5,
 		  unsigned long r6, unsigned long r7)
 {
+#ifdef CONFIG_PPC_PSERIES
+        unsigned int ret, i;
+#endif
+
 #ifdef CONFIG_XMON_DEFAULT
 	debugger = xmon;
 	debugger_bpt = xmon_bpt;
@@ -183,10 +191,33 @@ void setup_system(unsigned long r3, unsi
 #endif
 	}
 
+#ifdef CONFIG_PPC_PSERIES
 	if (systemcfg->platform & PLATFORM_PSERIES) {
 		early_console_initialized = 1;
 		register_console(&udbg_console);
+		finish_device_tree();
+		chrp_init(r3, r4, r5, r6, r7);
+
+#ifdef CONFIG_SMP
+		/* Start secondary threads on SMT systems */
+		for (i = 0; i < NR_CPUS; i++) {
+			if(cpu_available(i)  && !cpu_possible(i)) {
+				printk("%16.16x : starting thread\n", i);
+				rtas_call(rtas_token("start-cpu"), 3, 1, 
+					  (void *)&ret,
+					  get_hard_smp_processor_id(i), 
+					  *((unsigned long *)pseries_secondary_smp_init), i);
+				cpu_set(i, cpu_possible_map);
+				systemcfg->processorCount++;
+			}
+		}
+#endif
 	}
+#endif
+	/* Finish initializing the hash table (do the dynamic
+	 * patching for the fast-path hashtable.S code)
+	 */
+	htab_finish_init();
 
 	printk("Starting Linux PPC64 %s\n", UTS_RELEASE);
 
@@ -204,12 +235,16 @@ void setup_system(unsigned long r3, unsi
 	printk("htab_data.num_ptegs           = 0x%lx\n", htab_data.htab_num_ptegs);
 	printk("-----------------------------------------------------\n");
 
-	if (systemcfg->platform & PLATFORM_PSERIES) {
-		finish_device_tree();
-		chrp_init(r3, r4, r5, r6, r7);
+	mm_init_ppc64();
+
+#if defined(CONFIG_SMP) && defined(CONFIG_PPC_PSERIES)
+	if (cur_cpu_spec->firmware_features & FW_FEATURE_SPLPAR) {
+		vpa_init(boot_cpuid);
 	}
+#endif
 
-	mm_init_ppc64();
+	/* Select the correct idle loop for the platform. */
+	idle_setup();
 
 	switch (systemcfg->platform) {
 #ifdef CONFIG_PPC_ISERIES
@@ -277,36 +312,19 @@ static int show_cpuinfo(struct seq_file 
 	seq_printf(m, "processor\t: %lu\n", cpu_id);
 	seq_printf(m, "cpu\t\t: ");
 
-	switch (PVR_VER(pvr)) {
-	case PV_NORTHSTAR:
-		seq_printf(m, "RS64-II (northstar)\n");
-		break;
-	case PV_PULSAR:
-		seq_printf(m, "RS64-III (pulsar)\n");
-		break;
-	case PV_POWER4:
-		seq_printf(m, "POWER4 (gp)\n");
-		break;
-	case PV_ICESTAR:
-		seq_printf(m, "RS64-III (icestar)\n");
-		break;
-	case PV_SSTAR:
-		seq_printf(m, "RS64-IV (sstar)\n");
-		break;
-	case PV_POWER4p:
-		seq_printf(m, "POWER4+ (gq)\n");
-		break;
-	case PV_630:
-		seq_printf(m, "POWER3 (630)\n");
-		break;
-	case PV_630p:
-		seq_printf(m, "POWER3 (630+)\n");
-		break;
-	default:
-		seq_printf(m, "Unknown (%08x)\n", pvr);
-		break;
-	}
+	if (cur_cpu_spec->pvr_mask)
+		seq_printf(m, "%s", cur_cpu_spec->cpu_name);
+	else
+		seq_printf(m, "unknown (%08x)", pvr);
+
+#ifdef CONFIG_ALTIVEC
+	if (cur_cpu_spec->cpu_features & CPU_FTR_ALTIVEC)
+		seq_printf(m, ", altivec supported");
+#endif /* CONFIG_ALTIVEC */
 
+	seq_printf(m, "\n");
+
+#ifdef CONFIG_PPC_PSERIES
 	/*
 	 * Assume here that all clock rates are the same in a
 	 * smp system.  -- Cort
@@ -315,15 +333,17 @@ static int show_cpuinfo(struct seq_file 
 		struct device_node *cpu_node;
 		int *fp;
 
-		cpu_node = find_type_devices("cpu");
+		cpu_node = of_find_node_by_type(NULL, "cpu");
 		if (cpu_node) {
 			fp = (int *) get_property(cpu_node, "clock-frequency",
 						  NULL);
 			if (fp)
 				seq_printf(m, "clock\t\t: %dMHz\n",
 					   *fp / 1000000);
+			of_node_put(cpu_node);
 		}
 	}
+#endif
 
 	if (ppc_md.setup_residual != NULL)
 		ppc_md.setup_residual(m, cpu_id);
@@ -353,13 +373,11 @@ struct seq_operations cpuinfo_op = {
 };
 
 /*
- * Fetch the cmd_line from open firmware. */
+ * Fetch the cmd_line from open firmware. 
+ */
 void parse_cmd_line(unsigned long r3, unsigned long r4, unsigned long r5,
 		  unsigned long r6, unsigned long r7)
 {
-	struct device_node *chosen;
-	char *p;
-
 #ifdef CONFIG_BLK_DEV_INITRD
 	if ((initrd_start == 0) && r3 && r4 && r4 != 0xdeadbeef) {
 		initrd_start = (r3 >= KERNELBASE) ? r3 : (unsigned long)__va(r3);
@@ -375,12 +393,20 @@ void parse_cmd_line(unsigned long r3, un
 	strlcpy(cmd_line, CONFIG_CMDLINE, sizeof(cmd_line));
 #endif /* CONFIG_CMDLINE */
 
-	chosen = find_devices("chosen");
+#ifdef CONFIG_PPC_PSERIES
+	{
+	struct device_node *chosen;
+
+	chosen = of_find_node_by_name(NULL, "chosen");
 	if (chosen != NULL) {
+		char *p;
 		p = get_property(chosen, "bootargs", NULL);
 		if (p != NULL && p[0] != 0)
 			strlcpy(cmd_line, p, sizeof(cmd_line));
+		of_node_put(chosen);
 	}
+	}
+#endif
 
 	/* Look for mem= option on command line */
 	if (strstr(cmd_line, "mem=")) {
@@ -406,28 +432,7 @@ void parse_cmd_line(unsigned long r3, un
 }
 
 
-char *bi_tag2str(unsigned long tag)
-{
-	switch (tag) {
-	case BI_FIRST:
-		return "BI_FIRST";
-	case BI_LAST:
-		return "BI_LAST";
-	case BI_CMD_LINE:
-		return "BI_CMD_LINE";
-	case BI_BOOTLOADER_ID:
-		return "BI_BOOTLOADER_ID";
-	case BI_INITRD:
-		return "BI_INITRD";
-	case BI_SYSMAP:
-		return "BI_SYSMAP";
-	case BI_MACHTYPE:
-		return "BI_MACHTYPE";
-	default:
-		return "BI_UNKNOWN";
-	}
-}
-
+#ifdef CONFIG_PPC_PSERIES
 int parse_bootinfo(void)
 {
 	struct bi_record *rec;
@@ -445,8 +450,7 @@ int parse_bootinfo(void)
 			memcpy(cmd_line, (void *)rec->data, rec->size);
 			break;
 		case BI_SYSMAP:
-			sysmap = (char *)((rec->data[0] >= (KERNELBASE))
-					? rec->data[0] : (unsigned long)__va(rec->data[0]));
+			sysmap = __va(rec->data[0]);
 			sysmap_size = rec->data[1];
 			break;
 #ifdef CONFIG_BLK_DEV_INITRD
@@ -462,6 +466,7 @@ int parse_bootinfo(void)
 
 	return 0;
 }
+#endif
 
 int __init ppc_init(void)
 {
diff -purN linux-2.5/arch/ppc64/kernel/signal.c linuxppc64-2.5/arch/ppc64/kernel/signal.c
--- linux-2.5/arch/ppc64/kernel/signal.c	2003-11-16 21:52:15.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/signal.c	2003-12-17 04:27:52.000000000 +0000
@@ -95,7 +95,7 @@ long sys_rt_sigsuspend(sigset_t *unewset
 	regs->gpr[3] = EINTR;
 	regs->ccr |= 0x10000000;
 	while (1) {
-		current->state = TASK_INTERRUPTIBLE;
+		set_current_state(TASK_INTERRUPTIBLE);
 		schedule();
 		if (do_signal(&saveset, regs))
 			return regs->gpr[3];
@@ -114,19 +114,49 @@ long sys_sigaltstack(const stack_t *uss,
  * Set up the sigcontext for the signal frame.
  */
 
-static int
-setup_sigcontext(struct sigcontext *sc, struct pt_regs *regs,
+static int setup_sigcontext(struct sigcontext *sc, struct pt_regs *regs,
 		 int signr, sigset_t *set, unsigned long handler)
 {
+	/* When CONFIG_ALTIVEC is set, we _always_ setup v_regs even if the
+	 * process never used altivec yet (MSR_VEC is zero in pt_regs of
+	 * the context). This is very important because we must ensure we
+	 * don't lose the VRSAVE content that may have been set prior to
+	 * the process doing its first vector operation
+	 * Userland shall check AT_HWCAP to know wether it can rely on the
+	 * v_regs pointer or not
+	 */
+#ifdef CONFIG_ALTIVEC
+	elf_vrreg_t *v_regs = (elf_vrreg_t *)(((unsigned long)sc->vmx_reserve) & ~0xful);
+#endif
 	int err = 0;
 
 	if (regs->msr & MSR_FP)
 		giveup_fpu(current);
 
-	current->thread.saved_msr = regs->msr & ~(MSR_FP | MSR_FE0 | MSR_FE1);
-	regs->msr = current->thread.saved_msr | current->thread.fpexc_mode;
-	current->thread.saved_softe = regs->softe;
+	/* Make sure signal doesn't get spurrious FP exceptions */
+	current->thread.fpscr = 0;
+
+#ifdef CONFIG_ALTIVEC
+	err |= __put_user(v_regs, &sc->v_regs);
 
+	/* save altivec registers */
+	if (current->thread.used_vr) {		
+		if (regs->msr & MSR_VEC)
+			giveup_altivec(current);
+		/* Copy 33 vec registers (vr0..31 and vscr) to the stack */
+		err |= __copy_to_user(v_regs, current->thread.vr, 33 * sizeof(vector128));
+		/* set MSR_VEC in the MSR value in the frame to indicate that sc->v_reg)
+		 * contains valid data.
+		 */
+		regs->msr |= MSR_VEC;
+	}
+	/* We always copy to/from vrsave, it's 0 if we don't have or don't
+	 * use altivec.
+	 */
+	err |= __put_user(current->thread.vrsave, (u32 *)&v_regs[33]);
+#else /* CONFIG_ALTIVEC */
+	err |= __put_user(0, &sc->v_regs);
+#endif /* CONFIG_ALTIVEC */
 	err |= __put_user(&sc->gp_regs, &sc->regs);
 	err |= __copy_to_user(&sc->gp_regs, regs, GP_REGS_SIZE);
 	err |= __copy_to_user(&sc->fp_regs, &current->thread.fpr, FP_REGS_SIZE);
@@ -135,9 +165,6 @@ setup_sigcontext(struct sigcontext *sc, 
 	if (set != NULL)
 		err |=  __put_user(set->sig[0], &sc->oldmask);
 
-	regs->msr &= ~(MSR_FP | MSR_FE0 | MSR_FE1);
-	current->thread.fpscr = 0;
-
 	return err;
 }
 
@@ -145,23 +172,42 @@ setup_sigcontext(struct sigcontext *sc, 
  * Restore the sigcontext from the signal frame.
  */
 
-static int
-restore_sigcontext(struct pt_regs *regs, sigset_t *set, struct sigcontext *sc)
+static int restore_sigcontext(struct pt_regs *regs, sigset_t *set, int sig, struct sigcontext *sc)
 {
+#ifdef CONFIG_ALTIVEC
+	elf_vrreg_t *v_regs;
+#endif
 	unsigned int err = 0;
+	unsigned long save_r13;
 
-	if (regs->msr & MSR_FP)
-		giveup_fpu(current);
-
+	/* If this is not a signal return, we preserve the TLS in r13 */
+	if (!sig)
+		save_r13 = regs->gpr[13];
 	err |= __copy_from_user(regs, &sc->gp_regs, GP_REGS_SIZE);
+	if (!sig)
+		regs->gpr[13] = save_r13;
 	err |= __copy_from_user(&current->thread.fpr, &sc->fp_regs, FP_REGS_SIZE);
-	current->thread.fpexc_mode = regs->msr & (MSR_FE0 | MSR_FE1);
 	if (set != NULL)
 		err |=  __get_user(set->sig[0], &sc->oldmask);
 
-	/* Don't allow the signal handler to change these modulo FE{0,1} */
-	regs->msr = current->thread.saved_msr & ~(MSR_FP | MSR_FE0 | MSR_FE1);
-	regs->softe = current->thread.saved_softe;
+#ifdef CONFIG_ALTIVEC
+	err |= __get_user(v_regs, &sc->v_regs);
+	if (err)
+		return err;
+	/* Copy 33 vec registers (vr0..31 and vscr) from the stack */
+	if (v_regs != 0 && (regs->msr & MSR_VEC) != 0)
+		err |= __copy_from_user(current->thread.vr, v_regs, 33 * sizeof(vector128));
+	else if (current->thread.used_vr)
+		memset(&current->thread.vr, 0, 33);
+	/* Always get VRSAVE back */
+	if (v_regs != 0)
+		err |= __get_user(current->thread.vrsave, (u32 *)&v_regs[33]);
+	else
+		current->thread.vrsave = 0;
+#endif /* CONFIG_ALTIVEC */
+
+	/* Force reload of FP/VEC */
+	regs->msr &= ~(MSR_FP | MSR_FE0 | MSR_FE1 | MSR_VEC);
 
 	return err;
 }
@@ -169,8 +215,8 @@ restore_sigcontext(struct pt_regs *regs,
 /*
  * Allocate space for the signal frame
  */
-static inline void *
-get_sigframe(struct k_sigaction *ka, struct pt_regs *regs, size_t frame_size)
+static inline void * get_sigframe(struct k_sigaction *ka, struct pt_regs *regs,
+				  size_t frame_size)
 {
         unsigned long newsp;
 
@@ -185,8 +231,10 @@ get_sigframe(struct k_sigaction *ka, str
         return (void *)((newsp - frame_size) & -8ul);
 }
 
-static int
-setup_trampoline(unsigned int syscall, unsigned int *tramp)
+/*
+ * Setup the trampoline code on the stack
+ */
+static int setup_trampoline(unsigned int syscall, unsigned int *tramp)
 {
 	int i, err = 0;
 
@@ -209,6 +257,72 @@ setup_trampoline(unsigned int syscall, u
 }
 
 /*
+ * Restore the user process's signal mask (also used by signal32.c)
+ */
+void restore_sigmask(sigset_t *set)
+{
+	sigdelsetmask(set, ~_BLOCKABLE);
+	spin_lock_irq(&current->sighand->siglock);
+	current->blocked = *set;
+	recalc_sigpending();
+	spin_unlock_irq(&current->sighand->siglock);
+}
+
+
+/*
+ * Handle {get,set,swap}_context operations
+ */
+int sys_swapcontext(struct ucontext __user *old_ctx,
+		    struct ucontext __user *new_ctx,
+		    long ctx_size, long r6, long r7, long r8, struct pt_regs *regs)
+{
+	unsigned char tmp;
+	sigset_t set;
+
+	/* Context size is for future use. Right now, we only make sure
+	 * we are passed something we understand
+	 */
+	if (ctx_size < sizeof(struct ucontext))
+		return -EINVAL;
+
+	if (old_ctx != NULL) {
+		if (verify_area(VERIFY_WRITE, old_ctx, sizeof(*old_ctx))
+		    || setup_sigcontext(&old_ctx->uc_mcontext, regs, 0, NULL, 0)
+		    || __copy_to_user(&old_ctx->uc_sigmask,
+				      &current->blocked, sizeof(sigset_t)))
+			return -EFAULT;
+	}
+	if (new_ctx == NULL)
+		return 0;
+	if (verify_area(VERIFY_READ, new_ctx, sizeof(*new_ctx))
+	    || __get_user(tmp, (u8 *) new_ctx)
+	    || __get_user(tmp, (u8 *) (new_ctx + 1) - 1))
+		return -EFAULT;
+
+	/*
+	 * If we get a fault copying the context into the kernel's
+	 * image of the user's registers, we can't just return -EFAULT
+	 * because the user's registers will be corrupted.  For instance
+	 * the NIP value may have been updated but not some of the
+	 * other registers.  Given that we have done the verify_area
+	 * and successfully read the first and last bytes of the region
+	 * above, this should only happen in an out-of-memory situation
+	 * or if another thread unmaps the region containing the context.
+	 * We kill the task with a SIGSEGV in this situation.
+	 */
+
+	if (__copy_from_user(&set, &new_ctx->uc_sigmask, sizeof(set)))
+		do_exit(SIGSEGV);
+	restore_sigmask(&set);
+	if (restore_sigcontext(regs, NULL, 0, &new_ctx->uc_mcontext))
+		do_exit(SIGSEGV);
+
+	/* This returns like rt_sigreturn */
+	return 0;
+}
+
+
+/*
  * Do a signal return; undo the signal stack.
  */
 
@@ -218,7 +332,6 @@ int sys_rt_sigreturn(unsigned long r3, u
 {
 	struct ucontext *uc = (struct ucontext *)regs->gpr[1];
 	sigset_t set;
-	stack_t st;
 
 	/* Always make any pending restarted system calls return -EINTR */
 	current_thread_info()->restart_block.fn = do_no_restart_syscall;
@@ -228,20 +341,14 @@ int sys_rt_sigreturn(unsigned long r3, u
 
 	if (__copy_from_user(&set, &uc->uc_sigmask, sizeof(set)))
 		goto badframe;
-	sigdelsetmask(&set, ~_BLOCKABLE);
-	spin_lock_irq(&current->sighand->siglock);
-	current->blocked = set;
-	recalc_sigpending();
-	spin_unlock_irq(&current->sighand->siglock);
-
-	if (restore_sigcontext(regs, NULL, &uc->uc_mcontext))
+	restore_sigmask(&set);
+	if (restore_sigcontext(regs, NULL, 1, &uc->uc_mcontext))
 		goto badframe;
 
-	if (__copy_from_user(&st, &uc->uc_stack, sizeof(st)))
-		goto badframe;
-	/* This function sets back the stack flags into
-	   the current task structure.  */
-	sys_sigaltstack(&st, NULL, 0, 0, 0, 0, regs);
+	/* do_sigaltstack expects a __user pointer and won't modify
+	 * what's in there anyway
+	 */
+	do_sigaltstack(&uc->uc_stack, NULL, regs->gpr[1]);
 
 	return regs->result;
 
@@ -253,8 +360,7 @@ badframe:
 	do_exit(SIGSEGV);
 }
 
-static void
-setup_rt_frame(int signr, struct k_sigaction *ka, siginfo_t *info,
+static void setup_rt_frame(int signr, struct k_sigaction *ka, siginfo_t *info,
 		sigset_t *set, struct pt_regs *regs)
 {
 	/* Handler is *really* a pointer to the function descriptor for
@@ -332,9 +438,8 @@ badframe:
 /*
  * OK, we're invoking a handler
  */
-static void
-handle_signal(unsigned long sig, struct k_sigaction *ka,
-	      siginfo_t *info, sigset_t *oldset, struct pt_regs *regs)
+static void handle_signal(unsigned long sig, struct k_sigaction *ka,
+			  siginfo_t *info, sigset_t *oldset, struct pt_regs *regs)
 {
 	/* Set up Signal Frame */
 	setup_rt_frame(sig, ka, info, oldset, regs);
@@ -352,8 +457,7 @@ handle_signal(unsigned long sig, struct 
 	return;
 }
 
-static inline void
-syscall_restart(struct pt_regs *regs, struct k_sigaction *ka)
+static inline void syscall_restart(struct pt_regs *regs, struct k_sigaction *ka)
 {
 	switch ((int)regs->result) {
 	case -ERESTART_RESTARTBLOCK:
diff -purN linux-2.5/arch/ppc64/kernel/signal32.c linuxppc64-2.5/arch/ppc64/kernel/signal32.c
--- linux-2.5/arch/ppc64/kernel/signal32.c	2003-11-16 21:54:29.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/signal32.c	2003-12-17 04:27:52.000000000 +0000
@@ -32,71 +32,207 @@
 #define DEBUG_SIG 0
 
 #define _BLOCKABLE (~(sigmask(SIGKILL) | sigmask(SIGSTOP)))
-/* 
- * These are the flags in the MSR that the user is allowed to change
- * by modifying the saved value of the MSR on the stack.  SE and BE
- * should not be in this list since gdb may want to change these.  I.e,
- * you should be able to step out of a signal handler to see what
- * instruction executes next after the signal handler completes.
- * Alternately, if you stepped into a signal handler, you should be
- * able to continue 'til the next breakpoint from within the signal
- * handler, even if the handler returns.
- */
-#if 0
-#define MSR_USERCHANGE	(MSR_FE0 | MSR_FE1)
-#else
+
+#define GP_REGS_SIZE32	min(sizeof(elf_gregset_t32), sizeof(struct pt_regs32))
+
 /*
- * glibc tries to set FE0/FE1 via a signal handler. Since it only ever
- * sets both bits and this is the default setting we now disable this
- * behaviour. This is done to insure the new prctl which alters FE0/FE1 does
- * not get overriden by glibc. Setting and clearing FE0/FE1 via signal
- * handler has always been bogus since load_up_fpu used to set FE0/FE1
- * unconditionally.
+ * When we have signals to deliver, we set up on the
+ * user stack, going down from the original stack pointer:
+ *	a sigregs32 struct
+ *	a sigcontext32 struct
+ *	a gap of __SIGNAL_FRAMESIZE32 bytes
+ *
+ * Each of these things must be a multiple of 16 bytes in size.
+ *
  */
-#define MSR_USERCHANGE	0
-#endif
-
 struct sigregs32 {
-	/*
-	 * the gp_regs array is 32 bit representation of the pt_regs
-	 * structure that was stored on the kernel stack during the
-	 * system call that was interrupted for the signal.
-	 *
-	 * Note that the entire pt_regs regs structure will fit in
-	 * the gp_regs structure because the ELF_NREG value is 48 for
-	 * PPC and the pt_regs structure contains 44 registers
-	 */
-	elf_gregset_t32	gp_regs;
-	double		fp_regs[ELF_NFPREG];
-	unsigned int	tramp[2];
+	struct mcontext32	mctx;		/* all the register values */
 	/*
 	 * Programs using the rs6000/xcoff abi can save up to 19 gp
 	 * regs and 18 fp regs below sp before decrementing it.
 	 */
-	int		abigap[56];
+	int			abigap[56];
 };
 
+/* We use the mc_pad field for the signal return trampoline. */
+#define tramp	mc_pad
 
-struct rt_sigframe_32 {
-	/*
-	 * Unused space at start of frame to allow for storing of
-	 * stack pointers
-	 */
-	unsigned long _unused;
+/*
+ *  When we have rt signals to deliver, we set up on the
+ *  user stack, going down from the original stack pointer:
+ *	one rt_sigframe32 struct (siginfo + ucontext + ABI gap)
+ *	a gap of __SIGNAL_FRAMESIZE32+16 bytes
+ *  (the +16 is to get the siginfo and ucontext32 in the same
+ *  positions as in older kernels).
+ *
+ *  Each of these things must be a multiple of 16 bytes in size.
+ *
+ */
+struct rt_sigframe32 {
+	struct compat_siginfo	info;
+	struct ucontext32	uc;
 	/*
-	 * This is a 32 bit pointer in user address space 
-	 * it is a pointer to the siginfo stucture in the rt stack frame 
+	 * Programs using the rs6000/xcoff abi can save up to 19 gp
+	 * regs and 18 fp regs below sp before decrementing it.
 	 */
-	u32 pinfo;
+	int			abigap[56];
+};
+
+
+/*
+ * Common utility functions used by signal and context support
+ *
+ */
+
+/*
+ * Restore the user process's signal mask
+ * (implemented in signal.c)
+ */
+extern void restore_sigmask(sigset_t *set);
+
+/*
+ * Functions for flipping sigsets (thanks to brain dead generic
+ * implementation that makes things simple for little endian only
+ */
+static inline void compat_from_sigset(compat_sigset_t *compat, sigset_t *set)
+{
+	switch (_NSIG_WORDS) {
+	case 4: compat->sig[5] = set->sig[3] & 0xffffffffull ;
+		compat->sig[7] = set->sig[3] >> 32; 
+	case 3: compat->sig[4] = set->sig[2] & 0xffffffffull ;
+		compat->sig[5] = set->sig[2] >> 32; 
+	case 2: compat->sig[2] = set->sig[1] & 0xffffffffull ;
+		compat->sig[3] = set->sig[1] >> 32; 
+	case 1: compat->sig[0] = set->sig[0] & 0xffffffffull ;
+		compat->sig[1] = set->sig[0] >> 32; 
+	}
+}
+
+static inline void sigset_from_compat(sigset_t *set, compat_sigset_t *compat)
+{
+	switch (_NSIG_WORDS) {
+	case 4: set->sig[3] = compat->sig[6] | (((long)compat->sig[7]) << 32);
+	case 3: set->sig[2] = compat->sig[4] | (((long)compat->sig[5]) << 32);
+	case 2: set->sig[1] = compat->sig[2] | (((long)compat->sig[3]) << 32);
+	case 1: set->sig[0] = compat->sig[0] | (((long)compat->sig[1]) << 32);
+	}
+}
+
+
+/*
+ * Save the current user registers on the user stack.
+ * We only save the altivec registers if the process has used
+ * altivec instructions at some point.
+ */
+static int save_user_regs(struct pt_regs *regs, struct mcontext32 *frame, int sigret)
+{
+	elf_greg_t64 *gregs = (elf_greg_t64 *)regs;
+	int i, err = 0;
+	
+	/* Make sure floating point registers are stored in regs */ 
+	if (regs->msr & MSR_FP)
+		giveup_fpu(current);
+	
+	/* save general and floating-point registers */
+	for (i = 0; i <= PT_RESULT; i ++)
+		err |= __put_user((unsigned int)gregs[i], &frame->mc_gregs[i]);
+	err |= __copy_to_user(&frame->mc_fregs, current->thread.fpr,
+			      ELF_NFPREG * sizeof(double));
+	if (err)
+		return 1;
+
+	current->thread.fpscr = 0;	/* turn off all fp exceptions */
+
+#ifdef CONFIG_ALTIVEC
+	/* save altivec registers */
+	if (current->thread.used_vr) {
+		if (regs->msr & MSR_VEC)
+			giveup_altivec(current);
+		if (__copy_to_user(&frame->mc_vregs, current->thread.vr,
+				   ELF_NVRREG32 * sizeof(vector128)))
+			return 1;
+		/* set MSR_VEC in the saved MSR value to indicate that
+		   frame->mc_vregs contains valid data */
+		if (__put_user(regs->msr | MSR_VEC, &frame->mc_gregs[PT_MSR]))
+			return 1;
+	}
+	/* else assert((regs->msr & MSR_VEC) == 0) */
+
+	/* We always copy to/from vrsave, it's 0 if we don't have or don't
+	 * use altivec. Since VSCR only contains 32 bits saved in the least
+	 * significant bits of a vector, we "cheat" and stuff VRSAVE in the
+	 * most significant bits of that same vector. --BenH
+	 */
+	if (__put_user(current->thread.vrsave, (u32 *)&frame->mc_vregs[32]))
+		return 1;
+#endif /* CONFIG_ALTIVEC */
+
+	if (sigret) {
+		/* Set up the sigreturn trampoline: li r0,sigret; sc */
+		if (__put_user(0x38000000UL + sigret, &frame->tramp[0])
+		    || __put_user(0x44000002UL, &frame->tramp[1]))
+			return 1;
+		flush_icache_range((unsigned long) &frame->tramp[0],
+				   (unsigned long) &frame->tramp[2]);
+	}
+
+	return 0;
+}
+
+/*
+ * Restore the current user register values from the user stack,
+ * (except for MSR).
+ */
+static int restore_user_regs(struct pt_regs *regs, struct mcontext32 __user *sr, int sig)
+{
+	elf_greg_t64 *gregs = (elf_greg_t64 *)regs;
+	int i, err = 0;
+	unsigned int save_r2;
+#ifdef CONFIG_ALTIVEC
+	unsigned long msr;
+#endif
+
 	/*
-	 * This is a 32 bit pointer in user address space
-	 * it is a pointer to the user context in the rt stack frame
+	 * restore general registers but not including MSR. Also take
+	 * care of keeping r2 (TLS) intact if not a signal
 	 */
-	u32 puc;
-	struct siginfo32  info;
-	struct ucontext32 uc;
-};
+	if (!sig)
+		save_r2 = (unsigned int)regs->gpr[2];
+	for (i = 0; i < PT_MSR; i ++)
+		err |= __get_user(gregs[i], &sr->mc_gregs[i]);
+	for (i ++; i <= PT_RESULT; i ++)
+		err |= __get_user(gregs[i], &sr->mc_gregs[i]);
+	if (!sig)
+		regs->gpr[2] = (unsigned long) save_r2;
+	if (err)
+		return 1;
+
+	/* force the process to reload the FP registers from
+	   current->thread when it next does FP instructions */
+	regs->msr &= ~(MSR_FP | MSR_FE0 | MSR_FE1);
+	if (__copy_from_user(current->thread.fpr, &sr->mc_fregs,
+			     sizeof(sr->mc_fregs)))
+		return 1;
+
+#ifdef CONFIG_ALTIVEC
+	/* force the process to reload the altivec registers from
+	   current->thread when it next does altivec instructions */
+	regs->msr &= ~MSR_VEC;
+	if (!__get_user(msr, &sr->mc_gregs[PT_MSR]) && (msr & MSR_VEC) != 0) {
+		/* restore altivec registers from the stack */
+		if (__copy_from_user(current->thread.vr, &sr->mc_vregs,
+				     sizeof(sr->mc_vregs)))
+			return 1;
+	} else if (current->thread.used_vr)
+		memset(&current->thread.vr, 0, ELF_NVRREG32 * sizeof(vector128));
+
+	/* Always get VRSAVE back */
+	if (__get_user(current->thread.vrsave, (u32 *)&sr->mc_vregs[32]))
+		return 1;
+#endif /* CONFIG_ALTIVEC */
 
+	return 0;
+}
 
 
 /*
@@ -133,7 +269,7 @@ long sys32_sigsuspend(old_sigset_t mask,
 	regs->gpr[3] = EINTR;
 	regs->ccr |= 0x10000000;
 	while (1) {
-		current->state = TASK_INTERRUPTIBLE;
+		set_current_state(TASK_INTERRUPTIBLE);
 		schedule();
 		if (do_signal32(&saveset, regs))
 			/*
@@ -181,209 +317,6 @@ long sys32_sigaction(int sig, struct old
 }
 
 
-/*
- * When we have signals to deliver, we set up on the
- * user stack, going down from the original stack pointer:
- *	a sigregs struct
- *	one or more sigcontext structs
- *	a gap of __SIGNAL_FRAMESIZE32 bytes
- *
- * Each of these things must be a multiple of 16 bytes in size.
- *
- */
-
-
-/*
- * Do a signal return; undo the signal stack.
- */
-long sys32_sigreturn(unsigned long r3, unsigned long r4, unsigned long r5,
-		     unsigned long r6, unsigned long r7, unsigned long r8,
-		     struct pt_regs *regs)
-{
-	struct sigcontext32 *sc, sigctx;
-	struct sigregs32 *sr;
-	int ret;
-	elf_gregset_t32 saved_regs;  /* an array of ELF_NGREG unsigned ints (32 bits) */
-	sigset_t set;
-	int i;
-
-	sc = (struct sigcontext32 *)(regs->gpr[1] + __SIGNAL_FRAMESIZE32);
-	if (copy_from_user(&sigctx, sc, sizeof(sigctx)))
-		goto badframe;
-
-	/*
-	 * Note that PPC32 puts the upper 32 bits of the sigmask in the
-	 * unused part of the signal stackframe
-	 */
-	set.sig[0] = sigctx.oldmask + ((long)(sigctx._unused[3]) << 32);
-	sigdelsetmask(&set, ~_BLOCKABLE);
-	spin_lock_irq(&current->sighand->siglock);
-	current->blocked = set;
-	recalc_sigpending();
-	spin_unlock_irq(&current->sighand->siglock);
-	if (regs->msr & MSR_FP )
-		giveup_fpu(current);
-	/* Last stacked signal - restore registers */
-	sr = (struct sigregs32*)(u64)sigctx.regs;
-	/*
-	 * copy the 32 bit register values off the user stack
-	 * into the 32 bit register area
-	 */
-	if (copy_from_user(saved_regs, &sr->gp_regs, sizeof(sr->gp_regs)))
-		goto badframe;
-	/*
-	 * The saved reg structure in the frame is an elf_grepset_t32,
-	 * it is a 32 bit register save of the registers in the
-	 * pt_regs structure that was stored on the kernel stack
-	 * during the system call when the system call was interrupted
-	 * for the signal. Only 32 bits are saved because the
-	 * sigcontext contains a pointer to the regs and the sig
-	 * context address is passed as a pointer to the signal
-	 * handler.  
-	 *
-	 * The entries in the elf_grepset have the same index as the
-	 * elements in the pt_regs structure.
-	 */
-	saved_regs[PT_MSR] = (regs->msr & ~MSR_USERCHANGE)
-		| (saved_regs[PT_MSR] & MSR_USERCHANGE);
-	/*
-	 * Register 2 is the kernel toc - should be reset on
-	 * any calls into the kernel 
-	 */
-	for (i = 0; i < 32; i++)
-		regs->gpr[i] = (u64)(saved_regs[i]) & 0xFFFFFFFF;
-
-	/*
-	 *  restore the non gpr registers 
-	 */
-	regs->msr = (u64)(saved_regs[PT_MSR]) & 0xFFFFFFFF;
-	/*
-	 * Insure that the interrupt mode is 64 bit, during 32 bit
-	 * execution. (This is necessary because we only saved
-	 * lower 32 bits of msr.)
-	 */
-	regs->msr = regs->msr | MSR_ISF;  /* When this thread is interrupted it should run in 64 bit mode. */
-
-	regs->nip = (u64)(saved_regs[PT_NIP]) & 0xFFFFFFFF;
-	regs->orig_gpr3 = (u64)(saved_regs[PT_ORIG_R3]) & 0xFFFFFFFF; 
-	regs->ctr = (u64)(saved_regs[PT_CTR]) & 0xFFFFFFFF; 
-	regs->link = (u64)(saved_regs[PT_LNK]) & 0xFFFFFFFF; 
-	regs->xer = (u64)(saved_regs[PT_XER]) & 0xFFFFFFFF; 
-	regs->ccr = (u64)(saved_regs[PT_CCR]) & 0xFFFFFFFF;
-	/* regs->softe is left unchanged (like the MSR.EE bit) */
-	/******************************************************/
-	/* the DAR and the DSISR are only relevant during a   */
-	/*   data or instruction storage interrupt. The value */
-	/*   will be set to zero.                             */
-	/******************************************************/
-	regs->dar = 0; 
-	regs->dsisr = 0;
-	regs->result = (u64)(saved_regs[PT_RESULT]) & 0xFFFFFFFF;
-
-	if (copy_from_user(current->thread.fpr, &sr->fp_regs,
-			   sizeof(sr->fp_regs)))
-		goto badframe;
-
-	ret = regs->result;
-	return ret;
-
-badframe:
-	do_exit(SIGSEGV);
-}	
-
-/*
- * Set up a signal frame.
- */
-static void setup_frame32(struct pt_regs *regs, struct sigregs32 *frame,
-            unsigned int newsp)
-{
-	struct sigcontext32 *sc = (struct sigcontext32 *)(u64)newsp;
-	int i;
-
-	/* Always make any pending restarted system calls return -EINTR */
-	current_thread_info()->restart_block.fn = do_no_restart_syscall;
-
-	if (verify_area(VERIFY_WRITE, frame, sizeof(*frame)))
-		goto badframe;
-	if (regs->msr & MSR_FP)
-		giveup_fpu(current);
-
-	/*
-	 * Copy the register contents for the pt_regs structure on the
-	 *   kernel stack to the elf_gregset_t32 structure on the user
-	 *   stack. This is a copy of 64 bit register values to 32 bit
-	 *   register values. The high order 32 bits of the 64 bit
-	 *   registers are not needed since a 32 bit application is
-	 *   running and the saved registers are the contents of the
-	 *   user registers at the time of a system call.
-	 * 
-	 * The values saved on the user stack will be restored into
-	 *  the registers during the signal return processing
-	 */
-	for (i = 0; i < 32; i++) {
-		if (__put_user((u32)regs->gpr[i], &frame->gp_regs[i]))
-			goto badframe;
-	}
-
-	/*
-	 * Copy the non gpr registers to the user stack
-	 */
-	if (__put_user((u32)regs->gpr[PT_NIP], &frame->gp_regs[PT_NIP])
-	    || __put_user((u32)regs->gpr[PT_MSR], &frame->gp_regs[PT_MSR])
-	    || __put_user((u32)regs->gpr[PT_ORIG_R3], &frame->gp_regs[PT_ORIG_R3])
-	    || __put_user((u32)regs->gpr[PT_CTR], &frame->gp_regs[PT_CTR])
-	    || __put_user((u32)regs->gpr[PT_LNK], &frame->gp_regs[PT_LNK])
-	    || __put_user((u32)regs->gpr[PT_XER], &frame->gp_regs[PT_XER])
-	    || __put_user((u32)regs->gpr[PT_CCR], &frame->gp_regs[PT_CCR])
-#if 0
-	    || __put_user((u32)regs->gpr[PT_MQ], &frame->gp_regs[PT_MQ])
-#endif
-	    || __put_user((u32)regs->gpr[PT_RESULT], &frame->gp_regs[PT_RESULT]))
-		goto badframe;
-
-
-	/*
-	 * Now copy the floating point registers onto the user stack 
-	 *
-	 * Also set up so on the completion of the signal handler, the
-	 * sys_sigreturn will get control to reset the stack
-	 */
-	if (__copy_to_user(&frame->fp_regs, current->thread.fpr,
-			   ELF_NFPREG * sizeof(double))
-	    /* li r0, __NR_sigreturn */
-	    || __put_user(0x38000000U + __NR_sigreturn, &frame->tramp[0])
-	    /* sc */
-	    || __put_user(0x44000002U, &frame->tramp[1]))
-		goto badframe;
-	flush_icache_range((unsigned long)&frame->tramp[0],
-			   (unsigned long)&frame->tramp[2]);
-	current->thread.fpscr = 0;	/* turn off all fp exceptions */
-
-	newsp -= __SIGNAL_FRAMESIZE32;
-	if (put_user(regs->gpr[1], (u32*)(u64)newsp)
-	    || get_user(regs->nip, &sc->handler)
-	    || get_user(regs->gpr[3], &sc->signal))
-		goto badframe;
-
-	regs->gpr[1] = newsp & 0xFFFFFFFF;
-	/*
-	 * first parameter to the signal handler is the signal number
-	 *  - the value is in gpr3
-	 * second parameter to the signal handler is the sigcontext
-	 *   - set the value into gpr4
-	 */
-	regs->gpr[4] = (unsigned long) sc;
-	regs->link = (unsigned long) frame->tramp;
-	return;
-
-badframe:
-#if DEBUG_SIG
-	printk("badframe in setup_frame32, regs=%p frame=%p newsp=%lx\n",
-	       regs, frame, newsp);
-#endif
-	do_exit(SIGSEGV);
-}
-
 
 /*
  *  Start of RT signal support
@@ -405,115 +338,6 @@ badframe:
  *        siginfo32to64
  */
 
-/*
- * This code executes after the rt signal handler in 32 bit mode has
- * completed and returned  
- */
-long sys32_rt_sigreturn(unsigned long r3, unsigned long r4, unsigned long r5,
-			unsigned long r6, unsigned long r7, unsigned long r8,
-			struct pt_regs * regs)
-{
-	struct rt_sigframe_32 *rt_sf;
-	struct sigcontext32 sigctx;
-	struct sigregs32 *sr;
-	int ret;
-	elf_gregset_t32 saved_regs;   /* an array of 32 bit register values */
-	sigset_t set; 
-	stack_t st;
-	int i;
-	mm_segment_t old_fs;
-
-	/* Always make any pending restarted system calls return -EINTR */
-	current_thread_info()->restart_block.fn = do_no_restart_syscall;
-
-	/* Adjust the inputted reg1 to point to the first rt signal frame */
-	rt_sf = (struct rt_sigframe_32 *)(regs->gpr[1] + __SIGNAL_FRAMESIZE32);
-	/* Copy the information from the user stack  */
-	if (copy_from_user(&sigctx, &rt_sf->uc.uc_mcontext, sizeof(sigctx))
-	    || copy_from_user(&set, &rt_sf->uc.uc_sigmask, sizeof(set))
-	    || copy_from_user(&st,&rt_sf->uc.uc_stack, sizeof(st)))
-		goto badframe;
-
-	/*
-	 * Unblock the signal that was processed 
-	 *   After a signal handler runs - 
-	 *     if the signal is blockable - the signal will be unblocked  
-	 *       (sigkill and sigstop are not blockable)
-	 */
-	sigdelsetmask(&set, ~_BLOCKABLE); 
-	/* update the current based on the sigmask found in the rt_stackframe */
-	spin_lock_irq(&current->sighand->siglock);
-	current->blocked = set;
-	recalc_sigpending();
-	spin_unlock_irq(&current->sighand->siglock);
-
-	/* If currently owning the floating point - give them up */
-	if (regs->msr & MSR_FP)
-		giveup_fpu(current);
-	/*
-	 * Set to point to the next rt_sigframe - this is used to
-	 * determine whether this is the last signal to process
-	 */
-	sr = (struct sigregs32 *)(u64)sigctx.regs;
-	if (copy_from_user(saved_regs, &sr->gp_regs, sizeof(sr->gp_regs))) 
-		goto badframe;
-	/*
-	 * The saved reg structure in the frame is an elf_grepset_t32,
-	 * it is a 32 bit register save of the registers in the
-	 * pt_regs structure that was stored on the kernel stack
-	 * during the system call when the system call was interrupted
-	 * for the signal. Only 32 bits are saved because the
-	 * sigcontext contains a pointer to the regs and the sig
-	 * context address is passed as a pointer to the signal handler
-	 *
-	 * The entries in the elf_grepset have the same index as
-	 * the elements in the pt_regs structure.
-	 */
-	saved_regs[PT_MSR] = (regs->msr & ~MSR_USERCHANGE)
-		| (saved_regs[PT_MSR] & MSR_USERCHANGE);
-	/*
-	 * Register 2 is the kernel toc - should be reset on any
-	 * calls into the kernel
-	 */
-	for (i = 0; i < 32; i++)
-		regs->gpr[i] = (u64)(saved_regs[i]) & 0xFFFFFFFF;
-	/*
-	 * restore the non gpr registers
-	 */
-	regs->msr = (u64)(saved_regs[PT_MSR]) & 0xFFFFFFFF;
-	regs->nip = (u64)(saved_regs[PT_NIP]) & 0xFFFFFFFF;
-	regs->orig_gpr3 = (u64)(saved_regs[PT_ORIG_R3]) & 0xFFFFFFFF; 
-	regs->ctr = (u64)(saved_regs[PT_CTR]) & 0xFFFFFFFF; 
-	regs->link = (u64)(saved_regs[PT_LNK]) & 0xFFFFFFFF; 
-	regs->xer = (u64)(saved_regs[PT_XER]) & 0xFFFFFFFF; 
-	regs->ccr = (u64)(saved_regs[PT_CCR]) & 0xFFFFFFFF;
-	/* regs->softe is left unchanged (like MSR.EE) */
-	/*
-	 * the DAR and the DSISR are only relevant during a
-	 *   data or instruction storage interrupt. The value
-	 *   will be set to zero.
-	 */
-	regs->dar = 0; 
-	regs->dsisr = 0;
-	regs->result = (u64)(saved_regs[PT_RESULT]) & 0xFFFFFFFF;
-	if (copy_from_user(current->thread.fpr, &sr->fp_regs,
-			   sizeof(sr->fp_regs)))
-		goto badframe;
-	/* This function sets back the stack flags into
-	   the current task structure.  */
-	old_fs = get_fs();
-	set_fs(KERNEL_DS);
-	do_sigaltstack(&st, NULL, regs->gpr[1]);
-	set_fs(old_fs);
-
-	ret = regs->result;
-	return ret;
-
- badframe:
-	do_exit(SIGSEGV);     
-}
-
-
 
 long sys32_rt_sigaction(int sig, const struct sigaction32 *act,
 		struct sigaction32 *oact, size_t sigsetsize)
@@ -530,16 +354,7 @@ long sys32_rt_sigaction(int sig, const s
 		ret = get_user((long)new_ka.sa.sa_handler, &act->sa_handler);
 		ret |= __copy_from_user(&set32, &act->sa_mask,
 					sizeof(compat_sigset_t));
-		switch (_NSIG_WORDS) {
-		case 4: new_ka.sa.sa_mask.sig[3] = set32.sig[6]
-				| (((long)set32.sig[7]) << 32);
-		case 3: new_ka.sa.sa_mask.sig[2] = set32.sig[4]
-				| (((long)set32.sig[5]) << 32);
-		case 2: new_ka.sa.sa_mask.sig[1] = set32.sig[2]
-				| (((long)set32.sig[3]) << 32);
-		case 1: new_ka.sa.sa_mask.sig[0] = set32.sig[0]
-				| (((long)set32.sig[1]) << 32);
-		}
+		sigset_from_compat(&new_ka.sa.sa_mask, &set32);
 		ret |= __get_user(new_ka.sa.sa_flags, &act->sa_flags);
 		if (ret)
 			return -EFAULT;
@@ -547,20 +362,7 @@ long sys32_rt_sigaction(int sig, const s
 
 	ret = do_sigaction(sig, act ? &new_ka : NULL, oact ? &old_ka : NULL);
 	if (!ret && oact) {
-		switch (_NSIG_WORDS) {
-		case 4:
-			set32.sig[7] = (old_ka.sa.sa_mask.sig[3] >> 32);
-			set32.sig[6] = old_ka.sa.sa_mask.sig[3];
-		case 3:
-			set32.sig[5] = (old_ka.sa.sa_mask.sig[2] >> 32);
-			set32.sig[4] = old_ka.sa.sa_mask.sig[2];
-		case 2:
-			set32.sig[3] = (old_ka.sa.sa_mask.sig[1] >> 32);
-			set32.sig[2] = old_ka.sa.sa_mask.sig[1];
-		case 1:
-			set32.sig[1] = (old_ka.sa.sa_mask.sig[0] >> 32);
-			set32.sig[0] = old_ka.sa.sa_mask.sig[0];
-		}
+		compat_from_sigset(&set32, &old_ka.sa.sa_mask);
 		ret = put_user((long)old_ka.sa.sa_handler, &oact->sa_handler);
 		ret |= __copy_to_user(&oact->sa_mask, &set32,
 				      sizeof(compat_sigset_t));
@@ -586,14 +388,8 @@ long sys32_rt_sigprocmask(u32 how, compa
 
 	if (set) {
 		if (copy_from_user (&s32, set, sizeof(compat_sigset_t)))
-			return -EFAULT;
-    
-		switch (_NSIG_WORDS) {
-		case 4: s.sig[3] = s32.sig[6] | (((long)s32.sig[7]) << 32);
-		case 3: s.sig[2] = s32.sig[4] | (((long)s32.sig[5]) << 32);
-		case 2: s.sig[1] = s32.sig[2] | (((long)s32.sig[3]) << 32);
-		case 1: s.sig[0] = s32.sig[0] | (((long)s32.sig[1]) << 32);
-		}
+			return -EFAULT;    
+		sigset_from_compat(&s, &s32);
 	}
 	
 	set_fs(KERNEL_DS);
@@ -603,12 +399,7 @@ long sys32_rt_sigprocmask(u32 how, compa
 	if (ret)
 		return ret;
 	if (oset) {
-		switch (_NSIG_WORDS) {
-		case 4: s32.sig[7] = (s.sig[3] >> 32); s32.sig[6] = s.sig[3];
-		case 3: s32.sig[5] = (s.sig[2] >> 32); s32.sig[4] = s.sig[2];
-		case 2: s32.sig[3] = (s.sig[1] >> 32); s32.sig[2] = s.sig[1];
-		case 1: s32.sig[1] = (s.sig[0] >> 32); s32.sig[0] = s.sig[0];
-		}
+		compat_from_sigset(&s32, &s);
 		if (copy_to_user (oset, &s32, sizeof(compat_sigset_t)))
 			return -EFAULT;
 	}
@@ -626,12 +417,7 @@ long sys32_rt_sigpending(compat_sigset_t
 	ret = sys_rt_sigpending(&s, sigsetsize);
 	set_fs(old_fs);
 	if (!ret) {
-		switch (_NSIG_WORDS) {
-		case 4: s32.sig[7] = (s.sig[3] >> 32); s32.sig[6] = s.sig[3];
-		case 3: s32.sig[5] = (s.sig[2] >> 32); s32.sig[4] = s.sig[2];
-		case 2: s32.sig[3] = (s.sig[1] >> 32); s32.sig[2] = s.sig[1];
-		case 1: s32.sig[1] = (s.sig[0] >> 32); s32.sig[0] = s.sig[0];
-		}
+		compat_from_sigset(&s32, &s);
 		if (copy_to_user (set, &s32, sizeof(compat_sigset_t)))
 			return -EFAULT;
 	}
@@ -639,7 +425,7 @@ long sys32_rt_sigpending(compat_sigset_t
 }
 
 
-static int copy_siginfo_to_user32(siginfo_t32 *d, siginfo_t *s)
+static int copy_siginfo_to_user32(compat_siginfo_t *d, siginfo_t *s)
 {
 	int err;
 
@@ -681,7 +467,7 @@ static int copy_siginfo_to_user32(siginf
 	return err;
 }
 
-long sys32_rt_sigtimedwait(compat_sigset_t *uthese, siginfo_t32 *uinfo,
+long sys32_rt_sigtimedwait(compat_sigset_t *uthese, compat_siginfo_t *uinfo,
 		struct compat_timespec *uts, compat_size_t sigsetsize)
 {
 	sigset_t s;
@@ -693,12 +479,7 @@ long sys32_rt_sigtimedwait(compat_sigset
 
 	if (copy_from_user(&s32, uthese, sizeof(compat_sigset_t)))
 		return -EFAULT;
-	switch (_NSIG_WORDS) {
-	case 4: s.sig[3] = s32.sig[6] | (((long)s32.sig[7]) << 32);
-	case 3: s.sig[2] = s32.sig[4] | (((long)s32.sig[5]) << 32);
-	case 2: s.sig[1] = s32.sig[2] | (((long)s32.sig[3]) << 32);
-	case 1: s.sig[0] = s32.sig[0] | (((long)s32.sig[1]) << 32);
-	}
+	sigset_from_compat(&s, &s32);
 	if (uts && get_compat_timespec(&t, uts))
 		return -EFAULT;
 	set_fs(KERNEL_DS);
@@ -714,7 +495,7 @@ long sys32_rt_sigtimedwait(compat_sigset
 
 
 
-static siginfo_t * siginfo32to64(siginfo_t *d, siginfo_t32 *s)
+static siginfo_t * siginfo32to64(siginfo_t *d, compat_siginfo_t *s)
 {
 	d->si_signo = s->si_signo;
 	d->si_errno = s->si_errno;
@@ -758,14 +539,14 @@ static siginfo_t * siginfo32to64(siginfo
  * (msr in 32-bit mode) and the register representation of a signed int
  * (msr in 64-bit mode) is performed.
  */
-long sys32_rt_sigqueueinfo(u32 pid, u32 sig, siginfo_t32 *uinfo)
+long sys32_rt_sigqueueinfo(u32 pid, u32 sig, compat_siginfo_t *uinfo)
 {
 	siginfo_t info;
-	siginfo_t32 info32;
+	compat_siginfo_t info32;
 	int ret;
 	mm_segment_t old_fs = get_fs();
 	
-	if (copy_from_user (&info32, uinfo, sizeof(siginfo_t32)))
+	if (copy_from_user (&info32, uinfo, sizeof(compat_siginfo_t)))
 		return -EFAULT;
     	/* XXX: Is this correct? */
 	siginfo32to64(&info, &info32);
@@ -793,15 +574,9 @@ int sys32_rt_sigsuspend(compat_sigset_t*
 	 * Swap the 2 words of the 64-bit sigset_t (they are stored
 	 * in the "wrong" endian in 32-bit user storage).
 	 */
-	switch (_NSIG_WORDS) {
-	case 4: newset.sig[3] = s32.sig[6] | (((long)s32.sig[7]) << 32);
-	case 3: newset.sig[2] = s32.sig[4] | (((long)s32.sig[5]) << 32);
-	case 2: newset.sig[1] = s32.sig[2] | (((long)s32.sig[3]) << 32);
-	case 1: newset.sig[0] = s32.sig[0] | (((long)s32.sig[1]) << 32);
-	}
+	sigset_from_compat(&newset, &s32);
 
 	sigdelsetmask(&newset, ~_BLOCKABLE);
-
 	spin_lock_irq(&current->sighand->siglock);
 	saveset = current->blocked;
 	current->blocked = newset;
@@ -812,7 +587,7 @@ int sys32_rt_sigsuspend(compat_sigset_t*
 	regs->gpr[3] = EINTR;
 	regs->ccr |= 0x10000000;
 	while (1) {
-		current->state = TASK_INTERRUPTIBLE;
+		set_current_state(TASK_INTERRUPTIBLE);
 		schedule();
 		if (do_signal32(&saveset, regs))
 			/*
@@ -827,237 +602,324 @@ int sys32_rt_sigsuspend(compat_sigset_t*
 	}
 }
 
-
 /*
- * Set up a rt signal frame.
+ *  Start Alternate signal stack support
+ *
+ *  System Calls
+ *       sigaltatck               sys32_sigaltstack
  */
-static void setup_rt_frame32(struct pt_regs *regs, struct sigregs32 *frame,
-            unsigned int newsp)
+
+int sys32_sigaltstack(u32 newstack, u32 oldstack, int r5,
+		      int r6, int r7, int r8, struct pt_regs *regs)
 {
-	unsigned int copyreg4, copyreg5;
-	struct rt_sigframe_32 * rt_sf = (struct rt_sigframe_32 *) (u64)newsp;
-	int i;
-  
-	if (verify_area(VERIFY_WRITE, frame, sizeof(*frame)))
-		goto badframe;
-	if (regs->msr & MSR_FP)
-		giveup_fpu(current);
+	stack_t uss, uoss;
+	int ret;
+	mm_segment_t old_fs;
+	unsigned long sp;
 
 	/*
-	 * Copy the register contents for the pt_regs structure on the
-	 *   kernel stack to the elf_gregset_t32 structure on the user
-	 *   stack. This is a copy of 64 bit register values to 32 bit
-	 *   register values. The high order 32 bits of the 64 bit
-	 *   registers are not needed since a 32 bit application is
-	 *   running and the saved registers are the contents of the
-	 *   user registers at the time of a system call.
-	 *
-	 * The values saved on the user stack will be restored into
-	 *  the registers during the signal return processing
+	 * set sp to the user stack on entry to the system call
+	 * the system call router sets R9 to the saved registers
 	 */
-	for (i = 0; i < 32; i++) {
-		if (__put_user((u32)regs->gpr[i], &frame->gp_regs[i]))
-			goto badframe;
-	}
+	sp = regs->gpr[1];
 
-	/*
-	 * Copy the non gpr registers to the user stack
-	 */
-	if (__put_user((u32)regs->gpr[PT_NIP], &frame->gp_regs[PT_NIP])
-	    || __put_user((u32)regs->gpr[PT_MSR], &frame->gp_regs[PT_MSR])
-	    || __put_user((u32)regs->gpr[PT_ORIG_R3], &frame->gp_regs[PT_ORIG_R3])
-	    || __put_user((u32)regs->gpr[PT_CTR], &frame->gp_regs[PT_CTR])
-	    || __put_user((u32)regs->gpr[PT_LNK], &frame->gp_regs[PT_LNK])
-	    || __put_user((u32)regs->gpr[PT_XER], &frame->gp_regs[PT_XER])
-	    || __put_user((u32)regs->gpr[PT_CCR], &frame->gp_regs[PT_CCR])
-	    || __put_user((u32)regs->gpr[PT_RESULT], &frame->gp_regs[PT_RESULT]))
-		goto badframe;
+	/* Put new stack info in local 64 bit stack struct */
+	if (newstack &&
+		(get_user((long)uss.ss_sp,
+			  &((stack_32_t *)(long)newstack)->ss_sp) ||
+		 __get_user(uss.ss_flags,
+			 &((stack_32_t *)(long)newstack)->ss_flags) ||
+		 __get_user(uss.ss_size,
+			 &((stack_32_t *)(long)newstack)->ss_size)))
+		return -EFAULT; 
 
+	old_fs = get_fs();
+	set_fs(KERNEL_DS);
+	ret = do_sigaltstack(newstack ? &uss : NULL, oldstack ? &uoss : NULL,
+			sp);
+	set_fs(old_fs);
+	/* Copy the stack information to the user output buffer */
+	if (!ret && oldstack  &&
+		(put_user((long)uoss.ss_sp,
+			  &((stack_32_t *)(long)oldstack)->ss_sp) ||
+		 __put_user(uoss.ss_flags,
+			 &((stack_32_t *)(long)oldstack)->ss_flags) ||
+		 __put_user(uoss.ss_size,
+			 &((stack_32_t *)(long)oldstack)->ss_size)))
+		return -EFAULT;
+	return ret;
+}
 
-	/*
-	 * Now copy the floating point registers onto the user stack
-	 *
-	 * Also set up so on the completion of the signal handler, the
-	 * sys_sigreturn will get control to reset the stack
-	 */
-	if (__copy_to_user(&frame->fp_regs, current->thread.fpr,
-			   ELF_NFPREG * sizeof(double))
-	    || __put_user(0x38000000U + __NR_rt_sigreturn, &frame->tramp[0])    /* li r0, __NR_rt_sigreturn */
-	    || __put_user(0x44000002U, &frame->tramp[1]))   /* sc */
+
+/*
+ * Set up a signal frame for a "real-time" signal handler
+ * (one which gets siginfo).
+ */
+static void handle_rt_signal32(unsigned long sig, struct k_sigaction *ka,
+			       siginfo_t *info, sigset_t *oldset,
+			       struct pt_regs * regs, unsigned long newsp)
+{
+	struct rt_sigframe32 __user *rt_sf;
+	struct mcontext32 __user *frame;
+	unsigned long origsp = newsp;
+	compat_sigset_t c_oldset;
+
+	/* Set up Signal Frame */
+	/* Put a Real Time Context onto stack */
+	newsp -= sizeof(*rt_sf);
+	rt_sf = (struct rt_sigframe32 __user *)newsp;
+
+	/* create a stack frame for the caller of the handler */
+	newsp -= __SIGNAL_FRAMESIZE32 + 16;
+
+	if (verify_area(VERIFY_WRITE, (void __user *)newsp, origsp - newsp))
 		goto badframe;
 
-	flush_icache_range((unsigned long) &frame->tramp[0],
-			   (unsigned long) &frame->tramp[2]);
-	current->thread.fpscr = 0;	/* turn off all fp exceptions */
+	compat_from_sigset(&c_oldset, oldset);
 
-	/*
-	 * Retrieve rt_sigframe from stack and
-	 * set up registers for signal handler
-	 */
-	newsp -= __SIGNAL_FRAMESIZE32;
-      
+	/* Put the siginfo & fill in most of the ucontext */
+	if (copy_siginfo_to_user32(&rt_sf->info, info)
+	    || __put_user(0, &rt_sf->uc.uc_flags)
+	    || __put_user(0, &rt_sf->uc.uc_link)
+	    || __put_user(current->sas_ss_sp, &rt_sf->uc.uc_stack.ss_sp)
+	    || __put_user(sas_ss_flags(regs->gpr[1]),
+			  &rt_sf->uc.uc_stack.ss_flags)
+	    || __put_user(current->sas_ss_size, &rt_sf->uc.uc_stack.ss_size)
+	    || __put_user((u32)(u64)&rt_sf->uc.uc_mcontext, &rt_sf->uc.uc_regs)
+	    || __copy_to_user(&rt_sf->uc.uc_sigmask, &c_oldset, sizeof(c_oldset)))
+		goto badframe;
 
-	if (put_user((u32)(regs->gpr[1]), (unsigned int *)(u64)newsp)
-	    || get_user(regs->nip, &rt_sf->uc.uc_mcontext.handler)
-	    || get_user(regs->gpr[3], &rt_sf->uc.uc_mcontext.signal)
-	    || get_user(copyreg4, &rt_sf->pinfo)
-	    || get_user(copyreg5, &rt_sf->puc))
+	/* Save user registers on the stack */
+	frame = &rt_sf->uc.uc_mcontext;
+	if (save_user_regs(regs, frame, __NR_rt_sigreturn))
 		goto badframe;
 
-	regs->gpr[4] = copyreg4;
-	regs->gpr[5] = copyreg5;
-	regs->gpr[1] = newsp;
+	if (put_user(regs->gpr[1], (unsigned long __user *)newsp))
+		goto badframe;
+	regs->gpr[1] = (unsigned long) newsp;
+	regs->gpr[3] = sig;
+	regs->gpr[4] = (unsigned long) &rt_sf->info;
+	regs->gpr[5] = (unsigned long) &rt_sf->uc;
 	regs->gpr[6] = (unsigned long) rt_sf;
+	regs->nip = (unsigned long) ka->sa.sa_handler;
 	regs->link = (unsigned long) frame->tramp;
+	regs->trap = 0;
 
 	return;
 
 badframe:
 #if DEBUG_SIG
-	printk("badframe in setup_frame32, regs=%p frame=%p newsp=%lx\n",
+	printk("badframe in handle_rt_signal, regs=%p frame=%p newsp=%lx\n",
 	       regs, frame, newsp);
 #endif
-	do_exit(SIGSEGV);
+	if (sig == SIGSEGV)
+		ka->sa.sa_handler = SIG_DFL;
+	force_sig(SIGSEGV, current);
 }
 
+static long do_setcontext32(struct ucontext32 __user *ucp, struct pt_regs *regs, int sig)
+{
+	compat_sigset_t c_set;
+	sigset_t set;
+	u32 mcp;
+
+	if (__copy_from_user(&c_set, &ucp->uc_sigmask, sizeof(c_set))
+	    || __get_user(mcp, &ucp->uc_regs))
+		return -EFAULT;
+	sigset_from_compat(&set, &c_set);
+	restore_sigmask(&set);
+	if (restore_user_regs(regs, (struct mcontext32 *)(u64)mcp, sig))
+		return -EFAULT;
+
+	return 0;
+}
 
 /*
- * OK, we're invoking a handler
+ * Handle {get,set,swap}_context operations for 32 bits processes
  */
-static void handle_signal32(unsigned long sig, siginfo_t *info,
-		sigset_t *oldset, struct pt_regs * regs, unsigned int *newspp,
-		unsigned int frame)
-{
-	struct sigcontext32 *sc;
-	struct rt_sigframe_32 *rt_sf;
-	struct k_sigaction *ka = &current->sighand->action[sig-1];
-
-	if (regs->trap == 0x0C00 /* System Call! */
-	    && ((int)regs->result == -ERESTARTNOHAND ||
-		(int)regs->result == -ERESTART_RESTARTBLOCK ||
-		((int)regs->result == -ERESTARTSYS &&
-		 !(ka->sa.sa_flags & SA_RESTART)))) {
-		if ((int)regs->result == -ERESTART_RESTARTBLOCK)
-			current_thread_info()->restart_block.fn
-				= do_no_restart_syscall;
-		regs->result = -EINTR;
+
+long sys32_swapcontext(struct ucontext32 __user *old_ctx,
+		       struct ucontext32 __user *new_ctx,
+		       int ctx_size, int r6, int r7, int r8, struct pt_regs *regs)
+{
+	unsigned char tmp;
+	compat_sigset_t c_set;
+
+	/* Context size is for future use. Right now, we only make sure
+	 * we are passed something we understand
+	 */
+	if (ctx_size < sizeof(struct ucontext32))
+		return -EINVAL;
+
+	if (old_ctx != NULL) {
+		compat_from_sigset(&c_set, &current->blocked);
+		if (verify_area(VERIFY_WRITE, old_ctx, sizeof(*old_ctx))
+		    || save_user_regs(regs, &old_ctx->uc_mcontext, 0)
+		    || __copy_to_user(&old_ctx->uc_sigmask, &c_set, sizeof(c_set))
+		    || __put_user((u32)(u64)&old_ctx->uc_mcontext, &old_ctx->uc_regs))
+			return -EFAULT;
 	}
+	if (new_ctx == NULL)
+		return 0;
+	if (verify_area(VERIFY_READ, new_ctx, sizeof(*new_ctx))
+	    || __get_user(tmp, (u8 *) new_ctx)
+	    || __get_user(tmp, (u8 *) (new_ctx + 1) - 1))
+		return -EFAULT;
 
 	/*
-	 * Set up the signal frame
-	 * Determine if a real time frame and a siginfo is required
+	 * If we get a fault copying the context into the kernel's
+	 * image of the user's registers, we can't just return -EFAULT
+	 * because the user's registers will be corrupted.  For instance
+	 * the NIP value may have been updated but not some of the
+	 * other registers.  Given that we have done the verify_area
+	 * and successfully read the first and last bytes of the region
+	 * above, this should only happen in an out-of-memory situation
+	 * or if another thread unmaps the region containing the context.
+	 * We kill the task with a SIGSEGV in this situation.
 	 */
-	if (ka->sa.sa_flags & SA_SIGINFO) {
-		*newspp -= sizeof(*rt_sf);
-		rt_sf = (struct rt_sigframe_32 *)(u64)(*newspp);
-		if (verify_area(VERIFY_WRITE, rt_sf, sizeof(*rt_sf)))
-			goto badframe;
-		if (__put_user((u32)(u64)ka->sa.sa_handler,
-					&rt_sf->uc.uc_mcontext.handler)
-		    || __put_user((u32)(u64)&rt_sf->info, &rt_sf->pinfo)
-		    || __put_user((u32)(u64)&rt_sf->uc, &rt_sf->puc)
-		    /*  put the siginfo on the user stack                    */
-		    || copy_siginfo_to_user32(&rt_sf->info, info)
-		    /*  set the ucontext on the user stack                   */ 
-		    || __put_user(0, &rt_sf->uc.uc_flags)
-		    || __put_user(0, &rt_sf->uc.uc_link)
-		    || __put_user(current->sas_ss_sp, &rt_sf->uc.uc_stack.ss_sp)
-		    || __put_user(sas_ss_flags(regs->gpr[1]),
-			    &rt_sf->uc.uc_stack.ss_flags)
-		    || __put_user(current->sas_ss_size,
-			    &rt_sf->uc.uc_stack.ss_size)
-		    || __copy_to_user(&rt_sf->uc.uc_sigmask,
-			    oldset, sizeof(*oldset))
-		    /* point the mcontext.regs to the pramble register frame  */
-		    || __put_user(frame, &rt_sf->uc.uc_mcontext.regs)
-		    || __put_user(sig,&rt_sf->uc.uc_mcontext.signal))
-			goto badframe; 
-	} else {
-		/* Put a sigcontext on the stack */
-		*newspp -= sizeof(*sc);
-		sc = (struct sigcontext32 *)(u64)*newspp;
-		if (verify_area(VERIFY_WRITE, sc, sizeof(*sc)))
-			goto badframe;
-		/*
-		 * Note the upper 32 bits of the signal mask are stored
-		 * in the unused part of the signal stack frame
-		 */
-		if (__put_user((u32)(u64)ka->sa.sa_handler, &sc->handler)
-		    || __put_user(oldset->sig[0], &sc->oldmask)
-		    || __put_user((oldset->sig[0] >> 32), &sc->_unused[3])
-		    || __put_user((unsigned int)frame, &sc->regs)
-		    || __put_user(sig, &sc->signal))
-			goto badframe;
-	}
+	if (do_setcontext32(new_ctx, regs, 0))
+		do_exit(SIGSEGV);
 
-	if (ka->sa.sa_flags & SA_ONESHOT)
-		ka->sa.sa_handler = SIG_DFL;
+	return 0;
+}
+
+long sys32_rt_sigreturn(int r3, int r4, int r5, int r6, int r7, int r8,
+		     struct pt_regs *regs)
+{
+	struct rt_sigframe32 __user *rt_sf;
+	int ret;
+
+
+	/* Always make any pending restarted system calls return -EINTR */
+	current_thread_info()->restart_block.fn = do_no_restart_syscall;
+
+	rt_sf = (struct rt_sigframe32 __user *)
+		(regs->gpr[1] + __SIGNAL_FRAMESIZE32 + 16);
+	if (verify_area(VERIFY_READ, rt_sf, sizeof(*rt_sf)))
+		goto bad;
+	if (do_setcontext32(&rt_sf->uc, regs, 1))
+		goto bad;
+
+	/*
+	 * It's not clear whether or why it is desirable to save the
+	 * sigaltstack setting on signal delivery and restore it on
+	 * signal return.  But other architectures do this and we have
+	 * always done it up until now so it is probably better not to
+	 * change it.  -- paulus
+	 * We use the sys32_ version that does the 32/64 bits conversion
+	 * and takes userland pointer directly. What about error checking ?
+	 * nobody does any...
+	 */
+       	sys32_sigaltstack((u32)(u64)&rt_sf->uc.uc_stack, 0, 0, 0, 0, 0, regs);
+
+	regs->result &= 0xFFFFFFFF;
+	ret = regs->result;
+
+	return ret;
+
+ bad:
+	force_sig(SIGSEGV, current);
+	return 0;
+}
+
+
+/*
+ * OK, we're invoking a handler
+ */
+static void handle_signal32(unsigned long sig, struct k_sigaction *ka,
+			    siginfo_t *info, sigset_t *oldset,
+			    struct pt_regs * regs, unsigned long newsp)
+{
+	struct sigcontext32 __user *sc;
+	struct sigregs32 __user *frame;
+	unsigned long origsp = newsp;
+
+	/* Set up Signal Frame */
+	newsp -= sizeof(struct sigregs32);
+	frame = (struct sigregs32 __user *) newsp;
+
+	/* Put a sigcontext on the stack */
+	newsp -= sizeof(*sc);
+	sc = (struct sigcontext32 __user *) newsp;
+
+	/* create a stack frame for the caller of the handler */
+	newsp -= __SIGNAL_FRAMESIZE32;
+
+	if (verify_area(VERIFY_WRITE, (void *) newsp, origsp - newsp))
+		goto badframe;
+
+#if _NSIG != 64
+#error "Please adjust handle_signal32()"
+#endif
+	if (__put_user((u32)(u64)ka->sa.sa_handler, &sc->handler)
+	    || __put_user(oldset->sig[0], &sc->oldmask)
+	    || __put_user((oldset->sig[0] >> 32), &sc->_unused[3])
+	    || __put_user((u32)(u64)frame, &sc->regs)
+	    || __put_user(sig, &sc->signal))
+		goto badframe;
+
+	if (save_user_regs(regs, &frame->mctx, __NR_sigreturn))
+		goto badframe;
+
+	if (put_user(regs->gpr[1], (unsigned long __user *)newsp))
+		goto badframe;
+	regs->gpr[1] = (unsigned long) newsp;
+	regs->gpr[3] = sig;
+	regs->gpr[4] = (unsigned long) sc;
+	regs->nip = (unsigned long) ka->sa.sa_handler;
+	regs->link = (unsigned long) frame->mctx.tramp;
+	regs->trap = 0;
 
-	if (!(ka->sa.sa_flags & SA_NODEFER)) {
-		spin_lock_irq(&current->sighand->siglock);
-		sigorsets(&current->blocked,&current->blocked,&ka->sa.sa_mask);
-		sigaddset(&current->blocked,sig);
-		recalc_sigpending();
-		spin_unlock_irq(&current->sighand->siglock);
-	}
 	return;
 
 badframe:
 #if DEBUG_SIG
-	printk("badframe in handle_signal32, regs=%p frame=%lx newsp=%lx\n",
+	printk("badframe in handle_signal, regs=%p frame=%x newsp=%x\n",
 	       regs, frame, *newspp);
-	printk("sc=%p sig=%d ka=%p info=%p oldset=%p\n", sc, sig, ka, info, oldset);
 #endif
-	do_exit(SIGSEGV);
+	if (sig == SIGSEGV)
+		ka->sa.sa_handler = SIG_DFL;
+	force_sig(SIGSEGV, current);
 }
 
-
 /*
- *  Start Alternate signal stack support
- *
- *  System Calls
- *       sigaltatck               sys32_sigaltstack
+ * Do a signal return; undo the signal stack.
  */
-
-int sys32_sigaltstack(u32 newstack, u32 oldstack, int p3,
-		      int p4, int p6, int p7, struct pt_regs *regs)
+long sys32_sigreturn(int r3, int r4, int r5, int r6, int r7, int r8,
+		       struct pt_regs *regs)
 {
-	stack_t uss, uoss;
+	struct sigcontext32 __user *sc;
+	struct sigcontext32 sigctx;
+	struct mcontext32 __user *sr;
+	sigset_t set;
 	int ret;
-	mm_segment_t old_fs;
-	unsigned long sp;
+
+	/* Always make any pending restarted system calls return -EINTR */
+	current_thread_info()->restart_block.fn = do_no_restart_syscall;
+
+	sc = (struct sigcontext32 __user *)(regs->gpr[1] + __SIGNAL_FRAMESIZE32);
+	if (copy_from_user(&sigctx, sc, sizeof(sigctx)))
+		goto badframe;
 
 	/*
-	 * set sp to the user stack on entry to the system call
-	 * the system call router sets R9 to the saved registers
+	 * Note that PPC32 puts the upper 32 bits of the sigmask in the
+	 * unused part of the signal stackframe
 	 */
-	sp = regs->gpr[1];
+	set.sig[0] = sigctx.oldmask + ((long)(sigctx._unused[3]) << 32);
+	restore_sigmask(&set);
 
-	/* Put new stack info in local 64 bit stack struct */
-	if (newstack &&
-		(get_user((long)uss.ss_sp,
-			  &((stack_32_t *)(long)newstack)->ss_sp) ||
-		 __get_user(uss.ss_flags,
-			 &((stack_32_t *)(long)newstack)->ss_flags) ||
-		 __get_user(uss.ss_size,
-			 &((stack_32_t *)(long)newstack)->ss_size)))
-		return -EFAULT; 
+	sr = (struct mcontext32 *)(u64)sigctx.regs;
+	if (verify_area(VERIFY_READ, sr, sizeof(*sr))
+	    || restore_user_regs(regs, sr, 1))
+		goto badframe;
 
-	old_fs = get_fs();
-	set_fs(KERNEL_DS);
-	ret = do_sigaltstack(newstack ? &uss : NULL, oldstack ? &uoss : NULL,
-			sp);
-	set_fs(old_fs);
-	/* Copy the stack information to the user output buffer */
-	if (!ret && oldstack  &&
-		(put_user((long)uoss.ss_sp,
-			  &((stack_32_t *)(long)oldstack)->ss_sp) ||
-		 __put_user(uoss.ss_flags,
-			 &((stack_32_t *)(long)oldstack)->ss_flags) ||
-		 __put_user(uoss.ss_size,
-			 &((stack_32_t *)(long)oldstack)->ss_size)))
-		return -EFAULT;
+	regs->result &= 0xFFFFFFFF;
+	ret = regs->result;
 	return ret;
+
+badframe:
+	force_sig(SIGSEGV, current);
+	return 0;
 }
 
 
@@ -1082,7 +944,7 @@ int do_signal32(sigset_t *oldset, struct
 	siginfo_t info;
 	struct k_sigaction *ka;
 	unsigned int frame, newsp;
-	int signr;
+	int signr, ret;
 
 	if (!oldset)
 		oldset = &current->blocked;
@@ -1090,40 +952,60 @@ int do_signal32(sigset_t *oldset, struct
 	newsp = frame = 0;
 
 	signr = get_signal_to_deliver(&info, regs, NULL);
-	if (signr > 0) {
-		ka = &current->sighand->action[signr-1];
-		if ((ka->sa.sa_flags & SA_ONSTACK)
-		     && (!on_sig_stack(regs->gpr[1])))
-			newsp = (current->sas_ss_sp + current->sas_ss_size);
-		else
-			newsp = regs->gpr[1];
-		newsp = frame = newsp - sizeof(struct sigregs32);
 
-		/* Whee!  Actually deliver the signal.  */
-		handle_signal32(signr, &info, oldset, regs, &newsp, frame);
-	}
+	ka = (signr == 0)? NULL: &current->sighand->action[signr-1];
 
-	if (regs->trap == 0x0C00) {	/* System Call! */
-		if ((int)regs->result == -ERESTARTNOHAND ||
-		    (int)regs->result == -ERESTARTSYS ||
-		    (int)regs->result == -ERESTARTNOINTR) {
-			regs->gpr[3] = regs->orig_gpr3;
-			regs->nip -= 4; /* Back up & retry system call */
-			regs->result = 0;
-		} else if ((int)regs->result == -ERESTART_RESTARTBLOCK) {
-			regs->gpr[0] = __NR_restart_syscall;
-			regs->nip -= 4;
+	if (regs->trap == 0x0C00		/* System Call! */
+	    && regs->ccr & 0x10000000		/* error signalled */
+	    && ((ret = regs->gpr[3]) == ERESTARTSYS
+		|| ret == ERESTARTNOHAND || ret == ERESTARTNOINTR
+		|| ret == ERESTART_RESTARTBLOCK)) {
+
+		if (signr > 0
+		    && (ret == ERESTARTNOHAND || ret == ERESTART_RESTARTBLOCK
+			|| (ret == ERESTARTSYS
+			    && !(ka->sa.sa_flags & SA_RESTART)))) {
+			/* make the system call return an EINTR error */
+			regs->result = -EINTR;
+			regs->gpr[3] = EINTR;
+			/* note that the cr0.SO bit is already set */
+		} else {
+			regs->nip -= 4;	/* Back up & retry system call */
 			regs->result = 0;
+			regs->trap = 0;
+			if (ret == ERESTART_RESTARTBLOCK)
+				regs->gpr[0] = __NR_restart_syscall;
+			else
+				regs->gpr[3] = regs->orig_gpr3;
 		}
 	}
 
-	if (newsp == frame)
+	if (signr == 0)
 		return 0;		/* no signals delivered */
 
-	/* Invoke correct stack setup routine */
+	if ((ka->sa.sa_flags & SA_ONSTACK) && current->sas_ss_size
+	    && (!on_sig_stack(regs->gpr[1])))
+		newsp = (current->sas_ss_sp + current->sas_ss_size);
+	else
+		newsp = regs->gpr[1];
+	newsp &= ~0xfUL;
+
+	/* Whee!  Actually deliver the signal.  */
 	if (ka->sa.sa_flags & SA_SIGINFO)
-		setup_rt_frame32(regs, (struct sigregs32*)(u64)frame, newsp);
+		handle_rt_signal32(signr, ka, &info, oldset, regs, newsp);
 	else
-		setup_frame32(regs, (struct sigregs32*)(u64)frame, newsp);
+		handle_signal32(signr, ka, &info, oldset, regs, newsp);
+
+	if (ka->sa.sa_flags & SA_ONESHOT)
+		ka->sa.sa_handler = SIG_DFL;
+
+	if (!(ka->sa.sa_flags & SA_NODEFER)) {
+		spin_lock_irq(&current->sighand->siglock);
+		sigorsets(&current->blocked,&current->blocked,&ka->sa.sa_mask);
+		sigaddset(&current->blocked, signr);
+		recalc_sigpending();
+		spin_unlock_irq(&current->sighand->siglock);
+	}
+
 	return 1;
 }
diff -purN linux-2.5/arch/ppc64/kernel/smp.c linuxppc64-2.5/arch/ppc64/kernel/smp.c
--- linux-2.5/arch/ppc64/kernel/smp.c	2003-10-08 02:53:40.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/smp.c	2003-11-24 04:24:36.000000000 +0000
@@ -50,13 +50,21 @@
 #include <asm/xics.h>
 #include <asm/cputable.h>
 
+#ifdef CONFIG_KDB
+#include <linux/kdb.h>
+#endif
+
 int smp_threads_ready;
 unsigned long cache_decay_ticks;
 
-/* initialised so it doesn't end up in bss */
+/* Initialised so it doesn't end up in bss */
+cpumask_t cpu_possible_map    = CPU_MASK_NONE;
 cpumask_t cpu_online_map = CPU_MASK_NONE;
+cpumask_t cpu_available_map   = CPU_MASK_NONE;
+cpumask_t cpu_present_at_boot = CPU_MASK_NONE;
 
 EXPORT_SYMBOL(cpu_online_map);
+EXPORT_SYMBOL(cpu_possible_map);
 
 static struct smp_ops_t *smp_ops;
 
@@ -67,6 +75,8 @@ extern unsigned char stab_array[];
 extern int cpu_idle(void *unused);
 void smp_call_function_interrupt(void);
 void smp_message_pass(int target, int msg, unsigned long data, int wait);
+extern long register_vpa(unsigned long flags, unsigned long proc,
+			 unsigned long vpa);
 
 #define smp_message_pass(t,m,d,w) smp_ops->message_pass((t),(m),(d),(w))
 
@@ -77,6 +87,24 @@ static inline void set_tb(unsigned int u
 	mttbl(lower);
 }
 
+#ifdef CONFIG_KDB
+	/* save regs here before calling kdb_ipi */
+struct pt_regs *kdb_smp_regs[NR_CPUS];
+	
+/* called for each processor.. drop each into kdb. */
+static void smp_kdb_stop_proc(void *dummy)
+{
+    kdb_ipi(kdb_smp_regs[smp_processor_id()], NULL);
+}
+	
+void smp_kdb_stop(void)
+{
+    int ret=0;
+    ret = smp_call_function(smp_kdb_stop_proc, NULL, 1, 0);
+}
+#endif
+
+
 #ifdef CONFIG_PPC_ISERIES
 static unsigned long iSeries_smp_message[NR_CPUS];
 
@@ -120,6 +148,9 @@ static int smp_iSeries_numProcs(void)
         for (i=0; i < NR_CPUS; ++i) {
                 lpPaca = paca[i].xLpPacaPtr;
                 if ( lpPaca->xDynProcStatus < 2 ) {
+			cpu_set(i, cpu_available_map);
+			cpu_set(i, cpu_possible_map);
+			cpu_set(i, cpu_present_at_boot);
                         ++np;
                 }
         }
@@ -135,7 +166,7 @@ static int smp_iSeries_probe(void)
 	for (i=0; i < NR_CPUS; ++i) {
 		lpPaca = paca[i].xLpPacaPtr;
 		if (lpPaca->xDynProcStatus < 2) {
-			paca[i].active = 1;
+			/*paca[i].active = 1;*/
 			++np;
 		}
 	}
@@ -181,7 +212,6 @@ void __init smp_init_iSeries(void)
 	smp_ops->probe        = smp_iSeries_probe;
 	smp_ops->kick_cpu     = smp_iSeries_kick_cpu;
 	smp_ops->setup_cpu    = smp_iSeries_setup_cpu;
-#warning fix for iseries
 	systemcfg->processorCount	= smp_iSeries_numProcs();
 }
 #endif
@@ -266,6 +296,17 @@ static void __init smp_space_timers(unsi
 }
 
 #ifdef CONFIG_PPC_PSERIES
+void vpa_init(int cpu)
+{
+	unsigned long flags;
+
+	/* Register the Virtual Processor Area (VPA) */
+	printk(KERN_INFO "register_vpa: cpu 0x%x\n", cpu);
+	flags = 1UL << (63 - 18);
+	paca[cpu].xLpPaca.xSLBCount = 64; /* SLB restore highwater mark */
+	register_vpa(flags, cpu, __pa((unsigned long)&(paca[cpu].xLpPaca))); 
+}
+
 static void __devinit pSeries_setup_cpu(int cpu)
 {
 	if (OpenPIC_Addr) {
@@ -295,6 +336,8 @@ smp_xics_message_pass(int target, int ms
 	}
 }
 
+extern void xics_request_IPIs(void);
+
 static int __init smp_xics_probe(void)
 {
 	int i;
@@ -305,7 +348,6 @@ static int __init smp_xics_probe(void)
 			nr_cpus++;
 	}
 #ifdef CONFIG_SMP
-	extern void xics_request_IPIs(void);
 	xics_request_IPIs();
 #endif
 
@@ -372,6 +414,9 @@ void smp_message_recv(int msg, struct pt
 {
 	switch( msg ) {
 	case PPC_MSG_CALL_FUNCTION:
+#ifdef CONFIG_KDB
+	        kdb_smp_regs[smp_processor_id()]=regs;
+#endif
 		smp_call_function_interrupt();
 		break;
 	case PPC_MSG_RESCHEDULE: 
@@ -480,13 +525,13 @@ int smp_call_function (void (*func) (voi
 	while (atomic_read(&data.started) != cpus) {
 		HMT_low();
 		if (--timeout == 0) {
+			printk("smp_call_function on cpu %d: other cpus not "
+			       "responding (%d)\n", smp_processor_id(),
+			       atomic_read(&data.started));
 #ifdef CONFIG_DEBUG_KERNEL
 			if (debugger)
 				debugger(0);
 #endif
-			printk("smp_call_function on cpu %d: other cpus not "
-			       "responding (%d)\n", smp_processor_id(),
-			       atomic_read(&data.started));
 			goto out;
 		}
 	}
@@ -496,15 +541,15 @@ int smp_call_function (void (*func) (voi
 		while (atomic_read(&data.finished) != cpus) {
 			HMT_low();
 			if (--timeout == 0) {
-#ifdef CONFIG_DEBUG_KERNEL
-				if (debugger)
-					debugger(0);
-#endif
 				printk("smp_call_function on cpu %d: other "
 				       "cpus not finishing (%d/%d)\n",
 				       smp_processor_id(),
 				       atomic_read(&data.finished),
 				       atomic_read(&data.started));
+#ifdef CONFIG_DEBUG_KERNEL
+				if (debugger)
+					debugger(0);
+#endif
 				goto out;
 			}
 		}
@@ -513,6 +558,7 @@ int smp_call_function (void (*func) (voi
 	ret = 0;
 
 out:
+	call_data = NULL;
 	HMT_medium();
 	spin_unlock(&call_lock);
 	return ret;
@@ -520,9 +566,19 @@ out:
 
 void smp_call_function_interrupt(void)
 {
-	void (*func) (void *info) = call_data->func;
-	void *info = call_data->info;
-	int wait = call_data->wait;
+	void (*func) (void *info);
+	void *info;
+	int wait;
+
+	/* call_data will be NULL if the sender timed out while
+	 * waiting on us to receive the call.
+	 */
+	if (!call_data)
+		return;
+
+	func = call_data->func;
+	info = call_data->info;
+	wait = call_data->wait;
 
 	/*
 	 * Notify initiating CPU that I've grabbed the data and am
@@ -658,6 +714,14 @@ int __devinit start_secondary(void *unus
 	if (smp_ops->take_timebase)
 		smp_ops->take_timebase();
 
+	get_paca()->yielded = 0;
+
+#ifdef CONFIG_PPC_PSERIES
+	if (cur_cpu_spec->firmware_features & FW_FEATURE_SPLPAR) {
+		vpa_init(cpu); 
+	}
+#endif
+
 	local_irq_enable();
 
 	return cpu_idle(NULL);
diff -purN linux-2.5/arch/ppc64/kernel/stab.c linuxppc64-2.5/arch/ppc64/kernel/stab.c
--- linux-2.5/arch/ppc64/kernel/stab.c	2003-09-07 01:24:09.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/stab.c	2003-09-12 11:01:40.000000000 +0000
@@ -23,6 +23,7 @@
 #include <asm/pmc.h>
 #include <asm/cputable.h>
 
+
 int make_ste(unsigned long stab, unsigned long esid, unsigned long vsid);
 void make_slbe(unsigned long esid, unsigned long vsid, int large,
 	       int kernel_segment);
diff -purN linux-2.5/arch/ppc64/kernel/sys_ppc32.c linuxppc64-2.5/arch/ppc64/kernel/sys_ppc32.c
--- linux-2.5/arch/ppc64/kernel/sys_ppc32.c	2003-10-16 13:54:39.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/sys_ppc32.c	2003-12-17 04:27:52.000000000 +0000
@@ -2106,6 +2106,10 @@ long sys32_execve(unsigned long a0, unsi
 		goto out;
 	if (regs->msr & MSR_FP)
 		giveup_fpu(current);
+#ifdef CONFIG_ALTIVEC
+	if (regs->msr & MSR_VEC)
+		giveup_altivec(current);
+#endif /* CONFIG_ALTIVEC */
 
 	error = do_execve32(filename, (u32*) a1, (u32*) a2, regs);
 
@@ -2126,9 +2130,25 @@ void start_thread32(struct pt_regs* regs
 	regs->nip = nip;
 	regs->gpr[1] = sp;
 	regs->msr = MSR_USER32;
+#ifndef CONFIG_SMP
 	if (last_task_used_math == current)
 		last_task_used_math = 0;
+#endif /* CONFIG_SMP */
 	current->thread.fpscr = 0;
+	memset(current->thread.fpr, 0, sizeof(current->thread.fpr));
+#ifdef CONFIG_ALTIVEC
+#ifndef CONFIG_SMP
+	if (last_task_used_altivec == current)
+		last_task_used_altivec = 0;
+#endif /* CONFIG_SMP */
+	memset(current->thread.vr, 0, sizeof(current->thread.vr));
+	current->thread.vscr.u[0] = 0;
+	current->thread.vscr.u[1] = 0;
+	current->thread.vscr.u[2] = 0;
+	current->thread.vscr.u[3] = 0x00010000; /* Java mode disabled */
+	current->thread.vrsave = 0;
+	current->thread.used_vr = 0;
+#endif /* CONFIG_ALTIVEC */
 }
 
 extern asmlinkage int sys_prctl(int option, unsigned long arg2, unsigned long arg3,
@@ -2878,3 +2898,46 @@ long ppc32_fadvise64(int fd, u32 unused,
 			     advice);
 }
 
+long ppc32_fadvise64_64(int fd, int advice, u32 offset_high, u32 offset_low,
+			u32 len_high, u32 len_low)
+{
+	return sys_fadvise64(fd, (u64)offset_high << 32 | offset_low,
+			     (u64)len_high << 32 | len_low, advice);
+}
+
+extern long sys_timer_create(clockid_t, sigevent_t *, timer_t *);
+
+long ppc32_timer_create(clockid_t clock,
+			struct compat_sigevent __user *ev32,
+			timer_t __user *timer_id)
+{
+	sigevent_t event;
+	timer_t t;
+	long err;
+	mm_segment_t savefs;
+
+	if (ev32 == NULL)
+		return sys_timer_create(clock, NULL, timer_id);
+
+	memset(&event, 0, sizeof(event));
+	if (!access_ok(VERIFY_READ, ev32, sizeof(struct compat_sigevent))
+	    || __get_user(event.sigev_value.sival_int,
+			  &ev32->sigev_value.sival_int)
+	    || __get_user(event.sigev_signo, &ev32->sigev_signo)
+	    || __get_user(event.sigev_notify, &ev32->sigev_notify)
+	    || __get_user(event.sigev_notify_thread_id,
+			  &ev32->sigev_notify_thread_id))
+		return -EFAULT;
+
+	if (!access_ok(VERIFY_WRITE, timer_id, sizeof(timer_t)))
+		return -EFAULT;
+
+	savefs = get_fs();
+	err = sys_timer_create(clock, &event, &t);
+	set_fs(savefs);
+
+	if (err == 0)
+		err = __put_user(t, timer_id);
+
+	return err;
+}
diff -purN linux-2.5/arch/ppc64/kernel/syscalls.c linuxppc64-2.5/arch/ppc64/kernel/syscalls.c
--- linux-2.5/arch/ppc64/kernel/syscalls.c	2003-09-02 04:37:31.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/syscalls.c	2003-11-17 18:51:47.000000000 +0000
@@ -41,6 +41,7 @@
 #include <asm/ipc.h>
 #include <asm/semaphore.h>
 #include <asm/time.h>
+#include <asm/unistd.h>
 
 extern unsigned long wall_jiffies;
 
@@ -234,3 +235,6 @@ asmlinkage time_t sys64_time(time_t* tlo
 
 	return secs;
 }
+
+/* Only exists on P-series. */
+cond_syscall(ppc_rtas);
diff -purN linux-2.5/arch/ppc64/kernel/time.c linuxppc64-2.5/arch/ppc64/kernel/time.c
--- linux-2.5/arch/ppc64/kernel/time.c	2003-10-08 02:53:40.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/time.c	2003-11-18 09:04:18.000000000 +0000
@@ -91,6 +91,9 @@ unsigned      tb_to_us;
 unsigned long processor_freq;
 spinlock_t rtc_lock = SPIN_LOCK_UNLOCKED;
 
+unsigned long tb_to_ns_scale;
+unsigned long tb_to_ns_shift;
+
 struct gettimeofday_struct do_gtod;
 
 extern unsigned long wall_jiffies;
@@ -312,12 +315,10 @@ int timer_interrupt(struct pt_regs * reg
 
 /*
  * Scheduler clock - returns current time in nanosec units.
- *
- * This is wrong, but my CPUs run at 1GHz, so nyer nyer.
  */
 unsigned long long sched_clock(void)
 {
-	return get_tb();
+	return mulhdu(get_tb(), tb_to_ns_scale) << tb_to_ns_shift;
 }
 
 /*
@@ -473,9 +474,22 @@ void __init time_init(void)
 	/* This function is only called on the boot processor */
 	unsigned long flags;
 	struct rtc_time tm;
+	struct div_result res;
+	unsigned long scale, shift;
 
 	ppc_md.calibrate_decr();
 
+	/* Compute scale factor for sched_clock. */
+	/* The calibrate_decr() function has set tb_ticks_per_sec */
+	div128_by_32(1000000000, 0, tb_ticks_per_sec, &res);
+	scale = res.result_low;
+	for (shift = 0; res.result_high != 0; ++shift) {
+		scale = (scale >> 1) | (res.result_high << 63);
+		res.result_high >>= 1;
+	}
+	tb_to_ns_scale = scale;
+	tb_to_ns_shift = shift;
+
 #ifdef CONFIG_PPC_ISERIES
 	if (!piranha_simulator)
 #endif
diff -purN linux-2.5/arch/ppc64/kernel/traps.c linuxppc64-2.5/arch/ppc64/kernel/traps.c
--- linux-2.5/arch/ppc64/kernel/traps.c	2003-07-28 04:00:17.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/traps.c	2003-12-17 04:27:52.000000000 +0000
@@ -16,6 +16,7 @@
  * This file handles the architecture-dependent parts of hardware exceptions
  */
 
+#include <linux/config.h>
 #include <linux/errno.h>
 #include <linux/sched.h>
 #include <linux/kernel.h>
@@ -26,7 +27,6 @@
 #include <linux/user.h>
 #include <linux/a.out.h>
 #include <linux/interrupt.h>
-#include <linux/config.h>
 #include <linux/init.h>
 #include <linux/module.h>
 
@@ -40,8 +40,10 @@
 extern int fix_alignment(struct pt_regs *);
 extern void bad_page_fault(struct pt_regs *, unsigned long, int);
 
+#ifdef CONFIG_PPC_PSERIES
 /* This is true if we are using the firmware NMI handler (typically LPAR) */
 extern int fwnmi_active;
+#endif
 
 #ifdef CONFIG_DEBUG_KERNEL
 void (*debugger)(struct pt_regs *regs);
@@ -96,6 +98,7 @@ _exception(int signr, siginfo_t *info, s
 	force_sig_info(signr, info, current);
 }
 
+#ifdef CONFIG_PPC_PSERIES
 /* Get the error information for errors coming through the
  * FWNMI vectors.  The pt_regs' r3 will be updated to reflect
  * the actual r3 if possible, and a ptr to the error log entry
@@ -128,10 +131,12 @@ static void FWNMI_release_errinfo(void)
 	if (ret != 0)
 		printk("FWNMI: nmi-interlock failed: %ld\n", ret);
 }
+#endif
 
 void
 SystemResetException(struct pt_regs *regs)
 {
+#ifdef CONFIG_PPC_PSERIES
 	if (fwnmi_active) {
 		struct rtas_error_log *errhdr = FWNMI_get_errinfo(regs);
 		if (errhdr) {
@@ -139,6 +144,7 @@ SystemResetException(struct pt_regs *reg
 		}
 		FWNMI_release_errinfo();
 	}
+#endif
 
 #ifdef CONFIG_DEBUG_KERNEL
 	if (debugger)
@@ -154,6 +160,7 @@ SystemResetException(struct pt_regs *reg
 	/* What should we do here? We could issue a shutdown or hard reset. */
 }
 
+#ifdef CONFIG_PPC_PSERIES
 /* 
  * See if we can recover from a machine check exception.
  * This is only called on power4 (or above) and only via
@@ -190,6 +197,7 @@ static int recover_mce(struct pt_regs *r
 	}
 	return 0;
 }
+#endif
 
 /*
  * Handle a machine check.
@@ -207,6 +215,7 @@ static int recover_mce(struct pt_regs *r
 void
 MachineCheckException(struct pt_regs *regs)
 {
+#ifdef CONFIG_PPC_PSERIES
 	struct rtas_error_log err, *errp;
 
 	if (fwnmi_active) {
@@ -217,6 +226,7 @@ MachineCheckException(struct pt_regs *re
 		if (errp && recover_mce(regs, err))
 			return;
 	}
+#endif
 
 #ifdef CONFIG_DEBUG_KERNEL
 	if (debugger_fault_handler) {
@@ -410,6 +420,14 @@ KernelFPUnavailableException(struct pt_r
 }
 
 void
+KernelAltivecUnavailableException(struct pt_regs *regs)
+{
+	printk("Illegal VMX/Altivec used in kernel (task=0x%p, "
+		"pc=0x%016lx, trap=0x%lx)\n", current, regs->nip, regs->trap);
+	panic("Unrecoverable VMX/Altivec Unavailable Exception in Kernel");
+}
+
+void
 SingleStepException(struct pt_regs *regs)
 {
 	siginfo_t info;
@@ -478,6 +496,17 @@ AlignmentException(struct pt_regs *regs)
 	_exception(SIGBUS, &info, regs);	
 }
 
+#ifdef CONFIG_ALTIVEC
+void
+AltivecAssistException(struct pt_regs *regs)
+{
+	if (regs->msr & MSR_VEC)
+		giveup_altivec(current);
+	/* XXX quick hack for now: set the non-Java bit in the VSCR */
+	current->thread.vscr.u[3] |= 0x10000;
+}
+#endif /* CONFIG_ALTIVEC */
+
 void __init trap_init(void)
 {
 }
diff -purN linux-2.5/arch/ppc64/kernel/udbg.c linuxppc64-2.5/arch/ppc64/kernel/udbg.c
--- linux-2.5/arch/ppc64/kernel/udbg.c	2003-03-26 04:30:59.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/udbg.c	2003-10-14 20:19:05.000000000 +0000
@@ -127,8 +127,10 @@ udbg_write(const char *s, int n)
 {
 	int remain = n;
 	char c;
+
 	if (!ppc_md.udbg_putc)
-		for (;;);	/* stop here for cpuctl */
+		return 0;
+
 	if ( s && *s != '\0' ) {
 		while ( (( c = *s++ ) != '\0') && (remain-- > 0)) {
 			ppc_md.udbg_putc(c);
diff -purN linux-2.5/arch/ppc64/kernel/vio.c linuxppc64-2.5/arch/ppc64/kernel/vio.c
--- linux-2.5/arch/ppc64/kernel/vio.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/vio.c	2003-12-06 00:09:27.000000000 +0000
@@ -0,0 +1,534 @@
+/*
+ * IBM PowerPC Virtual I/O Infrastructure Support.
+ *
+ *    Copyright (c) 2003 IBM Corp.
+ *     Dave Engebretsen engebret@us.ibm.com
+ *     Santiago Leon santil@us.ibm.com
+ *
+ *      This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/init.h>
+#include <linux/console.h>
+#include <linux/pci.h>
+#include <linux/version.h>
+#include <linux/module.h>
+#include <asm/rtas.h>
+#include <asm/pci_dma.h>
+#include <asm/dma.h>
+#include <asm/ppcdebug.h>
+#include <asm/vio.h>
+#include <asm/hvcall.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,5,0)
+#include <linux/mm.h>
+#endif
+
+extern struct TceTable *build_tce_table(struct TceTable *tbl);
+
+extern dma_addr_t get_tces(struct TceTable *, unsigned order,
+			   void *page, unsigned numPages, int direction);
+extern void tce_free(struct TceTable *tbl, dma_addr_t dma_addr,
+		     unsigned order, unsigned num_pages);
+
+
+static struct vio_bus vio_bus;
+static LIST_HEAD(registered_vio_drivers);
+int vio_num_address_cells;
+EXPORT_SYMBOL(vio_num_address_cells);
+
+/* TODO:
+ *   really fit into driver model (see include/linux/device.h)
+ *   locking around list accesses
+ */
+
+/**
+ * vio_register_driver: - Register a new vio driver
+ * @drv:	The vio_driver structure to be registered.
+ *
+ * Adds the driver structure to the list of registered drivers
+ * Returns the number of vio devices which were claimed by the driver
+ * during registration.  The driver remains registered even if the
+ * return value is zero.
+ */
+int vio_register_driver(struct vio_driver *drv)
+{
+	int count = 0;
+	struct vio_dev *dev;
+
+	printk(KERN_DEBUG "%s: driver %s/%s registering\n", __FUNCTION__,
+		drv->id_table[0].type, drv->id_table[0].type);
+
+	/* find matching devices not already claimed by other drivers and pass
+	 * them to probe() */
+	list_for_each_entry(dev, &vio_bus.devices, devices_list) {
+		const struct vio_device_id* id;
+
+		if (dev->driver)
+			continue; /* this device is already owned */
+
+		id = vio_match_device(drv->id_table, dev);
+		if (drv && id) {
+			if (0 == drv->probe(dev, id)) {
+				printk(KERN_DEBUG "  took device %p\n", dev);
+				dev->driver = drv;
+				count++;
+			}
+		}
+	}
+
+	list_add_tail(&drv->node, &registered_vio_drivers);
+
+	return count;
+}
+EXPORT_SYMBOL(vio_register_driver);
+
+/**
+ * vio_unregister_driver - Remove registration of vio driver.
+ * @driver:	The vio_driver struct to be removed form registration
+ *
+ * Searches for devices that are assigned to the driver and calls
+ * driver->remove() for each one.  Removes the driver from the list
+ * of registered drivers.  Returns the number of devices that were
+ * assigned to that driver.
+ */
+int vio_unregister_driver(struct vio_driver *driver)
+{
+	struct vio_dev *dev;
+	int devices_found = 0;
+
+	list_for_each_entry(dev, &vio_bus.devices, devices_list) {
+		if (dev->driver == driver) {
+			driver->remove(dev);
+			dev->driver = NULL;
+			devices_found++;
+		}
+	}
+
+	list_del(&driver->node);
+
+	return devices_found;
+}
+EXPORT_SYMBOL(vio_unregister_driver);
+
+/**
+ * vio_match_device: - Tell if a VIO device has a matching VIO device id structure.
+ * @ids: 	array of VIO device id structures to search in
+ * @dev: 	the VIO device structure to match against
+ *
+ * Used by a driver to check whether a VIO device present in the
+ * system is in its list of supported devices. Returns the matching
+ * vio_device_id structure or NULL if there is no match.
+ */
+const struct vio_device_id *
+vio_match_device(const struct vio_device_id *ids, const struct vio_dev *dev)
+{
+	while (ids->type) {
+		if ((strncmp(dev->archdata->type, ids->type, strlen(ids->type)) == 0) &&
+			device_is_compatible((struct device_node*)dev->archdata, ids->compat))
+			return ids;
+		ids++;
+	}
+	return NULL;
+}
+
+/**
+ * vio_bus_init: - Initialize the virtual IO bus
+ */
+int __init
+vio_bus_init(void)
+{
+	struct device_node *node_vroot, *node_vdev;
+
+	INIT_LIST_HEAD(&vio_bus.devices);
+
+	/*
+	 * Create device node entries for each virtual device
+	 * identified in the device tree.
+	 * Functionally takes the place of pci_scan_bus
+	 */
+	node_vroot = find_devices("vdevice");
+	if (!node_vroot) {
+		printk(KERN_WARNING "%s: no /vdevice node\n", __FUNCTION__);
+		return 0;
+	}
+
+	vio_num_address_cells = prom_n_addr_cells(node_vroot->child);
+
+	for (node_vdev = node_vroot->child;
+			node_vdev != NULL;
+			node_vdev = node_vdev->sibling) {
+		printk(KERN_DEBUG "%s: processing %p\n", __FUNCTION__, node_vdev);
+
+		vio_register_device(node_vdev);
+	}
+
+	return 0;
+}
+
+__initcall(vio_bus_init);
+
+
+/**
+ * vio_probe_device - attach dev to appropriate driver
+ * @dev:	device to find a driver for
+ *
+ * Walks the list of registered VIO drivers looking for one to take this
+ * device.
+ *
+ * Returns a pointer to the matched driver or NULL if driver is not
+ * found.
+ */
+struct vio_driver * __devinit vio_probe_device(struct vio_dev* dev)
+{
+	struct vio_driver *driver;
+
+	list_for_each_entry(driver, &registered_vio_drivers, node) {
+		const struct vio_device_id* id;
+
+		id = vio_match_device(driver->id_table, dev);
+		if (id && (0 < driver->probe(dev, id))) {
+			printk(KERN_DEBUG "%s: driver %s/%s took device %p\n",
+				__FUNCTION__, id->type, id->compat, dev);
+			dev->driver = driver;
+			return driver;
+		}
+	}
+
+	printk(KERN_DEBUG "%s: device %p found no driver\n", __FUNCTION__, dev);
+	return NULL;
+}
+
+/**
+ * vio_register_device: - Register a new vio device.
+ * @archdata:	The OF node for this device.
+ *
+ * Creates and initializes a vio_dev structure from the data in
+ * node_vdev (archdata) and adds it to the list of virtual devices.
+ * Returns a pointer to the created vio_dev or NULL if node has
+ * NULL device_type or compatible fields.
+ */
+struct vio_dev * __devinit vio_register_device(struct device_node *node_vdev)
+{
+	struct vio_dev *dev;
+	unsigned int *unit_address;
+	unsigned int *irq_p;
+
+	/* guarantee all vio_devs have 'device_type' field*/
+	if ((NULL == node_vdev->type)) {
+		printk(KERN_WARNING
+			"%s: node %s missing 'device_type'\n", __FUNCTION__,
+			node_vdev->name ? node_vdev->name : "<unknown>");
+		return NULL;
+	}
+
+	unit_address = (unsigned int *)get_property(node_vdev, "reg", NULL);
+	if (!unit_address) {
+		printk(KERN_WARNING "%s: node %s missing 'reg'\n", __FUNCTION__,
+			node_vdev->name ? node_vdev->name : "<unknown>");
+		return NULL;
+	}
+
+	/* allocate a vio_dev for this node */
+	dev = kmalloc(sizeof(*dev), GFP_KERNEL);
+	if (!dev)
+		return NULL;
+	memset(dev, 0, sizeof(*dev));
+
+	dev->archdata = (void*)of_node_get(node_vdev);
+	dev->bus = &vio_bus;
+	dev->unit_address = *unit_address;
+	dev->tce_table = vio_build_tce_table(dev);
+
+	irq_p = (unsigned int *) get_property(node_vdev, "interrupts", 0);
+	if(irq_p) {
+		dev->irq = irq_offset_up(*irq_p);
+	} else {
+		dev->irq = (unsigned int) -1;
+	}
+
+	list_add_tail(&dev->devices_list, &vio_bus.devices);
+
+	vio_probe_device(dev); /* finally, assign it to a driver */
+
+	return dev;
+}
+
+int __devinit vio_unregister_device(struct vio_dev *dev)
+{
+	list_del(&dev->devices_list);
+	of_node_put(dev->archdata);
+
+	return 0;
+}
+
+/**
+ * vio_get_attribute: - get attribute for virtual device
+ * @vdev:	The vio device to get property.
+ * @which:	The property/attribute to be extracted.
+ * @length:	Pointer to length of returned data size (unused if NULL).
+ *
+ * Calls prom.c's get_property() to return the value of the
+ * attribute specified by the preprocessor constant @which
+*/
+const void * vio_get_attribute(struct vio_dev *vdev, void* which, int* length)
+{
+	return get_property((struct device_node *)vdev->archdata, (char*)which, length);
+}
+EXPORT_SYMBOL(vio_get_attribute);
+
+/**
+ * vio_build_tce_table: - gets the dma information from OF and builds the TCE tree.
+ * @dev: the virtual device.
+ *
+ * Returns a pointer to the built tce tree, or NULL if it can't
+ * find property.
+*/
+struct TceTable * vio_build_tce_table(struct vio_dev *dev)
+{
+	unsigned int *dma_window;
+	struct TceTable *newTceTable;
+	unsigned long offset;
+	unsigned long size;
+	int dma_window_property_size;
+
+	dma_window = (unsigned int *) get_property((struct device_node *)dev->archdata, "ibm,my-dma-window", &dma_window_property_size);
+	if(!dma_window) {
+		return NULL;
+	}
+
+	newTceTable = (struct TceTable *) kmalloc(sizeof(struct TceTable), GFP_KERNEL);
+
+	/* RPA docs say that #address-cells is always 1 for virtual
+		devices, but some older boxes' OF returns 2.  This should
+		be removed by GA, unless there is legacy OFs that still
+		have 2 for #address-cells */
+	size = ((dma_window[1+vio_num_address_cells]
+		>> PAGE_SHIFT) << 3) >> PAGE_SHIFT;
+
+	/* This is just an ugly kludge. Remove as soon as the OF for all
+	machines actually follow the spec and encodes the offset field
+	as phys-encode (that is, #address-cells wide)*/
+	if (dma_window_property_size == 12) {
+		size = ((dma_window[1] >> PAGE_SHIFT) << 3) >> PAGE_SHIFT;
+	} else if (dma_window_property_size == 20) {
+		size = ((dma_window[4] >> PAGE_SHIFT) << 3) >> PAGE_SHIFT;
+	} else {
+		printk(KERN_WARNING "vio_build_tce_table: Invalid size of ibm,my-dma-window=%i, using 0x80 for size\n", dma_window_property_size);
+		size = 0x80;
+	}
+
+	/*  There should be some code to extract the phys-encoded offset
+		using prom_n_addr_cells(). However, according to a comment
+		on earlier versions, it's always zero, so we don't bother */
+	offset = dma_window[1] >>  PAGE_SHIFT;
+
+	/* TCE table size - measured in units of pages of tce table */
+	newTceTable->size = size;
+	/* offset for VIO should always be 0 */
+	newTceTable->startOffset = offset;
+	newTceTable->busNumber   = 0;
+	newTceTable->index       = (unsigned long)dma_window[0];
+	newTceTable->tceType     = TCE_VB;
+
+	return build_tce_table(newTceTable);
+}
+
+int vio_enable_interrupts(struct vio_dev *dev)
+{
+	int rc = h_vio_signal(dev->unit_address, VIO_IRQ_ENABLE);
+	if (rc != H_Success) {
+		printk(KERN_ERR "vio: Error 0x%x enabling interrupts\n", rc);
+	}
+	return rc;
+}
+EXPORT_SYMBOL(vio_enable_interrupts);
+
+int vio_disable_interrupts(struct vio_dev *dev)
+{
+	int rc = h_vio_signal(dev->unit_address, VIO_IRQ_DISABLE);
+	if (rc != H_Success) {
+		printk(KERN_ERR "vio: Error 0x%x disabling interrupts\n", rc);
+	}
+	return rc;
+}
+EXPORT_SYMBOL(vio_disable_interrupts);
+
+
+dma_addr_t vio_map_single(struct vio_dev *dev, void *vaddr,
+			  size_t size, int direction )
+{
+	struct TceTable * tbl;
+	dma_addr_t dma_handle = NO_TCE;
+	unsigned long uaddr;
+	unsigned order, nPages;
+
+	if(direction == PCI_DMA_NONE) BUG();
+
+	uaddr = (unsigned long)vaddr;
+	nPages = PAGE_ALIGN( uaddr + size ) - ( uaddr & PAGE_MASK );
+	order = get_order( nPages & PAGE_MASK );
+	nPages >>= PAGE_SHIFT;
+
+ 	/* Client asked for way to much space.  This is checked later anyway */
+	/* It is easier to debug here for the drivers than in the tce tables.*/
+ 	if(order >= NUM_TCE_LEVELS) {
+ 		printk("VIO_DMA: vio_map_single size to large: 0x%lx \n",size);
+ 		return NO_TCE;
+ 	}
+
+	tbl = dev->tce_table;
+
+	if(tbl) {
+		dma_handle = get_tces(tbl, order, vaddr, nPages, direction);
+		dma_handle |= (uaddr & ~PAGE_MASK);
+	}
+
+	return dma_handle;
+}
+EXPORT_SYMBOL(vio_map_single);
+
+void vio_unmap_single(struct vio_dev *dev, dma_addr_t dma_handle,
+		      size_t size, int direction)
+{
+	struct TceTable * tbl;
+	unsigned order, nPages;
+
+	if (direction == PCI_DMA_NONE) BUG();
+
+	nPages = PAGE_ALIGN( dma_handle + size ) - ( dma_handle & PAGE_MASK );
+	order = get_order( nPages & PAGE_MASK );
+	nPages >>= PAGE_SHIFT;
+
+ 	/* Client asked for way to much space.  This is checked later anyway */
+	/* It is easier to debug here for the drivers than in the tce tables.*/
+ 	if(order >= NUM_TCE_LEVELS) {
+ 		printk("VIO_DMA: vio_unmap_single 0x%lx size to large: 0x%lx \n",(unsigned long)dma_handle,(unsigned long)size);
+ 		return;
+ 	}
+
+	tbl = dev->tce_table;
+	if(tbl) tce_free(tbl, dma_handle, order, nPages);
+}
+EXPORT_SYMBOL(vio_unmap_single);
+
+int vio_map_sg(struct vio_dev *vdev, struct scatterlist *sglist, int nelems,
+	       int direction)
+{
+	int i;
+
+	for (i = 0; i < nelems; i++) {
+
+		/* 2.4 scsi scatterlists use address field.
+		   Not sure about other subsystems. */
+		void *vaddr;
+#if LINUX_VERSION_CODE <= KERNEL_VERSION(2,5,0)
+		if (sglist->address)
+			vaddr = sglist->address;
+		else
+#endif
+			vaddr = page_address(sglist->page) + sglist->offset;
+
+		sglist->dma_address = vio_map_single(vdev, vaddr,
+						     sglist->length,
+						     direction);
+		sglist->dma_length = sglist->length;
+		sglist++;
+	}
+
+	return nelems;
+}
+EXPORT_SYMBOL(vio_map_sg);
+
+void vio_unmap_sg(struct vio_dev *vdev, struct scatterlist *sglist, int nelems,
+		  int direction)
+{
+	while (nelems--) {
+		vio_unmap_single(vdev, sglist->dma_address,
+				 sglist->dma_length, direction);
+		sglist++;
+	}
+}
+
+void *vio_alloc_consistent(struct vio_dev *dev, size_t size,
+			   dma_addr_t *dma_handle)
+{
+	struct TceTable * tbl;
+	void *ret = NULL;
+	unsigned order, nPages;
+	dma_addr_t tce;
+
+	size = PAGE_ALIGN(size);
+	order = get_order(size);
+	nPages = 1 << order;
+
+ 	/* Client asked for way to much space.  This is checked later anyway */
+	/* It is easier to debug here for the drivers than in the tce tables.*/
+ 	if(order >= NUM_TCE_LEVELS) {
+ 		printk("VIO_DMA: vio_alloc_consistent size to large: 0x%lx \n",size);
+ 		return (void *)NO_TCE;
+ 	}
+
+	tbl = dev->tce_table;
+
+	if ( tbl ) {
+		/* Alloc enough pages (and possibly more) */
+		ret = (void *)__get_free_pages( GFP_ATOMIC, order );
+		if ( ret ) {
+			/* Page allocation succeeded */
+			memset(ret, 0, nPages << PAGE_SHIFT);
+			/* Set up tces to cover the allocated range */
+			tce = get_tces( tbl, order, ret, nPages, PCI_DMA_BIDIRECTIONAL );
+			if ( tce == NO_TCE ) {
+				PPCDBG(PPCDBG_TCE, "vio_alloc_consistent: get_tces failed\n" );
+				free_pages( (unsigned long)ret, order );
+				ret = NULL;
+			}
+			else
+				{
+					*dma_handle = tce;
+				}
+		}
+		else PPCDBG(PPCDBG_TCE, "vio_alloc_consistent: __get_free_pages failed for order = %d\n", order);
+	}
+	else PPCDBG(PPCDBG_TCE, "vio_alloc_consistent: get_tce_table failed for 0x%016lx\n", dev);
+
+	PPCDBG(PPCDBG_TCE, "\tvio_alloc_consistent: dma_handle = 0x%16.16lx\n", *dma_handle);
+	PPCDBG(PPCDBG_TCE, "\tvio_alloc_consistent: return     = 0x%16.16lx\n", ret);
+	return ret;
+}
+EXPORT_SYMBOL(vio_alloc_consistent);
+
+void vio_free_consistent(struct vio_dev *dev, size_t size,
+			 void *vaddr, dma_addr_t dma_handle)
+{
+	struct TceTable * tbl;
+	unsigned order, nPages;
+
+	PPCDBG(PPCDBG_TCE, "vio_free_consistent:\n");
+	PPCDBG(PPCDBG_TCE, "\tdev = 0x%16.16lx, size = 0x%16.16lx, dma_handle = 0x%16.16lx, vaddr = 0x%16.16lx\n", dev, size, dma_handle, vaddr);
+
+	size = PAGE_ALIGN(size);
+	order = get_order(size);
+	nPages = 1 << order;
+
+ 	/* Client asked for way to much space.  This is checked later anyway */
+	/* It is easier to debug here for the drivers than in the tce tables.*/
+ 	if(order >= NUM_TCE_LEVELS) {
+ 		printk("PCI_DMA: pci_free_consistent size to large: 0x%lx \n",size);
+ 		return;
+ 	}
+
+	tbl = dev->tce_table;
+
+	if ( tbl ) {
+		tce_free(tbl, dma_handle, order, nPages);
+		free_pages( (unsigned long)vaddr, order );
+	}
+}
+EXPORT_SYMBOL(vio_free_consistent);
+
+EXPORT_SYMBOL(plpar_hcall_norets);
+EXPORT_SYMBOL(plpar_hcall_8arg_2ret);
diff -purN linux-2.5/arch/ppc64/kernel/viopath.c linuxppc64-2.5/arch/ppc64/kernel/viopath.c
--- linux-2.5/arch/ppc64/kernel/viopath.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/viopath.c	2003-11-21 05:43:24.000000000 +0000
@@ -0,0 +1,755 @@
+/* -*- linux-c -*-
+ *  arch/ppc64/kernel/viopath.c
+ *
+ *  iSeries Virtual I/O Message Path code
+ *
+ *  Authors: Dave Boutcher <boutcher@us.ibm.com>
+ *           Ryan Arnold <ryanarn@us.ibm.com>
+ *           Colin Devilbiss <devilbis@us.ibm.com>
+ *
+ * (C) Copyright 2000-2003 IBM Corporation
+ *
+ * This code is used by the iSeries virtual disk, cd,
+ * tape, and console to communicate with OS/400 in another
+ * partition.
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) anyu later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ */
+#include <linux/config.h>
+#include <asm/uaccess.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/vmalloc.h>
+#include <linux/string.h>
+#include <linux/proc_fs.h>
+#include <linux/pci.h>
+#include <linux/wait.h>
+
+#include <asm/hardirq.h>	/* for is_atomic */
+
+#include <asm/iSeries/LparData.h>
+#include <asm/iSeries/HvLpEvent.h>
+#include <asm/iSeries/HvLpConfig.h>
+#include <asm/iSeries/HvCallCfg.h>
+#include <asm/iSeries/mf.h>
+#include <asm/iSeries/iSeries_proc.h>
+#include <asm/iSeries/vio.h>
+
+extern struct pci_dev *iSeries_vio_dev;
+
+/* Status of the path to each other partition in the system.
+ * This is overkill, since we will only ever establish connections
+ * to our hosting partition and the primary partition on the system.
+ * But this allows for other support in the future.
+ */
+static struct viopathStatus {
+	int isOpen:1;		/* Did we open the path?            */
+	int isActive:1;		/* Do we have a mon msg outstanding */
+	int users[VIO_MAX_SUBTYPES];
+	HvLpInstanceId mSourceInst;
+	HvLpInstanceId mTargetInst;
+	int numberAllocated;
+} viopathStatus[HVMAXARCHITECTEDLPS];
+
+static spinlock_t statuslock = SPIN_LOCK_UNLOCKED;
+
+/*
+ * For each kind of event we allocate a buffer that is
+ * guaranteed not to cross a page boundary
+ */
+static unsigned char event_buffer[VIO_MAX_SUBTYPES * 256] __page_aligned;
+static atomic_t event_buffer_available[VIO_MAX_SUBTYPES];
+static int event_buffer_initialised;
+
+static void handleMonitorEvent(struct HvLpEvent *event);
+
+/*
+ * We use this structure to handle asynchronous responses.  The caller
+ * blocks on the semaphore and the handler posts the semaphore.  However,
+ * if in_atomic() is true in the caller, then wait_atomic is used ...
+ */
+struct doneAllocParms_t {
+	struct semaphore *sem;
+	int number;
+	volatile unsigned long *wait_atomic;
+	int used_wait_atomic;
+};
+
+/* Put a sequence number in each mon msg.  The value is not
+ * important.  Start at something other than 0 just for
+ * readability.  wrapping this is ok.
+ */
+static u8 viomonseq = 22;
+
+/* Our hosting logical partition.  We get this at startup
+ * time, and different modules access this variable directly.
+ */
+HvLpIndex viopath_hostLp = 0xff;	/* HvLpIndexInvalid */
+EXPORT_SYMBOL(viopath_hostLp);
+HvLpIndex viopath_ourLp = 0xff;
+EXPORT_SYMBOL(viopath_ourLp);
+
+/* For each kind of incoming event we set a pointer to a
+ * routine to call.
+ */
+static vio_event_handler_t *vio_handler[VIO_MAX_SUBTYPES];
+
+static unsigned char e2a(unsigned char x)
+{
+	switch (x) {
+	case 0xF0:
+		return '0';
+	case 0xF1:
+		return '1';
+	case 0xF2:
+		return '2';
+	case 0xF3:
+		return '3';
+	case 0xF4:
+		return '4';
+	case 0xF5:
+		return '5';
+	case 0xF6:
+		return '6';
+	case 0xF7:
+		return '7';
+	case 0xF8:
+		return '8';
+	case 0xF9:
+		return '9';
+	case 0xC1:
+		return 'A';
+	case 0xC2:
+		return 'B';
+	case 0xC3:
+		return 'C';
+	case 0xC4:
+		return 'D';
+	case 0xC5:
+		return 'E';
+	case 0xC6:
+		return 'F';
+	case 0xC7:
+		return 'G';
+	case 0xC8:
+		return 'H';
+	case 0xC9:
+		return 'I';
+	case 0xD1:
+		return 'J';
+	case 0xD2:
+		return 'K';
+	case 0xD3:
+		return 'L';
+	case 0xD4:
+		return 'M';
+	case 0xD5:
+		return 'N';
+	case 0xD6:
+		return 'O';
+	case 0xD7:
+		return 'P';
+	case 0xD8:
+		return 'Q';
+	case 0xD9:
+		return 'R';
+	case 0xE2:
+		return 'S';
+	case 0xE3:
+		return 'T';
+	case 0xE4:
+		return 'U';
+	case 0xE5:
+		return 'V';
+	case 0xE6:
+		return 'W';
+	case 0xE7:
+		return 'X';
+	case 0xE8:
+		return 'Y';
+	case 0xE9:
+		return 'Z';
+	}
+	return ' ';
+}
+
+/* Handle reads from the proc file system
+ */
+static int proc_read(char *buf, char **start, off_t offset,
+		     int blen, int *eof, void *data)
+{
+	HvLpEvent_Rc hvrc;
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	dma_addr_t dmaa =
+	    pci_map_single(iSeries_vio_dev, buf, PAGE_SIZE,
+			   PCI_DMA_FROMDEVICE);
+	int len = PAGE_SIZE;
+
+	if (len > blen)
+		len = blen;
+
+	memset(buf, 0x00, len);
+	hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+			HvLpEvent_Type_VirtualIo,
+			viomajorsubtype_config | vioconfigget,
+			HvLpEvent_AckInd_DoAck, HvLpEvent_AckType_ImmediateAck,
+			viopath_sourceinst(viopath_hostLp),
+			viopath_targetinst(viopath_hostLp),
+			(u64)(unsigned long)&Semaphore, VIOVERSION << 16,
+			((u64)dmaa) << 32, len, 0, 0);
+	if (hvrc != HvLpEvent_Rc_Good)
+		printk("viopath hv error on op %d\n", (int) hvrc);
+
+	down(&Semaphore);
+
+	pci_unmap_single(iSeries_vio_dev, dmaa, PAGE_SIZE,
+			 PCI_DMA_FROMDEVICE);
+
+	sprintf(buf + strlen(buf), "SRLNBR=");
+	buf[strlen(buf)] = e2a(xItExtVpdPanel.mfgID[2]);
+	buf[strlen(buf)] = e2a(xItExtVpdPanel.mfgID[3]);
+	buf[strlen(buf)] = e2a(xItExtVpdPanel.systemSerial[1]);
+	buf[strlen(buf)] = e2a(xItExtVpdPanel.systemSerial[2]);
+	buf[strlen(buf)] = e2a(xItExtVpdPanel.systemSerial[3]);
+	buf[strlen(buf)] = e2a(xItExtVpdPanel.systemSerial[4]);
+	buf[strlen(buf)] = e2a(xItExtVpdPanel.systemSerial[5]);
+	buf[strlen(buf)] = '\n';
+	*eof = 1;
+	return strlen(buf);
+}
+
+/* Handle writes to our proc file system
+ */
+static int proc_write(struct file *file, const char *buffer,
+		      unsigned long count, void *data)
+{
+	/* Doesn't do anything today!!!
+	 */
+	return count;
+}
+
+/* setup our proc file system entries
+ */
+static void vio_proc_init(struct proc_dir_entry *iSeries_proc)
+{
+	struct proc_dir_entry *ent;
+	ent = create_proc_entry("config", S_IFREG | S_IRUSR, iSeries_proc);
+	if (!ent)
+		return;
+	ent->nlink = 1;
+	ent->data = NULL;
+	ent->read_proc = proc_read;
+	ent->write_proc = proc_write;
+}
+
+/* See if a given LP is active.  Allow for invalid lps to be passed in
+ * and just return invalid
+ */
+int viopath_isactive(HvLpIndex lp)
+{
+	if (lp == HvLpIndexInvalid)
+		return 0;
+	if (lp < HVMAXARCHITECTEDLPS)
+		return viopathStatus[lp].isActive;
+	else
+		return 0;
+}
+EXPORT_SYMBOL(viopath_isactive);
+
+/*
+ * We cache the source and target instance ids for each
+ * partition.  
+ */
+HvLpInstanceId viopath_sourceinst(HvLpIndex lp)
+{
+	return viopathStatus[lp].mSourceInst;
+}
+EXPORT_SYMBOL(viopath_sourceinst);
+
+HvLpInstanceId viopath_targetinst(HvLpIndex lp)
+{
+	return viopathStatus[lp].mTargetInst;
+}
+EXPORT_SYMBOL(viopath_targetinst);
+
+/*
+ * Send a monitor message.  This is a message with the acknowledge
+ * bit on that the other side will NOT explicitly acknowledge.  When
+ * the other side goes down, the hypervisor will acknowledge any
+ * outstanding messages....so we will know when the other side dies.
+ */
+static void sendMonMsg(HvLpIndex remoteLp)
+{
+	HvLpEvent_Rc hvrc;
+
+	viopathStatus[remoteLp].mSourceInst =
+		HvCallEvent_getSourceLpInstanceId(remoteLp,
+				HvLpEvent_Type_VirtualIo);
+	viopathStatus[remoteLp].mTargetInst =
+		HvCallEvent_getTargetLpInstanceId(remoteLp,
+				HvLpEvent_Type_VirtualIo);
+
+	/*
+	 * Deliberately ignore the return code here.  if we call this
+	 * more than once, we don't care.
+	 */
+	vio_setHandler(viomajorsubtype_monitor, handleMonitorEvent);
+
+	hvrc = HvCallEvent_signalLpEventFast(remoteLp, HvLpEvent_Type_VirtualIo,
+			viomajorsubtype_monitor, HvLpEvent_AckInd_DoAck,
+			HvLpEvent_AckType_DeferredAck,
+			viopathStatus[remoteLp].mSourceInst,
+			viopathStatus[remoteLp].mTargetInst,
+			viomonseq++, 0, 0, 0, 0, 0);
+
+	if (hvrc == HvLpEvent_Rc_Good)
+		viopathStatus[remoteLp].isActive = 1;
+	else {
+		printk(KERN_WARNING_VIO "could not connect to partition %d\n",
+				remoteLp);
+		viopathStatus[remoteLp].isActive = 0;
+	}
+}
+
+static void handleMonitorEvent(struct HvLpEvent *event)
+{
+	HvLpIndex remoteLp;
+	int i;
+
+	/*
+	 * This handler is _also_ called as part of the loop
+	 * at the end of this routine, so it must be able to
+	 * ignore NULL events...
+	 */
+	if (!event)
+		return;
+
+	/*
+	 * First see if this is just a normal monitor message from the
+	 * other partition
+	 */
+	if (event->xFlags.xFunction == HvLpEvent_Function_Int) {
+		remoteLp = event->xSourceLp;
+		if (!viopathStatus[remoteLp].isActive)
+			sendMonMsg(remoteLp);
+		return;
+	}
+
+	/*
+	 * This path is for an acknowledgement; the other partition
+	 * died
+	 */
+	remoteLp = event->xTargetLp;
+	if ((event->xSourceInstanceId != viopathStatus[remoteLp].mSourceInst) ||
+	    (event->xTargetInstanceId != viopathStatus[remoteLp].mTargetInst)) {
+		printk(KERN_WARNING_VIO "ignoring ack....mismatched instances\n");
+		return;
+	}
+
+	printk(KERN_WARNING_VIO "partition %d ended\n", remoteLp);
+
+	viopathStatus[remoteLp].isActive = 0;
+
+	/*
+	 * For each active handler, pass them a NULL
+	 * message to indicate that the other partition
+	 * died
+	 */
+	for (i = 0; i < VIO_MAX_SUBTYPES; i++) {
+		if (vio_handler[i] != NULL)
+			(*vio_handler[i])(NULL);
+	}
+}
+
+int vio_setHandler(int subtype, vio_event_handler_t *beh)
+{
+	subtype = subtype >> VIOMAJOR_SUBTYPE_SHIFT;
+	if ((subtype < 0) || (subtype >= VIO_MAX_SUBTYPES))
+		return -EINVAL;
+	if (vio_handler[subtype] != NULL)
+		return -EBUSY;
+	vio_handler[subtype] = beh;
+	return 0;
+}
+EXPORT_SYMBOL(vio_setHandler);
+
+int vio_clearHandler(int subtype)
+{
+	subtype = subtype >> VIOMAJOR_SUBTYPE_SHIFT;
+	if ((subtype < 0) || (subtype >= VIO_MAX_SUBTYPES))
+		return -EINVAL;
+	if (vio_handler[subtype] == NULL)
+		return -EAGAIN;
+	vio_handler[subtype] = NULL;
+	return 0;
+}
+EXPORT_SYMBOL(vio_clearHandler);
+
+static void handleConfig(struct HvLpEvent *event)
+{
+	if (!event)
+		return;
+	if (event->xFlags.xFunction == HvLpEvent_Function_Int) {
+		printk(KERN_WARNING_VIO
+		       "unexpected config request from partition %d",
+		       event->xSourceLp);
+
+		if ((event->xFlags.xFunction == HvLpEvent_Function_Int) &&
+		    (event->xFlags.xAckInd == HvLpEvent_AckInd_DoAck)) {
+			event->xRc = HvLpEvent_Rc_InvalidSubtype;
+			HvCallEvent_ackLpEvent(event);
+		}
+		return;
+	}
+
+	up((struct semaphore *)event->xCorrelationToken);
+}
+
+/*
+ * Initialization of the hosting partition
+ */
+void vio_set_hostlp(void)
+{
+	/*
+	 * If this has already been set then we DON'T want to either change
+	 * it or re-register the proc file system
+	 */
+	if (viopath_hostLp != HvLpIndexInvalid)
+		return;
+
+	/*
+	 * Figure out our hosting partition.  This isn't allowed to change
+	 * while we're active
+	 */
+	viopath_ourLp = HvLpConfig_getLpIndex();
+	viopath_hostLp = HvCallCfg_getHostingLpIndex(viopath_ourLp);
+
+	/* If we have a valid hosting LP, create a proc file system entry
+	 * for config information
+	 */
+	if (viopath_hostLp != HvLpIndexInvalid) {
+		iSeries_proc_callback(&vio_proc_init);
+		vio_setHandler(viomajorsubtype_config, handleConfig);
+	}
+}
+EXPORT_SYMBOL(vio_set_hostlp);
+
+static void vio_handleEvent(struct HvLpEvent *event, struct pt_regs *regs)
+{
+	HvLpIndex remoteLp;
+	int subtype = (event->xSubtype & VIOMAJOR_SUBTYPE_MASK)
+		>> VIOMAJOR_SUBTYPE_SHIFT;
+
+	if (event->xFlags.xFunction == HvLpEvent_Function_Int) {
+		remoteLp = event->xSourceLp;
+		/*
+		 * The isActive is checked because if the hosting partition
+		 * went down and came back up it would not be active but it
+		 * would have different source and target instances, in which
+		 * case we'd want to reset them.  This case really protects
+		 * against an unauthorized active partition sending interrupts
+		 * or acks to this linux partition.
+		 */
+		if (viopathStatus[remoteLp].isActive
+		    && (event->xSourceInstanceId !=
+			viopathStatus[remoteLp].mTargetInst)) {
+			printk(KERN_WARNING_VIO
+			       "message from invalid partition. "
+			       "int msg rcvd, source inst (%d) doesnt match (%d)\n",
+			       viopathStatus[remoteLp].mTargetInst,
+			       event->xSourceInstanceId);
+			return;
+		}
+
+		if (viopathStatus[remoteLp].isActive
+		    && (event->xTargetInstanceId !=
+			viopathStatus[remoteLp].mSourceInst)) {
+			printk(KERN_WARNING_VIO
+			       "message from invalid partition. "
+			       "int msg rcvd, target inst (%d) doesnt match (%d)\n",
+			       viopathStatus[remoteLp].mSourceInst,
+			       event->xTargetInstanceId);
+			return;
+		}
+	} else {
+		remoteLp = event->xTargetLp;
+		if (event->xSourceInstanceId !=
+		    viopathStatus[remoteLp].mSourceInst) {
+			printk(KERN_WARNING_VIO
+			       "message from invalid partition. "
+			       "ack msg rcvd, source inst (%d) doesnt match (%d)\n",
+			       viopathStatus[remoteLp].mSourceInst,
+			       event->xSourceInstanceId);
+			return;
+		}
+
+		if (event->xTargetInstanceId !=
+		    viopathStatus[remoteLp].mTargetInst) {
+			printk(KERN_WARNING_VIO
+			       "message from invalid partition. "
+			       "viopath: ack msg rcvd, target inst (%d) doesnt match (%d)\n",
+			       viopathStatus[remoteLp].mTargetInst,
+			       event->xTargetInstanceId);
+			return;
+		}
+	}
+
+	if (vio_handler[subtype] == NULL) {
+		printk(KERN_WARNING_VIO
+		       "unexpected virtual io event subtype %d from partition %d\n",
+		       event->xSubtype, remoteLp);
+		/* No handler.  Ack if necessary */
+		if ((event->xFlags.xFunction == HvLpEvent_Function_Int) &&
+		    (event->xFlags.xAckInd == HvLpEvent_AckInd_DoAck)) {
+			event->xRc = HvLpEvent_Rc_InvalidSubtype;
+			HvCallEvent_ackLpEvent(event);
+		}
+		return;
+	}
+
+	/* This innocuous little line is where all the real work happens */
+	(*vio_handler[subtype])(event);
+}
+
+static void viopath_donealloc(void *parm, int number)
+{
+	struct doneAllocParms_t *parmsp = (struct doneAllocParms_t *)parm;
+
+	parmsp->number = number;
+	if (parmsp->used_wait_atomic)
+		*(parmsp->wait_atomic) = 0;
+	else
+		up(parmsp->sem);
+}
+
+static int allocateEvents(HvLpIndex remoteLp, int numEvents)
+{
+	struct doneAllocParms_t parms;
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	volatile unsigned long wait_atomic = 1;
+
+	if (in_atomic()) {
+		parms.used_wait_atomic = 1;
+		parms.wait_atomic = &wait_atomic;
+	} else {
+		parms.used_wait_atomic = 0;
+		parms.sem = &Semaphore;
+	}
+	mf_allocateLpEvents(remoteLp, HvLpEvent_Type_VirtualIo, 250,	/* It would be nice to put a real number here! */
+			    numEvents, &viopath_donealloc, &parms);
+	if (in_atomic()) {
+		while (wait_atomic)
+			mb();
+	} else
+		down(&Semaphore);
+	return parms.number;
+}
+
+int viopath_open(HvLpIndex remoteLp, int subtype, int numReq)
+{
+	int i;
+	unsigned long flags;
+	int tempNumAllocated;
+
+	if ((remoteLp >= HvMaxArchitectedLps) || (remoteLp == HvLpIndexInvalid))
+		return -EINVAL;
+
+	subtype = subtype >> VIOMAJOR_SUBTYPE_SHIFT;
+	if ((subtype < 0) || (subtype >= VIO_MAX_SUBTYPES))
+		return -EINVAL;
+
+	spin_lock_irqsave(&statuslock, flags);
+
+	if (!event_buffer_initialised) {
+		for (i = 0; i < VIO_MAX_SUBTYPES; i++)
+			atomic_set(&event_buffer_available[i], 1);
+		event_buffer_initialised = 1;
+	}
+
+	viopathStatus[remoteLp].users[subtype]++;
+
+	if (!viopathStatus[remoteLp].isOpen) {
+		viopathStatus[remoteLp].isOpen = 1;
+		HvCallEvent_openLpEventPath(remoteLp, HvLpEvent_Type_VirtualIo);
+
+		/*
+		 * Don't hold the spinlock during an operation that
+		 * can sleep.
+		 */
+		spin_unlock_irqrestore(&statuslock, flags);
+		tempNumAllocated = allocateEvents(remoteLp, 1);
+		spin_lock_irqsave(&statuslock, flags);
+
+		viopathStatus[remoteLp].numberAllocated += tempNumAllocated;
+
+		if (viopathStatus[remoteLp].numberAllocated == 0) {
+			HvCallEvent_closeLpEventPath(remoteLp,
+					HvLpEvent_Type_VirtualIo);
+
+			spin_unlock_irqrestore(&statuslock, flags);
+			return -ENOMEM;
+		}
+
+		viopathStatus[remoteLp].mSourceInst =
+			HvCallEvent_getSourceLpInstanceId(remoteLp,
+					HvLpEvent_Type_VirtualIo);
+		viopathStatus[remoteLp].mTargetInst =
+			HvCallEvent_getTargetLpInstanceId(remoteLp,
+					HvLpEvent_Type_VirtualIo);
+		HvLpEvent_registerHandler(HvLpEvent_Type_VirtualIo,
+					  &vio_handleEvent);
+		sendMonMsg(remoteLp);
+		printk(KERN_INFO_VIO
+		       "Opening connection to partition %d, setting sinst %d, tinst %d\n",
+		       remoteLp, viopathStatus[remoteLp].mSourceInst,
+		       viopathStatus[remoteLp].mTargetInst);
+	}
+
+	spin_unlock_irqrestore(&statuslock, flags);
+	tempNumAllocated = allocateEvents(remoteLp, numReq);
+	spin_lock_irqsave(&statuslock, flags);
+	viopathStatus[remoteLp].numberAllocated += tempNumAllocated;
+	spin_unlock_irqrestore(&statuslock, flags);
+
+	return 0;
+}
+EXPORT_SYMBOL(viopath_open);
+
+int viopath_close(HvLpIndex remoteLp, int subtype, int numReq)
+{
+	unsigned long flags;
+	int i;
+	int numOpen;
+	struct doneAllocParms_t doneAllocParms;
+	DECLARE_MUTEX_LOCKED(Semaphore);
+
+	if ((remoteLp >= HvMaxArchitectedLps) || (remoteLp == HvLpIndexInvalid))
+		return -EINVAL;
+
+	subtype = subtype >> VIOMAJOR_SUBTYPE_SHIFT;
+	if ((subtype < 0) || (subtype >= VIO_MAX_SUBTYPES))
+		return -EINVAL;
+
+	spin_lock_irqsave(&statuslock, flags);
+	/*
+	 * If the viopath_close somehow gets called before a
+	 * viopath_open it could decrement to -1 which is a non
+	 * recoverable state so we'll prevent this from
+	 * happening.
+	 */
+	if (viopathStatus[remoteLp].users[subtype] > 0)
+		viopathStatus[remoteLp].users[subtype]--;
+
+	spin_unlock_irqrestore(&statuslock, flags);
+
+	doneAllocParms.sem = &Semaphore;
+	mf_deallocateLpEvents(remoteLp, HvLpEvent_Type_VirtualIo,
+			      numReq, &viopath_donealloc, &doneAllocParms);
+	down(&Semaphore);
+
+	spin_lock_irqsave(&statuslock, flags);
+	for (i = 0, numOpen = 0; i < VIO_MAX_SUBTYPES; i++)
+		numOpen += viopathStatus[remoteLp].users[i];
+
+	if ((viopathStatus[remoteLp].isOpen) && (numOpen == 0)) {
+		printk(KERN_INFO_VIO "Closing connection to partition %d",
+				remoteLp);
+
+		HvCallEvent_closeLpEventPath(remoteLp,
+					     HvLpEvent_Type_VirtualIo);
+		viopathStatus[remoteLp].isOpen = 0;
+		viopathStatus[remoteLp].isActive = 0;
+
+		for (i = 0; i < VIO_MAX_SUBTYPES; i++)
+			atomic_set(&event_buffer_available[i], 0);
+		event_buffer_initialised = 0;
+	}
+	spin_unlock_irqrestore(&statuslock, flags);
+	return 0;
+}
+EXPORT_SYMBOL(viopath_close);
+
+void *vio_get_event_buffer(int subtype)
+{
+	subtype = subtype >> VIOMAJOR_SUBTYPE_SHIFT;
+	if ((subtype < 0) || (subtype >= VIO_MAX_SUBTYPES))
+		return NULL;
+
+	if (atomic_dec_if_positive(&event_buffer_available[subtype]) == 0)
+		return &event_buffer[subtype * 256];
+	else
+		return NULL;
+}
+EXPORT_SYMBOL(vio_get_event_buffer);
+
+void vio_free_event_buffer(int subtype, void *buffer)
+{
+	subtype = subtype >> VIOMAJOR_SUBTYPE_SHIFT;
+	if ((subtype < 0) || (subtype >= VIO_MAX_SUBTYPES)) {
+		printk(KERN_WARNING_VIO
+		       "unexpected subtype %d freeing event buffer\n",
+		       subtype);
+		return;
+	}
+
+	if (atomic_read(&event_buffer_available[subtype]) != 0) {
+		printk(KERN_WARNING_VIO
+		       "freeing unallocated event buffer, subtype %d\n",
+		       subtype);
+		return;
+	}
+
+	if (buffer != &event_buffer[subtype * 256]) {
+		printk(KERN_WARNING_VIO
+		       "freeing invalid event buffer, subtype %d\n",
+		       subtype);
+	}
+
+	atomic_set(&event_buffer_available[subtype], 1);
+}
+EXPORT_SYMBOL(vio_free_event_buffer);
+
+static const struct vio_error_entry vio_no_error =
+    { 0, 0, "Non-VIO Error" };
+static const struct vio_error_entry vio_unknown_error =
+    { 0, EIO, "Unknown Error" };
+
+static const struct vio_error_entry vio_default_errors[] = {
+	{0x0001, EIO, "No Connection"},
+	{0x0002, EIO, "No Receiver"},
+	{0x0003, EIO, "No Buffer Available"},
+	{0x0004, EBADRQC, "Invalid Message Type"},
+	{0x0000, 0, NULL},
+};
+
+const struct vio_error_entry *vio_lookup_rc(
+		const struct vio_error_entry *local_table, u16 rc)
+{
+	const struct vio_error_entry *cur;
+
+	if (!rc)
+		return &vio_no_error;
+	if (local_table)
+		for (cur = local_table; cur->rc; ++cur)
+			if (cur->rc == rc)
+				return cur;
+	for (cur = vio_default_errors; cur->rc; ++cur)
+		if (cur->rc == rc)
+			return cur;
+	return &vio_unknown_error;
+}
+EXPORT_SYMBOL(vio_lookup_rc);
diff -purN linux-2.5/arch/ppc64/kernel/vmlinux.lds.S linuxppc64-2.5/arch/ppc64/kernel/vmlinux.lds.S
--- linux-2.5/arch/ppc64/kernel/vmlinux.lds.S	2003-09-03 03:05:08.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/vmlinux.lds.S	2003-12-17 05:08:23.000000000 +0000
@@ -53,7 +53,6 @@ SECTIONS
     *(.data1)
     *(.sdata)
     *(.sdata2)
-    *(.got.plt) *(.got)
     *(.dynamic)
     CONSTRUCTORS
   }
@@ -126,6 +125,7 @@ SECTIONS
   /* freed after init ends here */
 
   __toc_start = .;
+  .got : { *(.got.plt) *(.got) }
   .toc : { *(.toc) }
   . = ALIGN(4096);
   __toc_end = .;
diff -purN linux-2.5/arch/ppc64/kernel/xics.c linuxppc64-2.5/arch/ppc64/kernel/xics.c
--- linux-2.5/arch/ppc64/kernel/xics.c	2003-09-03 04:01:38.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/xics.c	2003-12-03 04:04:55.000000000 +0000
@@ -58,7 +58,6 @@ struct hw_interrupt_type xics_8259_pic =
 };
 
 #define XICS_IPI		2
-#define XICS_IRQ_OFFSET		0x10
 #define XICS_IRQ_SPURIOUS	0
 
 /* Want a priority other than 0.  Various HW issues require this. */
@@ -202,7 +201,7 @@ static void pSeriesLP_qirr_info(int n_cp
 {
 	unsigned long lpar_rc;
 
-	lpar_rc = plpar_ipi(n_cpu, value);
+	lpar_rc = plpar_ipi(get_hard_smp_processor_id(n_cpu), value);
 	if (lpar_rc != H_Success)
 		panic("bad return code qirr - rc = %lx\n", lpar_rc); 
 }
@@ -214,22 +213,37 @@ xics_ops pSeriesLP_ops = {
 	pSeriesLP_qirr_info
 };
 
-void xics_enable_irq(u_int virq)
+/* XXX Fix this when we clean up large irq support */
+extern cpumask_t get_irq_affinity(unsigned int irq);
+
+void xics_enable_irq(unsigned int irq)
 {
-	u_int irq;
 	long call_status;
 	unsigned int server;
+	cpumask_t cpumask = get_irq_affinity(irq);
+	cpumask_t allcpus = CPU_MASK_ALL;
+	cpumask_t tmp = CPU_MASK_NONE;
 
-	virq -= XICS_IRQ_OFFSET;
-	irq = virt_irq_to_real(virq);
+	irq = irq_offset_down(irq);
 	if (irq == XICS_IPI)
 		return;
 
 #ifdef CONFIG_IRQ_ALL_CPUS
-	if (smp_threads_ready)
-		server = default_distrib_server;
-	else
+	/* For the moment only implement delivery to all cpus or one cpu */
+	if (smp_threads_ready) {
+		if (cpus_equal(cpumask, allcpus)) {
+			server = default_distrib_server;
+		} else {
+			cpus_and(tmp, cpu_online_map, cpumask);
+
+			if (cpus_empty(tmp))
+				server = default_distrib_server;
+			else
+				server = get_hard_smp_processor_id(first_cpu(tmp));
+		}
+	} else {
 		server = default_server;
+	}
 #else
 	server = default_server;
 #endif
@@ -251,13 +265,11 @@ void xics_enable_irq(u_int virq)
 	}
 }
 
-void xics_disable_irq(u_int virq)
+void xics_disable_irq(unsigned int irq)
 {
-	u_int irq;
 	long call_status;
 
-	virq -= XICS_IRQ_OFFSET;
-	irq = virt_irq_to_real(virq);
+	irq = irq_offset_down(irq);
 	if (irq == XICS_IPI)
 		return;
 
@@ -278,20 +290,20 @@ void xics_disable_irq(u_int virq)
 	}
 }
 
-void xics_end_irq(u_int	irq)
+void xics_end_irq(unsigned int irq)
 {
 	int cpu = smp_processor_id();
 
 	iosync();
-	ops->xirr_info_set(cpu, ((0xff<<24) |
-				 (virt_irq_to_real(irq-XICS_IRQ_OFFSET))));
+	ops->xirr_info_set(cpu, ((0xff<<24) | (irq_offset_down(irq))));
+
 }
 
 void xics_mask_and_ack_irq(u_int irq)
 {
 	int cpu = smp_processor_id();
 
-	if (irq < XICS_IRQ_OFFSET) {
+	if (irq < irq_offset_value()) {
 		i8259_pic.ack(irq);
 		iosync();
 		ops->xirr_info_set(cpu, ((0xff<<24) |
@@ -315,13 +327,14 @@ int xics_get_irq(struct pt_regs *regs)
 		irq = i8259_irq(cpu);
 		if (irq == -1) {
 			/* Spurious cascaded interrupt.  Still must ack xics */
-                        xics_end_irq(XICS_IRQ_OFFSET + xics_irq_8259_cascade);
+			xics_end_irq(irq_offset_up(xics_irq_8259_cascade));
+
 			irq = -1;
 		}
 	} else if (vec == XICS_IRQ_SPURIOUS) {
 		irq = -1;
 	} else {
-		irq = real_irq_to_virt(vec) + XICS_IRQ_OFFSET;
+		irq = irq_offset_up(vec);
 	}
 	return irq;
 }
@@ -379,6 +392,16 @@ void xics_setup_cpu(void)
 
 #endif /* CONFIG_SMP */
 
+void
+xics_init_irq_desc(irq_desc_t *desc)
+{
+	/* Don't mess with the handler if already set.
+	 * This leaves the setup of isa handlers undisturbed.
+	 */
+	if (!desc->handler)
+		desc->handler = &xics_pic;
+}
+
 void xics_init_IRQ(void)
 {
 	int i;
@@ -398,7 +421,7 @@ void xics_init_IRQ(void)
 	ibm_int_on  = rtas_token("ibm,int-on");
 	ibm_int_off = rtas_token("ibm,int-off");
 
-	np = find_type_devices("PowerPC-External-Interrupt-Presentation");
+	np = of_find_node_by_type(NULL, "PowerPC-External-Interrupt-Presentation");
 	if (!np) {
 		printk(KERN_WARNING "Can't find Interrupt Presentation\n");
 		udbg_printf("Can't find Interrupt Presentation\n");
@@ -433,13 +456,15 @@ nextnode:
 		if (indx >= NR_CPUS) break;
 	}
 
-	np = np->next;
+	np = of_find_node_by_type(np, "PowerPC-External-Interrupt-Presentation");
 	if ((indx < NR_CPUS) && np) goto nextnode;
 
 	/* Find the server numbers for the boot cpu. */
-	for (np = find_type_devices("cpu"); np; np = np->next) {
+	for (np = of_find_node_by_type(NULL, "cpu");
+	     np;
+	     np = of_find_node_by_type(np, "cpu")) {
 		ireg = (uint *)get_property(np, "reg", &ilen);
-		if (ireg && ireg[0] == smp_processor_id()) {
+		if (ireg && ireg[0] == hard_smp_processor_id()) {
 			ireg = (uint *)get_property(np, "ibm,ppc-interrupt-gserver#s", &ilen);
 			i = ilen / sizeof(int);
 			if (ireg && i > 0) {
@@ -449,11 +474,12 @@ nextnode:
 			break;
 		}
 	}
+	of_node_put(np);
 
 	intr_base = inodes[0].addr;
 	intr_size = (ulong)inodes[0].size;
 
-	np = find_type_devices("interrupt-controller");
+	np = of_find_node_by_type(NULL, "interrupt-controller");
 	if (!np) {
 		printk(KERN_WARNING "xics:  no ISA Interrupt Controller\n");
 		xics_irq_8259_cascade_real = -1;
@@ -466,7 +492,8 @@ nextnode:
 			while (1);
 		}
 		xics_irq_8259_cascade_real = *ireg;
-		xics_irq_8259_cascade = virt_irq_create_mapping(xics_irq_8259_cascade_real);
+		xics_irq_8259_cascade = xics_irq_8259_cascade_real;
+		of_node_put(np);
 	}
 
 	if (systemcfg->platform == PLATFORM_PSERIES) {
@@ -474,8 +501,8 @@ nextnode:
 		for (i = 0; i < NR_CPUS; ++i) {
 			if (!cpu_possible(i))
 				continue;
-			xics_per_cpu[i] = __ioremap((ulong)inodes[i].addr, 
-						    (ulong)inodes[i].size,
+			xics_per_cpu[i] = __ioremap((ulong)inodes[get_hard_smp_processor_id(i)].addr, 
+						    (ulong)inodes[get_hard_smp_processor_id(i)].size,
 						    _PAGE_NO_CACHE);
 		}
 #else
@@ -494,9 +521,7 @@ nextnode:
 	xics_8259_pic.enable = i8259_pic.enable;
 	xics_8259_pic.disable = i8259_pic.disable;
 	for (i = 0; i < 16; ++i)
-		irq_desc[i].handler = &xics_8259_pic;
-	for (; i < NR_IRQS; ++i)
-		irq_desc[i].handler = &xics_pic;
+		get_real_irq_desc(i)->handler = &xics_8259_pic;
 
 	ops->cppr_info(boot_cpuid, 0xff);
 	iosync();
@@ -512,7 +537,7 @@ static int __init xics_setup_i8259(void)
 {
 	if (naca->interrupt_controller == IC_PPC_XIC &&
 	    xics_irq_8259_cascade != -1) {
-		if (request_irq(xics_irq_8259_cascade + XICS_IRQ_OFFSET,
+		if (request_irq(irq_offset_up(xics_irq_8259_cascade), 
 				no_action, 0, "8259 cascade", 0))
 			printk(KERN_ERR "xics_init_IRQ: couldn't get 8259 cascade\n");
 		i8259_init();
@@ -524,19 +549,15 @@ arch_initcall(xics_setup_i8259);
 #ifdef CONFIG_SMP
 void xics_request_IPIs(void)
 {
-	real_irq_to_virt_map[XICS_IPI] = virt_irq_to_real_map[XICS_IPI] =
-		XICS_IPI;
 	/* IPIs are marked SA_INTERRUPT as they must run with irqs disabled */
-	request_irq(XICS_IPI + XICS_IRQ_OFFSET, xics_ipi_action, SA_INTERRUPT,
-		    "IPI", 0);
-	irq_desc[XICS_IPI+XICS_IRQ_OFFSET].status |= IRQ_PER_CPU;
+	request_irq(irq_offset_up(XICS_IPI), xics_ipi_action, SA_INTERRUPT, "IPI", 0);
+	get_real_irq_desc(irq_offset_up(XICS_IPI))->status |= IRQ_PER_CPU;
 }
 #endif
 
-void xics_set_affinity(unsigned int virq, cpumask_t cpumask)
+void xics_set_affinity(unsigned int irq, cpumask_t cpumask)
 {
-        irq_desc_t *desc = irq_desc + virq;
-	unsigned int irq;
+	irq_desc_t *desc = get_irq_desc(irq);
 	unsigned long flags;
 	long status;
 	unsigned long xics_status[2];
@@ -544,8 +565,7 @@ void xics_set_affinity(unsigned int virq
 	cpumask_t allcpus = CPU_MASK_ALL;
 	cpumask_t tmp = CPU_MASK_NONE;
 
-	virq -= XICS_IRQ_OFFSET;
-	irq = virt_irq_to_real(virq);
+	irq = irq_offset_down(irq);
 	if (irq == XICS_IPI)
 		return;
 
@@ -566,7 +586,7 @@ void xics_set_affinity(unsigned int virq
 		cpus_and(tmp, cpu_online_map, cpumask);
 		if (cpus_empty(tmp))
 			goto out;
-		newmask = first_cpu(cpumask);
+		newmask = get_hard_smp_processor_id(first_cpu(tmp));
 	}
 
 	status = rtas_call(ibm_set_xive, 3, 1, NULL,
diff -purN linux-2.5/arch/ppc64/mm/Makefile linuxppc64-2.5/arch/ppc64/mm/Makefile
--- linux-2.5/arch/ppc64/mm/Makefile	2003-09-07 01:24:09.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/mm/Makefile	2003-12-17 05:08:23.000000000 +0000
@@ -4,6 +4,6 @@
 
 EXTRA_CFLAGS += -mno-minimal-toc
 
-obj-y := fault.o init.o extable.o imalloc.o
+obj-y := fault.o init.o extable.o imalloc.o hash_utils.o hash_low.o
 obj-$(CONFIG_DISCONTIGMEM) += numa.o
 obj-$(CONFIG_HUGETLB_PAGE) += hugetlbpage.o
diff -purN linux-2.5/arch/ppc64/mm/hash_low.S linuxppc64-2.5/arch/ppc64/mm/hash_low.S
--- linux-2.5/arch/ppc64/mm/hash_low.S	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/mm/hash_low.S	2003-12-17 04:55:14.000000000 +0000
@@ -0,0 +1,285 @@
+/*
+ * ppc64 MMU hashtable management routines
+ *
+ * (c) Copyright IBM Corp. 2003
+ *
+ * Maintained by: Benjamin Herrenschmidt
+ *                <benh@kernel.crashing.org>
+ *
+ * This file is covered by the GNU Public Licence v2 as
+ * described in the kernel's COPYING file.
+ */
+
+#include <linux/config.h>
+#include <asm/processor.h>
+#include <asm/pgtable.h>
+#include <asm/mmu.h>
+#include <asm/page.h>
+#include <asm/types.h>
+#include <asm/ppc_asm.h>
+#include <asm/offsets.h>
+#include <asm/cputable.h>
+
+	.text
+
+/*
+ * Stackframe:
+ *		
+ *         +-> Back chain			(SP + 256)
+ *         |   General register save area	(SP + 112)
+ *         |   Parameter save area		(SP + 48)
+ *         |   TOC save area			(SP + 40)
+ *         |   link editor doubleword		(SP + 32)
+ *         |   compiler doubleword		(SP + 24)
+ *         |   LR save area			(SP + 16)
+ *         |   CR save area			(SP + 8)
+ * SP ---> +-- Back chain			(SP + 0)
+ */
+#define STACKFRAMESIZE	256
+
+/* Save parameters offsets */
+#define STK_PARM(i)	(STACKFRAMESIZE + 48 + ((i)-3)*8)
+
+/* Save non-volatile offsets */
+#define STK_REG(i)	(112 + ((i)-14)*8)
+
+/*
+ * _hash_page(unsigned long ea, unsigned long access, unsigned long vsid,
+ *		pte_t *ptep, unsigned long trap, int local)
+ *
+ * Adds a page to the hash table. This is the non-LPAR version for now
+ */
+
+_GLOBAL(__hash_page)
+	mflr	r0
+	std	r0,16(r1)
+	stdu	r1,-STACKFRAMESIZE(r1)
+	/* Save all params that we need after a function call */
+	std	r6,STK_PARM(r6)(r1)
+	std	r8,STK_PARM(r8)(r1)
+	
+	/* Add _PAGE_PRESENT to access */
+	ori	r4,r4,_PAGE_PRESENT
+
+	/* Save non-volatile registers.
+	 * r31 will hold "old PTE"
+	 * r30 is "new PTE"
+	 * r29 is "va"
+	 * r28 is a hash value
+	 * r27 is hashtab mask (maybe dynamic patched instead ?)
+	 */
+	std	r27,STK_REG(r27)(r1)
+	std	r28,STK_REG(r28)(r1)
+	std	r29,STK_REG(r29)(r1)
+	std	r30,STK_REG(r30)(r1)
+	std	r31,STK_REG(r31)(r1)
+	
+	/* Step 1:
+	 *
+	 * Check permissions, atomically mark the linux PTE busy
+	 * and hashed.
+	 */ 
+1:
+	ldarx	r31,0,r6
+	/* Check access rights (access & ~(pte_val(*ptep))) */
+	andc.	r0,r4,r31
+	bne-	htab_wrong_access
+	/* Check if PTE is busy */
+	andi.	r0,r31,_PAGE_BUSY
+	bne-	1b
+	/* Prepare new PTE value (turn access RW into DIRTY, then
+	 * add BUSY,HASHPTE and ACCESSED)
+	 */
+	rlwinm	r30,r4,5,24,24	/* _PAGE_RW -> _PAGE_DIRTY */
+	or	r30,r30,r31
+	ori	r30,r30,_PAGE_BUSY | _PAGE_ACCESSED | _PAGE_HASHPTE
+	/* Write the linux PTE atomically (setting busy) */
+	stdcx.	r30,0,r6
+	bne-	1b
+	
+
+	/* Step 2:
+	 *
+	 * Insert/Update the HPTE in the hash table. At this point,
+	 * r4 (access) is re-useable, we use it for the new HPTE flags
+	 */
+
+	/* Calc va and put it in r29 */
+	rldicr	r29,r5,28,63-28
+	rldicl	r3,r3,0,36
+	or	r29,r3,r29
+
+	/* Calculate hash value for primary slot and store it in r28 */
+	rldicl	r5,r5,0,25		/* vsid & 0x0000007fffffffff */
+	rldicl	r0,r3,64-12,48		/* (ea >> 12) & 0xffff */
+	xor	r28,r5,r0
+	
+	/* Convert linux PTE bits into HW equivalents
+	 */
+	andi.	r3,r30,0x1fa		/* Get basic set of flags */
+	rlwinm	r0,r30,32-2+1,30,30	/* _PAGE_RW -> _PAGE_USER (r0) */
+	rlwinm	r4,r30,32-7+1,30,30	/* _PAGE_DIRTY -> _PAGE_USER (r4) */
+	and	r0,r0,r4		/* _PAGE_RW & _PAGE_DIRTY -> r0 bit 30 */
+	andc	r0,r30,r0		/* r0 = pte & ~r0 */
+	rlwimi	r3,r0,32-1,31,31	/* Insert result into PP lsb */
+
+	/* We eventually do the icache sync here (maybe inline that
+	 * code rather than call a C function...) 
+	 */
+BEGIN_FTR_SECTION
+	mr	r4,r30
+	mr	r5,r7
+	bl	.hash_page_do_lazy_icache
+END_FTR_SECTION_IFSET(CPU_FTR_NOEXECUTE)
+
+	/* At this point, r3 contains new PP bits, save them in
+	 * place of "access" in the param area (sic)
+	 */
+	std	r3,STK_PARM(r4)(r1)
+
+	/* Get htab_hash_mask */
+	ld	r4,htab_data@got(2)
+	ld	r27,16(r4)	/* htab_data.htab_hash_mask -> r27 */
+
+	/* Check if we may already be in the hashtable, in this case, we
+	 * go to out-of-line code to try to modify the HPTE
+	 */
+	andi.	r0,r31,_PAGE_HASHPTE
+	bne	htab_modify_pte
+
+htab_insert_pte:
+	/* Clear hpte bits in new pte (we also clear BUSY btw) and
+	 * add _PAGE_HASHPTE
+	 */
+	lis	r0,_PAGE_HPTEFLAGS@h
+	ori	r0,r0,_PAGE_HPTEFLAGS@l
+	andc	r30,r30,r0
+	ori	r30,r30,_PAGE_HASHPTE
+
+1:
+	/* page number in r5 */
+	rldicl	r5,r31,64-PTE_SHIFT,PTE_SHIFT
+
+	/* Calculate primary group hash */
+	and	r0,r28,r27
+	rldicr	r3,r0,3,63-3	/* r0 = (hash & mask) << 3 */
+
+	/* Call ppc_md.hpte_insert */
+	ld	r7,STK_PARM(r4)(r1)	/* Retreive new pp bits */
+	mr	r4,r29			/* Retreive va */
+	li	r6,0			/* primary slot *
+	li	r8,0			/* not bolted and not large */
+	li	r9,0
+_GLOBAL(htab_call_hpte_insert1)
+	bl	.			/* Will be patched by htab_finish_init() */
+	cmpi	0,r3,0
+	bge	htab_pte_insert_ok	/* Insertion successful */
+	cmpi	0,r3,-2			/* Critical failure */
+	beq-	htab_pte_insert_failure
+
+	/* Now try secondary slot */
+	ori	r30,r30,_PAGE_SECONDARY
+	
+	/* page number in r5 */
+	rldicl	r5,r31,64-PTE_SHIFT,PTE_SHIFT
+
+	/* Calculate secondary group hash */
+	not	r3,r28
+	and	r0,r3,r27
+	rldicr	r3,r0,3,63-3	/* r0 = (~hash & mask) << 3 */
+	
+	/* Call ppc_md.hpte_insert */
+	ld	r7,STK_PARM(r4)(r1)	/* Retreive new pp bits */
+	mr	r4,r29			/* Retreive va */
+	li	r6,1			/* secondary slot *
+	li	r8,0			/* not bolted and not large */
+	li	r9,0
+_GLOBAL(htab_call_hpte_insert2)
+	bl	.			/* Will be patched by htab_finish_init() */
+	cmpi	0,r3,0
+	bge+	htab_pte_insert_ok	/* Insertion successful */
+	cmpi	0,r3,-2			/* Critical failure */
+	beq-	htab_pte_insert_failure
+
+	/* Both are full, we need to evict something */
+	mftb	r0
+	/* Pick a random group based on TB */
+	andi.	r0,r0,1
+	mr	r5,r28
+	bne	2f
+	not	r5,r5
+2:	and	r0,r5,r27
+	rldicr	r3,r0,3,63-3	/* r0 = (hash & mask) << 3 */	
+	/* Call ppc_md.hpte_remove */
+_GLOBAL(htab_call_hpte_remove)
+	bl	.			/* Will be patched by htab_finish_init() */
+
+	/* Try all again */
+	b	1b	
+
+htab_pte_insert_ok:
+	/* Insert slot number in PTE */
+	rldimi	r30,r3,12,63-14
+		
+	/* Write out the PTE with a normal write
+	 * (maybe add eieio may be good still ?)
+	 */
+htab_write_out_pte:
+	ld	r6,STK_PARM(r6)(r1)
+	std	r30,0(r6)
+	li	r3, 0
+bail:
+	ld	r27,STK_REG(r27)(r1)
+	ld	r28,STK_REG(r28)(r1)
+	ld	r29,STK_REG(r29)(r1)
+	ld      r30,STK_REG(r30)(r1)
+	ld      r31,STK_REG(r31)(r1)
+	addi    r1,r1,STACKFRAMESIZE
+	ld      r0,16(r1)
+	mtlr    r0
+	blr
+
+htab_modify_pte:
+	/* Keep PP bits in r4 and slot idx from the PTE around in r3 */
+	mr	r4,r3
+	rlwinm	r3,r31,32-12,29,31
+
+	/* Secondary group ? if yes, get a inverted hash value */
+	mr	r5,r28
+	andi.	r0,r31,_PAGE_SECONDARY
+	beq	1f
+	not	r5,r5
+1:
+	/* Calculate proper slot value for ppc_md.hpte_updatepp */
+	and	r0,r5,r27
+	rldicr	r0,r0,3,63-3	/* r0 = (hash & mask) << 3 */
+	add	r3,r0,r3	/* add slot idx */
+
+	/* Call ppc_md.hpte_updatepp */
+	mr	r5,r29			/* va */
+	li	r6,0			/* large is 0 */
+	ld	r7,STK_PARM(r8)(r1)	/* get "local" param */
+_GLOBAL(htab_call_hpte_updatepp)
+	bl	.			/* Will be patched by htab_finish_init() */
+
+	/* if we failed because typically the HPTE wasn't really here
+	 * we try an insertion. 
+	 */
+	cmpi	0,r3,-1
+	beq-	htab_insert_pte
+
+	/* Clear the BUSY bit and Write out the PTE */
+	li	r0,_PAGE_BUSY
+	andc	r30,r30,r0
+	b	htab_write_out_pte
+
+htab_wrong_access:
+	/* Bail out clearing reservation */
+	stdcx.	r31,0,r6
+	li	r3,1
+	b	bail
+
+htab_pte_insert_failure:
+	b	.htab_insert_failure
+
+
diff -purN linux-2.5/arch/ppc64/mm/hash_utils.c linuxppc64-2.5/arch/ppc64/mm/hash_utils.c
--- linux-2.5/arch/ppc64/mm/hash_utils.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/mm/hash_utils.c	2003-12-17 05:08:23.000000000 +0000
@@ -0,0 +1,366 @@
+/*
+ * PowerPC64 port by Mike Corrigan and Dave Engebretsen
+ *   {mikejc|engebret}@us.ibm.com
+ *
+ *    Copyright (c) 2000 Mike Corrigan <mikejc@us.ibm.com>
+ *
+ * SMP scalability work:
+ *    Copyright (C) 2001 Anton Blanchard <anton@au.ibm.com>, IBM
+ * 
+ *    Module name: htab.c
+ *
+ *    Description:
+ *      PowerPC Hashed Page Table functions
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/config.h>
+#include <linux/spinlock.h>
+#include <linux/errno.h>
+#include <linux/sched.h>
+#include <linux/proc_fs.h>
+#include <linux/stat.h>
+#include <linux/sysctl.h>
+#include <linux/ctype.h>
+#include <linux/cache.h>
+#include <linux/init.h>
+
+#include <asm/ppcdebug.h>
+#include <asm/processor.h>
+#include <asm/pgtable.h>
+#include <asm/mmu.h>
+#include <asm/mmu_context.h>
+#include <asm/page.h>
+#include <asm/types.h>
+#include <asm/system.h>
+#include <asm/uaccess.h>
+#include <asm/naca.h>
+#include <asm/pmc.h>
+#include <asm/machdep.h>
+#include <asm/lmb.h>
+#include <asm/abs_addr.h>
+#include <asm/tlbflush.h>
+#include <asm/io.h>
+#include <asm/eeh.h>
+#include <asm/tlb.h>
+#include <asm/cacheflush.h>
+#include <asm/cputable.h>
+/*
+ * Note:  pte   --> Linux PTE
+ *        HPTE  --> PowerPC Hashed Page Table Entry
+ *
+ * Execution context:
+ *   htab_initialize is called with the MMU off (of course), but
+ *   the kernel has been copied down to zero so it can directly
+ *   reference global data.  At this point it is very difficult
+ *   to print debug info.
+ *
+ */
+
+HTAB htab_data = {NULL, 0, 0, 0, 0};
+
+extern unsigned long _SDR1;
+
+#define KB (1024)
+#define MB (1024*KB)
+
+static inline void loop_forever(void)
+{
+	volatile unsigned long x = 1;
+	for(;x;x|=1)
+		;
+}
+
+#ifdef CONFIG_PPC_PSERIES
+static inline void create_pte_mapping(unsigned long start, unsigned long end,
+				      unsigned long mode, int large)
+{
+	unsigned long addr;
+	unsigned int step;
+
+	if (large)
+		step = 16*MB;
+	else
+		step = 4*KB;
+
+	for (addr = start; addr < end; addr += step) {
+		unsigned long vpn, hash, hpteg;
+		unsigned long vsid = get_kernel_vsid(addr);
+		unsigned long va = (vsid << 28) | (addr & 0xfffffff);
+		int ret;
+
+		if (large)
+			vpn = va >> LARGE_PAGE_SHIFT;
+		else
+			vpn = va >> PAGE_SHIFT;
+
+		hash = hpt_hash(vpn, large);
+
+		hpteg = ((hash & htab_data.htab_hash_mask)*HPTES_PER_GROUP);
+
+		if (systemcfg->platform == PLATFORM_PSERIES_LPAR)
+			ret = pSeries_lpar_hpte_insert(hpteg, va,
+				(unsigned long)__v2a(addr) >> PAGE_SHIFT,
+				0, mode, 1, large);
+		else
+			ret = pSeries_hpte_insert(hpteg, va,
+				(unsigned long)__v2a(addr) >> PAGE_SHIFT,
+				0, mode, 1, large);
+
+		if (ret == -1) {
+			ppc64_terminate_msg(0x20, "create_pte_mapping");
+			loop_forever();
+		}
+	}
+}
+
+void __init htab_initialize(void)
+{
+	unsigned long table, htab_size_bytes;
+	unsigned long pteg_count;
+	unsigned long mode_rw;
+
+	/*
+	 * Calculate the required size of the htab.  We want the number of
+	 * PTEGs to equal one half the number of real pages.
+	 */ 
+	htab_size_bytes = 1UL << naca->pftSize;
+	pteg_count = htab_size_bytes >> 7;
+
+	/* For debug, make the HTAB 1/8 as big as it normally would be. */
+	ifppcdebug(PPCDBG_HTABSIZE) {
+		pteg_count >>= 3;
+		htab_size_bytes = pteg_count << 7;
+	}
+
+	htab_data.htab_num_ptegs = pteg_count;
+	htab_data.htab_hash_mask = pteg_count - 1;
+
+	if (systemcfg->platform == PLATFORM_PSERIES) {
+		/* Find storage for the HPT.  Must be contiguous in
+		 * the absolute address space.
+		 */
+		table = lmb_alloc(htab_size_bytes, htab_size_bytes);
+		if ( !table ) {
+			ppc64_terminate_msg(0x20, "hpt space");
+			loop_forever();
+		}
+		htab_data.htab = (HPTE *)__a2v(table);
+
+		/* htab absolute addr + encoded htabsize */
+		_SDR1 = table + __ilog2(pteg_count) - 11;
+
+		/* Initialize the HPT with no entries */
+		memset((void *)table, 0, htab_size_bytes);
+	} else {
+		/* Using a hypervisor which owns the htab */
+		htab_data.htab = NULL;
+		_SDR1 = 0; 
+	}
+
+	mode_rw = _PAGE_ACCESSED | _PAGE_COHERENT | PP_RWXX;
+
+	/* XXX we currently map kernel text rw, should fix this */
+	if ((cur_cpu_spec->cpu_features & CPU_FTR_16M_PAGE)
+	    && systemcfg->physicalMemorySize > 256*MB) {
+		create_pte_mapping((unsigned long)KERNELBASE, 
+				   KERNELBASE + 256*MB, mode_rw, 0);
+		create_pte_mapping((unsigned long)KERNELBASE + 256*MB, 
+				   KERNELBASE + (systemcfg->physicalMemorySize), 
+				   mode_rw, 1);
+	} else {
+		create_pte_mapping((unsigned long)KERNELBASE, 
+				   KERNELBASE+(systemcfg->physicalMemorySize), 
+				   mode_rw, 0);
+	}
+}
+#undef KB
+#undef MB
+#endif
+
+/*
+ * Called by asm hashtable.S for doing lazy icache flush
+ */
+unsigned int hash_page_do_lazy_icache(unsigned int pp, pte_t pte, int trap)
+{
+	struct page *page;
+
+#define PPC64_HWNOEXEC (1 << 2)
+
+	if (!pfn_valid(pte_pfn(pte)))
+		return pp;
+
+	page = pte_page(pte);
+
+	/* page is dirty */
+	if (!test_bit(PG_arch_1, &page->flags) && !PageReserved(page)) {
+		if (trap == 0x400) {
+			__flush_dcache_icache(page_address(page));
+			set_bit(PG_arch_1, &page->flags);
+		} else
+			pp |= PPC64_HWNOEXEC;
+	}
+	return pp;
+}
+
+/*
+ * Called by asm hashtable.S in case of critical insert failure
+ */
+void htab_insert_failure(void)
+{
+	panic("hash_page: pte_insert failed\n");
+}
+
+/*
+ * Handle a fault by adding an HPTE. If the address can't be determined
+ * to be valid via Linux page tables, return 1. If handled return 0
+ */
+extern int __hash_page(unsigned long ea, unsigned long access, unsigned long vsid,
+		       pte_t *ptep, unsigned long trap, int local);
+
+
+int hash_page(unsigned long ea, unsigned long access, unsigned long trap)
+{
+	void *pgdir;
+	unsigned long vsid;
+	struct mm_struct *mm;
+	pte_t *ptep;
+	int ret;
+	int user_region = 0;
+	int local = 0;
+	cpumask_t tmp;
+
+	/* Check for invalid addresses. */
+	if (!IS_VALID_EA(ea))
+		return 1;
+
+ 	switch (REGION_ID(ea)) {
+	case USER_REGION_ID:
+		user_region = 1;
+		mm = current->mm;
+		if (mm == NULL)
+			return 1;
+
+		vsid = get_vsid(mm->context, ea);
+		break;
+	case IO_REGION_ID:
+		mm = &ioremap_mm;
+		vsid = get_kernel_vsid(ea);
+		break;
+	case VMALLOC_REGION_ID:
+		mm = &init_mm;
+		vsid = get_kernel_vsid(ea);
+		break;
+#if 0
+	case EEH_REGION_ID:
+		/*
+		 * Should only be hit if there is an access to MMIO space
+		 * which is protected by EEH.
+		 * Send the problem up to do_page_fault 
+		 */
+	case KERNEL_REGION_ID:
+		/*
+		 * Should never get here - entire 0xC0... region is bolted.
+		 * Send the problem up to do_page_fault 
+		 */
+#endif
+	default:
+		/* Not a valid range
+		 * Send the problem up to do_page_fault 
+		 */
+		return 1;
+		break;
+	}
+
+	pgdir = mm->pgd;
+
+	if (pgdir == NULL)
+		return 1;
+
+	tmp = cpumask_of_cpu(smp_processor_id());
+	if (user_region && cpus_equal(mm->cpu_vm_mask, tmp))
+		local = 1;
+
+	/* Is this a huge page ? */
+	if (unlikely(in_hugepage_area(mm->context, ea)))
+		ret = hash_huge_page(mm, access, ea, vsid, local);
+	else {
+		ptep = find_linux_pte(pgdir, ea);
+		if (ptep == NULL)
+			return 1;
+		ret = __hash_page(ea, access, vsid, ptep, trap, local);
+	}
+
+
+	return ret;
+}
+
+void flush_hash_page(unsigned long context, unsigned long ea, pte_t pte,
+		     int local)
+{
+	unsigned long vsid, vpn, va, hash, secondary, slot;
+
+	/* XXX fix for large ptes */
+	unsigned long large = 0;
+
+	if ((ea >= USER_START) && (ea <= USER_END))
+		vsid = get_vsid(context, ea);
+	else
+		vsid = get_kernel_vsid(ea);
+
+	va = (vsid << 28) | (ea & 0x0fffffff);
+	if (large)
+		vpn = va >> LARGE_PAGE_SHIFT;
+	else
+		vpn = va >> PAGE_SHIFT;
+	hash = hpt_hash(vpn, large);
+	secondary = (pte_val(pte) & _PAGE_SECONDARY) >> 15;
+	if (secondary)
+		hash = ~hash;
+	slot = (hash & htab_data.htab_hash_mask) * HPTES_PER_GROUP;
+	slot += (pte_val(pte) & _PAGE_GROUP_IX) >> 12;
+
+	ppc_md.hpte_invalidate(slot, va, large, local);
+}
+
+void flush_hash_range(unsigned long context, unsigned long number, int local)
+{
+	if (ppc_md.flush_hash_range) {
+		ppc_md.flush_hash_range(context, number, local);
+	} else {
+		int i;
+		struct ppc64_tlb_batch *batch =
+			&ppc64_tlb_batch[smp_processor_id()];
+
+		for (i = 0; i < number; i++)
+			flush_hash_page(context, batch->addr[i], batch->pte[i],
+					local);
+	}
+}
+
+static inline void make_bl(unsigned int *insn_addr, void *func)
+{
+	unsigned long funcp = *((unsigned long *)func);
+	int offset = funcp - (unsigned long)insn_addr;
+
+	*insn_addr = (unsigned int)(0x48000001 | (offset & 0x03fffffc));
+	flush_icache_range((unsigned long)insn_addr, 4+
+			   (unsigned long)insn_addr);
+}
+
+void __init htab_finish_init(void)
+{
+	extern unsigned int *htab_call_hpte_insert1;
+	extern unsigned int *htab_call_hpte_insert2;
+	extern unsigned int *htab_call_hpte_remove;
+	extern unsigned int *htab_call_hpte_updatepp;
+
+	make_bl(htab_call_hpte_insert1, ppc_md.hpte_insert);
+	make_bl(htab_call_hpte_insert2, ppc_md.hpte_insert);
+	make_bl(htab_call_hpte_remove, ppc_md.hpte_remove);
+	make_bl(htab_call_hpte_updatepp, ppc_md.hpte_updatepp);
+}
diff -purN linux-2.5/arch/ppc64/mm/hugetlbpage.c linuxppc64-2.5/arch/ppc64/mm/hugetlbpage.c
--- linux-2.5/arch/ppc64/mm/hugetlbpage.c	2003-09-25 21:23:49.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/mm/hugetlbpage.c	2003-12-17 05:08:23.000000000 +0000
@@ -652,13 +652,9 @@ int hash_huge_page(struct mm_struct *mm,
 	unsigned long va, vpn;
 	int is_write;
 	hugepte_t old_pte, new_pte;
-	unsigned long hpteflags, prpn;
+	unsigned long hpteflags, prpn, flags;
 	long slot;
 
-	/* Is this for us? */
-	if (!in_hugepage_area(mm->context, ea))
-		return -1;
-
 	ea &= ~(HPAGE_SIZE-1);
 
 	/* We have to find the first hugepte in the batch, since
@@ -698,6 +694,8 @@ int hash_huge_page(struct mm_struct *mm,
 	 *	page is currently not DIRTY. 
 	 */
 
+	spin_lock_irqsave(&mm->page_table_lock, flags);
+
 	old_pte = *ptep;
 	new_pte = old_pte;
 
@@ -769,6 +767,8 @@ repeat:
 		*ptep = new_pte;
 	}
 
+	spin_unlock_irqrestore(&mm->page_table_lock, flags);
+
 	return 0;
 }
 
diff -purN linux-2.5/arch/ppc64/mm/imalloc.c linuxppc64-2.5/arch/ppc64/mm/imalloc.c
--- linux-2.5/arch/ppc64/mm/imalloc.c	2002-08-16 06:30:31.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/mm/imalloc.c	2003-10-23 14:10:29.000000000 +0000
@@ -18,55 +18,264 @@
 rwlock_t imlist_lock = RW_LOCK_UNLOCKED;
 struct vm_struct * imlist = NULL;
 
-struct vm_struct *get_im_area(unsigned long size)
+static int get_free_im_addr(unsigned long size, unsigned long *im_addr)
 {
 	unsigned long addr;
-	struct vm_struct **p, *tmp, *area;
-  
-	area = (struct vm_struct *) kmalloc(sizeof(*area), GFP_KERNEL);
-	if (!area)
-		return NULL;
+	struct vm_struct **p, *tmp;
+
 	addr = IMALLOC_START;
-	write_lock(&imlist_lock);
 	for (p = &imlist; (tmp = *p) ; p = &tmp->next) {
 		if (size + addr < (unsigned long) tmp->addr)
 			break;
-		addr = tmp->size + (unsigned long) tmp->addr;
-		if (addr > IMALLOC_END-size) {
-			write_unlock(&imlist_lock);
-			kfree(area);
+		if ((unsigned long)tmp->addr >= IMALLOC_START) 
+			addr = tmp->size + (unsigned long) tmp->addr;
+		if (addr > IMALLOC_END-size) 
+			return 1;
+	}
+	*im_addr = addr;
+
+	return 0;
+}
+
+/* Return whether the region described by v_addr and size overlaps
+ * the region described by vm.  Overlapping regions meet the 
+ * following conditions:
+ * 1) The regions share some part of the address space
+ * 2) The regions aren't identical
+ * 3) The first region is not a subset of the second
+ */
+static inline int im_region_overlaps(unsigned long v_addr, unsigned long size,
+		     struct vm_struct *vm)
+{
+	return (v_addr + size > (unsigned long) vm->addr + vm->size &&
+		v_addr < (unsigned long) vm->addr + vm->size) ||
+	       (v_addr < (unsigned long) vm->addr &&
+		v_addr + size > (unsigned long) vm->addr);
+}
+
+/* Return whether the region described by v_addr and size is a subset
+ * of the region described by vm
+ */
+static inline int im_region_is_subset(unsigned long v_addr, unsigned long size,
+			struct vm_struct *vm)
+{
+	return (int) (v_addr >= (unsigned long) vm->addr && 
+	              v_addr < (unsigned long) vm->addr + vm->size &&
+	    	      size < vm->size);
+}
+
+/* Determine imalloc status of region described by v_addr and size.
+ * Can return one of the following:
+ * IM_REGION_UNUSED   -  Entire region is unallocated in imalloc space.
+ * IM_REGION_SUBSET -    Region is a subset of a region that is already
+ * 			 allocated in imalloc space.
+ * 		         vm will be assigned to a ptr to the parent region.
+ * IM_REGION_EXISTS -    Exact region already allocated in imalloc space.
+ *                       vm will be assigned to a ptr to the existing imlist
+ *                       member.
+ * IM_REGION_OVERLAPS -  A portion of the region is already allocated in 
+ *                       imalloc space.
+ */
+static int im_region_status(unsigned long v_addr, unsigned long size, 
+		    struct vm_struct **vm)
+{
+	struct vm_struct *tmp;
+
+	for (tmp = imlist; tmp; tmp = tmp->next) 
+		if (v_addr < (unsigned long) tmp->addr + tmp->size) 
+			break;
+					
+	if (tmp) {
+		if (im_region_overlaps(v_addr, size, tmp))
+			return IM_REGION_OVERLAP;
+
+		*vm = tmp;
+		if (im_region_is_subset(v_addr, size, tmp))
+			return IM_REGION_SUBSET;
+		else if (v_addr == (unsigned long) tmp->addr && 
+		 	 size == tmp->size) 
+			return IM_REGION_EXISTS;
+	}
+
+	*vm = NULL;
+	return IM_REGION_UNUSED;
+}
+
+static struct vm_struct * split_im_region(unsigned long v_addr, 
+		unsigned long size, struct vm_struct *parent)
+{
+	struct vm_struct *vm1 = NULL;
+	struct vm_struct *vm2 = NULL;
+	struct vm_struct *new_vm = NULL;
+	
+	vm1 = (struct vm_struct *) kmalloc(sizeof(*vm1), GFP_KERNEL);
+	if (vm1	== NULL) {
+		printk(KERN_ERR "%s() out of memory\n", __FUNCTION__);
+		return NULL;
+	}
+
+	if (v_addr == (unsigned long) parent->addr) {
+	        /* Use existing parent vm_struct to represent child, allocate
+		 * new one for the remainder of parent range
+		 */
+		vm1->size = parent->size - size;
+		vm1->addr = (void *) (v_addr + size);
+		vm1->next = parent->next;
+
+		parent->size = size;
+		parent->next = vm1;
+		new_vm = parent;
+	} else if (v_addr + size == (unsigned long) parent->addr + 
+			parent->size) {
+		/* Allocate new vm_struct to represent child, use existing
+		 * parent one for remainder of parent range
+		 */
+		vm1->size = size;
+		vm1->addr = (void *) v_addr;
+		vm1->next = parent->next;
+		new_vm = vm1;
+
+		parent->size -= size;
+		parent->next = vm1;
+	} else {
+	        /* Allocate two new vm_structs for the new child and 
+		 * uppermost remainder, and use existing parent one for the
+		 * lower remainder of parent range
+		 */
+		vm2 = (struct vm_struct *) kmalloc(sizeof(*vm2), GFP_KERNEL);
+		if (vm2 == NULL) {
+			printk(KERN_ERR "%s() out of memory\n", __FUNCTION__);
+			kfree(vm1);
 			return NULL;
 		}
+
+		vm1->size = size;
+		vm1->addr = (void *) v_addr;
+		vm1->next = vm2;
+		new_vm = vm1;
+
+		vm2->size = ((unsigned long) parent->addr + parent->size) - 
+				(v_addr + size);
+		vm2->addr = (void *) v_addr + size;
+		vm2->next = parent->next;
+
+		parent->size = v_addr - (unsigned long) parent->addr;
+		parent->next = vm1;
 	}
+
+	return new_vm;
+}
+
+static struct vm_struct * __add_new_im_area(unsigned long req_addr, 
+					    unsigned long size)
+{
+	struct vm_struct **p, *tmp, *area;
+		
+	for (p = &imlist; (tmp = *p) ; p = &tmp->next) {
+		if (req_addr + size <= (unsigned long)tmp->addr)
+			break;
+	}
+	
+	area = (struct vm_struct *) kmalloc(sizeof(*area), GFP_KERNEL);
+	if (!area)
+		return NULL;
 	area->flags = 0;
-	area->addr = (void *)addr;
+	area->addr = (void *)req_addr;
 	area->size = size;
 	area->next = *p;
 	*p = area;
+
+	return area;
+}
+
+static struct vm_struct * __im_get_area(unsigned long req_addr, 
+					unsigned long size,
+					int criteria)
+{
+	struct vm_struct *tmp;
+	int status;
+
+	status = im_region_status(req_addr, size, &tmp);
+	if ((criteria & status) == 0) {
+		return NULL;
+	}
+	
+	switch (status) {
+	case IM_REGION_UNUSED:
+		tmp = __add_new_im_area(req_addr, size);
+		break;
+	case IM_REGION_SUBSET:
+		tmp = split_im_region(req_addr, size, tmp);
+		break;
+	case IM_REGION_EXISTS:
+		break;
+	default:
+		printk(KERN_ERR "%s() unexpected imalloc region status\n",
+				__FUNCTION__);
+		tmp = NULL;
+	}
+
+	return tmp;
+}
+
+struct vm_struct * im_get_free_area(unsigned long size)
+{
+	struct vm_struct *area;
+	unsigned long addr;
+	
+	write_lock(&imlist_lock);
+	if (get_free_im_addr(size, &addr)) {
+		printk(KERN_ERR "%s() cannot obtain addr for size 0x%lx\n",
+				__FUNCTION__, size);
+		area = NULL;
+		goto next_im_done;
+	}
+
+	area = __im_get_area(addr, size, IM_REGION_UNUSED);
+	if (area == NULL) {
+		printk(KERN_ERR 
+		       "%s() cannot obtain area for addr 0x%lx size 0x%lx\n",
+			__FUNCTION__, addr, size);
+	}
+next_im_done:
 	write_unlock(&imlist_lock);
 	return area;
 }
 
-void ifree(void * addr)
+struct vm_struct * im_get_area(unsigned long v_addr, unsigned long size,
+		int criteria)
+{
+	struct vm_struct *area;
+
+	write_lock(&imlist_lock);
+	area = __im_get_area(v_addr, size, criteria);
+	write_unlock(&imlist_lock);
+	return area;
+}
+
+unsigned long im_free(void * addr)
 {
 	struct vm_struct **p, *tmp;
+	unsigned long ret_size = 0;
   
 	if (!addr)
-		return;
+		return ret_size;
 	if ((PAGE_SIZE-1) & (unsigned long) addr) {
-		printk(KERN_ERR "Trying to ifree() bad address (%p)\n", addr);
-		return;
+		printk(KERN_ERR "Trying to %s bad address (%p)\n", __FUNCTION__,			addr);
+		return ret_size;
 	}
 	write_lock(&imlist_lock);
 	for (p = &imlist ; (tmp = *p) ; p = &tmp->next) {
 		if (tmp->addr == addr) {
+			ret_size = tmp->size;
 			*p = tmp->next;
 			kfree(tmp);
 			write_unlock(&imlist_lock);
-			return;
+			return ret_size;
 		}
 	}
 	write_unlock(&imlist_lock);
-	printk(KERN_ERR "Trying to ifree() nonexistent area (%p)\n", addr);
+	printk(KERN_ERR "Trying to %s nonexistent area (%p)\n", __FUNCTION__,
+			addr);
+	return ret_size;
 }
-
diff -purN linux-2.5/arch/ppc64/mm/init.c linuxppc64-2.5/arch/ppc64/mm/init.c
--- linux-2.5/arch/ppc64/mm/init.c	2003-09-19 06:55:11.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/mm/init.c	2003-12-17 04:51:16.000000000 +0000
@@ -67,6 +67,7 @@
 struct mmu_context_queue_t mmu_context_queue;
 int mem_init_done;
 unsigned long ioremap_bot = IMALLOC_BASE;
+static unsigned long phbs_io_bot = PHBS_IO_BASE;
 
 extern pgd_t swapper_pg_dir[];
 extern struct task_struct *current_set[NR_CPUS];
@@ -74,6 +75,9 @@ extern struct task_struct *current_set[N
 extern pgd_t ioremap_dir[];
 pgd_t * ioremap_pgd = (pgd_t *)&ioremap_dir;
 
+static void * __ioremap_com(unsigned long addr, unsigned long pa, 
+			    unsigned long ea, unsigned long size, 
+			    unsigned long flags);
 static void map_io_page(unsigned long va, unsigned long pa, int flags);
 
 unsigned long klimit = (unsigned long)_end;
@@ -90,6 +94,52 @@ unsigned long __max_memory;
  * include/asm-ppc64/tlb.h file -- tgall
  */
 DEFINE_PER_CPU(struct mmu_gather, mmu_gathers);
+DEFINE_PER_CPU(struct pte_freelist_batch *, pte_freelist_cur);
+unsigned long pte_freelist_forced_free;
+
+static void pte_free_smp_sync(void *arg)
+{
+	/* Do nothing, just ensure we sync with all CPUs */
+}
+
+/* This is only called when we are critically out of memory
+ * (and fail to get a page in pte_free_tlb).
+ */
+void pte_free_now(struct page *ptepage)
+{
+	pte_freelist_forced_free++;
+
+	smp_call_function(pte_free_smp_sync, NULL, 0, 1);
+
+	pte_free(ptepage);
+}
+
+static void pte_free_rcu_callback(void *arg)
+{
+	struct pte_freelist_batch *batch = arg;
+	unsigned int i;
+
+	for (i = 0; i < batch->index; i++)
+		pte_free(batch->pages[i]);
+	free_page((unsigned long)batch);
+}
+
+void pte_free_submit(struct pte_freelist_batch *batch)
+{
+	INIT_RCU_HEAD(&batch->rcu);
+	call_rcu(&batch->rcu, pte_free_rcu_callback, batch);
+}
+
+void pte_free_finish(void)
+{
+	/* This is safe as we are holding page_table_lock */
+	struct pte_freelist_batch **batchp = &__get_cpu_var(pte_freelist_cur);
+	
+	if (*batchp == NULL)
+		return;
+	pte_free_submit(*batchp);
+	*batchp = NULL;
+}
 
 void show_mem(void)
 {
@@ -133,12 +183,10 @@ ioremap(unsigned long addr, unsigned lon
 #endif
 }
 
-extern struct vm_struct * get_im_area( unsigned long size );
-
 void *
 __ioremap(unsigned long addr, unsigned long size, unsigned long flags)
 {
-	unsigned long pa, ea, i;
+	unsigned long pa, ea;
 
 	/*
 	 * Choose an address to map it to.
@@ -157,26 +205,163 @@ __ioremap(unsigned long addr, unsigned l
 
 	if (mem_init_done) {
 		struct vm_struct *area;
-		area = get_im_area(size);
-		if (area == 0)
+		area = im_get_free_area(size);
+		if (area == NULL)
 			return NULL;
 		ea = (unsigned long)(area->addr);
-	} 
-	else {
+	} else {
 		ea = ioremap_bot;
 		ioremap_bot += size;
 	}
 
-	if ((flags & _PAGE_PRESENT) == 0)
-		flags |= pgprot_val(PAGE_KERNEL);
-	if (flags & (_PAGE_NO_CACHE | _PAGE_WRITETHRU))
-		flags |= _PAGE_GUARDED;
+	return __ioremap_com(addr, pa, ea, size, flags);
+}
 
-	for (i = 0; i < size; i += PAGE_SIZE) {
-		map_io_page(ea+i, pa+i, flags);
+#define IS_PAGE_ALIGNED(_val) ((_val) == ((_val) & PAGE_MASK))
+
+int __ioremap_explicit(unsigned long pa, unsigned long ea,
+		       unsigned long size, unsigned long flags)
+{
+	struct vm_struct *area;
+	
+	/* For now, require page-aligned values for pa, ea, and size */
+	if (!IS_PAGE_ALIGNED(pa) || !IS_PAGE_ALIGNED(ea) ||
+	    !IS_PAGE_ALIGNED(size)) {
+		printk(KERN_ERR	"unaligned value in %s\n", __FUNCTION__);
+		return 1;
+	}
+	
+	if (!mem_init_done) {
+		/* Two things to consider in this case:
+		 * 1) No records will be kept (imalloc, etc) that the region
+		 *    has been remapped
+		 * 2) It won't be easy to iounmap() the region later (because
+		 *    of 1)
+		 */
+		;
+	} else {
+		area = im_get_area(ea, size, IM_REGION_UNUSED|IM_REGION_SUBSET);
+		if (area == NULL) {
+			printk(KERN_ERR "could not obtain imalloc area for ea 0x%lx\n", ea);
+			return 1;
+		}
+		if (ea != (unsigned long) area->addr) {
+			printk(KERN_ERR "unexpected addr return from im_get_area\n");
+			return 1;
+		}
+	}
+	
+	if (__ioremap_com(pa, pa, ea, size, flags) != (void *) ea) {
+		printk(KERN_ERR "__ioremap_com() returned unexpected addr\n");
+		return 1;
 	}
 
-	return (void *) (ea + (addr & ~PAGE_MASK));
+	return 0;
+}
+
+static void unmap_im_area_pte(pmd_t *pmd, unsigned long address,
+				  unsigned long size)
+{
+	unsigned long end;
+	pte_t *pte;
+
+	if (pmd_none(*pmd))
+		return;
+	if (pmd_bad(*pmd)) {
+		pmd_ERROR(*pmd);
+		pmd_clear(pmd);
+		return;
+	}
+
+	pte = pte_offset_kernel(pmd, address);
+	address &= ~PMD_MASK;
+	end = address + size;
+	if (end > PMD_SIZE)
+		end = PMD_SIZE;
+
+	do {
+		pte_t page;
+		page = ptep_get_and_clear(pte);
+		address += PAGE_SIZE;
+		pte++;
+		if (pte_none(page))
+			continue;
+		if (pte_present(page))
+			continue;
+		printk(KERN_CRIT "Whee.. Swapped out page in kernel page table\n");
+	} while (address < end);
+}
+
+static void unmap_im_area_pmd(pgd_t *dir, unsigned long address,
+				  unsigned long size)
+{
+	unsigned long end;
+	pmd_t *pmd;
+
+	if (pgd_none(*dir))
+		return;
+	if (pgd_bad(*dir)) {
+		pgd_ERROR(*dir);
+		pgd_clear(dir);
+		return;
+	}
+
+	pmd = pmd_offset(dir, address);
+	address &= ~PGDIR_MASK;
+	end = address + size;
+	if (end > PGDIR_SIZE)
+		end = PGDIR_SIZE;
+
+	do {
+		unmap_im_area_pte(pmd, address, end - address);
+		address = (address + PMD_SIZE) & PMD_MASK;
+		pmd++;
+	} while (address < end);
+}
+
+/*  
+ * Unmap an IO region and remove it from imalloc'd list.
+ * Access to IO memory should be serialized by driver.
+ * This code is modeled after vmalloc code - unmap_vm_area()
+ *
+ * XXX	what about calls before mem_init_done (ie python_countermeasures())	
+ */
+void pSeries_iounmap(void *addr)
+{
+	unsigned long address, start, end, size;
+	struct mm_struct *mm;
+	pgd_t *dir;
+
+	if (!mem_init_done) {
+		return;
+	}
+	
+	/* addr could be in EEH or IO region, map it to IO region regardless.
+	 */
+	addr = (void *) (IO_TOKEN_TO_ADDR(addr) & PAGE_MASK);
+	
+	if ((size = im_free(addr)) == 0) {
+		return;
+	}
+
+	address = (unsigned long)addr; 
+	start = address;
+	end = address + size;
+
+	mm = &ioremap_mm;
+	spin_lock(&mm->page_table_lock);
+
+	dir = pgd_offset_i(address);
+	flush_cache_all();
+	do {
+		unmap_im_area_pmd(dir, address, end - address);
+		address = (address + PGDIR_SIZE) & PGDIR_MASK;
+		dir++;
+	} while (address && (address < end));
+	__flush_tlb_range(mm, start, end);
+
+	spin_unlock(&mm->page_table_lock);
+	return;
 }
 
 void iounmap(void *addr) 
@@ -186,10 +371,52 @@ void iounmap(void *addr) 
 	return;
 #else
 	/* DRENG / PPPBBB todo */
-	return;
+	return pSeries_iounmap(addr);
 #endif
 }
 
+int iounmap_explicit(void *addr, unsigned long size)
+{
+	struct vm_struct *area;
+	
+	/* addr could be in EEH or IO region, map it to IO region regardless.
+	 */
+	addr = (void *) (IO_TOKEN_TO_ADDR(addr) & PAGE_MASK);
+
+	/* Verify that the region either exists or is a subset of an existing
+	 * region.  In the latter case, split the parent region to create 
+	 * the exact region 
+	 */
+	area = im_get_area((unsigned long) addr, size, 
+			    IM_REGION_EXISTS | IM_REGION_SUBSET);
+	if (area == NULL) {
+		printk(KERN_ERR "%s() cannot unmap nonexistant range 0x%lx\n",
+				__FUNCTION__, (unsigned long) addr);
+		return 1;
+	}
+
+	iounmap(area->addr);
+	return 0;
+}
+
+static void * __ioremap_com(unsigned long addr, unsigned long pa, 
+			    unsigned long ea, unsigned long size, 
+			    unsigned long flags)
+{
+	unsigned long i;
+	
+	if ((flags & _PAGE_PRESENT) == 0)
+		flags |= pgprot_val(PAGE_KERNEL);
+	if (flags & (_PAGE_NO_CACHE | _PAGE_WRITETHRU))
+		flags |= _PAGE_GUARDED;
+
+	for (i = 0; i < size; i += PAGE_SIZE) {
+		map_io_page(ea+i, pa+i, flags);
+	}
+
+	return (void *) (ea + (addr & ~PAGE_MASK));
+}
+
 /*
  * map_io_page currently only called by __ioremap
  * map_io_page adds an entry to the ioremap page table
@@ -727,6 +954,19 @@ void update_mmu_cache(struct vm_area_str
 		    0x300, local);
 }
 
+void * reserve_phb_iospace(unsigned long size)
+{
+	void *virt_addr;
+		
+	if (phbs_io_bot >= IMALLOC_BASE) 
+		panic("reserve_phb_iospace(): phb io space overflow\n");
+			
+	virt_addr = (void *) phbs_io_bot;
+	phbs_io_bot += size;
+
+	return virt_addr;
+}
+
 kmem_cache_t *zero_cache;
 
 static void zero_ctor(void *pte, kmem_cache_t *cache, unsigned long flags)
diff -purN linux-2.5/arch/ppc64/mm/numa.c linuxppc64-2.5/arch/ppc64/mm/numa.c
--- linux-2.5/arch/ppc64/mm/numa.c	2003-09-28 23:26:35.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/mm/numa.c	2003-10-14 12:26:47.000000000 +0000
@@ -46,29 +46,29 @@ static inline void map_cpu_to_node(int c
 
 static int __init parse_numa_properties(void)
 {
-	struct device_node *cpu;
-	struct device_node *memory;
+	struct device_node *cpu = NULL;
+	struct device_node *memory = NULL;
 	int *cpu_associativity;
 	int *memory_associativity;
 	int depth;
 	int max_domain = 0;
 
-	cpu = find_type_devices("cpu");
+	cpu = of_find_node_by_type(NULL, "cpu");
 	if (!cpu)
-		return -1;
+		goto err;
 
-	memory = find_type_devices("memory");
+	memory = of_find_node_by_type(NULL, "memory");
 	if (!memory)
-		return -1;
+		goto err;
 
 	cpu_associativity = (int *)get_property(cpu, "ibm,associativity", NULL);
 	if (!cpu_associativity)
-		return -1;
+		goto err;
 
 	memory_associativity = (int *)get_property(memory, "ibm,associativity",
 						   NULL);
 	if (!memory_associativity)
-		return -1;
+		goto err;
 
 	/* find common depth */
 	if (cpu_associativity[0] < memory_associativity[0])
@@ -76,7 +76,7 @@ static int __init parse_numa_properties(
 	else
 		depth = memory_associativity[0];
 
-	for (cpu = find_type_devices("cpu"); cpu; cpu = cpu->next) {
+	for (; cpu; cpu = of_find_node_by_type(cpu, "cpu")) {
 		int *tmp;
 		int cpu_nr, numa_domain;
 
@@ -106,8 +106,7 @@ static int __init parse_numa_properties(
 		map_cpu_to_node(cpu_nr, numa_domain);
 	}
 
-	for (memory = find_type_devices("memory"); memory;
-	     memory = memory->next) {
+	for (; memory; memory = of_find_node_by_type(memory, "memory")) {
 		int *tmp1, *tmp2;
 		unsigned long i;
 		unsigned long start = 0;
@@ -196,6 +195,10 @@ new_range:
 	numnodes = max_domain + 1;
 
 	return 0;
+err:
+	of_node_put(cpu);
+	of_node_put(memory);
+	return -1;
 }
 
 void setup_nonnuma(void)
diff -purN linux-2.5/drivers/block/Makefile linuxppc64-2.5/drivers/block/Makefile
--- linux-2.5/drivers/block/Makefile	2003-10-16 04:38:46.000000000 +0000
+++ linuxppc64-2.5/drivers/block/Makefile	2003-11-21 06:45:02.000000000 +0000
@@ -38,3 +38,5 @@ obj-$(CONFIG_BLK_DEV_DAC960)	+= DAC960.o
 obj-$(CONFIG_BLK_DEV_UMEM)	+= umem.o
 obj-$(CONFIG_BLK_DEV_NBD)	+= nbd.o
 obj-$(CONFIG_BLK_DEV_CRYPTOLOOP) += cryptoloop.o
+
+obj-$(CONFIG_VIODASD)		+= viodasd.o
diff -purN linux-2.5/drivers/block/viodasd.c linuxppc64-2.5/drivers/block/viodasd.c
--- linux-2.5/drivers/block/viodasd.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/block/viodasd.c	2003-12-17 05:56:09.000000000 +0000
@@ -0,0 +1,1306 @@
+/* -*- linux-c -*-
+ * viodasd.c
+ *  Authors: Dave Boutcher <boutcher@us.ibm.com>
+ *           Ryan Arnold <ryanarn@us.ibm.com>
+ *           Colin Devilbiss <devilbis@us.ibm.com>
+ *           Stephen Rothwell <sfr@au1.ibm.com>
+ *
+ * (C) Copyright 2000-2003 IBM Corporation
+ * 
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ * 
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ * 
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA 
+ ***************************************************************************
+ * This routine provides access to disk space (termed "DASD" in historical
+ * IBM terms) owned and managed by an OS/400 partition running on the
+ * same box as this Linux partition.
+ *
+ * All disk operations are performed by sending messages back and forth to 
+ * the OS/400 partition. 
+ * 
+ * This device driver can either use its own major number, or it can
+ * pretend to be an IDE drive (grep 'IDE[0-9]_MAJOR' ../../include/linux/major.h).
+ * This is controlled with a CONFIG option.  You can either call this an
+ * elegant solution to the fact that a lot of software doesn't recognize
+ * a new disk major number...or you can call this a really ugly hack.
+ * Your choice.
+ *
+ * Changelog:
+ *	2001-11-27	devilbis	Added first pass at complete
+ *					IDE emulation
+ *	2002-07-07      boutcher        Added randomness
+ */
+#include <linux/config.h>
+#include <linux/major.h>
+#include <linux/fs.h>
+#include <linux/blkpg.h>
+#include <linux/sched.h>
+#include <linux/timer.h>
+#include <asm/uaccess.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/blkdev.h>
+#include <linux/genhd.h>
+#include <linux/hdreg.h>
+#include <linux/fd.h>
+#include <linux/proc_fs.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/vmalloc.h>
+#include <linux/string.h>
+#include <linux/pci.h>
+#include <linux/root_dev.h>
+#include <linux/kdev_t.h>
+
+#include <asm/iSeries/HvTypes.h>
+#include <asm/iSeries/HvLpEvent.h>
+#include <asm/iSeries/HvLpConfig.h>
+#include <asm/iSeries/vio.h>
+#include <asm/iSeries/iSeries_proc.h>
+
+MODULE_DESCRIPTION("iSeries Virtual DASD");
+MODULE_AUTHOR("Dave Boutcher");
+MODULE_LICENSE("GPL");
+
+/*
+ * Decide if we are using our own major or pretending to be an IDE drive
+ *
+ * If we are using our own major, we only support 7 partitions per physical
+ * disk....so with minor numbers 0-255 we get a maximum of 32 disks.  If we
+ * are emulating IDE, we get 63 partitions per disk, with a maximum of 4
+ * disks per major, but common practice is to place only 2 devices in /dev
+ * for each IDE major, for a total of 20 (since there are 10 IDE majors).
+ */
+
+#ifdef CONFIG_VIODASD_IDE
+static const int major_table[] = {
+	IDE0_MAJOR,
+	IDE1_MAJOR,
+	IDE2_MAJOR,
+	IDE3_MAJOR,
+	IDE4_MAJOR,
+	IDE5_MAJOR,
+	IDE6_MAJOR,
+	IDE7_MAJOR,
+	IDE8_MAJOR,
+	IDE9_MAJOR,
+};
+
+enum {
+	DEV_PER_MAJOR = 2,
+	PARTITION_SHIFT = 6,
+};
+
+static int major_to_index(int major)
+{
+	switch(major) {
+	case IDE0_MAJOR: return 0;
+	case IDE1_MAJOR: return 1;
+	case IDE2_MAJOR: return 2;
+	case IDE3_MAJOR: return 3;
+	case IDE4_MAJOR: return 4;
+	case IDE5_MAJOR: return 5;
+	case IDE6_MAJOR: return 6;
+	case IDE7_MAJOR: return 7;
+	case IDE8_MAJOR: return 8;
+	case IDE9_MAJOR: return 9;
+	default:
+		return -1;
+	}
+}
+
+#define VIOD_DEVICE_NAME	"ide"
+#define VIOD_GENHD_NAME		"hd"
+
+#else	/* !CONFIG_VIODASD_IDE */
+
+static const int major_table[] = {
+	VIODASD_MAJOR,
+};
+
+enum {
+	DEV_PER_MAJOR = 32,
+	PARTITION_SHIFT = 3,
+};
+
+static inline int major_to_index(int major)
+{
+	if (major != VIODASD_MAJOR)
+		return -1;
+	return 0;
+}
+
+#define VIOD_DEVICE_NAME	"viod"
+#ifdef CONFIG_DEVFS_FS
+#define VIOD_GENHD_NAME		"viod"
+#else
+#define VIOD_GENHD_NAME		"iseries/vd"
+#endif
+
+#endif	/* CONFIG_VIODASD_IDE */
+
+#define VIODASD_VERS	"1.60"
+
+#define VIOD_KERN_WARNING	KERN_WARNING_VIO VIOD_DEVICE_NAME ": "
+#define VIOD_KERN_INFO		KERN_INFO_VIO VIOD_DEVICE_NAME ": "
+
+enum {
+	NUM_MAJORS = sizeof(major_table) / sizeof(major_table[0]),
+	MAX_DISKNO = DEV_PER_MAJOR * NUM_MAJORS,
+	MAX_DISK_NAME = 16, /* maximum length of a gendisk->name */
+};
+
+static volatile int	viodasd_max_disk = MAX_DISKNO - 1;
+static spinlock_t	viodasd_spinlock = SPIN_LOCK_UNLOCKED;
+
+static inline int devt_to_diskno(dev_t dev)
+{
+	return major_to_index(MAJOR(dev)) * DEV_PER_MAJOR +
+	    (MINOR(dev) >> PARTITION_SHIFT);
+}
+
+#define VIOMAXREQ		16
+#define VIOMAXBLOCKDMA		12
+
+#define DEVICE_NO(cell)	((struct viodasd_device *)(cell) - &viodasd_devices[0])
+
+extern struct pci_dev *iSeries_vio_dev;
+
+struct openData {
+	u64 mDiskLen;
+	u16 mMaxDisks;
+	u16 mCylinders;
+	u16 mTracks;
+	u16 mSectors;
+	u16 mBytesPerSector;
+};
+
+struct rwData {			// Used during rw
+	u64 mOffset;
+	struct {
+		u32 mToken;
+		u32 reserved;
+		u64 mLen;
+	} dmaInfo[VIOMAXBLOCKDMA];
+};
+
+struct vioblocklpevent {
+	struct HvLpEvent event;
+	u32 mReserved1;
+	u16 mVersion;
+	u16 mSubTypeRc;
+	u16 mDisk;
+	u16 mFlags;
+	union {
+		struct openData openData;
+		struct rwData rwData;
+		u64 changed;
+	} u;
+};
+
+#define vioblockflags_ro   0x0001
+
+enum vioblocksubtype {
+	vioblockopen = 0x0001,
+	vioblockclose = 0x0002,
+	vioblockread = 0x0003,
+	vioblockwrite = 0x0004,
+	vioblockflush = 0x0005,
+	vioblockcheck = 0x0007
+};
+
+struct viodasd_waitevent {
+	struct semaphore *sem;
+	int rc;
+	union {
+		int changed;	/* Used only for check_change */
+		u16 subRC;
+	} data;
+};
+
+static const struct vio_error_entry viodasd_err_table[] = {
+	{ 0x0201, EINVAL, "Invalid Range" },
+	{ 0x0202, EINVAL, "Invalid Token" },
+	{ 0x0203, EIO, "DMA Error" },
+	{ 0x0204, EIO, "Use Error" },
+	{ 0x0205, EIO, "Release Error" },
+	{ 0x0206, EINVAL, "Invalid Disk" },
+	{ 0x0207, EBUSY, "Cant Lock" },
+	{ 0x0208, EIO, "Already Locked" },
+	{ 0x0209, EIO, "Already Unlocked" },
+	{ 0x020A, EIO, "Invalid Arg" },
+	{ 0x020B, EIO, "Bad IFS File" },
+	{ 0x020C, EROFS, "Read Only Device" },
+	{ 0x02FF, EIO, "Internal Error" },
+	{ 0x0000, 0, NULL },
+};
+
+/*
+ * Figure out the biggest I/O request (in sectors) we can accept
+ */
+#define VIODASD_MAXSECTORS (4096 / 512 * VIOMAXBLOCKDMA)
+
+/*
+ * Keep some statistics on what's happening for the PROC file system
+ */
+static struct {
+	long tot;
+	long nobh;
+	long ntce[VIOMAXBLOCKDMA];
+} viod_stats[MAX_DISKNO][2];
+
+/*
+ * Number of disk I/O requests we've sent to OS/400
+ */
+static int num_req_outstanding;
+
+/*
+ * This is our internal structure for keeping track of disk devices
+ */
+struct viodasd_device {
+	int			useCount;
+	u16			cylinders;
+	u16			tracks;
+	u16			sectors;
+	u16			bytesPerSector;
+	u64			size;
+	int			readOnly;
+	struct request_queue	*queue;
+	spinlock_t		q_lock;
+	struct gendisk		*disk;
+} viodasd_devices[MAX_DISKNO];
+
+static struct hd_struct *devt_to_partition(dev_t dev)
+{
+	return viodasd_devices[devt_to_diskno(dev)].disk->
+		part[MINOR(dev) & ((1 << PARTITION_SHIFT) - 1)];
+}
+
+/*
+ * Handle reads from the proc file system
+ */
+static int proc_read(char *buf, char **start, off_t offset,
+		     int blen, int *eof, void *data)
+{
+	int len = 0;
+	int i;
+	int j;
+
+#if defined(MODULE)
+	len += sprintf(buf + len,
+		    "viod Module opened %d times.  Major number %d\n",
+		    MOD_IN_USE, major_table[0]);
+#endif
+	len += sprintf(buf + len, "viod %d possible devices\n", MAX_DISKNO);
+
+	for (i = 0; i < 16; i++) {
+		if (viod_stats[i][0].tot || viod_stats[i][1].tot) {
+			len += sprintf(buf + len,
+				    "DISK %2.2d: rd %-10.10ld wr %-10.10ld (no buffer list rd %-10.10ld wr %-10.10ld\n",
+				    i, viod_stats[i][0].tot,
+				    viod_stats[i][1].tot,
+				    viod_stats[i][0].nobh,
+				    viod_stats[i][1].nobh);
+
+			len += sprintf(buf + len, "rd DMA: ");
+			for (j = 0; j < VIOMAXBLOCKDMA; j++)
+				len += sprintf(buf + len, " [%2.2d] %ld", j,
+					       viod_stats[i][0].ntce[j]);
+
+			len += sprintf(buf + len, "\nwr DMA: ");
+			for (j = 0; j < VIOMAXBLOCKDMA; j++)
+				len += sprintf(buf + len, " [%2.2d] %ld", j,
+					       viod_stats[i][1].ntce[j]);
+			len += sprintf(buf + len, "\n");
+		}
+	}
+
+	*eof = 1;
+	return len;
+}
+
+/*
+ * Handle writes to our proc file system
+ */
+static int proc_write(struct file *file, const char *buffer,
+		      unsigned long count, void *data)
+{
+	return count;
+}
+
+/*
+ * setup our proc file system entries
+ */
+void viodasd_proc_init(struct proc_dir_entry *iSeries_proc)
+{
+	struct proc_dir_entry *ent;
+
+	ent = create_proc_entry("viodasd", S_IFREG | S_IRUSR, iSeries_proc);
+	if (!ent)
+		return;
+	ent->owner = THIS_MODULE;
+	ent->nlink = 1;
+	ent->data = NULL;
+	ent->read_proc = proc_read;
+	ent->write_proc = proc_write;
+}
+
+/*
+ * clean up our proc file system entries
+ */
+void viodasd_proc_delete(struct proc_dir_entry *iSeries_proc)
+{
+	remove_proc_entry("viodasd", iSeries_proc);
+}
+
+/*
+ * End a request
+ */
+static void viodasd_end_request(struct request *req, int uptodate,
+		int num_sectors)
+{
+	if (!uptodate)
+		num_sectors = req->current_nr_sectors;
+	if (end_that_request_first(req, uptodate, num_sectors))
+		return;
+        add_disk_randomness(req->rq_disk);
+	end_that_request_last(req);
+}
+
+/*
+ * This rebuilds the partition information for a single disk device
+ */
+static int viodasd_revalidate(struct gendisk *gendisk)
+{
+	struct viodasd_device *device =
+		(struct viodasd_device *)gendisk->private_data;
+
+	set_capacity(gendisk, device->size >> 9);
+	return 0;
+}
+
+static inline u16 access_flags(mode_t mode)
+{
+	return (mode & FMODE_WRITE) ? 0 : vioblockflags_ro;
+}
+
+static void internal_register_disk(int diskno)
+{
+	static int registered[MAX_DISKNO];
+	struct gendisk *gendisk = viodasd_devices[diskno].disk;
+	int i;
+
+	if (registered[diskno])
+		return;
+	registered[diskno] = 1;
+
+	printk(VIOD_KERN_INFO
+	       "Disk %2.2d size %dM, sectors %d, heads %d, cylinders %d, sectsize %d\n",
+               diskno, (int)viodasd_devices[diskno].size >> 20,
+	       (int)viodasd_devices[diskno].sectors,
+	       (int)viodasd_devices[diskno].tracks,
+	       (int)viodasd_devices[diskno].cylinders,
+	       (int)viodasd_devices[diskno].bytesPerSector);
+
+	for (i = 1; i < (1 << PARTITION_SHIFT); ++i) {
+		struct hd_struct *partition = gendisk->part[i - 1];
+		if (partition && partition->nr_sects)
+			printk(VIOD_KERN_INFO
+			       "Disk %2.2d partition %2.2d start sector %ld, # sector %ld\n",
+			       diskno, i, partition->start_sect,
+			       partition->nr_sects);
+	}
+}
+
+/*
+ * This is the actual open code.  It gets called from the external
+ * open entry point, as well as from the init code when we're figuring
+ * out what disks we have
+ */
+static int internal_open(int device_no, u16 flags)
+{
+	struct gendisk *gendisk;
+	HvLpEvent_Rc hvrc;
+	/* This semaphore is raised in the interrupt handler */
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	struct viodasd_waitevent we = { .sem = &Semaphore };
+
+	/* Check that we are dealing with a valid hosting partition */
+	if (viopath_hostLp == HvLpIndexInvalid) {
+		printk(KERN_WARNING_VIO "Invalid hosting partition\n");
+		return -EIO;
+	}
+
+	/* Send the open event to OS/400 */
+	hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+			HvLpEvent_Type_VirtualIo,
+			viomajorsubtype_blockio | vioblockopen,
+			HvLpEvent_AckInd_DoAck, HvLpEvent_AckType_ImmediateAck,
+			viopath_sourceinst(viopath_hostLp),
+			viopath_targetinst(viopath_hostLp),
+			(u64)(unsigned long)&we, VIOVERSION << 16,
+			((u64)device_no << 48) | ((u64)flags << 32), 0, 0, 0);
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO "bad rc on signalLpEvent %d\n",
+				(int)hvrc);
+		return -EIO;
+	}
+
+	/* Wait for the interrupt handler to get the response */
+	down(&Semaphore);
+
+	/* Check the return code */
+	if (we.rc != 0) {
+		const struct vio_error_entry *err =
+			vio_lookup_rc(viodasd_err_table, we.data.subRC);
+		/*
+		 * Temporary patch to quiet down the viodasd when drivers
+		 * are probing for drives, especially lvm.  Collin is aware
+		 * and is working on this.
+		 */
+#if 0
+		printk(KERN_WARNING_VIO "bad rc opening disk: %d:0x%04x (%s)\n",
+				(int)we.rc, we.data.subRC, err->msg);
+#endif
+		return -err->errno;
+	}
+	
+	/* Bump the use count */
+	viodasd_devices[device_no].useCount++;
+
+	/* When we are probing, we have no gendisk structure yet ... */
+	gendisk = viodasd_devices[device_no].disk;
+	if (gendisk == NULL)
+		return 0;
+
+	/*
+	 * If this is the first open of this device, update the device
+	 * information.  If this is NOT the first open, assume that it
+	 * isn't changing
+	 */
+	if (viodasd_devices[device_no].useCount == 1) {
+		if (viodasd_devices[device_no].size > 0)
+                        set_capacity(gendisk,
+					viodasd_devices[device_no].size >> 9);
+	} else if (get_capacity(gendisk) !=
+			viodasd_devices[device_no].size >> 9)
+		/*
+		 * If the size of the device changed, weird things
+		 * are happening!
+		 */
+		printk(KERN_WARNING_VIO
+		       "disk size change (%d to %d sectors) for device %d\n",
+		       (int)get_capacity(gendisk),
+		       (int)viodasd_devices[device_no].size >> 9, device_no);
+
+	internal_register_disk(device_no);
+
+	return 0;
+}
+
+/*
+ * This is the actual release code.  It gets called from the external
+ * release entry point, as well as from the init code when we're figuring
+ * out what disks we have.
+ */
+static int internal_release(int device_no, u16 flags)
+{
+	HvLpEvent_Rc hvrc;
+
+	/* Send the event to OS/400.  We DON'T expect a response */
+	hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+			HvLpEvent_Type_VirtualIo,
+			viomajorsubtype_blockio | vioblockclose,
+			HvLpEvent_AckInd_NoAck, HvLpEvent_AckType_ImmediateAck,
+			viopath_sourceinst(viopath_hostLp),
+			viopath_targetinst(viopath_hostLp), 0,
+			VIOVERSION << 16,
+			((u64)device_no << 48) | ((u64)flags << 32), 0, 0, 0);
+	viodasd_devices[device_no].useCount--;
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO
+		       "bad rc sending event to OS/400 %d\n", (int)hvrc);
+		return -EIO;
+	}
+	return 0;
+}
+
+
+static int inode_to_device_no(struct inode *ino, const char *func)
+{
+	int device_no;
+
+	/* Do a bunch of sanity checks */
+	if (!ino) {
+		printk(KERN_WARNING_VIO "no inode provided in %s\n", func);
+		return -ENODEV;
+	}
+
+	if (major_to_index(MAJOR(ino->i_rdev)) < 0) {
+		printk(KERN_WARNING_VIO
+		       "Weird error...wrong major number in %s\n", func);
+		return -ENODEV;
+	}
+
+	device_no = devt_to_diskno(ino->i_rdev);
+	if ((device_no > MAX_DISKNO) || (device_no < 0)) {
+		printk(KERN_WARNING_VIO
+		       "Invalid disk number %d in %s\n", device_no, func);
+		return -ENODEV;
+	}
+	return device_no;
+}
+
+/*
+ * External open entry point.
+ */
+static int viodasd_open(struct inode *ino, struct file *fil)
+{
+	int device_no;
+	int i;
+	int old_max_disk = viodasd_max_disk;
+
+	device_no = inode_to_device_no(ino, "open");
+	if (device_no < 0)
+		return device_no;
+
+	/* Call the actual open code */
+	if (internal_open(device_no, access_flags(fil ? fil->f_mode : 0)) != 0)
+		return -EIO;
+
+	/* For each new disk: */
+	/* update the disk's geometry via internal_open and register it */
+	for (i = old_max_disk + 1; i <= viodasd_max_disk; ++i) {
+		internal_open(i, vioblockflags_ro);
+		internal_release(i, vioblockflags_ro);
+	}
+	return 0;
+}
+
+/*
+ * External release entry point.
+ */
+static int viodasd_release(struct inode *ino, struct file *fil)
+{
+	int device_no;
+
+	device_no = inode_to_device_no(ino, "open");
+	if (device_no < 0)
+		return device_no;
+
+	/* Call the actual release code */
+	internal_release(device_no, access_flags(fil ? fil->f_mode : 0));
+
+	return 0;
+}
+
+/* External ioctl entry point.
+ */
+static int viodasd_ioctl(struct inode *ino, struct file *fil,
+			 unsigned int cmd, unsigned long arg)
+{
+	int device_no;
+	int err;
+	struct hd_struct *partition;
+
+	/* Sanity checks */
+	if (!ino) {
+		printk(KERN_WARNING_VIO "no inode provided in ioctl\n");
+		return -ENODEV;
+	}
+
+	if (major_to_index(MAJOR(ino->i_rdev)) < 0) {
+		printk(KERN_WARNING_VIO
+		       "Weird error...wrong major number on ioctl\n");
+		return -ENODEV;
+	}
+
+	device_no = devt_to_diskno(ino->i_rdev);
+	if (device_no > viodasd_max_disk) {
+		printk(KERN_WARNING_VIO
+		       "Invalid device number %d in ioctl\n", device_no);
+		return -ENODEV;
+	}
+
+	partition = devt_to_partition(ino->i_rdev);
+
+	switch (cmd) {
+	case HDIO_GETGEO:
+	{
+		unsigned char sectors;
+		unsigned char heads;
+		unsigned short cylinders;
+
+		struct hd_geometry *geo = (struct hd_geometry *)arg;
+		if (geo == NULL)
+			return -EINVAL;
+		err = verify_area(VERIFY_WRITE, geo, sizeof(*geo));
+		if (err)
+			return err;
+		sectors = viodasd_devices[device_no].sectors;
+		if (sectors == 0)
+			sectors = 32;
+		heads = viodasd_devices[device_no].tracks;
+		if (heads == 0)
+			heads = 64;
+		cylinders = viodasd_devices[device_no].cylinders;
+		if ((cylinders == 0) && partition)
+			cylinders = partition->nr_sects / (sectors * heads);
+		put_user(sectors, &geo->sectors);
+		put_user(heads, &geo->heads);
+		put_user(cylinders, &geo->cylinders);
+
+		if (partition)
+			put_user(partition->start_sect, &geo->start);
+		else
+			put_user(0, &geo->start);
+
+		return 0;
+	}
+
+
+#define PRTIOC(x)	\
+	case x: printk(KERN_WARNING_VIO "got unsupported FD ioctl " #x "\n"); \
+                          return -EINVAL;
+	PRTIOC(FDCLRPRM);
+	PRTIOC(FDSETPRM);
+	PRTIOC(FDDEFPRM);
+	PRTIOC(FDGETPRM);
+	PRTIOC(FDMSGON);
+	PRTIOC(FDMSGOFF);
+	PRTIOC(FDFMTBEG);
+	PRTIOC(FDFMTTRK);
+	PRTIOC(FDFMTEND);
+	PRTIOC(FDSETEMSGTRESH);
+	PRTIOC(FDSETMAXERRS);
+	PRTIOC(FDGETMAXERRS);
+	PRTIOC(FDGETDRVTYP);
+	PRTIOC(FDSETDRVPRM);
+	PRTIOC(FDGETDRVPRM);
+	PRTIOC(FDGETDRVSTAT);
+	PRTIOC(FDPOLLDRVSTAT);
+	PRTIOC(FDRESET);
+	PRTIOC(FDGETFDCSTAT);
+	PRTIOC(FDWERRORCLR);
+	PRTIOC(FDWERRORGET);
+	PRTIOC(FDRAWCMD);
+	PRTIOC(FDEJECT);
+	PRTIOC(FDTWADDLE);
+	}
+
+	return -EINVAL;
+}
+
+/*
+ * Send an actual I/O request to OS/400
+ */
+static int send_request(struct request *req)
+{
+	u64 start;
+	int direction;
+	int nsg;
+	u16 viocmd;
+	HvLpEvent_Rc hvrc;
+	struct vioblocklpevent *bevent;
+	struct scatterlist sg[VIOMAXBLOCKDMA];
+	int sgindex;
+	int statindex;
+        int device_no = DEVICE_NO(req->rq_disk->private_data);
+	unsigned long flags;
+
+	start = (u64)req->sector << 9;
+
+	/* More paranoia checks */
+	if ((req->sector + req->nr_sectors) > get_capacity(req->rq_disk)) {
+		printk(KERN_WARNING_VIO "Invalid request offset & length\n");
+		printk(KERN_WARNING_VIO
+				"req->sector: %ld, req->nr_sectors: %ld\n",
+				req->sector, req->nr_sectors);
+		printk(KERN_WARNING_VIO "device: %s\n",
+				req->rq_disk->disk_name);
+		return -1;
+	}
+
+	if (rq_data_dir(req) == READ) {
+		direction = PCI_DMA_FROMDEVICE;
+		viocmd = viomajorsubtype_blockio | vioblockread;
+		statindex = 0;
+	} else {
+		direction = PCI_DMA_TODEVICE;
+		viocmd = viomajorsubtype_blockio | vioblockwrite;
+		statindex = 1;
+	}
+
+	/* Update totals */
+	viod_stats[device_no][statindex].tot++;
+
+	/* Now build the scatter-gather list */
+	nsg = blk_rq_map_sg(req->q, req, sg);
+	nsg = pci_map_sg(iSeries_vio_dev, sg, nsg, direction);
+	/* Update stats */
+	viod_stats[device_no][statindex].ntce[nsg]++;
+
+	spin_lock_irqsave(&viodasd_spinlock, flags);
+	num_req_outstanding++;
+
+	/* This optimization handles a single DMA block */
+	if (nsg == 1)
+		hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+				HvLpEvent_Type_VirtualIo,
+				viomajorsubtype_blockio | viocmd,
+				HvLpEvent_AckInd_DoAck,
+				HvLpEvent_AckType_ImmediateAck,
+				viopath_sourceinst(viopath_hostLp),
+				viopath_targetinst(viopath_hostLp),
+				(u64)(unsigned long)req, VIOVERSION << 16,
+				((u64)device_no << 48), start,
+				((u64)sg[0].dma_address) << 32,
+				sg[0].dma_length);
+	else {
+		bevent = (struct vioblocklpevent *)
+			vio_get_event_buffer(viomajorsubtype_blockio);
+		if (bevent == NULL) {
+			spin_unlock_irqrestore(&viodasd_spinlock, flags);
+			printk(KERN_WARNING_VIO
+			       "error allocating disk event buffer\n");
+			return -1;
+		}
+
+		/*
+		 * Now build up the actual request.  Note that we store
+		 * the pointer to the request in the correlation
+		 * token so we can match the response up later
+		 */
+		memset(bevent, 0, sizeof(struct vioblocklpevent));
+		bevent->event.xFlags.xValid = 1;
+		bevent->event.xFlags.xFunction = HvLpEvent_Function_Int;
+		bevent->event.xFlags.xAckInd = HvLpEvent_AckInd_DoAck;
+		bevent->event.xFlags.xAckType = HvLpEvent_AckType_ImmediateAck;
+		bevent->event.xType = HvLpEvent_Type_VirtualIo;
+		bevent->event.xSubtype = viocmd;
+		bevent->event.xSourceLp = HvLpConfig_getLpIndex();
+		bevent->event.xTargetLp = viopath_hostLp;
+		bevent->event.xSizeMinus1 =
+			offsetof(struct vioblocklpevent, u.rwData.dmaInfo) +
+			(sizeof(bevent->u.rwData.dmaInfo[0]) * nsg) - 1;
+		bevent->event.xSourceInstanceId =
+			viopath_sourceinst(viopath_hostLp);
+		bevent->event.xTargetInstanceId =
+			viopath_targetinst(viopath_hostLp);
+		bevent->event.xCorrelationToken = (u64)req;
+		bevent->mVersion = VIOVERSION;
+		bevent->mDisk = device_no;
+		bevent->u.rwData.mOffset = start;
+
+		/*
+		 * Copy just the dma information from the sg list
+		 * into the request
+		 */
+		for (sgindex = 0; sgindex < nsg; sgindex++) {
+			bevent->u.rwData.dmaInfo[sgindex].mToken =
+				sg[sgindex].dma_address;
+			bevent->u.rwData.dmaInfo[sgindex].mLen =
+				sg[sgindex].dma_length;
+		}
+
+		/* Send the request */
+		hvrc = HvCallEvent_signalLpEvent(&bevent->event);
+		vio_free_event_buffer(viomajorsubtype_blockio, bevent);
+	}
+
+	if (hvrc != HvLpEvent_Rc_Good) {
+		num_req_outstanding--;
+		spin_unlock_irqrestore(&viodasd_spinlock, flags);
+		printk(KERN_WARNING_VIO
+		       "error sending disk event to OS/400 (rc %d)\n",
+		       (int)hvrc);
+		return -1;
+	}
+	spin_unlock_irqrestore(&viodasd_spinlock, flags);
+	return 0;
+}
+
+/*
+ * This is the external request processing routine
+ */
+static void do_viodasd_request(request_queue_t *q)
+{
+	for (;;) {
+		struct request *req;
+		struct gendisk *gendisk;
+
+		if ((req = elv_next_request(q)) == NULL)
+			return;
+		/* check that request contains a valid command */
+		if (!blk_fs_request(req)) {
+			viodasd_end_request(req, 0, 0);
+			continue;
+		}
+
+		gendisk = req->rq_disk;
+		if (major_to_index(gendisk->major) < 0)
+			panic(VIOD_DEVICE_NAME ": request list destroyed");
+
+		if (get_capacity(gendisk) == 0) {
+			printk(KERN_WARNING_VIO
+					"Ouch! get_capacity(gendisk) is 0\n");
+			viodasd_end_request(req, 0, 0);
+			continue;
+		}
+
+		/* If the queue is plugged, don't dequeue anything right now */
+		if (blk_queue_plugged(q))
+			return;
+
+		/*
+		 * If we already have the maximum number of requests
+		 * outstanding to OS/400 just bail out. We'll come
+		 * back later.
+		 */
+		if (num_req_outstanding >= VIOMAXREQ)
+			return;
+
+		/* dequeue the current request from the queue */
+		blkdev_dequeue_request(req);
+
+		spin_unlock_irq(q->queue_lock);
+
+		/* Try sending the request */
+		if (send_request(req) != 0)
+			viodasd_end_request(req, 0, 0);
+
+		spin_lock_irq(q->queue_lock);
+	}
+}
+
+/*
+ * Check for changed disks
+ */
+static int viodasd_check_change(struct gendisk *gendisk)
+{
+	struct viodasd_waitevent we;
+	HvLpEvent_Rc hvrc;
+	int device_no = major_to_index(gendisk->major);
+
+	/* This semaphore is raised in the interrupt handler */
+	DECLARE_MUTEX_LOCKED(Semaphore);
+
+	/* Check that we are dealing with a valid hosting partition */
+	if (viopath_hostLp == HvLpIndexInvalid) {
+		printk(KERN_WARNING_VIO "Invalid hosting partition\n");
+		return -EIO;
+	}
+
+	we.sem = &Semaphore;
+
+	/* Send the open event to OS/400 */
+	hvrc = HvCallEvent_signalLpEventFast(viopath_hostLp,
+			HvLpEvent_Type_VirtualIo,
+			viomajorsubtype_blockio | vioblockcheck,
+			HvLpEvent_AckInd_DoAck, HvLpEvent_AckType_ImmediateAck,
+			viopath_sourceinst(viopath_hostLp),
+			viopath_targetinst(viopath_hostLp),
+			(u64)(unsigned long)&we, VIOVERSION << 16,
+			((u64)device_no << 48), 0, 0, 0);
+	if (hvrc != 0) {
+		printk(KERN_WARNING_VIO "bad rc on signalLpEvent %d\n",
+				(int)hvrc);
+		return -EIO;
+	}
+
+	/* Wait for the interrupt handler to get the response */
+	down(&Semaphore);
+
+	/* Check the return code.  If bad, assume no change */
+	if (we.rc != 0) {
+		printk(KERN_WARNING_VIO
+				"bad rc %d on check_change. Assuming no change\n",
+				(int)we.rc);
+		return 0;
+	}
+
+	return we.data.changed;
+}
+
+/*
+ * Our file operations table
+ */
+static struct block_device_operations viodasd_fops = {
+	.owner = THIS_MODULE,
+	.open = viodasd_open,
+	.release = viodasd_release,
+	.ioctl = viodasd_ioctl,
+	.media_changed = viodasd_check_change,
+	.revalidate_disk = viodasd_revalidate
+};
+
+/* returns the total number of scatterlist elements converted */
+static int block_event_to_scatterlist(const struct vioblocklpevent *bevent,
+		struct scatterlist *sg, int *total_len)
+{
+	int i, numsg;
+	const struct rwData *rwData = &bevent->u.rwData;
+	static const int offset =
+		offsetof(struct vioblocklpevent, u.rwData.dmaInfo);
+	static const int element_size = sizeof(rwData->dmaInfo[0]);
+
+	numsg = ((bevent->event.xSizeMinus1 + 1) - offset) / element_size;
+	if (numsg > VIOMAXBLOCKDMA)
+		numsg = VIOMAXBLOCKDMA;
+
+	*total_len = 0;
+	memset(sg, 0, sizeof(sg[0]) * VIOMAXBLOCKDMA);
+
+	for (i = 0; (i < numsg) && (rwData->dmaInfo[i].mLen > 0); ++i) {
+		sg[i].dma_address = rwData->dmaInfo[i].mToken;
+		sg[i].dma_length = rwData->dmaInfo[i].mLen;
+		*total_len += rwData->dmaInfo[i].mLen;
+	}
+	return i;
+}
+
+/*
+ * Restart all queues, starting with the one _after_ the disk given,
+ * thus reducing the chance of starvation of higher numbered disks.
+ */
+static void viodasd_restart_all_queues_starting_from(int first_index)
+{
+	int i;
+	request_queue_t *q;
+
+	for (i = first_index + 1; i < MAX_DISKNO; ++i) {
+		q = viodasd_devices[i].queue;
+		if (q)
+			blk_run_queue(q);
+	}
+	for (i = 0; i <= first_index; ++i) {
+		q = viodasd_devices[i].queue;
+		if (q)
+			blk_run_queue(q);
+	}
+}
+
+/*
+ * For read and write requests, decrement the number of outstanding requests,
+ * Free the DMA buffers we allocated.
+ */
+static int viodasd_handleReadWrite(struct vioblocklpevent *bevent)
+{
+	int num_sg, num_sect, pci_direction, total_len;
+	struct request *req;
+	struct scatterlist sg[VIOMAXBLOCKDMA];
+	struct HvLpEvent *event = &bevent->event;
+	unsigned long irq_flags;
+	int device_no;
+
+	num_sg = block_event_to_scatterlist(bevent, sg, &total_len);
+	num_sect = total_len >> 9;
+	if (event->xSubtype == (viomajorsubtype_blockio | vioblockread))
+		pci_direction = PCI_DMA_FROMDEVICE;
+	else
+		pci_direction = PCI_DMA_TODEVICE;
+	pci_unmap_sg(iSeries_vio_dev, sg, num_sg, pci_direction);
+
+	/*
+	 * Since this is running in interrupt mode, we need to make sure
+	 * we're not stepping on any global I/O operations
+	 */
+	spin_lock_irqsave(&viodasd_spinlock, irq_flags);
+	num_req_outstanding--;
+	spin_unlock_irqrestore(&viodasd_spinlock, irq_flags);
+
+	req = (struct request *)bevent->event.xCorrelationToken;
+	device_no = DEVICE_NO(req->rq_disk->private_data);
+
+	if (event->xRc != HvLpEvent_Rc_Good) {
+		const struct vio_error_entry *err;
+		err = vio_lookup_rc(viodasd_err_table, bevent->mSubTypeRc);
+		printk(KERN_WARNING_VIO "read/write error %d:0x%04x (%s)\n",
+				event->xRc, bevent->mSubTypeRc, err->msg);
+		viodasd_end_request(req, 0, 0);
+	} else
+		viodasd_end_request(req, 1, num_sect);
+
+	/* Finally, try to get more requests off of this device's queue */
+	viodasd_restart_all_queues_starting_from(device_no);
+
+	return 0;
+}
+
+/* This routine handles incoming block LP events */
+static void vioHandleBlockEvent(struct HvLpEvent *event)
+{
+	struct vioblocklpevent *bevent = (struct vioblocklpevent *)event;
+	struct viodasd_waitevent *pwe;
+
+	if (event == NULL)
+		/* Notification that a partition went away! */
+		return;
+	/* First, we should NEVER get an int here...only acks */
+	if (event->xFlags.xFunction == HvLpEvent_Function_Int) {
+		printk(KERN_WARNING_VIO
+		       "Yikes! got an int in viodasd event handler!\n");
+		if (event->xFlags.xAckInd == HvLpEvent_AckInd_DoAck) {
+			event->xRc = HvLpEvent_Rc_InvalidSubtype;
+			HvCallEvent_ackLpEvent(event);
+		}
+	}
+
+	switch (event->xSubtype & VIOMINOR_SUBTYPE_MASK) {
+	case vioblockopen:
+		/*
+		 * Handle a response to an open request.  We get all the
+		 * disk information in the response, so update it.  The
+		 * correlation token contains a pointer to a waitevent
+		 * structure that has a semaphore in it.  update the
+		 * return code in the waitevent structure and post the
+		 * semaphore to wake up the guy who sent the request
+		 */
+		pwe = (struct viodasd_waitevent *)event->xCorrelationToken;
+		pwe->rc = event->xRc;
+		pwe->data.subRC = bevent->mSubTypeRc;
+		if (event->xRc == HvLpEvent_Rc_Good) {
+			const struct openData *data = &bevent->u.openData;
+			struct viodasd_device *device =
+				&viodasd_devices[bevent->mDisk];
+			device->readOnly =
+				bevent->mFlags & vioblockflags_ro;
+			device->size = data->mDiskLen;
+			device->cylinders = data->mCylinders;
+			device->tracks = data->mTracks;
+			device->sectors = data->mSectors;
+			device->bytesPerSector = data->mBytesPerSector;
+			viodasd_max_disk = data->mMaxDisks;
+		}
+		up(pwe->sem);
+		break;
+	case vioblockclose:
+		break;
+	case vioblockcheck:
+		pwe = (struct viodasd_waitevent *)event->xCorrelationToken;
+		pwe->rc = event->xRc;
+		pwe->data.changed = bevent->u.changed;
+		up(pwe->sem);
+		break;
+	case vioblockflush:
+		up((void *)event->xCorrelationToken);
+		break;
+	case vioblockread:
+	case vioblockwrite:
+		viodasd_handleReadWrite(bevent);
+		break;
+
+	default:
+		printk(KERN_WARNING_VIO "invalid subtype!");
+		if (event->xFlags.xAckInd == HvLpEvent_AckInd_DoAck) {
+			event->xRc = HvLpEvent_Rc_InvalidSubtype;
+			HvCallEvent_ackLpEvent(event);
+		}
+	}
+}
+
+static const char *major_name(int major)
+{
+	static char major_names[NUM_MAJORS][MAX_DISK_NAME];
+	int index = major_to_index(major);
+
+	if (index < 0)
+		return NULL;
+	if (major_names[index][0] == '\0') {
+		if (index == 0)
+			strcpy(major_names[index], VIOD_GENHD_NAME);
+		else
+			sprintf(major_names[index], VIOD_GENHD_NAME"%d", index);
+	}
+	return major_names[index];
+}
+
+static const char *disk_names(int diskno)
+{
+        static char name[MAX_DISK_NAME];
+        char suffix[MAX_DISK_NAME];
+        int idx = 0;
+        int i, j;
+
+        /* convert the disk number to a letter(s) */
+        /* 0=a 25=z 26=aa 27=a, ....              */
+        do {
+                suffix[idx++] = 'a' + (diskno % 26);
+                diskno /= 26;
+        } while (diskno-- && (idx < MAX_DISK_NAME - 1));
+
+        suffix[idx] = 0;
+        /* reverse the array */
+        for (i = 0, j = idx - 1; i < j; i++, j--) {
+                char c = suffix[i];
+		suffix[i] = suffix[j];
+		suffix[j] = c;
+	}
+
+        /* put the name all together and return it */
+        snprintf(name, MAX_DISK_NAME, "%s%s", VIOD_GENHD_NAME, suffix);
+        return name;
+}
+
+static void viodasd_cleanup_major(int major)
+{
+	int i;
+
+        for (i = 0; i < DEV_PER_MAJOR; i++) {
+		int device_no = DEV_PER_MAJOR * major_to_index(major) + i;
+
+		blk_cleanup_queue(viodasd_devices[device_no].disk->queue);
+		del_gendisk(viodasd_devices[device_no].disk);
+		put_disk(viodasd_devices[device_no].disk);
+		kfree(viodasd_devices[device_no].disk);
+		unregister_blkdev(major, major_name(major));
+		viodasd_devices[device_no].disk = NULL;
+	}
+}
+
+/* in case of bad return code, caller must cleanup2() for this device */
+static int __init viodasd_init_major(int major)
+{
+	struct gendisk *gendisk;
+	struct request_queue *q;
+	int i;
+
+        /* register the block device */
+	if (register_blkdev(major, major_name(major))) {
+		printk(KERN_WARNING "Unable to get major number %d for %s\n",
+				major, major_name(major));
+		return -1;
+	}
+	for (i = 0; i < DEV_PER_MAJOR; i++) {
+		int deviceno = DEV_PER_MAJOR * major_to_index(major) + i;
+
+		/* create the request queue for the disk */
+		spin_lock_init(&viodasd_devices[deviceno].q_lock);
+		q = blk_init_queue(do_viodasd_request,
+				&viodasd_devices[deviceno].q_lock);
+		if (q == NULL)
+			return -ENOMEM;
+		blk_queue_max_hw_segments(q, VIOMAXBLOCKDMA);
+		blk_queue_max_sectors(q, VIODASD_MAXSECTORS);
+		viodasd_devices[deviceno].queue = q;
+
+		/* inialize the struct */
+		gendisk = alloc_disk(1 << PARTITION_SHIFT);
+		if (gendisk == NULL)
+			return -ENOMEM;
+		viodasd_devices[deviceno].disk = gendisk;
+		gendisk->major = major;
+		gendisk->first_minor = i * (1 << PARTITION_SHIFT);
+		strncpy(gendisk->disk_name, disk_names(deviceno),
+				MAX_DISK_NAME);
+		strncpy(gendisk->devfs_name, disk_names(deviceno),
+				MAX_DISK_NAME);
+		gendisk->fops = &viodasd_fops;
+		gendisk->queue = q;
+		gendisk->private_data = (void *)&viodasd_devices[deviceno];
+		set_capacity(gendisk, viodasd_devices[deviceno].size >> 9);
+
+		/* register us in the global list */
+		add_disk(gendisk);
+	}
+
+	return 0;
+}
+
+/*
+ * Initialize the whole device driver.  Handle module and non-module
+ * versions
+ */
+static int __init viodasd_init(void)
+{
+	int i, j;
+
+	/* Try to open to our host lp */
+	if (viopath_hostLp == HvLpIndexInvalid)
+		vio_set_hostlp();
+
+	if (viopath_hostLp == HvLpIndexInvalid) {
+		printk(VIOD_KERN_WARNING "invalid hosting partition\n");
+		return -EIO;
+	}
+
+	printk(VIOD_KERN_INFO "Disk vers " VIODASD_VERS
+			", major %d, max disks %d, hosting partition %d\n",
+			major_table[0], MAX_DISKNO, viopath_hostLp);
+
+	/* Actually open the path to the hosting partition */
+	if (viopath_open(viopath_hostLp, viomajorsubtype_blockio,
+				VIOMAXREQ + 2)) {
+		printk(KERN_WARNING_VIO
+		       "error opening path to host partition %d\n",
+		       viopath_hostLp);
+		return -EIO;
+	} else
+		printk(VIOD_KERN_INFO "opened path to hosting partition %d\n",
+				viopath_hostLp);
+
+	/*
+	 * Initialize our request handler
+	 */
+	vio_setHandler(viomajorsubtype_blockio, vioHandleBlockEvent);
+
+	viodasd_max_disk = MAX_DISKNO - 1;
+	for (i = 0; (i <= viodasd_max_disk) && (i < MAX_DISKNO); i++) {
+		// Note that internal_open has side effects:
+		//  a) it updates the size of the disk
+		//  b) it updates viodasd_max_disk
+		//  c) it registers the disk if it has not done so already
+		if (internal_open(i, vioblockflags_ro) == 0)
+			internal_release(i, vioblockflags_ro);
+	}
+
+	printk(VIOD_KERN_INFO "Currently %d disks connected\n",
+			(int)viodasd_max_disk + 1);
+	if (viodasd_max_disk > (MAX_DISKNO - 1))
+		printk(KERN_INFO_VIO "Only examining the first %d\n",
+		       MAX_DISKNO);
+
+	for (i = 0; i < NUM_MAJORS; ++i) {
+		int init_rc = viodasd_init_major(major_table[i]);
+		if (init_rc < 0) {
+			for (j = 0; j <= i; ++j)
+				viodasd_cleanup_major(major_table[j]);
+			return init_rc;
+		}
+	}
+
+	/* 
+	 * Create the proc entry
+	 */
+	iSeries_proc_callback(&viodasd_proc_init);
+
+	return 0;
+}
+module_init(viodasd_init);
+
+#if 0
+void viodasd_exit(void)
+{
+	int i;
+	for(i = 0; i < NUM_MAJORS; ++i)
+		viodasd_cleanup_major(major_table[i]);
+
+	CLEANIT(viodasd_devices);
+
+	viopath_close(viopath_hostLp, viomajorsubtype_blockio, VIOMAXREQ + 2);
+	iSeries_proc_callback(&viodasd_proc_delete);
+
+}
+
+module_exit(viodasd_exit);
+#endif
diff -purN linux-2.5/drivers/char/Makefile linuxppc64-2.5/drivers/char/Makefile
--- linux-2.5/drivers/char/Makefile	2003-10-22 05:10:32.000000000 +0000
+++ linuxppc64-2.5/drivers/char/Makefile	2003-11-21 06:45:02.000000000 +0000
@@ -43,6 +43,7 @@ obj-$(CONFIG_SH_SCI)		+= sh-sci.o generi
 obj-$(CONFIG_HVC_CONSOLE)	+= hvc_console.o
 obj-$(CONFIG_RAW_DRIVER)	+= raw.o
 obj-$(CONFIG_SGI_L1_SERIAL)	+= sn_serial.o
+obj-$(CONFIG_VIOCONS) += viocons.o
 
 obj-$(CONFIG_PRINTER) += lp.o
 obj-$(CONFIG_TIPAR) += tipar.o
diff -purN linux-2.5/drivers/char/viocons.c linuxppc64-2.5/drivers/char/viocons.c
--- linux-2.5/drivers/char/viocons.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/char/viocons.c	2003-12-17 05:58:47.000000000 +0000
@@ -0,0 +1,1416 @@
+/* -*- linux-c -*-
+ *
+ *  drivers/char/viocons.c
+ *
+ *  iSeries Virtual Terminal
+ *
+ *  Authors: Dave Boutcher <boutcher@us.ibm.com>
+ *           Ryan Arnold <ryanarn@us.ibm.com>
+ *           Colin Devilbiss <devilbis@us.ibm.com>
+ *
+ * (C) Copyright 2000, 2001, 2002, 2003 IBM Corporation
+ *
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) anyu later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ */
+#include <linux/config.h>
+#include <linux/version.h>
+#include <linux/kernel.h>
+#include <linux/proc_fs.h>
+#include <linux/errno.h>
+#include <linux/vmalloc.h>
+#include <linux/mm.h>
+#include <linux/console.h>
+#include <linux/module.h>
+#include <asm/uaccess.h>
+#include <linux/init.h>
+#include <linux/wait.h>
+#include <linux/spinlock.h>
+#include <asm/ioctls.h>
+#include <linux/kd.h>
+#include <linux/tty.h>
+#include <linux/tty_flip.h>
+#include <linux/sysrq.h>
+
+#include <asm/iSeries/vio.h>
+
+#include <asm/iSeries/HvLpEvent.h>
+#include <asm/iSeries/HvCallEvent.h>
+#include <asm/iSeries/HvLpConfig.h>
+#include <asm/iSeries/HvCall.h>
+
+#define VIOTTY_MAGIC (0x0DCB)
+#define VTTY_PORTS 10
+#define VIOTTY_SERIAL_START 65
+
+static spinlock_t consolelock = SPIN_LOCK_UNLOCKED;
+static spinlock_t consoleloglock = SPIN_LOCK_UNLOCKED;
+
+#ifdef CONFIG_MAGIC_SYSRQ
+static int vio_sysrq_pressed;
+extern int sysrq_enabled;
+#endif
+
+/*
+ * The structure of the events that flow between us and OS/400.  You can't
+ * mess with this unless the OS/400 side changes too
+ */
+struct viocharlpevent {
+	struct HvLpEvent event;
+	u32 reserved;
+	u16 version;
+	u16 subtype_result_code;
+	u8 virtual_device;
+	u8 len;
+	u8 data[VIOCHAR_MAX_DATA];
+};
+
+/*
+ * This is a place where we handle the distribution of memory
+ * for copy_from_user() calls.  The buffer_available array is to
+ * help us determine which buffer to use.
+ */
+#define VIOCHAR_NUM_CFU_BUFFERS	7
+static struct viocharlpevent viocons_cfu_buffer[VIOCHAR_NUM_CFU_BUFFERS];
+static atomic_t viocons_cfu_buffer_available[VIOCHAR_NUM_CFU_BUFFERS];
+
+#define VIOCHAR_WINDOW		10
+#define VIOCHAR_HIGHWATERMARK	3
+
+enum viocharsubtype {
+	viocharopen = 0x0001,
+	viocharclose = 0x0002,
+	viochardata = 0x0003,
+	viocharack = 0x0004,
+	viocharconfig = 0x0005
+};
+
+enum viochar_rc {
+	viochar_rc_ebusy = 1
+};
+
+#define VIOCHAR_NUM_BUF		8
+
+/*
+ * Our port information.  We store a pointer to one entry in the
+ * tty_driver_data
+ */
+static struct port_info {
+	int magic;
+	struct tty_struct *tty;
+	HvLpIndex lp;
+	u8 vcons;
+	u64 seq;	/* sequence number of last HV send */
+	u64 ack;	/* last ack from HV */
+/*
+ * When we get writes faster than we can send it to the partition,
+ * buffer the data here. Note that used is a bit map of used buffers.
+ * It had better have enough bits to hold VIOCHAR_NUM_BUF the bitops assume
+ * it is a multiple of unsigned long
+ */
+	unsigned long used;
+	u8 *buffer[VIOCHAR_NUM_BUF];
+	int bufferBytes[VIOCHAR_NUM_BUF];
+	int curbuf;
+	int bufferOverflow;
+	int overflowMessage;
+} port_info[VTTY_PORTS];
+
+static void initDataEvent(struct viocharlpevent *viochar, HvLpIndex lp);
+
+static struct tty_driver *viotty_driver;
+static struct tty_driver *viottyS_driver;
+
+void hvlog(char *fmt, ...)
+{
+	int i;
+	unsigned long flags;
+	va_list args;
+	static char buf[256];
+
+	spin_lock_irqsave(&consoleloglock, flags);
+	va_start(args, fmt);
+	i = vsnprintf(buf, sizeof(buf) - 1, fmt, args);
+	va_end(args);
+	buf[i++] = '\r';
+	HvCall_writeLogBuffer(buf, i);
+	spin_unlock_irqrestore(&consoleloglock, flags);
+}
+
+void hvlogOutput(const char *buf, int count)
+{
+	unsigned long flags;
+	int begin;
+	int index;
+	char cr;
+
+	cr = '\r';
+	begin = 0;
+	spin_lock_irqsave(&consoleloglock, flags);
+	for (index = 0; index < count; index++) {
+		if (buf[index] == '\n') {
+			/*
+			 * Start right after the last '\n' or at the zeroth
+			 * array position and output the number of characters
+			 * including the newline.
+			 */
+			HvCall_writeLogBuffer(&buf[begin], index - begin + 1);
+			begin = index + 1;
+			HvCall_writeLogBuffer(&cr, 1);
+		}
+	}
+	if ((index - begin) > 0)
+		HvCall_writeLogBuffer(&buf[begin], index - begin);
+	spin_unlock_irqrestore(&consoleloglock, flags);
+}
+
+/*
+ * Make sure we're pointing to a valid port_info structure.  Shamelessly
+ * plagerized from serial.c
+ */
+static inline int viotty_paranoia_check(struct port_info *pi,
+					char *name, const char *routine)
+{
+	static const char *bad_pi_addr = KERN_WARNING_VIO
+		"Warning: bad address for port_info struct (%s) in %s\n";
+	static const char *badmagic = KERN_WARNING_VIO
+		"Warning: bad magic number for port_info struct (%s) in %s\n";
+
+	if ((pi - &port_info[0]) > VTTY_PORTS) {
+		printk(bad_pi_addr, name, routine);
+		return 1;
+	}
+	if (pi->magic != VIOTTY_MAGIC) {
+		printk(badmagic, name, routine);
+		return 1;
+	}
+	return 0;
+}
+
+/*
+ * This function should ONLY be called once from viocons_init2
+ */
+static void viocons_init_cfu_buffer(void)
+{
+	int i;
+
+	for (i = 1; i < VIOCHAR_NUM_CFU_BUFFERS; i++)
+		atomic_set(&viocons_cfu_buffer_available[i], 1);
+}
+
+static struct viocharlpevent *viocons_get_cfu_buffer(void)
+{
+	int i;
+
+	/*
+	 * Grab the first available buffer.  It doesn't matter if we
+	 * are interrupted during this array traversal as long as we
+	 * get an available space.
+	 */
+	for (i = 0; i < VIOCHAR_NUM_CFU_BUFFERS; i++)
+		if (atomic_dec_if_positive(&viocons_cfu_buffer_available[i])
+				== 0 )
+			return &viocons_cfu_buffer[i];
+	hvlog("\n\rviocons: viocons_get_cfu_buffer : no free buffers found");
+	return NULL;
+}
+
+static void viocons_free_cfu_buffer(struct viocharlpevent *buffer)
+{
+	int i;
+
+	i = buffer - &viocons_cfu_buffer[0];
+	if (i >= (sizeof(viocons_cfu_buffer) / sizeof(viocons_cfu_buffer[0]))) {
+		hvlog("\n\rviocons: viocons_free_cfu_buffer : buffer pointer not found in list.");
+		return;
+	}
+	if (atomic_read(&viocons_cfu_buffer_available[i]) != 0) {
+		hvlog("\n\rviocons: WARNING : returning unallocated cfu buffer.");
+		return;
+	}
+	atomic_set(&viocons_cfu_buffer_available[i], 1);
+}
+
+/*
+ * Add data to our pending-send buffers.  
+ *
+ * NOTE: Don't use printk in here because it gets nastily recursive.
+ * hvlog can be used to log to the hypervisor buffer
+ */
+static int buffer_add(u8 port, const char *buf, size_t len)
+{
+	size_t bleft;
+	size_t curlen;
+	const char *curbuf;
+	int nextbuf;
+	struct port_info *pi = &port_info[port];
+
+	curbuf = buf;
+	bleft = len;
+	while (bleft > 0) {
+		/*
+		 * If there is no space left in the current buffer, we have
+		 * filled everything up, so return.  If we filled the previous
+		 * buffer we would already have moved to the next one.
+		 */
+		if (pi->bufferBytes[pi->curbuf] == VIOCHAR_MAX_DATA) {
+			hvlog ("\n\rviocons: No overflow buffer available for memcpy().\n");
+			pi->bufferOverflow++;
+			pi->overflowMessage = 1;
+			break;
+		}
+
+		/*
+		 * Turn on the "used" bit for this buffer.  If it's already on,
+		 * that's fine.
+		 */
+		set_bit(pi->curbuf, &pi->used);
+
+		/*
+		 * See if this buffer has been allocated.  If not, allocate it.
+		 */
+		if (pi->buffer[pi->curbuf] == NULL) {
+			pi->buffer[pi->curbuf] =
+			    kmalloc(VIOCHAR_MAX_DATA, GFP_ATOMIC);
+			if (pi->buffer[pi->curbuf] == NULL) {
+				hvlog("\n\rviocons: kmalloc failed allocating spaces for buffer %d.",
+					pi->curbuf);
+				break;
+			}
+		}
+
+		/* Figure out how much we can copy into this buffer. */
+		if (bleft < (VIOCHAR_MAX_DATA - pi->bufferBytes[pi->curbuf]))
+			curlen = bleft;
+		else
+			curlen = VIOCHAR_MAX_DATA - pi->bufferBytes[pi->curbuf];
+
+		/* Copy the data into the buffer. */
+		memcpy(pi->buffer[pi->curbuf] + pi->bufferBytes[pi->curbuf],
+				curbuf, curlen);
+
+		pi->bufferBytes[pi->curbuf] += curlen;
+		curbuf += curlen;
+		bleft -= curlen;
+
+		/*
+		 * Now see if we've filled this buffer.  If not then
+		 * we'll try to use it again later.  If we've filled it
+		 * up then we'll advance the curbuf to the next in the
+		 * circular queue.
+		 */
+		if (pi->bufferBytes[pi->curbuf] == VIOCHAR_MAX_DATA) {
+			nextbuf = (pi->curbuf + 1) % VIOCHAR_NUM_BUF;
+			/*
+			 * Move to the next buffer if it hasn't been used yet
+			 */
+			if (test_bit(nextbuf, &pi->used) == 0)
+				pi->curbuf = nextbuf;
+		}
+	}
+	return len - bleft;
+}
+
+/*
+ * Send pending data
+ *
+ * NOTE: Don't use printk in here because it gets nastily recursive.
+ * hvlog can be used to log to the hypervisor buffer
+ */
+static void send_buffers(u8 port, HvLpIndex lp)
+{
+	HvLpEvent_Rc hvrc;
+	int nextbuf;
+	struct viocharlpevent *viochar;
+	unsigned long flags;
+	struct port_info *pi = &port_info[port];
+
+	spin_lock_irqsave(&consolelock, flags);
+
+	viochar = (struct viocharlpevent *)
+	    vio_get_event_buffer(viomajorsubtype_chario);
+
+	/* Make sure we got a buffer */
+	if (viochar == NULL) {
+		hvlog("\n\rviocons: Can't get viochar buffer in sendBuffers().");
+		spin_unlock_irqrestore(&consolelock, flags);
+		return;
+	}
+
+	if (pi->used == 0) {
+		hvlog("\n\rviocons: in sendbuffers(), but no buffers used.\n");
+		vio_free_event_buffer(viomajorsubtype_chario, viochar);
+		spin_unlock_irqrestore(&consolelock, flags);
+		return;
+	}
+
+	/*
+	 * curbuf points to the buffer we're filling.  We want to
+	 * start sending AFTER this one.  
+	 */
+	nextbuf = (pi->curbuf + 1) % VIOCHAR_NUM_BUF;
+
+	/*
+	 * Loop until we find a buffer with the used bit on
+	 */
+	while (test_bit(nextbuf, &pi->used) == 0)
+		nextbuf = (nextbuf + 1) % VIOCHAR_NUM_BUF;
+
+	initDataEvent(viochar, lp);
+
+	/*
+	 * While we have buffers with data, and our send window
+	 * is open, send them
+	 */
+	while ((test_bit(nextbuf, &pi->used)) &&
+	       ((pi->seq - pi->ack) < VIOCHAR_WINDOW)) {
+		viochar->len = pi->bufferBytes[nextbuf];
+		viochar->event.xCorrelationToken = pi->seq++;
+		viochar->event.xSizeMinus1 =
+			offsetof(struct viocharlpevent, data) + viochar->len;
+
+		memcpy(viochar->data, pi->buffer[nextbuf], viochar->len);
+
+		hvrc = HvCallEvent_signalLpEvent(&viochar->event);
+		if (hvrc) {
+			/*
+			 * MUST unlock the spinlock before doing a printk
+			 */
+			vio_free_event_buffer(viomajorsubtype_chario, viochar);
+			spin_unlock_irqrestore(&consolelock, flags);
+
+			printk(KERN_WARNING_VIO
+			       "console error sending event! return code %d\n",
+			       (int)hvrc);
+			return;
+		}
+
+		/*
+		 * clear the used bit, zero the number of bytes in
+		 * this buffer, and move to the next buffer
+		 */
+		clear_bit(nextbuf, &pi->used);
+		pi->bufferBytes[nextbuf] = 0;
+		nextbuf = (nextbuf + 1) % VIOCHAR_NUM_BUF;
+	}
+
+	/*
+	 * If we have emptied all the buffers, start at 0 again.
+	 * this will re-use any allocated buffers
+	 */
+	if (pi->used == 0) {
+		pi->curbuf = 0;
+
+		if (pi->overflowMessage)
+			pi->overflowMessage = 0;
+
+		if (pi->tty) {
+			if ((pi->tty->flags & (1 << TTY_DO_WRITE_WAKEUP)) &&
+			    (pi->tty->ldisc.write_wakeup))
+				(pi->tty->ldisc.write_wakeup)(pi->tty);
+			wake_up_interruptible(&pi->tty->write_wait);
+		}
+	}
+
+	vio_free_event_buffer(viomajorsubtype_chario, viochar);
+	spin_unlock_irqrestore(&consolelock, flags);
+}
+
+/*
+ * Our internal writer.  Gets called both from the console device and
+ * the tty device.  the tty pointer will be NULL if called from the console.
+ * Return total number of bytes "written".
+ *
+ * NOTE: Don't use printk in here because it gets nastily recursive.  hvlog
+ * can be used to log to the hypervisor buffer
+ */
+static int internal_write(HvLpIndex lp, u8 port, const char *buf,
+			  size_t len, struct viocharlpevent *viochar)
+{
+	HvLpEvent_Rc hvrc;
+	size_t bleft;
+	size_t curlen;
+	const char *curbuf;
+	unsigned long flags;
+	int copy_needed = (viochar == NULL);
+	struct port_info *pi = &port_info[port];
+
+	/*
+	 * Write to the hvlog of inbound data are now done prior to
+	 * calling internal_write() since internal_write() is only called in
+	 * the event that an lp event path is active, which isn't the case for
+	 * logging attempts prior to console initialization.
+	 *
+	 * If there is already data queued for this port, send it prior to
+	 * attempting to send any new data.
+	 */
+	if (pi->used)
+		send_buffers(port, lp);
+
+	spin_lock_irqsave(&consolelock, flags);
+
+	/*
+	 * If the internal_write() was passed a pointer to a
+	 * viocharlpevent then we don't need to allocate a new one
+	 * (this is the case where we are internal_writing user space
+	 * data).  If we aren't writing user space data then we need
+	 * to get an event from viopath.
+	 */
+	if (copy_needed) {
+		/* This one is fetched from the viopath data structure */
+		viochar = (struct viocharlpevent *)
+			vio_get_event_buffer(viomajorsubtype_chario);
+		/* Make sure we got a buffer */
+		if (viochar == NULL) {
+			spin_unlock_irqrestore(&consolelock, flags);
+			hvlog("\n\rviocons: Can't get viochar buffer in internal_write().");
+			return -EAGAIN;
+		}
+		initDataEvent(viochar, lp);
+	}
+
+	curbuf = buf;
+	bleft = len;
+
+	while ((bleft > 0) && (pi->used == 0) &&
+	       ((pi->seq - pi->ack) < VIOCHAR_WINDOW)) {
+		if (bleft > VIOCHAR_MAX_DATA)
+			curlen = VIOCHAR_MAX_DATA;
+		else
+			curlen = bleft;
+
+		viochar->event.xCorrelationToken = pi->seq++;
+
+		if (copy_needed) {
+			memcpy(viochar->data, curbuf, curlen);
+			viochar->len = curlen;
+		}
+
+		viochar->event.xSizeMinus1 =
+		    offsetof(struct viocharlpevent, data) + curlen;
+
+		hvrc = HvCallEvent_signalLpEvent(&viochar->event);
+		if (hvrc) {
+			spin_unlock_irqrestore(&consolelock, flags);
+			if (copy_needed)
+				vio_free_event_buffer(viomajorsubtype_chario, viochar);
+
+			hvlog("viocons: error sending event! %d\n", (int)hvrc);
+			return len - bleft;
+		}
+
+		curbuf += curlen;
+		bleft -= curlen;
+	}
+
+	/* If we didn't send it all, buffer as much of it as we can. */
+	if (bleft > 0)
+		bleft -= buffer_add(port, curbuf, bleft);
+	/*
+	 * Since we grabbed it from the viopath data structure, return
+	 * it to the data structure.
+	 */
+	if (copy_needed)
+		vio_free_event_buffer(viomajorsubtype_chario, viochar);
+	spin_unlock_irqrestore(&consolelock, flags);
+
+	return len - bleft;
+}
+
+static int get_port_data(struct tty_struct *tty, HvLpIndex *lp, u8 *port)
+{
+	unsigned long flags;
+	struct port_info *pi = NULL;
+
+	spin_lock_irqsave(&consolelock, flags);
+	if (tty) {
+		pi = (struct port_info *)tty->driver_data;
+		if (!pi || viotty_paranoia_check(pi, tty->name,
+					     "get_port_data")) {
+			spin_unlock_irqrestore(&consolelock, flags);
+			return -ENODEV;
+		}
+
+		*lp = pi->lp;
+		*port = pi - &port_info[0];
+	} else {
+		/*
+		 * If this is the console device, use the lp from
+		 * the first port entry
+		 */
+		*port = 0;
+		*lp = port_info[0].lp;
+	}
+	spin_unlock_irqrestore(&consolelock, flags);
+	return 0;
+}
+
+/*
+ * Initialize the common fields in a charLpEvent
+ */
+static void initDataEvent(struct viocharlpevent *viochar, HvLpIndex lp)
+{
+	memset(viochar, 0, sizeof(struct viocharlpevent));
+
+	viochar->event.xFlags.xValid = 1;
+	viochar->event.xFlags.xFunction = HvLpEvent_Function_Int;
+	viochar->event.xFlags.xAckInd = HvLpEvent_AckInd_NoAck;
+	viochar->event.xFlags.xAckType = HvLpEvent_AckType_DeferredAck;
+	viochar->event.xType = HvLpEvent_Type_VirtualIo;
+	viochar->event.xSubtype = viomajorsubtype_chario | viochardata;
+	viochar->event.xSourceLp = HvLpConfig_getLpIndex();
+	viochar->event.xTargetLp = lp;
+	viochar->event.xSizeMinus1 = sizeof(struct viocharlpevent);
+	viochar->event.xSourceInstanceId = viopath_sourceinst(lp);
+	viochar->event.xTargetInstanceId = viopath_targetinst(lp);
+}
+
+/*
+ * console device write
+ */
+static void viocons_write(struct console *co, const char *s, unsigned count)
+{
+	int index;
+	int begin;
+	HvLpIndex lp;
+	u8 port;
+
+	static const char cr = '\r';
+
+	/* Check port data first because the target LP might be valid but
+	 * simply not active, in which case we want to hvlog the output.
+	 */
+	if (get_port_data(NULL, &lp, &port)) {
+		hvlog("\n\rviocons_write: unable to get port data.");
+		return;
+	}
+
+	hvlogOutput(s, count);
+
+	if (!viopath_isactive(lp)) {
+		/*
+		 * This is a VERY noisy trace message in the case where the
+		 * path manager is not active or in the case where this
+		 * function is called prior to viocons initialization.  It is
+		 * being commented out for the sake of a clear trace buffer.
+		 */
+#if 0
+		 hvlog("\n\rviocons_write: path not active to lp %d", lp );
+#endif
+		return;
+	}
+
+	/* 
+	 * Any newline character found will cause a
+	 * carriage return character to be emitted as well. 
+	 */
+	begin = 0;
+	for (index = 0; index < count; index++) {
+		if (s[index] == '\n') {
+			/* 
+			 * Newline found. Print everything up to and 
+			 * including the newline
+			 */
+			internal_write(lp, port, &s[begin], index - begin + 1,
+					NULL);
+			begin = index + 1;
+			/* Emit a carriage return as well */
+			internal_write(lp, port, &cr, 1, NULL);
+		}
+	}
+
+	/* If any characters left to write, write them now */
+	if ((index - begin) > 0)
+		internal_write(lp, port, &s[begin], index - begin, NULL);
+}
+
+/*
+ * Work out the device associate with this console
+ */
+static struct tty_driver *viocons_device(struct console *c, int *index)
+{
+	*index = c->index;
+	return viotty_driver;
+}
+
+/*
+ * console device I/O methods
+ */
+static struct console viocons = {
+	.name = "ttyS",
+	.write = viocons_write,
+	.device = viocons_device,
+	.flags = CON_PRINTBUFFER,
+	.index = -1,
+};
+
+/*
+ * TTY Open method
+ */
+static int viotty_open(struct tty_struct *tty, struct file *filp)
+{
+	int port;
+	unsigned long flags;
+	struct port_info *pi;
+
+	port = tty->index;
+
+	if (port >= VIOTTY_SERIAL_START)
+		port -= VIOTTY_SERIAL_START;
+
+	if ((port < 0) || (port >= VTTY_PORTS))
+		return -ENODEV;
+
+	spin_lock_irqsave(&consolelock, flags);
+
+	pi = &port_info[port];
+	/* If some other TTY is already connected here, reject the open */
+	if ((pi->tty) && (pi->tty != tty)) {
+		spin_unlock_irqrestore(&consolelock, flags);
+		printk(KERN_WARNING_VIO
+		       "console attempt to open device twice from different ttys\n");
+		return -EBUSY;
+	}
+	tty->driver_data = pi;
+	pi->tty = tty;
+	spin_unlock_irqrestore(&consolelock, flags);
+
+	return 0;
+}
+
+/*
+ * TTY Close method
+ */
+static void viotty_close(struct tty_struct *tty, struct file *filp)
+{
+	unsigned long flags;
+	struct port_info *pi;
+
+	spin_lock_irqsave(&consolelock, flags);
+	pi = (struct port_info *)tty->driver_data;
+
+	if (!pi || viotty_paranoia_check(pi, tty->name, "viotty_close")) {
+		spin_unlock_irqrestore(&consolelock, flags);
+		return;
+	}
+/*	if (atomic_read(&tty->count) == 1) { */
+	if (tty->count == 1)
+		pi->tty = NULL;
+	spin_unlock_irqrestore(&consolelock, flags);
+}
+
+/*
+ * TTY Write method
+ */
+static int viotty_write(struct tty_struct *tty, int from_user,
+			const unsigned char *buf, int count)
+{
+	int ret;
+	int total = 0;
+	HvLpIndex lp;
+	u8 port;
+
+	ret = get_port_data(tty, &lp, &port);
+	if (ret) {
+		hvlog("\n\rviotty_write: no port data.");
+		return -ENODEV;
+	}
+
+	if (port == 0)
+		hvlogOutput(buf, count);
+
+	/*
+	 * If the path to this LP is closed, don't bother doing anything more.
+	 * just dump the data on the floor and return count.  For some reason
+	 * some user level programs will attempt to probe available tty's and
+	 * they'll attempt a viotty_write on an invalid port which maps to an
+	 * invalid target lp.  If this is the case then ignore the
+	 * viotty_write call and, since the viopath isn't active to this
+	 * partition, return count.
+	 */
+	if (!viopath_isactive(lp)) {
+		/* Noisy trace.  Commented unless needed. */
+#if 0
+		 hvlog("\n\rviotty_write: viopath NOT active for lp %d.",lp);
+#endif
+		return count;
+	}
+
+	/*
+	 * If the viotty_write is invoked from user space we want to do the
+	 * copy_from_user() into an event buffer from the cfu buffer before
+	 * internal_write() is called because internal_write may need to buffer
+	 * data which will need to grab a spin_lock and we shouldn't
+	 * copy_from_user() while holding a spin_lock.  Should internal_write()
+	 * not need to buffer data then it'll just use the event we created here
+	 * rather than checking one out from vio_get_event_buffer().
+	 */
+	if (from_user) {
+		struct viocharlpevent *viochar;
+		int curlen;
+		const char *curbuf = buf;
+
+		viochar = viocons_get_cfu_buffer();
+		if (viochar == NULL)
+			return -EAGAIN;
+		initDataEvent(viochar, lp);
+		while (count > 0) {
+			if (count > VIOCHAR_MAX_DATA)
+				curlen = VIOCHAR_MAX_DATA;
+			else
+				curlen = count;
+			viochar->len = curlen;
+			ret = copy_from_user(viochar->data, curbuf, curlen);
+			if (ret)
+				break;
+			ret = internal_write(lp, port, viochar->data,
+					viochar->len, viochar);
+			total += ret;
+			if (ret != curlen)
+				break;
+			count -= curlen;
+			curbuf += curlen;
+		}
+		viocons_free_cfu_buffer(viochar);
+	} else
+		total = internal_write(lp, port, buf, count, NULL);
+	return total;
+}
+
+/*
+ * TTY put_char method
+ */
+static void viotty_put_char(struct tty_struct *tty, unsigned char ch)
+{
+	HvLpIndex lp;
+	u8 port;
+
+	if (get_port_data(tty, &lp, &port))
+		return;
+
+	/* This will append '\r' as well if the char is '\n' */
+	if (port == 0)
+		hvlogOutput(&ch, 1);
+
+	if (viopath_isactive(lp))
+		internal_write(lp, port, &ch, 1, NULL);
+}
+
+/*
+ * TTY flush_chars method
+ */
+static void viotty_flush_chars(struct tty_struct *tty)
+{
+}
+
+/*
+ * TTY write_room method
+ */
+static int viotty_write_room(struct tty_struct *tty)
+{
+	int i;
+	int room = 0;
+	struct port_info *pi;
+	unsigned long flags;
+
+	spin_lock_irqsave(&consolelock, flags);
+	pi = (struct port_info *)tty->driver_data;
+	if (!pi || viotty_paranoia_check(pi, tty->name, "viotty_write_room")) {
+		spin_unlock_irqrestore(&consolelock, flags);
+		return 0;
+	}
+
+	/* If no buffers are used, return the max size. */
+	if (pi->used == 0) {
+		spin_unlock_irqrestore(&consolelock, flags);
+		return VIOCHAR_MAX_DATA * VIOCHAR_NUM_BUF;
+	}
+
+	/*
+	 * We retain the spinlock because we want to get an accurate
+	 * count and it can change on us between each operation if we
+	 * don't hold the spinlock.
+	 */
+	for (i = 0; ((i < VIOCHAR_NUM_BUF) && (room < VIOCHAR_MAX_DATA)); i++)
+		room += (VIOCHAR_MAX_DATA - pi->bufferBytes[i]);
+	spin_unlock_irqrestore(&consolelock, flags);
+
+	if (room > VIOCHAR_MAX_DATA)
+		room = VIOCHAR_MAX_DATA;
+	return room;
+}
+
+/*
+ * TTY chars_in_buffer_room method
+ */
+static int viotty_chars_in_buffer(struct tty_struct *tty)
+{
+	return 0;
+}
+
+static void viotty_flush_buffer(struct tty_struct *tty)
+{
+}
+
+static int viotty_ioctl(struct tty_struct *tty, struct file *file,
+			unsigned int cmd, unsigned long arg)
+{
+	switch (cmd) {
+	/*
+	 * the ioctls below read/set the flags usually shown in the leds
+	 * don't use them - they will go away without warning
+	 */
+	case KDGETLED:
+	case KDGKBLED:
+		return put_user(0, (char *)arg);
+
+	case KDSKBLED:
+		return 0;
+	}
+
+	return n_tty_ioctl(tty, file, cmd, arg);
+}
+
+static void viotty_throttle(struct tty_struct *tty)
+{
+}
+
+static void viotty_unthrottle(struct tty_struct *tty)
+{
+}
+
+static void viotty_set_termios(struct tty_struct *tty,
+		struct termios *old_termios)
+{
+}
+
+static void viotty_stop(struct tty_struct *tty)
+{
+}
+
+static void viotty_start(struct tty_struct *tty)
+{
+}
+
+static void viotty_hangup(struct tty_struct *tty)
+{
+}
+
+static void viotty_break(struct tty_struct *tty, int break_state)
+{
+}
+
+static void viotty_send_xchar(struct tty_struct *tty, char ch)
+{
+}
+
+static void viotty_wait_until_sent(struct tty_struct *tty, int timeout)
+{
+}
+
+/*
+ * Handle an open charLpEvent.  Could be either interrupt or ack
+ */
+static void vioHandleOpenEvent(struct HvLpEvent *event)
+{
+	unsigned long flags;
+	struct viocharlpevent *cevent = (struct viocharlpevent *)event;
+	u8 port = cevent->virtual_device;
+	struct port_info *pi;
+	int reject = 0;
+
+	if (event->xFlags.xFunction == HvLpEvent_Function_Ack) {
+		if (port >= VTTY_PORTS)
+			return;
+
+		spin_lock_irqsave(&consolelock, flags);
+		/* Got the lock, don't cause console output */
+
+		pi = &port_info[port];
+		if (event->xRc == HvLpEvent_Rc_Good) {
+			pi->seq = pi->ack = 0;
+			/*
+			 * This line allows connections from the primary
+			 * partition but once one is connected from the
+			 * primary partition nothing short of a reboot
+			 * of linux will allow access from the hosting
+			 * partition again without a required iSeries fix.
+			 */
+			pi->lp = event->xTargetLp;
+		}
+
+		spin_unlock_irqrestore(&consolelock, flags);
+		if (event->xRc != HvLpEvent_Rc_Good)
+			printk(KERN_WARNING_VIO
+			       "viocons: event->xRc != HvLpEvent_Rc_Good, event->xRc == (%d).\n",
+			       event->xRc);
+
+		if (event->xCorrelationToken != 0) {
+			unsigned long semptr = event->xCorrelationToken;
+			up((struct semaphore *) semptr);
+		} else
+			printk(KERN_WARNING_VIO
+			       "viocons: wierd...got open ack without semaphore\n");
+		return;
+	}
+
+	/* This had better require an ack, otherwise complain */
+	if (event->xFlags.xAckInd != HvLpEvent_AckInd_DoAck) {
+		printk(KERN_WARNING_VIO
+		       "console: viocharopen without ack bit!\n");
+		return;
+	}
+
+	spin_lock_irqsave(&consolelock, flags);
+	/* Got the lock, don't cause console output */
+
+	/* Make sure this is a good virtual tty */
+	if (port >= VTTY_PORTS) {
+		event->xRc = HvLpEvent_Rc_SubtypeError;
+		cevent->subtype_result_code = viorc_openRejected;
+		/*
+		 * Flag state here since we can't printk while holding
+		 * a spinlock.
+		 */
+		reject = 1;
+	} else {
+		pi = &port_info[port];
+		if ((pi->lp != HvLpIndexInvalid) &&
+				(pi->lp != event->xSourceLp)) {
+			/*
+			 * If this is tty is already connected to a different
+			 * partition, fail.
+			 */
+			event->xRc = HvLpEvent_Rc_SubtypeError;
+			cevent->subtype_result_code = viorc_openRejected;
+			reject = 2;
+		} else {
+			pi->lp = event->xSourceLp;
+			event->xRc = HvLpEvent_Rc_Good;
+			cevent->subtype_result_code = viorc_good;
+			pi->seq = pi->ack = 0;
+			reject = 0;
+		}
+	}
+
+	spin_unlock_irqrestore(&consolelock, flags);
+
+	if (reject == 1)
+		printk(KERN_WARNING_VIO
+			"viocons: console open rejected : bad virtual tty.\n");
+	else if (reject == 2)
+		printk(KERN_WARNING_VIO
+			"viocons: console open rejected : console in exclusive use by another partition.\n");
+
+	/* Return the acknowledgement */
+	HvCallEvent_ackLpEvent(event);
+}
+
+/*
+ * Handle a close charLpEvent.  This should ONLY be an Interrupt because the
+ * virtual console should never actually issue a close event to the hypervisor
+ * because the virtual console never goes away.  A close event coming from the
+ * hypervisor simply means that there are no client consoles connected to the
+ * virtual console.
+ *
+ * Regardless of the number of connections masqueraded on the other side of
+ * the hypervisor ONLY ONE close event should be called to accompany the ONE
+ * open event that is called.  The close event should ONLY be called when NO
+ * MORE connections (masqueraded or not) exist on the other side of the
+ * hypervisor.
+ */
+static void vioHandleCloseEvent(struct HvLpEvent *event)
+{
+	unsigned long flags;
+	struct viocharlpevent *cevent = (struct viocharlpevent *)event;
+	u8 port = cevent->virtual_device;
+
+	if (event->xFlags.xFunction == HvLpEvent_Function_Int) {
+		if (port >= VTTY_PORTS) {
+			printk(KERN_WARNING_VIO "viocons: close message from invalid virtual device.\n");
+			return;
+		}
+
+		/* For closes, just mark the console partition invalid */
+		spin_lock_irqsave(&consolelock, flags);
+		/* Got the lock, don't cause console output */
+
+		if (port_info[port].lp == event->xSourceLp)
+			port_info[port].lp = HvLpIndexInvalid;
+
+		spin_unlock_irqrestore(&consolelock, flags);
+		printk(KERN_INFO_VIO
+		       "console close from %d\n", event->xSourceLp);
+	} else
+		printk(KERN_WARNING_VIO
+		       "console got unexpected close acknowlegement\n");
+}
+
+/*
+ * Handle a config charLpEvent.  Could be either interrupt or ack
+ */
+static void vioHandleConfig(struct HvLpEvent *event)
+{
+	struct viocharlpevent *cevent = (struct viocharlpevent *)event;
+
+	HvCall_writeLogBuffer(cevent->data, cevent->len);
+
+	if (cevent->data[0] == 0x01)
+		printk(KERN_INFO_VIO
+		       "console window resized to %d: %d: %d: %d\n",
+		       cevent->data[1], cevent->data[2],
+		       cevent->data[3], cevent->data[4]);
+	else
+		printk(KERN_WARNING_VIO "console unknown config event\n");
+}
+
+/*
+ * Handle a data charLpEvent. 
+ */
+static void vioHandleData(struct HvLpEvent *event)
+{
+	struct tty_struct *tty;
+	unsigned long flags;
+	struct viocharlpevent *cevent = (struct viocharlpevent *)event;
+	struct port_info *pi;
+	int index;
+	u8 port = cevent->virtual_device;
+
+	if (port >= VTTY_PORTS) {
+		printk(KERN_WARNING_VIO
+		       "console data on invalid virtual device %d\n", port);
+		return;
+	}
+
+	/*
+	 * Hold the spinlock so that we don't take an interrupt that
+	 * changes tty between the time we fetch the port_info
+	 * pointer and the time we paranoia check.
+	 */
+	spin_lock_irqsave(&consolelock, flags);
+	pi = &port_info[port];
+
+	/*
+	 * Change 05/01/2003 - Ryan Arnold: If a partition other than
+	 * the current exclusive partition tries to send us data
+	 * events then just drop them on the floor because we don't
+	 * want his stinking data.  He isn't authorized to receive
+	 * data because he wasn't the first one to get the console,
+	 * therefore he shouldn't be allowed to send data either.
+	 * This will work without an iSeries fix.
+	 */
+	if (pi->lp != event->xSourceLp) {
+		spin_unlock_irqrestore(&consolelock, flags);
+		return;
+	}
+
+	tty = pi->tty;
+	if (tty == NULL) {
+		spin_unlock_irqrestore(&consolelock, flags);
+		printk(KERN_WARNING_VIO "no tty for virtual device %d\n", port);
+		return;
+	}
+
+	if (tty->magic != TTY_MAGIC) {
+		spin_unlock_irqrestore(&consolelock, flags);
+		printk(KERN_WARNING_VIO "tty bad magic\n");
+		return;
+	}
+
+	/*
+	 * Just to be paranoid, make sure the tty points back to this port
+	 */
+	pi = (struct port_info *)tty->driver_data;
+	if (!pi || viotty_paranoia_check(pi, tty->name, "vioHandleData")) {
+		spin_unlock_irqrestore(&consolelock, flags);
+		return;
+	}
+	spin_unlock_irqrestore(&consolelock, flags);
+
+	/*
+	 * Change 07/21/2003 - Ryan Arnold: functionality added to
+	 * support sysrq utilizing ^O as the sysrq key.  The sysrq
+	 * functionality will only work if built into the kernel and
+	 * then only if sysrq is enabled through the proc filesystem.
+	 */
+	for (index = 0; index < cevent->len; index++) {
+#ifdef CONFIG_MAGIC_SYSRQ
+		if (sysrq_enabled) {
+			/* 0x0f is the ascii character for ^O */
+			if (cevent->data[index] == '\x0f') {
+				vio_sysrq_pressed = 1;
+				/*
+				 * continue because we don't want to add
+				 * the sysrq key into the data string.
+				 */
+				continue;
+			} else if (vio_sysrq_pressed) {
+				handle_sysrq(cevent->data[index], NULL, tty);
+				vio_sysrq_pressed = 0;
+				/*
+				 * continue because we don't want to add
+				 * the sysrq sequence into the data string.
+				 */
+				continue;
+			}
+		}
+#endif
+		/*
+		 * The sysrq sequence isn't included in this check if
+		 * sysrq is enabled and compiled into the kernel because
+		 * the sequence will never get inserted into the buffer.
+		 * Don't attempt to copy more data into the buffer than we
+		 * have room for because it would fail without indication.
+		 */
+		if ((tty->flip.count + 1) > TTY_FLIPBUF_SIZE) {
+			printk(KERN_WARNING_VIO
+					"console input buffer overflow!\n");
+			break;
+		}
+		tty_insert_flip_char(tty, cevent->data[index], TTY_NORMAL);
+	}
+
+	/* if cevent->len == 0 then no data was added to the buffer and flip.count == 0 */
+	if (tty->flip.count)
+		/* The next call resets flip.count when the data is flushed. */
+		tty_flip_buffer_push(tty);
+}
+
+/*
+ * Handle an ack charLpEvent. 
+ */
+static void vioHandleAck(struct HvLpEvent *event)
+{
+	struct viocharlpevent *cevent = (struct viocharlpevent *)event;
+	unsigned long flags;
+	u8 port = cevent->virtual_device;
+
+	if (port >= VTTY_PORTS) {
+		printk(KERN_WARNING_VIO
+		       "viocons: data on invalid virtual device\n");
+		return;
+	}
+
+	spin_lock_irqsave(&consolelock, flags);
+	port_info[port].ack = event->xCorrelationToken;
+	spin_unlock_irqrestore(&consolelock, flags);
+
+	if (port_info[port].used)
+		send_buffers(port, port_info[port].lp);
+}
+
+/*
+ * Handle charLpEvents and route to the appropriate routine
+ */
+static void vioHandleCharEvent(struct HvLpEvent *event)
+{
+	int charminor;
+
+	if (event == NULL)
+		return;
+
+	charminor = event->xSubtype & VIOMINOR_SUBTYPE_MASK;
+	switch (charminor) {
+	case viocharopen:
+		vioHandleOpenEvent(event);
+		break;
+	case viocharclose:
+		vioHandleCloseEvent(event);
+		break;
+	case viochardata:
+		vioHandleData(event);
+		break;
+	case viocharack:
+		vioHandleAck(event);
+		break;
+	case viocharconfig:
+		vioHandleConfig(event);
+		break;
+	default:
+		if ((event->xFlags.xFunction == HvLpEvent_Function_Int) &&
+		    (event->xFlags.xAckInd == HvLpEvent_AckInd_DoAck)) {
+			event->xRc = HvLpEvent_Rc_InvalidSubtype;
+			HvCallEvent_ackLpEvent(event);
+		}
+	}
+}
+
+/*
+ * Send an open event
+ */
+static int send_open(HvLpIndex remoteLp, void *sem)
+{
+	return HvCallEvent_signalLpEventFast(remoteLp,
+			HvLpEvent_Type_VirtualIo,
+			viomajorsubtype_chario | viocharopen,
+			HvLpEvent_AckInd_DoAck, HvLpEvent_AckType_ImmediateAck,
+			viopath_sourceinst(remoteLp),
+			viopath_targetinst(remoteLp),
+			(u64)(unsigned long)sem, VIOVERSION << 16,
+			0, 0, 0, 0);
+}
+
+static struct tty_operations serial_ops = {
+	.open = viotty_open,
+	.close = viotty_close,
+	.write = viotty_write,
+	.put_char = viotty_put_char,
+	.flush_chars = viotty_flush_chars,
+	.write_room = viotty_write_room,
+	.chars_in_buffer = viotty_chars_in_buffer,
+	.flush_buffer = viotty_flush_buffer,
+	.ioctl = viotty_ioctl,
+	.throttle = viotty_throttle,
+	.unthrottle = viotty_unthrottle,
+	.set_termios = viotty_set_termios,
+	.stop = viotty_stop,
+	.start = viotty_start,
+	.hangup = viotty_hangup,
+	.break_ctl = viotty_break,
+	.send_xchar = viotty_send_xchar,
+	.wait_until_sent = viotty_wait_until_sent,
+};
+
+int __init viocons_init2(void)
+{
+	DECLARE_MUTEX_LOCKED(Semaphore);
+	int rc;
+
+	/* Now open to the primary LP */
+	printk(KERN_INFO_VIO "console open path to primary\n");
+	/* +2 for fudge */
+	rc = viopath_open(HvLpConfig_getPrimaryLpIndex(),
+			viomajorsubtype_chario, VIOCHAR_WINDOW + 2);
+	if (rc)
+		printk(KERN_WARNING_VIO "console error opening to primary %d\n",
+				rc);
+
+	if (viopath_hostLp == HvLpIndexInvalid)
+		vio_set_hostlp();
+
+	/*
+	 * And if the primary is not the same as the hosting LP, open to the 
+	 * hosting lp
+	 */
+	if ((viopath_hostLp != HvLpIndexInvalid) &&
+	    (viopath_hostLp != HvLpConfig_getPrimaryLpIndex())) {
+		printk(KERN_INFO_VIO "console open path to hosting (%d)\n",
+				viopath_hostLp);
+		rc = viopath_open(viopath_hostLp, viomajorsubtype_chario,
+				VIOCHAR_WINDOW + 2);	/* +2 for fudge */
+		if (rc)
+			printk(KERN_WARNING_VIO
+				"console error opening to partition %d: %d\n",
+				viopath_hostLp, rc);
+	}
+
+	if (vio_setHandler(viomajorsubtype_chario, vioHandleCharEvent) < 0)
+		printk(KERN_WARNING_VIO
+				"Error seting handler for console events!\n");
+
+	printk(KERN_INFO_VIO "console major number is %d\n", TTY_MAJOR);
+
+	/* First, try to open the console to the hosting lp.
+	 * Wait on a semaphore for the response.
+	 */
+	if ((viopath_isactive(viopath_hostLp)) &&
+	    (send_open(viopath_hostLp, (void *)&Semaphore) == 0)) {
+		printk(KERN_INFO_VIO
+			"opening console to hosting partition %d\n",
+			viopath_hostLp);
+		//down(&Semaphore);
+		while (atomic_read(&Semaphore.count) == 0)
+			mb();
+		atomic_set(&Semaphore.count, 0);
+	}
+
+	/*
+	 * If we don't have an active console, try the primary
+	 */
+	if ((!viopath_isactive(port_info[0].lp)) &&
+	    (viopath_isactive(HvLpConfig_getPrimaryLpIndex())) &&
+	    (send_open(HvLpConfig_getPrimaryLpIndex(), (void *)&Semaphore)
+	     == 0)) {
+		printk(KERN_INFO_VIO "opening console to primary partition\n");
+		//down(&Semaphore);
+		while (atomic_read(&Semaphore.count) == 0)
+			mb();
+		atomic_set(&Semaphore.count, 0);
+	}
+	return 0;
+}
+
+static int viocons_init3(void)
+{
+#if 1
+	int ret = viocons_init2();
+
+	if (ret)
+		return ret;
+#endif
+
+	/* Initialize the tty_driver structure */
+	viotty_driver = alloc_tty_driver(VTTY_PORTS);
+	viotty_driver->owner = THIS_MODULE;
+	viotty_driver->driver_name = "vioconsole";
+	viotty_driver->devfs_name = "vcs/";
+	viotty_driver->name = "tty";
+	viotty_driver->major = TTY_MAJOR;
+	viotty_driver->minor_start = 1;
+	viotty_driver->type = TTY_DRIVER_TYPE_CONSOLE;
+	viotty_driver->subtype = 1;
+	viotty_driver->init_termios = tty_std_termios;
+	viotty_driver->flags = TTY_DRIVER_REAL_RAW | TTY_DRIVER_RESET_TERMIOS;
+	tty_set_operations(viotty_driver, &serial_ops);
+
+	viottyS_driver = alloc_tty_driver(VTTY_PORTS);
+	viottyS_driver->owner = THIS_MODULE;
+	viottyS_driver->driver_name = "vioconsole";
+	viottyS_driver->devfs_name = "tts/";
+	viottyS_driver->name = "ttyS";
+	viottyS_driver->major = TTY_MAJOR;
+	viottyS_driver->minor_start = VIOTTY_SERIAL_START;
+	viottyS_driver->type = TTY_DRIVER_TYPE_SERIAL;
+	viottyS_driver->subtype = 1;
+	viottyS_driver->init_termios = tty_std_termios;
+	viottyS_driver->flags = TTY_DRIVER_REAL_RAW | TTY_DRIVER_RESET_TERMIOS;
+	tty_set_operations(viottyS_driver, &serial_ops);
+
+	if (tty_register_driver(viotty_driver))
+		printk(KERN_WARNING_VIO "Couldn't register console driver\n");
+
+	if (tty_register_driver(viottyS_driver))
+		printk(KERN_WARNING_VIO "Couldn't register console S driver\n");
+
+	viocons_init_cfu_buffer();
+
+	return 0;
+}
+
+static int __init viocons_init(void)
+{
+	int i;
+
+	printk(KERN_INFO_VIO "registering console\n");
+	for (i = 0; i < VTTY_PORTS; i++) {
+		port_info[i].lp = HvLpIndexInvalid;
+		port_info[i].magic = VIOTTY_MAGIC;
+	}
+	HvCall_setLogBufferFormatAndCodepage(HvCall_LogBuffer_ASCII, 437);
+	register_console(&viocons);
+	return 0;
+}
+
+console_initcall(viocons_init);
+module_init(viocons_init3);
diff -purN linux-2.5/drivers/net/Kconfig linuxppc64-2.5/drivers/net/Kconfig
--- linux-2.5/drivers/net/Kconfig	2003-12-02 16:20:59.000000000 +0000
+++ linuxppc64-2.5/drivers/net/Kconfig	2003-12-08 05:22:35.000000000 +0000
@@ -2061,10 +2061,19 @@ config IXGB_NAPI
 endmenu
 
 
-config VETH
+config ISERIES_VETH
 	tristate "iSeries Virtual Ethernet driver support"
 	depends on NETDEVICES && PPC_ISERIES
 
+config IBMVETH
+	tristate "IBM LAN Virtual Ethernet support"
+	depends on NET_ETHERNET && PPC_PSERIES
+	---help---
+	  This is the Virtual Ethernet adapter driver.  
+
+	  To compile this driver as a module, choose M here and read
+	  <file:Documentation/networking/net-modules.txt>. 
+
 config FDDI
 	bool "FDDI driver support"
 	depends on NETDEVICES && (PCI || EISA)
diff -purN linux-2.5/drivers/net/Makefile linuxppc64-2.5/drivers/net/Makefile
--- linux-2.5/drivers/net/Makefile	2003-09-27 10:23:28.000000000 +0000
+++ linuxppc64-2.5/drivers/net/Makefile	2003-12-08 05:22:35.000000000 +0000
@@ -45,7 +45,7 @@ obj-$(CONFIG_SIS190) += sis190.o
 obj-$(CONFIG_SIS900) += sis900.o
 obj-$(CONFIG_YELLOWFIN) += yellowfin.o
 obj-$(CONFIG_ACENIC) += acenic.o
-obj-$(CONFIG_VETH) += veth.o
+obj-$(CONFIG_ISERIES_VETH) += iseries_veth.o
 obj-$(CONFIG_NATSEMI) += natsemi.o
 obj-$(CONFIG_NS83820) += ns83820.o
 obj-$(CONFIG_STNIC) += stnic.o 8390.o
@@ -175,6 +175,7 @@ obj-$(CONFIG_TUN) += tun.o
 obj-$(CONFIG_DL2K) += dl2k.o
 obj-$(CONFIG_R8169) += r8169.o
 obj-$(CONFIG_AMD8111_ETH) += amd8111e.o
+obj-$(CONFIG_IBMVETH) += ibmveth.o
 
 obj-$(CONFIG_ARM) += arm/
 obj-$(CONFIG_NET_FC) += fc/
diff -purN linux-2.5/drivers/net/e100/e100_main.c linuxppc64-2.5/drivers/net/e100/e100_main.c
--- linux-2.5/drivers/net/e100/e100_main.c	2003-09-25 03:34:49.000000000 +0000
+++ linuxppc64-2.5/drivers/net/e100/e100_main.c	2003-09-29 12:39:13.000000000 +0000
@@ -1500,7 +1500,7 @@ e100_setup_tcb_pool(tcb_t *head, unsigne
 		pcurr_tcb->tcb_skb = NULL;
 	}
 
-	wmb();
+	mb();
 }
 
 /***************************************************************************/
@@ -1753,7 +1753,7 @@ e100_watchdog(struct net_device *dev)
 	/* Check for command completion on next watchdog timer. */
 	e100_dump_stats_cntrs(bdp);
 
-	wmb();
+	mb();
 
 	/* relaunch watchdog timer in 2 sec */
 	mod_timer(&(bdp->watchdog_timer), jiffies + (2 * HZ));
@@ -2229,7 +2229,7 @@ e100_prepare_xmit_buff(struct e100_priva
 
 	bdp->tcb_pool.tail = NEXT_TCB_TOUSE(bdp->tcb_pool.tail);
 
-	wmb();
+	mb();
 
 	e100_start_cu(bdp, tcb);
 
@@ -2516,7 +2516,7 @@ e100_clr_cntrs(struct e100_private *bdp)
 	/* clear the dump counter complete word */
 	pcmd_complete = e100_cmd_complete_location(bdp);
 	*pcmd_complete = 0;
-	wmb();
+	mb();
 
 	if (!e100_wait_exec_cmplx(bdp, bdp->stat_cnt_phys, SCB_CUC_DUMP_ADDR, 0))
 		return false;
@@ -2651,7 +2651,7 @@ e100_exec_non_cu_cmd(struct e100_private
 	ntcb_hdr->cb_status = 0;
 	ntcb_hdr->cb_lnk_ptr = 0;
 
-	wmb();
+	mb();
 	if (in_interrupt())
 		return e100_delayed_exec_non_cu_cmd(bdp, command);
 
diff -purN linux-2.5/drivers/net/e100/e100_test.c linuxppc64-2.5/drivers/net/e100/e100_test.c
--- linux-2.5/drivers/net/e100/e100_test.c	2003-05-09 02:16:29.000000000 +0000
+++ linuxppc64-2.5/drivers/net/e100/e100_test.c	2003-05-28 20:29:48.000000000 +0000
@@ -316,7 +316,7 @@ e100_diag_loopback_alloc(struct e100_pri
 	memset((void *) ((u8 *) tbd + sizeof (tbd_t)), 0xFF, 512);
 	/* The value of second 512 bytes is BA */
 	memset((void *) ((u8 *) tbd + sizeof (tbd_t) + 512), 0xBA, 512);
-	wmb();
+	mb();
 	rfd = pci_alloc_consistent(bdp->pdev, sizeof (rfd_t), &dma_handle);
 
 	if (rfd == NULL) {
@@ -335,7 +335,7 @@ e100_diag_loopback_alloc(struct e100_pri
 	bdp->loopback.dma_handle = dma_handle;
 	bdp->loopback.tcb = tcb;
 	bdp->loopback.rfd = rfd;
-	wmb();
+	mb();
 	return true;
 }
 
diff -purN linux-2.5/drivers/net/ibmveth.c linuxppc64-2.5/drivers/net/ibmveth.c
--- linux-2.5/drivers/net/ibmveth.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/net/ibmveth.c	2003-12-04 18:25:52.000000000 +0000
@@ -0,0 +1,1136 @@
+/**************************************************************************/
+/*                                                                        */
+/* IBM eServer i/pSeries Virtual Ethernet Device Driver                   */
+/* Copyright (C) 2003 IBM Corp.                                           */
+/*  Dave Larson (larson1@us.ibm.com)                                      */
+/*  Santiago Leon (santil@us.ibm.com)                                     */
+/*                                                                        */
+/*  This program is free software; you can redistribute it and/or modify  */
+/*  it under the terms of the GNU General Public License as published by  */
+/*  the Free Software Foundation; either version 2 of the License, or     */
+/*  (at your option) any later version.                                   */
+/*                                                                        */
+/*  This program is distributed in the hope that it will be useful,       */
+/*  but WITHOUT ANY WARRANTY; without even the implied warranty of        */
+/*  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the         */
+/*  GNU General Public License for more details.                          */
+/*                                                                        */
+/*  You should have received a copy of the GNU General Public License     */
+/*  along with this program; if not, write to the Free Software           */
+/*  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  */
+/*                                                                   USA  */
+/*                                                                        */
+/* This module contains the implementation of a virtual ethernet device   */
+/* for use with IBM i/pSeries LPAR Linux.  It utilizes the logical LAN    */
+/* option of the RS/6000 Platform Architechture to interface with virtual */
+/* ethernet NICs that are presented to the partition by the hypervisor.   */
+/*                                                                        */ 
+/**************************************************************************/
+/*
+   TODO:
+     - remove frag processing code - no longer needed
+     - add support for sysfs
+     - possibly remove procfs support
+*/
+
+#include <linux/config.h>
+#include <linux/module.h>
+#include <linux/version.h>
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <linux/ioport.h>
+#include <linux/pci.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/skbuff.h>
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/mm.h>
+#ifdef SIOCETHTOOL
+#include <linux/ethtool.h>
+#endif
+#include <linux/proc_fs.h>
+#include <asm/semaphore.h>
+#include <asm/hvcall.h>
+#include <asm/atomic.h>
+#include <asm/pci_dma.h>
+#include <asm/vio.h>
+#include <asm/uaccess.h>
+#ifdef CONFIG_PROC_FS
+#include <linux/proc_fs.h>
+#endif
+
+#include "ibmveth.h"
+
+#define DEBUG 1
+
+#define ibmveth_printk(fmt, args...) \
+  printk(KERN_INFO "%s: " fmt, __FILE__, ## args)
+
+#define ibmveth_error_printk(fmt, args...) \
+  printk(KERN_ERR "(%s:%3.3d ua:%lx) ERROR: " fmt, __FILE__, __LINE__ , adapter->vdev->unit_address, ## args)
+
+#ifdef DEBUG
+#define ibmveth_debug_printk_no_adapter(fmt, args...) \
+  printk(KERN_DEBUG "(%s:%3.3d): " fmt, __FILE__, __LINE__ , ## args)
+#define ibmveth_debug_printk(fmt, args...) \
+  printk(KERN_DEBUG "(%s:%3.3d ua:%lx): " fmt, __FILE__, __LINE__ , adapter->vdev->unit_address, ## args)
+#define ibmveth_assert(expr) \
+  if(!(expr)) {                                   \
+    printk(KERN_DEBUG "assertion failed (%s:%3.3d ua:%lx): %s\n", __FILE__, __LINE__, adapter->vdev->unit_address, #expr); \
+    BUG(); \
+  }
+#else
+#define ibmveth_debug_printk_no_adapter(fmt, args...)
+#define ibmveth_debug_printk(fmt, args...)
+#define ibmveth_assert(expr) 
+#endif
+
+static int ibmveth_open(struct net_device *dev);
+static int ibmveth_close(struct net_device *dev);
+static int ibmveth_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd);
+static int ibmveth_poll(struct net_device *dev, int *budget);
+static int ibmveth_start_xmit(struct sk_buff *skb, struct net_device *dev);
+static struct net_device_stats *ibmveth_get_stats(struct net_device *dev);
+static void ibmveth_set_multicast_list(struct net_device *dev);
+static void ibmveth_proc_register_driver(void);
+static void ibmveth_proc_unregister_driver(void);
+static void ibmveth_proc_register_adapter(struct ibmveth_adapter *adapter);
+static void ibmveth_proc_unregister_adapter(struct ibmveth_adapter *adapter);
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,5,0)
+static irqreturn_t ibmveth_interrupt(int irq, void *dev_instance, struct pt_regs *regs);
+#define INIT_BOTTOM_HALF(x,y,z) INIT_WORK(x, y, (void*)z)
+#define SCHEDULE_BOTTOM_HALF(x) schedule_work(x)
+#else
+static void ibmveth_interrupt(int irq, void *dev_instance, struct pt_regs *regs);
+#define INIT_BOTTOM_HALF(x,y,z) tasklet_init(x, y, (unsigned long)z)
+#define SCHEDULE_BOTTOM_HALF(x) tasklet_schedule(x)
+#endif
+
+#ifdef CONFIG_PROC_FS
+#define IBMVETH_PROC_DIR "ibmveth"
+static struct proc_dir_entry *ibmveth_proc_dir;
+#endif
+
+char ibmveth_driver_name[] = "ibmveth";
+char ibmveth_driver_string[] = "IBM i/pSeries Virtual Ethernet Driver";
+char ibmveth_driver_version[] = "1.0";
+
+char ibmveth_driver_cvs_version[] = "$Revision: 1.7 $";
+char ibmveth_driver_cvs_tag[] = "$Name:  $";
+
+MODULE_AUTHOR("Dave Larson <larson1@us.ibm.com>");
+MODULE_DESCRIPTION("IBM i/pSeries Virtual Ethernet Driver");
+MODULE_LICENSE("GPL");
+
+/* simple methods of getting data from the current rxq entry */
+static inline int ibmveth_rxq_pending_buffer(struct ibmveth_adapter *adapter)
+{
+    return (adapter->rx_queue.queue_addr[adapter->rx_queue.index].toggle == adapter->rx_queue.toggle);
+}
+
+static inline int ibmveth_rxq_buffer_valid(struct ibmveth_adapter *adapter)
+{
+    return (adapter->rx_queue.queue_addr[adapter->rx_queue.index].valid);
+}
+
+static inline int ibmveth_rxq_frame_offset(struct ibmveth_adapter *adapter)
+{
+    return (adapter->rx_queue.queue_addr[adapter->rx_queue.index].offset);
+}
+
+static inline int ibmveth_rxq_frame_length(struct ibmveth_adapter *adapter)
+{
+    return (adapter->rx_queue.queue_addr[adapter->rx_queue.index].length);
+}
+
+/* setup the initial settings for a buffer pool */
+static void ibmveth_init_buffer_pool(struct ibmveth_buff_pool *pool, u32 pool_index, u32 pool_size, u32 buff_size)
+{
+    pool->size = pool_size;
+    pool->index = pool_index;
+    pool->buff_size = buff_size;
+    pool->threshold = pool_size / 2;
+}
+
+/* allocate and setup an buffer pool - called during open */
+static int ibmveth_alloc_buffer_pool(struct ibmveth_buff_pool *pool)
+{
+    int i;
+
+    pool->free_map = kmalloc(sizeof(u16) * pool->size, GFP_KERNEL); 
+
+    if(!pool->free_map) {
+	/* ibmveth_cleanup will take care of cleanup in this case */
+	return -1;
+    }
+
+    pool->dma_addr = kmalloc(sizeof(dma_addr_t) * pool->size, GFP_KERNEL); 
+    pool->skbuff = kmalloc(sizeof(void*) * pool->size, GFP_KERNEL);
+
+    if(!pool->skbuff || !pool->dma_addr) {
+	/* ibmveth_cleanup will take care of cleanup in this case */
+	return -1;
+    }
+
+    memset(pool->skbuff, 0, sizeof(void*) * pool->size);
+    memset(pool->dma_addr, 0, sizeof(dma_addr_t) * pool->size);
+
+    for(i = 0; i < pool->size; ++i) {
+	pool->free_map[i] = i;
+    }
+
+    atomic_set(&pool->available, 0);
+    pool->producer_index = 0;
+    pool->consumer_index = 0;
+
+    return 0;
+}
+
+/* replenish the buffers for a pool */
+static void ibmveth_replenish_buffer_pool(struct ibmveth_adapter *adapter, struct ibmveth_buff_pool *pool)
+{
+    u32 i;
+    u32 count = pool->size - atomic_read(&pool->available);
+    u32 buffers_added = 0;
+
+    mb();
+
+    for(i = 0; i < count; ++i) {
+	struct sk_buff *skb;
+	unsigned int free_index, index;
+	u64 correlator;
+	union ibmveth_buf_desc desc;
+	unsigned long lpar_rc;
+	dma_addr_t dma_addr;
+
+	skb = alloc_skb(pool->buff_size, GFP_ATOMIC);
+
+	if(!skb) {
+	    ibmveth_debug_printk("replenish: unable to allocate skb\n");
+	    adapter->replenish_no_mem++;
+	    break;
+	}
+
+	free_index = pool->consumer_index++ % pool->size;
+	index = pool->free_map[free_index];
+	
+	ibmveth_assert(index != 0xFFFF);
+	ibmveth_assert(pool->skbuff[index] == NULL);
+
+	dma_addr = vio_map_single(adapter->vdev, skb->data, pool->buff_size, PCI_DMA_FROMDEVICE);
+
+	pool->dma_addr[index] = dma_addr;
+	pool->skbuff[index] = skb;
+
+	correlator = ((u64)pool->index << 32) | index;
+	*(u64*)skb->data = correlator;
+
+	desc.desc = 0;
+	desc.fields.valid = 1;
+	desc.fields.length = pool->buff_size;
+	desc.fields.address = dma_addr; 
+
+	lpar_rc = h_add_logical_lan_buffer(adapter->vdev->unit_address, desc.desc);
+		    
+	if(lpar_rc != H_Success) {
+	    pool->skbuff[index] = NULL;
+	    pool->consumer_index--;
+	    vio_unmap_single(adapter->vdev, pool->dma_addr[index], pool->buff_size, PCI_DMA_FROMDEVICE);
+	    dev_kfree_skb_any(skb);
+	    adapter->replenish_add_buff_failure++;
+	    break;
+	} else {
+	    pool->free_map[free_index] = 0xFFFF;
+	    buffers_added++;
+	    adapter->replenish_add_buff_success++;
+	}
+    }
+    
+    atomic_add(buffers_added, &(pool->available));
+}
+
+/* check if replenishing is needed */
+static inline int ibmveth_is_replenishing_needed(struct ibmveth_adapter *adapter)
+{
+    return ((atomic_read(&adapter->rx_buff_pool[0].available) < adapter->rx_buff_pool[0].threshold) ||
+	    (atomic_read(&adapter->rx_buff_pool[1].available) < adapter->rx_buff_pool[1].threshold) ||
+	    (atomic_read(&adapter->rx_buff_pool[2].available) < adapter->rx_buff_pool[2].threshold));
+}
+
+/* replenish tasklet routine */
+static void ibmveth_replenish_task(struct ibmveth_adapter *adapter) 
+{
+    adapter->replenish_task_cycles++;
+
+    ibmveth_replenish_buffer_pool(adapter, &adapter->rx_buff_pool[0]);
+    ibmveth_replenish_buffer_pool(adapter, &adapter->rx_buff_pool[1]);
+    ibmveth_replenish_buffer_pool(adapter, &adapter->rx_buff_pool[2]);
+
+    adapter->rx_no_buffer = *(u64*)(((char*)adapter->buffer_list_addr) + 4096 - 8);
+
+    if(ibmveth_is_replenishing_needed(adapter) || (atomic_dec_return(&adapter->in_replenish_task) > 0)) {
+	/* there is more work, or we didn't change in_replenish_task from 1 -> 0 */
+	atomic_set(&adapter->in_replenish_task, 1);
+	SCHEDULE_BOTTOM_HALF(&adapter->replenish_task);
+    }
+}
+
+/* kick the replenish tasklet if we need replenishing and it isn't already running */
+static inline void ibmveth_schedule_replenishing(struct ibmveth_adapter *adapter)
+{
+    if(ibmveth_is_replenishing_needed(adapter)) {	
+	if(atomic_inc_return(&adapter->in_replenish_task) == 1) {
+		SCHEDULE_BOTTOM_HALF(&adapter->replenish_task);
+       }
+    }
+}
+
+/* empty and free ana buffer pool - also used to do cleanup in error paths */
+static void ibmveth_free_buffer_pool(struct ibmveth_adapter *adapter, struct ibmveth_buff_pool *pool)
+{
+    int i;
+
+    if(pool->free_map) {
+	kfree(pool->free_map);
+	pool->free_map  = NULL;
+    }
+
+    if(pool->skbuff && pool->dma_addr) {
+	for(i = 0; i < pool->size; ++i) {
+	    struct sk_buff *skb = pool->skbuff[i];
+	    if(skb) {
+		vio_unmap_single(adapter->vdev,
+				 pool->dma_addr[i],
+				 pool->buff_size,
+				 PCI_DMA_FROMDEVICE);
+		dev_kfree_skb_any(skb);
+		 pool->skbuff[i] = NULL;
+	    }
+	}
+    }
+
+    if(pool->dma_addr) {
+	kfree(pool->dma_addr);
+	pool->dma_addr = NULL;
+    }
+
+    if(pool->skbuff) {
+	kfree(pool->skbuff);
+	pool->skbuff = NULL;
+    }
+}
+
+/* remove a buffer from a pool */
+static void ibmveth_remove_buffer_from_pool(struct ibmveth_adapter *adapter, u64 correlator)
+{
+    unsigned int pool  = correlator >> 32;
+    unsigned int index = correlator & 0xFFFFFFFF;
+    unsigned int free_index;
+    struct sk_buff *skb;
+
+    ibmveth_assert(pool < IbmVethNumBufferPools);
+    ibmveth_assert(index < adapter->rx_buff_pool[pool].size);
+
+    skb = adapter->rx_buff_pool[pool].skbuff[index];
+
+    ibmveth_assert(skb != NULL);
+
+    adapter->rx_buff_pool[pool].skbuff[index] = NULL;
+
+    vio_unmap_single(adapter->vdev,
+		     adapter->rx_buff_pool[pool].dma_addr[index],
+		     adapter->rx_buff_pool[pool].buff_size,
+		     PCI_DMA_FROMDEVICE);
+
+    free_index = adapter->rx_buff_pool[pool].producer_index++ % adapter->rx_buff_pool[pool].size;
+    adapter->rx_buff_pool[pool].free_map[free_index] = index;
+
+    mb();
+
+    atomic_dec(&(adapter->rx_buff_pool[pool].available));
+}
+
+/* get the current buffer on the rx queue */
+static inline struct sk_buff *ibmveth_rxq_get_buffer(struct ibmveth_adapter *adapter)
+{
+    u64 correlator = adapter->rx_queue.queue_addr[adapter->rx_queue.index].correlator;
+    unsigned int pool = correlator >> 32;
+    unsigned int index = correlator & 0xFFFFFFFF;
+
+    ibmveth_assert(pool < IbmVethNumBufferPools);
+    ibmveth_assert(index < adapter->rx_buff_pool[pool].size);
+
+    return adapter->rx_buff_pool[pool].skbuff[index];
+}
+
+/* recycle the current buffer on the rx queue */
+static void ibmveth_rxq_recycle_buffer(struct ibmveth_adapter *adapter)
+{
+    u32 q_index = adapter->rx_queue.index;
+    u64 correlator = adapter->rx_queue.queue_addr[q_index].correlator;
+    unsigned int pool = correlator >> 32;
+    unsigned int index = correlator & 0xFFFFFFFF;
+    union ibmveth_buf_desc desc;
+    unsigned long lpar_rc;
+
+    ibmveth_assert(pool < IbmVethNumBufferPools);
+    ibmveth_assert(index < adapter->rx_buff_pool[pool].size);
+
+    desc.desc = 0;
+    desc.fields.valid = 1;
+    desc.fields.length = adapter->rx_buff_pool[pool].buff_size;
+    desc.fields.address = adapter->rx_buff_pool[pool].dma_addr[index];
+
+    lpar_rc = h_add_logical_lan_buffer(adapter->vdev->unit_address, desc.desc);
+		    
+    if(lpar_rc != H_Success) {
+	ibmveth_debug_printk("h_add_logical_lan_buffer failed during recycle rc=%ld", lpar_rc);
+	ibmveth_remove_buffer_from_pool(adapter, adapter->rx_queue.queue_addr[adapter->rx_queue.index].correlator);
+    }
+
+    if(++adapter->rx_queue.index == adapter->rx_queue.num_slots) {
+	adapter->rx_queue.index = 0;
+	adapter->rx_queue.toggle = !adapter->rx_queue.toggle;
+    }
+}
+
+static inline void ibmveth_rxq_harvest_buffer(struct ibmveth_adapter *adapter)
+{
+    ibmveth_remove_buffer_from_pool(adapter, adapter->rx_queue.queue_addr[adapter->rx_queue.index].correlator);
+
+    if(++adapter->rx_queue.index == adapter->rx_queue.num_slots) {
+	adapter->rx_queue.index = 0;
+	adapter->rx_queue.toggle = !adapter->rx_queue.toggle;
+    }
+}
+
+static void ibmveth_cleanup(struct ibmveth_adapter *adapter)
+{
+    if(adapter->buffer_list_addr != NULL) {
+	if(adapter->buffer_list_dma != NO_TCE) {
+	    vio_unmap_single(adapter->vdev, adapter->buffer_list_dma, 4096, PCI_DMA_BIDIRECTIONAL);
+	    adapter->buffer_list_dma = NO_TCE;
+	}
+	free_page((unsigned long)adapter->buffer_list_addr);
+	adapter->buffer_list_addr = NULL;
+    } 
+
+    if(adapter->filter_list_addr != NULL) {
+	if(adapter->filter_list_dma != NO_TCE) {
+	    vio_unmap_single(adapter->vdev, adapter->filter_list_dma, 4096, PCI_DMA_BIDIRECTIONAL);
+	    adapter->filter_list_dma = NO_TCE;
+	}
+	free_page((unsigned long)adapter->filter_list_addr);
+	adapter->filter_list_addr = NULL;
+    }
+
+    if(adapter->rx_queue.queue_addr != NULL) {
+	if(adapter->rx_queue.queue_dma != NO_TCE) {
+	    vio_unmap_single(adapter->vdev, adapter->rx_queue.queue_dma, adapter->rx_queue.queue_len, PCI_DMA_BIDIRECTIONAL);
+	    adapter->rx_queue.queue_dma = NO_TCE;
+	}
+	kfree(adapter->rx_queue.queue_addr);
+	adapter->rx_queue.queue_addr = NULL;
+    }
+
+    ibmveth_free_buffer_pool(adapter, &adapter->rx_buff_pool[0]);
+    ibmveth_free_buffer_pool(adapter, &adapter->rx_buff_pool[1]);
+    ibmveth_free_buffer_pool(adapter, &adapter->rx_buff_pool[2]);
+}
+
+static int ibmveth_open(struct net_device *netdev)
+{
+    struct ibmveth_adapter *adapter = netdev->priv;
+    u64 mac_address = 0;
+    int rxq_entries;
+    unsigned long lpar_rc;
+    union ibmveth_buf_desc rxq_desc;
+
+    ibmveth_debug_printk("open starting\n");
+
+    rxq_entries =
+      adapter->rx_buff_pool[0].size +
+      adapter->rx_buff_pool[1].size +
+      adapter->rx_buff_pool[2].size + 1;
+    
+    adapter->buffer_list_addr = (void*) get_zeroed_page(GFP_KERNEL);
+    adapter->filter_list_addr = (void*) get_zeroed_page(GFP_KERNEL);
+ 
+    if(!adapter->buffer_list_addr || !adapter->filter_list_addr) {
+	ibmveth_error_printk("unable to allocate filter or buffer list pages\n");
+	ibmveth_cleanup(adapter);
+	return -ENOMEM;
+    }
+
+    adapter->rx_queue.queue_len = sizeof(struct ibmveth_rx_q_entry) * rxq_entries;
+    adapter->rx_queue.queue_addr = kmalloc(adapter->rx_queue.queue_len, GFP_KERNEL);
+
+    if(!adapter->rx_queue.queue_addr) {
+	ibmveth_error_printk("unable to allocate rx queue pages\n");
+	ibmveth_cleanup(adapter);
+	return -ENOMEM;
+    }
+
+    adapter->buffer_list_dma = vio_map_single(adapter->vdev, adapter->buffer_list_addr, 4096, PCI_DMA_BIDIRECTIONAL);
+    adapter->filter_list_dma = vio_map_single(adapter->vdev, adapter->filter_list_addr, 4096, PCI_DMA_BIDIRECTIONAL);
+    adapter->rx_queue.queue_dma = vio_map_single(adapter->vdev, adapter->rx_queue.queue_addr, adapter->rx_queue.queue_len, PCI_DMA_BIDIRECTIONAL);
+
+    if((adapter->buffer_list_dma == NO_TCE) || 
+       (adapter->filter_list_dma == NO_TCE) || 
+       (adapter->rx_queue.queue_dma == NO_TCE)) {
+	ibmveth_error_printk("unable to map filter or buffer list pages\n");
+	ibmveth_cleanup(adapter);
+	return -ENOMEM;
+    }
+
+    adapter->rx_queue.index = 0;
+    adapter->rx_queue.num_slots = rxq_entries;
+    adapter->rx_queue.toggle = 1;
+
+    if(ibmveth_alloc_buffer_pool(&adapter->rx_buff_pool[0]) ||
+       ibmveth_alloc_buffer_pool(&adapter->rx_buff_pool[1]) ||
+       ibmveth_alloc_buffer_pool(&adapter->rx_buff_pool[2]))
+    {
+	ibmveth_error_printk("unable to allocate buffer pools\n");
+	ibmveth_cleanup(adapter);
+	return -ENOMEM;
+    }
+
+    atomic_set(&adapter->in_replenish_task, 0);    
+
+    memcpy(&mac_address, netdev->dev_addr, netdev->addr_len);
+    mac_address = mac_address >> 16;
+
+    rxq_desc.desc = 0;
+    rxq_desc.fields.valid = 1;
+    rxq_desc.fields.length = adapter->rx_queue.queue_len;
+    rxq_desc.fields.address = adapter->rx_queue.queue_dma;
+
+    ibmveth_debug_printk("buffer list @ 0x%p\n", adapter->buffer_list_addr);
+    ibmveth_debug_printk("filter list @ 0x%p\n", adapter->filter_list_addr);
+    ibmveth_debug_printk("receive q   @ 0x%p\n", adapter->rx_queue.queue_addr);
+
+    
+    lpar_rc = h_register_logical_lan(adapter->vdev->unit_address,
+				     adapter->buffer_list_dma,
+				     rxq_desc.desc,
+				     adapter->filter_list_dma,
+				     mac_address);
+
+    if(lpar_rc != H_Success) {
+	ibmveth_error_printk("h_register_logical_lan failed with %ld\n", lpar_rc);
+	ibmveth_error_printk("buffer TCE:0x%x filter TCE:0x%x rxq desc:0x%lx MAC:0x%lx\n",
+			     adapter->buffer_list_dma,
+			     adapter->filter_list_dma,
+			     rxq_desc.desc,
+			     mac_address);
+	ibmveth_cleanup(adapter);
+	return -ENONET; /* TODO: what error should we use? */
+    }
+
+    ibmveth_debug_printk("registering irq 0x%x\n", netdev->irq);
+    if(request_irq(netdev->irq, &ibmveth_interrupt, 0, netdev->name, netdev) != 0) {
+	ibmveth_error_printk("unable to request irq 0x%x\n", netdev->irq);
+	h_free_logical_lan(adapter->vdev->unit_address);
+	ibmveth_cleanup(adapter);
+	return -ENONET; /* TODO: what errno should this be? */
+    }
+
+    ibmveth_debug_printk("scheduling initial replenish cycle\n");
+    ibmveth_schedule_replenishing(adapter);
+
+    ibmveth_debug_printk("open complete\n");
+
+    return 0;
+}
+
+static int ibmveth_close(struct net_device *netdev)
+{
+    struct ibmveth_adapter *adapter = netdev->priv;
+    long lpar_rc;
+    
+    ibmveth_debug_printk("close starting\n");
+
+    free_irq(netdev->irq, netdev);
+
+    while(atomic_read(&adapter->in_replenish_task)) {
+	udelay(100);
+    }
+
+    lpar_rc = h_free_logical_lan(adapter->vdev->unit_address);
+
+    if(lpar_rc != H_Success)
+    {
+	ibmveth_error_printk("h_free_logical_lan failed with %lx, continuing with close\n",
+			     lpar_rc);
+    }
+
+    adapter->rx_no_buffer = *(u64*)(((char*)adapter->buffer_list_addr) + 4096 - 8);
+
+    ibmveth_cleanup(adapter);
+
+    ibmveth_debug_printk("close complete\n");
+
+    return 0;
+}
+
+static int ibmveth_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
+{
+#ifdef SIOCETHTOOL
+    struct ibmveth_adapter *adapter = dev->priv;
+    u32 ethtool_command; 
+
+    if (cmd != SIOCETHTOOL) {
+	ibmveth_debug_printk("unsupported ioctl 0x%xd\n", cmd);
+ 	return -EOPNOTSUPP;
+    }
+
+    if(get_user(ethtool_command, (u32*)ifr->ifr_data)) {
+	ibmveth_debug_printk("unable to copy from user memory\n");
+	return -EFAULT;
+    }
+
+    ibmveth_debug_printk("SIOCETHTOOL cmd %d\n", ethtool_command); 
+    switch (ethtool_command) {
+	case ETHTOOL_GSET: {
+		struct ethtool_cmd ecmd = {ETHTOOL_GSET};
+		ecmd.supported = (SUPPORTED_1000baseT_Full | SUPPORTED_Autoneg | SUPPORTED_FIBRE);
+		ecmd.advertising = (ADVERTISED_1000baseT_Full | ADVERTISED_Autoneg | ADVERTISED_FIBRE);
+		ecmd.speed = SPEED_1000;
+		ecmd.duplex = DUPLEX_FULL;
+		ecmd.port = PORT_FIBRE;
+		ecmd.phy_address = 0;
+		ecmd.transceiver = XCVR_INTERNAL;
+		ecmd.autoneg = AUTONEG_ENABLE;
+		ecmd.maxtxpkt = 0;
+		ecmd.maxrxpkt = 1;
+		if(copy_to_user(ifr->ifr_data, &ecmd, sizeof(ecmd)))
+		    return -EFAULT;
+		return 0;
+	    }	
+
+	case ETHTOOL_GDRVINFO: {
+		struct ethtool_drvinfo info = {ETHTOOL_GDRVINFO};
+		strncpy(info.driver, ibmveth_driver_name, sizeof(info.driver) - 1);
+		strncpy(info.version, ibmveth_driver_version, sizeof(info.version) - 1);
+
+		if (copy_to_user(ifr->ifr_data, &info, sizeof(info)))
+		    return -EFAULT;
+		return 0;
+	    }
+
+	case ETHTOOL_GLINK: {
+		struct ethtool_value edata = {ETHTOOL_GLINK};
+
+		if (copy_to_user(ifr->ifr_data, &edata, sizeof(edata)))
+		    return -EFAULT;
+		return 0;
+	    }
+
+	default:
+	    break;
+    }
+#endif
+    return -EOPNOTSUPP;
+}
+
+#define page_offset(v) ((unsigned long)(v) & ((1 << 12) - 1))
+
+static int ibmveth_start_xmit(struct sk_buff *skb, struct net_device *netdev)
+{
+    struct ibmveth_adapter *adapter = netdev->priv;
+    union ibmveth_buf_desc desc[IbmVethMaxSendFrags];
+    unsigned long lpar_rc;
+    int nfrags = 0, curfrag;
+
+    if ((skb_shinfo(skb)->nr_frags + 1) > IbmVethMaxSendFrags) {
+	adapter->tx_linearized++;
+	if(skb_linearize(skb, GFP_ATOMIC) != 0)
+	{
+	    ibmveth_debug_printk("tx: unable to linearize skb\n");
+	    adapter->tx_linearize_failed++;
+	    adapter->stats.tx_dropped++;
+	    dev_kfree_skb(skb);
+	    return 0;
+	}
+    }
+
+    memset(&desc, 0, sizeof(desc));
+
+    /* nfrags = number of frags after the initial fragment */
+    nfrags = skb_shinfo(skb)->nr_frags;
+
+    if(nfrags)
+	adapter->tx_multidesc_send++;
+
+    /* map the initial fragment */
+    desc[0].fields.length  = nfrags ? skb->len - skb->data_len : skb->len;
+    desc[0].fields.address = vio_map_single(adapter->vdev, skb->data, desc[0].fields.length, PCI_DMA_TODEVICE);
+    desc[0].fields.valid   = 1;
+
+    if(desc[0].fields.address == NO_TCE) {
+	ibmveth_error_printk("tx: unable to map initial fragment\n");
+	adapter->tx_map_failed++;
+	adapter->stats.tx_dropped++;
+	return 0;
+    }
+
+    curfrag = nfrags;
+
+    /* map fragments past the initial portion if there are any */
+    while(curfrag--) {
+	skb_frag_t *frag = &skb_shinfo(skb)->frags[curfrag];
+	desc[curfrag+1].fields.address = vio_map_single(adapter->vdev,
+							page_address(frag->page) + frag->page_offset,
+							frag->size, PCI_DMA_TODEVICE);
+	desc[curfrag+1].fields.length = frag->size;
+	desc[curfrag+1].fields.valid  = 1;
+
+	if(desc[curfrag+1].fields.address == NO_TCE) {
+	    ibmveth_error_printk("tx: unable to map fragment %d\n", curfrag);
+	    adapter->tx_map_failed++;
+	    adapter->stats.tx_dropped++;
+	    while(curfrag) {
+		vio_unmap_single(adapter->vdev,
+				 desc[curfrag].fields.address,
+				 desc[curfrag].fields.length,
+				 PCI_DMA_TODEVICE);
+		curfrag--;
+	    }
+	    return 0;
+	}
+    }
+
+    /* send the frame */
+    unsigned long correlator = 0;
+    do {
+	lpar_rc = h_send_logical_lan(adapter->vdev->unit_address,
+				     desc[0].desc,
+				     desc[1].desc,
+				     desc[2].desc,
+				     desc[3].desc,
+				     desc[4].desc,
+				     desc[5].desc,
+				     correlator);
+     } while(lpar_rc == H_Busy);
+
+    
+    if(lpar_rc != H_Success && lpar_rc != H_Dropped) {
+	int i;
+	ibmveth_error_printk("tx: h_send_logical_lan failed with rc=%ld\n", lpar_rc);
+	for(i = 0; i < 6; i++) {
+	    ibmveth_error_printk("tx: desc[%i] valid=%d, len=%d, address=0x%d\n", i,
+				 desc[i].fields.valid, desc[i].fields.length, desc[i].fields.address);
+	}
+	adapter->tx_send_failed++;
+	adapter->stats.tx_dropped++;
+    } else {
+	adapter->stats.tx_packets++;
+	adapter->stats.tx_bytes += skb->len;
+    }
+
+    do {
+	vio_unmap_single(adapter->vdev, desc[nfrags].fields.address, desc[nfrags].fields.length, PCI_DMA_TODEVICE);
+    } while(--nfrags >= 0);
+
+    dev_kfree_skb(skb);
+    return 0;
+}
+
+static int ibmveth_poll(struct net_device *netdev, int *budget)
+{
+    struct ibmveth_adapter *adapter = netdev->priv;
+    int max_frames_to_process = netdev->quota;
+    int frames_processed = 0;
+    int more_work = 1;
+    unsigned long lpar_rc;
+
+restart_poll:
+    do {
+	struct net_device *netdev = adapter->netdev;
+
+	if(ibmveth_rxq_pending_buffer(adapter)) {
+	    struct sk_buff *skb;
+
+	    if(!ibmveth_rxq_buffer_valid(adapter)) {
+		wmb();
+		adapter->rx_invalid_buffer++;
+		ibmveth_debug_printk("recycling invalid buffer\n");
+		ibmveth_rxq_recycle_buffer(adapter);
+	    } else {
+		int length = ibmveth_rxq_frame_length(adapter);
+		int offset = ibmveth_rxq_frame_offset(adapter);
+		skb = ibmveth_rxq_get_buffer(adapter);
+
+		ibmveth_rxq_harvest_buffer(adapter);
+
+		skb_reserve(skb, offset);
+		skb_put(skb, length);
+		skb->dev = netdev;
+		skb->protocol = eth_type_trans(skb, netdev);
+
+		netif_receive_skb(skb);	/* send it up */
+
+		adapter->stats.rx_packets++;
+		adapter->stats.rx_bytes += length;
+		frames_processed++;
+	    }
+	} else {
+	    more_work = 0;
+	}
+    } while(more_work && (frames_processed < max_frames_to_process));
+
+    ibmveth_schedule_replenishing(adapter);
+
+    if(more_work) {
+	/* more work to do - return that we are not done yet */
+	netdev->quota -= frames_processed;
+	*budget -= frames_processed;
+	return 1; 
+    }
+
+    /* we think we are done - reenable interrupts, then check once more to make sure we are done */
+    lpar_rc = h_vio_signal(adapter->vdev->unit_address, VIO_IRQ_ENABLE);
+
+    ibmveth_assert(lpar_rc == H_Success);
+    
+    netif_rx_complete(netdev);
+
+    if(ibmveth_rxq_pending_buffer(adapter) && netif_rx_reschedule(netdev, frames_processed))
+    {
+	lpar_rc = h_vio_signal(adapter->vdev->unit_address, VIO_IRQ_DISABLE);
+	ibmveth_assert(lpar_rc == H_Success);
+	more_work = 1;
+	goto restart_poll;
+    }
+
+    /* we really are done */
+    return 0;
+}
+
+static irqreturn_t ibmveth_interrupt(int irq, void *dev_instance, struct pt_regs *regs)
+{   
+    struct net_device *netdev = dev_instance;
+    struct ibmveth_adapter *adapter = netdev->priv;
+    unsigned long lpar_rc;
+
+    if(netif_rx_schedule_prep(netdev)) {
+	lpar_rc = h_vio_signal(adapter->vdev->unit_address, VIO_IRQ_DISABLE);
+	ibmveth_assert(lpar_rc == H_Success);
+	__netif_rx_schedule(netdev);
+    }
+	return IRQ_HANDLED;
+}
+
+static struct net_device_stats *ibmveth_get_stats(struct net_device *dev)
+{
+    struct ibmveth_adapter *adapter = dev->priv;
+    return &adapter->stats;
+}
+
+static void ibmveth_set_multicast_list(struct net_device *netdev)
+{
+    struct ibmveth_adapter *adapter = netdev->priv;
+    unsigned long lpar_rc;
+
+    if((netdev->flags & IFF_PROMISC) || (netdev->mc_count > adapter->mcastFilterSize)) {
+	lpar_rc = h_multicast_ctrl(adapter->vdev->unit_address,
+				   IbmVethMcastEnableRecv |
+				   IbmVethMcastDisableFiltering,
+				   0);
+	if(lpar_rc != H_Success) {
+	    ibmveth_error_printk("h_multicast_ctrl rc=%ld when entering promisc mode\n", lpar_rc);
+	}
+    } else {
+	struct dev_mc_list *mclist = netdev->mc_list;
+	int i;
+	/* clear the filter table & disable filtering */
+	lpar_rc = h_multicast_ctrl(adapter->vdev->unit_address,
+				   IbmVethMcastEnableRecv |
+				   IbmVethMcastDisableFiltering |
+				   IbmVethMcastClearFilterTable,
+				   0);
+	if(lpar_rc != H_Success) {
+	    ibmveth_error_printk("h_multicast_ctrl rc=%ld when attempting to clear filter table\n", lpar_rc);
+	}
+	/* add the addresses to the filter table */
+	for(i = 0; i < netdev->mc_count; ++i, mclist = mclist->next) {
+	    // add the multicast address to the filter table
+	      unsigned long mcast_addr = 0;
+	      memcpy(((char *)&mcast_addr)+2, mclist->dmi_addr, 6);
+	      lpar_rc = h_multicast_ctrl(adapter->vdev->unit_address,
+					 IbmVethMcastAddFilter,
+					 mcast_addr);
+	    if(lpar_rc != H_Success) {
+		ibmveth_error_printk("h_multicast_ctrl rc=%ld when adding an entry to the filter table\n", lpar_rc);
+	    }
+	}
+	
+	/* re-enable filtering */
+	lpar_rc = h_multicast_ctrl(adapter->vdev->unit_address,
+				   IbmVethMcastEnableFiltering,
+				   0);
+	if(lpar_rc != H_Success) {
+	    ibmveth_error_printk("h_multicast_ctrl rc=%ld when enabling filtering\n", lpar_rc);
+	}
+    }
+}
+
+static int ibmveth_set_mac_address(struct net_device *netdev, void *p)
+{
+    struct ibmveth_adapter *adapter = netdev->priv;
+    struct sockaddr *addr = p;
+    unsigned long mac_address;
+    unsigned long lpar_rc;
+
+    if(!is_valid_ether_addr(addr->sa_data))
+	return -EADDRNOTAVAIL;
+
+    memcpy(&mac_address, addr->sa_data, netdev->addr_len);
+    mac_address = mac_address >> 16;
+
+    lpar_rc = h_change_logical_lan_mac(adapter->vdev->unit_address, mac_address);
+
+    if(lpar_rc != H_Success) {
+	ibmveth_error_printk("h_change_logical_lan_mac rc=%ld when changing mac\n", lpar_rc);
+	return -1;
+    }
+
+    memcpy(netdev->dev_addr, addr->sa_data, netdev->addr_len);
+
+    return 0;
+}
+
+static int __devinit ibmveth_probe(struct vio_dev *dev, const struct vio_device_id *id)
+{
+	int rc;
+	struct net_device *netdev;
+	struct ibmveth_adapter *adapter;
+
+	unsigned int *mac_addr_p;
+	unsigned int *mcastFilterSize_p;
+
+
+	ibmveth_debug_printk_no_adapter("entering ibmveth_probe for UA 0x%lx\n", 
+				    dev->unit_address);
+
+	mac_addr_p = (unsigned int *) vio_get_attribute(dev, VETH_MAC_ADDR, 0);
+	if(!mac_addr_p) {
+		ibmveth_printk(KERN_WARNING "Can't find VETH_MAC_ADDR attribute\n");
+		return 0;
+	}
+	
+	mcastFilterSize_p= (unsigned int *) vio_get_attribute(dev, VETH_MCAST_FILTER_SIZE, 0);
+	if(!mcastFilterSize_p) {
+		ibmveth_printk(KERN_WARNING "Can't find VETH_MCAST_FILTER_SIZE attribute\n");
+		return 0;
+	}
+	
+	netdev = alloc_etherdev(sizeof(struct ibmveth_adapter));
+
+	if(!netdev)
+		return -ENOMEM;
+
+	SET_MODULE_OWNER(netdev);
+
+	adapter = netdev->priv;
+	memset(adapter, 0, sizeof(adapter));
+	dev->driver_data = netdev;
+
+	adapter->vdev = dev;
+	adapter->netdev = netdev;
+	adapter->mcastFilterSize= *mcastFilterSize_p;
+	
+	/* 	Some older boxes running PHYP non-natively have an OF that
+		returns a 8-byte local-mac-address field (and the first 
+		2 bytes have to be ignored) while newer boxes' OF return
+		a 6-byte field. Note that IEEE 1275 specifies that 
+		local-mac-address must be a 6-byte field.
+		The RPA doc specifies that the first byte must be 10b, so 
+		we'll just look for it to solve this 8 vs. 6 byte field issue */
+
+	while (*((char*)mac_addr_p) != (char)(0x02))
+		((char*)mac_addr_p)++;
+
+	adapter->mac_addr = 0;
+	memcpy(&adapter->mac_addr, mac_addr_p, 6);
+
+	adapter->liobn = dev->tce_table->index;
+	
+	netdev->irq = dev->irq;
+	netdev->mtu = 9000;
+	netdev->open               = ibmveth_open;
+	netdev->poll               = ibmveth_poll;
+	netdev->weight             = 16;
+	netdev->stop               = ibmveth_close;
+	netdev->hard_start_xmit    = ibmveth_start_xmit;
+	netdev->get_stats          = ibmveth_get_stats;
+	netdev->set_multicast_list = ibmveth_set_multicast_list;
+	netdev->set_mac_address    = ibmveth_set_mac_address;
+	netdev->do_ioctl           = ibmveth_ioctl;
+
+	memcpy(&netdev->dev_addr, &adapter->mac_addr, netdev->addr_len);
+
+	ibmveth_init_buffer_pool(&adapter->rx_buff_pool[0], 0, IbmVethPool0DftCnt, IbmVethPool0DftSize);
+	ibmveth_init_buffer_pool(&adapter->rx_buff_pool[1], 1, IbmVethPool1DftCnt, IbmVethPool1DftSize);
+	ibmveth_init_buffer_pool(&adapter->rx_buff_pool[2], 2, IbmVethPool2DftCnt, IbmVethPool2DftSize);
+
+	ibmveth_debug_printk("adapter @ 0x%p\n", adapter);
+
+	INIT_BOTTOM_HALF(&adapter->replenish_task, (void*)ibmveth_replenish_task, adapter);
+
+	adapter->buffer_list_dma = NO_TCE;
+	adapter->filter_list_dma = NO_TCE;
+	adapter->rx_queue.queue_dma = NO_TCE;
+
+	ibmveth_debug_printk("registering netdev...\n");
+
+	rc = register_netdev(netdev);
+
+	if(rc) {
+		ibmveth_debug_printk("failed to register netdev rc=%d\n", rc);
+		kfree(dev);
+		return rc;
+	}
+
+	ibmveth_debug_printk("registered\n");
+
+	ibmveth_proc_register_adapter(adapter);
+
+	return 0;
+}
+
+static void __devexit ibmveth_remove(struct vio_dev *dev)
+{
+    struct net_device *netdev = dev->driver_data;
+    struct ibmveth_adapter *adapter = netdev->priv;
+
+    unregister_netdev(netdev);
+
+    ibmveth_proc_unregister_adapter(adapter);
+
+    kfree(netdev);
+    return;
+}
+
+#ifdef CONFIG_PROC_FS
+static int ibmveth_proc_read(char *page, char **start, off_t off, int count, int *eof, void *data)
+{
+    struct ibmveth_adapter *adapter = data;
+    char *buf = page;
+    char *current_mac = ((char*) &adapter->netdev->dev_addr);
+    char *firmware_mac = ((char*) &adapter->mac_addr) ;
+    int len = 0;
+
+    buf += sprintf(buf, "%s %s\n\n", ibmveth_driver_string, ibmveth_driver_version);
+    buf += sprintf(buf, "Detailed Version Info: %s %s\n\n", ibmveth_driver_cvs_tag, ibmveth_driver_cvs_version);
+
+    buf += sprintf(buf, "Unit Address:    0x%lx\n", adapter->vdev->unit_address);
+    buf += sprintf(buf, "LIOBN:           0x%lx\n", adapter->liobn);
+    buf += sprintf(buf, "Current MAC:     %02X:%02X:%02X:%02X:%02X:%02X\n",
+		   current_mac[0], current_mac[1], current_mac[2],
+		   current_mac[3], current_mac[4], current_mac[5]);
+    buf += sprintf(buf, "Firmware MAC:    %02X:%02X:%02X:%02X:%02X:%02X\n",
+		   firmware_mac[0], firmware_mac[1], firmware_mac[2],
+		   firmware_mac[3], firmware_mac[4], firmware_mac[5]);
+
+    buf += sprintf(buf, "\nAdapter Statistics:\n");
+    buf += sprintf(buf, "  TX:  skbuffs linearized:          %ld\n", adapter->tx_linearized);
+    buf += sprintf(buf, "       multi-descriptor sends:      %ld\n", adapter->tx_multidesc_send);
+    buf += sprintf(buf, "       skb_linearize failures:      %ld\n", adapter->tx_linearize_failed);
+    buf += sprintf(buf, "       vio_map_single failres:      %ld\n", adapter->tx_map_failed);
+    buf += sprintf(buf, "       send failures:               %ld\n", adapter->tx_send_failed);
+    buf += sprintf(buf, "  RX:  replenish task cycles:       %ld\n", adapter->replenish_task_cycles);
+    buf += sprintf(buf, "       alloc_skb_failures:          %ld\n", adapter->replenish_no_mem);
+    buf += sprintf(buf, "       add buffer failures:         %ld\n", adapter->replenish_add_buff_failure);
+    buf += sprintf(buf, "       invalid buffers:             %ld\n", adapter->rx_invalid_buffer);
+    buf += sprintf(buf, "       no buffers:                  %ld\n", adapter->rx_no_buffer);
+
+    len = buf - page;    /* len = bytes in this proc file */
+
+    len -= off;          /* take the offset off the front of the read */
+    if(len < count) {    /* if they want more than we have, then this is EOF */
+	*eof = 1;
+	if(len <= 0) {   /* if len -= off is < 0, then they are reading past EOF, return 0 bytes */
+	    return 0;
+	}
+    } else {
+	len = count;     /* only return what they asked for */
+    }
+    *start = page + off; /* set the start ptr to the beginning of what they asked for */
+    return len;
+}
+#endif
+
+static void ibmveth_proc_register_driver(void)
+{
+#ifdef CONFIG_PROC_FS
+    ibmveth_proc_dir = create_proc_entry(IBMVETH_PROC_DIR, S_IFDIR, proc_net);
+    SET_MODULE_OWNER(ibmveth_proc_dir);
+#endif
+}
+
+static void ibmveth_proc_unregister_driver(void)
+{
+#ifdef CONFIG_PROC_FS
+    remove_proc_entry(IBMVETH_PROC_DIR, proc_net);
+#endif
+}
+
+static void ibmveth_proc_register_adapter(struct ibmveth_adapter *adapter)
+{
+#ifdef CONFIG_PROC_FS
+    struct proc_dir_entry *entry;
+    entry = create_proc_entry(adapter->netdev->name, S_IFREG, ibmveth_proc_dir);
+    SET_MODULE_OWNER(entry);
+    entry->data = (void *) adapter;
+    entry->read_proc = ibmveth_proc_read;
+#endif
+}
+
+static void ibmveth_proc_unregister_adapter(struct ibmveth_adapter *adapter)
+{
+#ifdef CONFIG_PROC_FS
+    remove_proc_entry(adapter->netdev->name, ibmveth_proc_dir);
+#endif
+}
+
+
+static struct vio_device_id ibmveth_device_table[] __devinitdata= {
+    { "network", "IBM,l-lan"},
+    { 0,}
+};
+
+MODULE_DEVICE_TABLE(vio, ibmveth_device_table);
+
+static struct vio_driver ibmveth_driver = {
+    .name        = ibmveth_driver_name,
+    .id_table    = ibmveth_device_table,
+    .probe       = ibmveth_probe,
+    .remove      = ibmveth_remove
+};
+
+static int __init ibmveth_module_init(void)
+{
+    int rc;
+
+    ibmveth_printk("%s: %s %s\n", ibmveth_driver_name, ibmveth_driver_string, ibmveth_driver_version);
+
+    ibmveth_proc_register_driver();
+
+    rc = vio_module_init(&ibmveth_driver);
+
+    return rc;
+}
+
+static void __exit ibmveth_module_exit(void)
+{
+	vio_unregister_driver(&ibmveth_driver);
+	ibmveth_proc_unregister_driver();
+}	
+
+module_init(ibmveth_module_init);
+module_exit(ibmveth_module_exit);
diff -purN linux-2.5/drivers/net/ibmveth.h linuxppc64-2.5/drivers/net/ibmveth.h
--- linux-2.5/drivers/net/ibmveth.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/net/ibmveth.h	2003-12-04 18:25:53.000000000 +0000
@@ -0,0 +1,158 @@
+/**************************************************************************/
+/*                                                                        */
+/* IBM eServer i/[Series Virtual Ethernet Device Driver                   */
+/* Copyright (C) 2003 IBM Corp.                                           */
+/*  Dave Larson (larson1@us.ibm.com)                                      */
+/*  Santiago Leon (santil@us.ibm.com)                                     */
+/*                                                                        */
+/*  This program is free software; you can redistribute it and/or modify  */
+/*  it under the terms of the GNU General Public License as published by  */
+/*  the Free Software Foundation; either version 2 of the License, or     */
+/*  (at your option) any later version.                                   */
+/*                                                                        */
+/*  This program is distributed in the hope that it will be useful,       */
+/*  but WITHOUT ANY WARRANTY; without even the implied warranty of        */
+/*  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the         */
+/*  GNU General Public License for more details.                          */
+/*                                                                        */
+/*  You should have received a copy of the GNU General Public License     */
+/*  along with this program; if not, write to the Free Software           */
+/*  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  */
+/*                                                                   USA  */
+/*                                                                        */
+/**************************************************************************/
+
+#ifndef _IBMVETH_H
+#define _IBMVETH_H
+
+#define IbmVethMaxSendFrags 6
+
+/* constants for H_MULTICAST_CTRL */
+#define IbmVethMcastReceptionModifyBit     0x80000UL
+#define IbmVethMcastReceptionEnableBit     0x20000UL
+#define IbmVethMcastFilterModifyBit        0x40000UL
+#define IbmVethMcastFilterEnableBit        0x10000UL
+
+#define IbmVethMcastEnableRecv       (IbmVethMcastReceptionModifyBit | IbmVethMcastReceptionEnableBit)
+#define IbmVethMcastDisableRecv      (IbmVethMcastReceptionModifyBit)
+#define IbmVethMcastEnableFiltering  (IbmVethMcastFilterModifyBit | IbmVethMcastFilterEnableBit)
+#define IbmVethMcastDisableFiltering (IbmVethMcastFilterModifyBit)
+#define IbmVethMcastAddFilter        0x1UL
+#define IbmVethMcastRemoveFilter     0x2UL
+#define IbmVethMcastClearFilterTable 0x3UL
+
+/* hcall numbers */
+#define H_VIO_SIGNAL             0x104
+#define H_REGISTER_LOGICAL_LAN   0x114
+#define H_FREE_LOGICAL_LAN       0x118
+#define H_ADD_LOGICAL_LAN_BUFFER 0x11C
+#define H_SEND_LOGICAL_LAN       0x120
+#define H_MULTICAST_CTRL         0x130
+#define H_CHANGE_LOGICAL_LAN_MAC 0x14C
+
+/* hcall macros */
+#define h_register_logical_lan(ua, buflst, rxq, fltlst, mac) \
+  plpar_hcall_norets(H_REGISTER_LOGICAL_LAN, ua, buflst, rxq, fltlst, mac)
+
+#define h_free_logical_lan(ua) \
+  plpar_hcall_norets(H_FREE_LOGICAL_LAN, ua)
+
+#define h_add_logical_lan_buffer(ua, buf) \
+  plpar_hcall_norets(H_ADD_LOGICAL_LAN_BUFFER, ua, buf)
+
+#define h_send_logical_lan(ua, buf1, buf2, buf3, buf4, buf5, buf6, correlator) \
+  plpar_hcall_8arg_2ret(H_SEND_LOGICAL_LAN, ua, buf1, buf2, buf3, buf4, buf5, buf6, correlator, &correlator)
+
+#define h_multicast_ctrl(ua, cmd, mac) \
+  plpar_hcall_norets(H_MULTICAST_CTRL, ua, cmd, mac)
+
+#define h_change_logical_lan_mac(ua, mac) \
+  plpar_hcall_norets(H_CHANGE_LOGICAL_LAN_MAC, ua, mac)
+
+#define IbmVethNumBufferPools 3
+#define IbmVethPool0DftSize (1024 * 2)
+#define IbmVethPool1DftSize (1024 * 4)
+#define IbmVethPool2DftSize (1024 * 10)
+#define IbmVethPool0DftCnt  256
+#define IbmVethPool1DftCnt  256
+#define IbmVethPool2DftCnt  256
+
+struct ibmveth_buff_pool {
+    u32 size;
+    u32 index;
+    u32 buff_size;
+    u32 threshold;
+    atomic_t available;
+    u32 consumer_index;
+    u32 producer_index;
+    u16 *free_map;
+    dma_addr_t *dma_addr;
+    struct sk_buff **skbuff;
+};
+
+struct ibmveth_rx_q {
+    u64        index;
+    u64        num_slots;
+    u64        toggle;
+    dma_addr_t queue_dma;
+    u32        queue_len;
+    struct ibmveth_rx_q_entry *queue_addr;
+};
+
+struct ibmveth_adapter {
+    struct vio_dev *vdev;
+    struct net_device *netdev;
+    struct net_device_stats stats;
+    unsigned int mcastFilterSize;
+    unsigned long mac_addr;
+    unsigned long liobn;
+    void * buffer_list_addr;
+    void * filter_list_addr;
+    dma_addr_t buffer_list_dma;
+    dma_addr_t filter_list_dma;
+    struct ibmveth_buff_pool rx_buff_pool[IbmVethNumBufferPools];
+    struct ibmveth_rx_q rx_queue;
+    /* helper tasks */
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,5,0)
+    struct work_struct replenish_task;
+#else    
+    struct tasklet_struct replenish_task;
+#endif
+    atomic_t in_replenish_task;
+    /* adapter specific stats */
+    u64 replenish_task_cycles;
+    u64 replenish_no_mem;
+    u64 replenish_add_buff_failure;
+    u64 replenish_add_buff_success;
+    u64 rx_invalid_buffer;
+    u64 rx_no_buffer;
+    u64 tx_multidesc_send;
+    u64 tx_linearized;
+    u64 tx_linearize_failed;
+    u64 tx_map_failed;
+    u64 tx_send_failed;
+};
+
+struct ibmveth_buf_desc_fields {	
+    u32 valid : 1;
+    u32 toggle : 1;
+    u32 reserved : 6;
+    u32 length : 24;
+    u32 address;
+};
+
+union ibmveth_buf_desc {
+    u64 desc;	
+    struct ibmveth_buf_desc_fields fields;
+};
+
+struct ibmveth_rx_q_entry {
+    u16 toggle : 1;
+    u16 valid : 1;
+    u16 reserved : 14;
+    u16 offset;
+    u32 length;
+    u64 correlator;
+};
+
+#endif /* _IBMVETH_H */
diff -purN linux-2.5/drivers/net/iseries_veth.c linuxppc64-2.5/drivers/net/iseries_veth.c
--- linux-2.5/drivers/net/iseries_veth.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/net/iseries_veth.c	2003-12-17 03:49:48.000000000 +0000
@@ -0,0 +1,1359 @@
+/* File veth.c created by Kyle A. Lucke on Mon Aug  7 2000. */
+
+/*
+ * IBM eServer iSeries Virtual Ethernet Device Driver
+ * Copyright (C) 2001 Kyle A. Lucke (klucke@us.ibm.com), IBM Corp.
+ * Substantially cleaned up by:
+ * Copyright (C) 2003 David Gibson <dwg@au1.ibm.com>, IBM Corporation.
+ */
+
+/*  This program is free software; you can redistribute it and/or modify  */
+/*  it under the terms of the GNU General Public License as published by  */
+/*  the Free Software Foundation; either version 2 of the License, or     */
+/*  (at your option) any later version.                                   */
+/*                                                                        */
+/*  This program is distributed in the hope that it will be useful,       */
+/*  but WITHOUT ANY WARRANTY; without even the implied warranty of        */
+/*  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the         */
+/*  GNU General Public License for more details.                          */
+/*                                                                        */
+/*  You should have received a copy of the GNU General Public License     */
+/*  along with this program; if not, write to the Free Software           */
+/*  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  */
+/*                                                                   USA  */
+/*                                                                        */
+/* This module contains the implementation of a virtual ethernet device   */
+/* for use with iSeries LPAR Linux.  It utilizes low-level message passing*/
+/* provided by the hypervisor to enable an ethernet-like network device   */
+/* that can be used to enable inter-partition communications on the same  */
+/* physical iSeries.                                                      */
+/*                                                                        */
+/* The iSeries LPAR hypervisor has currently defined the ability for a    */
+/* partition to communicate on up to 16 different virtual ethernets, all  */
+/* dynamically configurable, at least for an OS/400 partition.  The       */
+/* dynamic nature is not supported for Linux yet.                         */
+/*                                                                        */
+/* Each virtual ethernet a given Linux partition participates in will     */
+/* cause a network device with the form ethXX to be created,              */
+/*                                                                        */
+/* This driver (and others like it on other partitions) is responsible for*/
+/* routing packets to and from other partitions.  The MAC addresses used  */
+/* by the virtual ethernets contain meaning, and should not be modified.  */
+/* Doing so could disable the ability of your Linux partition to          */
+/* communicate with the other OS/400 partitions on your physical iSeries. */
+/* Similarly, setting the MAC address to something other than the         */
+/* "virtual burned-in" address is not allowed, for the same reason.       */
+/*                                                                        */
+/* Notes:                                                                 */
+/*                                                                        */
+/* 1. Although there is the capability to talk on multiple shared         */
+/*    ethernets to communicate to the same partition, each shared         */
+/*    ethernet to a given partition X will use a finite, shared amount    */
+/*    of hypervisor messages to do the communication.  So having 2 shared */
+/*    ethernets to the same remote partition DOES NOT double the          */
+/*    available bandwidth.  Each of the 2 shared ethernets will share the */
+/*    same bandwidth available to another.                                */
+/*                                                                        */
+/* 2. It is allowed to have a virtual ethernet that does not communicate  */
+/*    with any other partition.  It won't do anything, but it's allowed.  */
+/*                                                                        */
+/* 3. There is no "loopback" mode for a virtual ethernet device.  If you  */
+/*    send a packet to your own mac address, it will just be dropped, you */
+/*    won't get it on the receive side.  Such a thing could be done,      */
+/*    but my default driver DOES NOT do so.                               */
+/*                                                                        */
+/* 4. Multicast addressing is implemented via broadcasting the multicast  */
+/*    frames to other partitions.  It is the responsibility of the        */
+/*    receiving partition to filter the addresses desired.                */
+/*                                                                        */
+/* 5. This module utilizes several different bottom half handlers for     */
+/*    non-high-use path function (setup, error handling, etc.).  Multiple */
+/*    bottom halves were used because only one would not keep up to the   */
+/*    much faster iSeries device drivers this Linux driver is talking to. */
+/*    All hi-priority work (receiving frames, handling frame acks) is done*/
+/*    in the interrupt handler for maximum performance.                   */
+/*                                                                        */
+/* Tunable parameters:                                                    */
+/*                                                                        */
+/* VETH_NUMBUFFERS: This compile time option defaults to 120. It can      */
+/* be safely changed to something greater or less than the default.  It   */
+/* controls how much memory Linux will allocate per remote partition it is*/
+/* communicating with.  The user can play with this to see how it affects */
+/* performance, packets dropped, etc.  Without trying to understand the   */
+/* complete driver, it can be thought of as the maximum number of packets */
+/* outstanding to a remote partition at a time.                           */
+/*                                                                        */
+/**************************************************************************/
+
+#include <linux/config.h>
+#include <linux/module.h>
+#include <linux/version.h>
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <linux/ioport.h>
+#include <linux/pci.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/skbuff.h>
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/mm.h>
+#ifdef SIOCETHTOOL
+#include <linux/ethtool.h>
+#endif
+#include <asm/iSeries/mf.h>
+#include <asm/uaccess.h>
+
+#include "iseries_veth.h"
+#include <asm/iSeries/HvLpConfig.h>
+#include <asm/iSeries/HvTypes.h>
+#include <asm/iSeries/iSeries_dma.h>
+#include <asm/semaphore.h>
+
+#define veth_printk(prio, fmt, args...) \
+	printk(prio "%s: " fmt, __FILE__, ## args)
+
+#define veth_error(fmt, args...) \
+	printk(KERN_ERR "(%s:%3.3d) ERROR: " fmt, __FILE__, __LINE__ , ## args)
+
+#define VETH_NUMBUFFERS		120
+
+static struct VethFabricMgr *mFabricMgr = NULL;
+
+static int veth_open(struct net_device *dev);
+static int veth_close(struct net_device *dev);
+static int veth_start_xmit(struct sk_buff *skb, struct net_device *dev);
+static int veth_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd);
+static void veth_handle_event(struct HvLpEvent *, struct pt_regs *);
+static void veth_handle_ack(struct VethLpEvent *);
+static void veth_handle_int(struct VethLpEvent *);
+static void veth_init_connection(struct VethLpConnection *cnx, u8 rlp);
+static void veth_open_connection(u8);
+static void veth_finish_open_connection(void *parm);
+static void veth_closeConnection(u8);
+static void veth_set_multicast_list(struct net_device *dev);
+
+static void veth_take_cap(struct VethLpConnection *, struct VethLpEvent *);
+static void veth_take_cap_ack(struct VethLpConnection *, struct VethLpEvent *);
+static void veth_take_monitor_ack(struct VethLpConnection *,
+				  struct VethLpEvent *);
+static void veth_recycle_msg(struct VethLpConnection *, struct VethMsg *);
+static void veth_monitor_ack_task(void *);
+static void veth_receive(struct VethLpConnection *, struct VethLpEvent *);
+static int veth_pTransmit(struct sk_buff *skb, HvLpIndex rlp,
+			  struct net_device *dev);
+static struct net_device_stats *veth_get_stats(struct net_device *dev);
+static void veth_timed_ack(unsigned long connectionPtr);
+static void veth_startQueues(void);
+static void veth_failMe(struct VethLpConnection *cnx);
+
+extern struct pci_dev *iSeries_veth_dev;
+static struct net_device *veth_devices[HVMAXARCHITECTEDVIRTUALLANS];
+static int veth_num_devices; /* = 0 */
+
+#define VETH_MAX_MTU		9000
+
+MODULE_AUTHOR("Kyle Lucke <klucke@us.ibm.com>");
+MODULE_DESCRIPTION("iSeries Virtual ethernet driver");
+MODULE_LICENSE("GPL");
+
+int VethModuleReopen = 1;
+
+static inline u64 veth_dma_addr(void *p)
+{
+	return 0x8000000000000000LL | virt_to_absolute((unsigned long) p);
+}
+
+static inline HvLpEvent_Rc 
+veth_signalevent(struct VethLpConnection *cnx, u16 subtype, 
+		 HvLpEvent_AckInd ackind, HvLpEvent_AckType acktype,
+		 u64 token,
+		 u64 data1, u64 data2, u64 data3, u64 data4, u64 data5)
+{
+	return HvCallEvent_signalLpEventFast(cnx->remote_lp,
+					     HvLpEvent_Type_VirtualLan,
+					     subtype, ackind, acktype,
+					     cnx->src_inst,
+					     cnx->dst_inst,
+					     token, data1, data2, data3,
+					     data4, data5);
+}
+
+static inline HvLpEvent_Rc
+veth_signaldata(struct VethLpConnection *cnx, u16 subtype,
+		u64 token, void *data)
+{
+	u64 *p = (u64 *) data;
+
+	return veth_signalevent(cnx, subtype, HvLpEvent_AckInd_NoAck,
+				HvLpEvent_AckType_ImmediateAck,
+				token, p[0], p[1], p[2], p[3], p[4]);
+}
+
+struct veth_allocation {
+	struct completion c;
+	int num;
+};
+
+static void veth_complete_allocation(void *parm, int number)
+{
+	struct veth_allocation *vc = (struct veth_allocation *)parm;
+
+	vc->num = number;
+	complete(&vc->c);
+}
+
+static int veth_allocate_events(HvLpIndex rlp, int number)
+{
+	struct veth_allocation vc = { COMPLETION_INITIALIZER(vc.c), 0 };
+
+	mf_allocateLpEvents(rlp, HvLpEvent_Type_VirtualLan,
+			    sizeof(struct VethLpEvent), number,
+			    &veth_complete_allocation, &vc);
+	wait_for_completion(&vc.c);
+
+	return vc.num;
+}
+
+struct net_device * __init veth_probe_one(int idx)
+{
+	struct net_device *dev;
+	struct veth_port *port;
+	int rc;
+
+	dev = alloc_etherdev(sizeof (struct veth_port));
+	if (! dev) {
+		veth_error("Unable to allocate net_device structure!\n");
+		return NULL;
+	}
+
+	port = (struct veth_port *) dev->priv;
+
+	memset(port, 0, sizeof(*port));
+	
+	port->mDev = dev;
+	rwlock_init(&port->mcast_gate);
+
+	dev->dev_addr[0] = 0x02;
+	dev->dev_addr[1] = 0x01;
+	dev->dev_addr[2] = 0xff;
+	dev->dev_addr[3] = idx;
+	dev->dev_addr[4] = 0xff;
+	dev->dev_addr[5] = HvLpConfig_getLpIndex_outline();
+
+	dev->mtu = VETH_MAX_MTU;
+
+	memcpy(&port->mMyAddress, dev->dev_addr, 6);
+
+	dev->open = &veth_open;
+	dev->hard_start_xmit = &veth_start_xmit;
+	dev->stop = &veth_close;
+	dev->get_stats = veth_get_stats;
+	dev->set_multicast_list = &veth_set_multicast_list;
+	dev->do_ioctl = &veth_ioctl;
+
+	rc = register_netdev(dev);
+	if (rc != 0) {
+		veth_printk(KERN_ERR,
+			    "Failed to register an ethernet device (veth=%d)\n",
+			    idx);
+		kfree(dev);
+		return NULL;
+	}
+
+	veth_printk(KERN_DEBUG, "Found an ethernet device %s (veth=%d) (addr=%p)\n",
+		    dev->name, idx, dev);
+
+	return dev;
+}
+
+int __init veth_probe(void)
+{
+	int vlans_found = 0;
+	u16 vlan_map = HvLpConfig_getVirtualLanIndexMap();
+	int i;
+
+	memset(veth_devices, 0, sizeof(veth_devices));
+
+	for (i = 0; vlan_map != 0; vlan_map <<= 1, i++) {
+		struct net_device *dev = NULL;
+
+		if (! (vlan_map & 0x8000))
+			continue;
+
+		vlans_found++;
+		dev = veth_probe_one(i);
+
+		if (dev) {
+			mFabricMgr->mPorts[i] = (struct veth_port *)dev->priv;
+			veth_devices[veth_num_devices] = dev;
+			veth_num_devices++;
+		}
+	}
+
+	if (vlans_found == 0)
+		return -ENODEV;
+
+	return 0;
+}
+
+void __exit veth_module_cleanup(void)
+{
+	int i;
+	struct VethFabricMgr *myFm = mFabricMgr;
+
+	if (! mFabricMgr)
+		return;
+
+	VethModuleReopen = 0;
+	
+	for (i = 0; i < HVMAXARCHITECTEDLPS; ++i) {
+		struct VethLpConnection *cnx = &(mFabricMgr->mConnection[i]);
+		unsigned long flags;
+
+		spin_lock_irqsave(&cnx->status_gate, flags);
+		veth_closeConnection(i);
+		spin_unlock_irqrestore(&cnx->status_gate, flags);
+	}
+	
+	flush_scheduled_work();
+	
+	HvLpEvent_unregisterHandler(HvLpEvent_Type_VirtualLan);
+	
+	mb();
+	mFabricMgr = NULL;
+	mb();
+	
+	for (i = 0; i < HVMAXARCHITECTEDLPS; ++i) {
+		struct VethLpConnection *cnx = &myFm->mConnection[i];
+
+		if (cnx->mNumberAllocated + cnx->mNumberRcvMsgs > 0) {
+			mf_deallocateLpEvents(cnx->remote_lp,
+					      HvLpEvent_Type_VirtualLan,
+					      cnx->mNumberAllocated
+					      + cnx->mNumberRcvMsgs,
+					      NULL, NULL);
+		}
+		
+		if (cnx->mMsgs)
+			kfree(cnx->mMsgs);
+	}
+	
+	for (i = 0; i < HvMaxArchitectedVirtualLans; ++i) {
+		struct net_device *dev;
+
+		if (! myFm->mPorts[i])
+			continue;
+
+		dev = myFm->mPorts[i]->mDev;
+		myFm->mPorts[i] = NULL;
+
+		mb();
+			
+		if (dev) {
+			unregister_netdev(dev);
+			kfree(dev);
+		}
+	}
+	
+	kfree(myFm);
+}
+
+module_exit(veth_module_cleanup);
+
+int __init veth_module_init(void)
+{
+	int i;
+	int this_lp = mFabricMgr->mThisLp;
+	int rc;
+
+	mFabricMgr = kmalloc(sizeof (struct VethFabricMgr), GFP_KERNEL);
+	if (! mFabricMgr) {
+		veth_error("Unable to allocate fabric manager\n");
+		return -ENOMEM;
+	}
+
+	memset(mFabricMgr, 0, sizeof (*mFabricMgr));
+
+	mFabricMgr->mEyecatcher = 0x56455448464D4752ULL;
+	this_lp = HvLpConfig_getLpIndex_outline();
+	mFabricMgr->mThisLp = this_lp;
+
+	for (i = 0; i < HVMAXARCHITECTEDLPS; ++i) {
+		struct VethLpConnection *cnx = &mFabricMgr->mConnection[i];
+
+		veth_init_connection(cnx, i);
+	}
+
+	rc = veth_probe();
+	if (rc != 0)
+		return rc;
+
+	HvLpEvent_registerHandler(HvLpEvent_Type_VirtualLan, &veth_handle_event);
+
+	/* Run through the active lps and open connections to the ones
+	 * we need to */
+	/* FIXME: is there any reason to do this backwards? */
+	for (i = HVMAXARCHITECTEDLPS - 1; i >= 0; --i) {
+		struct VethLpConnection *cnx = &mFabricMgr->mConnection[i];
+
+		if ( (i == this_lp) 
+		     || ! HvLpConfig_doLpsCommunicateOnVirtualLan(this_lp, i) )
+			continue;
+
+		spin_lock_irq(&cnx->status_gate);
+		veth_open_connection(i);
+		spin_unlock_irq(&cnx->status_gate);
+	}
+
+	return 0;
+}
+
+module_init(veth_module_init);
+
+static int veth_open(struct net_device *dev)
+{
+	struct veth_port *port = (struct veth_port *) dev->priv;
+
+	memset(&port->stats, 0, sizeof (port->stats));
+
+	netif_start_queue(dev);
+
+	return 0;
+}
+
+static int veth_close(struct net_device *dev)
+{
+	netif_stop_queue(dev);
+
+	return 0;
+}
+
+static struct net_device_stats *veth_get_stats(struct net_device *dev)
+{
+	struct veth_port *port = (struct veth_port *) dev->priv;
+
+	return &port->stats;
+}
+
+static int veth_start_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	unsigned char *frame = skb->data;
+	struct veth_port *port = (struct veth_port *) dev->priv;
+	int i;
+	int rc = 1;
+	int individual_rc;
+	int skb_len = skb->len;
+
+	if (! mFabricMgr) {
+		veth_error("NULL fabric manager with active ports!\n");
+		netif_stop_queue(dev);
+		BUG();
+		return 1;
+	}
+
+	if (! (frame[0] & 0x01)) {
+		/* unicast packet */
+		HvLpIndex rlp = frame[5];
+
+		if ((rlp != mFabricMgr->mThisLp)
+		    &&
+		    (HvLpConfig_doLpsCommunicateOnVirtualLan
+		     (mFabricMgr->mThisLp, rlp))) {
+			rc = veth_pTransmit(skb, rlp, dev);
+		} else {
+			dev_kfree_skb(skb);
+			rc = 0;
+		}
+	} else {
+		/* broadcast or multicast */
+		for (i = 0; i < HVMAXARCHITECTEDLPS; ++i) {
+			if (i == mFabricMgr->mThisLp)
+				continue;
+
+			if (HvLpConfig_doLpsCommunicateOnVirtualLan
+			    (mFabricMgr->mThisLp, i)) {
+				struct sk_buff *clone =
+					skb_clone(skb, GFP_ATOMIC);
+				
+				if (! clone) {
+					veth_error("skb_clone failed %p\n",
+						   skb);
+					rc = 0;
+					break;
+				}
+				
+				/* the ack handles deleting the skb */
+				individual_rc = veth_pTransmit(clone, i, dev);
+				
+				/* tx failed, we need to free the sbk */
+				if (individual_rc != 0)
+					dev_kfree_skb(clone);
+				
+				/* if we didn't fail from lack of
+				 * buffers, the tx as a whole is
+				 * successful */
+				if (individual_rc != 1)
+					rc = 0;
+			}
+		}
+
+		/* broadcast/multicast - If every connection is out of
+		   buffers (highly unlikely) then we leave rc set to 1
+		   and stop the queue. If any connection fails for any
+		   reason other than out of buffers, then we say the
+		   tx succeeded.
+		 */
+		if (rc == 0)
+			dev_kfree_skb(skb);
+	}
+
+	if (rc != 0) {
+		if (rc == 1) {
+			/* reasons for stopping the queue:
+			   - a non broadcast/multicast packet was destined for a connection that is out of buffers
+			   - a broadcast/multicast packet and every connection was out of buffers
+			 */
+
+			netif_stop_queue(dev);
+		} else {
+			/* reasons for not stopping the queue:
+			   - a non broadcast/multicast packet was destined for a failed connection
+			   - a broadcast/multicast packet and at least one connection had available buffers
+			 */
+			dev_kfree_skb(skb);
+			rc = 0;
+		}
+	} else {
+		port->stats.tx_packets++;
+		port->stats.tx_bytes += skb_len;
+	}
+
+	return rc;
+}
+
+static int
+veth_pTransmit(struct sk_buff *skb, HvLpIndex rlp, struct net_device *dev)
+{
+	struct VethLpConnection *cnx = mFabricMgr->mConnection + rlp;
+	HvLpEvent_Rc rc;
+	u32 dma_address, dma_length;
+	struct VethMsg *msg = NULL;
+
+	if (! cnx->status.ready)
+		return 2;
+
+	if ((skb->len - 14) > VETH_MAX_MTU)
+		return 2;
+
+	VETHSTACKPOP(&cnx->mMsgStack, msg);
+
+	if (! msg)
+		return 1;
+
+	dma_length = skb->len;
+	dma_address = pci_map_single(iSeries_veth_dev, skb->data,
+				     dma_length, PCI_DMA_TODEVICE);
+	
+	/* Is it really necessary to check the length and address
+	 * fields of the first entry here? */
+	if (dma_address != NO_TCE) {
+		msg->skb = skb;
+		msg->mSendData.addr[0] = dma_address;
+		msg->mSendData.len[0] = dma_length;
+		msg->mSendData.eof = 1;
+		set_bit(0, &(msg->mInUse));
+		rc = veth_signaldata(cnx, VethEventTypeFrames,
+				     msg->mIndex, &msg->mSendData);
+	} else {
+		struct veth_port *port = (struct veth_port *) dev->priv;
+		rc = -1;	/* Bad return code */
+		port->stats.tx_errors++;
+	}
+	
+	if (rc != HvLpEvent_Rc_Good) {
+		msg->skb = NULL;
+		/* need to set in use to make veth_recycle_msg in case
+		 * this was a mapping failure */
+		set_bit(0, &msg->mInUse);
+		veth_recycle_msg(cnx, msg);
+		return 2;
+	}
+
+	return 0;
+}
+
+static int
+veth_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
+{
+#ifdef SIOCETHTOOL
+	struct ethtool_cmd ecmd;
+
+	if (cmd != SIOCETHTOOL)
+		return -EOPNOTSUPP;
+	if (copy_from_user(&ecmd, ifr->ifr_data, sizeof (ecmd)))
+		return -EFAULT;
+	switch (ecmd.cmd) {
+	case ETHTOOL_GSET:
+		ecmd.supported = (SUPPORTED_1000baseT_Full
+				  | SUPPORTED_Autoneg | SUPPORTED_FIBRE);
+		ecmd.advertising = (SUPPORTED_1000baseT_Full
+				    | SUPPORTED_Autoneg | SUPPORTED_FIBRE);
+
+		ecmd.port = PORT_FIBRE;
+		ecmd.transceiver = XCVR_INTERNAL;
+		ecmd.phy_address = 0;
+		ecmd.speed = SPEED_1000;
+		ecmd.duplex = DUPLEX_FULL;
+		ecmd.autoneg = AUTONEG_ENABLE;
+		ecmd.maxtxpkt = 120;
+		ecmd.maxrxpkt = 120;
+		if (copy_to_user(ifr->ifr_data, &ecmd, sizeof(ecmd)))
+			return -EFAULT;
+		return 0;
+
+	case ETHTOOL_GDRVINFO:{
+			struct ethtool_drvinfo info = { ETHTOOL_GDRVINFO };
+			strncpy(info.driver, "veth", sizeof(info.driver) - 1);
+			info.driver[sizeof(info.driver) - 1] = '\0';
+			strncpy(info.version, "1.0", sizeof(info.version) - 1);
+			if (copy_to_user(ifr->ifr_data, &info, sizeof(info)))
+				return -EFAULT;
+			return 0;
+		}
+		/* get link status */
+	case ETHTOOL_GLINK:{
+			struct ethtool_value edata = { ETHTOOL_GLINK };
+			edata.data = 1;
+			if (copy_to_user(ifr->ifr_data, &edata, sizeof(edata)))
+				return -EFAULT;
+			return 0;
+		}
+
+	default:
+		break;
+	}
+
+#endif
+	return -EOPNOTSUPP;
+}
+
+static void veth_set_multicast_list(struct net_device *dev)
+{
+	char *addrs;
+	struct veth_port *port = (struct veth_port *) dev->priv;
+	unsigned long flags;
+
+	write_lock_irqsave(&port->mcast_gate, flags);
+
+	if (dev->flags & IFF_PROMISC) {	/* set promiscuous mode */
+		port->promiscuous = 1;
+	} else if ( (dev->flags & IFF_ALLMULTI)
+		    || (dev->mc_count > VETH_MAX_MCAST) ) {
+		port->all_mcast = 1;
+	} else {
+		struct dev_mc_list *dmi = dev->mc_list;
+		int i;
+
+		/* Update table */
+		port->num_mcast = 0;
+		
+		for (i = 0; i < dev->mc_count; i++) {
+			u8 *addr = dmi->dmi_addr;
+			u64 xaddr = 0;
+
+			if (addrs[0] & 0x01) {/* multicast address? */
+				memcpy(&xaddr, addr, 6);
+				port->mcast_addr[port->num_mcast] = xaddr;
+				port->num_mcast++;
+			}
+			dmi = dmi->next;
+		}
+	}
+
+	write_unlock_irqrestore(&port->mcast_gate, flags);
+}
+
+static void
+veth_handle_event(struct HvLpEvent *event, struct pt_regs *regs)
+{
+	struct VethLpEvent *veth_event = (struct VethLpEvent *)event;
+
+	if (event->xFlags.xFunction == HvLpEvent_Function_Ack)
+		veth_handle_ack(veth_event);
+	else if (event->xFlags.xFunction == HvLpEvent_Function_Int)
+		veth_handle_int(veth_event);
+}
+
+static void
+veth_handle_ack(struct VethLpEvent *event)
+{
+	HvLpIndex rlp = event->mBaseEvent.xTargetLp;
+	struct VethLpConnection *cnx = &mFabricMgr->mConnection[rlp];
+
+	switch (event->mBaseEvent.xSubtype) {
+	case VethEventTypeCap:
+		veth_take_cap_ack(cnx, event);
+		break;
+	case VethEventTypeMonitor:
+		veth_take_monitor_ack(cnx, event);
+		break;
+	default:
+		veth_error("Unknown ack type %d from lpar %d\n",
+			   event->mBaseEvent.xSubtype, rlp);
+	};
+}
+
+static void
+veth_handle_int(struct VethLpEvent *event)
+{
+	HvLpIndex rlp = event->mBaseEvent.xSourceLp;
+	struct VethLpConnection *cnx = &mFabricMgr->mConnection[rlp];
+	int i;
+
+	switch (event->mBaseEvent.xSubtype) {
+	case VethEventTypeCap:
+		veth_take_cap(cnx, event);
+		break;
+	case VethEventTypeMonitor:
+		/* do nothing... this'll hang out here til we're dead,
+		 * and the hypervisor will return it for us. */
+		break;
+	case VethEventTypeFramesAck:
+		for (i = 0; i < VETH_MAX_ACKS_PER_MSG; ++i) {
+			u16 msgnum = event->u.mFramesAckData.mToken[i];
+
+			if (msgnum < cnx->mNumMsgs)
+				veth_recycle_msg(cnx, cnx->mMsgs + msgnum);
+		}
+		break;
+	case VethEventTypeFrames:
+		veth_receive(cnx, event);
+		break;
+	default:
+		veth_error("Unknown interrupt type %d from lpar %d\n",
+			   event->mBaseEvent.xSubtype, rlp);
+	};
+}
+
+static void veth_failMe(struct VethLpConnection *cnx)
+{
+	cnx->status.ready = 0;
+}
+
+static void veth_init_connection(struct VethLpConnection *cnx, u8 rlp)
+{
+	struct VethMsg *msgs;
+	HvLpIndex this_lp = mFabricMgr->mThisLp;
+	int i;
+
+	veth_failMe(cnx);
+
+	cnx->remote_lp = rlp;
+
+	spin_lock_init(&cnx->ack_gate);
+	spin_lock_init(&cnx->status_gate);
+
+	cnx->status.got_cap = 0;
+	cnx->status.got_cap_ack = 0;
+
+	INIT_WORK(&cnx->finish_open_wq, veth_finish_open_connection, cnx);
+	INIT_WORK(&cnx->monitor_ack_wq, veth_monitor_ack_task, cnx);
+
+	init_timer(&cnx->ack_timer);
+	cnx->ack_timer.function = veth_timed_ack;
+	cnx->ack_timer.data = (unsigned long) cnx;
+
+	if ( (rlp == this_lp) 
+	     || ! HvLpConfig_doLpsCommunicateOnVirtualLan(this_lp, rlp) )
+		return;
+
+	msgs = kmalloc(VETH_NUMBUFFERS * sizeof(struct VethMsg), GFP_KERNEL);
+	if (! msgs)
+		return;
+
+	cnx->mMsgs = msgs;
+	memset(msgs, 0, VETH_NUMBUFFERS * sizeof(struct VethMsg));
+	spin_lock_init(&cnx->mMsgStack.lock);
+
+	for (i = 0; i < VETH_NUMBUFFERS; i++) {
+		msgs[i].mIndex = i;
+		VETHSTACKPUSH(&cnx->mMsgStack, msgs + i);
+	}
+
+	cnx->mNumMsgs = VETH_NUMBUFFERS;
+
+	cnx->mNumberAllocated = veth_allocate_events(rlp, 2);
+
+	if (cnx->mNumberAllocated < 2) {
+		veth_error("Couldn't allocate base msgs for lpar %d, only got %d\n",
+			   cnx->remote_lp, cnx->mNumberAllocated);
+		veth_failMe(cnx);
+		return;
+	}
+
+	cnx->mNumberRcvMsgs = veth_allocate_events(cnx->remote_lp,
+						   VETH_NUMBUFFERS);
+}
+
+static void veth_open_connection(u8 rlp)
+{
+	struct VethLpConnection *cnx = &mFabricMgr->mConnection[rlp];
+	HvLpEvent_Rc rc;
+	u64 *rawcap = (u64 *) &cnx->mMyCap;
+
+	if (! cnx->mMsgs || (cnx->mNumberAllocated < 2)
+	    || ! cnx->mNumberRcvMsgs) {
+		veth_failMe(cnx);
+		return;
+	}
+
+	spin_lock_irq(&cnx->ack_gate);
+
+	memset(&cnx->pending_acks, 0xff, sizeof (cnx->pending_acks));
+	cnx->num_pending_acks = 0;
+
+	HvCallEvent_openLpEventPath(rlp, HvLpEvent_Type_VirtualLan);
+
+	cnx->status.mOpen = 1;
+
+	cnx->src_inst = 
+		HvCallEvent_getSourceLpInstanceId(rlp,
+						  HvLpEvent_Type_VirtualLan);
+	cnx->dst_inst =
+		HvCallEvent_getTargetLpInstanceId(rlp,
+						  HvLpEvent_Type_VirtualLan);
+
+	spin_unlock_irq(&cnx->ack_gate);
+
+	cnx->mMyCap.mNumberBuffers = cnx->mNumMsgs;
+
+	if (cnx->mNumMsgs < 10)
+		cnx->mMyCap.mThreshold = 1;
+	else if (cnx->mNumMsgs < 20)
+		cnx->mMyCap.mThreshold = 4;
+	else if (cnx->mNumMsgs < 40)
+		cnx->mMyCap.mThreshold = 10;
+	else
+		cnx->mMyCap.mThreshold = 20;
+
+	cnx->mMyCap.mTimer = VETH_ACKTIMEOUT;
+
+	rc = veth_signalevent(cnx, VethEventTypeCap,
+			      HvLpEvent_AckInd_DoAck,
+			      HvLpEvent_AckType_ImmediateAck,
+			      0, rawcap[0], rawcap[1], rawcap[2], rawcap[3],
+			      rawcap[4]);
+
+	if ( (rc == HvLpEvent_Rc_PartitionDead)
+	     || (rc == HvLpEvent_Rc_PathClosed)) {
+		/* Never mind we'll resend out caps when we get the
+		 * caps from the other end comes up and sends
+		 * theirs */
+		return;
+	} else if (rc != HvLpEvent_Rc_Good) {
+		veth_error("Couldn't send capabilities to lpar %d, rc=%x\n",
+				  cnx->remote_lp, (int) rc);
+		veth_failMe(cnx);
+		return;
+	}
+
+	cnx->status.sent_caps = 1;
+}
+
+static void veth_finish_open_connection(void *parm)
+{
+	struct VethLpConnection *cnx = (struct VethLpConnection *)parm;
+	struct VethCapData *remoteCap = &cnx->mRemoteCap;
+	u64 numAcks = 0;
+	HvLpEvent_Rc rc;
+
+	spin_lock_irq(&cnx->status_gate);
+
+	memcpy(remoteCap, &cnx->cap_event.u.mCapabilitiesData,
+	       sizeof(*remoteCap));
+
+	if ( (remoteCap->mNumberBuffers == 0)
+	     || (remoteCap->mThreshold > VETH_MAX_ACKS_PER_MSG)
+	     || (remoteCap->mThreshold == 0) ) {
+		veth_error("Received incompatible capabilities from lpar %d\n",
+			   cnx->remote_lp);
+		cnx->cap_event.mBaseEvent.xRc = HvLpEvent_Rc_InvalidSubtypeData;
+		HvCallEvent_ackLpEvent((struct HvLpEvent *)&cnx->cap_event);
+
+		veth_failMe(cnx);
+		return;
+	}
+
+	numAcks = (remoteCap->mNumberBuffers / remoteCap->mThreshold) + 1;
+
+	if (cnx->mNumberLpAcksAlloced < numAcks) {
+		int num;
+		
+		numAcks = numAcks - cnx->mNumberLpAcksAlloced;
+		
+		spin_unlock_irq(&cnx->status_gate);
+		
+		num = veth_allocate_events(cnx->remote_lp, numAcks);
+		
+		if (num > 0)
+			cnx->mNumberLpAcksAlloced += num;
+		spin_lock_irq(&cnx->status_gate);
+		
+	}
+	
+	/* Convert timer to jiffies */
+	if (cnx->mMyCap.mTimer)
+		cnx->ack_timeout = remoteCap->mTimer * HZ / 1000000;
+	else
+		cnx->ack_timeout = VETH_ACKTIMEOUT * HZ / 1000000;
+
+	if (cnx->mNumberLpAcksAlloced < numAcks) {
+		veth_error("Couldn't allocate all the frames ack events for lpar %d\n",
+			   cnx->remote_lp);
+
+		cnx->cap_event.mBaseEvent.xRc = HvLpEvent_Rc_BufferNotAvailable;
+		HvCallEvent_ackLpEvent((struct HvLpEvent *)&cnx->cap_event);
+
+		veth_failMe(cnx);
+		return;
+	}
+
+	rc = HvCallEvent_ackLpEvent((struct HvLpEvent *)&cnx->cap_event);
+	if (rc != HvLpEvent_Rc_Good) {
+		veth_error("Failed to ack remote cap for lpar %d with rc %x\n",
+			   cnx->remote_lp, (int) rc);
+		veth_failMe(cnx);
+		return;
+
+	}
+
+	if (cnx->cap_ack_event.mBaseEvent.xRc != HvLpEvent_Rc_Good) {
+		veth_printk(KERN_ERR, "Bad rc(%d) from lpar %d on capabilities\n",
+			    cnx->cap_ack_event.mBaseEvent.xRc, cnx->remote_lp);
+		veth_failMe(cnx);
+		return;
+	}
+
+	/* Send the monitor */
+	rc = veth_signalevent(cnx, VethEventTypeMonitor,
+			      HvLpEvent_AckInd_DoAck,
+			      HvLpEvent_AckType_DeferredAck,
+			      0, 0, 0, 0, 0, 0);
+	
+	if (rc != HvLpEvent_Rc_Good) {
+		veth_error("Monitor send to lpar %d failed with rc %x\n",
+				  cnx->remote_lp, (int) rc);
+		veth_failMe(cnx);
+		return;
+	}
+
+	cnx->status.ready = 1;
+	
+	/* Start the ACK timer */
+	cnx->ack_timer.expires = jiffies + cnx->ack_timeout;
+	add_timer(&cnx->ack_timer);
+
+	spin_unlock_irq(&cnx->status_gate);
+}
+
+static void
+veth_closeConnection(u8 rlp)
+{
+	struct VethLpConnection *cnx = &mFabricMgr->mConnection[rlp];
+	unsigned long flags;
+
+	del_timer_sync(&cnx->ack_timer);
+
+	cnx->status.sent_caps = 0;
+	cnx->status.got_cap = 0;
+	cnx->status.got_cap_ack = 0;
+
+	if (cnx->status.mOpen) {
+		int i;		
+
+		HvCallEvent_closeLpEventPath(rlp, HvLpEvent_Type_VirtualLan);
+		cnx->status.mOpen = 0;
+		veth_failMe(cnx);
+
+		/* reset ack data */
+		spin_lock_irqsave(&cnx->ack_gate, flags);
+
+		memset(&cnx->pending_acks, 0xff, sizeof (cnx->pending_acks));
+		cnx->num_pending_acks = 0;
+
+		spin_unlock_irqrestore(&cnx->ack_gate, flags);
+
+		/* Clean up any leftover messages */
+		for (i = 0; i < cnx->mNumMsgs; ++i)
+			veth_recycle_msg(cnx, cnx->mMsgs + i);
+	}
+
+}
+
+static void veth_take_cap(struct VethLpConnection *cnx, struct VethLpEvent *event)
+{
+	unsigned long flags;
+	HvLpEvent_Rc rc;
+
+	spin_lock_irqsave(&cnx->status_gate, flags);
+
+	if (cnx->status.got_cap) {
+		veth_error("Received a second capabilities from lpar %d\n",
+			   cnx->remote_lp);
+		event->mBaseEvent.xRc = HvLpEvent_Rc_BufferNotAvailable;
+		HvCallEvent_ackLpEvent((struct HvLpEvent *) event);
+		goto out;
+	}
+
+	memcpy(&cnx->cap_event, event, sizeof (cnx->cap_event));
+	/* If we failed to send caps out before (presumably because
+	 * the target lpar was down), send them now. */
+	if (! cnx->status.sent_caps) {
+		u64 *rawcap = (u64 *) &cnx->mMyCap;
+
+		cnx->dst_inst =
+			HvCallEvent_getTargetLpInstanceId(cnx->remote_lp,
+							  HvLpEvent_Type_VirtualLan);
+
+		rc = veth_signalevent(cnx, VethEventTypeCap,
+				      HvLpEvent_AckInd_DoAck,
+				      HvLpEvent_AckType_ImmediateAck,
+				      0, rawcap[0], rawcap[1], rawcap[2], rawcap[3],
+				      rawcap[4]);
+		if ( (rc == HvLpEvent_Rc_PartitionDead)
+		     || (rc == HvLpEvent_Rc_PathClosed)) {
+			veth_error("Partition down when resending capabilities!!\n");
+			goto out;
+		} else if (rc != HvLpEvent_Rc_Good) {
+			veth_error("Couldn't send cap to lpar %d, rc %x\n",
+				   cnx->remote_lp, (int) rc);
+			veth_failMe(cnx);
+			goto out;
+		}
+		cnx->status.sent_caps = 1;
+	}
+
+	cnx->status.got_cap = 1;
+	if (cnx->status.got_cap_ack)
+		schedule_work(&cnx->finish_open_wq);
+
+ out:
+	spin_unlock_irqrestore(&cnx->status_gate, flags);
+}
+
+static void
+veth_take_cap_ack(struct VethLpConnection *cnx, struct VethLpEvent *event)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&cnx->status_gate, flags);
+
+	if (cnx->status.got_cap_ack) {
+		veth_error("Received a second capabilities ack from lpar %d\n",
+			   cnx->remote_lp);
+		goto out;
+	}
+
+	memcpy(&cnx->cap_ack_event, event, sizeof(&cnx->cap_ack_event));
+	cnx->status.got_cap_ack = 1;
+
+	if (cnx->status.got_cap)
+		schedule_work(&cnx->finish_open_wq);
+
+ out:
+	spin_unlock_irqrestore(&cnx->status_gate, flags);
+}
+
+static void
+veth_take_monitor_ack(struct VethLpConnection *cnx,  struct VethLpEvent *event)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&cnx->status_gate, flags);
+
+	veth_printk(KERN_DEBUG, "Monitor ack returned for lpar %d\n", cnx->remote_lp);
+
+	if (cnx->status.monitor_ack_pending) {
+		veth_error("Received a monitor ack from lpar %d while already processing one\n",
+			   cnx->remote_lp);
+		goto out;
+	}
+
+	schedule_work(&cnx->monitor_ack_wq);
+
+ out:
+	spin_unlock_irqrestore(&cnx->status_gate, flags);
+}
+
+static void
+veth_recycle_msg(struct VethLpConnection *cnx, struct VethMsg *myMsg)
+{
+	u32 dma_address, dma_length;
+
+	if (test_and_clear_bit(0, &myMsg->mInUse)) {
+		dma_address = myMsg->mSendData.addr[0];
+		dma_length = myMsg->mSendData.len[0];
+
+		pci_unmap_single(iSeries_veth_dev, dma_address, dma_length,
+				 PCI_DMA_TODEVICE);
+
+		if (myMsg->skb) {
+			dev_kfree_skb_any(myMsg->skb);
+			myMsg->skb = NULL;
+		}
+
+		memset(&myMsg->mSendData, 0, sizeof(myMsg->mSendData));
+		VETHSTACKPUSH(&cnx->mMsgStack, myMsg);
+	} else {
+		if (cnx->status.mOpen) {
+			veth_error("Received a frames ack for msg %d from lpar %d while not outstanding\n",
+				   myMsg->mIndex, cnx->remote_lp);
+		}
+	}
+
+	veth_startQueues();
+}
+
+static void
+veth_startQueues(void)
+{
+	int i;
+
+	for (i = 0; i < veth_num_devices; ++i)
+		netif_wake_queue(veth_devices[i]);
+}
+
+static void veth_monitor_ack_task(void *parm)
+{
+	struct VethLpConnection *cnx = (struct VethLpConnection *) parm;
+
+	spin_lock_irq(&cnx->status_gate);
+
+	veth_failMe(cnx);
+
+	if (cnx->status.mOpen) {
+		veth_closeConnection(cnx->remote_lp);
+
+		udelay(100);
+	}
+
+	if (VethModuleReopen) {
+		veth_open_connection(cnx->remote_lp);
+	}
+	cnx->status.monitor_ack_pending = 0;
+
+	spin_unlock_irq(&cnx->status_gate);
+}
+
+static inline int veth_frame_wanted(struct veth_port *port, u64 mac_addr)
+{
+	int wanted = 0;
+	int i;
+	unsigned long flags;
+
+	if ( (mac_addr == port->mMyAddress)
+	     || (mac_addr == 0xffffffffffff0000)
+	     || port->promiscuous )
+		return 1;
+	
+	if (! (((char *) &mac_addr)[0] & 0x01))
+		return 0;
+
+	read_lock_irqsave(&port->mcast_gate, flags);
+
+	if (port->all_mcast) {
+		wanted = 1;
+		goto out;
+	}
+
+	for (i = 0; i < port->num_mcast; ++i) {
+		if (port->mcast_addr[i] == mac_addr) {
+			wanted = 1;
+			break;
+		}
+	}
+
+ out:
+	read_unlock_irqrestore(&port->mcast_gate, flags);
+
+	return wanted;
+}
+
+struct dma_chunk {
+	u64 addr;
+	u64 size;
+};
+
+#define VETH_MAX_PAGES_PER_FRAME ( (VETH_MAX_MTU+PAGE_SIZE-2)/PAGE_SIZE + 1 )
+
+static inline void veth_build_dma_list(struct dma_chunk *list, unsigned char *p,
+				      unsigned long length)
+{
+	unsigned long done;
+	int i = 1;
+
+	/* FIXME: skbs are continguous in real addresses.  Do we
+	 * really need to break it into PAGE_SIZE chunks, or can we do
+	 * it just at the granularity of iSeries real->absolute
+	 * mapping? */
+	list[0].addr = veth_dma_addr(p);
+	list[0].size = min(length,
+			   PAGE_SIZE - ((unsigned long)p & ~PAGE_MASK));
+
+	done = list[0].size;
+	while (done < length) {
+		list[i].addr = veth_dma_addr(p + done);
+		list[i].size = min(done, PAGE_SIZE);
+		done += list[i].size;
+		i++;
+	}
+}
+
+static void veth_receive(struct VethLpConnection *cnx, struct VethLpEvent *event)
+{
+	struct VethFramesData *senddata = &event->u.mSendData;
+	int startchunk = 0;
+	int nchunks;
+	unsigned long flags;
+	HvLpDma_Rc rc;
+
+	do {
+		u16 length = 0;
+		struct sk_buff *skb;
+		struct dma_chunk local_list[VETH_MAX_PAGES_PER_FRAME];
+		struct dma_chunk remote_list[VETH_MAXFRAMESPERMSG];
+		u64 dest;
+		HvLpVirtualLanIndex vlan;
+		struct veth_port *port;
+
+		/* FIXME: do we need this? */
+		memset(local_list, 0, sizeof(local_list));
+		memset(remote_list, 0, sizeof(VETH_MAXFRAMESPERMSG));
+
+		nchunks = 0;
+
+		/* a 0 address marks the end of the valid entries */
+		if (senddata->addr[startchunk] == 0)
+			break;
+
+		/* make sure that we have at least 1 EOF entry in the
+		 * remaining entries */
+		if (! (senddata->eof >> startchunk)) {
+			veth_error("missing EOF frag in event: 0x%x startchunk=%d\n",
+				   (unsigned) senddata->eof, startchunk);
+			break;
+		}
+
+		/* build list of chunks in this frame */
+		do {
+			remote_list[nchunks].addr =
+				(u64) senddata->addr[startchunk + nchunks] << 32;
+			remote_list[nchunks].size =
+				senddata->len[startchunk + nchunks];
+			length += remote_list[nchunks].size;
+		} while (! (senddata->eof & (1 << (startchunk + nchunks++))));
+
+		/* length == total length of all chunks */
+		/* nchunks == # of chunks in this frame */
+
+		if ((length - ETH_HLEN) > VETH_MAX_MTU)
+			continue;
+
+		skb = alloc_skb(length, GFP_ATOMIC);
+		if (!skb)
+			continue;
+
+		veth_build_dma_list(local_list, skb->data, length);
+
+		rc = HvCallEvent_dmaBufList(HvLpEvent_Type_VirtualLan,
+					    event->mBaseEvent.xSourceLp,
+					    HvLpDma_Direction_RemoteToLocal,
+					    cnx->src_inst,
+					    cnx->dst_inst,
+					    HvLpDma_AddressType_RealAddress,
+					    HvLpDma_AddressType_TceIndex,
+					    veth_dma_addr(&local_list),
+					    veth_dma_addr(&remote_list),
+					    length);
+		if (rc != HvLpDma_Rc_Good) {
+			dev_kfree_skb_irq(skb);
+			continue;
+		}
+		
+		vlan = skb->data[9];
+		port = mFabricMgr->mPorts[vlan];
+		dest = *((u64 *) skb->data) & 0xFFFFFFFFFFFF0000;
+
+		if ((vlan > HVMAXARCHITECTEDVIRTUALLANS) || !port) {
+			dev_kfree_skb_irq(skb);
+			continue;
+		}
+		if (! veth_frame_wanted(port, dest)) {
+			dev_kfree_skb_irq(skb);
+			continue;
+		}
+			
+		skb_put(skb, length);
+		skb->dev = port->mDev;
+		skb->protocol = eth_type_trans(skb, port->mDev);
+		skb->ip_summed = CHECKSUM_NONE;
+		netif_rx(skb);	/* send it up */
+		port->stats.rx_packets++;
+		port->stats.rx_bytes += length;
+	} while (startchunk += nchunks, startchunk < VETH_MAXFRAMESPERMSG);
+
+	/* Ack it */
+	spin_lock_irqsave(&cnx->ack_gate, flags);
+	
+	if (cnx->num_pending_acks < VETH_MAX_ACKS_PER_MSG) {
+		cnx->pending_acks[cnx->num_pending_acks] =
+			event->mBaseEvent.xCorrelationToken;
+		++cnx->num_pending_acks;
+		
+		if (cnx->num_pending_acks == cnx->mRemoteCap.mThreshold) {
+			rc = veth_signaldata(cnx, VethEventTypeFramesAck,
+					     0, &cnx->pending_acks);
+			
+			if (rc != HvLpEvent_Rc_Good)
+				veth_error("Error 0x%x acking frames from lpar %d!\n",
+					   (unsigned)rc, cnx->remote_lp);
+			
+			cnx->num_pending_acks = 0;
+			memset(&cnx->pending_acks, 0xff, sizeof(cnx->pending_acks));
+		}
+		
+	}
+	
+	spin_unlock_irqrestore(&cnx->ack_gate, flags);
+}
+
+static void veth_timed_ack(unsigned long ptr)
+{
+	unsigned long flags;
+	HvLpEvent_Rc rc;
+	struct VethLpConnection *cnx = (struct VethLpConnection *) ptr;
+
+	/* Ack all the events */
+	spin_lock_irqsave(&cnx->ack_gate, flags);
+
+	if (cnx->num_pending_acks > 0) {
+		rc = veth_signaldata(cnx, VethEventTypeFramesAck,
+				     0, &cnx->pending_acks);
+		if (rc != HvLpEvent_Rc_Good)
+			veth_error("Error 0x%x acking frames from lpar %d!\n", 
+				   (unsigned) rc, cnx->remote_lp);
+
+		cnx->num_pending_acks = 0;
+		memset(&cnx->pending_acks, 0xff, sizeof(cnx->pending_acks));
+	}
+
+	spin_unlock_irqrestore(&cnx->ack_gate, flags);
+
+	veth_startQueues();
+
+	/* Reschedule the timer */
+	cnx->ack_timer.expires = jiffies + cnx->ack_timeout;
+	add_timer(&cnx->ack_timer);
+}
diff -purN linux-2.5/drivers/net/iseries_veth.h linuxppc64-2.5/drivers/net/iseries_veth.h
--- linux-2.5/drivers/net/iseries_veth.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/net/iseries_veth.h	2003-12-08 05:21:20.000000000 +0000
@@ -0,0 +1,172 @@
+/* File veth.h created by Kyle A. Lucke on Mon Aug  7 2000. */
+
+/* Change Activity: */
+/* End Change Activity */
+
+#ifndef _VETH_H
+#define _VETH_H
+
+#include <asm/iSeries/HvTypes.h>
+#include <asm/iSeries/HvLpEvent.h>
+#include <linux/netdevice.h>
+
+#define VethEventTypeCap (0)
+#define VethEventTypeFrames (1)
+#define VethEventTypeMonitor (2)
+#define VethEventTypeFramesAck (3)
+
+#define VethMaxFramesMsgsAcked (20)
+#define VethMaxFramesMsgs (0xFFFF)
+#define VethMaxFramesPerMsg (6)
+#define VethAckTimeoutUsec (1000000)
+
+#define VETHSTACK(T) \
+	struct VethStack##T \
+	{ \
+		struct T *head; \
+		spinlock_t lock; \
+	}
+#define VETHSTACKCTOR(s) \
+	do { (s)->head = NULL; spin_lock_init(&(s)->lock); } while(0)
+#define VETHSTACKPUSH(s, p) \
+	do { \
+		unsigned long flags; \
+		spin_lock_irqsave(&(s)->lock,flags); \
+		(p)->next = (s)->head; \
+		(s)->head = (p); \
+		spin_unlock_irqrestore(&(s)->lock, flags); \
+	} while(0)
+
+#define VETHSTACKPOP(s,p) \
+	do { \
+		unsigned long flags; \
+		spin_lock_irqsave(&(s)->lock,flags); \
+		(p) = (s)->head; \
+		if ((s)->head != NULL) { \
+			(s)->head = (s)->head->next; \
+		} \
+		spin_unlock_irqrestore(&(s)->lock, flags); \
+	} while(0)
+
+struct VethFramesData {
+	u32 mAddress[6];
+	u16 mLength[6];
+	u32 mEofMask:6;
+	u32 mReserved:26;
+};
+
+struct VethFramesAckData {
+	u16 mToken[VethMaxFramesMsgsAcked];
+};
+
+struct VethCapData {
+	u8 mVersion;
+	u8 mReserved1;
+	u16 mNumberBuffers;
+	u16 mThreshold;
+	u16 mReserved2;
+	u32 mTimer;
+	u32 mReserved3;
+	u64 mReserved4;
+	u64 mReserved5;
+	u64 mReserved6;
+};
+
+struct VethFastPathData {
+	u64 mData1;
+	u64 mData2;
+	u64 mData3;
+	u64 mData4;
+	u64 mData5;
+};
+
+struct VethLpEvent {
+	struct HvLpEvent mBaseEvent;
+	union {
+		struct VethFramesData mSendData;
+		struct VethCapData mCapabilitiesData;
+		struct VethFramesAckData mFramesAckData;
+		struct VethFastPathData mFastPathData;
+	} mDerivedData;
+
+};
+
+struct VethMsg {
+	struct VethMsg *next;
+	union {
+		struct VethFramesData mSendData;
+		u64 raw[5];
+	} mEvent;
+	int mIndex;
+	unsigned long mInUse;
+	struct sk_buff *skb;
+};
+
+struct VethLpConnection {
+	u64 mEyecatcher;
+	HvLpIndex remote_lp;
+	HvLpInstanceId src_inst;
+	HvLpInstanceId dst_inst;
+	u32 mNumMsgs;
+	struct VethMsg *mMsgs;
+	int mNumberRcvMsgs;
+	int mNumberLpAcksAlloced;
+	union {
+		struct VethFramesAckData mAckData;
+		u64 raw[5];
+	} mEventData;
+	spinlock_t ack_gate;
+	u32 mNumAcks;
+	spinlock_t status_gate;
+	struct {
+		u64 mOpen:1;
+		u64 mCapMonAlloced:1;
+		u64 mBaseMsgsAlloced:1;
+		u64 mSentCap:1;
+		u64 mCapAcked:1;
+		u64 mGotCap:1;
+		u64 mGotCapAcked:1;
+		u64 mSentMonitor:1;
+		u64 mPopulatedRings:1;
+		u64 mReserved:54;
+		u64 mFailed:1;
+	} status;
+	struct VethCapData mMyCap;
+	struct VethCapData mRemoteCap;
+	unsigned long mCapAckTaskPending;
+	struct work_struct mCapAckTaskTq;
+	struct VethLpEvent mCapAckEvent;
+	unsigned long mCapTaskPending;
+	struct work_struct mCapTaskTq;
+	struct VethLpEvent mCapEvent;
+	unsigned long mMonitorAckTaskPending;
+	struct work_struct mMonitorAckTaskTq;
+	struct VethLpEvent mMonitorAckEvent;
+	unsigned long mAllocTaskPending;
+	struct work_struct mAllocTaskTq;
+	int mNumberAllocated;
+	struct timer_list ack_timer;
+	u32 mTimeout;
+	VETHSTACK(VethMsg) mMsgStack;
+};
+
+#define HVMAXARCHITECTEDVIRTUALLANS 16
+struct veth_port {
+	struct net_device *mDev;
+	struct net_device_stats stats;
+	u64 mMyAddress;
+	int mPromiscuous;
+	int all_mcast;
+	rwlock_t mcast_gate;
+	int mNumAddrs;
+	u64 mMcasts[12];
+};
+
+struct VethFabricMgr {
+	u64 mEyecatcher;
+	HvLpIndex mThisLp;
+	struct VethLpConnection mConnection[HVMAXARCHITECTEDLPS];
+	struct veth_port *mPorts[HVMAXARCHITECTEDVIRTUALLANS];
+};
+
+#endif	/* _VETH_H */
diff -purN linux-2.5/drivers/pci/Makefile linuxppc64-2.5/drivers/pci/Makefile
--- linux-2.5/drivers/pci/Makefile	2003-09-26 16:11:09.000000000 +0000
+++ linuxppc64-2.5/drivers/pci/Makefile	2003-10-03 18:53:00.000000000 +0000
@@ -22,6 +22,7 @@ obj-$(CONFIG_ALPHA) += setup-bus.o setup
 obj-$(CONFIG_ARM) += setup-bus.o setup-irq.o
 obj-$(CONFIG_PARISC) += setup-bus.o
 obj-$(CONFIG_SUPERH) += setup-bus.o setup-irq.o
+obj-$(CONFIG_PPC64) += setup-bus.o
 obj-$(CONFIG_PPC32) += setup-irq.o
 obj-$(CONFIG_PPC64) += setup-bus.o
 obj-$(CONFIG_SGI_IP27) += setup-irq.o
diff -purN linux-2.5/drivers/pci/hotplug/Kconfig linuxppc64-2.5/drivers/pci/hotplug/Kconfig
--- linux-2.5/drivers/pci/hotplug/Kconfig	2003-10-08 22:33:16.000000000 +0000
+++ linuxppc64-2.5/drivers/pci/hotplug/Kconfig	2003-12-09 17:03:38.000000000 +0000
@@ -122,5 +122,28 @@ config HOTPLUG_PCI_CPCI_GENERIC
 
 	  When in doubt, say N.
 
+config HOTPLUG_PCI_RPA
+	tristate "RPA PCI Hotplug driver"
+	depends on HOTPLUG_PCI
+	help
+	  Say Y here if you have a a RPA system that supports PCI Hotplug.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called rpaphp.
+
+	  When in doubt, say N.
+
+config HOTPLUG_PCI_RPA_DLPAR
+	tristate "RPA Dynamic Logical Partitioning for I/O slots"
+ 	depends on HOTPLUG_PCI_RPA
+ 	help
+	  Say Y here if your system supports Dynamic Logical Partitioning
+	  for I/O slots.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called rpadlpar_io.
+ 
+ 	  When in doubt, say N.
+
 endmenu
 
diff -purN linux-2.5/drivers/pci/hotplug/Makefile linuxppc64-2.5/drivers/pci/hotplug/Makefile
--- linux-2.5/drivers/pci/hotplug/Makefile	2003-06-26 00:24:41.000000000 +0000
+++ linuxppc64-2.5/drivers/pci/hotplug/Makefile	2003-11-19 17:10:13.000000000 +0000
@@ -33,6 +35,12 @@ acpiphp-objs		:=	acpiphp_core.o	\
 				acpiphp_pci.o	\
 				acpiphp_res.o
 
+rpaphp-objs		:=	rpaphp_core.o	\
+				rpaphp_pci.o	
+
+rpadlpar_io-objs	:=	rpadlpar_core.o \
+				rpadlpar_sysfs.o
+
 ifdef CONFIG_HOTPLUG_PCI_ACPI
   EXTRA_CFLAGS  += -D_LINUX -I$(TOPDIR)/drivers/acpi
   ifdef CONFIG_ACPI_DEBUG
diff -purN linux-2.5/drivers/pci/hotplug/rpadlpar.h linuxppc64-2.5/drivers/pci/hotplug/rpadlpar.h
--- linux-2.5/drivers/pci/hotplug/rpadlpar.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/pci/hotplug/rpadlpar.h	2003-11-19 17:06:10.000000000 +0000
@@ -0,0 +1,23 @@
+/*
+ * Interface for Dynamic Logical Partitioning of I/O Slots
+ *
+ * John Rose <johnrose@austin.ibm.com>
+ * October 2003
+ *
+ * Copyright (C) 2003 IBM.
+ *
+ *      This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+#ifndef _RPADLPAR_IO_H_
+#define _RPADLPAR_IO_H_
+
+extern int dlpar_sysfs_init(void);
+extern void dlpar_sysfs_exit(void);
+
+extern int dlpar_add_slot(char *drc_name);
+extern int dlpar_remove_slot(char *drc_name);
+
+#endif
diff -purN linux-2.5/drivers/pci/hotplug/rpadlpar_core.c linuxppc64-2.5/drivers/pci/hotplug/rpadlpar_core.c
--- linux-2.5/drivers/pci/hotplug/rpadlpar_core.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/pci/hotplug/rpadlpar_core.c	2003-11-21 22:59:56.000000000 +0000
@@ -0,0 +1,317 @@
+/*
+ * Interface for Dynamic Logical Partitioning of I/O Slots
+ *
+ * John Rose <johnrose@austin.ibm.com>
+ * Linda Xie <lxie@us.ibm.com>
+ * 
+ * October 2003
+ *
+ * Copyright (C) 2003 IBM.
+ *
+ *      This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+#include <linux/init.h>
+#include <linux/pci.h>
+#include <asm/pci-bridge.h>
+#include "../pci.h"
+#include "rpaphp.h"
+#include "rpadlpar.h"
+
+#define MODULE_VERSION "1.0"
+#define MODULE_NAME "rpadlpar_io"
+
+static inline int is_hotplug_capable(struct device_node *dn)
+{
+	unsigned char *ptr = get_property(dn, "ibm,fw-pci-hot-plug-ctrl", NULL);
+
+	return (int) (ptr != NULL);
+}
+
+static char *get_node_drc_name(struct device_node *dn)
+{
+	char *ptr = NULL;
+	int *drc_names;
+
+	if ((drc_names = (int *) get_property(dn, "ibm,drc-names", NULL)))
+		ptr = (char *) &drc_names[1];
+
+	return ptr;
+}
+
+static struct device_node *find_php_slot_node(char *drc_name)
+{
+	struct device_node *np = NULL;
+	char *name;
+
+	while ((np = of_find_node_by_type(np, "pci")))
+		if (is_hotplug_capable(np)) {
+			name = get_node_drc_name(np);
+			if (name && (!strcmp(drc_name, name)))
+				break;
+		}
+
+	return np;
+}
+
+static inline struct hotplug_slot *find_php_slot(char *drc_name)
+{
+	struct kobject *k;
+
+	if (!(k = kset_find_obj(&pci_hotplug_slots_subsys.kset, drc_name)))
+		return NULL;
+
+	return to_hotplug_slot(k);
+}
+
+static struct slot *find_slot(char *drc_name)
+{
+	struct hotplug_slot *php_slot;
+
+	if (!(php_slot = find_php_slot(drc_name)))
+		return NULL;
+
+	return (struct slot *) php_slot->private;
+}
+
+static void rpadlpar_claim_one_bus(struct pci_bus *b)
+{
+	struct list_head *ld;
+	struct pci_bus *child_bus;
+
+	for (ld = b->devices.next; ld != &b->devices; ld = ld->next) {
+		struct pci_dev *dev = pci_dev_b(ld);
+		int i;
+
+		for (i = 0; i < PCI_NUM_RESOURCES; i++) {
+			struct resource *r = &dev->resource[i];
+
+			if (r->parent || !r->start || !r->flags)
+				continue;
+			rpaphp_claim_resource(dev, i);
+		}
+	}
+
+	list_for_each_entry(child_bus, &b->children, node)
+		rpadlpar_claim_one_bus(child_bus);
+}
+
+static int pci_add_secondary_bus(struct device_node *dn,
+		struct pci_dev *bridge_dev)
+{
+	struct pci_controller *hose = dn->phb;
+	struct pci_bus *child;
+	u8 sec_busno;
+
+	/* Get busno of downstream bus */
+	pci_read_config_byte(bridge_dev, PCI_SECONDARY_BUS, &sec_busno);
+
+	/* Allocate and add to children of bridge_dev->bus */
+	child = pci_add_new_bus(bridge_dev->bus, bridge_dev, sec_busno);
+	if (!child) {
+		printk(KERN_ERR "%s: could not add secondary bus\n", __FUNCTION__);
+		return 1;
+	}
+
+	sprintf(child->name, "PCI Bus #%02x", child->number);
+
+	/* Fixup subordinate bridge bases and resources */
+	pcibios_fixup_bus(child);
+
+	/* Claim new bus resources */
+	rpadlpar_claim_one_bus(bridge_dev->bus);
+
+	if (hose->last_busno < child->number)
+	    	hose->last_busno = child->number;
+
+	dn->bussubno = child->number;
+
+	/* ioremap() for child bus */
+	if (remap_bus_range(child)) {
+		printk(KERN_ERR "%s: could not ioremap() child bus\n",
+				__FUNCTION__);
+		return 1;
+	}
+
+	return 0;
+}
+
+static struct pci_dev *dlpar_pci_add_bus(struct device_node *dn)
+{
+	struct pci_controller *hose = dn->phb;
+	struct pci_dev *dev = NULL;
+
+	/* Scan phb bus for devices, adding new ones to bus->devices */
+	if (!pci_scan_slot(hose->bus, dn->devfn)) {
+		printk(KERN_ERR "%s: found no devices on bus\n", __FUNCTION__);
+		return NULL;
+	}
+
+	/* Add new devices to global lists.  Register in proc, sysfs. */
+	pci_bus_add_devices(hose->bus);
+
+	/* Confirm new bridge dev was created */
+	if (!(dev = rpaphp_find_pci_dev(dn))) {
+		printk(KERN_ERR "%s: failed to add pci device\n", __FUNCTION__);
+		return NULL;
+	}
+
+	if (dev->hdr_type != PCI_HEADER_TYPE_BRIDGE)  {
+		printk(KERN_ERR "%s: unexpected header type %d\n",
+				__FUNCTION__, dev->hdr_type);
+		return NULL;
+	}
+
+	if (pci_add_secondary_bus(dn, dev))
+		return NULL;
+
+	return dev;
+}
+
+static int dlpar_pci_remove_bus(struct pci_dev *bridge_dev)
+{
+	struct pci_bus *secondary_bus;
+
+	if (!bridge_dev) {
+		printk(KERN_ERR "%s: %s() unexpected null device\n", 
+				MODULE_NAME, __FUNCTION__);
+		return 1;
+	}
+
+	secondary_bus = bridge_dev->subordinate;
+
+	if (unmap_bus_range(secondary_bus)) {
+		printk(KERN_ERR "%s: failed to unmap bus range\n", 
+				__FUNCTION__);
+		return 1;
+	}
+
+	pci_remove_bus_device(bridge_dev);
+
+	return 0;
+}
+
+/**
+ * dlpar_add_slot - DLPAR add an I/O Slot
+ * @drc_name: drc-name of newly added slot
+ *
+ * Make the hotplug module and the kernel aware
+ * of a newly added I/O Slot.
+ * Return Codes -
+ * 0			Success
+ * -ENODEV		Not a valid drc_name
+ * -EINVAL		Slot already added
+ * -EIO			Internal PCI Error
+ */
+int dlpar_add_slot(char *drc_name)
+{
+	struct device_node *dn = find_php_slot_node(drc_name);
+	struct pci_dev *dev;
+
+	if (!dn)
+		return -ENODEV;
+
+	/* Check for existing hotplug slot */
+	if (find_slot(drc_name))
+		return -EINVAL;
+
+	/* Add pci bus */
+	dev = dlpar_pci_add_bus(dn);
+	if (!dev) {
+		printk(KERN_ERR "%s: unable to add bus %s\n", __FUNCTION__,
+				drc_name);
+		return -EIO;
+	}
+
+	/* Add hotplug slot for new bus */
+	if (rpaphp_add_slot(drc_name)) {
+		printk(KERN_ERR "%s: unable to add hotplug slot %s\n",
+				__FUNCTION__, drc_name);
+		return -EIO;
+	}
+
+	return 0;
+}
+
+/**
+ * dlpar_remove_slot - DLPAR remove an I/O Slot
+ * @drc_name: drc-name of newly added slot
+ *
+ * Remove the kernel and hotplug representations
+ * of an I/O Slot.
+ * Return Codes:
+ * 0			Success
+ * -ENODEV		Not a valid drc_name
+ * -EINVAL		Slot already removed
+ * -EIO			Internal PCI Error
+ */
+int dlpar_remove_slot(char *drc_name)
+{
+	struct device_node *dn = find_php_slot_node(drc_name);
+	struct slot *slot;
+	struct pci_dev *bridge_dev;
+
+	if (!dn)
+		return -ENODEV;
+
+	if (!(slot = find_slot(drc_name)))
+		return -EINVAL;
+
+	bridge_dev = slot->bridge;
+	if (!bridge_dev) {
+		printk(KERN_ERR "%s: %s(): unexpected null bridge device\n",
+				MODULE_NAME, __FUNCTION__);
+		return -EIO;
+	}
+
+	/* Remove hotplug slot */
+	if (rpaphp_remove_slot(slot)) {
+		printk(KERN_ERR "%s: %s(): unable to remove hotplug slot %s\n",
+				MODULE_NAME, __FUNCTION__, drc_name);
+		return -EIO;
+	}
+
+	/* Remove pci bus */
+	if (dlpar_pci_remove_bus(bridge_dev)) {
+		printk(KERN_ERR "%s: %s() unable to remove pci bus %s\n",
+				MODULE_NAME, __FUNCTION__, drc_name);
+		return -EIO;
+	}
+
+	return 0;
+}
+
+static inline int is_dlpar_capable(void)
+{
+	int rc = rtas_token("ibm,configure-connector");
+
+	return (int) (rc != RTAS_UNKNOWN_SERVICE);
+}
+
+int __init rpadlpar_io_init(void)
+{
+	int rc;
+
+	if (!is_dlpar_capable()) {
+		printk(KERN_WARNING "partition not DLPAR capable, exiting %s\n",
+				MODULE_NAME);
+		return -EPERM;
+	}
+
+	if ((rc = dlpar_sysfs_init()))
+		return rc;
+
+	return 0;
+}
+
+void rpadlpar_io_exit(void)
+{
+	dlpar_sysfs_exit();
+	return;
+}
+
+module_init(rpadlpar_io_init);
+module_exit(rpadlpar_io_exit);
+MODULE_LICENSE("GPL");
diff -purN linux-2.5/drivers/pci/hotplug/rpadlpar_sysfs.c linuxppc64-2.5/drivers/pci/hotplug/rpadlpar_sysfs.c
--- linux-2.5/drivers/pci/hotplug/rpadlpar_sysfs.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/pci/hotplug/rpadlpar_sysfs.c	2003-11-19 17:06:21.000000000 +0000
@@ -0,0 +1,148 @@
+/*
+ * Interface for Dynamic Logical Partitioning of I/O Slots
+ *
+ * John Rose <johnrose@austin.ibm.com>
+ * October 2003
+ *
+ * Copyright (C) 2003 IBM.
+ *
+ *      This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+#include <linux/kobject.h>
+#include <linux/string.h>
+#include "pci_hotplug.h"
+#include "rpadlpar.h"
+
+#define DLPAR_KOBJ_NAME       "control"
+#define ADD_SLOT_ATTR_NAME    "add_slot"
+#define REMOVE_SLOT_ATTR_NAME "remove_slot"
+
+#define MAX_DRC_NAME_LEN 64
+
+/* Store return code of dlpar operation in attribute struct */
+struct dlpar_io_attr {
+	int rc;
+	struct attribute attr;
+	ssize_t (*store)(struct dlpar_io_attr *dlpar_attr, const char *buf,
+		size_t nbytes);
+};
+
+/* Common show callback for all attrs, display the return code
+ * of the dlpar op */
+static ssize_t
+dlpar_attr_show(struct kobject * kobj, struct attribute * attr, char * buf)
+{
+	struct dlpar_io_attr *dlpar_attr = container_of(attr,
+						struct dlpar_io_attr, attr);
+	return sprintf(buf, "%d\n", dlpar_attr->rc);
+}
+
+static ssize_t
+dlpar_attr_store(struct kobject * kobj, struct attribute * attr,
+		 const char *buf, size_t nbytes)
+{
+	struct dlpar_io_attr *dlpar_attr = container_of(attr,
+						struct dlpar_io_attr, attr);
+	return dlpar_attr->store ?
+		dlpar_attr->store(dlpar_attr, buf, nbytes) : 0;
+}
+
+static struct sysfs_ops dlpar_attr_sysfs_ops = {
+	.show = dlpar_attr_show,
+	.store = dlpar_attr_store,
+};
+
+static ssize_t add_slot_store(struct dlpar_io_attr *dlpar_attr,
+				const char *buf, size_t nbytes)
+{
+	char drc_name[MAX_DRC_NAME_LEN];
+	char *end;
+
+	if (nbytes > MAX_DRC_NAME_LEN)
+		return 0;
+
+	memcpy(drc_name, buf, nbytes);
+
+	if (!(end = strchr(drc_name, '\n')))
+		end = &drc_name[nbytes];
+	*end = '\0';
+
+	dlpar_attr->rc = dlpar_add_slot(drc_name);
+
+	return nbytes;
+}
+
+static ssize_t remove_slot_store(struct dlpar_io_attr *dlpar_attr,
+		 		const char *buf, size_t nbytes)
+{
+	char drc_name[MAX_DRC_NAME_LEN];
+	char *end;
+
+	if (nbytes > MAX_DRC_NAME_LEN)
+		return 0;
+
+	memcpy(drc_name, buf, nbytes);
+
+	if (!(end = strchr(drc_name, '\n')))
+		end = &drc_name[nbytes];
+	*end = '\0';
+
+	dlpar_attr->rc = dlpar_remove_slot(drc_name);
+
+	return nbytes;
+}
+
+static struct dlpar_io_attr add_slot_attr = {
+	.rc = 0,
+	.attr = { .name = ADD_SLOT_ATTR_NAME, .mode = 0644, },
+	.store = add_slot_store,
+};
+
+static struct dlpar_io_attr remove_slot_attr = {
+	.rc = 0,
+	.attr = { .name = REMOVE_SLOT_ATTR_NAME, .mode = 0644},
+	.store = remove_slot_store,
+};
+
+static struct attribute *default_attrs[] = {
+	&add_slot_attr.attr,
+	&remove_slot_attr.attr,
+	NULL,
+};
+
+static void dlpar_io_release(struct kobject *kobj)
+{
+	/* noop */
+	return;	
+}
+
+struct kobj_type ktype_dlpar_io = {
+	.release = dlpar_io_release,
+	.sysfs_ops = &dlpar_attr_sysfs_ops,
+	.default_attrs = default_attrs,
+};
+
+struct kset dlpar_io_kset = {
+	.subsys = &pci_hotplug_slots_subsys,
+	.kobj = {.name = DLPAR_KOBJ_NAME, .ktype=&ktype_dlpar_io,},
+	.ktype = &ktype_dlpar_io,
+};
+
+int dlpar_sysfs_init(void)
+{
+	if (kset_register(&dlpar_io_kset)) {
+		printk(KERN_ERR "rpadlpar_io: cannot register kset for %s\n",
+				dlpar_io_kset.kobj.name);
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+void dlpar_sysfs_exit(void)
+{
+	kset_unregister(&dlpar_io_kset);
+}
diff -purN linux-2.5/drivers/pci/hotplug/rpaphp.h linuxppc64-2.5/drivers/pci/hotplug/rpaphp.h
--- linux-2.5/drivers/pci/hotplug/rpaphp.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/pci/hotplug/rpaphp.h	2003-11-17 23:08:19.000000000 +0000
@@ -0,0 +1,106 @@
+/*
+ * PPC64 PCI Hot Plug Controller Driver
+ *
+ * Copyright (c) 2003 Linda Xie <lxie@us.ibm.com>
+ *
+ * All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or (at
+ * your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
+ * NON INFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ * Send feedback to <lxie@us.ibm.com>,
+ *
+ */
+
+#ifndef _PPC64PHP_H
+#define _PPC64PHP_H
+#include "pci_hotplug.h"
+
+#define DR_INDICATOR 9002
+#define DR_ENTITY_SENSE 9003
+
+#define POWER_ON	100
+#define POWER_OFF	0
+
+#define LED_OFF		0 
+#define LED_ON		1	/* continuous on */ 
+#define LED_ID		2	/* slow blinking */
+#define LED_ACTION	3	/* fast blinking */
+
+#define SLOT_NAME_SIZE 12
+
+/* Error status from rtas_get-sensor */
+#define NEED_POWER    -9000     /* slot must be power up and unisolated to get state */
+#define PWR_ONLY      -9001     /* slot must be powerd up to get state, leave isolated */
+#define ERR_SENSE_USE -9002     /* No DR operation will succeed, slot is unusable  */
+
+/* Sensor values from rtas_get-sensor */
+#define EMPTY           0       /* No card in slot */
+#define PRESENT         1       /* Card in slot */
+
+#if !defined(CONFIG_HOTPLUG_PCI_MODULE)
+	#define MY_NAME "rpaphp"
+#else
+	#define MY_NAME THIS_MODULE->name
+#endif
+
+
+#define dbg(format, arg...)					\
+	do {							\
+		if (rpaphp_debug)				\
+			printk(KERN_DEBUG "%s: " format,	\
+				MY_NAME , ## arg); 		\
+	} while (0)
+#define err(format, arg...) printk(KERN_ERR "%s: " format, MY_NAME , ## arg)
+#define info(format, arg...) printk(KERN_INFO "%s: " format, MY_NAME , ## arg)
+#define warn(format, arg...) printk(KERN_WARNING "%s: " format, MY_NAME , ## arg)
+
+#define SLOT_MAGIC	0x67267322
+
+/* slot states */
+
+#define	NOT_VALID	3
+#define	NOT_CONFIGURED	2
+#define	CONFIGURED	1
+#define	EMPTY		0
+
+/*
+ * struct slot - slot information for each *physical* slot
+ */
+struct slot {
+	u32	magic;
+        int     state;
+        u32     index;
+        u32     type;
+        u32     power_domain;
+        char    *name;
+	struct	device_node *dn;/* slot's device_node in OFDT		*/
+				/* dn has phb info			*/
+	struct	pci_dev	*bridge;/* slot's pci_dev in pci_devices	*/
+
+	struct	pci_dev	*dev;	/* pci_dev of device in this slot 	*/
+				/* it will be used for unconfig		*/ 
+				/* NULL if slot is empty		*/
+
+	struct  hotplug_slot    *hotplug_slot;
+	struct list_head	rpaphp_slot_list;
+};
+
+extern struct pci_dev *rpaphp_find_pci_dev(struct device_node *dn);
+extern int rpaphp_add_slot(char *slot_name);
+extern int rpaphp_remove_slot(struct slot *slot);
+extern int rpaphp_claim_resource(struct pci_dev *dev, int resource);
+
+#endif /* _PPC64PHP_H */
diff -purN linux-2.5/drivers/pci/hotplug/rpaphp_core.c linuxppc64-2.5/drivers/pci/hotplug/rpaphp_core.c
--- linux-2.5/drivers/pci/hotplug/rpaphp_core.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/pci/hotplug/rpaphp_core.c	2003-12-09 17:03:38.000000000 +0000
@@ -0,0 +1,1040 @@
+/*
+ * PRA PCI Hot Plug Controller Driver
+ * Copyright (c) 2003 Linda Xie <lxie@us.ibm.com>
+ *
+ * All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or (at
+ * your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
+ * NON INFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ * Send feedback to <lxie@us.ibm.com>
+ *
+ */
+#include <linux/config.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/pci.h>
+#include <linux/slab.h>
+#include <linux/smp.h>
+#include <linux/smp_lock.h>
+#include <linux/init.h>
+#include <asm/rtas.h>		/* rtas_call */
+#include <asm/pci-bridge.h>	/* for pci_controller */
+#include "../pci.h"		/* for pci_add_new_bus*/
+				/* and pci_do_scan_bus*/
+#include "rpaphp.h"
+#include "pci_hotplug.h"
+
+
+static int debug; 
+static struct semaphore rpaphp_sem; 
+static int rpaphp_debug;
+static LIST_HEAD (rpaphp_slot_head);
+static int num_slots = 0;
+
+#define DRIVER_VERSION	"0.1"
+#define DRIVER_AUTHOR	"Linda Xie <lxie@us.ibm.com>"
+#define DRIVER_DESC	"RPA HOT Plug PCI Controller Driver"
+
+MODULE_AUTHOR(DRIVER_AUTHOR);
+MODULE_DESCRIPTION(DRIVER_DESC);
+MODULE_LICENSE("GPL");
+MODULE_PARM(debug, "i");
+MODULE_PARM_DESC(debug, "Debugging mode enabled or not");
+
+static int enable_slot		(struct hotplug_slot *slot);
+static int disable_slot		(struct hotplug_slot *slot);
+static int set_attention_status (struct hotplug_slot *slot, u8 value);
+static int get_power_status	(struct hotplug_slot *slot, u8 *value);
+static int get_attention_status	(struct hotplug_slot *slot, u8 *value);
+static int get_adapter_status	(struct hotplug_slot *slot, u8 *value);
+static int get_max_bus_speed	(struct hotplug_slot *hotplug_slot, enum pci_bus_speed *value);
+static int get_cur_bus_speed	(struct hotplug_slot *hotplug_slot, enum pci_bus_speed *value);
+
+static struct hotplug_slot_ops rpaphp_hotplug_slot_ops = {
+	.owner			= THIS_MODULE,
+	.enable_slot		= enable_slot,
+	.disable_slot		= disable_slot,
+	.set_attention_status	= set_attention_status,
+	.get_power_status	= get_power_status,
+	.get_attention_status	= get_attention_status,
+	.get_adapter_status	= get_adapter_status,
+	.get_max_bus_speed	= get_max_bus_speed,
+	.get_cur_bus_speed	= get_cur_bus_speed,
+};
+
+static int rpaphp_get_sensor_state(int index, int *state)
+{
+	int rc;
+
+        rc = rtas_get_sensor(DR_ENTITY_SENSE, index, state);
+			
+        if (rc) {
+		if (rc ==  NEED_POWER || rc == PWR_ONLY) {
+			dbg("%s: slot must be power up to get sensor-state\n", 
+				__FUNCTION__);
+		} else if (rc == ERR_SENSE_USE) 
+			info("%s: slot is unusable\n", __FUNCTION__);
+		   else err("%s failed to get sensor state\n", __FUNCTION__);
+	}
+	return rc;
+}
+
+static struct pci_dev *rpaphp_find_bridge_pdev(struct slot *slot)
+{
+	struct pci_dev		*retval_dev = NULL;
+			
+	retval_dev = rpaphp_find_pci_dev(slot->dn);	
+
+	return retval_dev;
+}
+
+static struct pci_dev *rpaphp_find_adapter_pdev(struct slot *slot)
+{
+	struct pci_dev * retval_dev = NULL;
+
+	retval_dev = rpaphp_find_pci_dev(slot->dn->child);	
+
+	return retval_dev;
+}
+
+
+/* Inline functions to check the sanity of a pointer that is passed to us */
+static inline int slot_paranoia_check(struct slot *slot, const char *function)
+{
+	if (!slot) {
+		dbg("%s - slot == NULL\n", function);
+		return -1;
+	}
+	
+	if (!slot->hotplug_slot) {
+		dbg("%s - slot->hotplug_slot == NULL!\n", function);
+		return -1;
+	}
+	return 0;
+}
+
+static inline struct slot *get_slot(struct hotplug_slot *hotplug_slot, const char *function)
+{
+	struct slot *slot;
+
+	if (!hotplug_slot) {
+		dbg("%s - hotplug_slot == NULL\n", function);
+		return NULL;
+	}
+
+	slot = (struct slot *)hotplug_slot->private;
+	if (slot_paranoia_check(slot, function))
+                return NULL;
+	return slot;
+}
+
+static inline int rpaphp_set_attention_status(struct slot *slot, u8 status)
+{
+	int	rc;
+
+	dbg("Entry %s: status=%d\n", __FUNCTION__, status);
+	
+	/* status: LED_OFF or LED_ON */
+	rc = rtas_set_indicator(DR_INDICATOR, slot->index, status);
+	if (rc)
+		err("slot(%s) set attention-status(%d) failed! rc=0x%x\n", 
+			slot->name, status, rc); 
+
+	dbg("Exit %s, rc=0x%x\n", __FUNCTION__, rc);
+
+	return rc;
+}
+
+static int rpaphp_get_power_status(struct slot *slot, u8 *value)
+{
+	int	rc;
+
+        rc = rtas_get_power_level(slot->power_domain, (int *)value);
+        if (rc) 
+                err("failed to get power-level for slot(%s), rc=0x%x\n", 
+			slot->name, rc);
+	
+        return rc;
+}
+
+static int rpaphp_get_attention_status(struct slot *slot)
+{
+
+	return slot->hotplug_slot->info->attention_status;
+}
+
+/**
+ * set_attention_status - set attention LED
+ * echo 0 > attention -- set LED OFF
+ * echo 1 > attention -- set LED ON
+ * echo 2 > attention -- set LED ID(identify, light is blinking)
+ *
+ */
+static int set_attention_status (struct hotplug_slot *hotplug_slot, u8 value)
+{
+	int retval = 0;
+	struct slot *slot = get_slot(hotplug_slot, __FUNCTION__);
+
+	if (slot == NULL)
+		return -ENODEV;
+
+	dbg("%s - Entry: slot[%s] value[0x%x]\n", 
+		__FUNCTION__, slot->name, value);
+	down(&rpaphp_sem);
+	switch (value) {
+		case 0:
+			retval = rpaphp_set_attention_status(slot, LED_OFF);
+			hotplug_slot->info->attention_status = 0;
+			break;
+
+		case 1:
+		default:
+			retval = rpaphp_set_attention_status(slot, LED_ON);
+			hotplug_slot->info->attention_status = 1;
+			break;
+
+		case 2:
+			retval = rpaphp_set_attention_status(slot, LED_ID);
+			hotplug_slot->info->attention_status = 2;
+			break;
+
+	}
+	up(&rpaphp_sem);
+
+	dbg("%s - Exit: rc[%d]\n",  __FUNCTION__, retval);
+	return retval;
+}
+
+/**
+ * get_power_status - get power status of a slot
+ * @hotplug_slot: slot to get status
+ * @value: pointer to store status
+ *
+ *
+ */
+static int get_power_status (struct hotplug_slot *hotplug_slot, u8 *value)
+{
+	int retval;
+	struct slot *slot = get_slot(hotplug_slot, __FUNCTION__);
+	
+	if (slot == NULL)
+		return -ENODEV;
+
+	down(&rpaphp_sem);
+	retval = rpaphp_get_power_status(slot, value);
+	up(&rpaphp_sem);
+
+	return retval;
+}
+
+/**
+ * get_attention_status - get attention LED status
+ *
+ *
+ */
+static int get_attention_status (struct hotplug_slot *hotplug_slot, u8 *value)
+{
+	int retval = 0;
+	struct slot *slot = get_slot(hotplug_slot, __FUNCTION__);
+
+	if (slot == NULL)
+		return -ENODEV;
+
+
+	dbg("%s - Entry: slot[%s]\n",
+		__FUNCTION__, slot->name);
+
+	down(&rpaphp_sem);
+	*value = rpaphp_get_attention_status(slot);
+	up(&rpaphp_sem);
+
+	dbg("%s - Exit: value[0x%x] rc[%d]\n",
+		__FUNCTION__, *value, retval);
+	return retval;
+}
+
+/*
+ * get_adapter_status - get  the status of a slot
+ * 
+ * 0-- slot is empty
+ * 1-- adapter is configured
+ * 2-- adapter is not configured
+ * 3-- not valid
+ */
+static int rpaphp_get_adapter_status(struct slot *slot, int is_init, u8 *value)
+{
+	int	state, rc;
+
+	dbg("Entry %s\n", __FUNCTION__);
+
+	*value 		  = NOT_VALID;	
+
+	rc = rpaphp_get_sensor_state(slot->index, &state);	
+		
+	if (rc) 
+		goto exit;		
+
+	if (state == PRESENT) {
+		dbg("slot is occupied\n");
+	   
+		if (!is_init) /* at run-time slot->state can be changed by */
+			  /* config/unconfig adapter	 		   */
+			*value = slot->state;
+		else {
+		if (!slot->dn->child) 
+			dbg("%s: %s is not valid OFDT node\n", 
+				__FUNCTION__, slot->dn->full_name);
+		else 
+			if (rpaphp_find_pci_dev(slot->dn->child)) 
+				*value = CONFIGURED;
+			else {
+				dbg("%s: can't find pdev of adapter in slot[%s]\n",
+					__FUNCTION__, slot->name);
+				*value = NOT_CONFIGURED;	
+				}
+		}
+	}
+	else
+		if (state == EMPTY) {
+		dbg("slot is empty\n");
+			*value = state;
+		}
+		 
+exit:    dbg("Exit %s slot[%s] has adapter-status %d rtas call's rc=0x%x\n",
+		__FUNCTION__, slot->name, *value, rc);
+
+	return rc;
+}
+
+static int get_adapter_status (struct hotplug_slot *hotplug_slot, u8 *value)
+{
+	struct slot *slot = get_slot(hotplug_slot, __FUNCTION__);
+	int retval = 0;
+
+	if (slot == NULL)
+		return -ENODEV;
+
+	down(&rpaphp_sem);
+
+	/*  have to go through this */
+	retval = rpaphp_get_adapter_status(slot, 0, value);
+
+	up(&rpaphp_sem);
+
+	return retval;
+}
+
+
+static int get_max_bus_speed (struct hotplug_slot *hotplug_slot, enum pci_bus_speed *value)
+{
+	struct slot *slot = get_slot(hotplug_slot, __FUNCTION__);
+
+	if (slot == NULL)
+		return -ENODEV;
+	
+	dbg("%s - Entry: slot->name[%s] slot->type[%d]\n",
+		__FUNCTION__, slot->name, slot->type);
+
+	down(&rpaphp_sem);	
+
+	switch (slot->type) {
+		case 1:	
+		case 2:
+		case 3:
+		case 4:
+		case 5:
+		case 6:
+			*value = PCI_SPEED_33MHz;	/* speed for case 1-6 */
+			break;
+		case 7:
+		case 8:
+			*value = PCI_SPEED_66MHz;
+			break;
+		case 11:
+		case 14:
+			*value = PCI_SPEED_66MHz_PCIX;
+			break;
+		case 12:
+		case 15:
+			*value = PCI_SPEED_100MHz_PCIX;
+			break;
+		case 13:
+		case 16:
+			*value = PCI_SPEED_133MHz_PCIX;
+			break;
+		default:
+			*value = PCI_SPEED_UNKNOWN;
+			break;
+ 
+	}
+
+	up(&rpaphp_sem);	
+
+	return 0;
+}
+
+
+/* return dummy value because not sure if PRA provides any method... */
+static int get_cur_bus_speed (struct hotplug_slot *hotplug_slot, enum pci_bus_speed *value)
+{
+	struct slot *slot = get_slot(hotplug_slot, __FUNCTION__);
+
+	if (slot == NULL)
+		return -ENODEV;
+
+	*value = PCI_SPEED_UNKNOWN;
+
+	return 0;
+}
+
+/* 
+ * rpaphp_validate_slot - make sure the name of the slot matches
+ * 				the location code , if the slots is not
+ *				empty.
+ */
+static int rpaphp_validate_slot(const char *slot_name, const int slot_index)
+{
+	struct device_node	*dn;
+	int			retval = 0;
+	
+	dbg("Entry %s: (name: %s index: 0x%x\n", 
+		__FUNCTION__, slot_name, slot_index);
+
+	for(dn = find_all_nodes(); dn; dn = dn->next) { 
+
+		int 		*index;
+		unsigned char	*loc_code;
+
+		index  = (int *)get_property(dn, "ibm,my-drc-index", NULL);
+
+		if (index && *index == slot_index) {
+		char *slash, tmp_str[128];
+
+			loc_code = get_property(dn, "ibm,loc-code", NULL);     
+		if (!loc_code) {
+			retval = -1;
+			goto exit;
+		}
+
+		dbg("%s: name=%s loc-code=%s index=0x%x\n",
+			__FUNCTION__, slot_name, loc_code, slot_index);
+		
+		strcpy(tmp_str, loc_code);
+		slash = strrchr(tmp_str, '/');
+		if (slash) {
+			*slash = '\0';
+		}
+		if (strcmp(slot_name, tmp_str)) 
+			retval = -1;
+		goto exit;	
+		}
+
+	}
+
+exit:
+	dbg("Exit %s with retval=%d\n", __FUNCTION__, retval);
+
+	return retval;
+}
+
+/* Must be called before pci_bus_add_devices */
+static void rpaphp_fixup_new_devices(struct pci_bus *bus)
+{
+	struct pci_dev *dev;
+	
+	dbg("Enter rpaphp_fixup_new_devices()\n");
+
+	list_for_each_entry(dev, &bus->devices, bus_list) {
+	/*
+	 * Skip already-present devices (which are on the
+	 * global device list.)
+	 */
+		if (list_empty(&dev->global_list)) {
+			int i;
+			pcibios_fixup_device_resources(dev, bus);
+			pci_read_irq_line(dev);
+			for (i = 0; i < PCI_NUM_RESOURCES; i++) {
+                        	struct resource *r = &dev->resource[i];
+                        	if (r->parent || !r->start || !r->flags)
+                                	continue;
+                        	rpaphp_claim_resource(dev, i);
+                	}
+
+		}
+	}
+}
+
+static struct pci_dev *rpaphp_config_adapter(struct slot *slot) 
+{
+	struct pci_bus 		*pci_bus;
+	struct device_node	*dn;
+	int 			num;
+	struct pci_dev		*dev = NULL;
+
+	dbg("Entry %s: slot[%s]\n",
+		__FUNCTION__, slot->name); 
+
+	if (slot->bridge) {
+				
+		pci_bus = slot->bridge->subordinate;
+		
+		if (!pci_bus) {
+			err("%s: can't find bus structure\n", __FUNCTION__);
+			goto exit;
+		}
+
+		for (dn = slot->dn->child; dn; dn = dn->sibling) {
+			dbg("child dn's devfn=[%x]\n", dn->devfn);
+				num = pci_scan_slot(pci_bus, 
+				PCI_DEVFN(PCI_SLOT(dn->devfn),  0));
+
+				dbg("pci_scan_slot return num=%d\n", num);
+
+			if (num) {
+				dbg("%s: calling rpaphp_fixup_new_devices()\n",
+					__FUNCTION__);
+				rpaphp_fixup_new_devices(pci_bus);
+				pci_bus_add_devices(pci_bus);
+			}
+		}
+
+		dev = rpaphp_find_pci_dev(slot->dn->child);
+	}
+	else {
+		/* slot is not enabled */
+		err("slot doesn't have pci_dev structure\n");
+		dev = NULL;
+		goto exit;
+	} 	
+
+exit:	
+	dbg("Exit %s: pci_dev %s\n", __FUNCTION__, dev? "found":"not found");
+
+	return dev;
+}
+
+static int rpaphp_unconfig_adapter(struct slot *slot) 
+{
+	int			retval = 0;
+
+	dbg("Entry %s: slot[%s]\n", 
+		__FUNCTION__, slot->name); 
+	if (!slot->dev) {
+		info("%s: no card in slot[%s]\n",
+			__FUNCTION__, slot->name);
+
+		retval = -EINVAL;
+		goto exit;	
+	}
+
+
+        /* remove the device from the pci core */
+        pci_remove_bus_device(slot->dev);
+
+        pci_dev_put(slot->dev);
+        slot->state = NOT_CONFIGURED;
+	
+	dbg("%s: adapter in slot[%s] unconfigured.\n", __FUNCTION__, slot->name);
+		
+exit:
+	dbg("Exit %s, rc=0x%x\n", __FUNCTION__, retval);
+
+	return retval;
+		
+}
+
+/* free up the memory user be a slot */
+
+static void rpaphp_release_slot(struct hotplug_slot *hotplug_slot)
+{
+	struct slot *slot = get_slot(hotplug_slot, __FUNCTION__);
+	
+	if (slot == NULL)
+		return;
+
+	dbg("%s - Entry: slot[%s]\n",
+		__FUNCTION__, slot->name);
+	kfree(slot->hotplug_slot->info);
+	kfree(slot->hotplug_slot->name);
+	kfree(slot->hotplug_slot);
+	pci_dev_put(slot->bridge);
+	pci_dev_put(slot->dev);
+	kfree(slot);
+	dbg("%s - Exit\n", __FUNCTION__);
+}
+
+int rpaphp_remove_slot(struct slot *slot)
+{
+	int retval = 0;
+
+	dbg("%s - Entry: slot[%s]\n",
+		__FUNCTION__, slot->name);
+
+  	sysfs_remove_link(slot->hotplug_slot->kobj.parent,
+                          slot->bridge->slot_name);
+	
+	list_del(&slot->rpaphp_slot_list);
+	retval = pci_hp_deregister(slot->hotplug_slot);
+	if (retval)
+		err("Problem unregistering a slot %s\n", slot->name);
+	num_slots--;
+
+	dbg("%s - Exit: rc[%d]\n", __FUNCTION__, retval);
+	return retval;	
+}
+
+static int is_php_dn(struct device_node *dn, int **indexes,  int **names, int **types, int **power_domains)
+{
+	*indexes = (int *)get_property(dn, "ibm,drc-indexes", NULL);
+	if (!*indexes)
+		return(0);
+
+	/* &names[1] contains NULL terminated slot names */
+	*names = (int *)get_property(dn, "ibm,drc-names", NULL);
+	if (!*names) 
+		return(0);
+
+	/* &types[1] contains NULL terminated slot types */
+	*types = (int *)get_property(dn, "ibm,drc-types", NULL);
+	if (!*types)
+		return(0);
+
+	/* power_domains[1...n] are the slot power domains */
+	*power_domains = (int *)get_property(dn,
+		"ibm,drc-power-domains", NULL);
+	if (!*power_domains) 
+		return(0);
+
+	if (!get_property(dn, "ibm,fw-pci-hot-plug-ctrl", NULL))
+		return(0);
+
+	return(1);
+}
+
+static struct slot *alloc_slot_struct(void)
+{
+	struct slot *slot;
+
+	slot = kmalloc(sizeof(struct slot), GFP_KERNEL);
+	if (!slot) 
+		return (NULL);
+	memset(slot, 0, sizeof(struct slot));
+	slot->hotplug_slot = kmalloc(sizeof(struct hotplug_slot), 	
+		GFP_KERNEL);
+	if (!slot->hotplug_slot) {
+		kfree(slot);
+		return (NULL);
+        }                
+	memset(slot->hotplug_slot, 0, sizeof(struct hotplug_slot));
+	slot->hotplug_slot->info = kmalloc(sizeof(struct hotplug_slot_info), 
+		GFP_KERNEL);
+	if (!slot->hotplug_slot->info) {
+		kfree(slot->hotplug_slot);
+		kfree(slot);
+		return (NULL);
+	}
+	memset(slot->hotplug_slot->info, 0, sizeof(struct hotplug_slot_info));
+	slot->hotplug_slot->name = kmalloc(SLOT_NAME_SIZE, GFP_KERNEL);
+	if (!slot->hotplug_slot->name) {
+		kfree(slot->hotplug_slot->info);
+		kfree(slot->hotplug_slot);
+		kfree(slot);
+		return (NULL);
+	}
+	return (slot);
+}
+
+static int setup_hotplug_slot_info(struct slot *slot)
+{
+	dbg("%s Initilize the slot info structure ...\n",
+		__FUNCTION__);
+
+	rpaphp_get_power_status(slot, 
+		&slot->hotplug_slot->info->power_status);  
+
+	rpaphp_get_adapter_status(slot, 1,
+		&slot->hotplug_slot->info->adapter_status); 
+
+	if (slot->hotplug_slot->info->adapter_status == NOT_VALID) {
+		dbg("%s: NOT_VALID: skip dn->full_name=%s\n", 
+			__FUNCTION__, slot->dn->full_name);
+		    kfree(slot->hotplug_slot->info);
+		    kfree(slot->hotplug_slot->name);
+		    kfree(slot->hotplug_slot);
+		    kfree(slot);
+		return (-1);
+	}
+	return (0);
+}
+
+static int register_slot(struct slot *slot)
+{
+	int retval; 
+
+	retval = pci_hp_register(slot->hotplug_slot);
+	if (retval) {
+		err("pci_hp_register failed with error %d\n", retval);
+		rpaphp_release_slot(slot->hotplug_slot);
+		return (retval);
+	}
+	/* create symlink between slot->name and it's bus_id */
+	dbg("%s: sysfs_create_link: %s --> %s\n", __FUNCTION__,
+		slot->bridge->slot_name, slot->name);					
+	retval = sysfs_create_link(slot->hotplug_slot->kobj.parent,
+			&slot->hotplug_slot->kobj,
+			slot->bridge->slot_name);
+	if (retval) {
+		err("sysfs_create_link failed with error %d\n", retval);
+		rpaphp_release_slot(slot->hotplug_slot);
+		return (retval);
+	}
+	/* add slot to our internal list */
+	dbg("%s adding slot[%s] to rpaphp_slot_list\n", 
+		__FUNCTION__, slot->name);
+
+	list_add(&slot->rpaphp_slot_list, &rpaphp_slot_head);
+
+	info("Slot [%s] (bus_id=%s) registered\n", 
+		slot->name, slot->bridge->slot_name);
+	return (0);
+}
+
+/*************************************
+ * Add  Hot Plug slot(s) to sysfs
+ *
+ ************************************/
+int rpaphp_add_slot(char *slot_name)
+{
+	struct slot		*slot;
+	int 			retval = 0;
+	int 			i;
+        struct device_node 	*dn;
+        int 			*indexes, *names, *types, *power_domains;
+        char 			*name, *type;
+
+	dbg("Entry %s: %s\n", __FUNCTION__,
+			slot_name? slot_name: "init");
+
+	for (dn = find_all_nodes(); dn; dn = dn->next) {
+
+		if (dn->name != 0 && strcmp(dn->name, "pci") == 0)	{
+			if (!is_php_dn(dn, &indexes, &names, &types, &power_domains))
+				continue;
+			
+			dbg("%s : found device_node in OFDT full_name=%s, name=%s\n",
+				__FUNCTION__, dn->full_name, dn->name);
+
+			name = (char *)&names[1];
+			type = (char *)&types[1];
+			
+			dbg("%s: indexes=%d\n", __FUNCTION__, indexes[0]);
+
+			for (i = 0; i < indexes[0]; 
+				i++, 
+				name += (strlen(name) + 1),
+				type += (strlen(type) + 1)) {
+
+				dbg("%s: name[%s] index[%x]\n",
+					__FUNCTION__, name, indexes[i+1]);
+
+				if (slot_name && strcmp(slot_name, name)) 
+					continue;
+		
+				if (rpaphp_validate_slot(name, indexes[i + 1])) {
+					dbg("%s: slot(%s, 0x%x) is invalid.\n",
+						__FUNCTION__, name, indexes[i+ 1]);
+					continue;
+				}
+
+				if (!(slot = alloc_slot_struct())) {
+					retval = -ENOMEM;
+					goto exit;
+				}
+
+				slot->name = slot->hotplug_slot->name;
+				slot->index = indexes[i + 1];
+				strcpy(slot->name, name);
+				slot->type = simple_strtoul(type, NULL, 10);	
+				if (slot->type < 1  || slot->type > 16)
+					slot->type = 0;
+
+				slot->power_domain = power_domains[i + 1];
+				slot->magic = SLOT_MAGIC;
+				slot->hotplug_slot->private = slot;
+				slot->hotplug_slot->ops = &rpaphp_hotplug_slot_ops;
+				slot->hotplug_slot->release = &rpaphp_release_slot;
+				slot->dn = dn;
+
+				/*
+			 	* Initilize the slot info structure with some known 
+			 	* good values.
+			 	*/
+				if (setup_hotplug_slot_info(slot))
+					continue;
+
+				slot->bridge = rpaphp_find_bridge_pdev(slot);
+				if (!slot->bridge && slot_name) { /* slot being added doesn't have pci_dev yet*/
+					dbg("%s: no pci_dev for bridge dn %s\n", 
+							__FUNCTION__, slot_name);
+					    kfree(slot->hotplug_slot->info);
+					    kfree(slot->hotplug_slot->name);
+					    kfree(slot->hotplug_slot);
+					    kfree(slot);
+					continue;
+				}
+ 
+				/* find slot's pci_dev if it's not empty*/
+				if (slot->hotplug_slot->info->adapter_status == EMPTY) {
+					slot->state = EMPTY;  /* slot is empty */
+					slot->dev = NULL;
+				}
+				else {  /* slot is occupied */
+					if(!(slot->dn->child)) { /* non-empty slot has to have child */
+						err("%s: slot[%s]'s device_node doesn't have child for adapter\n",
+						__FUNCTION__, slot->name);
+						kfree(slot->hotplug_slot->info);
+						kfree(slot->hotplug_slot->name);
+						kfree(slot->hotplug_slot);
+						kfree(slot);
+						continue;
+
+					}
+					
+					slot->dev = rpaphp_find_adapter_pdev(slot);
+
+					if (!slot->dev && slot_name) { 
+						 /* adapter being added doesn't have pci_dev yet */
+						slot->dev = rpaphp_config_adapter(slot);
+						if (!slot->dev) {
+							err("%s: add new adapter device for slot[%s] failed\n",
+							__FUNCTION__, slot->name);
+							kfree(slot->hotplug_slot->info);
+							kfree(slot->hotplug_slot->name);
+							kfree(slot->hotplug_slot);
+							kfree(slot);
+							pci_dev_put(slot->bridge);
+							continue;
+
+						}
+					}
+
+					if(slot->dev) {
+						slot->state = CONFIGURED;
+						pci_dev_get(slot->dev);
+					}
+					else
+						slot->state = NOT_CONFIGURED;
+				}
+				dbg("%s registering slot:path[%s] index[%x], name[%s] pdomain[%x] type[%d]\n",
+					__FUNCTION__, dn->full_name, slot->index, slot->name, 
+					slot->power_domain, slot->type);	
+
+				if ((retval = register_slot(slot)))
+					goto exit;
+
+				num_slots++;
+			
+				if (slot_name)  
+					goto exit;
+
+			}/* for indexes */
+		}/* "pci" */
+	}/* find_all_nodes */
+exit:
+	dbg("%s - Exit: num_slots=%d rc[%d]\n", 
+		__FUNCTION__, num_slots, retval);
+	return retval;
+}
+
+/*
+ * init_slots - initialize 'struct slot' structures for each slot
+ *
+ */
+static int init_slots (void)
+{
+	int 			retval = 0;
+
+	dbg("Entry %s\n", __FUNCTION__);
+
+	retval = rpaphp_add_slot(NULL);
+
+	dbg("Exit %s with retval=%d\n", __FUNCTION__, retval);
+
+	return retval;
+}
+
+
+static int init_rpa (void)
+{
+	int 			retval = 0;
+
+	dbg("Entry %s\n", __FUNCTION__);
+
+	init_MUTEX(&rpaphp_sem);
+	
+	/* initialize internal data structure etc. */
+	retval = init_slots();
+	if (!num_slots)
+		retval = -ENODEV;
+	
+	dbg("Exit %s with retval=%d, num_slots=%d\n", 
+		__FUNCTION__, retval, num_slots);
+
+	return retval;
+}
+
+static void cleanup_slots (void)
+{
+	struct list_head *tmp, *n;
+	struct slot *slot;
+
+	/*
+	 * Unregister all of our slots with the pci_hotplug subsystem,
+	 * and free up all memory that we had allocated.
+	 * memory will be freed in release_slot callback. 
+	 */
+
+	list_for_each_safe (tmp, n, &rpaphp_slot_head) {
+		slot = list_entry(tmp, struct slot, rpaphp_slot_list);
+		sysfs_remove_link(slot->hotplug_slot->kobj.parent, 
+			slot->bridge->slot_name);
+		list_del(&slot->rpaphp_slot_list);
+		pci_hp_deregister(slot->hotplug_slot);
+	}
+
+	return;
+}
+
+
+static int __init rpaphp_init(void)
+{
+	int retval = 0;
+
+	dbg("Entry %s\n", __FUNCTION__);
+	info(DRIVER_DESC " version: " DRIVER_VERSION "\n");
+
+	rpaphp_debug = debug;
+
+	/* read all the PRA info from the system */
+	retval = init_rpa();
+
+	dbg("Exit %s with retval=%d\n", __FUNCTION__, retval);
+	return retval;
+}
+
+
+static void __exit rpaphp_exit(void)
+{
+	cleanup_slots();
+}
+
+
+static int enable_slot(struct hotplug_slot *hotplug_slot)
+{
+	int retval = 0, state;
+
+	struct slot *slot = get_slot(hotplug_slot, __FUNCTION__);
+
+	if (slot == NULL)
+		return -ENODEV;
+
+	dbg("%s - Entry: slot[%s]\n",
+		__FUNCTION__, slot->name);
+	
+	dbg("ENABLING SLOT %s\n", slot->name);
+
+	down(&rpaphp_sem);
+
+	retval = rpaphp_get_sensor_state(slot->index, &state);	
+		
+	if (retval) 
+		goto exit;		
+
+	dbg("%s: sensor state[%d]\n", __FUNCTION__, state);
+
+	/* if slot is not empty, enable the adapter */	
+	if (state == PRESENT) {
+		dbg("%s : slot[%s] is occupid.\n", __FUNCTION__, slot->name);
+
+		if ((slot->dev = rpaphp_config_adapter(slot)) != NULL) {
+			slot->state = CONFIGURED;
+
+			dbg("%s: adapter %s in slot[%s] has been configured\n",
+				__FUNCTION__, slot->dev->slot_name,
+				slot->name);
+		}
+		else {
+			slot->state = NOT_CONFIGURED;
+
+			dbg("%s: no pci_dev struct for adapter in slot[%s]\n",
+				__FUNCTION__, slot->name);
+		}
+
+	}
+	else if (state == EMPTY) { 
+		dbg("%s : slot[%s] is empty\n", __FUNCTION__, slot->name);
+		slot->state = EMPTY;
+	}
+	else {
+		err("%s: slot[%s] is in invalid state\n", __FUNCTION__, slot->name);
+		slot->state = NOT_VALID;
+		retval = -EINVAL;
+	}
+	
+exit:	
+	if (slot->state != NOT_VALID)
+		rpaphp_set_attention_status(slot, LED_ON);
+	else
+		rpaphp_set_attention_status(slot, LED_ID);
+
+	up(&rpaphp_sem);
+	dbg("%s - Exit: rc[%d]\n",  __FUNCTION__, retval);
+	
+        return retval;
+}
+
+static int disable_slot(struct hotplug_slot *hotplug_slot)
+{
+	int	retval;
+	struct slot *slot = get_slot(hotplug_slot, __FUNCTION__);
+	
+
+	if (slot == NULL)
+		return -ENODEV;
+	
+	dbg("%s - Entry: slot[%s]\n",
+		__FUNCTION__, slot->name);
+	dbg("DISABLING SLOT %s\n", slot->name);
+
+	down(&rpaphp_sem);
+
+	rpaphp_set_attention_status(slot, LED_ID);
+
+	retval = rpaphp_unconfig_adapter(slot);
+			
+	rpaphp_set_attention_status(slot, LED_OFF);
+
+	up(&rpaphp_sem);
+
+	dbg("%s - Exit: rc[%d]\n",  __FUNCTION__, retval);
+        return retval;
+}
+
+module_init(rpaphp_init);
+module_exit(rpaphp_exit);
+
+EXPORT_SYMBOL_GPL(rpaphp_add_slot);
+EXPORT_SYMBOL_GPL(rpaphp_remove_slot);
diff -purN linux-2.5/drivers/pci/hotplug/rpaphp_pci.c linuxppc64-2.5/drivers/pci/hotplug/rpaphp_pci.c
--- linux-2.5/drivers/pci/hotplug/rpaphp_pci.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/pci/hotplug/rpaphp_pci.c	2003-11-17 23:08:21.000000000 +0000
@@ -0,0 +1,75 @@
+/*
+ * PRA PCI Hot Plug Controller Driver
+ * Copyright (c) 2003 Linda Xie <lxie@us.ibm.com>
+ *
+ * All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or (at
+ * your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or
+ * NON INFRINGEMENT.  See the GNU General Public License for more
+ * details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ *
+ * Send feedback to <lxie@us.ibm.com>
+ *
+ */
+#include <linux/pci.h>
+#include <asm/pci-bridge.h>	/* for pci_controller */
+#include "rpaphp.h"
+
+
+struct pci_dev *rpaphp_find_pci_dev(struct device_node *dn)
+{
+	struct pci_dev		*retval_dev = NULL, *dev = NULL;
+
+	while ((dev = pci_get_device(PCI_ANY_ID, PCI_ANY_ID, dev)) != NULL) {
+		if(!dev->bus) 
+			continue;
+	
+		if (dev->devfn != dn->devfn) 
+			continue;
+		
+		if (dn->phb->global_number == pci_domain_nr(dev->bus) &&
+		    dn->busno == dev->bus->number) {
+			retval_dev = dev;
+			break;
+		}
+	}
+
+	return retval_dev;
+	
+}
+ 
+int rpaphp_claim_resource(struct pci_dev *dev, int resource)
+{
+	struct resource *res = &dev->resource[resource];
+	struct resource *root = pci_find_parent_resource(dev, res);
+	char *dtype = resource < PCI_BRIDGE_RESOURCES ? "device" : "bridge";
+	int err;
+
+	err = -EINVAL;
+	if (root != NULL) {
+		err = request_resource(root, res);
+	}
+
+	if (err) {
+		err("PCI: %s region %d of %s %s [%lx:%lx]\n",
+		       root ? "Address space collision on" :
+			      "No parent found for",
+		       resource, dtype, pci_name(dev), res->start, res->end);
+	}
+
+	return err;
+}
+
+EXPORT_SYMBOL_GPL(rpaphp_find_pci_dev);
+EXPORT_SYMBOL_GPL(rpaphp_claim_resource);
diff -purN linux-2.5/drivers/pci/pci-sysfs.c linuxppc64-2.5/drivers/pci/pci-sysfs.c
--- linux-2.5/drivers/pci/pci-sysfs.c	2003-08-15 01:17:20.000000000 +0000
+++ linuxppc64-2.5/drivers/pci/pci-sysfs.c	2003-09-12 11:04:02.000000000 +0000
@@ -15,6 +15,7 @@
 
 #include <linux/config.h>
 #include <linux/kernel.h>
+#include <linux/stat.h>
 #include <linux/pci.h>
 #include <linux/stat.h>
 
diff -purN linux-2.5/drivers/pci/probe.c linuxppc64-2.5/drivers/pci/probe.c
--- linux-2.5/drivers/pci/probe.c	2003-08-06 15:34:30.000000000 +0000
+++ linuxppc64-2.5/drivers/pci/probe.c	2003-11-05 22:12:33.000000000 +0000
@@ -176,7 +176,7 @@ void __devinit pci_read_bridge_bases(str
 		limit |= (io_limit_hi << 16);
 	}
 
-	if (base && base <= limit) {
+	if (base <= limit) {
 		res->flags = (io_base_lo & PCI_IO_RANGE_TYPE_MASK) | IORESOURCE_IO;
 		res->start = base;
 		res->end = limit + 0xfff;
diff -purN linux-2.5/fs/proc/proc_devtree.c linuxppc64-2.5/fs/proc/proc_devtree.c
--- linux-2.5/fs/proc/proc_devtree.c	2003-10-08 21:27:19.000000000 +0000
+++ linuxppc64-2.5/fs/proc/proc_devtree.c	2003-10-15 20:59:40.000000000 +0000
@@ -11,6 +11,20 @@
 #include <asm/prom.h>
 #include <asm/uaccess.h>
 
+#ifndef HAVE_ARCH_DEVTREE_FIXUPS
+static inline void set_node_proc_entry(struct device_node *np, struct proc_dir_entry *de)
+{
+}
+
+static void inline set_node_name_link(struct device_node *np, struct proc_dir_entry *de)
+{
+}
+
+static void inline set_node_addr_link(struct device_node *np, struct proc_dir_entry *de)
+{
+}
+#endif
+
 static struct proc_dir_entry *proc_device_tree;
 
 /*
@@ -44,7 +58,7 @@ static int property_read_proc(char *page
 /*
  * Process a node, adding entries for its children and its properties.
  */
-static void add_node(struct device_node *np, struct proc_dir_entry *de)
+void proc_device_tree_add_node(struct device_node *np, struct proc_dir_entry *de)
 {
 	struct property *pp;
 	struct proc_dir_entry *ent;
@@ -53,6 +67,7 @@ static void add_node(struct device_node 
 	int l;
 	struct proc_dir_entry *list, **lastp, *al;
 
+	set_node_proc_entry(np, de);
 	lastp = &list;
 	for (pp = np->properties; pp != 0; pp = pp->next) {
 		/*
@@ -70,7 +85,8 @@ static void add_node(struct device_node 
 		*lastp = ent;
 		lastp = &ent->next;
 	}
-	for (child = np->child; child != 0; child = child->sibling) {
+	child = NULL;
+	while ((child = of_get_next_child(np, child))) {
 		p = strrchr(child->full_name, '/');
 		if (p == 0)
 			p = child->full_name;
@@ -85,7 +101,7 @@ static void add_node(struct device_node 
 			break;
 		*lastp = ent;
 		lastp = &ent->next;
-		add_node(child, ent);
+		proc_device_tree_add_node(child, ent);
 
 		/*
 		 * If we left the address part on the name, consider
@@ -98,26 +114,32 @@ static void add_node(struct device_node 
 		 * If this is the first node with a given name property,
 		 * add a symlink with the name property as its name.
 		 */
-		for (sib = np->child; sib != child; sib = sib->sibling)
+		sib = NULL;
+		while ((sib = of_get_next_child(np, sib)) && sib != child)
 			if (sib->name && strcmp(sib->name, child->name) == 0)
 				break;
 		if (sib == child && strncmp(p, child->name, l) != 0) {
 			al = proc_symlink(child->name, de, ent->name);
-			if (al == 0)
+			if (al == 0) {
+				of_node_put(sib);
 				break;
+			}
+			set_node_name_link(child, al);
 			*lastp = al;
 			lastp = &al->next;
 		}
-
+		of_node_put(sib);
 		/*
 		 * Add another directory with the @address part as its name.
 		 */
 		al = proc_symlink(at, de, ent->name);
 		if (al == 0)
 			break;
+		set_node_addr_link(child, al);
 		*lastp = al;
 		lastp = &al->next;
 	}
+	of_node_put(child);
 	*lastp = 0;
 	de->subdir = list;
 }
@@ -133,10 +155,11 @@ void proc_device_tree_init(void)
 	proc_device_tree = proc_mkdir("device-tree", 0);
 	if (proc_device_tree == 0)
 		return;
-	root = find_path_device("/");
+	root = of_find_node_by_path("/");
 	if (root == 0) {
 		printk(KERN_ERR "/proc/device-tree: can't find root\n");
 		return;
 	}
-	add_node(root, proc_device_tree);
+	proc_device_tree_add_node(root, proc_device_tree);
+	of_node_put(root);
 }
diff -purN linux-2.5/include/asm-ppc/unistd.h linuxppc64-2.5/include/asm-ppc/unistd.h
--- linux-2.5/include/asm-ppc/unistd.h	2003-08-23 02:15:18.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc/unistd.h	2003-11-19 22:02:56.000000000 +0000
@@ -259,8 +259,9 @@
 #define __NR_statfs64		252
 #define __NR_fstatfs64		253
 #define __NR_fadvise64_64	254
+#define __NR_rtas		255
 
-#define __NR_syscalls		255
+#define __NR_syscalls		256
 
 #define __NR(n)	#n
 
@@ -279,6 +280,7 @@
 		register unsigned long __sc_5  __asm__ ("r5");		\
 		register unsigned long __sc_6  __asm__ ("r6");		\
 		register unsigned long __sc_7  __asm__ ("r7");		\
+		register unsigned long __sc_8  __asm__ ("r8");		\
 									\
 		__sc_loadargs_##nr(name, args);				\
 		__asm__ __volatile__					\
@@ -287,10 +289,10 @@
 			: "=&r" (__sc_0),				\
 			  "=&r" (__sc_3),  "=&r" (__sc_4),		\
 			  "=&r" (__sc_5),  "=&r" (__sc_6),		\
-			  "=&r" (__sc_7)				\
+			  "=&r" (__sc_7),  "=&r" (__sc_8)		\
 			: __sc_asm_input_##nr				\
 			: "cr0", "ctr", "memory",			\
-			  "r8", "r9", "r10","r11", "r12");		\
+			  "r9", "r10","r11", "r12");			\
 		__sc_ret = __sc_3;					\
 		__sc_err = __sc_0;					\
 	}								\
@@ -318,6 +320,9 @@
 #define __sc_loadargs_5(name, arg1, arg2, arg3, arg4, arg5)		\
 	__sc_loadargs_4(name, arg1, arg2, arg3, arg4);			\
 	__sc_7 = (unsigned long) (arg5)
+#define __sc_loadargs_6(name, arg1, arg2, arg3, arg4, arg5, arg6)	\
+	__sc_loadargs_5(name, arg1, arg2, arg3, arg4, arg5);		\
+	__sc_8 = (unsigned long) (arg6)
 
 #define __sc_asm_input_0 "0" (__sc_0)
 #define __sc_asm_input_1 __sc_asm_input_0, "1" (__sc_3)
@@ -325,6 +330,7 @@
 #define __sc_asm_input_3 __sc_asm_input_2, "3" (__sc_5)
 #define __sc_asm_input_4 __sc_asm_input_3, "4" (__sc_6)
 #define __sc_asm_input_5 __sc_asm_input_4, "5" (__sc_7)
+#define __sc_asm_input_6 __sc_asm_input_5, "6" (__sc_8)
 
 #define _syscall0(type,name)						\
 type name(void)								\
@@ -362,6 +368,12 @@ type name(type1 arg1, type2 arg2, type3 
 	__syscall_nr(5, type, name, arg1, arg2, arg3, arg4, arg5);	\
 }
 
+#define _syscall6(type,name,type1,arg1,type2,arg2,type3,arg3,type4,arg4,type5,arg5,type6,arg6) \
+type name(type1 arg1, type2 arg2, type3 arg3, type4 arg4, type5 arg5, type6 arg6) \
+{									\
+	__syscall_nr(6, type, name, arg1, arg2, arg3, arg4, arg5, arg6); \
+}
+
 #ifdef __KERNEL__
 
 #define __NR__exit __NR_exit
diff -purN linux-2.5/include/asm-ppc64/cputable.h linuxppc64-2.5/include/asm-ppc64/cputable.h
--- linux-2.5/include/asm-ppc64/cputable.h	2003-10-01 22:32:13.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/cputable.h	2003-10-28 18:47:22.000000000 +0000
@@ -74,10 +74,20 @@ extern struct cpu_spec		*cur_cpu_spec;
 #define FW_FEATURE_COPY		(1UL<<4)	
 #define FW_FEATURE_ASR		(1UL<<5)	
 #define FW_FEATURE_DEBUG	(1UL<<6)	
-#define FW_FEATURE_PERF		(1UL<<7)	
-#define FW_FEATURE_DUMP		(1UL<<8)	
-#define FW_FEATURE_INTERRUPT	(1UL<<9)	
-#define FW_FEATURE_MIGRATE	(1UL<<10)	
+#define FW_FEATURE_TERM		(1UL<<7)	
+#define FW_FEATURE_PERF		(1UL<<8)	
+#define FW_FEATURE_DUMP		(1UL<<9)	
+#define FW_FEATURE_INTERRUPT	(1UL<<10)	
+#define FW_FEATURE_MIGRATE	(1UL<<11)	
+#define FW_FEATURE_PERFMON	(1UL<<12)	
+#define FW_FEATURE_CRQ   	(1UL<<13)	
+#define FW_FEATURE_VIO   	(1UL<<14)	
+#define FW_FEATURE_RDMA   	(1UL<<15)	
+#define FW_FEATURE_LLAN   	(1UL<<16)	
+#define FW_FEATURE_BULK   	(1UL<<17)	
+#define FW_FEATURE_XDABR   	(1UL<<18)	
+#define FW_FEATURE_MULTITCE   	(1UL<<19)	
+#define FW_FEATURE_SPLPAR   	(1UL<<20)	
 
 typedef struct {
     unsigned long val;
@@ -144,11 +154,24 @@ extern firmware_feature_t firmware_featu
 	.llong 99b;	 		        \
 	.previous
 
-#define END_FTR_SECTION_IFSET(msk)	END_FTR_SECTION((msk), (msk))
-#define END_FTR_SECTION_IFCLR(msk)	END_FTR_SECTION((msk), 0)
+#else
+
+#define BEGIN_FTR_SECTION		"98:\n"
+#define END_FTR_SECTION(msk, val)		\
+"99:\n"						\
+"	.section __ftr_fixup,\"a\";\n"		\
+"	.align 3;\n"				\
+"	.llong "#msk";\n"			\
+"	.llong "#val";\n"			\
+"	.llong 98b;\n"			        \
+"	.llong 99b;\n"	 		        \
+"	.previous\n"
 
 #endif /* __ASSEMBLY__ */
 
+#define END_FTR_SECTION_IFSET(msk)	END_FTR_SECTION((msk), (msk))
+#define END_FTR_SECTION_IFCLR(msk)	END_FTR_SECTION((msk), 0)
+
 #endif /* __ASM_PPC_CPUTABLE_H */
 #endif /* __KERNEL__ */
 
diff -purN linux-2.5/include/asm-ppc64/elf.h linuxppc64-2.5/include/asm-ppc64/elf.h
--- linux-2.5/include/asm-ppc64/elf.h	2003-09-30 21:11:46.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/elf.h	2003-12-17 04:27:52.000000000 +0000
@@ -1,6 +1,10 @@
 #ifndef __PPC64_ELF_H
 #define __PPC64_ELF_H
 
+#include <asm/types.h>
+#include <asm/ptrace.h>
+#include <asm/cputable.h>
+
 /* PowerPC relocations defined by the ABIs */
 #define R_PPC_NONE		0
 #define R_PPC_ADDR32		1	/* 32bit absolute address */
@@ -39,8 +43,39 @@
 #define R_PPC_SECTOFF_LO	34
 #define R_PPC_SECTOFF_HI	35
 #define R_PPC_SECTOFF_HA	36
+
+/* PowerPC relocations defined for the TLS access ABI.  */
+#define R_PPC_TLS		67 /* none	(sym+add)@tls */
+#define R_PPC_DTPMOD32		68 /* word32	(sym+add)@dtpmod */
+#define R_PPC_TPREL16		69 /* half16*	(sym+add)@tprel */
+#define R_PPC_TPREL16_LO	70 /* half16	(sym+add)@tprel@l */
+#define R_PPC_TPREL16_HI	71 /* half16	(sym+add)@tprel@h */
+#define R_PPC_TPREL16_HA	72 /* half16	(sym+add)@tprel@ha */
+#define R_PPC_TPREL32		73 /* word32	(sym+add)@tprel */
+#define R_PPC_DTPREL16		74 /* half16*	(sym+add)@dtprel */
+#define R_PPC_DTPREL16_LO	75 /* half16	(sym+add)@dtprel@l */
+#define R_PPC_DTPREL16_HI	76 /* half16	(sym+add)@dtprel@h */
+#define R_PPC_DTPREL16_HA	77 /* half16	(sym+add)@dtprel@ha */
+#define R_PPC_DTPREL32		78 /* word32	(sym+add)@dtprel */
+#define R_PPC_GOT_TLSGD16	79 /* half16*	(sym+add)@got@tlsgd */
+#define R_PPC_GOT_TLSGD16_LO	80 /* half16	(sym+add)@got@tlsgd@l */
+#define R_PPC_GOT_TLSGD16_HI	81 /* half16	(sym+add)@got@tlsgd@h */
+#define R_PPC_GOT_TLSGD16_HA	82 /* half16	(sym+add)@got@tlsgd@ha */
+#define R_PPC_GOT_TLSLD16	83 /* half16*	(sym+add)@got@tlsld */
+#define R_PPC_GOT_TLSLD16_LO	84 /* half16	(sym+add)@got@tlsld@l */
+#define R_PPC_GOT_TLSLD16_HI	85 /* half16	(sym+add)@got@tlsld@h */
+#define R_PPC_GOT_TLSLD16_HA	86 /* half16	(sym+add)@got@tlsld@ha */
+#define R_PPC_GOT_TPREL16	87 /* half16*	(sym+add)@got@tprel */
+#define R_PPC_GOT_TPREL16_LO	88 /* half16	(sym+add)@got@tprel@l */
+#define R_PPC_GOT_TPREL16_HI	89 /* half16	(sym+add)@got@tprel@h */
+#define R_PPC_GOT_TPREL16_HA	90 /* half16	(sym+add)@got@tprel@ha */
+#define R_PPC_GOT_DTPREL16	91 /* half16*	(sym+add)@got@dtprel */
+#define R_PPC_GOT_DTPREL16_LO	92 /* half16*	(sym+add)@got@dtprel@l */
+#define R_PPC_GOT_DTPREL16_HI	93 /* half16*	(sym+add)@got@dtprel@h */
+#define R_PPC_GOT_DTPREL16_HA	94 /* half16*	(sym+add)@got@dtprel@ha */
+
 /* Keep this the last entry.  */
-#define R_PPC_NUM		37
+#define R_PPC_NUM		95
 
 /*
  * ELF register definitions..
@@ -54,7 +89,8 @@
 
 #define ELF_NGREG	48	/* includes nip, msr, lr, etc. */
 #define ELF_NFPREG	33	/* includes fpscr */
-#define ELF_NVRREG	33	/* includes vscr */
+#define ELF_NVRREG32	33	/* includes vscr & vrsave stuffed together */
+#define ELF_NVRREG	34	/* includes vscr & vrsave in split vectors */
 
 typedef unsigned long elf_greg_t64;
 typedef elf_greg_t64 elf_gregset_t64[ELF_NGREG];
@@ -82,6 +118,27 @@ typedef elf_greg_t32 elf_gregset_t32[ELF
 typedef double elf_fpreg_t;
 typedef elf_fpreg_t elf_fpregset_t[ELF_NFPREG];
 
+/* Altivec registers */
+/*
+ * The entries with indexes 0-31 contain the corresponding vector registers. 
+ * The entry with index 32 contains the vscr as the last word (offset 12) 
+ * within the quadword.  This allows the vscr to be stored as either a 
+ * quadword (since it must be copied via a vector register to/from storage) 
+ * or as a word.  The entry with index 33 contains the vrsave as the first 
+ * word (offset 0) within the quadword.
+ *
+ * This definition of the VMX state is compatible with the current PPC32 
+ * ptrace interface.  This allows signal handling and ptrace to use the same 
+ * structures.  This also simplifies the implementation of a bi-arch 
+ * (combined (32- and 64-bit) gdb.
+ *
+ * Note that it's _not_ compatible with 32 bits ucontext which stuffs the
+ * vrsave along with vscr and so only uses 33 vectors for the register set
+ */
+typedef __vector128 elf_vrreg_t;
+typedef elf_vrreg_t elf_vrregset_t[ELF_NVRREG];
+typedef elf_vrreg_t elf_vrregset_t32[ELF_NVRREG32];
+
 /*
  * This is used to ensure we don't load something for the wrong architecture.
  */
@@ -128,13 +185,15 @@ static inline int dump_task_regs(struct 
 extern int dump_task_fpu(struct task_struct *, elf_fpregset_t *); 
 #define ELF_CORE_COPY_FPREGS(tsk, elf_fpregs) dump_task_fpu(tsk, elf_fpregs)
 
+/* XXX Should we define the XFPREGS using altivec ??? */
+
 #endif
 
 /* This yields a mask that user programs can use to figure out what
    instruction set this cpu supports.  This could be done in userspace,
    but it's not easy, and we've already done it here.  */
 
-#define ELF_HWCAP	(0)
+#define ELF_HWCAP	(cur_cpu_spec->cpu_user_features)
 
 /* This yields a string that ld.so will use to load implementation
    specific libraries for optimization.  This is more specific in
@@ -272,7 +331,50 @@ do {									\
 #define R_PPC64_TOC16_LO_DS    64 /* half16ds  #lo(S + A - .TOC.) >> 2.  */
 #define R_PPC64_PLTGOT16_DS    65 /* half16ds* (M + A) >> 2.  */
 #define R_PPC64_PLTGOT16_LO_DS 66 /* half16ds  #lo(M + A) >> 2.  */
+
+/* PowerPC64 relocations defined for the TLS access ABI.  */
+#define R_PPC64_TLS		67 /* none	(sym+add)@tls */
+#define R_PPC64_DTPMOD64	68 /* doubleword64 (sym+add)@dtpmod */
+#define R_PPC64_TPREL16		69 /* half16*	(sym+add)@tprel */
+#define R_PPC64_TPREL16_LO	70 /* half16	(sym+add)@tprel@l */
+#define R_PPC64_TPREL16_HI	71 /* half16	(sym+add)@tprel@h */
+#define R_PPC64_TPREL16_HA	72 /* half16	(sym+add)@tprel@ha */
+#define R_PPC64_TPREL64		73 /* doubleword64 (sym+add)@tprel */
+#define R_PPC64_DTPREL16	74 /* half16*	(sym+add)@dtprel */
+#define R_PPC64_DTPREL16_LO	75 /* half16	(sym+add)@dtprel@l */
+#define R_PPC64_DTPREL16_HI	76 /* half16	(sym+add)@dtprel@h */
+#define R_PPC64_DTPREL16_HA	77 /* half16	(sym+add)@dtprel@ha */
+#define R_PPC64_DTPREL64	78 /* doubleword64 (sym+add)@dtprel */
+#define R_PPC64_GOT_TLSGD16	79 /* half16*	(sym+add)@got@tlsgd */
+#define R_PPC64_GOT_TLSGD16_LO	80 /* half16	(sym+add)@got@tlsgd@l */
+#define R_PPC64_GOT_TLSGD16_HI	81 /* half16	(sym+add)@got@tlsgd@h */
+#define R_PPC64_GOT_TLSGD16_HA	82 /* half16	(sym+add)@got@tlsgd@ha */
+#define R_PPC64_GOT_TLSLD16	83 /* half16*	(sym+add)@got@tlsld */
+#define R_PPC64_GOT_TLSLD16_LO	84 /* half16	(sym+add)@got@tlsld@l */
+#define R_PPC64_GOT_TLSLD16_HI	85 /* half16	(sym+add)@got@tlsld@h */
+#define R_PPC64_GOT_TLSLD16_HA	86 /* half16	(sym+add)@got@tlsld@ha */
+#define R_PPC64_GOT_TPREL16_DS	87 /* half16ds*	(sym+add)@got@tprel */
+#define R_PPC64_GOT_TPREL16_LO_DS 88 /* half16ds (sym+add)@got@tprel@l */
+#define R_PPC64_GOT_TPREL16_HI	89 /* half16	(sym+add)@got@tprel@h */
+#define R_PPC64_GOT_TPREL16_HA	90 /* half16	(sym+add)@got@tprel@ha */
+#define R_PPC64_GOT_DTPREL16_DS	91 /* half16ds*	(sym+add)@got@dtprel */
+#define R_PPC64_GOT_DTPREL16_LO_DS 92 /* half16ds (sym+add)@got@dtprel@l */
+#define R_PPC64_GOT_DTPREL16_HI	93 /* half16	(sym+add)@got@dtprel@h */
+#define R_PPC64_GOT_DTPREL16_HA	94 /* half16	(sym+add)@got@dtprel@ha */
+#define R_PPC64_TPREL16_DS	95 /* half16ds*	(sym+add)@tprel */
+#define R_PPC64_TPREL16_LO_DS	96 /* half16ds	(sym+add)@tprel@l */
+#define R_PPC64_TPREL16_HIGHER	97 /* half16	(sym+add)@tprel@higher */
+#define R_PPC64_TPREL16_HIGHERA	98 /* half16	(sym+add)@tprel@highera */
+#define R_PPC64_TPREL16_HIGHEST	99 /* half16	(sym+add)@tprel@highest */
+#define R_PPC64_TPREL16_HIGHESTA 100 /* half16	(sym+add)@tprel@highesta */
+#define R_PPC64_DTPREL16_DS	101 /* half16ds* (sym+add)@dtprel */
+#define R_PPC64_DTPREL16_LO_DS	102 /* half16ds	(sym+add)@dtprel@l */
+#define R_PPC64_DTPREL16_HIGHER	103 /* half16	(sym+add)@dtprel@higher */
+#define R_PPC64_DTPREL16_HIGHERA 104 /* half16	(sym+add)@dtprel@highera */
+#define R_PPC64_DTPREL16_HIGHEST 105 /* half16	(sym+add)@dtprel@highest */
+#define R_PPC64_DTPREL16_HIGHESTA 106 /* half16	(sym+add)@dtprel@highesta */
+
 /* Keep this the last entry.  */
-#define R_PPC64_NUM		67
+#define R_PPC64_NUM		107
 
 #endif /* __PPC64_ELF_H */
diff -purN linux-2.5/include/asm-ppc64/hvcall.h linuxppc64-2.5/include/asm-ppc64/hvcall.h
--- linux-2.5/include/asm-ppc64/hvcall.h	2003-09-07 01:40:37.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/hvcall.h	2003-12-10 21:43:51.000000000 +0000
@@ -9,6 +9,14 @@
 #define H_PTEG_Full	-6	/* PTEG is full */
 #define H_Not_Found	-7	/* PTE was not found" */
 #define H_Reserved_DABR	-8	/* DABR address is reserved by the hypervisor on this processor" */
+#define H_NoMem                 -9
+#define H_Authority            -10
+#define H_Permission           -11
+#define H_Dropped              -12
+#define H_SourceParm           -13
+#define H_DestParm             -14
+#define H_RemoteParm           -15
+#define H_Resource             -16
 
 /* Flags */
 #define H_LARGE_PAGE		(1UL<<(63-16))
@@ -58,6 +66,21 @@
 #define H_IPOLL			0x70
 #define H_XIRR			0x74
 #define H_PERFMON		0x7c
+#define H_MIGRATE_DMA		0x78
+#define H_REGISTER_VPA		0xDC
+#define H_CEDE		        0xE0
+#define H_CONFER		0xE4
+#define H_PROD		        0xE8
+#define H_GET_PPP		0xEC
+#define H_SET_PPP		0xF0
+#define H_SET_PURR		0xF4
+#define H_PIC		        0xF8
+#define H_REG_CRQ		0xFC
+#define H_FREE_CRQ		0x100
+#define H_VIO_SIGNAL		0x104
+#define H_SEND_CRQ		0x108
+#define H_COPY_RDMA             0x110
+#define H_POLL_PENDING	        0x1D8
 
 /* plpar_hcall() -- Generic call interface using above opcodes
  *
@@ -76,7 +99,44 @@ long plpar_hcall(unsigned long opcode,
 		 unsigned long *out2,
 		 unsigned long *out3);
 
+#define HVSC			".long 0x44000022\n"
+
 /* Same as plpar_hcall but for those opcodes that return no values
  * other than status.  Slightly more efficient.
  */
 long plpar_hcall_norets(unsigned long opcode, ...);
+
+/* 
+ * Special hcall interface for ibmveth support.
+ * Takes 8 input parms. Returns a rc and stores the
+ * R4 return value in *out1.
+ */
+long plpar_hcall_8arg_2ret(unsigned long opcode,
+			   unsigned long arg1,
+		  	   unsigned long arg2,
+			   unsigned long arg3,
+			   unsigned long arg4,
+			   unsigned long arg5,
+			   unsigned long arg6,
+			   unsigned long arg7,
+			   unsigned long arg8,
+			   unsigned long *out1);
+
+
+ 
+ 
+/* plpar_hcall_4out()
+ *
+ * same as plpar_hcall except with 4 output arguments.  
+ * 
+ */
+long plpar_hcall_4out(unsigned long opcode,
+		      unsigned long arg1,
+		      unsigned long arg2,
+		      unsigned long arg3,
+		      unsigned long arg4,
+		      unsigned long *out1,
+		      unsigned long *out2,
+		      unsigned long *out3,
+		      unsigned long *out4);
+
diff -purN linux-2.5/include/asm-ppc64/hw_irq.h linuxppc64-2.5/include/asm-ppc64/hw_irq.h
--- linux-2.5/include/asm-ppc64/hw_irq.h	2003-02-19 02:59:01.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/hw_irq.h	2003-10-22 13:10:35.000000000 +0000
@@ -21,18 +21,15 @@ extern void ppc_irq_dispatch_handler(str
 
 #ifdef CONFIG_PPC_ISERIES
 
-extern void __no_use_sti(void);
-extern void __no_use_cli(void);
-extern void __no_use_restore_flags(unsigned long);
-extern unsigned long __no_use_save_flags(void);
-extern void __no_use_set_lost(unsigned long);
-extern void __no_lpq_restore_flags(unsigned long);
-
-#define local_irq_disable()			__no_use_cli()
-#define local_irq_enable()			__no_use_sti()
-#define local_save_flags(flags)	((flags) = __no_use_save_flags())
-#define local_irq_restore(flags)	__no_use_restore_flags((unsigned long)flags)
-#define local_irq_save(flags)	({local_save_flags(flags);local_irq_disable();})
+extern unsigned long local_get_flags(void);
+extern unsigned long local_irq_disable(void);
+extern void local_irq_restore(unsigned long);
+
+#define local_irq_enable()	local_irq_restore(1)
+#define local_save_flags(flags)	((flags) = local_get_flags())
+#define local_irq_save(flags)	((flags) = local_irq_disable())
+
+#define irqs_disabled()		(local_get_flags() == 0)
 
 #else
 
@@ -69,11 +66,33 @@ static inline void __do_save_and_cli(uns
 
 #define local_irq_save(flags)          __do_save_and_cli(&flags)
 
+#define irqs_disabled()				\
+({						\
+	unsigned long flags;			\
+	local_save_flags(flags);		\
+	!(flags & MSR_EE);			\
+})
+
 #endif /* CONFIG_PPC_ISERIES */
 
-#define mask_irq(irq) ({if (irq_desc[irq].handler && irq_desc[irq].handler->disable) irq_desc[irq].handler->disable(irq);})
-#define unmask_irq(irq) ({if (irq_desc[irq].handler && irq_desc[irq].handler->enable) irq_desc[irq].handler->enable(irq);})
-#define ack_irq(irq) ({if (irq_desc[irq].handler && irq_desc[irq].handler->ack) irq_desc[irq].handler->ack(irq);})
+#define mask_irq(irq)						\
+	({							\
+	 	irq_desc_t *desc = get_irq_desc(irq);		\
+		if (desc->handler && desc->handler->disable)	\
+			desc->handler->disable(irq);		\
+	})
+#define unmask_irq(irq)						\
+	({							\
+	 	irq_desc_t *desc = get_irq_desc(irq);		\
+		if (desc->handler && desc->handler->enable)	\
+			desc->handler->enable(irq);		\
+	})
+#define ack_irq(irq)						\
+	({							\
+	 	irq_desc_t *desc = get_irq_desc(irq);		\
+		if (desc->handler && desc->handler->ack)	\
+			desc->handler->ack(irq);		\
+	})
 
 /* Should we handle this via lost interrupts and IPIs or should we don't care like
  * we do now ? --BenH.
diff -purN linux-2.5/include/asm-ppc64/iSeries/HvCall.h linuxppc64-2.5/include/asm-ppc64/iSeries/HvCall.h
--- linux-2.5/include/asm-ppc64/iSeries/HvCall.h	2002-12-08 05:08:27.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/HvCall.h	2003-11-21 06:45:03.000000000 +0000
@@ -23,25 +23,16 @@
 //	drive the hypervisor from the OS.
 //
 //===========================================================================
+#ifndef _HVCALL_H
+#define _HVCALL_H
 
 //-------------------------------------------------------------------
 // Standard Includes
 //-------------------------------------------------------------------
-#ifndef  _HVCALLSC_H
-#include "HvCallSc.h"
-#endif
-
-#ifndef  _HVTYPES_H
+#include <asm/iSeries/HvCallSc.h>
 #include <asm/iSeries/HvTypes.h>
-#endif
-
 #include <asm/paca.h>
 
-//-------------------------------------------------------------------
-// Constants
-//-------------------------------------------------------------------
-#ifndef _HVCALL_H
-#define _HVCALL_H
 /*
 enum HvCall_ReturnCode
 {
@@ -211,5 +202,4 @@ static inline void HvCall_setDebugBus(un
 	HvCall1(HvCallBaseSetDebugBus, val);
 }
 
-#endif // _HVCALL_H
-
+#endif /* _HVCALL_H */
diff -purN linux-2.5/include/asm-ppc64/iSeries/HvCallCfg.h linuxppc64-2.5/include/asm-ppc64/iSeries/HvCallCfg.h
--- linux-2.5/include/asm-ppc64/iSeries/HvCallCfg.h	2002-02-14 12:14:36.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/HvCallCfg.h	2003-11-21 06:45:03.000000000 +0000
@@ -23,23 +23,18 @@
 //	drive the hypervisor from the OS.
 //
 //=====================================================================================
+#ifndef _HVCALLCFG_H
+#define _HVCALLCFG_H
 
 //-------------------------------------------------------------------
 // Standard Includes
 //-------------------------------------------------------------------
-#ifndef  _HVCALLSC_H
-#include "HvCallSc.h"
-#endif
-
-#ifndef  _HVTYPES_H
+#include <asm/iSeries/HvCallSc.h>
 #include <asm/iSeries/HvTypes.h>
-#endif
 
 //-------------------------------------------------------------------------------------
 // Constants
 //-------------------------------------------------------------------------------------
-#ifndef _HVCALLCFG_H
-#define _HVCALLCFG_H
 
 enum HvCallCfg_ReqQual
 {
@@ -215,5 +210,4 @@ static inline HvLpIndex	HvCallCfg_getHos
 
 }
 
-#endif // _HVCALLCFG_H
-
+#endif /* _HVCALLCFG_H */
diff -purN linux-2.5/include/asm-ppc64/iSeries/HvCallEvent.h linuxppc64-2.5/include/asm-ppc64/iSeries/HvCallEvent.h
--- linux-2.5/include/asm-ppc64/iSeries/HvCallEvent.h	2002-02-14 12:14:36.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/HvCallEvent.h	2003-11-21 06:45:03.000000000 +0000
@@ -17,44 +17,27 @@
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
 
-//==================================================================
-//
-//	This file contains the "hypervisor call" interface which is used to
-//	drive the hypervisor from the OS.
-//
-//==================================================================
-
-//-------------------------------------------------------------------
-// Standard Includes
-//-------------------------------------------------------------------
-#ifndef  _HVCALLSC_H
-#include <asm/iSeries/HvCallSc.h>
-#endif
+/*
+ *	This file contains the "hypervisor call" interface which is used to
+ *	drive the hypervisor from the OS.
+ */
+#ifndef _HVCALLEVENT_H
+#define _HVCALLEVENT_H
 
-#ifndef  _HVTYPES_H
+/*
+ * Standard Includes
+ */
+#include <asm/iSeries/HvCallSc.h>
 #include <asm/iSeries/HvTypes.h>
-#endif
-
 #include <asm/abs_addr.h>
 
-//-------------------------------------------------------------------
-// Other Includes
-//-------------------------------------------------------------------
-
-//-------------------------------------------------------------------
-// Constants
-//-------------------------------------------------------------------
-#ifndef _HVCALLEVENT_H
-#define _HVCALLEVENT_H
-
 struct HvLpEvent;
 
 typedef u8 HvLpEvent_Type;
 typedef u8 HvLpEvent_AckInd;
 typedef u8 HvLpEvent_AckType;
 
-struct	HvCallEvent_PackedParms
-{
+struct	HvCallEvent_PackedParms {
 	u8		xAckType:1;
 	u8		xAckInd:1;
 	u8		xRsvd:1;
@@ -68,8 +51,7 @@ struct	HvCallEvent_PackedParms
 typedef u8 HvLpDma_Direction;
 typedef u8 HvLpDma_AddressType;
 
-struct	HvCallEvent_PackedDmaParms
-{
+struct	HvCallEvent_PackedDmaParms {
 	u8		xDirection:1;
 	u8		xLocalAddrType:1;
 	u8		xRemoteAddrType:1;
@@ -101,69 +83,63 @@ typedef u64 HvLpDma_Rc;
 #define HvCallEventSetLpEventQueueInterruptProc		HvCallEvent + 14
 #define HvCallEventRouter15				HvCallEvent + 15
 
-//======================================================================
-static inline void		HvCallEvent_getOverflowLpEvents(u8 queueIndex)
+static inline void HvCallEvent_getOverflowLpEvents(u8 queueIndex)
 {
 	HvCall1(HvCallEventGetOverflowLpEvents,queueIndex);
 	// getPaca()->adjustHmtForNoOfSpinLocksHeld();
 }
-//======================================================================
-static inline void		HvCallEvent_setInterLpQueueIndex(u8 queueIndex)
+
+static inline void HvCallEvent_setInterLpQueueIndex(u8 queueIndex)
 {
 	HvCall1(HvCallEventSetInterLpQueueIndex,queueIndex);
 	// getPaca()->adjustHmtForNoOfSpinLocksHeld();
 }
-//======================================================================
-static inline void		HvCallEvent_setLpEventStack(u8 queueIndex,
-					     char * eventStackAddr,
-					     u32 eventStackSize)
+
+static inline void HvCallEvent_setLpEventStack(u8 queueIndex,
+		char *eventStackAddr, u32 eventStackSize)
 {
 	u64 abs_addr;
-	abs_addr = virt_to_absolute( (unsigned long) eventStackAddr );
 
-	HvCall3(HvCallEventSetLpEventStack, queueIndex, abs_addr, eventStackSize);
+	abs_addr = virt_to_absolute((unsigned long)eventStackAddr);
+	HvCall3(HvCallEventSetLpEventStack, queueIndex, abs_addr,
+			eventStackSize);
 	// getPaca()->adjustHmtForNoOfSpinLocksHeld();
 }
-//======================================================================
-static inline void		HvCallEvent_setLpEventQueueInterruptProc(u8 queueIndex,
-							  u16 lpLogicalProcIndex)
+
+static inline void HvCallEvent_setLpEventQueueInterruptProc(u8 queueIndex,
+		u16 lpLogicalProcIndex)
 {
-	HvCall2(HvCallEventSetLpEventQueueInterruptProc,queueIndex,lpLogicalProcIndex);
+	HvCall2(HvCallEventSetLpEventQueueInterruptProc, queueIndex,
+			lpLogicalProcIndex);
 	// getPaca()->adjustHmtForNoOfSpinLocksHeld();
 }
-//=====================================================================
-static inline HvLpEvent_Rc HvCallEvent_signalLpEvent(struct HvLpEvent* event)
+
+static inline HvLpEvent_Rc HvCallEvent_signalLpEvent(struct HvLpEvent *event)
 {
 	u64 abs_addr;
 	HvLpEvent_Rc retVal;
+
 #ifdef DEBUG_SENDEVENT
-	printk("HvCallEvent_signalLpEvent: *event = %016lx\n ", (unsigned long)event);
+	printk("HvCallEvent_signalLpEvent: *event = %016lx\n ",
+			(unsigned long)event);
 #endif
-	abs_addr = virt_to_absolute( (unsigned long) event );
+	abs_addr = virt_to_absolute((unsigned long)event);
 	retVal = (HvLpEvent_Rc)HvCall1(HvCallEventSignalLpEvent, abs_addr);
 	// getPaca()->adjustHmtForNoOfSpinLocksHeld();
 	return retVal;
 }
-//=====================================================================
-static inline HvLpEvent_Rc  HvCallEvent_signalLpEventFast(HvLpIndex targetLp,
-					   HvLpEvent_Type type,
-					   u16 subtype,
-					   HvLpEvent_AckInd ackInd,
-					   HvLpEvent_AckType ackType,
-					   HvLpInstanceId sourceInstanceId,
-					   HvLpInstanceId targetInstanceId,
-					   u64 correlationToken,
-					   u64 eventData1,
-					   u64 eventData2,
-					   u64 eventData3,
-					   u64 eventData4,
-					   u64 eventData5)
+
+static inline HvLpEvent_Rc HvCallEvent_signalLpEventFast(HvLpIndex targetLp,
+		HvLpEvent_Type type, u16 subtype, HvLpEvent_AckInd ackInd,
+		HvLpEvent_AckType ackType, HvLpInstanceId sourceInstanceId,
+		HvLpInstanceId targetInstanceId, u64 correlationToken,
+		u64 eventData1, u64 eventData2, u64 eventData3,
+		u64 eventData4, u64 eventData5)
 {
 	HvLpEvent_Rc retVal;
 
 	// Pack the misc bits into a single Dword to pass to PLIC
-	union
-	{
+	union {
 		struct HvCallEvent_PackedParms	parms;
 		u64		dword;
 	} packed;
@@ -177,88 +153,84 @@ static inline HvLpEvent_Rc  HvCallEvent_
 	packed.parms.xTargetInstId	= targetInstanceId;
 
 	retVal = (HvLpEvent_Rc)HvCall7(HvCallEventSignalLpEventParms,
-				       packed.dword,
-				       correlationToken,
-				       eventData1,eventData2,
-				       eventData3,eventData4,
-				       eventData5);
+			packed.dword, correlationToken, eventData1,eventData2,
+			eventData3,eventData4, eventData5);
 	// getPaca()->adjustHmtForNoOfSpinLocksHeld();
 	return retVal;
 }
-//====================================================================
-static inline HvLpEvent_Rc	HvCallEvent_ackLpEvent(struct HvLpEvent* event)
+
+static inline HvLpEvent_Rc HvCallEvent_ackLpEvent(struct HvLpEvent *event)
 {
 	u64 abs_addr;
 	HvLpEvent_Rc retVal;
-	abs_addr = virt_to_absolute( (unsigned long) event );
 
+	abs_addr = virt_to_absolute((unsigned long)event);
 	retVal = (HvLpEvent_Rc)HvCall1(HvCallEventAckLpEvent, abs_addr);
 	// getPaca()->adjustHmtForNoOfSpinLocksHeld();
 	return retVal;
 }
-//====================================================================
-static inline HvLpEvent_Rc   HvCallEvent_cancelLpEvent(struct HvLpEvent* event)
+
+static inline HvLpEvent_Rc HvCallEvent_cancelLpEvent(struct HvLpEvent *event)
 {
 	u64 abs_addr;
 	HvLpEvent_Rc retVal;
-	abs_addr = virt_to_absolute( (unsigned long) event );
 
+	abs_addr = virt_to_absolute((unsigned long)event);
 	retVal = (HvLpEvent_Rc)HvCall1(HvCallEventCancelLpEvent, abs_addr);
 	// getPaca()->adjustHmtForNoOfSpinLocksHeld();
 	return retVal;
 }
-//===================================================================
-static inline HvLpInstanceId	HvCallEvent_getSourceLpInstanceId(HvLpIndex targetLp, HvLpEvent_Type type)
+
+static inline HvLpInstanceId HvCallEvent_getSourceLpInstanceId(
+		HvLpIndex targetLp, HvLpEvent_Type type)
 {
 	HvLpInstanceId retVal;	
-	retVal = HvCall2(HvCallEventGetSourceLpInstanceId,targetLp,type);
+
+	retVal = HvCall2(HvCallEventGetSourceLpInstanceId, targetLp, type);
 	// getPaca()->adjustHmtForNoOfSpinLocksHeld();
 	return retVal;
 }
-//===================================================================
-static inline HvLpInstanceId	HvCallEvent_getTargetLpInstanceId(HvLpIndex targetLp, HvLpEvent_Type type)
+
+static inline HvLpInstanceId HvCallEvent_getTargetLpInstanceId(
+		HvLpIndex targetLp, HvLpEvent_Type type)
 {
 	HvLpInstanceId retVal;	
-	retVal = HvCall2(HvCallEventGetTargetLpInstanceId,targetLp,type);
+
+	retVal = HvCall2(HvCallEventGetTargetLpInstanceId, targetLp, type);
 	// getPaca()->adjustHmtForNoOfSpinLocksHeld();
 	return retVal;
 }
-//===================================================================
-static inline void		HvCallEvent_openLpEventPath(HvLpIndex targetLp,
-					     HvLpEvent_Type type)
+
+static inline void HvCallEvent_openLpEventPath(HvLpIndex targetLp,
+		HvLpEvent_Type type)
 {
-	HvCall2(HvCallEventOpenLpEventPath,targetLp,type);
+	HvCall2(HvCallEventOpenLpEventPath, targetLp, type);
 	// getPaca()->adjustHmtForNoOfSpinLocksHeld();
 }
-//===================================================================
-static inline void		HvCallEvent_closeLpEventPath(HvLpIndex targetLp,
-					      HvLpEvent_Type type)
+
+static inline void HvCallEvent_closeLpEventPath(HvLpIndex targetLp,
+		HvLpEvent_Type type)
 {
-	HvCall2(HvCallEventCloseLpEventPath,targetLp,type);
+	HvCall2(HvCallEventCloseLpEventPath, targetLp, type);
 	// getPaca()->adjustHmtForNoOfSpinLocksHeld();
 }
-//===================================================================
-static inline HvLpDma_Rc	HvCallEvent_dmaBufList(HvLpEvent_Type type,
-					HvLpIndex remoteLp,
-					HvLpDma_Direction direction,
-					HvLpInstanceId localInstanceId,
-					HvLpInstanceId remoteInstanceId,
-					HvLpDma_AddressType localAddressType,
-					HvLpDma_AddressType remoteAddressType,
-					// Do these need to be converted to
-					// absolute addresses?
-					u64 localBufList,
-					u64 remoteBufList,
 
-					u32 transferLength)
+static inline HvLpDma_Rc HvCallEvent_dmaBufList(HvLpEvent_Type type,
+		HvLpIndex remoteLp, HvLpDma_Direction direction,
+		HvLpInstanceId localInstanceId,
+		HvLpInstanceId remoteInstanceId,
+		HvLpDma_AddressType localAddressType,
+		HvLpDma_AddressType remoteAddressType,
+		/* Do these need to be converted to absolute addresses? */
+		u64 localBufList, u64 remoteBufList, u32 transferLength)
 {
-	HvLpDma_Rc retVal;	
+	HvLpDma_Rc retVal;
 	// Pack the misc bits into a single Dword to pass to PLIC
-	union
-	{
+	union {
 		struct HvCallEvent_PackedDmaParms	parms;
 		u64		dword;
 	} packed;
+
 	packed.parms.xDirection		= direction;
 	packed.parms.xLocalAddrType	= localAddressType;
 	packed.parms.xRemoteAddrType	= remoteAddressType;
@@ -270,32 +242,27 @@ static inline HvLpDma_Rc	HvCallEvent_dma
 	packed.parms.xRemoteInstId	= remoteInstanceId;
 
 	retVal = (HvLpDma_Rc)HvCall4(HvCallEventDmaBufList,
-				     packed.dword,
-				     localBufList,
-				     remoteBufList,
-				     transferLength);
+			packed.dword, localBufList, remoteBufList,
+			transferLength);
 	// getPaca()->adjustHmtForNoOfSpinLocksHeld();
 	return retVal;
 }
-//=================================================================
-static inline HvLpDma_Rc	HvCallEvent_dmaSingle(HvLpEvent_Type type,
-				       HvLpIndex remoteLp,
-				       HvLpDma_Direction direction,
-				       HvLpInstanceId localInstanceId,
-				       HvLpInstanceId remoteInstanceId,
-				       HvLpDma_AddressType localAddressType,
-				       HvLpDma_AddressType remoteAddressType,
-				       u64 localAddrOrTce,
-				       u64 remoteAddrOrTce,
-				       u32 transferLength)
+
+static inline HvLpDma_Rc HvCallEvent_dmaSingle(HvLpEvent_Type type,
+		HvLpIndex remoteLp, HvLpDma_Direction direction,
+		HvLpInstanceId localInstanceId,
+		HvLpInstanceId remoteInstanceId,
+		HvLpDma_AddressType localAddressType,
+		HvLpDma_AddressType remoteAddressType,
+		u64 localAddrOrTce, u64 remoteAddrOrTce, u32 transferLength)
 {
-	HvLpDma_Rc retVal;	
+	HvLpDma_Rc retVal;
 	// Pack the misc bits into a single Dword to pass to PLIC
-	union
-	{
+	union {
 		struct HvCallEvent_PackedDmaParms	parms;
 		u64		dword;
 	} packed;
+
 	packed.parms.xDirection		= direction;
 	packed.parms.xLocalAddrType	= localAddressType;
 	packed.parms.xRemoteAddrType	= remoteAddressType;
@@ -307,29 +274,24 @@ static inline HvLpDma_Rc	HvCallEvent_dma
 	packed.parms.xRemoteInstId	= remoteInstanceId;
 
 	retVal = (HvLpDma_Rc)HvCall4(HvCallEventDmaSingle,
-				     packed.dword,
-				     localAddrOrTce,
-				     remoteAddrOrTce,
-				     transferLength);
+			packed.dword, localAddrOrTce, remoteAddrOrTce,
+			transferLength);
 	// getPaca()->adjustHmtForNoOfSpinLocksHeld();
 	return retVal;
 }
-//=================================================================
-static inline HvLpDma_Rc	HvCallEvent_dmaToSp(void* local, u32 remote, u32 length, HvLpDma_Direction dir)
+
+static inline HvLpDma_Rc HvCallEvent_dmaToSp(void* local, u32 remote,
+		u32 length, HvLpDma_Direction dir)
 {
 	u64 abs_addr;
 	HvLpDma_Rc retVal;
-	abs_addr = virt_to_absolute( (unsigned long) local );
-    
-	retVal = (HvLpDma_Rc)HvCall4(HvCallEventDmaToSp, 
-				     abs_addr,
-				     remote,
-				     length,
-				     dir);
+
+	abs_addr = virt_to_absolute((unsigned long)local);
+	retVal = (HvLpDma_Rc)HvCall4(HvCallEventDmaToSp, abs_addr, remote,
+			length, dir);
 	// getPaca()->adjustHmtForNoOfSpinLocksHeld();
 	return retVal;
 }
-//================================================================
 
-#endif // _HVCALLEVENT_H
 
+#endif /* _HVCALLEVENT_H */
diff -purN linux-2.5/include/asm-ppc64/iSeries/HvCallHpt.h linuxppc64-2.5/include/asm-ppc64/iSeries/HvCallHpt.h
--- linux-2.5/include/asm-ppc64/iSeries/HvCallHpt.h	2002-06-09 09:59:38.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/HvCallHpt.h	2003-11-21 06:45:03.000000000 +0000
@@ -16,6 +16,8 @@
  * along with this program; if not, write to the Free Software
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
+#ifndef _HVCALLHPT_H
+#define _HVCALLHPT_H
 
 //============================================================================
 //
@@ -24,30 +26,13 @@
 //
 //============================================================================
 
-//-------------------------------------------------------------------
-// Standard Includes
-//-------------------------------------------------------------------
-#ifndef  _HVCALLSC_H
-#include "HvCallSc.h"
-#endif
-
-#ifndef  _HVTYPES_H
+#include <asm/iSeries/HvCallSc.h>
 #include <asm/iSeries/HvTypes.h>
-#endif
-
-//-------------------------------------------------------------------
-// Other Includes
-//-------------------------------------------------------------------
-
-#ifndef _PPC_MMU_H
 #include <asm/mmu.h>
-#endif
 
 //-----------------------------------------------------------------------------
 // Constants
 //-----------------------------------------------------------------------------
-#ifndef _HVCALLHPT_H
-#define _HVCALLHPT_H
 
 #define HvCallHptGetHptAddress		HvCallHpt +  0
 #define HvCallHptGetHptPages		HvCallHpt +  1
@@ -139,5 +124,4 @@ static inline void		HvCallHpt_addValidat
 
 //=============================================================================
 
-#endif // _HVCALLHPT_H
-
+#endif /* _HVCALLHPT_H */
diff -purN linux-2.5/include/asm-ppc64/iSeries/HvCallPci.h linuxppc64-2.5/include/asm-ppc64/iSeries/HvCallPci.h
--- linux-2.5/include/asm-ppc64/iSeries/HvCallPci.h	2002-03-26 07:32:21.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/HvCallPci.h	2003-12-10 04:13:07.000000000 +0000
@@ -1,6 +1,6 @@
 /************************************************************************/
 /* Provides the Hypervisor PCI calls for iSeries Linux Parition.        */
-/* Copyright (C) 20yy  <Wayne G Holm> <IBM Corporation>                 */
+/* Copyright (C) 2001  <Wayne G Holm> <IBM Corporation>                 */
 /*                                                                      */
 /* This program is free software; you can redistribute it and/or modify */
 /* it under the terms of the GNU General Public License as published by */
@@ -21,50 +21,25 @@
 /* Change Activity:                                                     */
 /*   Created, Jan 9, 2001                                               */
 /************************************************************************/
-//============================================================================
-//							 Header File Id
-// Name______________:	HvCallPci.H
-//
-// Description_______:
-//
-//	This file contains the "hypervisor call" interface which is used to
-//	drive the hypervisor from SLIC.
-//
-//============================================================================
-
-//-------------------------------------------------------------------
-// Forward declarations 
-//-------------------------------------------------------------------
-
-//-------------------------------------------------------------------
-// Standard Includes
-//-------------------------------------------------------------------
-#ifndef  _HVCALLSC_H
-#include "HvCallSc.h"
-#endif
-
-#ifndef  _HVTYPES_H
-#include <asm/iSeries/HvTypes.h>
-#endif
-
-//-------------------------------------------------------------------
-// Other Includes
-//-------------------------------------------------------------------
-
 
-//-----------------------------------------------------------------------------
-// Constants
-//-----------------------------------------------------------------------------
 #ifndef _HVCALLPCI_H
 #define _HVCALLPCI_H
 
-struct HvCallPci_DsaAddr { // make sure this struct size is 64-bits total
-	u16		busNumber;
-	u8		subBusNumber; 
-	u8		deviceId;     
+#include <asm/iSeries/HvCallSc.h>
+#include <asm/iSeries/HvTypes.h>
+
+/*
+ * DSA == Direct Select Address
+ * this struct must be 64 bits in total
+ */
+struct HvCallPci_DsaAddr {
+	u16		busNumber;		/* PHB index? */
+	u8		subBusNumber; 		/* PCI bus number? */
+	u8		deviceId;     		/* device and function? */
 	u8		barNumber;
 	u8		reserved[3];
 };
+
 union HvDsaMap {
 	u64	DsaAddr;
 	struct HvCallPci_DsaAddr Dsa;
@@ -75,12 +50,13 @@ struct HvCallPci_LoadReturn {
 	u64		value;
 };
 
-enum HvCallPci_DeviceType {HvCallPci_NodeDevice	= 1,
-                        HvCallPci_SpDevice	= 2,	
-                        HvCallPci_IopDevice     = 3,	
-                        HvCallPci_BridgeDevice	= 4,	
-                        HvCallPci_MultiFunctionDevice = 5,	
-                        HvCallPci_IoaDevice	= 6	
+enum HvCallPci_DeviceType {
+	HvCallPci_NodeDevice	= 1,
+	HvCallPci_SpDevice	= 2,	
+	HvCallPci_IopDevice     = 3,	
+	HvCallPci_BridgeDevice	= 4,	
+	HvCallPci_MultiFunctionDevice = 5,	
+	HvCallPci_IoaDevice	= 6	
 };
 
 
@@ -148,9 +124,9 @@ enum HvCallPci_VpdType {
 #define HvCallPciGetBusUnitInfo		HvCallPci + 50
 
 //============================================================================
-static inline u64	HvCallPci_configLoad8(u16 busNumber, u8 subBusNumber,
-					      u8 deviceId, u32 offset,
-					      u8 *value)
+static inline u64 HvCallPci_configLoad8(u16 busNumber, u8 subBusNumber,
+					u8 deviceId, u32 offset,
+					u8 *value)
 {
 	struct HvCallPci_DsaAddr dsa;
 	struct HvCallPci_LoadReturn retVal;
@@ -170,9 +146,9 @@ static inline u64	HvCallPci_configLoad8(
 	return retVal.rc;
 }
 //============================================================================
-static inline u64	HvCallPci_configLoad16(u16 busNumber, u8 subBusNumber,
-					      u8 deviceId, u32 offset,
-					      u16 *value)
+static inline u64 HvCallPci_configLoad16(u16 busNumber, u8 subBusNumber,
+					 u8 deviceId, u32 offset,
+					 u16 *value)
 {
 	struct HvCallPci_DsaAddr dsa;
 	struct HvCallPci_LoadReturn retVal;
@@ -694,4 +670,4 @@ static inline int HvCallPci_getBusAdapte
 	return xRetSize;
 }
 //============================================================================
-#endif // _HVCALLPCI_H
+#endif /* _HVCALLPCI_H */
diff -purN linux-2.5/include/asm-ppc64/iSeries/HvCallSc.h linuxppc64-2.5/include/asm-ppc64/iSeries/HvCallSc.h
--- linux-2.5/include/asm-ppc64/iSeries/HvCallSc.h	2002-06-09 09:59:38.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/HvCallSc.h	2003-11-21 06:45:03.000000000 +0000
@@ -16,14 +16,11 @@
  * along with this program; if not, write to the Free Software
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
-
-#ifndef  _HVTYPES_H
-#include <asm/iSeries/HvTypes.h>
-#endif
-
 #ifndef _HVCALLSC_H
 #define _HVCALLSC_H
 
+#include <asm/iSeries/HvTypes.h>
+
 #define HvCallBase		0x8000000000000000
 #define HvCallCc		0x8001000000000000
 #define HvCallCfg		0x8002000000000000
diff -purN linux-2.5/include/asm-ppc64/iSeries/HvCallSm.h linuxppc64-2.5/include/asm-ppc64/iSeries/HvCallSm.h
--- linux-2.5/include/asm-ppc64/iSeries/HvCallSm.h	2002-02-14 12:14:36.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/HvCallSm.h	2003-11-21 06:45:03.000000000 +0000
@@ -16,6 +16,8 @@
  * along with this program; if not, write to the Free Software
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
+#ifndef _HVCALLSM_H
+#define _HVCALLSM_H
 
 //============================================================================
 //
@@ -27,19 +29,12 @@
 //-------------------------------------------------------------------
 // Standard Includes
 //-------------------------------------------------------------------
-#ifndef  _HVCALLSC_H
-#include "HvCallSc.h"
-#endif
-
-#ifndef  _HVTYPES_H
+#include <asm/iSeries/HvCallSc.h>
 #include <asm/iSeries/HvTypes.h>
-#endif
 
 //-----------------------------------------------------------------------------
 // Constants
 //-----------------------------------------------------------------------------
-#ifndef _HVCALLSM_H
-#define _HVCALLSM_H
 
 #define HvCallSmGet64BitsOfAccessMap	HvCallSm  + 11
 
@@ -54,5 +49,4 @@ static inline u64		HvCallSm_get64BitsOfA
 	return retval;
 }
 //============================================================================
-#endif // _HVCALLSM_H
-
+#endif /* _HVCALLSM_H */
diff -purN linux-2.5/include/asm-ppc64/iSeries/HvCallXm.h linuxppc64-2.5/include/asm-ppc64/iSeries/HvCallXm.h
--- linux-2.5/include/asm-ppc64/iSeries/HvCallXm.h	2002-02-14 12:14:36.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/HvCallXm.h	2003-11-21 06:45:03.000000000 +0000
@@ -8,6 +8,8 @@
 //	drive the hypervisor from SLIC.
 //
 //============================================================================
+#ifndef _HVCALLXM_H
+#define _HVCALLXM_H
 
 //-------------------------------------------------------------------
 // Forward declarations 
@@ -16,24 +18,12 @@
 //-------------------------------------------------------------------
 // Standard Includes
 //-------------------------------------------------------------------
-#ifndef  _HVCALLSC_H
-#include "HvCallSc.h"
-#endif
-
-#ifndef  _HVTYPES_H
+#include <asm/iSeries/HvCallSc.h>
 #include <asm/iSeries/HvTypes.h>
-#endif
-
-//-------------------------------------------------------------------
-// Other Includes
-//-------------------------------------------------------------------
-
 
 //-----------------------------------------------------------------------------
 // Constants
 //-----------------------------------------------------------------------------
-#ifndef _HVCALLXM_H
-#define _HVCALLXM_H
 
 #define HvCallXmGetTceTableParms	HvCallXm +  0
 #define HvCallXmTestBus			HvCallXm +  1
@@ -102,5 +92,4 @@ static inline u64	HvCallXm_loadTod(void)
 }
 //=====================================================================================
 
-#endif // _HVCALLXM_H
-
+#endif /* _HVCALLXM_H */
diff -purN linux-2.5/include/asm-ppc64/iSeries/HvLpConfig.h linuxppc64-2.5/include/asm-ppc64/iSeries/HvLpConfig.h
--- linux-2.5/include/asm-ppc64/iSeries/HvLpConfig.h	2002-02-14 12:14:36.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/HvLpConfig.h	2003-11-21 06:45:03.000000000 +0000
@@ -16,6 +16,8 @@
  * along with this program; if not, write to the Free Software
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
+#ifndef _HVLPCONFIG_H
+#define _HVLPCONFIG_H
 
 //===========================================================================
 //
@@ -24,24 +26,10 @@
 //
 //===========================================================================
 
-#ifndef  _HVCALLCFG_H
-#include "HvCallCfg.h"
-#endif
-
-#ifndef  _HVTYPES_H
+#include <asm/iSeries/HvCallCfg.h>
 #include <asm/iSeries/HvTypes.h>
-#endif
-
-#ifndef  _ITLPNACA_H
 #include <asm/iSeries/ItLpNaca.h>
-#endif
-
-#ifndef  _LPARDATA_H
 #include <asm/iSeries/LparData.h>
-#endif
-
-#ifndef _HVLPCONFIG_H
-#define _HVLPCONFIG_H
 
 //-------------------------------------------------------------------
 // Constants
@@ -289,4 +277,4 @@ static inline HvLpIndex		HvLpConfig_getH
 }
 //================================================================
 
-#endif // _HVLPCONFIG_H
+#endif /* _HVLPCONFIG_H */
diff -purN linux-2.5/include/asm-ppc64/iSeries/HvLpEvent.h linuxppc64-2.5/include/asm-ppc64/iSeries/HvLpEvent.h
--- linux-2.5/include/asm-ppc64/iSeries/HvLpEvent.h	2002-02-14 12:14:36.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/HvLpEvent.h	2003-11-21 06:45:03.000000000 +0000
@@ -28,10 +28,7 @@
 #include <asm/types.h>
 #include <asm/ptrace.h>
 #include <asm/iSeries/HvTypes.h>
-#ifndef _HVCALLEVENT_H
 #include <asm/iSeries/HvCallEvent.h>
-#endif
-
 
 //=====================================================================
 //
diff -purN linux-2.5/include/asm-ppc64/iSeries/HvReleaseData.h linuxppc64-2.5/include/asm-ppc64/iSeries/HvReleaseData.h
--- linux-2.5/include/asm-ppc64/iSeries/HvReleaseData.h	2002-04-23 04:01:41.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/HvReleaseData.h	2003-11-21 06:45:03.000000000 +0000
@@ -16,6 +16,8 @@
  * along with this program; if not, write to the Free Software
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
+#ifndef _HVRELEASEDATA_H
+#define _HVRELEASEDATA_H
 
 //=============================================================================
 //
@@ -23,15 +25,7 @@
 //   release so that it can be changed in the future (ie, the virtual 
 //   address of the OS's NACA).
 //
-//-----------------------------------------------------------------------------
-// Standard Includes
-//-----------------------------------------------------------------------------
-#ifndef	_PPC64_TYPES_H
-#include	<asm/types.h>
-#endif
-
-#ifndef _HVRELEASEDATA_H
-#define _HVRELEASEDATA_H
+#include <asm/types.h>
 
 //=============================================================================
 //
@@ -67,4 +61,4 @@ struct	HvReleaseData
 	char	xRsvd3[20];		// Reserved			x2C-x3F
 };
 
-#endif // _HVRELEASEDATA_H
+#endif /* _HVRELEASEDATA_H */
diff -purN linux-2.5/include/asm-ppc64/iSeries/HvTypes.h linuxppc64-2.5/include/asm-ppc64/iSeries/HvTypes.h
--- linux-2.5/include/asm-ppc64/iSeries/HvTypes.h	2002-02-14 12:14:36.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/HvTypes.h	2003-11-21 06:45:03.000000000 +0000
@@ -16,6 +16,8 @@
  * along with this program; if not, write to the Free Software
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
+#ifndef _HVTYPES_H
+#define _HVTYPES_H
 
 //===========================================================================
 //                                                             Header File Id
@@ -29,13 +31,7 @@
 //
 //===========================================================================
 
-#ifndef _PPC_TYPES_H
-#include        <asm/types.h>
-#endif
-
-
-#ifndef _HVTYPES_H
-#define _HVTYPES_H
+#include <asm/types.h>
 
 //-------------------------------------------------------------------
 // Typedefs
@@ -124,4 +120,4 @@ struct HvLpBufferList {
 	u64 len;
 };
 
-#endif // _HVTYPES_H
+#endif /* _HVTYPES_H */
diff -purN linux-2.5/include/asm-ppc64/iSeries/IoHriProcessorVpd.h linuxppc64-2.5/include/asm-ppc64/iSeries/IoHriProcessorVpd.h
--- linux-2.5/include/asm-ppc64/iSeries/IoHriProcessorVpd.h	2002-02-14 12:14:36.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/IoHriProcessorVpd.h	2003-11-21 06:45:03.000000000 +0000
@@ -16,18 +16,15 @@
  * along with this program; if not, write to the Free Software
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
+#ifndef _IOHRIPROCESSORVPD_H
+#define _IOHRIPROCESSORVPD_H
 
 //===================================================================
 //
 //	This struct maps Processor Vpd that is DMAd to SLIC by CSP 
 //
 
-#ifndef	_TYPES_H
 #include <asm/types.h>
-#endif
-
-#ifndef _IOHRIPROCESSORVPD_H
-#define _IOHRIPROCESSORVPD_H
 
 struct IoHriProcessorVpd
 {
@@ -87,4 +84,5 @@ struct IoHriProcessorVpd
 
 	char xProcSrc[72];		// CSP format SRC		xB8-xFF
 };
-#endif // _IOHRIPROCESSORVPD_H
+
+#endif /* _IOHRIPROCESSORVPD_H */
diff -purN linux-2.5/include/asm-ppc64/iSeries/ItExtVpdPanel.h linuxppc64-2.5/include/asm-ppc64/iSeries/ItExtVpdPanel.h
--- linux-2.5/include/asm-ppc64/iSeries/ItExtVpdPanel.h	2002-07-18 22:09:42.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/ItExtVpdPanel.h	2003-11-21 06:45:03.000000000 +0000
@@ -16,6 +16,8 @@
  * along with this program; if not, write to the Free Software
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
+#ifndef _ITEXTVPDPANEL_H
+#define _ITEXTVPDPANEL_H
 
 /*
  *
@@ -31,12 +33,8 @@
  * Standard Includes
  *------------------------------------------------------------------- 
 */
-#ifndef	_PPC_TYPES_H
-#include	<asm/types.h>
-#endif
+#include <asm/types.h>
 
-#ifndef _ITEXTVPDPANEL_H
-#define _ITEXTVPDPANEL_H
 struct ItExtVpdPanel
 {
   // Definition of the Extended Vpd On Panel Data Area
diff -purN linux-2.5/include/asm-ppc64/iSeries/ItIplParmsReal.h linuxppc64-2.5/include/asm-ppc64/iSeries/ItIplParmsReal.h
--- linux-2.5/include/asm-ppc64/iSeries/ItIplParmsReal.h	2002-02-14 12:14:36.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/ItIplParmsReal.h	2003-11-21 06:45:03.000000000 +0000
@@ -16,6 +16,8 @@
  * along with this program; if not, write to the Free Software
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
+#ifndef _ITIPLPARMSREAL_H
+#define _ITIPLPARMSREAL_H
 
 //==============================================================================
 //
@@ -31,12 +33,7 @@
 //-------------------------------------------------------------------
 // Standard Includes
 //-------------------------------------------------------------------
-#ifndef	_PPC_TYPES_H
-#include	<asm/types.h>
-#endif
-
-#ifndef _ITIPLPARMSREAL_H
-#define _ITIPLPARMSREAL_H
+#include <asm/types.h>
 
 struct ItIplParmsReal
 {
@@ -75,4 +72,5 @@ struct ItIplParmsReal
 	u64	xRsvd12;		// Reserved				x30-x37
 	u64	xRsvd13;		// Reserved				x38-x3F
 };
-#endif // _ITIPLPARMSREAL_H
+
+#endif /* _ITIPLPARMSREAL_H */
diff -purN linux-2.5/include/asm-ppc64/iSeries/ItLpNaca.h linuxppc64-2.5/include/asm-ppc64/iSeries/ItLpNaca.h
--- linux-2.5/include/asm-ppc64/iSeries/ItLpNaca.h	2003-02-18 18:25:13.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/ItLpNaca.h	2003-11-21 06:45:03.000000000 +0000
@@ -16,6 +16,8 @@
  * along with this program; if not, write to the Free Software
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
+#ifndef _ITLPNACA_H
+#define _ITLPNACA_H
 
 //=============================================================================
 //
@@ -24,10 +26,6 @@
 //
 //=============================================================================
 
-
-#ifndef _ITLPNACA_H
-#define _ITLPNACA_H
-
 struct ItLpNaca
 {
 //=============================================================================
@@ -87,4 +85,4 @@ struct ItLpNaca
 
 //=============================================================================
 
-#endif // _ITLPNACA_H
+#endif /* _ITLPNACA_H */
diff -purN linux-2.5/include/asm-ppc64/iSeries/ItLpPaca.h linuxppc64-2.5/include/asm-ppc64/iSeries/ItLpPaca.h
--- linux-2.5/include/asm-ppc64/iSeries/ItLpPaca.h	2002-02-14 12:14:36.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/ItLpPaca.h	2003-11-21 06:45:03.000000000 +0000
@@ -16,6 +16,8 @@
  * along with this program; if not, write to the Free Software
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
+#ifndef _ITLPPACA_H
+#define _ITLPPACA_H
 
 //=============================================================================
 //                                   
@@ -24,13 +26,7 @@
 //    
 //
 //----------------------------------------------------------------------------
-#ifndef  _PPC_TYPES_H
 #include <asm/types.h>
-#endif
-
-#ifndef _ITLPPACA_H
-#define _ITLPPACA_H
-
 
 struct ItLpPaca
 {
@@ -110,7 +106,10 @@ struct ItLpPaca
 	u64     xPDCSavedSPRG1;         // Saved SPRG1 for PMC int      x68-x6F
 	u64     xPDCSavedSRR0;          // Saved SRR0 for PMC int       x70-x77
 	volatile u32 xVirtualDecr;	// Virtual DECR for shared procsx78-x7B
-	u32	    xRsvd2_2;		// Reserved			x7C-x7F
+	u16     xSLBCount;              // # of SLBs to maintain        x7C-x7D
+	u8      xIdle;                  // Indicate OS is idle          x7E
+	u8      xRsvd2_2;               // Reserved                     x7F
+
 
 //=============================================================================
 // CACHE_LINE_3 0x0100 - 0x007F: This line is shared with other processors
@@ -131,4 +130,5 @@ struct ItLpPaca
 
 
 };
-#endif // _ITLPPACA_H
+
+#endif /* _ITLPPACA_H */
diff -purN linux-2.5/include/asm-ppc64/iSeries/ItLpQueue.h linuxppc64-2.5/include/asm-ppc64/iSeries/ItLpQueue.h
--- linux-2.5/include/asm-ppc64/iSeries/ItLpQueue.h	2002-02-14 12:14:36.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/ItLpQueue.h	2003-11-21 06:45:03.000000000 +0000
@@ -16,6 +16,8 @@
  * along with this program; if not, write to the Free Software
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
+#ifndef _ITLPQUEUE_H
+#define _ITLPQUEUE_H
 
 //=============================================================================
 //
@@ -24,18 +26,11 @@
 //	events to an LP.  
 //    
 
-#ifndef _PPC_TYPES_H
 #include <asm/types.h>
-#endif
 #include <asm/ptrace.h>
 
-
 struct HvLpEvent;
 
-
-#ifndef _ITLPQUEUE_H
-#define _ITLPQUEUE_H
-
 #define ITMaxLpQueues 8
 
 #define NotUsed		0	// Queue will not be used by PLIC
@@ -94,6 +89,4 @@ static __inline__ void process_iSeries_e
 	: : : "r0", "r3" );	
 }
 
-
-//=============================================================================
-#endif // _ITLPQUEUE_H
+#endif /* _ITLPQUEUE_H */
diff -purN linux-2.5/include/asm-ppc64/iSeries/ItLpRegSave.h linuxppc64-2.5/include/asm-ppc64/iSeries/ItLpRegSave.h
--- linux-2.5/include/asm-ppc64/iSeries/ItLpRegSave.h	2002-02-14 12:14:36.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/ItLpRegSave.h	2003-11-21 06:45:03.000000000 +0000
@@ -16,6 +16,8 @@
  * along with this program; if not, write to the Free Software
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
+#ifndef _ITLPREGSAVE_H
+#define _ITLPREGSAVE_H
 
 //=====================================================================================
 //
@@ -24,9 +26,6 @@
 //    
 //
 
-#ifndef _ITLPREGSAVE_H
-#define _ITLPREGSAVE_H
-
 struct ItLpRegSave
 {
 	u32	xDesc;		// Eye catcher  "LpRS" ebcdic	000-003
@@ -84,4 +83,5 @@ struct ItLpRegSave
 
 	u8	xRsvd3[176];	// Reserved			350-3FF
 };
-#endif // _ITLPREGSAVE_H
+
+#endif /* _ITLPREGSAVE_H */
diff -purN linux-2.5/include/asm-ppc64/iSeries/ItVpdAreas.h linuxppc64-2.5/include/asm-ppc64/iSeries/ItVpdAreas.h
--- linux-2.5/include/asm-ppc64/iSeries/ItVpdAreas.h	2002-02-14 12:14:36.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/ItVpdAreas.h	2003-11-21 06:45:03.000000000 +0000
@@ -16,6 +16,8 @@
  * along with this program; if not, write to the Free Software
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
+#ifndef _ITVPDAREAS_H
+#define _ITVPDAREAS_H
 
 //=====================================================================================
 //
@@ -23,13 +25,7 @@
 //	the OS from PLIC (most of which start from the SP).
 //
 
-#ifndef _PPC_TYPES_H
-#include        <asm/types.h>
-#endif
-
-
-#ifndef _ITVPDAREAS_H
-#define _ITVPDAREAS_H
+#include <asm/types.h>
 
 // VPD Entry index is carved in stone - cannot be changed (easily).
 #define ItVpdCecVpd				   0
@@ -97,4 +93,4 @@ struct	ItVpdAreas
 	void * xSlicVpdAdrs[ItVpdMaxEntries];// Array of VPD buffers	130-1EF
 };
 
-#endif // _ITVPDAREAS_H
+#endif /* _ITVPDAREAS_H */
diff -purN linux-2.5/include/asm-ppc64/iSeries/LparMap.h linuxppc64-2.5/include/asm-ppc64/iSeries/LparMap.h
--- linux-2.5/include/asm-ppc64/iSeries/LparMap.h	2002-02-14 12:14:36.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/LparMap.h	2003-11-21 06:45:03.000000000 +0000
@@ -16,14 +16,11 @@
  * along with this program; if not, write to the Free Software
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
-
-#ifndef	_PPC_TYPES_H
-#include	<asm/types.h>
-#endif
-
 #ifndef _LPARMAP_H
 #define _LPARMAP_H
 
+#include <asm/types.h>
+
 /* The iSeries hypervisor will set up mapping for one or more 
  * ESID/VSID pairs (in SLB/segment registers) and will set up
  * mappings of one or more ranges of pages to VAs.
diff -purN linux-2.5/include/asm-ppc64/iSeries/iSeries_dma.h linuxppc64-2.5/include/asm-ppc64/iSeries/iSeries_dma.h
--- linux-2.5/include/asm-ppc64/iSeries/iSeries_dma.h	2002-02-14 12:14:36.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/iSeries_dma.h	2003-11-21 06:45:03.000000000 +0000
@@ -21,9 +21,7 @@
 #define _ISERIES_DMA_H
 
 #include <asm/types.h>
-#ifndef __LINUX_SPINLOCK_H
 #include <linux/spinlock.h>
-#endif
 
 // NUM_TCE_LEVELS defines the largest contiguous block
 // of dma (tce) space we can get.  NUM_TCE_LEVELS = 10 
@@ -94,4 +92,4 @@ extern void              create_virtual_
 
 extern void		 create_pci_bus_tce_table( unsigned busNumber );
 
-#endif // _ISERIES_DMA_H
+#endif /* _ISERIES_DMA_H */
diff -purN linux-2.5/include/asm-ppc64/iSeries/iSeries_io.h linuxppc64-2.5/include/asm-ppc64/iSeries/iSeries_io.h
--- linux-2.5/include/asm-ppc64/iSeries/iSeries_io.h	2002-02-14 12:14:36.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/iSeries_io.h	2003-11-21 06:45:03.000000000 +0000
@@ -1,8 +1,9 @@
+#ifndef _ISERIES_IO_H
+#define _ISERIES_IO_H
+
 #include <linux/config.h>
 
 #ifdef CONFIG_PPC_ISERIES
-#ifndef _ISERIES_IO_H
-#define _ISERIES_IO_H
 #include <linux/types.h>
 /************************************************************************/
 /* File iSeries_io.h created by Allan Trautman on Thu Dec 28 2000.      */
@@ -41,6 +42,5 @@ extern void* iSeries_memset_io(void *des
 extern void* iSeries_memcpy_toio(void *dest, void *source, size_t n);
 extern void* iSeries_memcpy_fromio(void *dest, void *source, size_t n);
 
-#endif /*  _ISERIES_IO_H         */
-#endif /*  CONFIG_PPC_ISERIES  */
-
+#endif /* CONFIG_PPC_ISERIES */
+#endif /* _ISERIES_IO_H */
diff -purN linux-2.5/include/asm-ppc64/iSeries/iSeries_irq.h linuxppc64-2.5/include/asm-ppc64/iSeries/iSeries_irq.h
--- linux-2.5/include/asm-ppc64/iSeries/iSeries_irq.h	2002-02-14 12:14:37.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/iSeries_irq.h	2003-11-21 06:45:03.000000000 +0000
@@ -1,19 +1,11 @@
-
 #ifndef	__ISERIES_IRQ_H__
 #define	__ISERIES_IRQ_H__
 
-
 #ifdef __cplusplus
 extern "C" {
 #endif
 
-unsigned int iSeries_startup_IRQ(unsigned int);
-void iSeries_shutdown_IRQ(unsigned int);
-void iSeries_enable_IRQ(unsigned int);
-void iSeries_disable_IRQ(unsigned int);
-void iSeries_end_IRQ(unsigned int);
 void iSeries_init_IRQ(void);
-void iSeries_init_irqMap(int);
 int  iSeries_allocate_IRQ(HvBusNumber, HvSubBusNumber, HvAgentId);
 int  iSeries_assign_IRQ(int, HvBusNumber, HvSubBusNumber, HvAgentId);
 void iSeries_activate_IRQs(void);
diff -purN linux-2.5/include/asm-ppc64/iSeries/iSeries_pci.h linuxppc64-2.5/include/asm-ppc64/iSeries/iSeries_pci.h
--- linux-2.5/include/asm-ppc64/iSeries/iSeries_pci.h	2002-04-25 04:46:36.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/iSeries_pci.h	2003-12-10 04:13:07.000000000 +0000
@@ -1,10 +1,11 @@
 #ifndef _ISERIES_64_PCI_H
 #define _ISERIES_64_PCI_H
+
 /************************************************************************/
 /* File iSeries_pci.h created by Allan Trautman on Tue Feb 20, 2001.    */
 /************************************************************************/
 /* Define some useful macros for the iSeries pci routines.              */
-/* Copyright (C) 20yy  Allan H Trautman, IBM Corporation                */
+/* Copyright (C) 2001  Allan H Trautman, IBM Corporation                */
 /*                                                                      */
 /* This program is free software; you can redistribute it and/or modify */
 /* it under the terms of the GNU General Public License as published by */
@@ -28,77 +29,81 @@
 /*   Ported to ppc64, May 25, 2001                                      */
 /* End Change Activity                                                  */
 /************************************************************************/
+
 #include <asm/iSeries/HvCallPci.h>
 
 struct pci_dev;				/* For Forward Reference        */
 struct iSeries_Device_Node;
+
 /************************************************************************/
-/* Gets iSeries Bus, SubBus, of DevFn using pci_dev* structure          */
+/* Gets iSeries Bus, SubBus, DevFn using iSeries_Device_Node structure */
 /************************************************************************/
-#define ISERIES_BUS(DevPtr)    DevPtr->DsaAddr.busNumber
-#define ISERIES_SUBBUS(DevPtr) DevPtr->DsaAddr.subBusNumber
-#define ISERIES_DEVICE(DevPtr) DevPtr->DsaAddr.deviceId
-#define ISERIES_DEVFUN(DevPtr) DevPtr->DevFn
-#define ISERIES_DSA(DevPtr)   (*(u64*)&DevPtr->DsaAddr)
+
+#define ISERIES_BUS(DevPtr)	DevPtr->DsaAddr.Dsa.busNumber
+#define ISERIES_SUBBUS(DevPtr)	DevPtr->DsaAddr.Dsa.subBusNumber
+#define ISERIES_DEVICE(DevPtr)	DevPtr->DsaAddr.Dsa.deviceId
+#define ISERIES_DSA(DevPtr)	DevPtr->DsaAddr.DsaAddr
+#define ISERIES_DEVFUN(DevPtr)	DevPtr->DevFn
 #define ISERIES_DEVNODE(PciDev) ((struct iSeries_Device_Node*)PciDev->sysdata)
 
 #define EADsMaxAgents 7
-/************************************************************************************/
-/* Decodes Linux DevFn to iSeries DevFn, bridge device, or function.                */
-/* For Linux, see PCI_SLOT and PCI_FUNC in include/linux/pci.h                      */
-/************************************************************************************/
-#define ISERIES_DECODE_DEVFN(linuxdevfn)  (((linuxdevfn & 0x71) << 1) | (linuxdevfn & 0x07))
-#define ISERIES_DECODE_DEVICE(linuxdevfn) (((linuxdevfn & 0x38) >> 3) |(((linuxdevfn & 0x40) >> 2) + 0x10))
-#define ISERIES_DECODE_FUNCTION(linuxdevfn) (linuxdevfn & 0x07)
+
+/************************************************************************/
+/* Decodes Linux DevFn to iSeries DevFn, bridge device, or function.    */
+/* For Linux, see PCI_SLOT and PCI_FUNC in include/linux/pci.h          */
+/************************************************************************/
+
 #define ISERIES_PCI_AGENTID(idsel,func)	((idsel & 0x0F) << 4) | (func  & 0x07)
+#define ISERIES_ENCODE_DEVICE(agentid)	((0x10) | ((agentid&0x20)>>2) | (agentid&07))
 
 #define ISERIES_GET_DEVICE_FROM_SUBBUS(subbus)   ((subbus >> 5) & 0x7)
 #define ISERIES_GET_FUNCTION_FROM_SUBBUS(subbus) ((subbus >> 2) & 0x7)
 
-#define ISERIES_ENCODE_DEVICE(agentid)	((0x10) | ((agentid&0x20)>>2) | (agentid&07))
-/************************************************************************************/
-/* Converts Virtual Address to Real Address for Hypervisor calls                    */
-/************************************************************************************/
-#define REALADDR(virtaddr)  (0x8000000000000000 | (virt_to_absolute((u64)virtaddr) ))
+/*
+ * N.B. the ISERIES_DECODE_* macros are not used anywhere, and I think
+ * the 0x71 (at least) must be wrong - 0x78 maybe?  -- paulus.
+ */
+#define ISERIES_DECODE_DEVFN(linuxdevfn)  (((linuxdevfn & 0x71) << 1) | (linuxdevfn & 0x07))
+#define ISERIES_DECODE_DEVICE(linuxdevfn) (((linuxdevfn & 0x38) >> 3) |(((linuxdevfn & 0x40) >> 2) + 0x10))
+#define ISERIES_DECODE_FUNCTION(linuxdevfn) (linuxdevfn & 0x07)
 
-/************************************************************************************/
-/* Define TRUE and FALSE Values for Al                                              */
-/************************************************************************************/
-#ifndef TRUE
-#define TRUE 1
-#endif
-#ifndef FALSE
-#define FALSE 0
-#endif
+/************************************************************************/
+/* Converts Virtual Address to Real Address for Hypervisor calls        */
+/************************************************************************/
+
+#define REALADDR(virtaddr)  (0x8000000000000000 | (virt_to_absolute((u64)virtaddr) ))
 
 /************************************************************************/
 /* iSeries Device Information                                           */
 /************************************************************************/
+
 struct iSeries_Device_Node {
-	struct list_head Device_List;    /* Must be first for cast to wo*/
-	struct pci_dev*  PciDev;         /* Pointer to pci_dev structure*/
-        struct HvCallPci_DsaAddr DsaAddr;/* Direct Select Address       */
-                                         /* busNumber,subBusNumber,     */ 
-	                                 /* deviceId, barNumber         */
-	HvAgentId        AgentId;	 /* Hypervisor DevFn            */
-	int              DevFn;          /* Linux devfn                 */
-	int              BarOffset;
-	int              Irq;            /* Assigned IRQ                */
-	int              ReturnCode;	 /* Return Code Holder          */
-	int              IoRetry;        /* Current Retry Count         */
-	int              Flags;          /* Possible flags(disable/bist)*/
-	u16              Vendor;         /* Vendor ID                   */
-	u8               LogicalSlot;    /* Hv Slot Index for Tces      */
-	struct TceTable* DevTceTable;    /* Device TCE Table            */ 
-	u8               PhbId;          /* Phb Card is on.             */
-	u16              Board;          /* Board Number                */
-	u8               FrameId;	 /* iSeries spcn Frame Id       */
-	char             CardLocation[4];/* Char format of planar vpd   */
-	char             Location[20];   /* Frame  1, Card C10          */
+	struct list_head Device_List;
+	struct pci_dev* PciDev;         /* Pointer to pci_dev structure*/
+        union HvDsaMap	DsaAddr;	/* Direct Select Address       */
+                                        /* busNumber,subBusNumber,     */ 
+	                                /* deviceId, barNumber         */
+	HvAgentId       AgentId;	/* Hypervisor DevFn            */
+	int             DevFn;          /* Linux devfn                 */
+	int             BarOffset;
+	int             Irq;            /* Assigned IRQ                */
+	int             ReturnCode;	/* Return Code Holder          */
+	int             IoRetry;        /* Current Retry Count         */
+	int             Flags;          /* Possible flags(disable/bist)*/
+	u16             Vendor;         /* Vendor ID                   */
+	u8              LogicalSlot;    /* Hv Slot Index for Tces      */
+	struct TceTable* DevTceTable;   /* Device TCE Table            */ 
+	u8              PhbId;          /* Phb Card is on.             */
+	u16             Board;          /* Board Number                */
+	u8              FrameId;	/* iSeries spcn Frame Id       */
+	char            CardLocation[4];/* Char format of planar vpd   */
+	char            Location[20];   /* Frame  1, Card C10          */
 };
+
 /************************************************************************/
 /* Location Data extracted from the VPD list and device info.           */
 /************************************************************************/
+
 struct LocationDataStruct { 	/* Location data structure for device  */
 	u16  Bus;               /* iSeries Bus Number              0x00*/
 	u16  Board;             /* iSeries Board                   0x02*/
@@ -108,17 +113,14 @@ struct LocationDataStruct { 	/* Location
 	u8   Card;
 	char CardLocation[4];      
 };
+
 typedef struct LocationDataStruct  LocationData;
 #define LOCATION_DATA_SIZE      48
-/************************************************************************/
-/* Flight Recorder tracing                                              */
-/************************************************************************/
-extern int  iSeries_Set_PciTraceFlag(int TraceFlag);
-extern int  iSeries_Get_PciTraceFlag(void);
 
 /************************************************************************/
 /* Functions                                                            */
 /************************************************************************/
+
 extern LocationData* iSeries_GetLocationData(struct pci_dev* PciDev);
 extern int           iSeries_Device_Information(struct pci_dev*,char*, int);
 extern void          iSeries_Get_Location_Code(struct iSeries_Device_Node*);
diff -purN linux-2.5/include/asm-ppc64/iSeries/iSeries_proc.h linuxppc64-2.5/include/asm-ppc64/iSeries/iSeries_proc.h
--- linux-2.5/include/asm-ppc64/iSeries/iSeries_proc.h	2002-02-14 12:14:37.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/iSeries_proc.h	2003-12-17 05:46:52.000000000 +0000
@@ -16,22 +16,15 @@
  * along with this program; if not, write to the Free Software
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
-
-
-/* Change Activity: */
-/* End Change Activity */
-
 #ifndef _ISERIES_PROC_H
 #define _ISERIES_PROC_H
 
 #include <linux/proc_fs.h>
 
 extern void iSeries_proc_early_init(void);
-extern void iSeries_proc_create(void);
 
 typedef void (*iSeriesProcFunction)(struct proc_dir_entry *iSeries_proc);
 
 extern void iSeries_proc_callback(iSeriesProcFunction initFunction);
 
 #endif /* _iSeries_PROC_H */
-
diff -purN linux-2.5/include/asm-ppc64/iSeries/mf.h linuxppc64-2.5/include/asm-ppc64/iSeries/mf.h
--- linux-2.5/include/asm-ppc64/iSeries/mf.h	2002-02-14 12:14:37.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/mf.h	2003-12-17 05:50:27.000000000 +0000
@@ -23,61 +23,50 @@
  * along with this program; if not, write to the Free Software
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
-
 #ifndef MF_H_INCLUDED
 #define MF_H_INCLUDED
 
+#include <linux/proc_fs.h>
+
 #include <asm/iSeries/HvTypes.h>
 #include <asm/iSeries/HvLpEvent.h>
 
 struct rtc_time;
 
-typedef void (*MFCompleteHandler)( void * clientToken, int returnCode );
-
-extern void mf_allocateLpEvents( HvLpIndex targetLp,
-				 HvLpEvent_Type type,
-				 unsigned size,
-				 unsigned amount,
-				 MFCompleteHandler hdlr,
-				 void * userToken );
-
-extern void mf_deallocateLpEvents( HvLpIndex targetLp,
-				   HvLpEvent_Type type,
-				   unsigned count,
-				   MFCompleteHandler hdlr,
-				   void * userToken );
-
-extern void mf_powerOff( void );
-
-extern void mf_reboot( void );
-
-extern void mf_displaySrc( u32 word );
-extern void mf_displayProgress( u16 value );
+typedef void (*MFCompleteHandler)(void *clientToken, int returnCode);
 
-extern void mf_clearSrc( void );
+extern void mf_allocateLpEvents(HvLpIndex targetLp, HvLpEvent_Type type,
+		unsigned size, unsigned amount, MFCompleteHandler hdlr,
+		void *userToken);
+extern void mf_deallocateLpEvents(HvLpIndex targetLp, HvLpEvent_Type type,
+		unsigned count, MFCompleteHandler hdlr, void *userToken);
+
+extern void mf_powerOff(void);
+extern void mf_reboot(void);
+
+extern void mf_displaySrc(u32 word);
+extern void mf_displayProgress(u16 value);
+extern void mf_clearSrc(void);
 
-extern void mf_init( void );
+extern void mf_init(void);
 
 extern void mf_setSide(char side);
-
 extern char mf_getSide(void);
 
 extern void mf_setCmdLine(const char *cmdline, int size, u64 side);
-
 extern int  mf_getCmdLine(char *cmdline, int *size, u64 side);
 
 extern void mf_getSrcHistory(char *buffer, int size);
 
-extern int mf_setVmlinuxChunk(const char *buffer, int size, int offset, u64 side);
-
+extern int mf_setVmlinuxChunk(const char *buffer, int size, int offset,
+		u64 side);
 extern int mf_getVmlinuxChunk(char *buffer, int *size, int offset, u64 side);
 
 extern int mf_setRtcTime(unsigned long time);
-
 extern int mf_getRtcTime(unsigned long *time);
-
 extern int mf_getRtc( struct rtc_time * tm );
-
 extern int mf_setRtc( struct rtc_time * tm );
 
+extern void mf_proc_init(struct proc_dir_entry *iSeries_proc);
+
 #endif /* MF_H_INCLUDED */
diff -purN linux-2.5/include/asm-ppc64/iSeries/mf_proc.h linuxppc64-2.5/include/asm-ppc64/iSeries/mf_proc.h
--- linux-2.5/include/asm-ppc64/iSeries/mf_proc.h	2002-02-14 12:14:37.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/mf_proc.h	1970-01-01 00:00:00.000000000 +0000
@@ -1,33 +0,0 @@
-/*
- * mf_proc.h
- * Copyright (C) 2001  Kyle A. Lucke IBM Corporation
- * 
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- * 
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- * 
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
- */
-
-
-/* Change Activity: */
-/* End Change Activity */
-
-#ifndef _MF_PROC_H
-#define _MF_PROC_H
-
-#include <linux/proc_fs.h>
-
-void mf_proc_init(struct proc_dir_entry *iSeries_proc);
-
-
-#endif /* _MF_PROC_H */
-
diff -purN linux-2.5/include/asm-ppc64/iSeries/veth-proc.h linuxppc64-2.5/include/asm-ppc64/iSeries/veth-proc.h
--- linux-2.5/include/asm-ppc64/iSeries/veth-proc.h	2002-02-14 12:14:37.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/veth-proc.h	1970-01-01 00:00:00.000000000 +0000
@@ -1,32 +0,0 @@
-/*
- * veth-proc.h
- * Copyright (C) 2001  Kyle A. Lucke IBM Corporation
- * 
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- * 
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- * 
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
- */
-
-
-/* Change Activity: */
-/* End Change Activity */
-
-#ifndef _VETH_PROC_H
-#define _VETH_PROC_H
-
-#include <linux/proc_fs.h>
-
-void veth_proc_init(struct proc_dir_entry *iSeries_proc);
-
-#endif /* _VETH-PROC_H */
-
diff -purN linux-2.5/include/asm-ppc64/iSeries/vio.h linuxppc64-2.5/include/asm-ppc64/iSeries/vio.h
--- linux-2.5/include/asm-ppc64/iSeries/vio.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/iSeries/vio.h	2003-11-21 05:46:23.000000000 +0000
@@ -0,0 +1,129 @@
+/* -*- linux-c -*-
+ *  drivers/char/vio.h
+ *
+ *  iSeries Virtual I/O Message Path header
+ *
+ *  Authors: Dave Boutcher <boutcher@us.ibm.com>
+ *           Ryan Arnold <ryanarn@us.ibm.com>
+ *           Colin Devilbiss <devilbis@us.ibm.com>
+ *
+ * (C) Copyright 2000 IBM Corporation
+ * 
+ * This header file is used by the iSeries virtual I/O device
+ * drivers.  It defines the interfaces to the common functions
+ * (implemented in drivers/char/viopath.h) as well as defining
+ * common functions and structures.  Currently (at the time I 
+ * wrote this comment) the iSeries virtual I/O device drivers
+ * that use this are 
+ *   drivers/block/viodasd.c 
+ *   drivers/char/viocons.c
+ *   drivers/char/viotape.c
+ *   drivers/cdrom/viocd.c
+ *
+ * The iSeries virtual ethernet support (veth.c) uses a whole
+ * different set of functions.
+ * 
+ * This program is free software;  you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) anyu later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of 
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.  
+ *
+ * You should have received a copy of the GNU General Public License 
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ */
+#ifndef _VIO_H
+#define _VIO_H
+
+#include <asm/iSeries/HvTypes.h>
+#include <asm/iSeries/HvLpEvent.h>
+
+/* iSeries virtual I/O events use the subtype field in
+ * HvLpEvent to figure out what kind of vio event is coming
+ * in.  We use a table to route these, and this defines
+ * the maximum number of distinct subtypes
+ */
+#define VIO_MAX_SUBTYPES 7
+
+/* Each subtype can register a handler to process their events.
+ * The handler must have this interface.
+ */
+typedef void (vio_event_handler_t) (struct HvLpEvent * event);
+
+int viopath_open(HvLpIndex remoteLp, int subtype, int numReq);
+int viopath_close(HvLpIndex remoteLp, int subtype, int numReq);
+int vio_setHandler(int subtype, vio_event_handler_t * beh);
+int vio_clearHandler(int subtype);
+int viopath_isactive(HvLpIndex lp);
+HvLpInstanceId viopath_sourceinst(HvLpIndex lp);
+HvLpInstanceId viopath_targetinst(HvLpIndex lp);
+void vio_set_hostlp(void);
+void *vio_get_event_buffer(int subtype);
+void vio_free_event_buffer(int subtype, void *buffer);
+
+extern HvLpIndex viopath_hostLp;
+extern HvLpIndex viopath_ourLp;
+
+#define VIO_MESSAGE "iSeries virtual I/O: "
+#define KERN_DEBUG_VIO KERN_DEBUG VIO_MESSAGE
+#define KERN_INFO_VIO KERN_INFO VIO_MESSAGE
+#define KERN_WARNING_VIO KERN_WARNING VIO_MESSAGE
+
+#define VIOCHAR_MAX_DATA 200
+
+#define VIOMAJOR_SUBTYPE_MASK 0xff00
+#define VIOMINOR_SUBTYPE_MASK 0x00ff
+#define VIOMAJOR_SUBTYPE_SHIFT 8
+
+#define VIOVERSION            0x0101
+
+/*
+This is the general structure for VIO errors; each module should have a table
+of them, and each table should be terminated by an entry of { 0, 0, NULL }.
+Then, to find a specific error message, a module should pass its local table
+and the return code.
+*/
+struct vio_error_entry {
+	u16 rc;
+	int errno;
+	const char *msg;
+};
+const struct vio_error_entry *vio_lookup_rc(const struct vio_error_entry
+					    *local_table, u16 rc);
+
+enum viosubtypes {
+	viomajorsubtype_monitor = 0x0100,
+	viomajorsubtype_blockio = 0x0200,
+	viomajorsubtype_chario = 0x0300,
+	viomajorsubtype_config = 0x0400,
+	viomajorsubtype_cdio = 0x0500,
+	viomajorsubtype_tape = 0x0600
+};
+
+
+enum vioconfigsubtype {
+	vioconfigget = 0x0001,
+};
+
+enum viorc {
+	viorc_good = 0x0000,
+	viorc_noConnection = 0x0001,
+	viorc_noReceiver = 0x0002,
+	viorc_noBufferAvailable = 0x0003,
+	viorc_invalidMessageType = 0x0004,
+	viorc_invalidRange = 0x0201,
+	viorc_invalidToken = 0x0202,
+	viorc_DMAError = 0x0203,
+	viorc_useError = 0x0204,
+	viorc_releaseError = 0x0205,
+	viorc_invalidDisk = 0x0206,
+	viorc_openRejected = 0x0301
+};
+
+#endif /* _VIO_H */
diff -purN linux-2.5/include/asm-ppc64/io.h linuxppc64-2.5/include/asm-ppc64/io.h
--- linux-2.5/include/asm-ppc64/io.h	2003-09-16 16:07:45.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/io.h	2003-10-23 14:10:29.000000000 +0000
@@ -120,11 +120,15 @@ extern void _outsl_ns(volatile u32 *port
  * Map in an area of physical address space, for accessing
  * I/O devices etc.
  */
+extern int __ioremap_explicit(unsigned long p_addr, unsigned long v_addr,
+		     	      unsigned long size, unsigned long flags);
 extern void *__ioremap(unsigned long address, unsigned long size,
 		       unsigned long flags);
 extern void *ioremap(unsigned long address, unsigned long size);
 #define ioremap_nocache(addr, size)	ioremap((addr), (size))
+extern int iounmap_explicit(void *addr, unsigned long size);
 extern void iounmap(void *addr);
+extern void * reserve_phb_iospace(unsigned long size);
 
 /*
  * Change virtual addresses to physical addresses and vv, for
diff -purN linux-2.5/include/asm-ppc64/irq.h linuxppc64-2.5/include/asm-ppc64/irq.h
--- linux-2.5/include/asm-ppc64/irq.h	2003-04-23 07:49:34.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/irq.h	2003-12-10 04:13:07.000000000 +0000
@@ -11,47 +11,63 @@
 
 #include <asm/atomic.h>
 
+/*
+ * This definition means virtually nothing now and could/should go away.
+ */
+#define NR_IRQS		512
+
 extern void disable_irq(unsigned int);
 extern void disable_irq_nosync(unsigned int);
 extern void enable_irq(unsigned int);
 
 /*
- * this is the maximum number of virtual irqs we will use.
+ * Valid interrupt numbers are from 0 to MAX_IRQS - 1.
  */
-#define NR_IRQS			512
+#define MAX_IRQS	(1<<24)
 
-#define NUM_8259_INTERRUPTS	16
-
-/* Interrupt numbers are virtual in case they are sparsely
- * distributed by the hardware.
- */
-#define NR_HW_IRQS		8192
-extern unsigned short real_irq_to_virt_map[NR_HW_IRQS];
-extern unsigned short virt_irq_to_real_map[NR_IRQS];
-/* Create a mapping for a real_irq if it doesn't already exist.
- * Return the virtual irq as a convenience.
- */
-unsigned long virt_irq_create_mapping(unsigned long real_irq);
+extern void *_get_irq_desc(unsigned int irq);
+extern void *_get_real_irq_desc(unsigned int irq);
+#define get_irq_desc(irq) ((irq_desc_t *)_get_irq_desc(irq))
+#define get_real_irq_desc(irq) ((irq_desc_t *)_get_real_irq_desc(irq))
+
+/* Define a way to iterate across irqs fairly efficiently. */
+#define for_each_irq(i) \
+	for ((i) = 0; (i) < MAX_IRQS; (i) = _next_irq(i))
+unsigned int _next_irq(unsigned int irq);
 
-/* These funcs map irqs between real and virtual */
-static inline unsigned long real_irq_to_virt(unsigned long real_irq) {
-	return real_irq_to_virt_map[real_irq];
-}
-static inline unsigned long virt_irq_to_real(unsigned long virt_irq) {
-	return virt_irq_to_real_map[virt_irq];
+static __inline__ int irq_canonicalize(int irq)
+{
+	return irq;
 }
 
 /*
- * This gets called from serial.c, which is now used on
- * powermacs as well as prep/chrp boxes.
- * Prep and chrp both have cascaded 8259 PICs.
+ * Because many systems have two overlapping names spaces for
+ * interrupts (ISA and XICS for example), and the ISA interrupts
+ * have historically not been easy to renumber, we allow ISA
+ * interrupts to take values 0 - 15, and shift up the remaining 
+ * interrupts by 0x10.  
+ *
+ * This would be nice to remove at some point as it adds confusion
+ * and adds a nasty end case if any platform native interrupts have 
+ * values within 0x10 of the end of that namespace.
  */
-static __inline__ int irq_canonicalize(int irq)
+
+#define NUM_ISA_INTERRUPTS	0x10
+
+extern inline int irq_offset_up(int irq)
 {
-	return irq;
+	return(irq + NUM_ISA_INTERRUPTS);
 }
 
-#define NR_MASK_WORDS	((NR_IRQS + 63) / 64)
+extern inline int irq_offset_down(int irq)
+{
+	return(irq - NUM_ISA_INTERRUPTS);
+}
+
+extern inline int irq_offset_value(void)
+{
+	return NUM_ISA_INTERRUPTS;
+}
 
 #endif /* _ASM_IRQ_H */
 #endif /* __KERNEL__ */
diff -purN linux-2.5/include/asm-ppc64/kdb.h linuxppc64-2.5/include/asm-ppc64/kdb.h
--- linux-2.5/include/asm-ppc64/kdb.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/kdb.h	2003-10-13 19:47:30.000000000 +0000
@@ -0,0 +1,82 @@
+/*
+ * Minimalist Kernel Debugger
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *	Scott Lurndal			1999/12/12
+ *		v1.0 restructuring.
+ */
+#if !defined(_ASM_KDB_H)
+#define _ASM_KDB_H
+	/*
+	 * KDB_ENTER() is a macro which causes entry into the kernel
+	 * debugger from any point in the kernel code stream.  If it 
+	 * is intended to be used from interrupt level, it must  use
+	 * a non-maskable entry method.
+	 */
+#define KDB_ENTER()	kdb(KDB_REASON_CALL,0,0);
+
+#ifndef ElfW
+# if ELFCLASSM == ELFCLASS32
+#  define ElfW(x)  Elf32_ ## x
+#  define ELFW(x)  ELF32_ ## x
+# else
+#  define ElfW(x)  Elf64_ ## x
+#  define ELFW(x)  ELF64_ ## x
+# endif
+#endif
+
+	/*
+	 * Define the exception frame for this architecture
+	 */
+struct pt_regs;
+typedef struct pt_regs	*kdb_eframe_t;
+
+	/*
+	 * Needed for exported symbols.
+	 */
+typedef unsigned long kdb_machreg_t;
+
+#define kdb_machreg_fmt		"0x%016lx"
+#define kdb_machreg_fmt0	"0x%016lx"
+#define kdb_bfd_vma_fmt		"0x%016lx"
+#define kdb_bfd_vma_fmt0	"0x%016lx"
+#define kdb_elfw_addr_fmt	"0x%016lx"
+#define kdb_elfw_addr_fmt0	"0x%016lx"
+
+	/*
+	 * Per cpu arch specific kdb state.  Must be in range 0xff000000.
+	 */
+#define KDB_STATE_A_IF		0x01000000	/* Saved IF flag */
+
+	 /*
+	  * Interface from kernel trap handling code to kernel debugger.
+	  */
+extern int	kdba_callback_die(struct pt_regs *, int, long, void*);
+extern int	kdba_callback_bp(struct pt_regs *, int, long, void*);
+extern int	kdba_callback_debug(struct pt_regs *, int, long, void *);
+
+#include <linux/types.h>
+extern int kdba_putarea_size(unsigned long to_xxx, void *from, size_t size);
+extern int kdba_getarea_size(void *to, unsigned long from_xxx, size_t size);
+
+static inline int
+kdba_verify_rw(unsigned long addr, size_t size)
+{
+	unsigned char data[size];
+	return(kdba_getarea_size(data, addr, size) || kdba_putarea_size(addr, data, size));
+}
+
+#endif	/* ASM_KDB_H */
diff -purN linux-2.5/include/asm-ppc64/kdbprivate.h linuxppc64-2.5/include/asm-ppc64/kdbprivate.h
--- linux-2.5/include/asm-ppc64/kdbprivate.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/kdbprivate.h	2003-10-13 19:50:10.000000000 +0000
@@ -0,0 +1,120 @@
+/*
+ * Minimalist Kernel Debugger
+ *
+ * Copyright (C) 1999 Silicon Graphics, Inc.
+ * Copyright (C) Scott Lurndal (slurn@engr.sgi.com)
+ * Copyright (C) Scott Foehner (sfoehner@engr.sgi.com)
+ * Copyright (C) Srinivasa Thirumalachar (sprasad@engr.sgi.com)
+ *
+ * See the file LIA-COPYRIGHT for additional information.
+ *
+ * Written March 1999 by Scott Lurndal at Silicon Graphics, Inc.
+ *
+ * Modifications from:
+ *      Richard Bass                    1999/07/20
+ *              Many bug fixes and enhancements.
+ *      Scott Foehner
+ *              Port to ia64
+ *	Scott Lurndal			1999/12/12
+ *		v1.0 restructuring.
+ *	Keith Owens			2000/05/23
+ *		KDB v1.2
+ */
+#if !defined(_ASM_KDBPRIVATE_H)
+#define _ASM_KDBPRIVATE_H
+
+typedef unsigned long kdb_machinst_t;
+
+	/*
+	 * KDB_MAXBPT describes the total number of breakpoints
+	 * supported by this architecure.  
+	 */
+#define KDB_MAXBPT	4
+	/*
+	 * KDB_MAXHARDBPT describes the total number of hardware
+	 * breakpoint registers that exist.
+	 */
+#define KDB_MAXHARDBPT	 1
+        /*
+         * Provide space for KDB_MAX_COMMANDS commands.
+         */
+#define KDB_MAX_COMMANDS        125
+
+	/*
+	 * Platform specific environment entries
+	 */
+#define KDB_PLATFORM_ENV	"IDMODE=PPC64", "BYTESPERWORD=8", "IDCOUNT=16"
+
+	/*
+	 * Define the direction that the stack grows
+	 */
+#define KDB_STACK_DIRECTION	-1	/* Stack grows down */
+
+	/*
+	 * Support for ia32 debug registers 
+	 */
+typedef struct _kdbhard_bp {
+	kdb_machreg_t	bph_reg;	/* Register this breakpoint uses */
+
+	unsigned int	bph_free:1;	/* Register available for use */
+	unsigned int	bph_data:1;	/* Data Access breakpoint */
+
+	unsigned int	bph_write:1;	/* Write Data breakpoint */
+	unsigned int	bph_mode:2;	/* 0=inst, 1=write, 2=io, 3=read */
+	unsigned int	bph_length:2;	/* 0=1, 1=2, 2=BAD, 3=4 (bytes) */
+} kdbhard_bp_t;
+
+extern kdbhard_bp_t	kdb_hardbreaks[/* KDB_MAXHARDBPT */];
+
+#define PPC64_BREAKPOINT_INSTRUCTION 0x7fe00008    
+#define PPC64_ADJUST_OFFSET 0x00   
+
+#define KDB_HAVE_LONGJMP 
+#ifdef KDB_HAVE_LONGJMP
+typedef struct __kdb_jmp_buf {
+	unsigned int regs[100];
+} kdb_jmp_buf;
+extern int kdb_setjmp(kdb_jmp_buf *);
+extern void kdba_longjmp(kdb_jmp_buf *, int);
+extern kdb_jmp_buf  kdbjmpbuf[];
+#endif	/* KDB_HAVE_LONGJMP */
+
+
+/*
+ A traceback table typically follows each function.
+ The find_tb_table() func will fill in this struct.  Note that the struct
+ is not an exact match with the encoded table defined by the ABI.  It is
+ defined here more for programming convenience.
+ */
+typedef struct {
+    unsigned long	flags;		/* flags: */
+#define KDBTBTAB_FLAGSGLOBALLINK	(1L<<47)
+#define KDBTBTAB_FLAGSISEPROL		(1L<<46)
+#define KDBTBTAB_FLAGSHASTBOFF		(1L<<45)
+#define KDBTBTAB_FLAGSINTPROC		(1L<<44)
+#define KDBTBTAB_FLAGSHASCTL		(1L<<43)
+#define KDBTBTAB_FLAGSTOCLESS		(1L<<42)
+#define KDBTBTAB_FLAGSFPPRESENT		(1L<<41)
+#define KDBTBTAB_FLAGSNAMEPRESENT	(1L<<38)
+#define KDBTBTAB_FLAGSUSESALLOCA	(1L<<37)
+#define KDBTBTAB_FLAGSSAVESCR		(1L<<33)
+#define KDBTBTAB_FLAGSSAVESLR		(1L<<32)
+#define KDBTBTAB_FLAGSSTORESBC		(1L<<31)
+#define KDBTBTAB_FLAGSFIXUP		(1L<<30)
+#define KDBTBTAB_FLAGSPARMSONSTK	(1L<<0)
+    unsigned char	fp_saved;	/* num fp regs saved f(32-n)..f31 */
+    unsigned char	gpr_saved;	/* num gpr's saved */
+    unsigned char	fixedparms;	/* num fixed point parms */
+    unsigned char	floatparms;	/* num float parms */
+    unsigned char	parminfo[32];	/* types of args.  null terminated */
+#define KDBTBTAB_PARMFIXED 1
+#define KDBTBTAB_PARMSFLOAT 2
+#define KDBTBTAB_PARMDFLOAT 3
+    unsigned int	tb_offset;	/* offset from start of func */
+    unsigned long	funcstart;	/* addr of start of function */
+    char		name[64];	/* name of function (null terminated)*/
+    kdb_symtab_t	symtab;		/* fake symtab entry */
+} kdbtbtable_t;
+int kdba_find_tb_table(kdb_machreg_t eip, kdbtbtable_t *tab);
+
+#endif	/* !_ASM_KDBPRIVATE_H */
diff -purN linux-2.5/include/asm-ppc64/machdep.h linuxppc64-2.5/include/asm-ppc64/machdep.h
--- linux-2.5/include/asm-ppc64/machdep.h	2003-06-07 01:59:39.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/machdep.h	2003-11-21 04:51:10.000000000 +0000
@@ -11,6 +11,7 @@
 
 #include <linux/config.h>
 #include <linux/seq_file.h>
+#include <linux/irq.h>
 
 struct pt_regs;
 struct pci_bus;	
@@ -67,6 +68,7 @@ struct machdep_calls {
 	void		(*get_cpuinfo)(struct seq_file *m);
 
 	void		(*init_IRQ)(void);
+	void		(*init_irq_desc)(irq_desc_t *desc);
 	int		(*get_irq)(struct pt_regs *);
 
 	/* Optional, may be NULL. */
@@ -89,6 +91,12 @@ struct machdep_calls {
 	unsigned char	(*udbg_getc)(void);
 	int		(*udbg_getc_poll)(void);
 
+	/* Interface for platform error logging */
+	void 		(*log_error)(char *buf, unsigned int err_type, int fatal);
+
+	ssize_t		(*nvram_write)(char *buf, size_t count, loff_t *index);
+	ssize_t		(*nvram_read)(char *buf, size_t count, loff_t *index);	
+
 #ifdef CONFIG_SMP
 	/* functions for dealing with other cpus */
 	struct smp_ops_t smp_ops;
@@ -113,5 +121,11 @@ void ppc64_attention_msg(unsigned int sr
 /* Print a dump progress message. */
 void ppc64_dump_msg(unsigned int src, const char *msg);
 
+static inline void log_error(char *buf, unsigned int err_type, int fatal)
+{
+	if (ppc_md.log_error)
+		ppc_md.log_error(buf, err_type, fatal);
+}
+
 #endif /* _PPC64_MACHDEP_H */
 #endif /* __KERNEL__ */
diff -purN linux-2.5/include/asm-ppc64/memory.h linuxppc64-2.5/include/asm-ppc64/memory.h
--- linux-2.5/include/asm-ppc64/memory.h	2003-10-01 22:32:13.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/memory.h	2003-11-06 20:34:42.000000000 +0000
@@ -42,23 +42,28 @@ static inline void isync(void)
 #endif
 
 /* Macros for adjusting thread priority (hardware multi-threading) */
-
-#if defined(CONFIG_PPC_ISERIES) || defined(CONFIG_HMT)
+#define HMT_very_low()    asm volatile("or 31,31,31   # very low priority")
 #define HMT_low()	asm volatile("or 1,1,1		# low priority")
+#define HMT_medium_low()  asm volatile("or 6,6,6      # medium low priority")
 #define HMT_medium()	asm volatile("or 2,2,2		# medium priority")
+#define HMT_medium_high() asm volatile("or 5,5,5      # medium high priority")
 #define HMT_high()	asm volatile("or 3,3,3		# high priority")
 
+#define HMT_VERY_LOW    "\tor   31,31,31        # very low priority\n"
 #define HMT_LOW		"\tor	1,1,1		# low priority\n"
+#define HMT_MEDIUM_LOW  "\tor   6,6,6           # medium low priority\n"
 #define HMT_MEDIUM	"\tor	2,2,2		# medium priority\n"
+#define HMT_MEDIUM_HIGH "\tor   5,5,5           # medium high priority\n"
 #define HMT_HIGH	"\tor	3,3,3		# high priority\n"
-#else
-#define HMT_low()	do { } while(0)
-#define HMT_medium()	do { } while(0)
-#define HMT_high()	do { } while(0)
 
-#define HMT_LOW
-#define HMT_MEDIUM
-#define HMT_HIGH
-#endif
+/* 
+ * Various operational modes for SMT
+ * Off    : never run threaded
+ * On     : always run threaded
+ * Dynamic: Allow the system to switch modes as needed
+ */
+#define SMT_OFF      0
+#define SMT_ON       1
+#define SMT_DYNAMIC  2
 
 #endif
diff -purN linux-2.5/include/asm-ppc64/mmu.h linuxppc64-2.5/include/asm-ppc64/mmu.h
--- linux-2.5/include/asm-ppc64/mmu.h	2003-09-07 01:24:09.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/mmu.h	2003-12-17 05:08:23.000000000 +0000
@@ -202,29 +202,51 @@ static inline unsigned long hpt_hash(uns
 	return (vsid & 0x7fffffffff) ^ page;
 }
 
-static inline void _tlbie(unsigned long va, int large)
+static inline void __tlbie(unsigned long va, int large)
 {
-	asm volatile("ptesync": : :"memory");
+	/* clear top 16 bits, non SLS segment */
+	va &= ~(0xffffULL << 48);
 
-	if (large) {
-		asm volatile("clrldi	%0,%0,16\n\
-			      tlbie	%0,1" : : "r"(va) : "memory");
-	} else {
-		asm volatile("clrldi	%0,%0,16\n\
-			      tlbie	%0,0" : : "r"(va) : "memory");
-	}
+	if (large)
+		asm volatile("tlbie %0,1" : : "r"(va) : "memory");
+	else
+		asm volatile("tlbie %0,0" : : "r"(va) : "memory");
+}
 
+static inline void tlbie(unsigned long va, int large)
+{
+	asm volatile("ptesync": : :"memory");
+	__tlbie(va, large);
 	asm volatile("eieio; tlbsync; ptesync": : :"memory");
 }
 
-static inline void _tlbiel(unsigned long va)
+static inline void __tlbiel(unsigned long va)
+{
+	/* clear top 16 bits, non SLS segment */
+	va &= ~(0xffffULL << 48);
+
+	/* 
+	 * Thanks to Alan Modra we are now able to use machine specific 
+	 * assembly instructions (like tlbiel) by using the gas -many flag.
+	 * However we have to support older toolchains so for the moment 
+	 * we hardwire it.
+	 */
+#if 0
+	asm volatile("tlbiel %0" : : "r"(va) : "memory");
+#else
+	asm volatile(".long 0x7c000224 | (%0 << 11)" : : "r"(va) : "memory");
+#endif
+}
+
+static inline void tlbiel(unsigned long va)
 {
 	asm volatile("ptesync": : :"memory");
-	asm volatile("clrldi	%0,%0,16\n\
-		      tlbiel	%0" : : "r"(va) : "memory");
+	__tlbiel(va);
 	asm volatile("ptesync": : :"memory");
 }
 
+extern void htab_finish_init(void);
+
 #endif /* __ASSEMBLY__ */
 
 /*
diff -purN linux-2.5/include/asm-ppc64/mmu_context.h linuxppc64-2.5/include/asm-ppc64/mmu_context.h
--- linux-2.5/include/asm-ppc64/mmu_context.h	2003-09-07 01:24:09.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/mmu_context.h	2003-12-17 04:27:52.000000000 +0000
@@ -6,6 +6,7 @@
 #include <linux/mm.h>	
 #include <asm/mmu.h>	
 #include <asm/ppcdebug.h>	
+#include <asm/cputable.h>
 
 /*
  * Copyright (C) 2001 PPC 64 Team, IBM Corp
@@ -139,10 +140,16 @@ extern void flush_stab(struct task_struc
  * switch_mm is the entry point called from the architecture independent
  * code in kernel/sched.c
  */
-static inline void
-switch_mm(struct mm_struct *prev, struct mm_struct *next,
-	  struct task_struct *tsk)
+static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,
+			     struct task_struct *tsk)
 {
+#ifdef CONFIG_ALTIVEC
+	asm volatile (
+ BEGIN_FTR_SECTION
+	"dssall;\n"
+ END_FTR_SECTION_IFSET(CPU_FTR_ALTIVEC)
+	 : : );
+#endif /* CONFIG_ALTIVEC */
 	flush_stab(tsk, next);
 	cpu_set(smp_processor_id(), next->cpu_vm_mask);
 }
diff -purN linux-2.5/include/asm-ppc64/naca.h linuxppc64-2.5/include/asm-ppc64/naca.h
--- linux-2.5/include/asm-ppc64/naca.h	2003-03-26 04:30:59.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/naca.h	2003-11-06 20:34:42.000000000 +0000
@@ -37,7 +37,12 @@ struct naca_struct {
 	u32 dCacheL1LinesPerPage;	/* L1 d-cache lines / page   0x64 */
 	u32 iCacheL1LogLineSize;	/* L1 i-cache line size Log2 0x68 */
 	u32 iCacheL1LinesPerPage;	/* L1 i-cache lines / page   0x6c */
-	u64 resv0[2];                   /* Reserved           0x70 - 0x7F */
+	u64 smt_snooze_delay;           /* Delay (in usec) before    0x70 */
+                                        /* entering ST mode               */
+	u8  smt_state;                  /* 0 = SMT off               0x78 */
+	                                /* 1 = SMT on                     */
+	                                /* 2 = SMT dynamic                */
+	u8  resv0[7];                   /* Reserved           0x70 - 0x7F */
 };
 
 extern struct naca_struct *naca;
diff -purN linux-2.5/include/asm-ppc64/nvram.h linuxppc64-2.5/include/asm-ppc64/nvram.h
--- linux-2.5/include/asm-ppc64/nvram.h	2002-12-30 12:29:15.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/nvram.h	2003-11-21 04:51:10.000000000 +0000
@@ -11,6 +11,12 @@
 #ifndef _PPC64_NVRAM_H
 #define _PPC64_NVRAM_H
 
+#define NVRW_CNT 0x20
+#define NVRAM_HEADER_LEN 16 /* sizeof(struct nvram_header) */
+#define NVRAM_BLOCK_LEN 16
+#define NVRAM_MAX_REQ (2080/NVRAM_BLOCK_LEN)
+#define NVRAM_MIN_REQ (1056/NVRAM_BLOCK_LEN)
+
 #define NVRAM_AS0  0x74
 #define NVRAM_AS1  0x75
 #define NVRAM_DATA 0x77
@@ -28,4 +34,37 @@
 #define MOTO_RTC_CONTROLA       0x1FF8
 #define MOTO_RTC_CONTROLB       0x1FF9
 
+#define NVRAM_SIG_SP	0x02	/* support processor */
+#define NVRAM_SIG_OF	0x50	/* open firmware config */
+#define NVRAM_SIG_FW	0x51	/* general firmware */
+#define NVRAM_SIG_HW	0x52	/* hardware (VPD) */
+#define NVRAM_SIG_SYS	0x70	/* system env vars */
+#define NVRAM_SIG_CFG	0x71	/* config data */
+#define NVRAM_SIG_ELOG	0x72	/* error log */
+#define NVRAM_SIG_VEND	0x7e	/* vendor defined */
+#define NVRAM_SIG_FREE	0x7f	/* Free space */
+#define NVRAM_SIG_OS	0xa0	/* OS defined */
+
+/* If change this size, then change the size of NVNAME_LEN */
+struct nvram_header {
+	unsigned char signature;
+	unsigned char checksum;
+	unsigned short length;
+	char name[12];
+};
+
+struct nvram_partition {
+	struct list_head partition;
+	struct nvram_header header;
+	unsigned int index;
+};
+
+
+ssize_t pSeries_nvram_read(char *buf, size_t count, loff_t *index);
+ssize_t pSeries_nvram_write(char *buf, size_t count, loff_t *index);
+int nvram_write_error_log(char * buff, int length, unsigned int err_type);
+int nvram_read_error_log(char * buff, int length, unsigned int * err_type);
+int nvram_clear_error_log(void);
+void nvram_print_partitions(char * label);
+
 #endif /* _PPC64_NVRAM_H */
diff -purN linux-2.5/include/asm-ppc64/paca.h linuxppc64-2.5/include/asm-ppc64/paca.h
--- linux-2.5/include/asm-ppc64/paca.h	2003-09-01 23:50:18.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/paca.h	2003-11-18 09:09:58.000000000 +0000
@@ -61,7 +61,7 @@ struct paca_struct {
 	struct ItLpRegSave *xLpRegSavePtr; /* Pointer to LpRegSave for PLIC	0x08 */
 	u64 xCurrent;  		        /* Pointer to current			0x10 */
 	u16 xPacaIndex;			/* Logical processor number		0x18 */
-	u16 active;			/* Is this cpu active?			0x1a */
+        u16 xHwProcNum;                 /* Physical processor number            0x1A */
 	u32 default_decr;		/* Default decrementer value		0x1c */	
 	u64 unused1;
 	u64 xKsave;			/* Saved Kernel stack addr or zero	0x28 */
@@ -94,7 +94,9 @@ struct paca_struct {
 	u32 *prof_buffer;		/* iSeries profiling buffer		0x38 */
 	u32 *prof_stext;		/* iSeries start of kernel text		0x40 */
 	u32 prof_len;			/* iSeries length of profile buffer -1	0x48 */
-	u8  rsvd2[128-76];		/*					0x4C */
+	u8  yielded;                    /* 0 = this processor is running        0x4c */
+	                                /* 1 = this processor is yielded             */
+	u8  rsvd2[128-77];		/*					0x49 */
 
 /*=====================================================================================
  * CACHE_LINE_3 0x0100 - 0x017F
@@ -117,7 +119,7 @@ struct paca_struct {
 	struct ItLpRegSave xRegSav;	/* Register save for proc */
 
 /*=====================================================================================
- * CACHE_LINE_17-18 0x0800 - 0x0EFF Reserved
+ * CACHE_LINE_17-18 0x0800 - 0x08FF Reserved
  *=====================================================================================
  */
 	struct rtas_args xRtas;		/* Per processor RTAS struct */
@@ -126,10 +128,12 @@ struct paca_struct {
 	u8 rsvd5[256-16-sizeof(struct rtas_args)];
 
 /*=====================================================================================
- * CACHE_LINE_19-30 0x0800 - 0x0EFF Reserved
+ * CACHE_LINE_19-30 0x0900 - 0x0EFF Reserved
  *=====================================================================================
  */
-	u8 rsvd6[0x600];
+	u64 slb_shadow[0x20];
+	u64 dispatch_log;
+	u8  rsvd6[0x500 - 0x8];
 
 /*=====================================================================================
  * CACHE_LINE_31 0x0F00 - 0x0F7F Exception stack
diff -purN linux-2.5/include/asm-ppc64/pci.h linuxppc64-2.5/include/asm-ppc64/pci.h
--- linux-2.5/include/asm-ppc64/pci.h	2003-10-16 03:16:27.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/pci.h	2003-11-17 23:08:19.000000000 +0000
@@ -135,6 +135,17 @@ extern void
 pcibios_resource_to_bus(struct pci_dev *dev, struct pci_bus_region *region,
 			struct resource *res);
 
+extern int
+unmap_bus_range(struct pci_bus *bus);
+
+extern int
+remap_bus_range(struct pci_bus *bus);
+
+extern void
+pcibios_fixup_device_resources(struct pci_dev *dev, struct pci_bus *bus);
+
+extern int pci_read_irq_line(struct pci_dev *dev);
+
 #endif	/* __KERNEL__ */
 
 #endif /* __PPC64_PCI_H */
diff -purN linux-2.5/include/asm-ppc64/pgalloc.h linuxppc64-2.5/include/asm-ppc64/pgalloc.h
--- linux-2.5/include/asm-ppc64/pgalloc.h	2003-09-19 06:55:11.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/pgalloc.h	2003-12-17 04:51:16.000000000 +0000
@@ -3,7 +3,10 @@
 
 #include <linux/mm.h>
 #include <linux/slab.h>
+#include <linux/cpumask.h>
+#include <linux/percpu.h>
 #include <asm/processor.h>
+#include <asm/tlb.h>
 
 extern kmem_cache_t *zero_cache;
 
@@ -40,8 +43,6 @@ pmd_free(pmd_t *pmd)
 	kmem_cache_free(zero_cache, pmd);
 }
 
-#define __pmd_free_tlb(tlb, pmd)	pmd_free(pmd)
-
 #define pmd_populate_kernel(mm, pmd, pte) pmd_set(pmd, pte)
 #define pmd_populate(mm, pmd, pte_page) \
 	pmd_populate_kernel(mm, pmd, page_address(pte_page))
@@ -62,15 +63,57 @@ pte_alloc_one(struct mm_struct *mm, unsi
 
 	return NULL;
 }
-
-static inline void
-pte_free_kernel(pte_t *pte)
+		
+static inline void pte_free_kernel(pte_t *pte)
 {
 	kmem_cache_free(zero_cache, pte);
 }
 
 #define pte_free(pte_page)	pte_free_kernel(page_address(pte_page))
-#define __pte_free_tlb(tlb, pte)	pte_free(pte)
+
+struct pte_freelist_batch
+{
+	struct rcu_head	rcu;
+	unsigned int	index;
+	struct page *	pages[0];
+};
+
+#define PTE_FREELIST_SIZE	((PAGE_SIZE - sizeof(struct pte_freelist_batch) / \
+				  sizeof(struct page *)))
+
+extern void pte_free_now(struct page *ptepage);
+extern void pte_free_submit(struct pte_freelist_batch *batch);
+
+DECLARE_PER_CPU(struct pte_freelist_batch *, pte_freelist_cur);
+
+static inline void __pte_free_tlb(struct mmu_gather *tlb, struct page *ptepage)
+{
+	/* This is safe as we are holding page_table_lock */
+        cpumask_t local_cpumask = cpumask_of_cpu(smp_processor_id());
+	struct pte_freelist_batch **batchp = &__get_cpu_var(pte_freelist_cur);
+
+	if (atomic_read(&tlb->mm->mm_users) < 2 ||
+	    cpus_equal(tlb->mm->cpu_vm_mask, local_cpumask)) {
+		pte_free(ptepage);
+		return;
+	}
+
+	if (*batchp == NULL) {
+		*batchp = (struct pte_freelist_batch *)__get_free_page(GFP_ATOMIC);
+		if (*batchp == NULL) {
+			pte_free_now(ptepage);
+			return;
+		}
+		(*batchp)->index = 0;
+	}
+	(*batchp)->pages[(*batchp)->index++] = ptepage;
+	if ((*batchp)->index == PTE_FREELIST_SIZE) {
+		pte_free_submit(*batchp);
+		*batchp = NULL;
+	}
+}
+
+#define __pmd_free_tlb(tlb, pmd)	__pte_free_tlb(tlb, virt_to_page(pmd))
 
 #define check_pgt_cache()	do { } while (0)
 
diff -purN linux-2.5/include/asm-ppc64/pgtable.h linuxppc64-2.5/include/asm-ppc64/pgtable.h
--- linux-2.5/include/asm-ppc64/pgtable.h	2003-10-10 05:16:19.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/pgtable.h	2003-12-17 05:08:23.000000000 +0000
@@ -7,6 +7,7 @@
  */
 
 #ifndef __ASSEMBLY__
+#include <linux/stddef.h>
 #include <asm/processor.h>		/* For TASK_SIZE */
 #include <asm/mmu.h>
 #include <asm/page.h>
@@ -51,10 +52,11 @@
  * Define the address range of the imalloc VM area.
  * (used for ioremap)
  */
-#define IMALLOC_START (ioremap_bot)
+#define IMALLOC_START     (ioremap_bot)
 #define IMALLOC_VMADDR(x) ((unsigned long)(x))
-#define IMALLOC_BASE  (0xE000000000000000)
-#define IMALLOC_END   (IMALLOC_BASE + VALID_EA_BITS)
+#define PHBS_IO_BASE  	  (0xE000000000000000)	/* Reserve 2 gigs for PHBs */
+#define IMALLOC_BASE      (0xE000000080000000)  
+#define IMALLOC_END       (IMALLOC_BASE + VALID_EA_BITS)
 
 /*
  * Define the address range mapped virt <-> physical
@@ -73,22 +75,23 @@
  * Bits in a linux-style PTE.  These match the bits in the
  * (hardware-defined) PowerPC PTE as closely as possible.
  */
-#define _PAGE_PRESENT	0x001UL	/* software: pte contains a translation */
-#define _PAGE_USER	0x002UL	/* matches one of the PP bits */
-#define _PAGE_RW	0x004UL	/* software: user write access allowed */
-#define _PAGE_GUARDED	0x008UL
-#define _PAGE_COHERENT	0x010UL	/* M: enforce memory coherence (SMP systems) */
-#define _PAGE_NO_CACHE	0x020UL	/* I: cache inhibit */
-#define _PAGE_WRITETHRU	0x040UL	/* W: cache write-through */
-#define _PAGE_DIRTY	0x080UL	/* C: page changed */
-#define _PAGE_ACCESSED	0x100UL	/* R: page referenced */
-#define _PAGE_FILE	0x200UL /* software: pte holds file offset */
-#define _PAGE_HASHPTE	0x400UL	/* software: pte has an associated HPTE */
-#define _PAGE_EXEC	0x800UL	/* software: i-cache coherence required */
-#define _PAGE_SECONDARY 0x8000UL /* software: HPTE is in secondary group */
-#define _PAGE_GROUP_IX  0x7000UL /* software: HPTE index within group */
+#define _PAGE_PRESENT	0x0001 /* software: pte contains a translation */
+#define _PAGE_USER	0x0002 /* matches one of the PP bits */
+#define _PAGE_FILE	0x0002 /* (!present only) software: pte holds file offset */
+#define _PAGE_RW	0x0004 /* software: user write access allowed */
+#define _PAGE_GUARDED	0x0008
+#define _PAGE_COHERENT	0x0010 /* M: enforce memory coherence (SMP systems) */
+#define _PAGE_NO_CACHE	0x0020 /* I: cache inhibit */
+#define _PAGE_WRITETHRU	0x0040 /* W: cache write-through */
+#define _PAGE_DIRTY	0x0080 /* C: page changed */
+#define _PAGE_ACCESSED	0x0100 /* R: page referenced */
+#define _PAGE_EXEC	0x0200 /* software: i-cache coherence required */
+#define _PAGE_HASHPTE	0x0400 /* software: pte has an associated HPTE */
+#define _PAGE_BUSY	0x0800 /* software: PTE & hash are busy */ 
+#define _PAGE_SECONDARY 0x8000 /* software: HPTE is in secondary group */
+#define _PAGE_GROUP_IX  0x7000 /* software: HPTE index within group */
 /* Bits 0x7000 identify the index within an HPT Group */
-#define _PAGE_HPTEFLAGS (_PAGE_HASHPTE | _PAGE_SECONDARY | _PAGE_GROUP_IX)
+#define _PAGE_HPTEFLAGS (_PAGE_BUSY | _PAGE_HASHPTE | _PAGE_SECONDARY | _PAGE_GROUP_IX)
 /* PAGE_MASK gives the right answer below, but only by accident */
 /* It should be preserving the high 48 bits and then specifically */
 /* preserving _PAGE_SECONDARY | _PAGE_GROUP_IX */
@@ -156,8 +159,10 @@ extern unsigned long empty_zero_page[PAG
 #define _PMD_HUGEPAGE	0x00000001U
 #define HUGEPTE_BATCH_SIZE (1<<(HPAGE_SHIFT-PMD_SHIFT))
 
+#ifndef __ASSEMBLY__
 int hash_huge_page(struct mm_struct *mm, unsigned long access,
 		   unsigned long ea, unsigned long vsid, int local);
+#endif /* __ASSEMBLY__ */
 
 #define HAVE_ARCH_UNMAPPED_AREA
 #else
@@ -287,15 +292,17 @@ static inline unsigned long pte_update( 
 					unsigned long set )
 {
 	unsigned long old, tmp;
-
+	
 	__asm__ __volatile__(
 	"1:	ldarx	%0,0,%3		# pte_update\n\
+	andi.	%1,%0,%7\n\
+	bne-	1b \n\
 	andc	%1,%0,%4 \n\
 	or	%1,%1,%5 \n\
 	stdcx.	%1,0,%3 \n\
 	bne-	1b"
 	: "=&r" (old), "=&r" (tmp), "=m" (*p)
-	: "r" (p), "r" (clr), "r" (set), "m" (*p)
+	: "r" (p), "r" (clr), "r" (set), "m" (*p), "i" (_PAGE_BUSY)
 	: "cc" );
 	return old;
 }
@@ -399,6 +406,17 @@ void pgtable_cache_init(void);
 extern void hpte_init_pSeries(void);
 extern void hpte_init_iSeries(void);
 
+/* imalloc region types */
+#define IM_REGION_UNUSED	0x1
+#define IM_REGION_SUBSET	0x2
+#define IM_REGION_EXISTS	0x4
+#define IM_REGION_OVERLAP	0x8
+
+extern struct vm_struct * im_get_free_area(unsigned long size);
+extern struct vm_struct * im_get_area(unsigned long v_addr, unsigned long size,
+			int region_type);
+unsigned long im_free(void *addr);
+
 typedef pte_t *pte_addr_t;
 
 long pSeries_lpar_hpte_insert(unsigned long hpte_group,
@@ -410,5 +428,31 @@ long pSeries_hpte_insert(unsigned long h
 			 unsigned long prpn, int secondary,
 			 unsigned long hpteflags, int bolted, int large);
 
+/*
+ * find_linux_pte returns the address of a linux pte for a given 
+ * effective address and directory.  If not found, it returns zero.
+ */
+static inline pte_t *find_linux_pte(pgd_t *pgdir, unsigned long ea)
+{
+	pgd_t *pg;
+	pmd_t *pm;
+	pte_t *pt = NULL;
+	pte_t pte;
+
+	pg = pgdir + pgd_index(ea);
+	if (!pgd_none(*pg)) {
+
+		pm = pmd_offset(pg, ea);
+		if (pmd_present(*pm)) { 
+			pt = pte_offset_kernel(pm, ea);
+			pte = *pt;
+			if (!pte_present(pte))
+				pt = NULL;
+		}
+	}
+
+	return pt;
+}
+
 #endif /* __ASSEMBLY__ */
 #endif /* _PPC64_PGTABLE_H */
diff -purN linux-2.5/include/asm-ppc64/ppc32.h linuxppc64-2.5/include/asm-ppc64/ppc32.h
--- linux-2.5/include/asm-ppc64/ppc32.h	2003-01-17 03:02:40.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/ppc32.h	2003-12-17 04:27:52.000000000 +0000
@@ -40,12 +40,7 @@
 
 /* These are here to support 32-bit syscalls on a 64-bit kernel. */
 
-typedef union sigval32 {
-	int sival_int;
-	unsigned int sival_ptr;
-} sigval_t32;
-
-typedef struct siginfo32 {
+typedef struct compat_siginfo {
 	int si_signo;
 	int si_errno;
 	int si_code;
@@ -69,7 +64,7 @@ typedef struct siginfo32 {
 		struct {
 			compat_pid_t _pid;		/* sender's pid */
 			compat_uid_t _uid;		/* sender's uid */
-			sigval_t32 _sigval;
+			compat_sigval_t _sigval;
 		} _rt;
 
 		/* SIGCHLD */
@@ -92,7 +87,7 @@ typedef struct siginfo32 {
 			int _fd;
 		} _sigpoll;
 	} _sifields;
-} siginfo_t32;
+} compat_siginfo_t;
 
 #define __old_sigaction32	old_sigaction32
 
@@ -126,14 +121,40 @@ struct sigcontext32 {
 	u32 regs;  /* 4 byte pointer to the pt_regs32 structure. */
 };
 
+struct mcontext32 {
+	elf_gregset_t32		mc_gregs;
+	elf_fpregset_t		mc_fregs;
+	unsigned int		mc_pad[2];
+	elf_vrregset_t32	mc_vregs __attribute__((__aligned__(16)));
+};
+
 struct ucontext32 { 
-	unsigned int	  uc_flags;
-	unsigned int 	  uc_link;
-	stack_32_t	  uc_stack;
-	struct sigcontext32 uc_mcontext;
-	sigset_t	  uc_sigmask;	/* mask last for extensibility */
+	unsigned int	  	uc_flags;
+	unsigned int 	  	uc_link;
+	stack_32_t	 	uc_stack;
+	int		 	uc_pad[7];
+	u32			uc_regs;	/* points to uc_mcontext field */
+	compat_sigset_t	 	uc_sigmask;	/* mask last for extensibility */
+	/* glibc has 1024-bit signal masks, ours are 64-bit */
+	int		 	uc_maskext[30];
+	int		 	uc_pad2[3];
+	struct mcontext32	uc_mcontext;
 };
 
+typedef struct compat_sigevent {
+	compat_sigval_t sigev_value;
+	int sigev_signo;
+	int sigev_notify;
+	union {
+		int _pad[SIGEV_PAD_SIZE];
+		int _tid;
+		struct {
+			compat_uptr_t _function;
+			compat_uptr_t _attribute;
+		} _sigev_thread;
+	} _sigev_un;
+} compat_sigevent_t;
+
 struct ipc_kludge_32 {
 	unsigned int msgp;
 	int msgtyp;
diff -purN linux-2.5/include/asm-ppc64/ppc_asm.h linuxppc64-2.5/include/asm-ppc64/ppc_asm.h
--- linux-2.5/include/asm-ppc64/ppc_asm.h	2002-09-17 23:32:53.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/ppc_asm.h	2003-12-17 04:27:52.000000000 +0000
@@ -39,6 +39,19 @@
 #define REST_16FPRS(n, base)	REST_8FPRS(n, base); REST_8FPRS(n+8, base)
 #define REST_32FPRS(n, base)	REST_16FPRS(n, base); REST_16FPRS(n+16, base)
 
+#define SAVE_VR(n,b,base)	li b,THREAD_VR0+(16*(n));  stvx n,b,base
+#define SAVE_2VRS(n,b,base)	SAVE_VR(n,b,base); SAVE_VR(n+1,b,base)
+#define SAVE_4VRS(n,b,base)	SAVE_2VRS(n,b,base); SAVE_2VRS(n+2,b,base)
+#define SAVE_8VRS(n,b,base)	SAVE_4VRS(n,b,base); SAVE_4VRS(n+4,b,base)
+#define SAVE_16VRS(n,b,base)	SAVE_8VRS(n,b,base); SAVE_8VRS(n+8,b,base)
+#define SAVE_32VRS(n,b,base)	SAVE_16VRS(n,b,base); SAVE_16VRS(n+16,b,base)
+#define REST_VR(n,b,base)	li b,THREAD_VR0+(16*(n)); lvx n,b,base
+#define REST_2VRS(n,b,base)	REST_VR(n,b,base); REST_VR(n+1,b,base)
+#define REST_4VRS(n,b,base)	REST_2VRS(n,b,base); REST_2VRS(n+2,b,base)
+#define REST_8VRS(n,b,base)	REST_4VRS(n,b,base); REST_4VRS(n+4,b,base)
+#define REST_16VRS(n,b,base)	REST_8VRS(n,b,base); REST_8VRS(n+8,b,base)
+#define REST_32VRS(n,b,base)	REST_16VRS(n,b,base); REST_16VRS(n+16,b,base)
+
 #define CHECKANYINT(ra,rb)			\
 	mfspr	rb,SPRG3;		/* Get Paca address */\
 	ld	ra,PACALPPACA+LPPACAANYINT(rb); /* Get pending interrupt flags */\
diff -purN linux-2.5/include/asm-ppc64/proc_fs.h linuxppc64-2.5/include/asm-ppc64/proc_fs.h
--- linux-2.5/include/asm-ppc64/proc_fs.h	2003-03-26 04:30:59.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/proc_fs.h	2003-11-21 04:51:10.000000000 +0000
@@ -34,5 +34,6 @@ struct proc_ppc64_t {
 };
 
 extern struct proc_ppc64_t proc_ppc64;
+extern int proc_ppc64_init(void);
 
 #endif /* _PPC64_PROC_FS_H */
diff -purN linux-2.5/include/asm-ppc64/processor.h linuxppc64-2.5/include/asm-ppc64/processor.h
--- linux-2.5/include/asm-ppc64/processor.h	2003-10-01 22:41:11.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/processor.h	2003-12-17 04:27:52.000000000 +0000
@@ -311,6 +311,7 @@
 #define	SPRN_USIA	0x3AB	/* User Sampled Instruction Address Register */
 #define	SPRN_XER	0x001	/* Fixed Point Exception Register */
 #define	SPRN_ZPR	0x3B0	/* Zone Protection Register */
+#define SPRN_VRSAVE     0x100   /* Vector save */
 
 /* Short-hand versions for a number of the above SPRNs */
 
@@ -371,6 +372,7 @@
 #define	PV_ICESTAR	0x0036
 #define	PV_SSTAR	0x0037
 #define	PV_POWER4p	0x0038
+#define	PV_POWER5	0x003A
 #define	PV_630        	0x0040
 #define	PV_630p	        0x0041
 
@@ -378,6 +380,7 @@
 #define PLATFORM_PSERIES      0x0100
 #define PLATFORM_PSERIES_LPAR 0x0101
 #define PLATFORM_ISERIES_LPAR 0x0201
+#define PLATFORM_LPAR         0x0001
 	
 /*
  * List of interrupt controllers.
@@ -462,11 +465,9 @@ void start_thread(struct pt_regs *regs, 
 void release_thread(struct task_struct *);
 
 /* Prepare to copy thread state - unlazy all lazy status */
-#define prepare_to_copy(tsk)	do { } while (0)
+extern void prepare_to_copy(struct task_struct *tsk);
 
-/*
- * Create a new kernel thread.
- */
+/* Create a new kernel thread. */
 extern long kernel_thread(int (*fn)(void *), void *arg, unsigned long flags);
 
 /*
@@ -477,6 +478,7 @@ extern long kernel_thread(int (*fn)(void
 
 /* Lazy FPU handling on uni-processor */
 extern struct task_struct *last_task_used_math;
+extern struct task_struct *last_task_used_altivec;
 
 
 #ifdef __KERNEL__
@@ -516,6 +518,14 @@ struct thread_struct {
 	unsigned long	fpexc_mode;	/* Floating-point exception mode */
 	unsigned long	saved_msr;	/* Save MSR across signal handlers */
 	unsigned long	saved_softe;	/* Ditto for Soft Enable/Disable */
+#ifdef CONFIG_ALTIVEC
+	/* Complete AltiVec register set */
+	vector128	vr[32] __attribute((aligned(16)));
+	/* AltiVec status */
+	vector128	vscr __attribute((aligned(16)));
+	unsigned long	vrsave;
+	int		used_vr;	/* set if process has used altivec */
+#endif /* CONFIG_ALTIVEC */
 };
 
 #define INIT_SP		(sizeof(init_stack) + (unsigned long) &init_stack)
diff -purN linux-2.5/include/asm-ppc64/prom.h linuxppc64-2.5/include/asm-ppc64/prom.h
--- linux-2.5/include/asm-ppc64/prom.h	2003-08-15 00:07:45.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/prom.h	2003-10-29 16:10:41.000000000 +0000
@@ -14,6 +14,8 @@
  * as published by the Free Software Foundation; either version
  * 2 of the License, or (at your option) any later version.
  */
+#include <linux/proc_fs.h>
+#include <asm/atomic.h>
 
 #define PTRRELOC(x)     ((typeof(x))((unsigned long)(x) - offset))
 #define PTRUNRELOC(x)   ((typeof(x))((unsigned long)(x) + offset))
@@ -47,6 +49,17 @@ struct pci_address {
 	u32 a_lo;
 };
 
+struct isa_address {
+	u32 a_hi;
+	u32 a_lo;
+};
+
+struct isa_range {
+	struct isa_address isa_addr;
+	struct pci_address pci_addr;
+	unsigned int size;
+};
+
 struct pci_range32 {
 	struct pci_address child_addr;
 	unsigned int  parent_addr;
@@ -120,6 +133,7 @@ struct device_node {
 	char	*name;
 	char	*type;
 	phandle	node;
+	phandle linux_phandle;
 	int	n_addrs;
 	struct	address_range *addrs;
 	int	n_intrs;
@@ -143,7 +157,43 @@ struct device_node {
 	struct	device_node *sibling;
 	struct	device_node *next;	/* next device of same type */
 	struct	device_node *allnext;	/* next in list of all nodes */
-};
+	struct  proc_dir_entry *pde;       /* this node's proc directory */
+	struct  proc_dir_entry *name_link; /* name symlink */
+	struct  proc_dir_entry *addr_link; /* addr symlink */
+	atomic_t _users;                 /* reference count */
+	unsigned long _flags;
+};
+
+/* flag descriptions */
+#define OF_STALE   0 /* node is slated for deletion */
+#define OF_DYNAMIC 1 /* node and properties were allocated via kmalloc */
+
+#define OF_IS_STALE(x) test_bit(OF_STALE, &x->_flags)
+#define OF_MARK_STALE(x) set_bit(OF_STALE, &x->_flags)
+#define OF_IS_DYNAMIC(x) test_bit(OF_DYNAMIC, &x->_flags)
+#define OF_MARK_DYNAMIC(x) set_bit(OF_DYNAMIC, &x->_flags)
+
+/*
+ * Until 32-bit ppc can add proc_dir_entries to its device_node
+ * definition, we cannot refer to pde, name_link, and addr_link
+ * in arch-independent code.
+ */
+#define HAVE_ARCH_DEVTREE_FIXUPS
+
+static inline void set_node_proc_entry(struct device_node *dn, struct proc_dir_entry *de)
+{
+	dn->pde = de;
+}
+
+static void inline set_node_name_link(struct device_node *dn, struct proc_dir_entry *de)
+{
+	dn->name_link = de;
+}
+
+static void inline set_node_addr_link(struct device_node *dn, struct proc_dir_entry *de)
+{
+	dn->addr_link = de;
+}
 
 typedef u32 prom_arg_t;
 
@@ -168,22 +218,43 @@ struct prom_t {
 };
 
 extern struct prom_t prom;
+extern char *of_stdout_device;
 
 extern int boot_cpuid;
 
-/* Prototypes */
-extern void abort(void);
-extern unsigned long prom_init(unsigned long, unsigned long, unsigned long,
-    unsigned long, unsigned long);
-extern void prom_print(const char *msg);
-extern void relocate_nodes(void);
-extern void finish_device_tree(void);
+/* OBSOLETE: Old stlye node lookup */
 extern struct device_node *find_devices(const char *name);
 extern struct device_node *find_type_devices(const char *type);
 extern struct device_node *find_path_device(const char *path);
 extern struct device_node *find_compatible_devices(const char *type,
 						   const char *compat);
 extern struct device_node *find_all_nodes(void);
+
+/* New style node lookup */
+extern struct device_node *of_find_node_by_name(struct device_node *from,
+	const char *name);
+extern struct device_node *of_find_node_by_type(struct device_node *from,
+	const char *type);
+extern struct device_node *of_find_compatible_node(struct device_node *from,
+	const char *type, const char *compat);
+extern struct device_node *of_find_node_by_path(const char *path);
+extern struct device_node *of_find_all_nodes(struct device_node *prev);
+extern struct device_node *of_get_parent(const struct device_node *node);
+extern struct device_node *of_get_next_child(const struct device_node *node,
+					     struct device_node *prev);
+extern struct device_node *of_node_get(struct device_node *node);
+extern void of_node_put(struct device_node *node);
+
+/* For updating the device tree at runtime */
+extern int of_add_node(const char *path, struct property *proplist);
+extern int of_remove_node(struct device_node *np);
+
+/* Other Prototypes */
+extern unsigned long prom_init(unsigned long, unsigned long, unsigned long,
+	unsigned long, unsigned long);
+extern void prom_print(const char *msg);
+extern void relocate_nodes(void);
+extern void finish_device_tree(void);
 extern int device_is_compatible(struct device_node *device, const char *);
 extern int machine_is_compatible(const char *compat);
 extern unsigned char *get_property(struct device_node *node, const char *name,
diff -purN linux-2.5/include/asm-ppc64/ptrace.h linuxppc64-2.5/include/asm-ppc64/ptrace.h
--- linux-2.5/include/asm-ppc64/ptrace.h	2003-03-26 03:38:55.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/ptrace.h	2003-11-19 21:34:28.000000000 +0000
@@ -16,7 +16,7 @@
  * that the overall structure is a multiple of 16 bytes in length.
  *
  * Note that the offsets of the fields in this struct correspond with
- * the PT_* values below.  This simplifies arch/ppc/kernel/ptrace.c.
+ * the PT_* values below.  This simplifies arch/ppc64/kernel/ptrace.c.
  *
  * This program is free software; you can redistribute it and/or
  * modify it under the terms of the GNU General Public License
@@ -120,13 +120,41 @@ struct pt_regs32 {
 #define PT_RESULT 43
 
 #define PT_FPR0	48
+
+/* Kernel and userspace will both use this PT_FPSCR value.  32-bit apps will have
+ * visibility to the asm-ppc/ptrace.h header instead of this one.
+ */
+#define PT_FPSCR (PT_FPR0 + 32 + 1)	  /* each FP reg occupies 1 slot in 64-bit space */
+
 #ifdef __KERNEL__
-#define PT_FPSCR (PT_FPR0 + 32 + 1)	  /* each FP reg occupies 1 slot in this space */
-#define PT_FPSCR32 (PT_FPR0 + 2*32 + 1)	  /* To the 32-bit user - each FP reg occupies 2 slots in this space */
-#else
-#define PT_FPSCR (PT_FPR0 + 2*32 + 1)	/* each FP reg occupies 2 slots in this space -- Fix when 64-bit apps. */
+#define PT_FPSCR32 (PT_FPR0 + 2*32 + 1)	  /* each FP reg occupies 2 32-bit userspace slots */
 #endif
 
+#define PT_VR0 82	/* each Vector reg occupies 2 slots in 64-bit */
+#define PT_VSCR (PT_VR0 + 32*2 + 1)
+#define PT_VRSAVE (PT_VR0 + 33*2)
+
+#ifdef __KERNEL__
+#define PT_VR0_32 164	/* each Vector reg occupies 4 slots in 32-bit */
+#define PT_VSCR_32 (PT_VR0 + 32*4 + 3)
+#define PT_VRSAVE_32 (PT_VR0 + 33*4)
+#endif
+
+/*
+ * Get/set all the altivec registers vr0..vr31, vscr, vrsave, in one go. 
+ * The transfer totals 34 quadword.  Quadwords 0-31 contain the 
+ * corresponding vector registers.  Quadword 32 contains the vscr as the 
+ * last word (offset 12) within that quadword.  Quadword 33 contains the 
+ * vrsave as the first word (offset 0) within the quadword.
+ *
+ * This definition of the VMX state is compatible with the current PPC32 
+ * ptrace interface.  This allows signal handling and ptrace to use the same 
+ * structures.  This also simplifies the implementation of a bi-arch 
+ * (combined (32- and 64-bit) gdb.
+ */
+#define PTRACE_GETVRREGS	18
+#define PTRACE_SETVRREGS	19
+
 /* Additional PTRACE requests implemented on PowerPC. */
 #define PPC_PTRACE_GETREGS	      0x99  /* Get GPRs 0 - 31 */
 #define PPC_PTRACE_SETREGS	      0x98  /* Set GPRs 0 - 31 */
diff -purN linux-2.5/include/asm-ppc64/rtas.h linuxppc64-2.5/include/asm-ppc64/rtas.h
--- linux-2.5/include/asm-ppc64/rtas.h	2003-09-02 06:46:42.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/rtas.h	2003-12-12 16:13:55.000000000 +0000
@@ -19,6 +19,16 @@
 #define RTAS_UNKNOWN_SERVICE (-1)
 #define RTAS_INSTANTIATE_MAX (1UL<<30) /* Don't instantiate rtas at/above this value */
 
+/* Buffer size for ppc_rtas system call. */
+#define RTAS_RMOBUF_MAX (64 * 1024)
+
+/* RTAS return codes */
+#define RTAS_BUSY		-2	/* RTAS Return Status - Busy */
+#define RTAS_EXTENDED_DELAY_MIN 9900
+#define RTAS_EXTENDED_DELAY_MAX 9905
+
+#define RTAS_UNKNOWN_OP		-1099	/* Return Status - Unknown RTAS Token */
+
 /*
  * In general to call RTAS use rtas_token("string") to lookup
  * an RTAS token for the given string (e.g. "event-scan").
@@ -57,11 +67,11 @@ struct rtas_t {
 };
 
 /* Event classes */
-#define INTERNAL_ERROR		0x80000000 /* set bit 0 */
-#define EPOW_WARNING		0x40000000 /* set bit 1 */
-#define POWERMGM_EVENTS		0x20000000 /* set bit 2 */
-#define HOTPLUG_EVENTS		0x10000000 /* set bit 3 */
-#define EVENT_SCAN_ALL_EVENTS	0xf0000000
+#define RTAS_INTERNAL_ERROR		0x80000000 /* set bit 0 */
+#define RTAS_EPOW_WARNING		0x40000000 /* set bit 1 */
+#define RTAS_POWERMGM_EVENTS		0x20000000 /* set bit 2 */
+#define RTAS_HOTPLUG_EVENTS		0x10000000 /* set bit 3 */
+#define RTAS_EVENT_SCAN_ALL_EVENTS	0xf0000000
 
 /* event-scan returns */
 #define SEVERITY_FATAL		0x5
@@ -165,6 +175,9 @@ extern void call_rtas_display_status(cha
 extern void rtas_restart(char *cmd);
 extern void rtas_power_off(void);
 extern void rtas_halt(void);
+extern int rtas_get_sensor(int sensor, int index, int *state);
+extern int rtas_get_power_level(int powerdomain, int *level);
+extern int rtas_set_indicator(int indicator, int index, int new_value);
 
 /* Given an RTAS status code of 9900..9905 compute the hinted delay */
 unsigned int rtas_extended_busy_delay_time(int status);
@@ -173,13 +186,39 @@ static inline int rtas_is_extended_busy(
 	return status >= 9900 && status <= 9909;
 }
 
+extern void pSeries_log_error(char *buf, unsigned int err_type, int fatal);
+
+/* Error types logged.  */
+#define ERR_FLAG_ALREADY_LOGGED	0x0
+#define ERR_FLAG_BOOT		0x1 	/* log was pulled from NVRAM on boot */
+#define ERR_TYPE_RTAS_LOG	0x2	/* from rtas event-scan */
+#define ERR_TYPE_KERNEL_PANIC	0x4	/* from panic() */
+
+/* All the types and not flags */
+#define ERR_TYPE_MASK	(ERR_TYPE_RTAS_LOG | ERR_TYPE_KERNEL_PANIC)
+
+#define RTAS_ERR KERN_ERR "RTAS: "
+ 
+#define RTAS_ERROR_LOG_MAX 2048
+ 
+ 
+/* Event Scan Parameters */
+#define EVENT_SCAN_ALL_EVENTS	0xf0000000
+#define SURVEILLANCE_TOKEN	9000
+#define LOG_NUMBER		64		/* must be a power of two */
+#define LOG_NUMBER_MASK		(LOG_NUMBER-1)
+
 /* Some RTAS ops require a data buffer and that buffer must be < 4G.
  * Rather than having a memory allocator, just use this buffer
  * (get the lock first), make the RTAS call.  Copy the data instead
  * of holding the buffer for long.
  */
-#define RTAS_DATA_BUF_SIZE 1024
+
+#define RTAS_DATA_BUF_SIZE 4096
 extern spinlock_t rtas_data_buf_lock;
 extern char rtas_data_buf[RTAS_DATA_BUF_SIZE];
 
+/* RMO buffer reserved for user-space RTAS use */
+extern unsigned long rtas_rmo_buf;
+
 #endif /* _PPC64_RTAS_H */
diff -purN linux-2.5/include/asm-ppc64/sigcontext.h linuxppc64-2.5/include/asm-ppc64/sigcontext.h
--- linux-2.5/include/asm-ppc64/sigcontext.h	2003-03-26 03:38:55.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/sigcontext.h	2003-11-19 21:34:28.000000000 +0000
@@ -21,6 +21,27 @@ struct sigcontext {
 	struct pt_regs	*regs;
 	elf_gregset_t	gp_regs;
 	elf_fpregset_t	fp_regs;
+/*
+ * To maintain compatibility with current implementations the sigcontext is 
+ * extended by appending a pointer (v_regs) to a quadword type (elf_vrreg_t) 
+ * followed by an unstructured (vmx_reserve) field of 69 doublewords.  This 
+ * allows the array of vector registers to be quadword aligned independent of 
+ * the alignment of the containing sigcontext or ucontext. It is the 
+ * responsibility of the code setting the sigcontext to set this pointer to 
+ * either NULL (if this processor does not support the VMX feature) or the 
+ * address of the first quadword within the allocated (vmx_reserve) area.
+ *
+ * The pointer (v_regs) of vector type (elf_vrreg_t) is type compatible with 
+ * an array of 34 quadword entries (elf_vrregset_t).  The entries with 
+ * indexes 0-31 contain the corresponding vector registers.  The entry with 
+ * index 32 contains the vscr as the last word (offset 12) within the 
+ * quadword.  This allows the vscr to be stored as either a quadword (since 
+ * it must be copied via a vector register to/from storage) or as a word.  
+ * The entry with index 33 contains the vrsave as the first word (offset 0) 
+ * within the quadword.
+ */
+	elf_vrreg_t	*v_regs;
+	long		vmx_reserve[ELF_NVRREG+ELF_NVRREG+1];
 };
 
 #endif /* _ASM_PPC64_SIGCONTEXT_H */
diff -purN linux-2.5/include/asm-ppc64/smp.h linuxppc64-2.5/include/asm-ppc64/smp.h
--- linux-2.5/include/asm-ppc64/smp.h	2003-08-19 02:46:23.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/smp.h	2003-11-18 09:08:41.000000000 +0000
@@ -22,21 +22,40 @@
 #include <linux/cpumask.h>
 #include <linux/kernel.h>
 
-#ifdef CONFIG_SMP
-
 #ifndef __ASSEMBLY__
 
 #include <asm/paca.h>
 
+#ifdef CONFIG_SMP
+
 extern void smp_message_pass(int target, int msg, unsigned long data, int wait);
 extern void smp_send_tlb_invalidate(int);
 extern void smp_send_xmon_break(int cpu);
 struct pt_regs;
 extern void smp_message_recv(int, struct pt_regs *);
 
-#define cpu_possible(cpu)	paca[cpu].active
 
 #define smp_processor_id() (get_paca()->xPacaIndex)
+#define hard_smp_processor_id() (get_paca()->xHwProcNum)
+
+/*
+ * Retrieve the state of a CPU:
+ * online:          CPU is in a normal run state
+ * possible:        CPU is a candidate to be made online
+ * available:       CPU is candidate for the 'possible' pool
+ *                  Used to get SMT threads started at boot time.
+ * present_at_boot: CPU was available at boot time.  Used in DLPAR
+ *                  code to handle special cases for processor start up.
+ */
+extern cpumask_t cpu_present_at_boot;
+extern cpumask_t cpu_online_map;
+extern cpumask_t cpu_possible_map;
+extern cpumask_t cpu_available_map;
+
+#define cpu_present_at_boot(cpu) cpu_isset(cpu, cpu_present_at_boot)
+#define cpu_online(cpu)          cpu_isset(cpu, cpu_online_map) 
+#define cpu_possible(cpu)        cpu_isset(cpu, cpu_possible_map) 
+#define cpu_available(cpu)       cpu_isset(cpu, cpu_available_map) 
 
 /* Since OpenPIC has only 4 IPIs, we use slightly different message numbers.
  *
@@ -50,7 +69,11 @@ extern void smp_message_recv(int, struct
 void smp_init_iSeries(void);
 void smp_init_pSeries(void);
 
-#endif /* __ASSEMBLY__ */
 #endif /* !(CONFIG_SMP) */
+#endif /* __ASSEMBLY__ */
+
+#define get_hard_smp_processor_id(CPU) (paca[(CPU)].xHwProcNum)
+#define set_hard_smp_processor_id(CPU, VAL) do { (paca[(CPU)].xHwProcNum = VAL); } while (0)
+
 #endif /* !(_PPC64_SMP_H) */
 #endif /* __KERNEL__ */
diff -purN linux-2.5/include/asm-ppc64/spinlock.h linuxppc64-2.5/include/asm-ppc64/spinlock.h
--- linux-2.5/include/asm-ppc64/spinlock.h	2002-09-10 09:50:15.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/spinlock.h	2003-11-14 18:45:32.000000000 +0000
@@ -6,16 +6,27 @@
  *
  * Copyright (C) 2001 Paul Mackerras <paulus@au.ibm.com>, IBM
  * Copyright (C) 2001 Anton Blanchard <anton@au.ibm.com>, IBM
- *
- * Type of int is used as a full 64b word is not necessary.
+ * Copyright (C) 2002 Dave Engebretsen <engebret@us.ibm.com>, IBM
+ *   Rework to support virtual processors
  *
  * This program is free software; you can redistribute it and/or
  * modify it under the terms of the GNU General Public License
  * as published by the Free Software Foundation; either version
  * 2 of the License, or (at your option) any later version.
  */
+
+#include <asm/hvcall.h>
+
+/*
+ * The following define is being used to select basic or shared processor
+ * locking when running on an RPA platform.  As we do more performance
+ * tuning, I would expect this selection mechanism to change.  Dave E. 
+ */
+#define SPLPAR_LOCKS
+#define HVSC			".long 0x44000022\n"
+
 typedef struct {
-	volatile unsigned int lock;
+	volatile unsigned long lock;
 } spinlock_t;
 
 #ifdef __KERNEL__
@@ -25,15 +36,15 @@ typedef struct {
 
 static __inline__ int _raw_spin_trylock(spinlock_t *lock)
 {
-	unsigned int tmp;
+	unsigned long tmp;
 
 	__asm__ __volatile__(
-"1:	lwarx		%0,0,%1		# spin_trylock\n\
-	cmpwi		0,%0,0\n\
+"1:	ldarx		%0,0,%1		# spin_trylock\n\
+	cmpdi		0,%0,0\n\
 	li		%0,0\n\
 	bne-		2f\n\
 	li		%0,1\n\
-	stwcx.		%0,0,%1\n\
+	stdcx.		13,0,%1\n\
 	bne-		1b\n\
 	isync\n\
 2:"	: "=&r"(tmp)
@@ -43,28 +54,115 @@ static __inline__ int _raw_spin_trylock(
 	return tmp;
 }
 
+/*
+ * Spin lock states:
+ *   0        : Unlocked
+ *   Negative : Locked.  Value is paca pointer (0xc...0) of holder
+ */
+#ifdef CONFIG_PPC_ISERIES
+static __inline__ void _raw_spin_lock(spinlock_t *lock)
+{
+	unsigned long tmp, tmp2;
+
+	__asm__ __volatile__(
+	"b		2f		# spin_lock\n\
+1:"
+	HMT_LOW
+"       ldx		%0,0,%2         # load the lock value\n\
+	cmpdi		0,%0,0          # if not locked, try to acquire\n\
+	beq-		2f\n\
+        lwz             5,0x280(%0)     # load yield counter\n\
+        andi.           %1,5,1          # if even then spin\n\
+        beq             1b\n\
+        lwsync                          # if odd, give up cycles\n\
+        ldx             %1,0,%2         # reverify the lock holder\n\
+        cmpd            %0,%1\n\
+        bne             1b              # new holder so restart\n\
+        li              3,0x25          # yield hcall 0x8-12 \n\
+        rotrdi          3,3,1           #   put the bits in the right spot\n\
+        lhz             4,0x18(%0)      # processor number\n\
+        sldi            4,4,32          #   move into top half of word\n\
+        or              5,5,4           # r5 has yield cnt - or it in\n\
+        li              4,2             # yield to processor\n\
+        li              0,-1            # indicate an hcall\n\
+        sc                              # do the hcall \n\
+        b               1b\n\
+2: \n"
+	HMT_MEDIUM
+" 	ldarx		%0,0,%2\n\
+	cmpdi		0,%0,0\n\
+	bne-		1b\n\
+	stdcx.		13,0,%2\n\
+	bne-		2b\n\
+	isync"
+	: "=&r"(tmp), "=&r"(tmp2)
+	: "r"(&lock->lock)
+	: "r0", "r3", "r4", "r5", "ctr", "cr0", "cr1", "cr2", "cr3", "cr4", 
+	  "xer", "memory");
+}
+#else
+#ifdef SPLPAR_LOCKS
 static __inline__ void _raw_spin_lock(spinlock_t *lock)
 {
-	unsigned int tmp;
+	unsigned long tmp, tmp2;
 
 	__asm__ __volatile__(
 	"b		2f		# spin_lock\n\
 1:"
 	HMT_LOW
-"	lwzx		%0,0,%1\n\
-	cmpwi		0,%0,0\n\
-	bne+		1b\n"
+"       ldx		%0,0,%2         # load the lock value\n\
+	cmpdi		0,%0,0          # if not locked, try to acquire\n\
+	beq-		2f\n\
+        lwz             5,0x280(%0)     # load dispatch counter\n\
+        andi.           %1,5,1          # if even then spin\n\
+        beq             1b\n\
+        lwsync                          # if odd, give up cycles\n\
+        ldx             %1,0,%2         # reverify the lock holder\n\
+        cmpd            %0,%1\n\
+        bne             1b              # new holder so restart\n\
+        li              3,0xE4          # give up the cycles H_CONFER\n\
+        lhz             4,0x18(%0)      # processor number\n\
+                                        # r5 has dispatch cnt already\n"
+	HVSC
+"        b               1b\n\
+2: \n"
 	HMT_MEDIUM
-"2:	lwarx		%0,0,%1\n\
-	cmpwi		0,%0,0\n\
+" 	ldarx		%0,0,%2\n\
+	cmpdi		0,%0,0\n\
 	bne-		1b\n\
-	stwcx.		%2,0,%1\n\
+	stdcx.		13,0,%2\n\
+	bne-		2b\n\
+	isync"
+	: "=&r"(tmp), "=&r"(tmp2)
+	: "r"(&lock->lock)
+	: "r3", "r4", "r5", "cr0", "cr1", "ctr", "xer", "memory");
+}
+#else
+static __inline__ void _raw_spin_lock(spinlock_t *lock)
+{
+	unsigned long tmp;
+
+	__asm__ __volatile__(
+       "b		2f		# spin_lock\n\
+1:"
+	HMT_LOW
+"       ldx		%0,0,%1         # load the lock value\n\
+	cmpdi		0,%0,0          # if not locked, try to acquire\n\
+	bne+		1b\n\
+2: \n"
+	HMT_MEDIUM
+" 	ldarx		%0,0,%1\n\
+	cmpdi		0,%0,0\n\
+	bne-		1b\n\
+	stdcx.		13,0,%1\n\
 	bne-		2b\n\
 	isync"
 	: "=&r"(tmp)
-	: "r"(&lock->lock), "r"(1)
+	: "r"(&lock->lock)
 	: "cr0", "memory");
 }
+#endif
+#endif
 
 static __inline__ void _raw_spin_unlock(spinlock_t *lock)
 {
@@ -81,25 +179,35 @@ static __inline__ void _raw_spin_unlock(
  * can "mix" irq-safe locks - any writer needs to get a
  * irq-safe write-lock, but readers can get non-irqsafe
  * read-locks.
+ *
+ * Write lock states:
+ *   0        : Unlocked
+ *   Positive : Reader count
+ *   Negative : Writer locked.  Value is paca pointer (0xc...0) of holder
+ *
+ * If lock is not held, try to acquire.
+ * If lock is held by a writer, confer cycles to the holder.
+ * If lock is help by reader(s), spin.  Need to experiment with confer all
+ *   option.  This is likely a pretty weak optimization and may not be 
+ *   worth the additional path length.
  */
 typedef struct {
-	volatile signed int lock;
+	volatile signed long lock;
 } rwlock_t;
 
 #define RW_LOCK_UNLOCKED (rwlock_t) { 0 }
 
 static __inline__ int _raw_read_trylock(rwlock_t *rw)
 {
-	unsigned int tmp;
-	unsigned int ret;
+	unsigned long tmp;
+	unsigned long ret;
 
 	__asm__ __volatile__(
-"1:	lwarx		%0,0,%2		# read_trylock\n\
+"1:	ldarx		%0,0,%2		# read_trylock\n\
 	li		%1,0\n\
-	extsw		%0,%0\n\
 	addic.		%0,%0,1\n\
 	ble-		2f\n\
-	stwcx.		%0,0,%2\n\
+	stdcx.		%0,0,%2\n\
 	bne-		1b\n\
 	li		%1,1\n\
 	isync\n\
@@ -110,39 +218,118 @@ static __inline__ int _raw_read_trylock(
 	return ret;
 }
 
+#ifdef CONFIG_PPC_ISERIES
 static __inline__ void _raw_read_lock(rwlock_t *rw)
 {
-	unsigned int tmp;
+	unsigned long tmp, tmp2;
 
 	__asm__ __volatile__(
 	"b		2f		# read_lock\n\
 1:"
 	HMT_LOW
-"	lwax		%0,0,%1\n\
-	cmpwi		0,%0,0\n\
-	blt+		1b\n"
+"	ldx		%0,0,%2\n\
+	cmpdi		0,%0,0\n\
+	bge-		2f\n\
+        lwz             5,0x280(%0)     # load yield counter\n\
+        andi.           %1,5,1          # if even then spin\n\
+        beq             1b\n\
+        lwsync                          # if odd, give up cycles\n\
+        ldx             %1,0,%2         # reverify the lock holder\n\
+        cmpd            %0,%1\n\
+        bne             1b              # new holder so restart\n\
+        li              3,0x25          # yield hcall 0x8-12 \n\
+        rotrdi          3,3,1           #   put the bits in the right spot\n\
+        lhz             4,0x18(%0)      # processor number\n\
+        sldi            4,4,32          #   move into top half of word\n\
+        or              5,5,4           # r5 has yield cnt - or it in\n\
+        li              4,2             # yield to processor\n\
+        li              0,-1            # indicate an hcall\n\
+        sc                              # do the hcall \n\
+2: \n"
 	HMT_MEDIUM
-"2:	lwarx		%0,0,%1\n\
-	extsw		%0,%0\n\
+" 	ldarx		%0,0,%2\n\
 	addic.		%0,%0,1\n\
 	ble-		1b\n\
-	stwcx.		%0,0,%1\n\
+	stdcx.		%0,0,%2\n\
+	bne-		2b\n\
+	isync"
+	: "=&r"(tmp), "=&r"(tmp2)
+	: "r"(&rw->lock)
+	: "r0", "r3", "r4", "r5", "ctr", "cr0", "cr1", "cr2", "cr3", "cr4", 
+	  "xer", "memory");
+}
+#else
+#ifdef SPLPAR_LOCKS
+static __inline__ void _raw_read_lock(rwlock_t *rw)
+{
+	unsigned long tmp, tmp2;
+
+	__asm__ __volatile__(
+	"b		2f		# read_lock\n\
+1:"
+	HMT_LOW
+"	ldx		%0,0,%2\n\
+	cmpdi		0,%0,0\n\
+	bge-		2f\n\
+        lwz             5,0x280(%0)     # load dispatch counter\n\
+        andi.           %1,5,1          # if even then spin\n\
+        beq             1b\n\
+        lwsync                          # if odd, give up cycles\n\
+        ldx             %1,0,%2         # reverify the lock holder\n\
+        cmpd            %0,%1\n\
+        bne             1b              # new holder so restart\n\
+        li              3,0xE4          # give up the cycles H_CONFER\n\
+        lhz             4,0x18(%0)      # processor number\n\
+                                        # r5 has dispatch cnt already\n"
+	HVSC
+"2: \n"
+	HMT_MEDIUM
+" 	ldarx		%0,0,%2\n\
+	addic.		%0,%0,1\n\
+	ble-		1b\n\
+	stdcx.		%0,0,%2\n\
+	bne-		2b\n\
+	isync"
+	: "=&r"(tmp), "=&r"(tmp2)
+	: "r"(&rw->lock)
+	: "r3", "r4", "r5", "cr0", "cr1", "ctr", "xer", "memory");
+}
+#else
+static __inline__ void _raw_read_lock(rwlock_t *rw)
+{
+	unsigned long tmp;
+
+	__asm__ __volatile__(
+	"b		2f		# read_lock\n\
+1:"
+	HMT_LOW
+"	ldx		%0,0,%1\n\
+	cmpdi		0,%0,0\n\
+	blt+		1b\n\
+2: \n"
+	HMT_MEDIUM
+" 	ldarx		%0,0,%1\n\
+	addic.		%0,%0,1\n\
+	ble-		1b\n\
+	stdcx.		%0,0,%1\n\
 	bne-		2b\n\
 	isync"
 	: "=&r"(tmp)
 	: "r"(&rw->lock)
 	: "cr0", "memory");
 }
+#endif
+#endif
 
 static __inline__ void _raw_read_unlock(rwlock_t *rw)
 {
-	unsigned int tmp;
+	unsigned long tmp;
 
 	__asm__ __volatile__(
 	"eieio				# read_unlock\n\
-1:	lwarx		%0,0,%1\n\
+1:	ldarx		%0,0,%1\n\
 	addic		%0,%0,-1\n\
-	stwcx.		%0,0,%1\n\
+	stdcx.		%0,0,%1\n\
 	bne-		1b"
 	: "=&r"(tmp)
 	: "r"(&rw->lock)
@@ -151,15 +338,15 @@ static __inline__ void _raw_read_unlock(
 
 static __inline__ int _raw_write_trylock(rwlock_t *rw)
 {
-	unsigned int tmp;
-	unsigned int ret;
+	unsigned long tmp;
+	unsigned long ret;
 
 	__asm__ __volatile__(
-"1:	lwarx		%0,0,%2		# write_trylock\n\
-	cmpwi		0,%0,0\n\
+"1:	ldarx		%0,0,%2		# write_trylock\n\
+	cmpdi		0,%0,0\n\
 	li		%1,0\n\
 	bne-		2f\n\
-	stwcx.		%3,0,%2\n\
+	stdcx.		13,0,%2\n\
 	bne-		1b\n\
 	li		%1,1\n\
 	isync\n\
@@ -170,28 +357,112 @@ static __inline__ int _raw_write_trylock
 	return ret;
 }
 
+#ifdef CONFIG_PPC_ISERIES
+static __inline__ void _raw_write_lock(rwlock_t *rw)
+{
+	unsigned long tmp, tmp2;
+
+	__asm__ __volatile__(
+	"b		2f		# spin_lock\n\
+1:"
+	HMT_LOW
+"       ldx		%0,0,%2         # load the lock value\n\
+	cmpdi		0,%0,0          # if not locked(0), try to acquire\n\
+	beq-		2f\n\
+        bgt             1b              # negative(0xc..)->cycles to holder\n"
+"3:     lwz             5,0x280(%0)     # load yield counter\n\
+        andi.           %1,5,1          # if even then spin\n\
+        beq             1b\n\
+        lwsync                          # if odd, give up cycles\n\
+        ldx             %1,0,%2         # reverify the lock holder\n\
+        cmpd            %0,%1\n\
+        bne             1b              # new holder so restart\n\
+        lhz             4,0x18(%0)      # processor number\n\
+        sldi            4,4,32          #   move into top half of word\n\
+        or              5,5,4           # r5 has yield cnt - or it in\n\
+        li              3,0x25          # yield hcall 0x8-12 \n\
+        rotrdi          3,3,1           #   put the bits in the right spot\n\
+        li              4,2             # yield to processor\n\
+        li              0,-1            # indicate an hcall\n\
+        sc                              # do the hcall \n\
+2: \n"
+	HMT_MEDIUM
+" 	ldarx		%0,0,%2\n\
+	cmpdi		0,%0,0\n\
+	bne-		1b\n\
+	stdcx.		13,0,%2\n\
+	bne-		2b\n\
+	isync"
+	: "=&r"(tmp), "=&r"(tmp2)
+	: "r"(&rw->lock)
+	: "r0", "r3", "r4", "r5", "ctr", "cr0", "cr1", "cr2", "cr3", "cr4", 
+	  "xer", "memory");
+}
+#else
+#ifdef SPLPAR_LOCKS
+static __inline__ void _raw_write_lock(rwlock_t *rw)
+{
+	unsigned long tmp, tmp2;
+
+	__asm__ __volatile__(
+	"b		2f		# spin_lock\n\
+1:"
+	HMT_LOW
+"       ldx		%0,0,%2         # load the lock value\n\
+        li              3,0xE4          # give up the cycles H_CONFER\n\
+	cmpdi		0,%0,0          # if not locked(0), try to acquire\n\
+	beq-		2f\n\
+        blt             3f              # negative(0xc..)->confer to holder\n\
+        b               1b\n"
+"3:      lwz             5,0x280(%0)     # load dispatch counter\n\
+        andi.           %1,5,1          # if even then spin\n\
+        beq             1b\n\
+        lwsync                          # if odd, give up cycles\n\
+        ldx             %1,0,%2         # reverify the lock holder\n\
+        cmpd            %0,%1\n\
+        bne             1b              # new holder so restart\n\
+        lhz             4,0x18(%0)      # processor number\n\
+                                        # r5 has dispatch cnt already\n"
+	HVSC
+"        b               1b\n\
+2: \n"
+	HMT_MEDIUM
+" 	ldarx		%0,0,%2\n\
+	cmpdi		0,%0,0\n\
+	bne-		1b\n\
+	stdcx.		13,0,%2\n\
+	bne-		2b\n\
+	isync"
+	: "=&r"(tmp), "=&r"(tmp2)
+	: "r"(&rw->lock)
+	: "r3", "r4", "r5", "cr0", "cr1", "ctr", "xer", "memory");
+}
+#else
 static __inline__ void _raw_write_lock(rwlock_t *rw)
 {
-	unsigned int tmp;
+	unsigned long tmp;
 
 	__asm__ __volatile__(
-	"b		2f		# write_lock\n\
+	"b		2f		# spin_lock\n\
 1:"
 	HMT_LOW
-	"lwax		%0,0,%1\n\
-	cmpwi		0,%0,0\n\
-	bne+		1b\n"
+"       ldx		%0,0,%1         # load the lock value\n\
+	cmpdi		0,%0,0          # if not locked(0), try to acquire\n\
+	bne+		1b\n\
+2: \n"
 	HMT_MEDIUM
-"2:	lwarx		%0,0,%1\n\
-	cmpwi		0,%0,0\n\
+" 	ldarx		%0,0,%1\n\
+	cmpdi		0,%0,0\n\
 	bne-		1b\n\
-	stwcx.		%2,0,%1\n\
+	stdcx.		13,0,%1\n\
 	bne-		2b\n\
 	isync"
 	: "=&r"(tmp)
-	: "r"(&rw->lock), "r"(-1)
+	: "r"(&rw->lock)
 	: "cr0", "memory");
 }
+#endif
+#endif
 
 static __inline__ void _raw_write_unlock(rwlock_t *rw)
 {
@@ -214,7 +485,7 @@ static __inline__ int is_write_locked(rw
 
 #define rwlock_init(x)         do { *(x) = RW_LOCK_UNLOCKED; } while(0)
 
-#define rwlock_is_locked(x)	((x)->lock)
+#define rwlock_is_locked(x) ((x)->lock != 0)
 
 #endif /* __KERNEL__ */
 #endif /* __ASM_SPINLOCK_H */
diff -purN linux-2.5/include/asm-ppc64/system.h linuxppc64-2.5/include/asm-ppc64/system.h
--- linux-2.5/include/asm-ppc64/system.h	2003-06-24 04:58:49.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/system.h	2003-12-17 04:27:52.000000000 +0000
@@ -85,6 +85,9 @@ extern int _get_PVR(void);
 extern void giveup_fpu(struct task_struct *);
 extern void disable_kernel_fp(void);
 extern void enable_kernel_fp(void);
+extern void giveup_altivec(struct task_struct *);
+extern void disable_kernel_altivec(void);
+extern void enable_kernel_altivec(void);
 extern void cvt_fd(float *from, double *to, unsigned long *fpscr);
 extern void cvt_df(double *from, float *to, unsigned long *fpscr);
 extern int abs(int);
@@ -101,13 +104,6 @@ extern struct task_struct * _switch(stru
 struct pt_regs;
 extern void dump_regs(struct pt_regs *);
 
-#define irqs_disabled()				\
-({						\
-	unsigned long flags;			\
-	local_save_flags(flags);		\
-	!(flags & MSR_EE);			\
-})
-
 static inline int __is_processor(unsigned long pv)
 {
 	unsigned long pvr;
diff -purN linux-2.5/include/asm-ppc64/tlb.h linuxppc64-2.5/include/asm-ppc64/tlb.h
--- linux-2.5/include/asm-ppc64/tlb.h	2003-08-19 02:46:23.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/tlb.h	2003-12-17 04:51:16.000000000 +0000
@@ -74,6 +74,8 @@ static inline void __tlb_remove_tlb_entr
 	batch->index = i;
 }
 
+extern void pte_free_finish(void);
+
 static inline void tlb_flush(struct mmu_gather *tlb)
 {
 	int cpu = smp_processor_id();
@@ -86,6 +88,8 @@ static inline void tlb_flush(struct mmu_
 
 	flush_hash_range(tlb->mm->context, batch->index, local);
 	batch->index = 0;
+
+	pte_free_finish();
 }
 
 #endif /* _PPC64_TLB_H */
diff -purN linux-2.5/include/asm-ppc64/unistd.h linuxppc64-2.5/include/asm-ppc64/unistd.h
--- linux-2.5/include/asm-ppc64/unistd.h	2003-07-18 23:52:28.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/unistd.h	2003-11-27 05:07:50.000000000 +0000
@@ -264,8 +264,10 @@
 #define __NR_utimes		251
 #define __NR_statfs64		252
 #define __NR_fstatfs64		253
+#define __NR_fadvise64_64	254
+#define __NR_rtas		255
 
-#define __NR_syscalls		254
+#define __NR_syscalls		256
 #ifdef __KERNEL__
 #define NR_syscalls	__NR_syscalls
 #endif
@@ -287,6 +289,7 @@
 		register unsigned long __sc_5  __asm__ ("r5");		\
 		register unsigned long __sc_6  __asm__ ("r6");		\
 		register unsigned long __sc_7  __asm__ ("r7");		\
+		register unsigned long __sc_8  __asm__ ("r8");		\
 									\
 		__sc_loadargs_##nr(name, args);				\
 		__asm__ __volatile__					\
@@ -295,10 +298,10 @@
 			: "=&r" (__sc_0),				\
 			  "=&r" (__sc_3),  "=&r" (__sc_4),		\
 			  "=&r" (__sc_5),  "=&r" (__sc_6),		\
-			  "=&r" (__sc_7)				\
+			  "=&r" (__sc_7),  "=&r" (__sc_8)		\
 			: __sc_asm_input_##nr				\
 			: "cr0", "ctr", "memory",			\
-			  "r8", "r9", "r10","r11", "r12");		\
+			        "r9", "r10","r11", "r12");		\
 		__sc_ret = __sc_3;					\
 		__sc_err = __sc_0;					\
 	}								\
@@ -326,6 +329,9 @@
 #define __sc_loadargs_5(name, arg1, arg2, arg3, arg4, arg5)		\
 	__sc_loadargs_4(name, arg1, arg2, arg3, arg4);			\
 	__sc_7 = (unsigned long) (arg5)
+#define __sc_loadargs_6(name, arg1, arg2, arg3, arg4, arg5, arg6)	\
+	__sc_loadargs_5(name, arg1, arg2, arg3, arg4, arg5);		\
+	__sc_8 = (unsigned long) (arg6)
 
 #define __sc_asm_input_0 "0" (__sc_0)
 #define __sc_asm_input_1 __sc_asm_input_0, "1" (__sc_3)
@@ -333,6 +339,7 @@
 #define __sc_asm_input_3 __sc_asm_input_2, "3" (__sc_5)
 #define __sc_asm_input_4 __sc_asm_input_3, "4" (__sc_6)
 #define __sc_asm_input_5 __sc_asm_input_4, "5" (__sc_7)
+#define __sc_asm_input_6 __sc_asm_input_5, "6" (__sc_8)
 
 #define _syscall0(type,name)						\
 type name(void)								\
@@ -369,6 +376,11 @@ type name(type1 arg1, type2 arg2, type3 
 {									\
 	__syscall_nr(5, type, name, arg1, arg2, arg3, arg4, arg5);	\
 }
+#define _syscall6(type,name,type1,arg1,type2,arg2,type3,arg3,type4,arg4,type5,arg5,type6,arg6) \
+type name(type1 arg1, type2 arg2, type3 arg3, type4 arg4, type5 arg5, type6 arg6)	\
+{									\
+	__syscall_nr(6, type, name, arg1, arg2, arg3, arg4, arg5, arg6);	\
+}
 
 #ifdef __KERNEL_SYSCALLS__
 
diff -purN linux-2.5/include/asm-ppc64/vio.h linuxppc64-2.5/include/asm-ppc64/vio.h
--- linux-2.5/include/asm-ppc64/vio.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/vio.h	2003-12-16 21:22:18.000000000 +0000
@@ -0,0 +1,130 @@
+/*
+ * IBM PowerPC Virtual I/O Infrastructure Support.
+ *
+ *    Copyright (c) 2003 IBM Corp.
+ *     Dave Engebretsen engebret@us.ibm.com
+ *     Santiago Leon santil@us.ibm.com
+ *
+ *      This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+#ifndef _VIO_H
+#define _VIO_H
+
+#include <linux/init.h>
+#include <linux/errno.h>
+#include <asm/hvcall.h>
+#include <asm/prom.h>
+#include <asm/scatterlist.h>
+/* 
+ * Architecture-specific constants for drivers to
+ * extract attributes of the device using vio_get_attribute()
+*/
+#define VETH_MAC_ADDR "local-mac-address"
+#define VETH_MCAST_FILTER_SIZE "ibm,mac-address-filters"
+
+/* End architecture-specific constants */
+
+#define h_vio_signal(ua, mode) \
+  plpar_hcall_norets(H_VIO_SIGNAL, ua, mode)
+
+#define VIO_IRQ_DISABLE		0UL
+#define VIO_IRQ_ENABLE		1UL
+
+struct vio_dev;
+struct vio_driver;
+struct vio_device_id;
+struct TceTable;
+
+int vio_register_driver(struct vio_driver *drv);
+int vio_unregister_driver(struct vio_driver *drv);
+const struct vio_device_id * vio_match_device(const struct vio_device_id *ids, 
+						const struct vio_dev *dev);
+struct vio_dev * __devinit vio_register_device(struct device_node *node_vdev);
+int __devinit vio_unregister_device(struct vio_dev *dev);
+const void * vio_get_attribute(struct vio_dev *vdev, void* which, int* length);
+int vio_get_irq(struct vio_dev *dev);
+struct TceTable * vio_build_tce_table(struct vio_dev *dev);
+int vio_enable_interrupts(struct vio_dev *dev);
+int vio_disable_interrupts(struct vio_dev *dev);
+
+dma_addr_t vio_map_single(struct vio_dev *dev, void *vaddr, 
+			  size_t size, int direction);
+void vio_unmap_single(struct vio_dev *dev, dma_addr_t dma_handle, 
+		      size_t size, int direction);
+int vio_map_sg(struct vio_dev *vdev, struct scatterlist *sglist, 
+	       int nelems, int direction);
+void vio_unmap_sg(struct vio_dev *vdev, struct scatterlist *sglist, 
+		  int nelems, int direction);
+void *vio_alloc_consistent(struct vio_dev *dev, size_t size, 
+			   dma_addr_t *dma_handle);
+void vio_free_consistent(struct vio_dev *dev, size_t size, void *vaddr, 
+			 dma_addr_t dma_handle);
+
+struct vio_device_id {
+	char *type;
+	char *compat;
+/* I don't think we need this
+	unsigned long driver_data;	*/ /* Data private to the driver */
+};
+
+struct vio_driver {
+	struct list_head node;
+	char *name;
+	const struct vio_device_id *id_table;	/* NULL if wants all devices */
+	int  (*probe)  (struct vio_dev *dev, const struct vio_device_id *id);	/* New device inserted */
+	void (*remove) (struct vio_dev *dev);	/* Device removed (NULL if not a hot-plug capable driver) */
+	unsigned long driver_data;
+};
+
+struct vio_bus;
+/*
+ * The vio_dev structure is used to describe virtual I/O devices.
+ */
+struct vio_dev {
+	struct list_head devices_list;   /* node in list of all vio devices */
+	struct device_node *archdata;    /* Open Firmware node */
+	struct vio_bus *bus;            /* bus this device is on */
+	struct vio_driver *driver;      /* owning driver */
+	void *driver_data;              /* data private to the driver */
+	unsigned long unit_address;	
+
+	struct TceTable *tce_table; /* vio_map_* uses this */
+	unsigned int irq;
+	struct proc_dir_entry *procent; /* device entry in /proc/bus/vio */
+};
+
+struct vio_bus {
+	struct list_head devices;       /* list of virtual devices */
+};
+
+
+static inline int vio_module_init(struct vio_driver *drv)
+{
+        int rc = vio_register_driver (drv);
+
+        if (rc > 0)
+                return 0;
+
+        /* iff CONFIG_HOTPLUG and built into kernel, we should
+         * leave the driver around for future hotplug events.
+         * For the module case, a hotplug daemon of some sort
+         * should load a module in response to an insert event. */
+#if defined(CONFIG_HOTPLUG) && !defined(MODULE)
+        if (rc == 0)
+                return 0;
+#else
+        if (rc == 0)
+                rc = -ENODEV;
+#endif
+
+        /* if we get here, we need to clean up vio driver instance
+         * and return some sort of error */
+
+        return rc;
+}
+
+#endif /* _PHYP_H */
diff -purN linux-2.5/include/asm-ppc64/xics.h linuxppc64-2.5/include/asm-ppc64/xics.h
--- linux-2.5/include/asm-ppc64/xics.h	2003-03-28 06:31:06.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/xics.h	2003-09-12 19:50:40.000000000 +0000
@@ -15,6 +15,7 @@
 #include <linux/cache.h>
 
 void xics_init_IRQ(void);
+void xics_init_irq_desc(irq_desc_t *);
 int xics_get_irq(struct pt_regs *);
 void xics_setup_cpu(void);
 void xics_cause_IPI(int cpu);
diff -purN linux-2.5/include/linux/compat.h linuxppc64-2.5/include/linux/compat.h
--- linux-2.5/include/linux/compat.h	2003-10-05 08:07:51.000000000 +0000
+++ linuxppc64-2.5/include/linux/compat.h	2003-11-21 05:10:27.000000000 +0000
@@ -97,5 +97,10 @@ struct compat_dirent {
 	char		d_name[256];
 };
 
+typedef union compat_sigval {
+	compat_int_t	sival_int;
+	compat_uptr_t	sival_ptr;
+} compat_sigval_t;
+
 #endif /* CONFIG_COMPAT */
 #endif /* _LINUX_COMPAT_H */
diff -purN linux-2.5/include/linux/cpumask.h linuxppc64-2.5/include/linux/cpumask.h
--- linux-2.5/include/linux/cpumask.h	2003-08-19 02:46:23.000000000 +0000
+++ linuxppc64-2.5/include/linux/cpumask.h	2003-12-03 04:04:55.000000000 +0000
@@ -68,4 +68,20 @@ static inline int next_online_cpu(int cp
 		cpu < NR_CPUS;						\
 		cpu = next_online_cpu(cpu,map))
 
+static inline int format_cpumask(char *buf, cpumask_t cpus)
+{
+	int k, len = 0;
+
+	for (k = sizeof(cpumask_t)/sizeof(long) - 1; k >= 0; --k) {
+		int m;
+		cpumask_t tmp;
+
+		cpus_shift_right(tmp, cpus, BITS_PER_LONG*k);
+		m = sprintf(buf, "%0*lx", (int)(2*sizeof(long)), cpus_coerce(tmp));
+		len += m;
+		buf += m;
+	}
+	return len;
+}
+
 #endif /* __LINUX_CPUMASK_H */
diff -purN linux-2.5/include/linux/major.h linuxppc64-2.5/include/linux/major.h
--- linux-2.5/include/linux/major.h	2003-09-24 06:09:46.000000000 +0000
+++ linuxppc64-2.5/include/linux/major.h	2003-12-17 06:02:16.000000000 +0000
@@ -126,6 +126,9 @@
 #define COMPAQ_CISS_MAJOR6      110
 #define COMPAQ_CISS_MAJOR7      111
 
+#define VIODASD_MAJOR		112
+#define VIOCD_MAJOR		113
+
 #define ATARAID_MAJOR		114
 
 #define SCSI_DISK8_MAJOR	128
diff -purN linux-2.5/include/linux/proc_fs.h linuxppc64-2.5/include/linux/proc_fs.h
--- linux-2.5/include/linux/proc_fs.h	2003-09-13 00:22:02.000000000 +0000
+++ linuxppc64-2.5/include/linux/proc_fs.h	2003-11-18 09:08:41.000000000 +0000
@@ -131,7 +131,9 @@ extern void proc_tty_unregister_driver(s
 /*
  * proc_devtree.c
  */
+struct device_node;
 extern void proc_device_tree_init(void);
+extern void proc_device_tree_add_node(struct device_node *, struct proc_dir_entry *);
 
 /*
  * proc_rtas.c
