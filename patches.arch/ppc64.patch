diff -purN linux-2.5/Documentation/powerpc/00-INDEX linuxppc64-2.5/Documentation/powerpc/00-INDEX
--- linux-2.5/Documentation/powerpc/00-INDEX	2002-02-05 17:40:37.000000000 +0000
+++ linuxppc64-2.5/Documentation/powerpc/00-INDEX	2004-03-16 22:03:08.000000000 +0000
@@ -5,6 +5,9 @@ please mail me.
 
 00-INDEX
 	- this file
+hvcs.txt
+	 Hypervisor Virtual Console Server device driver installation guide
+	 for Power5 ppc64 systems.
 ppc_htab.txt
 	- info about the Linux/PPC /proc/ppc_htab entry
 smp.txt
diff -purN linux-2.5/Documentation/powerpc/hvcs.txt linuxppc64-2.5/Documentation/powerpc/hvcs.txt
--- linux-2.5/Documentation/powerpc/hvcs.txt	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/Documentation/powerpc/hvcs.txt	2004-03-16 22:03:08.000000000 +0000
@@ -0,0 +1,380 @@
+===========================================================================
+				    HVCS
+	IBM "Hypervisor Virtual Console Server" Installation Guide
+			    for Linux Kernel 2.6.4+
+		      Copyright (C) 2004 IBM Corporation
+
+===========================================================================
+NOTE:Eight space tabs are the optimum editor setting for reading this file.
+===========================================================================
+
+		 Author(s) :  Ryan S. Arnold <rsa@us.ibm.com>
+			Date Created: March, 02, 2004
+
+---------------------------------------------------------------------------
+Table of contents:
+
+	1.  Driver Introduction:
+	2.  System Requirements
+	3.  Build Options:
+		3.1  Built-in:
+		3.2  Module:
+	4.  Installation:
+	5.  Connection:
+	6.  Configuration:
+	7.  Questions & Answers:
+	8.  Reporting Bugs:
+
+---------------------------------------------------------------------------
+1. Driver Introduction:
+
+This is the device driver for the IBM Hypervisor Virtual Console Server,
+"hvcs".  The IBM hvcs provides a tty driver interface to allow Linux user
+space applications access to the system consoles of logically partitioned
+operating systems (Linux and AIX) running on the same partitioned Power5
+ppc64 system.  Physical hardware consoles per partition are not practical
+on this hardware so system consoles are accessed by this driver using
+firmware interfaces to virtual terminal devices.
+
+---------------------------------------------------------------------------
+2. System Requirements:
+
+This device driver was written using 2.6.4 Linux kernel APIs and will only
+build and run on kernels of this version or later.
+
+This driver was written to operate solely on IBM Power5 ppc64 hardware
+though some care was taken to abstract the architecture dependent hcalls
+from the driver code.
+
+Sysfs must be mounted on the system so that the user can determine which
+major and minor numbers are associated with each vty-server.  Directions
+for sysfs mounting are outside the scope of this document.
+
+---------------------------------------------------------------------------
+3. Build Options:
+
+The hvcs driver registers itself as a tty driver.  The tty layer
+dynamically allocates a block of major and minor numbers in a quantity
+requested by the registering driver.  The hvcs driver asks the tty layer
+for 64 of these major/minor numbers by default to use for hvcs device node
+entries.
+
+If the default number of device entries is adequate then this driver can be
+built into the kernel.  If not, the default can be over-ridden by inserting
+the driver as a module with insmod parameters.
+
+---------------------------------------------------------------------------
+3.1 Built-in:
+
+The following menuconfig example demonstrates selecting to build this
+driver into the kernel.
+
+	Device Drivers  --->
+		Character devices  --->
+			<*> IBM Hypervisor Virtual Console Server Support
+
+Begin the kernel make process.
+
+---------------------------------------------------------------------------
+3.2 Module:
+
+The following menuconfig example demonstrates selecting to build this
+driver as a kernel module.
+
+	Device Drivers  --->
+		Character devices  --->
+			<M> IBM Hypervisor Virtual Console Server Support
+
+The make process will build a kernel module named:
+
+	hvcs.ko
+
+To insert the module with the default allocation execute the
+following command:
+
+	insmod hvcs.ko
+
+To override the default use an insmod parameter as follows (requesting 4
+tty devices as an example):
+
+	insmod hvcs.ko hvcs_parm_num_devs=4
+
+There is a maximum number of dev entries that can be specified on insmod.
+We think that 1024 is currently a decent maximum number of server adapters
+to allow.  This can always be changed by modifying the constant in the
+source file before building.
+
+NOTE: The length of time it takes to insmod the driver seems to be related
+to the number of tty interfaces the registering driver requests.
+
+In order to remove the driver module execute the following command:
+
+	rmmod hvcs.ko
+
+---------------------------------------------------------------------------
+4. Installation:
+
+The tty layer creates sysfs entries which contain the major and minor
+numbers allocated for the hvcs driver.  The following snippet of "tree"
+output of the sysfs directory shows where these numbers are presented:
+
+	sys/
+	|-- *other sysfs base dirs*
+	|
+	|-- class
+	|   |-- *other classes of devices*
+	|   |
+	|   `-- tty
+	|       |-- *other tty devices*
+	|       |
+	|       |-- hvcs0
+	|       |   `-- dev
+	|       |-- hvcs1
+	|       |   `-- dev
+	|       |-- hvcs2
+	|       |   `-- dev
+	|       |-- hvcs3
+	|       |   `-- dev
+	|       |
+	|       |-- *other tty devices*
+	|
+	|-- *other sysfs base dirs*
+
+For the above examples the following output is a result of cat'ing the
+"dev" entry in the hvcs directory:
+
+	Pow5:/sys/class/tty/hvcs0/ # cat dev
+	254:0
+
+	Pow5:/sys/class/tty/hvcs1/ # cat dev
+	254:1
+
+	Pow5:/sys/class/tty/hvcs2/ # cat dev
+	254:2
+
+	Pow5:/sys/class/tty/hvcs3/ # cat dev
+	254:3
+
+The output from reading the "dev" attribute is the char device major and
+minor numbers that the tty layer has allocated for this driver's use.  Most
+systems running hvcs will already have the device entries created or udev
+will do it automatically.
+
+Given the example output above, to manually create a /dev/hvcs* node entry
+mknod can be used as follows:
+
+	mknod /dev/hvcs0 c 254 0
+	mknod /dev/hvcs1 c 254 1
+	mknod /dev/hvcs2 c 254 2
+	mknod /dev/hvcs3 c 254 3
+
+Using mknod to manually create the device entries makes these device nodes
+persistent.  Once created they will exist prior to the driver insmod.
+
+Attempting to connect an application to /dev/hvcs* prior to insertion of
+the hvcs module will result in an error message similar to the following:
+
+	"/dev/hvcs*: No such device".
+
+NOTE: Just because there is a device node present doesn't mean that there
+is a vty-server device configured for that node.
+
+---------------------------------------------------------------------------
+5. Connection
+
+Since this driver implements a tty interface a user can interact with the
+device node entries in a standard tty-interactive method.  The original
+purpose of this driver, however, is to provide real time console
+interaction with a Linux partition's console.  This is easily done using
+"minicom".
+
+As a precautionary measure an hvcs user should always "exit" from their
+session before disconnecting a program such as minicom from the device.  If
+this is not done, the next user to connect to the console will continue
+using the previous user's logged in session.
+
+---------------------------------------------------------------------------
+6. Configuration
+
+Each vty-server has a sysfs entry in the sys/devices/vio directory, which
+is symlinked in several other sysfs tree directories, notably under the
+hvcs driver entry, which looks like the following example:
+
+	Pow5:/sys/bus/vio/drivers/hvcs # ls
+	.  ..  30000003  30000004  rescan
+
+By design, firmware notifies the hvcs driver of vty-server lifetimes and
+partner vty removals but not the addition of partner vtys.  Since an HMC
+Super Admin can add partner info dynamically we have provided the hvcs
+driver sysfs directory with the "rescan" update attribute which will query
+firmware and update the partner info for all the vty-servers that this
+driver manages.  Writing a '1' to the attribute triggers the update.  An
+explicit example follows:
+
+	Pow5:/sys/bus/vio/drivers/hvcs # echo 1 > rescan
+
+Reading the attribute will indicate a state of '1' or '0'.  A one indicates
+that an update is in process.  A zero indicates that an update has
+completed or was never executed.
+
+Vty-server entries in this directory are a 32 bit partition unique unit
+address that is created by firmware.  An example vty-server sysfs entry
+looks like the following:
+
+	Pow5:/sys/bus/vio/drivers/hvcs/30000004 # ls
+	.   current_vty   name          partner_vtys
+	..  detach_state  partner_clcs
+
+Each entry is provided, by default with a "name" attribute.  Reading the
+"name" attribute will reveal the device type as shown in the following
+example:
+
+	Pow5:/sys/bus/vio/drivers/hvcs/30000003 # cat name
+	vty-server
+
+Each vty-server sysfs dir is provided with two read-only attributes that
+provide lists of easily parsed partner vty data: "partner_vtys" and
+"partner_clcs".
+
+	Pow5:/sys/bus/vio/drivers/hvcs/30000004 # cat partner_vtys
+	30000000
+	30000001
+	30000002
+	30000000
+	30000000
+
+	Pow5:/sys/bus/vio/drivers/hvcs/30000004 # cat partner_clcs
+	U5112.428.103048A-V3-C0
+	U5112.428.103048A-V3-C2
+	U5112.428.103048A-V3-C3
+	U5112.428.103048A-V4-C0
+	U5112.428.103048A-V5-C0
+
+Reading partner_vtys returns a list of partner vtys.  Vty unit address
+numbering is only per-partition-unique so entries will frequently repeat.
+
+Reading partner_clcs returns a list of "converged location codes" which are
+composed of a system serial number followed by "-V*", where the '*' is the
+target partition number, and "-C*", where the '*' is the slot of the
+adapter.  The first vty partner corresponds to the first clc item, the
+second vty partner to the second clc item, etc.
+
+A vty-server can only be connected to a single vty at a time.  The entry,
+"current_vty" prints the clc of the currently selected partner vty when
+read.
+
+The current_vty can be changed by writing a valid partner clc to the entry
+as in the following example:
+
+	Pow5:/sys/bus/vio/drivers/hvcs/30000004 # echo U5112.428.10304
+	8A-V4-C0 > current_vty
+
+Changing the current_vty when a vty-server is already connected to a vty
+does not affect the current connection.  The change takes effect when the
+currently open connection is freed.
+
+---------------------------------------------------------------------------
+7. Questions & Answers:
+===========================================================================
+Q: What are the security concerns involving hvcs?
+
+A: There are three main security concerns:
+
+	1. The creator of the /dev/hvcs* nodes has the ability to restrict
+	the access of the device entries to certain users or groups.  It
+	may be best to create a special hvcs group privilege for providing
+	access to system consoles.
+
+	2. To provide network security when grabbing the console it is
+	suggested that the user connect to the console hosting partition
+	using a secure method, such as SSH or sit at a hardware console.
+
+	3. Make sure to exit the user session when done with a console.
+	Minicom can be configured to do it automatically by adding "exit^M"
+	as a reset string.
+
+---------------------------------------------------------------------------
+Q: How do I multiplex a console that I grab through hvcs so that other
+people can see it:
+
+A: Use "screen" and setup a session on your machine with the console group
+privileges.  Then inside of the screen session execute minicom to connect
+to the target console.
+
+---------------------------------------------------------------------------
+Q: When I use screen to multiplex the console how do I get out of minicom?
+
+A: Screen interprets Ctrl-A-a as an indication to allow a Ctrl-A pass
+through, at which point the user should hit 'X' to terminate minicom.
+
+---------------------------------------------------------------------------
+Q: Why are the colors all messed up?
+Q: Why are the control characters acting strange?
+Q: Why is the console output all strange and unintelligible?
+
+A: The user should be aware that minicom can either do vt100 or ANSI
+terminal emulation, (which can be specified using "Terminal settings..T"
+under the "Minicom Command Summary").  Once logged in the user will want to
+set the $TERM env variable of the console to "vt100" (recommended) or
+"ANSI".
+
+Just because you logged into the console with minicom (native vt100
+$TERM) type, doesn't mean someone else didn't log into the console with the
+HMC console (vt320) before you and leave the session logged in.  The best
+thing to do is to "export TERM=vt100" when you get the console.
+Additionally make sure to "exit" the console before you disconnect minicom
+(which can be set up to be done automatically).  This will ensure that the
+next user gets their own TERM type set when they login.
+
+---------------------------------------------------------------------------
+Q: When I try to use minicom to connect to a device I get "minicom: cannot
+open /dev/hvcs*: No such device".  What is happening?
+
+A: You probably haven't inserted the hvcs.ko module yet or there is not a
+corresponding vty-server device that maps to the /dev/hvcs* entry.
+
+---------------------------------------------------------------------------
+Q: When I try to use minicom to connect to a device I get "Cannot create
+lockfile. Sorry."  What is happening?
+
+A: The user profile you are using to execute minicom probably doesn't have
+permissions to use the /dev/hvcs* device.
+
+---------------------------------------------------------------------------
+Q: No matter what, I keep getting a rejection from minicom that the console
+is busy.  What is happening?
+
+A: Some other Power5 console mechanism has a connection to the vty and
+isn't giving it up.  You can try to force disconnect the consoles from the
+HMC by right clicking on the partition and then selecting "close terminal".
+Otherwise you have to hunt down the people who have console authority.  It
+is possible that you already have the console open using minicom and just
+forgot about it.  Please review the console options for Power5 systems to
+determine the many ways a system console can be held.
+
+---------------------------------------------------------------------------
+Q: If I already have one linux partition installed can I use hvcs on said
+partition to provide the console for the install of a second linux
+partition?
+
+A: As far as we know, yes.
+
+---------------------------------------------------------------------------
+Q: Can I connect to more than one partition's console at a time using this
+driver?
+
+A: Yes.  Of course this means that there must be more than one vty-server
+configured for this partition and it must point to a disconnected vty.
+
+---------------------------------------------------------------------------
+8. Reporting Bugs:
+
+The proper channel for reporting bugs is either through the Linux OS
+distribution company that provided your OS or by posting issues to the
+ppc64 development mailing list at:
+
+linuxppc64-dev@lists.linuxppc.org
+
+This request is to provide a documented and searchable public exchange
+of the problems and solutions surrounding this driver for the benefit of
+all users.
diff -purN linux-2.5/MAINTAINERS linuxppc64-2.5/MAINTAINERS
--- linux-2.5/MAINTAINERS	2004-03-16 18:13:59.000000000 +0000
+++ linuxppc64-2.5/MAINTAINERS	2004-03-17 01:53:45.000000000 +0000
@@ -970,6 +970,11 @@ M:	langa2@kph.uni-mainz.de
 W:	http://www.uni-mainz.de/~langm000/linux.html
 S:	Maintained
 
+IBM Power Linux RAID adapter
+P:	Brian King
+M:	brking@us.ibm.com
+S:	Supported
+
 IBM ServeRAID RAID DRIVER
 P:	Jack Hammer
 P:	Dave Jeffery
diff -purN linux-2.5/arch/ppc64/Kconfig linuxppc64-2.5/arch/ppc64/Kconfig
--- linux-2.5/arch/ppc64/Kconfig	2004-03-18 02:43:04.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/Kconfig	2004-03-20 10:56:41.000000000 +0000
@@ -89,6 +89,16 @@ config PPC_PMAC
 	bool "Apple PowerMac G5 support"
 	select ADB_PMU
 
+config PPC_SPLPAR
+	depends on PPC_PSERIES
+	bool "Support for shared-processor logical partitions"
+	default n
+	help
+	  Enabling this option will make the kernel run more efficiently
+	  on logically-partitioned pSeries systems which use shared
+	  processors, that is, which share physical processors between
+	  two or more partitions.
+
 config PMAC_DART
 	bool "Enable DART/IOMMU on PowerMac (allow >2G of RAM)"
 	depends on PPC_PMAC
@@ -203,11 +213,27 @@ config SCANLOG
 	depends on PPC_RTAS
 
 config LPARCFG
-	bool "LPAR Configuration Data"
+	tristate "LPAR Configuration Data"
 	help
 	Provide system capacity information via human readable 
 	<key word>=<value> pairs through a /proc/ppc64/lparcfg interface.
 
+config VIOCFG
+	tristate "Virtual I/O Configuration Data"
+	depends on PPC_PSERIES && SCSI_IBMVSCSI
+	help
+	Provide configuration information from a virtual I/O host.
+	Data is in the form
+	<key word>=<value> pairs in /proc/vioconfig.
+
+config PPC_VPURR
+	bool "Virtual Processor Utilization Values"
+	default n
+	depends on PPC_PSERIES
+	help
+	Provide framework to collect virtual processor utilization
+	periodically. If unsure say NO.
+
 endmenu
 
 
@@ -248,6 +274,14 @@ source "fs/Kconfig.binfmt"
 
 source "drivers/pci/Kconfig"
 
+config HOTPLUG_CPU
+	bool "Support for hot-pluggable CPUs (EXPERIMENTAL)"
+	depends on SMP && HOTPLUG && EXPERIMENTAL
+	---help---
+	  Say Y here to experiment with turning CPUs off and on.
+
+	  Say N if you are unsure.
+
 source "drivers/pcmcia/Kconfig"
 
 source "drivers/pci/hotplug/Kconfig"
@@ -363,17 +397,46 @@ config DEBUGGER
 	  Include in-kernel hooks for kernel debuggers. Unless you are
 	  intending to debug the kernel, say N here.
 
-config XMON
-	bool "Include xmon kernel debugger"
+choice
+	optional
 	depends on DEBUGGER
+	prompt "Kernel Debugger"
+
+config XMON
+	bool "XMON"
 	help
 	  Include in-kernel hooks for the xmon kernel monitor/debugger.
 	  Unless you are intending to debug the kernel, say N here.
 
+config KDB
+	bool "KDB"
+	help
+	  Include in-kernel hooks for the kdb kernel monitor/debugger.
+	  Unless you are intending to debug the kernel, say N here.
+
+config KDB_MODULES
+	bool "Enable additional KDB modules"
+	depends on KDB
+	help
+	  Include additional KDB support for viewing buffer heads,
+	  pages, inodes, dentrys, vm areas and scsi devices and
+	  scsi commands.
+
+endchoice
+
+
 config XMON_DEFAULT
 	bool "Enable xmon by default"
 	depends on XMON
 
+config KDB_OFF
+	bool "Turn KDB off as default."
+	depends on KDB
+
+	help
+ 	   KDB will remain built into the kernel, but will be turned off. 
+	   "cat 1 > /proc/sys/kernel/kdb" to turn it on. 
+
 config PPCDBG
 	bool "Include PPCDBG realtime debugging"
 	depends on DEBUG_KERNEL
@@ -386,6 +449,13 @@ config DEBUG_INFO
 	  debugging info resulting in a larger kernel image.
 	  Say Y here only if you plan to use gdb to debug the kernel.
 	  If you don't debug the kernel, you can say N.
+
+config DEBUG_SPINLOCK_SLEEP
+	bool "Sleep-inside-spinlock checking"
+	depends on DEBUG_KERNEL
+	help
+	  If you say Y here, various routines which may sleep will become very
+	  noisy if they are called with a spinlock held.
 	  
 endmenu
 
diff -purN linux-2.5/arch/ppc64/Makefile linuxppc64-2.5/arch/ppc64/Makefile
--- linux-2.5/arch/ppc64/Makefile	2004-03-16 11:30:37.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/Makefile	2004-03-17 00:46:40.000000000 +0000
@@ -43,6 +43,11 @@ libs-y				+= arch/ppc64/lib/
 core-y				+= arch/ppc64/kernel/
 core-y				+= arch/ppc64/mm/
 core-$(CONFIG_XMON)		+= arch/ppc64/xmon/
+ifeq ($(CONFIG_KDB),y)
+  # Use ifeq for now because kdb subdirs are not in bk yet
+  # Otherwise make mrproper will die because it also cleans core-n
+  core-y			+= arch/ppc64/kdb/
+endif
 drivers-$(CONFIG_OPROFILE)	+= arch/ppc64/oprofile/
 
 boot := arch/ppc64/boot
diff -purN linux-2.5/arch/ppc64/configs/iSeries_defconfig linuxppc64-2.5/arch/ppc64/configs/iSeries_defconfig
--- linux-2.5/arch/ppc64/configs/iSeries_defconfig	2004-03-16 11:30:38.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/configs/iSeries_defconfig	2004-03-18 23:01:08.000000000 +0000
@@ -90,7 +90,7 @@ CONFIG_PCI_NAMES=y
 #
 # Generic Driver Options
 #
-CONFIG_FW_LOADER=m
+CONFIG_FW_LOADER=y
 # CONFIG_DEBUG_DRIVER is not set
 
 #
@@ -178,8 +178,12 @@ CONFIG_SCSI_FC_ATTRS=y
 # CONFIG_SCSI_FUTURE_DOMAIN is not set
 # CONFIG_SCSI_GDTH is not set
 # CONFIG_SCSI_IPS is not set
+# CONFIG_SCSI_IBMVSCSI is not set
 # CONFIG_SCSI_INIA100 is not set
 # CONFIG_SCSI_SYM53C8XX_2 is not set
+CONFIG_SCSI_IPR=y
+# CONFIG_SCSI_IPR_TRACE is not set
+# CONFIG_SCSI_IPR_DUMP is not set
 # CONFIG_SCSI_QLOGIC_ISP is not set
 # CONFIG_SCSI_QLOGIC_FC is not set
 # CONFIG_SCSI_QLOGIC_1280 is not set
@@ -417,7 +421,7 @@ CONFIG_E1000=y
 #
 CONFIG_IXGB=m
 # CONFIG_IXGB_NAPI is not set
-# CONFIG_VETH is not set
+# CONFIG_ISERIES_VETH is not set
 # CONFIG_FDDI is not set
 # CONFIG_HIPPI is not set
 CONFIG_PPP=m
@@ -758,7 +762,7 @@ CONFIG_NLS_DEFAULT="iso8859-1"
 CONFIG_VIOCONS=y
 CONFIG_VIODASD=y
 CONFIG_VIOCD=y
-# CONFIG_VIOTAPE is not set
+CONFIG_VIOTAPE=y
 CONFIG_VIOPATH=y
 
 #
@@ -778,6 +782,7 @@ CONFIG_MAGIC_SYSRQ=y
 # CONFIG_DEBUGGER is not set
 # CONFIG_PPCDBG is not set
 # CONFIG_DEBUG_INFO is not set
+CONFIG_DEBUG_SPINLOCK_SLEEP=y
 
 #
 # Security options
diff -purN linux-2.5/arch/ppc64/configs/pSeries_defconfig linuxppc64-2.5/arch/ppc64/configs/pSeries_defconfig
--- linux-2.5/arch/ppc64/configs/pSeries_defconfig	2004-03-17 12:02:30.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/configs/pSeries_defconfig	2004-03-18 08:10:24.000000000 +0000
@@ -107,7 +107,7 @@ CONFIG_PROC_DEVICETREE=y
 #
 # Generic Driver Options
 #
-CONFIG_FW_LOADER=m
+CONFIG_FW_LOADER=y
 # CONFIG_DEBUG_DRIVER is not set
 
 #
@@ -250,12 +250,17 @@ CONFIG_SCSI_FC_ATTRS=y
 # CONFIG_SCSI_FUTURE_DOMAIN is not set
 # CONFIG_SCSI_GDTH is not set
 # CONFIG_SCSI_IPS is not set
+CONFIG_SCSI_IBMVSCSI=m
+CONFIG_SCSI_IBMVSCSIS=m
 # CONFIG_SCSI_INIA100 is not set
 CONFIG_SCSI_SYM53C8XX_2=y
 CONFIG_SCSI_SYM53C8XX_DMA_ADDRESSING_MODE=0
 CONFIG_SCSI_SYM53C8XX_DEFAULT_TAGS=16
 CONFIG_SCSI_SYM53C8XX_MAX_TAGS=64
 # CONFIG_SCSI_SYM53C8XX_IOMAPPED is not set
+CONFIG_SCSI_IPR=y
+# CONFIG_SCSI_IPR_TRACE is not set
+# CONFIG_SCSI_IPR_DUMP is not set
 # CONFIG_SCSI_QLOGIC_ISP is not set
 # CONFIG_SCSI_QLOGIC_FC is not set
 # CONFIG_SCSI_QLOGIC_1280 is not set
@@ -495,9 +500,9 @@ CONFIG_TIGON3=y
 #
 CONFIG_IXGB=m
 # CONFIG_IXGB_NAPI is not set
+CONFIG_IBMVETH=m
 # CONFIG_FDDI is not set
 # CONFIG_HIPPI is not set
-CONFIG_IBMVETH=m
 CONFIG_PPP=m
 # CONFIG_PPP_MULTILINK is not set
 # CONFIG_PPP_FILTER is not set
@@ -630,6 +635,7 @@ CONFIG_UNIX98_PTYS=y
 CONFIG_LEGACY_PTYS=y
 CONFIG_LEGACY_PTY_COUNT=256
 CONFIG_HVC_CONSOLE=y
+CONFIG_HVCS=m
 
 #
 # Mice
@@ -658,7 +664,7 @@ CONFIG_HVC_CONSOLE=y
 # CONFIG_AGP is not set
 # CONFIG_DRM is not set
 CONFIG_RAW_DRIVER=y
-CONFIG_MAX_RAW_DEVS=256
+CONFIG_MAX_RAW_DEVS=2048
 
 #
 # I2C support
@@ -1058,9 +1064,11 @@ CONFIG_DEBUG_STACK_USAGE=y
 CONFIG_MAGIC_SYSRQ=y
 CONFIG_DEBUGGER=y
 CONFIG_XMON=y
+# CONFIG_KDB is not set
 CONFIG_XMON_DEFAULT=y
 # CONFIG_PPCDBG is not set
 # CONFIG_DEBUG_INFO is not set
+CONFIG_DEBUG_SPINLOCK_SLEEP=y
 
 #
 # Security options
diff -purN linux-2.5/arch/ppc64/defconfig linuxppc64-2.5/arch/ppc64/defconfig
--- linux-2.5/arch/ppc64/defconfig	2004-03-17 12:02:30.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/defconfig	2004-03-18 08:10:24.000000000 +0000
@@ -107,7 +107,7 @@ CONFIG_PROC_DEVICETREE=y
 #
 # Generic Driver Options
 #
-CONFIG_FW_LOADER=m
+CONFIG_FW_LOADER=y
 # CONFIG_DEBUG_DRIVER is not set
 
 #
@@ -250,12 +250,17 @@ CONFIG_SCSI_FC_ATTRS=y
 # CONFIG_SCSI_FUTURE_DOMAIN is not set
 # CONFIG_SCSI_GDTH is not set
 # CONFIG_SCSI_IPS is not set
+CONFIG_SCSI_IBMVSCSI=m
+CONFIG_SCSI_IBMVSCSIS=m
 # CONFIG_SCSI_INIA100 is not set
 CONFIG_SCSI_SYM53C8XX_2=y
 CONFIG_SCSI_SYM53C8XX_DMA_ADDRESSING_MODE=0
 CONFIG_SCSI_SYM53C8XX_DEFAULT_TAGS=16
 CONFIG_SCSI_SYM53C8XX_MAX_TAGS=64
 # CONFIG_SCSI_SYM53C8XX_IOMAPPED is not set
+CONFIG_SCSI_IPR=y
+# CONFIG_SCSI_IPR_TRACE is not set
+# CONFIG_SCSI_IPR_DUMP is not set
 # CONFIG_SCSI_QLOGIC_ISP is not set
 # CONFIG_SCSI_QLOGIC_FC is not set
 # CONFIG_SCSI_QLOGIC_1280 is not set
@@ -495,9 +500,9 @@ CONFIG_TIGON3=y
 #
 CONFIG_IXGB=m
 # CONFIG_IXGB_NAPI is not set
+CONFIG_IBMVETH=m
 # CONFIG_FDDI is not set
 # CONFIG_HIPPI is not set
-CONFIG_IBMVETH=m
 CONFIG_PPP=m
 # CONFIG_PPP_MULTILINK is not set
 # CONFIG_PPP_FILTER is not set
@@ -630,6 +635,7 @@ CONFIG_UNIX98_PTYS=y
 CONFIG_LEGACY_PTYS=y
 CONFIG_LEGACY_PTY_COUNT=256
 CONFIG_HVC_CONSOLE=y
+CONFIG_HVCS=m
 
 #
 # Mice
@@ -658,7 +664,7 @@ CONFIG_HVC_CONSOLE=y
 # CONFIG_AGP is not set
 # CONFIG_DRM is not set
 CONFIG_RAW_DRIVER=y
-CONFIG_MAX_RAW_DEVS=256
+CONFIG_MAX_RAW_DEVS=2048
 
 #
 # I2C support
@@ -1058,9 +1064,11 @@ CONFIG_DEBUG_STACK_USAGE=y
 CONFIG_MAGIC_SYSRQ=y
 CONFIG_DEBUGGER=y
 CONFIG_XMON=y
+# CONFIG_KDB is not set
 CONFIG_XMON_DEFAULT=y
 # CONFIG_PPCDBG is not set
 # CONFIG_DEBUG_INFO is not set
+CONFIG_DEBUG_SPINLOCK_SLEEP=y
 
 #
 # Security options
diff -purN linux-2.5/arch/ppc64/kernel/Makefile linuxppc64-2.5/arch/ppc64/kernel/Makefile
--- linux-2.5/arch/ppc64/kernel/Makefile	2004-03-16 11:30:38.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/Makefile	2004-03-20 10:56:41.000000000 +0000
@@ -44,6 +44,8 @@ obj-$(CONFIG_PPC_RTAS)		+= rtas-proc.o
 obj-$(CONFIG_SCANLOG)		+= scanlog.o
 obj-$(CONFIG_VIOPATH)		+= viopath.o
 obj-$(CONFIG_LPARCFG)		+= lparcfg.o
+obj-$(CONFIG_PPC_VPURR)		+= vpurr.o
+obj-$(CONFIG_VIOCFG)		+= vioconfig.o
 obj-$(CONFIG_HVC_CONSOLE)	+= hvconsole.o
 obj-$(CONFIG_BOOTX_TEXT)	+= btext.o
 
diff -purN linux-2.5/arch/ppc64/kernel/chrp_setup.c linuxppc64-2.5/arch/ppc64/kernel/chrp_setup.c
--- linux-2.5/arch/ppc64/kernel/chrp_setup.c	2004-02-27 23:02:35.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/chrp_setup.c	2004-03-01 13:40:28.000000000 +0000
@@ -142,10 +142,6 @@ chrp_setup_arch(void)
 #ifdef CONFIG_BLK_DEV_INITRD
 	/* this is fine for chrp */
 	initrd_below_start_ok = 1;
-	
-	if (initrd_start)
-		ROOT_DEV = Root_RAM0;
-	else
 #endif
 	ROOT_DEV = Root_SDA2;
 
@@ -255,7 +251,7 @@ chrp_init(unsigned long r3, unsigned lon
 	ppc_md.setup_arch     = chrp_setup_arch;
 	ppc_md.setup_residual = NULL;
 	ppc_md.get_cpuinfo    = chrp_get_cpuinfo;
-	if(naca->interrupt_controller == IC_OPEN_PIC) {
+	if (naca->interrupt_controller == IC_OPEN_PIC) {
 		ppc_md.init_IRQ       = pSeries_init_openpic; 
 		ppc_md.get_irq        = openpic_get_irq;
 	} else {
@@ -270,6 +266,7 @@ chrp_init(unsigned long r3, unsigned lon
 	ppc_md.restart        = rtas_restart;
 	ppc_md.power_off      = rtas_power_off;
 	ppc_md.halt           = rtas_halt;
+	ppc_md.panic          = rtas_os_term;
 
 	ppc_md.get_boot_time  = pSeries_get_boot_time;
 	ppc_md.get_rtc_time   = pSeries_get_rtc_time;
diff -purN linux-2.5/arch/ppc64/kernel/hvconsole.c linuxppc64-2.5/arch/ppc64/kernel/hvconsole.c
--- linux-2.5/arch/ppc64/kernel/hvconsole.c	2004-02-05 21:10:52.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/hvconsole.c	2004-03-19 17:27:30.000000000 +0000
@@ -19,16 +19,230 @@
  * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
  */
 
+/* TODO:
+ * finish DTR/CD ioctls
+ * use #defines instead of "16" "12" etc
+ * comment lack of locking
+ */
+
 #include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/list.h>
+#include <linux/time.h>
+#include <linux/ctype.h>
+#include <asm/delay.h>
 #include <asm/hvcall.h>
 #include <asm/prom.h>
 #include <asm/hvconsole.h>
+#include <asm/termios.h>
+#include <asm/uaccess.h>
+#include <asm/io.h>
+
+#define __ALIGNED__	__attribute__((__aligned__(8)))
+
+#define HVCS_LONG_INVALID	0xFFFFFFFFFFFFFFFF
+
+struct vtty_struct {
+	uint32_t vtermno;
+	int (*get_chars)(struct vtty_struct *vtty, char *buf, int count);
+	int (*put_chars)(struct vtty_struct *vtty, const char *buf, int count);
+	int (*tiocmget)(struct vtty_struct *vtty);
+	int (*tiocmset)(struct vtty_struct *vtty, uint16_t set, uint16_t clear);
+	uint16_t seqno; /* HVSI packet sequence number */
+	uint16_t mctrl;
+};
+static struct vtty_struct vttys[MAX_NR_HVC_CONSOLES];
+
+#define WAIT_LOOPS 10000
+#define WAIT_USECS 100
+
+#define HVSI_VERSION 1
+
+#define VS_DATA_PACKET_HEADER           0xff
+#define VS_CONTROL_PACKET_HEADER        0xfe
+#define VS_QUERY_PACKET_HEADER          0xfd
+#define VS_QUERY_RESPONSE_PACKET_HEADER 0xfc
+
+/* control verbs */
+#define VSV_SET_MODEM_CTL    1 /* to service processor only */
+#define VSV_MODEM_CTL_UPDATE 2 /* from service processor only */
+#define VSV_CLOSE_PROTOCOL   3
+
+/* query verbs */
+#define VSV_SEND_VERSION_NUMBER 1
+#define VSV_SEND_MODEM_CTL_STATUS 2
+
+/* yes, these masks are not consecutive. */
+#define HVSI_TSDTR 0x1
+#define HVSI_TSCD  0x20
+
+struct hvsi_header {
+	uint8_t  type;
+	uint8_t  len;
+	uint16_t seqno;
+} __attribute__((packed));
+
+struct hvsi_control {
+	uint8_t  type;
+	uint8_t  len;
+	uint16_t seqno;
+	uint16_t verb;
+	/* optional depending on verb: */
+	uint32_t word;
+	uint32_t mask;
+} __attribute__((packed));
+
+struct hvsi_query {
+	uint8_t  type;
+	uint8_t  len;
+	uint16_t seqno;
+	uint16_t verb;
+} __attribute__((packed));
+
+struct hvsi_query_response {
+	uint8_t  type;
+	uint8_t  len;
+	uint16_t seqno;
+	uint16_t verb;
+	uint16_t query_seqno;
+	union {
+		uint8_t  version;
+		uint32_t mctrl_word;
+	} u;
+} __attribute__((packed));
+
+/* ring buffer stuff: */
+struct packet_desc {
+	union {
+		struct hvsi_header hdr;
+		char pkt[256]; /* console_initcall is pre-mem_init(), so no kmalloc */
+	} data;
+	unsigned int want;
+	unsigned int got;
+};
+#define N_PACKETS 4
+static struct packet_desc ring[N_PACKETS];
+static struct packet_desc *write=ring; /* next packet to write to */
+static struct packet_desc *read=ring;  /* next packet to read from */
+
+static struct packet_desc *next_desc(struct packet_desc *cur)
+{
+	if ((cur+1) > ring + (N_PACKETS-1))
+		return ring;
+	return (cur+1);
+}
+
+static int desc_hdr_done(struct packet_desc *desc)
+{
+	if (desc->got < sizeof(struct hvsi_header))
+		return 0;
+	return 1;
+}
+
+static unsigned int desc_want(struct packet_desc *desc)
+{
+	if (desc_hdr_done(desc))
+		return desc->data.hdr.len;
+	else
+		return UINT_MAX;
+}
+
+static int desc_done(struct packet_desc *desc)
+{
+	if (!desc_hdr_done(desc) || (desc->got < desc->want))
+		return 0;
+	return 1;
+}
+
+static int desc_overflow(struct packet_desc *desc)
+{
+	int overflow = desc->got - desc->want;
+	if (desc_hdr_done(desc) && (overflow > 0))
+		return overflow;
+	return 0;
+}
+
+static void desc_clear(struct packet_desc *desc)
+{
+	desc->got = desc->want = 0;
+}
+
+/* these only work on well-formed and complete packets */
+
+static inline int hdrlen(const struct hvsi_header *pkt)
+{
+	const int lengths[] = { 4, 6, 6, 8, };
+	int index = VS_DATA_PACKET_HEADER - pkt->type;
+
+	return lengths[index];
+}
 
-int hvc_get_chars(int index, char *buf, int count)
+static inline uint8_t *payload(const struct hvsi_header *pkt)
+{
+	return (uint8_t *)pkt + hdrlen(pkt);
+}
+
+static inline int len_packet(const struct hvsi_header *pkt)
+{
+	return (int)pkt->len;
+}
+
+static inline int len_payload(const struct hvsi_header *pkt)
+{
+	return len_packet(pkt) - hdrlen(pkt);
+}
+
+static void dump_packet(struct hvsi_header *pkt)
+{
+	int i;
+	char *data = payload(pkt);
+
+	printk("type 0x%x, len %i, seqno %i:", pkt->type, pkt->len, pkt->seqno);
+
+	if (len_payload(pkt))
+		printk("\n     ");
+	for (i=0; i < len_payload(pkt); i++)
+		printk("%.2x", data[i]);
+
+	if (len_payload(pkt))
+		printk("\n     ");
+	for (i=0; i < len_payload(pkt); i++) {
+		if (isprint(data[i]))
+			printk(" %c", data[i]);
+		else
+			printk("..");
+	}
+	printk("\n");
+}
+
+#ifdef DEBUG
+static void dump_ring(void)
+{
+	int i;
+	for (i=0; i < N_PACKETS; i++) {
+		struct packet_desc *desc = &ring[i];
+		if (read == desc)
+			printk("r");
+		else
+			printk(" ");
+		if (write == desc)
+			printk("w");
+		else
+			printk(" ");
+		printk(" ");
+		printk("desc %i: want %i got %i\n", i, desc->want, desc->got);
+		printk("    ");
+		dump_packet(&desc->data.hdr);
+	}
+}
+#endif /* DEBUG */
+
+/* normal hypervisor virtual console code */
+int hvterm_get_chars(uint32_t vtermno, char *buf, int count)
 {
 	unsigned long got;
 
-	if (plpar_hcall(H_GET_TERM_CHAR, index, 0, 0, 0, &got,
+	if (plpar_hcall(H_GET_TERM_CHAR, vtermno, 0, 0, 0, &got,
 		(unsigned long *)buf, (unsigned long *)buf+1) == H_Success) {
 		/*
 		 * Work around a HV bug where it gives us a null
@@ -49,40 +263,669 @@ int hvc_get_chars(int index, char *buf, 
 	}
 	return 0;
 }
+EXPORT_SYMBOL(hvterm_get_chars);
 
-int hvc_put_chars(int index, const char *buf, int count)
+/* wrapper exists just so that hvterm_get_chars() is callable by outside
+ * drivers without a vtty_struct */
+int hvc_hvterm_get_chars(struct vtty_struct *vtty, char *buf, int count)
+{
+	return hvterm_get_chars(vtty->vtermno, buf, count);
+}
+
+int hvterm_put_chars(uint32_t vtermno, const char *buf, int count)
 {
 	unsigned long *lbuf = (unsigned long *) buf;
 	long ret;
 
-	ret = plpar_hcall_norets(H_PUT_TERM_CHAR, index, count, lbuf[0],
+	ret = plpar_hcall_norets(H_PUT_TERM_CHAR, vtermno, count, lbuf[0],
 				 lbuf[1]);
 	if (ret == H_Success)
 		return count;
 	if (ret == H_Busy)
 		return 0;
-	return -1;
+	return -EIO;
+}
+EXPORT_SYMBOL(hvterm_put_chars);
+
+/* wrapper exists just so that hvterm_put_chars() is callable by outside
+ * drivers without a vtty_struct */
+int hvc_hvterm_put_chars(struct vtty_struct *vtty, const char *buf, int count)
+{
+	return hvterm_put_chars(vtty->vtermno, buf, count);
+}
+
+/* Host Virtual Serial Interface (HVSI) code */
+
+static int hvsi_read(struct vtty_struct *vtty, char *buf, int count)
+{
+	unsigned long got;
+
+	if (plpar_hcall(H_GET_TERM_CHAR, vtty->vtermno, 0, 0, 0, &got,
+		(unsigned long *)buf, (unsigned long *)buf+1) == H_Success) {
+		return got;
+	}
+	return 0;
+}
+
+/* like memcpy, but only copy at most a single packet from the src bytestream */
+static int copy_packet(uint8_t *dest, uint8_t *src, uint8_t len)
+{
+	int copylen;
+
+	if (len == 1) {
+		/* we don't have the len header */
+		*dest = *src;
+		return 1;
+	}
+
+	/* if we have more than one packet here, only copy the first */
+	copylen = min(len_packet((struct hvsi_header *)src), (int)len);
+	memcpy(dest, src, copylen);
+	return copylen;
+}
+
+/* load up ring buffers */
+static int hvsi_load_chunk(struct vtty_struct *vtty)
+{
+	struct packet_desc *old = write;
+	unsigned int chunklen;
+	unsigned int overflow;
+
+	/* copy up to 16 bytes into the write buffer */
+	chunklen = hvsi_read(vtty, write->data.pkt + write->got, 16);
+	if (!chunklen)
+		return 0;
+	write->got += chunklen;
+	write->want = desc_want(write);
+
+	overflow = desc_overflow(write);
+	while (overflow) {
+		/* copied too much into 'write'; memcpy it into the next buffers */
+		int nextlen;
+		write = next_desc(write);
+
+		nextlen = copy_packet(write->data.pkt, old->data.pkt + old->want,
+			overflow);
+		write->got = nextlen;
+		write->want = desc_want(write);
+		overflow -= nextlen;
+	}
+	if (desc_done(write))
+		write = next_desc(write);
+	return 1;
+}
+
+/* keep reading from hypervisor until there's no more */
+static void hvsi_load_buffers(struct vtty_struct *vtty)
+{
+	/* XXX perhaps we should limit this */
+	while (hvsi_load_chunk(vtty)) {
+		if (write == read) {
+			/* we've filled all our ring buffers; let the hypervisor queue
+			 * the rest for us */
+			break;
+		}
+	}
+}
+
+static int hvsi_recv_control(struct vtty_struct *vtty, struct hvsi_control *pkt)
+{
+	int ret = 0;
+	
+	//dump_packet((struct hvsi_header *)pkt);
+	
+	switch (pkt->verb) {
+		case VSV_MODEM_CTL_UPDATE:
+			if ((pkt->word & HVSI_TSCD) == 0) {
+				/* CD went away; no more connection */
+				vtty->mctrl &= TIOCM_CD;
+				ret = -EPIPE;
+			}
+			break;
+		case VSV_CLOSE_PROTOCOL:
+			/* XXX handle this by reopening on open/read/write() ? */
+			panic("%s: service processor closed HVSI connection!\n", __FUNCTION__);
+			break;
+		default:
+			printk(KERN_WARNING "unknown HVSI control packet: ");
+			dump_packet((struct hvsi_header *)pkt);
+			break;
+	}
+	return ret;
+}
+
+/* transfer from ring buffers to caller's buffer */
+static int hvsi_deliver(struct vtty_struct *vtty, uint8_t *buf, int buflen)
+{
+	int written = 0;
+	int ret;
+
+	for (; (read != write) && (buflen > 0); read = next_desc(read)) {
+		struct hvsi_header *pkt = &read->data.hdr;
+		int size;
+
+#ifdef DEBUG
+		dump_ring();
+#endif
+
+		switch (pkt->type) {
+			case VS_DATA_PACKET_HEADER:
+				size = min(len_payload(pkt), buflen);
+				memcpy(buf, payload(pkt), size);
+				buf += size;
+				buflen -= size;
+				written += size;
+				break;
+			case VS_CONTROL_PACKET_HEADER:
+				ret = hvsi_recv_control(vtty, (struct hvsi_control *)pkt);
+				/* if we got an error (like CD dropped), stop now.
+				 * otherwise keep dispatching packets */
+				if (ret < 0) {
+					desc_clear(read);
+					read = next_desc(read);
+					return ret;
+				}
+				break;
+			default:
+				printk(KERN_WARNING "unknown HVSI packet: ");
+				dump_packet(pkt);
+				break;
+		}
+		desc_clear(read);
+	}
+
+	return written;
+}
+
+static int hvsi_get_chars(struct vtty_struct *vtty, char *databuf, int count)
+{
+	hvsi_load_buffers(vtty); /* get pending data */
+	return hvsi_deliver(vtty, databuf, count); /* hand it up */
+}
+
+static struct hvsi_header *search_for_packet(struct vtty_struct *vtty, int type)
+{
+	/* bring in queued packets */
+	hvsi_load_buffers(vtty);
+
+	/* look for the version query response packet */
+	for (; read != write; read = next_desc(read)) {
+		struct hvsi_header *pkt = &read->data.hdr;
+
+		if (pkt->type == type) {
+			desc_clear(read);
+			read = next_desc(read);
+			return pkt;
+		}
+		printk("%s: ignoring packet while waiting for type 0x%x:\n",
+			__FUNCTION__, type);
+		dump_packet(pkt);
+	}
+
+	return NULL;
+}
+
+static int wait_for_packet(struct vtty_struct *vtty, struct hvsi_header **hdr,
+	int type)
+{
+	struct hvsi_header *found;
+	int count = 0;
+
+	do {
+		if (count++ > WAIT_LOOPS)
+			return -EIO;
+		udelay(WAIT_USECS);
+		found = search_for_packet(vtty, type);
+	} while (!found);
+
+	*hdr = found;
+	return 0;
+}
+
+static int hvsi_query(struct vtty_struct *vtty, uint16_t verb)
+{
+	struct hvsi_query query __ALIGNED__ = {
+		.type = VS_QUERY_PACKET_HEADER,
+		.len = sizeof(struct hvsi_query),
+	};
+	int wrote;
+
+	query.seqno = vtty->seqno++;
+	query.verb = verb;
+	wrote = hvc_hvterm_put_chars(vtty, (char *)&query, query.len);
+	if (wrote != query.len) {
+		printk(KERN_ERR "%s: couldn't send query!\n", __FUNCTION__);
+		return -EIO;
+	}
+
+	return 0;
+}
+
+/* respond to service processor's version query */
+static int hvsi_version_respond(struct vtty_struct *vtty, uint16_t query_seqno)
+{
+	struct hvsi_query_response response __ALIGNED__ = {
+		.type = VS_QUERY_RESPONSE_PACKET_HEADER,
+		.len = sizeof(struct hvsi_query_response),
+		.verb = VSV_SEND_VERSION_NUMBER,
+		.u.version = HVSI_VERSION,
+	};
+	int wrote;
+
+	response.seqno = vtty->seqno++;
+	response.query_seqno = query_seqno+1,
+	wrote = hvc_hvterm_put_chars(vtty, (char *)&response, response.len);
+	if (wrote != response.len) {
+		printk(KERN_ERR "%s: couldn't send query response!\n", __FUNCTION__);
+		return -EIO;
+	}
+
+	return 0;
+}
+
+static int hvsi_get_mctrl(struct vtty_struct *vtty)
+{
+	struct hvsi_header *hdr;
+	int ret = 0;
+	uint16_t mctrl;
+
+	if (hvsi_query(vtty, VSV_SEND_MODEM_CTL_STATUS)) {
+		ret = -EIO;
+		goto out;
+	}
+	if (wait_for_packet(vtty, &hdr, VS_QUERY_RESPONSE_PACKET_HEADER)) {
+		ret = -EIO;
+		goto out;
+	}
+	/* XXX see if it's the right response */
+
+	vtty->mctrl = 0;
+
+	mctrl = ((struct hvsi_query_response *)hdr)->u.mctrl_word;
+	if (mctrl & HVSI_TSDTR)
+		vtty->mctrl |= TIOCM_DTR;
+	if (mctrl & HVSI_TSCD)
+		vtty->mctrl |= TIOCM_CD;
+	pr_debug("%s: mctrl 0x%x\n", __FUNCTION__, vtty->mctrl);
+
+out:
+	return ret;
+}
+
+static int hvsi_handshake(struct vtty_struct *vtty)
+{
+	struct hvsi_header *hdr;
+	int ret = 0;
+
+	if (hvsi_query(vtty, VSV_SEND_VERSION_NUMBER)) {
+		ret = -EIO;
+		goto out;
+	}
+	if (wait_for_packet(vtty, &hdr, VS_QUERY_RESPONSE_PACKET_HEADER)) {
+		ret = -EIO;
+		goto out;
+	}
+	/* XXX see if it's the right response */
+
+	if (wait_for_packet(vtty, &hdr, VS_QUERY_PACKET_HEADER)) {
+		ret = -EIO;
+		goto out;
+	}
+	/* XXX see if it's the right query */
+	if (hvsi_version_respond(vtty, hdr->seqno)) {
+		ret = -EIO;
+		goto out;
+	}
+
+	if (hvsi_get_mctrl(vtty)) {
+		ret = -EIO;
+		goto out;
+	}
+
+out:
+	if (ret < 0)
+		printk(KERN_ERR "HVSI handshaking failed\n");
+	return ret;
+}
+
+static int hvsi_put_chars(struct vtty_struct *vtty, const char *buf, int count)
+{
+	char packet[16] __ALIGNED__;
+	uint64_t *lbuf = (uint64_t *)packet;
+	struct hvsi_header *hdr = (struct hvsi_header *)packet;
+	int ret;
+
+	hdr->type = VS_DATA_PACKET_HEADER;
+	hdr->seqno = vtty->seqno++;
+
+	if (count > 12)
+		count = 12; /* we'll leave some chars behind in buf */
+	hdr->len = count + sizeof(struct hvsi_header);
+	memcpy(packet + sizeof(struct hvsi_header), buf, count);
+
+	/* note: we can't use hvc_hvterm_put_chars() here, as it would return
+	 * _packet_ length, not _payload_ length */
+	ret = plpar_hcall_norets(H_PUT_TERM_CHAR, vtty->vtermno, hdr->len,
+			lbuf[0], lbuf[1]);
+	if (ret == H_Success)
+		return count;
+	if (ret == H_Busy)
+		return 0;
+	return -EIO;
+}
+
+/* note that we can only set DTR */
+static int hvsi_set_mctrl(struct vtty_struct *vtty, uint16_t mctrl)
+{
+	struct hvsi_control command __ALIGNED__ = {
+		.type = VS_CONTROL_PACKET_HEADER,
+		.len = sizeof(struct hvsi_control),
+		.verb = VSV_SET_MODEM_CTL,
+		.mask = HVSI_TSDTR,
+	};
+	int wrote;
+
+	command.seqno = vtty->seqno++;
+	if (mctrl & TIOCM_DTR)
+		command.word = HVSI_TSDTR;
+
+	wrote = hvc_hvterm_put_chars(vtty, (char *)&command, command.len);
+	if (wrote != command.len) {
+		printk(KERN_ERR "%s: couldn't set DTR!\n", __FUNCTION__);
+		return -EIO;
+	}
+
+	return 0;
+}
+
+static int hvsi_tiocmset(struct vtty_struct *vtty, uint16_t set, uint16_t clear)
+{
+	uint16_t old_mctrl;
+
+	/* we can only set DTR */
+	if (set & ~TIOCM_DTR)
+		return -EINVAL;
+
+	old_mctrl = vtty->mctrl;
+	vtty->mctrl = (old_mctrl & ~clear) | set;
+
+	pr_debug("%s: new mctrl 0x%x\n", __FUNCTION__, vtty->mctrl);
+	if (old_mctrl != vtty->mctrl) {
+		if (hvsi_set_mctrl(vtty, vtty->mctrl) < 0)
+			return -EIO;
+	} else {
+		pr_debug("  (not writing to SP)\n");
+	}
+
+	return 0;
+}
+
+static int hvsi_tiocmget(struct vtty_struct *vtty)
+{
+	if (hvsi_get_mctrl(vtty))
+		return -EIO;
+	pr_debug("%s: mctrl 0x%x\n", __FUNCTION__, vtty->mctrl);
+	return vtty->mctrl;
+}
+
+/* external (hvc_console.c) interface: */
+
+int hvc_arch_get_chars(int index, char *buf, int count)
+{
+	struct vtty_struct *vtty = &vttys[index];
+
+	if (index >= MAX_NR_HVC_CONSOLES)
+		return -ENODEV;
+
+	return vtty->get_chars(vtty, buf, count);
+}
+
+int hvc_arch_put_chars(int index, const char *buf, int count)
+{
+	struct vtty_struct *vtty = &vttys[index];
+
+	if (index >= MAX_NR_HVC_CONSOLES)
+		return -ENODEV;
+
+	return vtty->put_chars(vtty, buf, count);
+}
+
+int hvc_arch_tiocmset(int index, unsigned int set, unsigned int clear)
+{
+	struct vtty_struct *vtty = &vttys[index];
+
+	if (index >= MAX_NR_HVC_CONSOLES)
+		return -ENODEV;
+
+	if (vtty->tiocmset)
+		return vtty->tiocmset(vtty, set, clear);
+	return -EINVAL;
+}
+
+int hvc_arch_tiocmget(int index)
+{
+	struct vtty_struct *vtty = &vttys[index];
+
+	if (index >= MAX_NR_HVC_CONSOLES)
+		return -ENODEV;
+
+	if (vtty->tiocmset)
+		return vtty->tiocmget(vtty);
+	return -EINVAL;
 }
 
-/* return the number of client vterms present */
-/* XXX this requires an interface change to handle multiple discontiguous
- * vterms */
-int hvc_count(int *start_termno)
+int hvc_arch_find_vterms(void)
 {
 	struct device_node *vty;
-	int num_found = 0;
+	int count = 0;
 
-	/* consider only the first vty node.
-	 * we should _always_ be able to find one. */
-	vty = of_find_node_by_name(NULL, "vty");
-	if (vty && device_is_compatible(vty, "hvterm1")) {
-		u32 *termno = (u32 *)get_property(vty, "reg", 0);
+	for (vty = of_find_node_by_name(NULL, "vty"); vty != NULL;
+			vty = of_find_node_by_name(vty, "vty")) {
+		struct vtty_struct *vtty;
+		uint32_t *vtermno;
+
+		vtermno = (uint32_t *)get_property(vty, "reg", NULL);
+		if (!vtermno)
+			continue;
+
+		if (count >= MAX_NR_HVC_CONSOLES)
+			break;
+
+		vtty = &vttys[count];
+		if (device_is_compatible(vty, "hvterm1")) {
+			vtty->vtermno = *vtermno;
+			vtty->get_chars = hvc_hvterm_get_chars;
+			vtty->put_chars = hvc_hvterm_put_chars;
+			vtty->tiocmget = NULL;
+			vtty->tiocmset = NULL;
+			hvc_instantiate();
+			count++;
+		} else if (device_is_compatible(vty, "hvterm-protocol")) {
+			vtty->vtermno = *vtermno;
+			vtty->seqno = 0;
+			vtty->get_chars = hvsi_get_chars;
+			vtty->put_chars = hvsi_put_chars;
+			vtty->tiocmget = hvsi_tiocmget;
+			vtty->tiocmset = hvsi_tiocmset;
+			if (hvsi_handshake(vtty)) {
+				continue;
+			}
+			vtty->put_chars(vtty, "\nHVSI\n", 6);
+			hvc_instantiate();
+			count++;
+		}
+	}
+
+	return count;
+}
+
+/* Convert arch specific return codes into relevant errnos.  The hvcs
+ * functions aren't performance sensitive, so this conversion isn't an
+ * issue. */
+int hvcs_convert(long to_convert)
+{
+	switch (to_convert) {
+		case H_Success:
+			return 0;
+		case H_Parameter:
+			return -EINVAL;
+		case H_Hardware:
+			return -EIO;
+		case H_Busy:
+			return -EBUSY;
+		case H_Function: /* fall through */
+		default:
+			return -EPERM;
+	}
+}
+
+int hvcs_free_partner_info(struct list_head *head)
+{
+	struct hvcs_partner_info *pi;
+	struct list_head *element;
+
+	if(!head) {
+		return -EINVAL;
+	}
 
-		if (termno && start_termno)
-			*start_termno = *termno;
-		num_found = 1;
-		of_node_put(vty);
+	while (!list_empty(head)) {
+		element = head->next;
+		pi = list_entry(element,struct hvcs_partner_info,node);
+		list_del(element);
+		kfree(pi);
 	}
 
-	return num_found;
+	return 0;
+}
+EXPORT_SYMBOL(hvcs_free_partner_info);
+
+/* Helper function for hvcs_get_partner_info */
+int hvcs_next_partner(unsigned int unit_address, unsigned long last_p_partition_ID, unsigned long last_p_unit_address, unsigned long *pi_buff)
+{
+	long retval;
+	retval = plpar_hcall_norets(H_VTERM_PARTNER_INFO, unit_address,
+			last_p_partition_ID,
+				last_p_unit_address, virt_to_phys(pi_buff));
+	return hvcs_convert(retval);
+}
+
+/* The unit_address parameter is the unit address of the vty-server@ vdevice
+ * in whose partner information the caller is interested.  This function
+ * uses a pointer to a list_head instance in which to store the partner info.
+ * This function returns Non-Zero on success, or if there is no partner info.
+ *
+ * Invocation of this function should always be followed by an invocation of
+ * hvcs_free_partner_info() using a pointer to the SAME list head instance
+ * that was used to store the partner_info list.
+ */
+int hvcs_get_partner_info(unsigned int unit_address, struct list_head *head)
+{
+	/* This is a page sized buffer to be passed to hvcall per invocation.
+	 * NOTE: the first long returned is unit_address.  The second long
+	 * returned is the partition ID and starting with pi_buff[2] are
+	 * HVCS_CLC_LENGTH characters, which are diff size than the unsigned
+	 * long, hence the casting mumbojumbo you see later. */
+	unsigned long	*pi_buff;
+	unsigned long	last_p_partition_ID;
+	unsigned long	last_p_unit_address;
+	struct hvcs_partner_info *next_partner_info = NULL;
+	int more = 1;
+	int retval;
+
+	/* invalid parameters */
+	if (!head)
+		return -EINVAL;
+
+	last_p_partition_ID = last_p_unit_address = HVCS_LONG_INVALID;
+	INIT_LIST_HEAD(head);
+
+	pi_buff = kmalloc(PAGE_SIZE, GFP_KERNEL);
+
+	if(!pi_buff)
+		return -ENOMEM;
+
+	do {
+		retval = hvcs_next_partner(unit_address, last_p_partition_ID,
+				last_p_unit_address, pi_buff);
+		if(retval) {
+			kfree(pi_buff);
+			pi_buff = 0;
+			/* don't indicate that we've failed if we have
+			 * any list elements. */
+			if(!list_empty(head))
+				return 0;
+			return retval;
+		}
+
+		last_p_partition_ID = pi_buff[0];
+		last_p_unit_address = pi_buff[1];
+
+		/* This indicates that there are no further partners */
+		if (last_p_partition_ID == HVCS_LONG_INVALID
+				&& last_p_unit_address == HVCS_LONG_INVALID)
+			break;
+
+		next_partner_info = kmalloc(sizeof(struct hvcs_partner_info),
+				GFP_KERNEL);
+
+		if (!next_partner_info) {
+			printk(KERN_WARNING "HVCONSOLE: kmalloc() failed to"
+				" allocate partner info struct.\n");
+			hvcs_free_partner_info(head);
+			kfree(pi_buff);
+			pi_buff = 0;
+			return -ENOMEM;
+		}
+
+		next_partner_info->unit_address
+			= (unsigned int)last_p_unit_address;
+		next_partner_info->partition_ID
+			= (unsigned int)last_p_partition_ID;
+
+		/* copy the Null-term char too */
+		strncpy(&next_partner_info->location_code[0],
+			(char *)&pi_buff[2],
+			strlen((char *)&pi_buff[2]) + 1);
+
+		list_add_tail(&(next_partner_info->node), head);
+		next_partner_info = NULL;
+
+	} while (more);
+
+	kfree(pi_buff);
+	pi_buff = 0;
+
+	return 0;
+}
+EXPORT_SYMBOL(hvcs_get_partner_info);
+
+/* If this function is called once and -EINVAL is returned it may
+ * indicate that the partner info needs to be refreshed for the
+ * target unit address at which point the caller must invoke
+ * hvcs_get_partner_info() and then call this function again.  If,
+ * for a second time, -EINVAL is returned then it indicates that
+ * there is probably already a partner connection registered to a
+ * different vty-server@ vdevice.  It is also possible that a second
+ * -EINVAL may indicate that one of the parms is not valid, for
+ * instance if the link was removed between the vty-server@ vdevice
+ * and the vty@ vdevice that you are trying to open.  Don't shoot the
+ * messenger.  Firmware implemented it this way.
+ */
+int hvcs_register_connection( unsigned int unit_address, unsigned int p_partition_ID, unsigned int p_unit_address)
+{
+	long retval;
+	retval = plpar_hcall_norets(H_REGISTER_VTERM, unit_address,
+				p_partition_ID, p_unit_address);
+	return hvcs_convert(retval);
+}
+EXPORT_SYMBOL(hvcs_register_connection);
+
+/* If -EBUSY is returned continue to call this function
+ * until 0 is returned */
+int hvcs_free_connection(unsigned int unit_address)
+{
+	long retval;
+	retval = plpar_hcall_norets(H_FREE_VTERM, unit_address);
+	return hvcs_convert(retval);
 }
+EXPORT_SYMBOL(hvcs_free_connection);
diff -purN linux-2.5/arch/ppc64/kernel/i8259.c linuxppc64-2.5/arch/ppc64/kernel/i8259.c
--- linux-2.5/arch/ppc64/kernel/i8259.c	2003-02-11 11:51:44.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/i8259.c	2003-09-12 19:50:39.000000000 +0000
@@ -124,8 +124,8 @@ static void i8259_unmask_irq(unsigned in
 
 static void i8259_end_irq(unsigned int irq)
 {
-	if (!(irq_desc[irq].status & (IRQ_DISABLED|IRQ_INPROGRESS)) &&
-	    irq_desc[irq].action)
+	if (!(get_irq_desc(irq)->status & (IRQ_DISABLED|IRQ_INPROGRESS)) &&
+	    get_irq_desc(irq)->action)
 		i8259_unmask_irq(irq);
 }
 
diff -purN linux-2.5/arch/ppc64/kernel/iSeries_irq.c linuxppc64-2.5/arch/ppc64/kernel/iSeries_irq.c
--- linux-2.5/arch/ppc64/kernel/iSeries_irq.c	2004-02-25 02:54:12.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/iSeries_irq.c	2004-02-26 06:29:56.000000000 +0000
@@ -122,8 +122,8 @@ void __init iSeries_activate_IRQs()
 	int irq;
 	unsigned long flags;
 
-	for (irq = 0; irq < NR_IRQS; irq++) {
-		irq_desc_t *desc = &irq_desc[irq];
+	for_each_irq (irq) {
+		irq_desc_t *desc = get_irq_desc(irq);
 
 		if (desc && desc->handler && desc->handler->startup) {
 			spin_lock_irqsave(&desc->lock, flags);
diff -purN linux-2.5/arch/ppc64/kernel/iSeries_pci.c linuxppc64-2.5/arch/ppc64/kernel/iSeries_pci.c
--- linux-2.5/arch/ppc64/kernel/iSeries_pci.c	2004-03-16 11:30:37.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/iSeries_pci.c	2004-03-17 00:46:42.000000000 +0000
@@ -46,6 +46,7 @@
 #include <asm/iSeries/iSeries_irq.h>
 #include <asm/iSeries/iSeries_pci.h>
 #include <asm/iSeries/mf.h>
+#include <asm/iSeries/vio.h>
 
 #include "iSeries_IoMmTable.h"
 #include "pci.h"
@@ -283,6 +284,11 @@ void __init iSeries_pci_final_fixup(void
 	iSeries_IoMmTable_Status();
 	iSeries_activate_IRQs();
 	mf_displaySrc(0xC9000200);
+
+	/* Now set up virtual bus device information */
+	if (device_register(iSeries_vio_dev)) {
+	    printk("pcibios error registering iSeries_vio_dev\n");
+	}
 }
 
 void pcibios_fixup_bus(struct pci_bus *PciBus)
diff -purN linux-2.5/arch/ppc64/kernel/iSeries_setup.c linuxppc64-2.5/arch/ppc64/kernel/iSeries_setup.c
--- linux-2.5/arch/ppc64/kernel/iSeries_setup.c	2004-03-16 11:30:38.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/iSeries_setup.c	2004-03-17 00:46:42.000000000 +0000
@@ -298,7 +298,6 @@ void __init iSeries_init_early(void)
 		initrd_start = (unsigned long)__va(naca->xRamDisk);
 		initrd_end = initrd_start + naca->xRamDiskSize * PAGE_SIZE;
 		initrd_below_start_ok = 1;	// ramdisk in kernel space
-		ROOT_DEV = Root_RAM0;
 		if (((rd_size * 1024) / PAGE_SIZE) < naca->xRamDiskSize)
 			rd_size = (naca->xRamDiskSize * PAGE_SIZE) / 1024;
 	} else
diff -purN linux-2.5/arch/ppc64/kernel/iSeries_setup.h linuxppc64-2.5/arch/ppc64/kernel/iSeries_setup.h
--- linux-2.5/arch/ppc64/kernel/iSeries_setup.h	2004-01-21 01:50:56.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/iSeries_setup.h	2004-02-26 10:56:03.000000000 +0000
@@ -29,7 +29,6 @@ extern void iSeries_setup_arch(void);
 extern void iSeries_setup_residual(struct seq_file *m, int cpu_id);
 extern void iSeries_get_cpuinfo(struct seq_file *m);
 extern void iSeries_init_IRQ(void);
-extern void iSeries_init_irq_desc(irq_desc_t *);
 extern int iSeries_get_irq(struct pt_regs *regs);
 extern void iSeries_restart(char *cmd);
 extern void iSeries_power_off(void);
diff -purN linux-2.5/arch/ppc64/kernel/idle.c linuxppc64-2.5/arch/ppc64/kernel/idle.c
--- linux-2.5/arch/ppc64/kernel/idle.c	2004-02-12 04:39:21.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/idle.c	2004-03-09 23:39:55.000000000 +0000
@@ -26,6 +26,7 @@
 #include <linux/unistd.h>
 #include <linux/slab.h>
 #include <linux/interrupt.h>
+#include <linux/cpu.h>
 
 #include <asm/pgtable.h>
 #include <asm/uaccess.h>
@@ -150,6 +151,8 @@ int default_idle(void)
 		}
 
 		schedule();
+		if (cpu_is_offline(smp_processor_id()) && system_running)
+			cpu_die();
 	}
 
 	return 0;
@@ -236,6 +239,8 @@ int dedicated_idle(void)
 		HMT_medium();
 		lpaca->xLpPaca.xIdle = 0;
 		schedule();
+		if (cpu_is_offline(smp_processor_id()) && system_running)
+			cpu_die();
 	}
 	return 0;
 }
@@ -245,6 +250,9 @@ int shared_idle(void)
 	struct paca_struct *lpaca = get_paca();
 
 	while (1) {
+		if (cpu_is_offline(smp_processor_id()) && system_running)
+			cpu_die();
+
 		/* Indicate to the HV that we are idle.  Now would be
 		 * a good time to find other work to dispatch. */
 		lpaca->xLpPaca.xIdle = 1;
diff -purN linux-2.5/arch/ppc64/kernel/irq.c linuxppc64-2.5/arch/ppc64/kernel/irq.c
--- linux-2.5/arch/ppc64/kernel/irq.c	2004-03-16 11:30:37.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/irq.c	2004-03-17 01:47:27.000000000 +0000
@@ -67,6 +67,7 @@ irq_desc_t irq_desc[NR_IRQS] __cacheline
 	}
 };
 
+int __irq_offset_value;
 int ppc_spurious_interrupts = 0;
 unsigned long lpEvent_count = 0;
 
@@ -76,7 +77,7 @@ setup_irq(unsigned int irq, struct irqac
 	int shared = 0;
 	unsigned long flags;
 	struct irqaction *old, **p;
-	irq_desc_t *desc = irq_desc + irq;
+	irq_desc_t *desc = get_irq_desc(irq);
 
 	/*
 	 * Some drivers like serial.c use request_irq() heavily,
@@ -134,7 +135,7 @@ setup_irq(unsigned int irq, struct irqac
 
 inline void synchronize_irq(unsigned int irq)
 {
-	while (irq_desc[irq].status & IRQ_INPROGRESS)
+	while (get_irq_desc(irq)->status & IRQ_INPROGRESS)
 		cpu_relax();
 }
 
@@ -148,11 +149,10 @@ EXPORT_SYMBOL(synchronize_irq);
 static int
 do_free_irq(int irq, void* dev_id)
 {
-	irq_desc_t *desc;
+	irq_desc_t *desc = get_irq_desc(irq);
 	struct irqaction **p;
 	unsigned long flags;
 
-	desc = irq_desc + irq;
 	spin_lock_irqsave(&desc->lock,flags);
 	p = &desc->action;
 	for (;;) {
@@ -247,7 +247,7 @@ EXPORT_SYMBOL(free_irq);
  
 inline void disable_irq_nosync(unsigned int irq)
 {
-	irq_desc_t *desc = irq_desc + irq;
+	irq_desc_t *desc = get_irq_desc(irq);
 	unsigned long flags;
 
 	spin_lock_irqsave(&desc->lock, flags);
@@ -276,7 +276,7 @@ EXPORT_SYMBOL(disable_irq_nosync);
  
 void disable_irq(unsigned int irq)
 {
-	irq_desc_t *desc = irq_desc + irq;
+	irq_desc_t *desc = get_irq_desc(irq);
 	disable_irq_nosync(irq);
 	if (desc->action)
 		synchronize_irq(irq);
@@ -296,7 +296,7 @@ EXPORT_SYMBOL(disable_irq);
  
 void enable_irq(unsigned int irq)
 {
-	irq_desc_t *desc = irq_desc + irq;
+	irq_desc_t *desc = get_irq_desc(irq);
 	unsigned long flags;
 
 	spin_lock_irqsave(&desc->lock, flags);
@@ -327,6 +327,7 @@ int show_interrupts(struct seq_file *p, 
 {
 	int i = *(loff_t *) v, j;
 	struct irqaction * action;
+	irq_desc_t *desc;
 	unsigned long flags;
 
 	if (i == 0) {
@@ -339,8 +340,9 @@ int show_interrupts(struct seq_file *p, 
 	}
 
 	if (i < NR_IRQS) {
-		spin_lock_irqsave(&irq_desc[i].lock, flags);
-		action = irq_desc[i].action;
+		desc = get_irq_desc(i);
+		spin_lock_irqsave(&desc->lock, flags);
+		action = desc->action;
 		if (!action || !action->handler)
 			goto skip;
 		seq_printf(p, "%3d: ", i);
@@ -352,17 +354,17 @@ int show_interrupts(struct seq_file *p, 
 #else
 		seq_printf(p, "%10u ", kstat_irqs(i));
 #endif /* CONFIG_SMP */
-		if (irq_desc[i].handler)		
-			seq_printf(p, " %s ", irq_desc[i].handler->typename );
+		if (desc->handler)
+			seq_printf(p, " %s ", desc->handler->typename );
 		else
 			seq_printf(p, "  None      ");
-		seq_printf(p, "%s", (irq_desc[i].status & IRQ_LEVEL) ? "Level " : "Edge  ");
+		seq_printf(p, "%s", (desc->status & IRQ_LEVEL) ? "Level " : "Edge  ");
 		seq_printf(p, "    %s",action->name);
 		for (action=action->next; action; action = action->next)
 			seq_printf(p, ", %s", action->name);
 		seq_putc(p, '\n');
 skip:
-		spin_unlock_irqrestore(&irq_desc[i].lock, flags);
+		spin_unlock_irqrestore(&desc->lock, flags);
 	} else if (i == NR_IRQS)
 		seq_printf(p, "BAD: %10u\n", ppc_spurious_interrupts);
 	return 0;
@@ -482,7 +484,7 @@ void ppc_irq_dispatch_handler(struct pt_
 	int status;
 	struct irqaction *action;
 	int cpu = smp_processor_id();
-	irq_desc_t *desc = irq_desc + irq;
+	irq_desc_t *desc = get_irq_desc(irq);
 	irqreturn_t action_ret;
 
 	kstat_cpu(cpu).irqs[irq]++;
@@ -564,11 +566,11 @@ out:
 	 * The ->end() handler has to deal with interrupts which got
 	 * disabled while the handler was running.
 	 */
-	if (irq_desc[irq].handler) {
-		if (irq_desc[irq].handler->end)
-			irq_desc[irq].handler->end(irq);
-		else if (irq_desc[irq].handler->enable)
-			irq_desc[irq].handler->enable(irq);
+	if (desc->handler) {
+		if (desc->handler->end)
+			desc->handler->end(irq);
+		else if (desc->handler->enable)
+			desc->handler->enable(irq);
 	}
 	spin_unlock(&desc->lock);
 }
@@ -683,6 +685,7 @@ static struct proc_dir_entry * root_irq_
 static struct proc_dir_entry * irq_dir [NR_IRQS];
 static struct proc_dir_entry * smp_affinity_entry [NR_IRQS];
 
+/* Protected by get_irq_desc(irq)->lock. */
 #ifdef CONFIG_IRQ_ALL_CPUS
 cpumask_t irq_affinity [NR_IRQS] = { [0 ... NR_IRQS-1] = CPU_MASK_ALL };
 #else  /* CONFIG_IRQ_ALL_CPUS */
@@ -702,16 +705,18 @@ static int irq_affinity_read_proc (char 
 static int irq_affinity_write_proc (struct file *file, const char *buffer,
 					unsigned long count, void *data)
 {
-	int irq = (long)data, full_count = count, err;
+	unsigned int irq = (long)data;
+	irq_desc_t *desc = get_irq_desc(irq);
+	int ret;
 	cpumask_t new_value, tmp;
 	cpumask_t allcpus = CPU_MASK_ALL;
 
-	if (!irq_desc[irq].handler->set_affinity)
+	if (!desc->handler->set_affinity)
 		return -EIO;
 
-	err = cpumask_parse(buffer, count, new_value);
-	if (err)
-		return err;
+	ret = cpumask_parse(buffer, count, new_value);
+	if (ret != 0)
+		return ret;
 
 	/*
 	 * We check for CPU_MASK_ALL in xics to send irqs to all cpus.
@@ -721,19 +726,28 @@ static int irq_affinity_write_proc (stru
 	 */
 	cpus_and(new_value, new_value, allcpus);
 
+	/* Grab lock here so cpu_online_map can't change, and also
+	 * protect irq_affinity[]. */
+        spin_lock(&desc->lock);
+
 	/*
 	 * Do not allow disabling IRQs completely - it's a too easy
 	 * way to make the system unusable accidentally :-) At least
 	 * one online CPU still has to be targeted.
 	 */
 	cpus_and(tmp, new_value, cpu_online_map);
-	if (cpus_empty(tmp))
-		return -EINVAL;
+	if (cpus_empty(tmp)) {
+		ret = -EINVAL;
+		goto out;
+	}
 
 	irq_affinity[irq] = new_value;
-	irq_desc[irq].handler->set_affinity(irq, new_value);
+	desc->handler->set_affinity(irq, new_value);
+	ret = count;
 
-	return full_count;
+out:
+	spin_unlock(&desc->lock);
+	return ret;
 }
 
 static int prof_cpu_mask_read_proc (char *page, char **start, off_t off,
@@ -828,8 +842,8 @@ void init_irq_proc (void)
 	/*
 	 * Create entries for all existing IRQs.
 	 */
-	for (i = 0; i < NR_IRQS; i++) {
-		if (irq_desc[i].handler == NULL)
+	for_each_irq(i) {
+		if (get_irq_desc(i)->handler == NULL)
 			continue;
 		register_irq_proc(i);
 	}
@@ -857,7 +871,7 @@ unsigned int virt_irq_to_real_map[NR_IRQ
  * we don't end up with an interrupt number >= NR_IRQS.
  */
 #define MIN_VIRT_IRQ	3
-#define MAX_VIRT_IRQ	(NR_IRQS - NUM_8259_INTERRUPTS - 1)
+#define MAX_VIRT_IRQ	(NR_IRQS - NUM_ISA_INTERRUPTS - 1)
 #define NR_VIRT_IRQS	(MAX_VIRT_IRQ - MIN_VIRT_IRQ + 1)
 
 void
@@ -946,5 +960,4 @@ unsigned int real_irq_to_virt_slowpath(u
 
 }
 
-
 #endif
diff -purN linux-2.5/arch/ppc64/kernel/lparcfg.c linuxppc64-2.5/arch/ppc64/kernel/lparcfg.c
--- linux-2.5/arch/ppc64/kernel/lparcfg.c	2004-02-27 05:33:08.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/lparcfg.c	2004-03-02 22:14:49.000000000 +0000
@@ -29,9 +29,31 @@
 #include <asm/hvcall.h>
 #include <asm/cputable.h>
 
-#define MODULE_VERS "1.0"
+#define MODULE_VERS "1.1"
 #define MODULE_NAME "lparcfg"
 
+/* #define LPARCFG_DEBUG */
+
+/* find a better place for this function... */
+void log_plpar_hcall_return(unsigned long rc,char * tag)
+{
+	if (rc==0) /* success, return */
+		return;
+/* check for null tag ? */
+	if (rc == H_Hardware)
+		printk("plpar-hcall (%s) failed with hardware fault\n",tag);
+	else if (rc == H_Function)
+		printk("plpar-hcall (%s) failed; function not allowed\n",tag);
+	else if (rc == H_Authority)
+		printk("plpar-hcall (%s) failed; not authorized to this function\n",tag);
+	else if (rc == H_Parameter)
+		printk("plpar-hcall (%s) failed; Bad parameter(s)\n",tag);
+	else
+		printk("plpar-hcall (%s) failed with unexpected rc(0x%lx)\n",tag,rc);
+
+}
+
+
 static struct proc_dir_entry *proc_ppc64_lparcfg;
 #define LPARCFG_BUFF_SIZE 4096
 
@@ -217,7 +239,7 @@ static int lparcfg_data(unsigned char *b
  *          XXXX - reserved (0)
  *              XXXX - Group Number
  *                  XXXX - Pool Number.
- *  R7 (PPOONNMMLLKKJJII)
+ *  R7 (IIJJKKLLMMNNOOPP).
  *      XX - reserved. (0)
  *        XX - bit 0-6 reserved (0).   bit 7 is Capped indicator.
  *          XX - variable processor Capacity Weight
@@ -225,76 +247,140 @@ static int lparcfg_data(unsigned char *b
  *              XXXX - Active processors in Physical Processor Pool.
  *                  XXXX  - Processors active on platform. 
  */
-unsigned int h_get_ppp(unsigned long *entitled,unsigned long  *unallocated,unsigned long *aggregation,unsigned long *resource)
+static unsigned int h_get_ppp(unsigned long *entitled,unsigned long  *unallocated,unsigned long *aggregation,unsigned long *resource)
 {
 	unsigned long rc;
 	rc = plpar_hcall_4out(H_GET_PPP,0,0,0,0,entitled,unallocated,aggregation,resource);
-	return 0;
+
+	log_plpar_hcall_return(rc,"H_GET_PPP");
+
+	return rc;
+}
+
+static void h_pic(unsigned long *pool_idle_time,unsigned long *num_procs)
+{
+	unsigned long rc;
+	unsigned long dummy;
+	rc = plpar_hcall(H_PIC,0,0,0,0,pool_idle_time,num_procs,&dummy);
+
+	log_plpar_hcall_return(rc,"H_PIC");
+}
+
+/* prototyping h_purr functionality.  this may need to be moved into decrementer code. */
+static void h_purr(unsigned long *purr)
+{
+	unsigned long rc;
+
+	unsigned long dummy;
+	rc = plpar_hcall(H_PURR, 0, 0, 0, 0,purr, &dummy, &dummy);
+
+	log_plpar_hcall_return(rc,"H_PURR");
 }
 
-/*
- * get_splpar_potential_characteristics().
- * Retrieve the potential_processors and max_entitled_capacity values
- * through the get-system-parameter rtas call.
- */
 #define SPLPAR_CHARACTERISTICS_TOKEN 20
 #define SPLPAR_MAXLENGTH 1026*(sizeof(char))
-unsigned int get_splpar_potential_characteristics(void)
+
+/*
+ * parse_system_parameter_string()
+ * Retrieve the potential_processors, max_entitled_capacity and friends 
+ * through the get-system-parameter rtas call.  Replace keyword strings as
+ * necessary, and add the contents to 'buf'.
+ */
+unsigned long parse_system_parameter_string(unsigned long n, char * buf)
 {
-	/* return 0 for now.  Underlying rtas functionality is not yet complete. 12/01/2003*/
-	return 0; 
-#if 0 
 	long call_status;
 	unsigned long ret[2];
 
-	char * buffer = kmalloc(SPLPAR_MAXLENGTH, GFP_KERNEL);
-
-	printk("token for ibm,get-system-parameter (0x%x)\n",rtas_token("ibm,get-system-parameter"));
+	char * local_buffer = kmalloc(SPLPAR_MAXLENGTH, GFP_KERNEL);
+	if (!local_buffer) {
+		printk("%s %s kmalloc failure at line %d \n",__FILE__,__FUNCTION__,__LINE__);
+		return n;
+	}
 
+	spin_lock(&rtas_data_buf_lock);
+	memset(rtas_data_buf, 0, SPLPAR_MAXLENGTH); 
 	call_status = rtas_call(rtas_token("ibm,get-system-parameter"), 3, 1,
 				NULL,
 				SPLPAR_CHARACTERISTICS_TOKEN,
-				&buffer,
+				__pa(rtas_data_buf),
 				SPLPAR_MAXLENGTH,
 				(void *)&ret);
+	memcpy(local_buffer,rtas_data_buf, SPLPAR_MAXLENGTH);
+	spin_unlock(&rtas_data_buf_lock);
 
 	if (call_status!=0) {
-		printk("Error calling get-system-parameter (0x%lx)\n",call_status);
-		kfree(buffer);
-		return -1;
+		printk("%s %s Error calling get-system-parameter (0x%lx)\n",__FILE__,__FUNCTION__,call_status);
 	} else {
-		printk("get-system-parameter (%s)\n",buffer);
-		kfree(buffer);
-		/* TODO: Add code here to parse out value for system_potential_processors and partition_max_entitled_capacity */
-		return 1;
-	}
+		int splpar_strlen;
+		int idx,w_idx;
+		char * workbuffer = kmalloc(SPLPAR_MAXLENGTH, GFP_KERNEL);
+		if (!workbuffer) {
+			printk("%s %s kmalloc failure at line %d \n",__FILE__,__FUNCTION__,__LINE__);
+			return n;
+		}
+
+#ifdef LPARCFG_DEBUG
+		printk("success calling get-system-parameter \n");
 #endif
+		splpar_strlen=local_buffer[0]*16+local_buffer[1];
+		local_buffer+=2; /* step over strlen value */
+
+		memset(workbuffer, 0, SPLPAR_MAXLENGTH);
+		w_idx=0; idx=0;
+		while ((*local_buffer) && (idx<splpar_strlen)) {
+			workbuffer[w_idx++]=local_buffer[idx++];
+			if ((local_buffer[idx]==',')||(local_buffer[idx]=='\0')) {
+				workbuffer[w_idx]='\0';
+				if (w_idx) /* avoid the empty string */
+				{
+					n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+						      "%s\n",workbuffer);
+				}
+				memset(workbuffer, 0, SPLPAR_MAXLENGTH);
+				idx++; /* skip the comma */
+				w_idx=0;
+			} else if (local_buffer[idx]=='=') {
+				/* code here to replace workbuffer contents
+				 with different keyword strings */
+				if (0==strcmp(workbuffer,"MaxEntCap")) {
+					strcpy(workbuffer,"partition_max_entitled_capacity\0");
+					w_idx=strlen(workbuffer);
+				}
+				if (0==strcmp(workbuffer,"MaxPlatProcs")) {
+					strcpy(workbuffer,"system_potential_processors\0");
+					w_idx=strlen(workbuffer);
+				}
+			}
+		}
+		kfree(workbuffer);
+		local_buffer-=2; /* back up over strlen value */
+	}
+	kfree(local_buffer);
+	return n;
 }
 
 static int lparcfg_data(unsigned char *buf, unsigned long size)
 {
-	unsigned long n = 0;
-	int shared, max_entitled_capacity;
-	int processors, system_active_processors, system_potential_processors;
-	struct device_node *root;
+	unsigned long n = 0;		/* scnprintf index */
+	int system_active_processors;
+	struct device_node *rootdn;
 	const char *model = "";
 	const char *system_id = "";
 	unsigned int *lp_index_ptr, lp_index = 0;
 	struct device_node *rtas_node;
-	int *ip;
-	unsigned long h_entitled,h_unallocated,h_aggregation,h_resource;
+	int *lrdrp;
 
 	if((buf == NULL) || (size > LPARCFG_BUFF_SIZE)) {
 		return -EFAULT;
 	}
 	memset(buf, 0, size); 
 
-	root = find_path_device("/");
-	if (root) {
-		model = get_property(root, "model", NULL);
-		system_id = get_property(root, "system-id", NULL);
-		lp_index_ptr = (unsigned int *)get_property(root, "ibm,partition-no", NULL);
-		if(lp_index_ptr) lp_index = *lp_index_ptr;
+	rootdn = find_path_device("/");
+	if (rootdn) {
+		model = get_property(rootdn, "model", NULL);
+		system_id = get_property(rootdn, "system-id", NULL);
+		lp_index_ptr = (unsigned int *)get_property(rootdn, "ibm,partition-no", NULL);
+		if (lp_index_ptr) lp_index = *lp_index_ptr;
 	}
 
 	n  = scnprintf(buf, LPARCFG_BUFF_SIZE - n,
@@ -307,16 +393,22 @@ static int lparcfg_data(unsigned char *b
 		      "partition_id=%d\n", (int)lp_index); 
 
 	rtas_node = find_path_device("/rtas");
-	ip = (int *)get_property(rtas_node, "ibm,lrdr-capacity", NULL);
-	if (ip == NULL) {
+	lrdrp = (int *)get_property(rtas_node, "ibm,lrdr-capacity", NULL);
+
+	if (lrdrp == NULL) {
 		system_active_processors = systemcfg->processorCount; 
 	} else {
-		system_active_processors = *(ip + 4);
-	}
+		system_active_processors = *(lrdrp + 4);
+	} 
 
 	if (cur_cpu_spec->firmware_features & FW_FEATURE_SPLPAR) {
+		unsigned long h_entitled,h_unallocated,h_aggregation,h_resource;
+		unsigned long pool_idle_time,pool_procs;
+		unsigned long purr;
+
 		h_get_ppp(&h_entitled,&h_unallocated,&h_aggregation,&h_resource);
-#ifdef DEBUG
+
+#ifdef LPARCFG_DEBUG
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
 			      "R4=0x%lx\n", h_entitled);
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
@@ -325,68 +417,93 @@ static int lparcfg_data(unsigned char *b
 			      "R6=0x%lx\n", h_aggregation);
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
 			      "R7=0x%lx\n", h_resource);
-#endif /* DEBUG */
-	}
+#endif /* LPARCFG_DEBUG */
+
+		h_pic(&pool_idle_time,&pool_procs);
+
+		h_purr(&purr);
+
+		/* this call handles the ibm,get-system-parameter contents */
+		n = parse_system_parameter_string(n,buf);
+
+		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
+			      "partition_entitled_capacity=%ld\n",
+			      h_entitled);
+
+		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
+			      "pool=%ld\n",
+			      (h_aggregation >> 0*8) & 0xffff);
+
+		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
+			      "group=%ld\n",
+			      (h_aggregation >> 2*8) & 0xffff);
 
-	if (cur_cpu_spec->firmware_features & FW_FEATURE_SPLPAR) {
-		system_potential_processors =  get_splpar_potential_characteristics();
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
 			      "system_active_processors=%ld\n", 
 			      (h_resource >> 2*8) & 0xffff);
+
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-			      "system_potential_processors=%d\n", 
-			      system_potential_processors);
-	} else {
-		system_potential_processors = system_active_processors;
-		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-			      "system_active_processors=%d\n", 
-			      system_active_processors);
+			      "pool_capacity=%ld\n",
+			      (h_resource >> 3*8) & 0xffff);
+
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-			      "system_potential_processors=%d\n", 
-			      system_potential_processors);
-	}
+			      "capacity_weight=%ld\n",
+			      (h_resource>>5*8) & 0xFF);
 
-	processors = systemcfg->processorCount;
-	n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-		      "partition_active_processors=%d\n", processors);  
-	n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-		      "partition_potential_processors=%d\n",
-		      system_active_processors);
+		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
+			      "capped=%ld\n",
+			      (h_resource >> 6*8) & 0x40);
 
-	/* max_entitled_capacity will come out of get_splpar_potential_characteristics() when that function is complete */
-	max_entitled_capacity = system_active_processors * 100; 
-	if (cur_cpu_spec->firmware_features & FW_FEATURE_SPLPAR) {
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-			      "partition_entitled_capacity=%ld\n", h_entitled);
-	} else {
+			      "unallocated_variable_weight=%ld\n",
+			      (h_resource>>7*8) & 0xFF);
+
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-			      "partition_entitled_capacity=%d\n", system_active_processors*100);
-	}
+			      "unallocated_capacity=%ld\n",
+			      h_unallocated);
 
-	n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-		      "partition_max_entitled_capacity=%d\n", 
-		      max_entitled_capacity);
+		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "pool_idle_time=%ld\n",
+			      pool_idle_time);
 
-	shared = 0;
-	n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-		      "shared_processor_mode=%d\n", shared);
+		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "pool_num_procs=%ld\n",
+			      pool_procs);
 
-	if (cur_cpu_spec->firmware_features & FW_FEATURE_SPLPAR) {
-		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-			      "pool=%ld\n", (h_aggregation >> 0*8)&0xffff);
+		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "purr=%ld\n",
+			      purr);
 
-		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-			      "pool_capacity=%ld\n", (h_resource >> 3*8) &0xffff);
+	} else /* non SPLPAR case */ {
+		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n, 
+			      "system_active_processors=%d\n",
+			      system_active_processors);
 
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-			      "group=%ld\n", (h_aggregation >> 2*8)&0xffff);
+			      "system_potential_processors=%d\n",
+			      system_active_processors);
 
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-			      "capped=%ld\n", (h_resource >> 6*8)&0x40);
+			      "partition_max_entitled_capacity=%d\n",
+			      100*system_active_processors);
 
 		n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
-			      "capacity_weight=%d\n", (int)(h_resource>>5*8)&0xFF);
+			      "partition_entitled_capacity=%d\n",
+			      system_active_processors*100);
 	}
+
+	n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
+		      "partition_active_processors=%d\n",
+		      (int) systemcfg->processorCount);
+
+	n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
+		      "partition_potential_processors=%d\n",
+		      system_active_processors);
+
+	n += scnprintf(buf+n, LPARCFG_BUFF_SIZE - n,
+		      "shared_processor_mode=%d\n",
+		      paca[0].xLpPaca.xSharedProc);
+
 	return 0;
 }
 
diff -purN linux-2.5/arch/ppc64/kernel/open_pic.c linuxppc64-2.5/arch/ppc64/kernel/open_pic.c
--- linux-2.5/arch/ppc64/kernel/open_pic.c	2004-02-23 16:39:14.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/open_pic.c	2004-02-26 10:56:03.000000000 +0000
@@ -67,7 +67,6 @@ static void openpic_disable_irq(u_int ir
 static void openpic_initirq(u_int irq, u_int pri, u_int vector, int polarity,
 			    int is_level);
 static void openpic_mapirq(u_int irq, u_int cpumask);
-static void openpic_set_sense(u_int irq, int sense);
 
 static void find_ISUs(void);
 
@@ -170,7 +169,7 @@ void __init pSeries_init_openpic(void)
         int i;
         unsigned int *addrp;
         unsigned char* chrp_int_ack_special = 0;
-        unsigned char init_senses[NR_IRQS - NUM_8259_INTERRUPTS];
+        unsigned char init_senses[NR_IRQS - NUM_ISA_INTERRUPTS];
         int nmi_irq = -1;
 #if defined(CONFIG_VT) && defined(CONFIG_ADB_KEYBOARD) && defined(XMON)
         struct device_node *kbd;
@@ -185,12 +184,12 @@ void __init pSeries_init_openpic(void)
 			__ioremap(addrp[prom_n_addr_cells(np)-1], 1, _PAGE_NO_CACHE);
         /* hydra still sets OpenPIC_InitSenses to a static set of values */
         if (OpenPIC_InitSenses == NULL) {
-                prom_get_irq_senses(init_senses, NUM_8259_INTERRUPTS, NR_IRQS);
+                prom_get_irq_senses(init_senses, NUM_ISA_INTERRUPTS, NR_IRQS);
                 OpenPIC_InitSenses = init_senses;
-                OpenPIC_NumInitSenses = NR_IRQS - NUM_8259_INTERRUPTS;
+                OpenPIC_NumInitSenses = NR_IRQS - NUM_ISA_INTERRUPTS;
         }
-        openpic_init(1, NUM_8259_INTERRUPTS, chrp_int_ack_special, nmi_irq);
-        for ( i = 0 ; i < NUM_8259_INTERRUPTS  ; i++ )
+        openpic_init(1, NUM_ISA_INTERRUPTS, chrp_int_ack_special, nmi_irq);
+        for (i = 0; i < NUM_ISA_INTERRUPTS; i++)
                 irq_desc[i].handler = &i8259_pic;
 	of_node_put(np);
 }
@@ -441,7 +440,7 @@ static int __init openpic_setup_i8259(vo
 
 	if (naca->interrupt_controller == IC_OPEN_PIC) {
 		/* Initialize the cascade */
-		if (request_irq(NUM_8259_INTERRUPTS, no_action, SA_INTERRUPT,
+		if (request_irq(NUM_ISA_INTERRUPTS, no_action, SA_INTERRUPT,
 				"82c59 cascade", NULL))
 			printk(KERN_ERR "Unable to get OpenPIC IRQ 0 for cascade\n");
 		i8259_init();
@@ -820,13 +819,21 @@ static void openpic_mapirq(u_int irq, u_
  *
  *  sense: 1 for level, 0 for edge
  */
-static inline void openpic_set_sense(u_int irq, int sense)
+#if 0	/* not used */
+static void openpic_set_sense(u_int irq, int sense)
 {
 	openpic_safe_writefield(&GET_ISU(irq).Vector_Priority,
 				OPENPIC_SENSE_LEVEL,
 				(sense ? OPENPIC_SENSE_LEVEL : 0));
 }
 
+static int openpic_get_sense(u_int irq)
+{
+	return openpic_readfield(&GET_ISU(irq).Vector_Priority,
+				 OPENPIC_SENSE_LEVEL) != 0;
+}
+#endif
+
 static void openpic_end_irq(unsigned int irq_nr)
 {
 	openpic_eoi();
diff -purN linux-2.5/arch/ppc64/kernel/open_pic.h linuxppc64-2.5/arch/ppc64/kernel/open_pic.h
--- linux-2.5/arch/ppc64/kernel/open_pic.h	2004-02-12 04:30:16.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/open_pic.h	2004-02-26 10:56:03.000000000 +0000
@@ -14,6 +14,7 @@
 
 #include <linux/config.h>
 #include <linux/cpumask.h>
+#include <linux/irq.h>
 
 #define OPENPIC_SIZE	0x40000
 
@@ -38,11 +39,4 @@ extern void openpic_init_processor(u_int
 extern void openpic_setup_ISU(int isu_num, unsigned long addr);
 extern void openpic_cause_IPI(u_int ipi, u_int cpumask);
 
-extern inline int openpic_to_irq(int irq)
-{
-	if (systemcfg->platform == PLATFORM_POWERMAC)
-		return irq;
-	return irq += NUM_8259_INTERRUPTS;
-}
-/*extern int open_pic_irq_offset;*/
 #endif /* _PPC64_KERNEL_OPEN_PIC_H */
diff -purN linux-2.5/arch/ppc64/kernel/open_pic_u3.c linuxppc64-2.5/arch/ppc64/kernel/open_pic_u3.c
--- linux-2.5/arch/ppc64/kernel/open_pic_u3.c	2004-02-12 03:47:52.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/open_pic_u3.c	2004-02-26 10:56:03.000000000 +0000
@@ -251,6 +251,14 @@ static inline void openpic2_set_sense(u_
 				 (sense ? OPENPIC_SENSE_LEVEL : 0));
 }
 
+#if 0	/* not used */
+static int openpic2_get_sense(u_int irq)
+{
+	return openpic2_readfield(&GET_ISU(irq).Vector_Priority,
+				  OPENPIC_SENSE_LEVEL) != 0;
+}
+#endif
+
 static void openpic2_end_irq(unsigned int irq_nr)
 {
 	openpic2_eoi();
diff -purN linux-2.5/arch/ppc64/kernel/proc_ppc64.c linuxppc64-2.5/arch/ppc64/kernel/proc_ppc64.c
--- linux-2.5/arch/ppc64/kernel/proc_ppc64.c	2004-03-16 11:30:38.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/proc_ppc64.c	2004-03-17 04:34:55.000000000 +0000
@@ -32,6 +32,8 @@
 #include <asm/uaccess.h>
 #include <asm/prom.h>
 
+void proc_ppc64_create_smt(void);
+
 static loff_t  page_map_seek( struct file *file, loff_t off, int whence);
 static ssize_t page_map_read( struct file *file, char *buf, size_t nbytes, loff_t *ppos);
 static int     page_map_mmap( struct file *file, struct vm_area_struct *vma );
@@ -130,6 +132,8 @@ static int __init proc_ppc64_init(void)
 		proc_create_paca(pde, i);
 
 #ifdef CONFIG_PPC_PSERIES
+	proc_ppc64_create_smt();
+
 	if ((systemcfg->platform & PLATFORM_PSERIES))
 		proc_ppc64_create_ofdt();
 #endif
@@ -412,3 +416,84 @@ static void release_prop_list(const stru
 
 }
 #endif	/* defined(CONFIG_PPC_PSERIES) */
+
+static int proc_ppc64_smt_snooze_read(char *page, char **start, off_t off,
+				      int count, int *eof, void *data)
+{
+	if (naca->smt_snooze_delay)
+		return sprintf(page, "%lu\n", naca->smt_snooze_delay);
+	else 
+		return sprintf(page, "disabled\n");
+}
+ 
+static int proc_ppc64_smt_snooze_write(struct file* file, const char *buffer,
+				       unsigned long count, void *data)
+{
+	unsigned long val;
+	char val_string[22];
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EACCES;
+
+	if (count > sizeof(val_string) - 1)
+		return -EINVAL;
+
+	if (copy_from_user(val_string, buffer, count))
+		return -EFAULT;
+
+	val_string[count] = '\0';
+
+	if (val_string[0] == '0' && (val_string[1] == '\n' || val_string[1] == '\0')) {
+		naca->smt_snooze_delay = 0;
+		return count;
+	}
+ 
+	val = simple_strtoul(val_string, NULL, 10);
+	if (val != 0) 
+		naca->smt_snooze_delay = val;
+	else
+		return -EINVAL;
+
+	return count;
+}
+ 
+static int proc_ppc64_smt_state_read(char *page, char **start, off_t off,
+				      int count, int *eof, void *data)
+{
+	switch(naca->smt_state) {
+	case SMT_OFF:
+		return sprintf(page, "off\n");
+		break;
+	case SMT_ON:
+		return sprintf(page, "on\n");
+		break;
+	case SMT_DYNAMIC:
+		return sprintf(page, "dynamic\n");
+		break;
+	default:
+		return sprintf(page, "unknown\n");
+		break;
+	}
+}
+ 
+void proc_ppc64_create_smt(void)
+{
+	struct proc_dir_entry *ent_snooze = 
+		create_proc_entry("ppc64/smt-snooze-delay", S_IRUGO | S_IWUSR,
+				  NULL);
+	struct proc_dir_entry *ent_enabled = 
+		create_proc_entry("ppc64/smt-enabled", S_IRUGO | S_IWUSR, NULL);
+	if (ent_snooze) {
+		ent_snooze->nlink = 1;
+		ent_snooze->data = NULL;
+		ent_snooze->read_proc = (void *)proc_ppc64_smt_snooze_read;
+		ent_snooze->write_proc = (void *)proc_ppc64_smt_snooze_write;
+	}
+
+	if (ent_enabled) {
+		ent_enabled->nlink = 1;
+		ent_enabled->data = NULL;
+		ent_enabled->read_proc = (void *)proc_ppc64_smt_state_read;
+		ent_enabled->write_proc = NULL;
+	}
+}
diff -purN linux-2.5/arch/ppc64/kernel/prom.c linuxppc64-2.5/arch/ppc64/kernel/prom.c
--- linux-2.5/arch/ppc64/kernel/prom.c	2004-03-19 05:59:30.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/prom.c	2004-03-20 11:56:08.000000000 +0000
@@ -1327,7 +1327,7 @@ smt_setup(void)
 		}
 
 		if (!found) {
-			my_smt_snooze_delay = 30000; /* default value */
+			my_smt_snooze_delay = 0; /* default value */
 		}
 	} else {
 		my_smt_snooze_delay = 0; /* default value */
@@ -2176,7 +2176,7 @@ finish_node_interrupts(struct device_nod
 			printk(KERN_CRIT "Could not allocate interrupt "
 			       "number for %s\n", np->full_name);
 		} else
-			np->intrs[i].line = openpic_to_irq(virq);
+			np->intrs[i].line = irq_offset_up(virq);
 
 		/* We offset irq numbers for the u3 MPIC by 128 in PowerMac */
 		if (systemcfg->platform == PLATFORM_POWERMAC && ic && ic->parent) {
@@ -2972,7 +2972,7 @@ static void remove_node_proc_entries(str
 }
 #endif /* CONFIG_PROC_DEVICETREE */
 
-/*
+/* 
  * Fix up n_intrs and intrs fields in a new device node
  *
  */
@@ -2988,7 +2988,7 @@ static int of_finish_dynamic_node_interr
 	node->n_intrs = intlen;
 	node->intrs = kmalloc(sizeof(struct interrupt_info) * intlen,
 			      GFP_KERNEL);
-	if (!node->intrs)
+	if (!node->intrs) 
 		return -ENOMEM;
 
 	for (i = 0; i < intlen; ++i) {
@@ -3004,7 +3004,7 @@ static int of_finish_dynamic_node_interr
 			       "number for %s\n", node->full_name);
 			return -ENOMEM;
 		}
-		node->intrs[i].line = openpic_to_irq(virq);
+		node->intrs[i].line = irq_offset_up(virq);
 		if (n > 1)
 			node->intrs[i].sense = irq[1];
 		if (n > 2) {
diff -purN linux-2.5/arch/ppc64/kernel/ras.c linuxppc64-2.5/arch/ppc64/kernel/ras.c
--- linux-2.5/arch/ppc64/kernel/ras.c	2004-03-16 11:30:38.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/ras.c	2004-03-17 00:46:45.000000000 +0000
@@ -1,4 +1,3 @@
-
 /*
  * ras.c
  * Copyright (C) 2001 Dave Engebretsen IBM Corporation
@@ -80,7 +79,7 @@ static int __init init_ras_IRQ(void)
 				       "number for %s\n", np->full_name);
 				break;
 			}
-			request_irq(virq + NUM_8259_INTERRUPTS, 
+			request_irq(irq_offset_up(virq),
 				    ras_error_interrupt, 0, 
 				    "RAS_ERROR", NULL);
 			ireg++;
@@ -98,7 +97,7 @@ static int __init init_ras_IRQ(void)
 				       " number for %s\n", np->full_name);
 				break;
 			}
-			request_irq(virq + NUM_8259_INTERRUPTS, 
+			request_irq(irq_offset_up(virq), 
 				    ras_epow_interrupt, 0, 
 				    "RAS_EPOW", NULL);
 			ireg++;
diff -purN linux-2.5/arch/ppc64/kernel/rtas.c linuxppc64-2.5/arch/ppc64/kernel/rtas.c
--- linux-2.5/arch/ppc64/kernel/rtas.c	2004-03-16 11:30:38.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/rtas.c	2004-03-17 16:17:19.000000000 +0000
@@ -130,7 +130,7 @@ rtas_token(const char *service)
 void
 log_rtas_error(struct rtas_args	*rtas_args)
 {
-	struct rtas_args err_args;
+	struct rtas_args err_args, temp_args;
 
 	err_args.token = rtas_token("rtas-last-error");
 	err_args.nargs = 2;
@@ -141,15 +141,17 @@ log_rtas_error(struct rtas_args	*rtas_ar
 	err_args.args[1] = RTAS_ERROR_LOG_MAX;
 	err_args.args[2] = 0;
 
+	temp_args = *rtas_args;
 	get_paca()->xRtas = err_args;
 
 	PPCDBG(PPCDBG_RTAS, "\tentering rtas with 0x%lx\n",
 	       (void *)__pa((unsigned long)&err_args));
 	enter_rtas((void *)__pa((unsigned long)&get_paca()->xRtas));
 	PPCDBG(PPCDBG_RTAS, "\treturned from rtas ...\n");
+	
 
 	err_args = get_paca()->xRtas;
-	get_paca()->xRtas = *rtas_args;
+	get_paca()->xRtas = temp_args;
 
 	if (err_args.rets[0] == 0)
 		log_error(rtas_err_buf, ERR_TYPE_RTAS_LOG, 0);
@@ -446,6 +448,23 @@ rtas_halt(void)
         rtas_power_off();
 }
 
+void
+rtas_os_term(char *str)
+{
+	long status;
+	char buf[1035];
+
+	sprintf(buf, "OS panic: %s", str);
+	do {
+		status = rtas_call(rtas_token("ibm,os-term"), 1, 1, NULL, __pa(buf));
+		if (status == RTAS_BUSY)
+			udelay(1);
+		else if (status != 0)
+			printk(KERN_EMERG "ibm,os-term call failed %ld\n", status);
+	} while (status != RTAS_BUSY);
+}
+
+
 unsigned long rtas_rmo_buf = 0;
 
 asmlinkage int ppc_rtas(struct rtas_args __user *uargs)
@@ -492,6 +511,25 @@ asmlinkage int ppc_rtas(struct rtas_args
 	return 0;
 }
 
+#ifdef CONFIG_HOTPLUG_CPU
+/* This version can't take the spinlock. */
+
+void rtas_stop_self(void)
+{
+	struct rtas_args *rtas_args = &(get_paca()->xRtas);
+
+	rtas_args->token = rtas_token("stop-self");
+	BUG_ON(rtas_args->token == RTAS_UNKNOWN_SERVICE);
+	rtas_args->nargs = 0;
+	rtas_args->nret  = 1;
+	rtas_args->rets  = &(rtas_args->args[0]);
+
+	printk("%u %u Ready to die...\n",
+	       smp_processor_id(), hard_smp_processor_id());
+	enter_rtas((void *)__pa(rtas_args));
+	panic("Alas, I survived.\n");
+}
+#endif /* CONFIG_HOTPLUG_CPU */
 
 EXPORT_SYMBOL(rtas_firmware_flash_list);
 EXPORT_SYMBOL(rtas_token);
diff -purN linux-2.5/arch/ppc64/kernel/rtasd.c linuxppc64-2.5/arch/ppc64/kernel/rtasd.c
--- linux-2.5/arch/ppc64/kernel/rtasd.c	2004-03-16 11:30:38.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/rtasd.c	2004-03-17 00:46:46.000000000 +0000
@@ -119,10 +119,11 @@ static int log_rtas_len(char * buf)
 
 		/* extended header */
 		len += err->extended_log_length;
-
-		if (len > RTAS_ERROR_LOG_MAX)
-			len = RTAS_ERROR_LOG_MAX;
 	}
+
+	if (len > rtas_error_log_max)
+		len = rtas_error_log_max;
+
 	return len;
 }
 
@@ -330,6 +331,10 @@ static int get_eventscan_parms(void)
 		printk(KERN_ERR "rtasd: truncated error log from %d to %d bytes\n", rtas_error_log_max, RTAS_ERROR_LOG_MAX);
 		rtas_error_log_max = RTAS_ERROR_LOG_MAX;
 	}
+
+	/* Make room for the sequence number */
+	rtas_error_log_buffer_max = rtas_error_log_max + sizeof(int);
+
 	of_node_put(node);
 
 	return 0;
@@ -405,8 +410,7 @@ static int rtasd(void *unused)
 
 	if (surveillance_timeout != -1) {
 		DEBUG("enabling surveillance\n");
-		if (enable_surveillance(surveillance_timeout))
-			goto error_vfree;
+		enable_surveillance(surveillance_timeout);
 		DEBUG("surveillance enabled\n");
 	}
 
@@ -430,10 +434,6 @@ static int rtasd(void *unused)
 			cpu = first_cpu_const(mk_cpumask_const(cpu_online_map));
 	}
 
-error_vfree:
-	if (rtas_log_buf)
-		vfree(rtas_log_buf);
-	rtas_log_buf = NULL;
 error:
 	/* Should delete proc entries */
 	return -EINVAL;
@@ -459,9 +459,6 @@ static int __init rtas_init(void)
 	if (kernel_thread(rtasd, 0, CLONE_FS) < 0)
 		printk(KERN_ERR "Failed to start RTAS daemon\n");
 
-	/* Make room for the sequence number */
-	rtas_error_log_buffer_max = rtas_error_log_max + sizeof(int);
-
 	return 0;
 }
 
diff -purN linux-2.5/arch/ppc64/kernel/setup.c linuxppc64-2.5/arch/ppc64/kernel/setup.c
--- linux-2.5/arch/ppc64/kernel/setup.c	2004-02-23 16:39:09.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/setup.c	2004-03-17 15:42:02.000000000 +0000
@@ -25,6 +25,8 @@
 #include <linux/version.h>
 #include <linux/tty.h>
 #include <linux/root_dev.h>
+#include <linux/notifier.h>
+#include <linux/cpu.h>
 #include <asm/io.h>
 #include <asm/prom.h>
 #include <asm/processor.h>
@@ -93,6 +95,13 @@ unsigned long SYSRQ_KEY;
 
 struct machdep_calls ppc_md;
 
+static int ppc64_panic_event(struct notifier_block *, unsigned long, void *);
+
+static struct notifier_block ppc64_panic_block = {
+	notifier_call: ppc64_panic_event,
+	priority: INT_MIN /* may not return; must be done last */
+};
+
 /*
  * Perhaps we can put the pmac screen_info[] here
  * on pmac as well so we don't need the ifdef's.
@@ -215,6 +224,7 @@ void setup_system(unsigned long r3, unsi
 	if (systemcfg->platform & PLATFORM_PSERIES) {
 		early_console_initialized = 1;
 		register_console(&udbg_console);
+		__irq_offset_value = NUM_ISA_INTERRUPTS;
 		finish_device_tree();
 		chrp_init(r3, r4, r5, r6, r7);
 
@@ -318,6 +328,14 @@ EXPORT_SYMBOL(machine_halt);
 unsigned long ppc_proc_freq;
 unsigned long ppc_tb_freq;
 
+static int ppc64_panic_event(struct notifier_block *this,
+                             unsigned long event, void *ptr)
+{
+	ppc_md.panic((char *)ptr);  /* May not return */
+	return NOTIFY_DONE;
+}
+
+
 #ifdef CONFIG_SMP
 DEFINE_PER_CPU(unsigned int, pvr);
 #endif
@@ -337,8 +355,13 @@ static int show_cpuinfo(struct seq_file 
 		return 0;
 	}
 
-	if (!cpu_online(cpu_id))
+	/* We only show online cpus: disable preempt (overzealous, I
+	 * knew) to prevent cpu going down. */
+	preempt_disable();
+	if (!cpu_online(cpu_id)) {
+		preempt_enable();
 		return 0;
+	}
 
 #ifdef CONFIG_SMP
 	pvr = per_cpu(pvr, cpu_id);
@@ -370,11 +393,11 @@ static int show_cpuinfo(struct seq_file 
 	 */
 	if (systemcfg->platform != PLATFORM_ISERIES_LPAR) {
 		struct device_node *cpu_node;
-		int *fp;
+		unsigned int *fp;
 
 		cpu_node = of_find_node_by_type(NULL, "cpu");
 		if (cpu_node) {
-			fp = (int *) get_property(cpu_node, "clock-frequency",
+			fp = (unsigned int *) get_property(cpu_node, "clock-frequency",
 						  NULL);
 			if (fp)
 				seq_printf(m, "clock\t\t: %dMHz\n",
@@ -388,7 +411,8 @@ static int show_cpuinfo(struct seq_file 
 		ppc_md.setup_residual(m, cpu_id);
 
 	seq_printf(m, "revision\t: %hd.%hd\n\n", maj, min);
-	
+
+	preempt_enable();
 	return 0;
 }
 
@@ -421,7 +445,6 @@ void parse_cmd_line(unsigned long r3, un
 	if ((initrd_start == 0) && r3 && r4 && r4 != 0xdeadbeef) {
 		initrd_start = (r3 >= KERNELBASE) ? r3 : (unsigned long)__va(r3);
 		initrd_end = initrd_start + r4;
-		ROOT_DEV = Root_RAM0;
 		initrd_below_start_ok = 1;
 	}
 #endif
@@ -546,7 +569,6 @@ int parse_bootinfo(void)
 		case BI_INITRD:
 			initrd_start = (unsigned long)__va(rec->data[0]);
 			initrd_end = initrd_start + rec->data[1];
-			ROOT_DEV = Root_RAM0;
 			initrd_below_start_ok = 1;
 			break;
 #endif /* CONFIG_BLK_DEV_INITRD */
@@ -614,6 +636,9 @@ void __init setup_arch(char **cmdline_p)
 	/* reboot on panic */
 	panic_timeout = 180;
 
+	if (ppc_md.panic)
+		notifier_chain_register(&panic_notifier_list, &ppc64_panic_block);
+
 	init_mm.start_code = PAGE_OFFSET;
 	init_mm.end_code = (unsigned long) _etext;
 	init_mm.end_data = (unsigned long) _edata;
diff -purN linux-2.5/arch/ppc64/kernel/signal.c linuxppc64-2.5/arch/ppc64/kernel/signal.c
--- linux-2.5/arch/ppc64/kernel/signal.c	2004-01-31 08:15:29.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/signal.c	2004-02-01 02:40:22.000000000 +0000
@@ -96,7 +96,7 @@ long sys_rt_sigsuspend(sigset_t *unewset
 	regs->gpr[3] = EINTR;
 	regs->ccr |= 0x10000000;
 	while (1) {
-		current->state = TASK_INTERRUPTIBLE;
+		set_current_state(TASK_INTERRUPTIBLE);
 		schedule();
 		if (do_signal(&saveset, regs))
 			return regs->gpr[3];
diff -purN linux-2.5/arch/ppc64/kernel/signal32.c linuxppc64-2.5/arch/ppc64/kernel/signal32.c
--- linux-2.5/arch/ppc64/kernel/signal32.c	2004-02-25 10:31:13.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/signal32.c	2004-02-26 05:41:43.000000000 +0000
@@ -271,7 +271,7 @@ long sys32_sigsuspend(old_sigset_t mask,
 	regs->gpr[3] = EINTR;
 	regs->ccr |= 0x10000000;
 	while (1) {
-		current->state = TASK_INTERRUPTIBLE;
+		set_current_state(TASK_INTERRUPTIBLE);
 		schedule();
 		if (do_signal32(&saveset, regs))
 			/*
@@ -589,7 +589,7 @@ int sys32_rt_sigsuspend(compat_sigset_t*
 	regs->gpr[3] = EINTR;
 	regs->ccr |= 0x10000000;
 	while (1) {
-		current->state = TASK_INTERRUPTIBLE;
+		set_current_state(TASK_INTERRUPTIBLE);
 		schedule();
 		if (do_signal32(&saveset, regs))
 			/*
diff -purN linux-2.5/arch/ppc64/kernel/smp.c linuxppc64-2.5/arch/ppc64/kernel/smp.c
--- linux-2.5/arch/ppc64/kernel/smp.c	2004-02-23 16:39:11.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/smp.c	2004-03-10 07:17:39.000000000 +0000
@@ -53,6 +53,10 @@
 #include <asm/cputable.h>
 #include <asm/system.h>
 
+#ifdef CONFIG_KDB
+#include <linux/kdb.h>
+#endif
+
 int smp_threads_ready;
 unsigned long cache_decay_ticks;
 
@@ -75,6 +79,23 @@ void smp_call_function_interrupt(void);
 extern long register_vpa(unsigned long flags, unsigned long proc,
 			 unsigned long vpa);
 
+#ifdef CONFIG_KDB
+	/* save regs here before calling kdb_ipi */
+struct pt_regs *kdb_smp_regs[NR_CPUS];
+	
+/* called for each processor.. drop each into kdb. */
+static void smp_kdb_stop_proc(void *dummy)
+{
+    kdb_ipi(kdb_smp_regs[smp_processor_id()], NULL);
+}
+	
+void smp_kdb_stop(void)
+{
+    int ret=0;
+    ret = smp_call_function(smp_kdb_stop_proc, NULL, 1, 0);
+}
+#endif
+
 /* Low level assembly function used to backup CPU 0 state */
 extern void __save_cpu_setup(void);
 
@@ -230,10 +251,302 @@ static void __devinit smp_openpic_setup_
 	do_openpic_setup_cpu();
 }
 
+#ifdef CONFIG_HOTPLUG_CPU
+/* Get state of physical CPU.
+ * Return codes:
+ *	0	- The processor is in the RTAS stopped state
+ *	1	- stop-self is in progress
+ *	2	- The processor is not in the RTAS stopped state
+ *	-1	- Hardware Error
+ *	-2	- Hardware Busy, Try again later.
+ */
+static int query_cpu_stopped(unsigned int pcpu)
+{
+	long cpu_status;
+	int status, qcss_tok;
+
+	qcss_tok = rtas_token("query-cpu-stopped-state");
+	BUG_ON(qcss_tok == RTAS_UNKNOWN_SERVICE);
+	status = rtas_call(qcss_tok, 1, 2, &cpu_status, pcpu);
+	if (status != 0) {
+		printk(KERN_ERR
+		       "RTAS query-cpu-stopped-state failed: %i\n", status);
+		return status;
+	}
+
+	return cpu_status;
+}
+
+int __cpu_disable(void)
+{
+	/* FIXME: go put this in a header somewhere */
+	extern void xics_migrate_irqs_away(void);
+
+	systemcfg->processorCount--;
+
+	/*fix boot_cpuid here*/
+	if (smp_processor_id() == boot_cpuid)
+		boot_cpuid = any_online_cpu(cpu_online_map);
+
+	/* FIXME: abstract this to not be platform specific later on */
+	xics_migrate_irqs_away();
+	return 0; 
+}
+
+void __cpu_die(unsigned int cpu)
+{
+	int tries;
+	int cpu_status;
+	unsigned int pcpu = get_hard_smp_processor_id(cpu);
+
+	for (tries = 0; tries < 5; tries++) {
+		cpu_status = query_cpu_stopped(pcpu);
+
+		if (cpu_status == 0)
+			break;
+		set_current_state(TASK_UNINTERRUPTIBLE);
+		schedule_timeout(HZ);
+	}
+	if (cpu_status != 0) {
+		printk("Querying DEAD? cpu %i (%i) shows %i\n",
+		       cpu, pcpu, cpu_status);
+	}
+
+	/* Isolation and deallocation are definatly done by
+	 * drslot_chrp_cpu.  If they were not they would be
+	 * done here.  Change isolate state to Isolate and
+	 * change allocation-state to Unusable.
+	 */
+	paca[cpu].xProcStart = 0;
+
+	/* So we can recognize if it fails to come up next time. */
+	cpu_callin_map[cpu] = 0;
+}
+
+/* Kill this cpu */
+void cpu_die(void)
+{
+	local_irq_disable();
+	rtas_stop_self();
+	/* Should never get here... */
+	BUG();
+	for(;;);
+}
+
+/* RPA 7.3.7.5 defines the state return values.
+ * 7.3.7.5 and 17.4.3.3 define the status return values. */
+static int check_dr_state(int drc_index)
+{
+	const int sensor = 9003;	/* DR-entity-sense */
+	/* Don't spend forever waiting for RTAS to give us an answer. */
+	unsigned long max_wait_tb = __get_tb() + 5 * tb_ticks_per_sec;
+
+	do {
+		unsigned long state;
+		int status = rtas_call(rtas_token("get-sensor-state"),
+				       2, 2, &state, sensor, drc_index);
+
+		switch (status) {
+		    case -1:	/* Hardware error */
+		    case -3:	/* No such sensor implemented */
+			return -ENOSYS;
+		    case -9000:	/* Need DR entity to be powered & unisolated */
+		    case -9001:	/* Need DR entity to be powered up */
+		    case -9002:	/* Legacy.  Treat as state == 2 (unusable) */
+			return -EBADSLT;
+		    case 0:
+			switch (state) {
+			    case 1:	/* DR entity present */
+				return 0;
+			    case 0:	/* DR connector empty */
+			    case 2:	/* DR entity unusable */
+			    case 3:	/* DR entity available for exchange */
+				return -EBADSLT;
+			    default:	/* shouldn't happen */
+				return -ENOMSG;
+			}
+		    case 9000 ... 9005: {
+			unsigned int wait_time;
+
+			wait_time = rtas_extended_busy_delay_time(status);
+			set_current_state(TASK_UNINTERRUPTIBLE);
+			schedule_timeout(wait_time);
+		    }
+		    default:	/* shouldn't happen */
+			return -ENOMSG;
+		}
+	} while (__get_tb() < max_wait_tb);
+
+	return -ETIMEDOUT;
+}
+
+/* Search all cpu DR entities, looking for one which is present.  If
+ * the same hw index as before is available, grab that in preference.a
+ * Match the dr-index to a cpu node in the device tree.  Use the reg
+ * (hw index) from the node to query rtas if the cpu is in a stopped
+ * state.
+ */
+static unsigned int find_physical_cpu_to_start(unsigned int old_hwindex)
+{
+	int i, idx;
+	int count = 0;
+	int num_addr_cell, num_size_cell, len;
+	struct device_node *np;
+	unsigned int *ireg, *indexes;
+	int status;
+	int dr_state ;
+	unsigned int best = -1U;
+
+	if ((np = find_path_device("/cpus")) == NULL) {
+		printk(KERN_ERR "Could not find /cpus in device tree!");
+		goto out;
+	}
+	num_addr_cell = prom_n_addr_cells(np); 
+	num_size_cell = prom_n_size_cells(np); 
+
+	indexes = (unsigned int *)get_property(np, "ibm,drc-indexes", &len);
+	if (indexes == NULL) {
+		printk(KERN_INFO "Couldn't find ibm,drc-indexes in /cpus\n");
+		goto out;
+	}
+
+	count = indexes[0];
+	for (i = 0; i < count; i++) {
+		idx = indexes[i+1];
+
+		/* Returns 0 for candidate processor slots */
+		dr_state = check_dr_state(idx); 
+		if (dr_state)
+			continue;
+
+		for (np = of_find_node_by_type(NULL, "cpu");
+		     np; np = of_find_node_by_type(np, "cpu")) {
+			ireg = (unsigned int *)
+				get_property(np, "ibm,my-drc-index", &len);
+
+			if (!ireg || ireg[0] != idx)
+				continue;
+			ireg = (unsigned int *)get_property(np, "reg", &len);
+
+			if (!ireg)
+				continue;
+
+			status = query_cpu_stopped(*ireg);
+			if (status == 0) {
+				best = *ireg;
+				if (best == old_hwindex) {
+					of_node_put(np);
+					goto out;
+				}
+			}
+		}
+	}
+ out:
+	return best;
+}
+
+/**
+ * smp_startup_cpu() - start the given cpu
+ *
+ * At boot time, there is nothing to do.  At run-time, call RTAS with
+ * the appropriate start location, if the cpu is in the RTAS stopped
+ * state.
+ *
+ * Returns:
+ *	0	- failure
+ *	1	- success
+ */
+static inline int __devinit smp_startup_cpu(unsigned int lcpu)
+{
+	int status;
+	extern void (*pseries_secondary_smp_init)(unsigned int cpu);
+	unsigned long start_here = __pa(pseries_secondary_smp_init);
+	unsigned int pcpu;
+
+	/* At boot time the cpus are already spinning in hold
+	 * loops, so nothing to do. */
+ 	if (!system_running)
+		return 1;
+
+	pcpu = find_physical_cpu_to_start(get_hard_smp_processor_id(lcpu));
+	if (pcpu == -1U) {
+		printk(KERN_INFO "No more cpus available, failing\n");
+		return 0;
+	}
+
+	/* Fixup atomic count: it exited inside IRQ handler. */
+	((struct task_struct *)paca[lcpu].xCurrent)->thread_info->preempt_count
+		= 0;
+	/* Fixup SLB round-robin so next segment (kernel) goes in segment 0 */
+	paca[lcpu].xStab_data.next_round_robin = 0;
+
+	/* At boot this is done in prom.c. */
+	paca[lcpu].xHwProcNum = pcpu;
+
+	status = rtas_call(rtas_token("start-cpu"), 3, 1, NULL,
+			   pcpu, start_here, lcpu);
+	if (status != 0) {
+		printk(KERN_ERR "start-cpu failed: %i\n", status);
+		return 0;
+	}
+	return 1;
+}
+
+static inline void look_for_more_cpus(void)
+{
+	int num_addr_cell, num_size_cell, len, i, maxcpus;
+	struct device_node *np;
+	unsigned int *ireg;
+
+	/* Find the property which will tell us about how many CPUs
+	 * we're allowed to have. */
+	if ((np = find_path_device("/rtas")) == NULL) {
+		printk(KERN_ERR "Could not find /rtas in device tree!");
+		return;
+	}
+	num_addr_cell = prom_n_addr_cells(np); 
+	num_size_cell = prom_n_size_cells(np); 
+
+	ireg = (unsigned int *)get_property(np, "ibm,lrdr-capacity", &len);
+	if (ireg == NULL) {
+		printk(KERN_INFO "Couldn't find ibm,lrdr-capacity in /rtas\n");
+		/* FIXME: make sure not marked as lrdr_capable() */
+		return;
+	}
+
+	maxcpus = ireg[num_addr_cell + num_size_cell];
+	/* DRENG need to account for threads here too */
+
+	if (maxcpus > NR_CPUS) {
+		printk(KERN_WARNING
+		       "Partition configured for %d cpus, "
+		       "operating system maximum is %d.\n", maxcpus, NR_CPUS);
+		maxcpus = NR_CPUS;
+	} else
+		printk(KERN_INFO "Partition configured for %d cpus.\n",
+		       maxcpus);
+
+	/* Make those cpus (which might appear later) possible too. */
+	for (i = 0; i < maxcpus; i++)
+		cpu_set(i, cpu_possible_map);
+}
+#else /* ... CONFIG_HOTPLUG_CPU */
+static inline int __devinit smp_startup_cpu(unsigned int lcpu)
+{
+	return 1;
+}
+static inline void look_for_more_cpus(void)
+{
+}
+#endif /* CONFIG_HOTPLUG_CPU */
+
 static void smp_pSeries_kick_cpu(int nr)
 {
 	BUG_ON(nr < 0 || nr >= NR_CPUS);
 
+	if (!smp_startup_cpu(nr))
+		return;
+
 	/* The processor is currently spinning, waiting
 	 * for the xProcStart field to become non-zero
 	 * After we set xProcStart, the processor will
@@ -241,7 +554,7 @@ static void smp_pSeries_kick_cpu(int nr)
 	 */
 	paca[nr].xProcStart = 1;
 }
-#endif
+#endif /* CONFIG_PPC_PSERIES */
 
 static void __init smp_space_timers(unsigned int max_cpus)
 {
@@ -378,6 +691,9 @@ void smp_message_recv(int msg, struct pt
 {
 	switch(msg) {
 	case PPC_MSG_CALL_FUNCTION:
+#ifdef CONFIG_KDB
+	        kdb_smp_regs[smp_processor_id()]=regs;
+#endif
 		smp_call_function_interrupt();
 		break;
 	case PPC_MSG_RESCHEDULE: 
@@ -462,12 +778,9 @@ int smp_call_function (void (*func) (voi
 		       int wait)
 { 
 	struct call_data_struct data;
-	int ret = -1, cpus = num_online_cpus()-1;
+	int ret = -1, cpus;
 	unsigned long timeout;
 
-	if (!cpus)
-		return 0;
-
 	data.func = func;
 	data.info = info;
 	atomic_set(&data.started, 0);
@@ -476,6 +789,14 @@ int smp_call_function (void (*func) (voi
 		atomic_set(&data.finished, 0);
 
 	spin_lock(&call_lock);
+	/* Must grab online cpu count with preempt disabled, otherwise
+	 * it can change. */
+	cpus = num_online_cpus() - 1;
+	if (!cpus) {
+		ret = 0;
+		goto out;
+	}
+
 	call_data = &data;
 	wmb();
 	/* Send a message to all other CPUs and wait for them to respond */
@@ -565,8 +886,31 @@ static void __devinit smp_store_cpu_info
 	per_cpu(pvr, id) = _get_PVR();
 }
 
+static void __init smp_create_idle(unsigned int cpu)
+{
+	struct pt_regs regs;
+	struct task_struct *p;
+
+	/* create a process for the processor */
+	/* only regs.msr is actually used, and 0 is OK for it */
+	memset(&regs, 0, sizeof(struct pt_regs));
+	p = copy_process(CLONE_VM | CLONE_IDLETASK,
+			 0, &regs, 0, NULL, NULL);
+	if (IS_ERR(p))
+		panic("failed fork for CPU %u: %li", cpu, PTR_ERR(p));
+
+	wake_up_forked_process(p);
+	init_idle(p, cpu);
+	unhash_process(p);
+
+	paca[cpu].xCurrent = (u64)p;
+	current_set[cpu] = p->thread_info;
+}
+
 void __init smp_prepare_cpus(unsigned int max_cpus)
 {
+	unsigned int cpu;
+
 	/* 
 	 * setup_cpu may need to be called on the boot cpu. We havent
 	 * spun any cpus up but lets be paranoid.
@@ -593,6 +937,8 @@ void __init smp_prepare_cpus(unsigned in
 	 * number of msecs off until someone does a settimeofday()
 	 */
 	do_gtod.tb_orig_stamp = tb_last_stamp;
+
+	look_for_more_cpus();
 #endif
 
 	max_cpus = smp_ops->probe();
@@ -601,20 +947,31 @@ void __init smp_prepare_cpus(unsigned in
 	__save_cpu_setup();
 
 	smp_space_timers(max_cpus);
+
+	for_each_cpu(cpu)
+		if (cpu != boot_cpuid)
+			smp_create_idle(cpu);
 }
 
 void __devinit smp_prepare_boot_cpu(void)
 {
-	cpu_set(smp_processor_id(), cpu_online_map);
-	/* FIXME: what about cpu_possible()? */
+	BUG_ON(smp_processor_id() != boot_cpuid);
+
+	/* cpu_possible is set up in prom.c */
+	cpu_set(boot_cpuid, cpu_online_map);
+
+	paca[boot_cpuid].xCurrent = (u64)current;
+	current_set[boot_cpuid] = current->thread_info;
 }
 
 int __devinit __cpu_up(unsigned int cpu)
 {
-	struct pt_regs regs;
-	struct task_struct *p;
 	int c;
 
+	/* At boot, don't bother with non-present cpus -JSCHOPP */
+	if (!system_running && !cpu_present_at_boot(cpu))
+		return -ENOENT;
+
 	paca[cpu].prof_counter = 1;
 	paca[cpu].prof_multiplier = 1;
 	paca[cpu].default_decr = tb_ticks_per_jiffy / decr_overclock;
@@ -632,19 +989,9 @@ int __devinit __cpu_up(unsigned int cpu)
 		paca[cpu].xStab_data.real = (unsigned long)__v2a(tmp);
 	}
 
-	/* create a process for the processor */
-	/* only regs.msr is actually used, and 0 is OK for it */
-	memset(&regs, 0, sizeof(struct pt_regs));
-	p = copy_process(CLONE_VM|CLONE_IDLETASK, 0, &regs, 0, NULL, NULL);
-	if (IS_ERR(p))
-		panic("failed fork for CPU %u: %li", cpu, PTR_ERR(p));
-
-	wake_up_forked_process(p);
-	init_idle(p, cpu);
-	unhash_process(p);
-
-	paca[cpu].xCurrent = (u64)p;
-	current_set[cpu] = p->thread_info;
+	/* The information for processor bringup must be written out
+	 * to main store before we release the processor. */
+	mb();
 
 	/* The information for processor bringup must
 	 * be written out to main store before we release
@@ -676,6 +1023,7 @@ int __devinit __cpu_up(unsigned int cpu)
 	return 0;
 }
 
+extern unsigned int default_distrib_server;
 /* Activate a secondary processor. */
 int __devinit start_secondary(void *unused)
 {
@@ -698,6 +1046,15 @@ int __devinit start_secondary(void *unus
 	if (cur_cpu_spec->firmware_features & FW_FEATURE_SPLPAR) {
 		vpa_init(cpu); 
 	}
+
+#ifdef CONFIG_IRQ_ALL_CPUS
+	/* Put the calling processor into the GIQ.  This is really only 
+	 * necessary from a secondary thread as the OF start-cpu interface
+	 * performs this function for us on primary threads.
+	 */
+	/* TODO: 9005 is #defined in rtas-proc.c -- move to a header */
+	rtas_set_indicator(9005, default_distrib_server, 1);
+#endif
 #endif
 
 	local_irq_enable();
diff -purN linux-2.5/arch/ppc64/kernel/vio.c linuxppc64-2.5/arch/ppc64/kernel/vio.c
--- linux-2.5/arch/ppc64/kernel/vio.c	2004-03-16 11:30:36.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/vio.c	2004-03-17 00:46:46.000000000 +0000
@@ -25,7 +25,6 @@
 #include <asm/ppcdebug.h>
 #include <asm/vio.h>
 #include <asm/hvcall.h>
-#include "open_pic.h"
 
 #define DBGENTER() pr_debug("%s entered\n", __FUNCTION__)
 
@@ -255,7 +254,7 @@ struct vio_dev * __devinit vio_register_
 			printk(KERN_ERR "Unable to allocate interrupt "
 			       "number for %s\n", of_node->full_name);
 		} else
-			viodev->irq = openpic_to_irq(virq);
+			viodev->irq = irq_offset_up(virq);
 	}
 
 	/* init generic 'struct device' fields: */
diff -purN linux-2.5/arch/ppc64/kernel/vioconfig.c linuxppc64-2.5/arch/ppc64/kernel/vioconfig.c
--- linux-2.5/arch/ppc64/kernel/vioconfig.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/vioconfig.c	2004-03-16 20:52:15.000000000 +0000
@@ -0,0 +1,89 @@
+/* ------------------------------------------------------------
+ * vioconfig.c
+ * (C) Copyright IBM Corporation 1994, 2004
+ * Author: Dave Boutcher (sleddog@us.ibm.com)
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307
+ * USA
+ *
+ * ------------------------------------------------------------
+ * proc file to display configuration information retrieved froma vio
+ * host
+ */
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/proc_fs.h>
+#include <asm/vio.h>
+
+static struct proc_dir_entry *vio_config;
+
+extern int ibmvscsi_get_host_config(struct vio_dev *vdev,
+				    unsigned char *buffer, 
+				    int length);
+
+static int proc_read(char *buf, char **start, off_t offset,
+		     int blen, int *eof, void *data)
+{
+	struct device_node *node_vroot, *of_node;
+	struct vio_dev * vdev;
+	
+	node_vroot = find_devices("vdevice");
+	if ((node_vroot == NULL) || (node_vroot->child == NULL)) {
+		/* this machine doesn't do virtual IO, and that's ok */
+		return 0;
+	}
+
+	/*
+	 * loop through all vdevices
+	 */
+	for (of_node = node_vroot->child;
+			of_node != NULL;
+			of_node = of_node->sibling) {
+		/* see if this is a vscsi device */
+		if ((of_node->type != NULL) &&
+		    (strncmp(of_node->type, "vscsi", 5) == 0)) {
+			/* see if we get a vdevice for that */
+			vdev = vio_find_node(of_node);
+			
+			/* Finally see if we get host config data */
+			if (ibmvscsi_get_host_config(vdev, buf, blen) == 0) {
+				break;
+			}
+		}
+	}
+
+	*eof = 1;
+	return strlen(buf);
+}
+
+int __init vioconfig_module_init(void)
+{
+	vio_config = create_proc_read_entry("vioconfig",
+					    S_IFREG | S_IRUSR,
+					    NULL,
+					    proc_read,
+					    NULL);
+	if (!vio_config)
+		return -1;
+	return 0;
+}
+
+void __exit vioconfig_module_exit(void)
+{
+	remove_proc_entry("vioconfig", vio_config->parent);
+}	
+
+module_init(vioconfig_module_init);
+module_exit(vioconfig_module_exit);
diff -purN linux-2.5/arch/ppc64/kernel/vmlinux.lds.S linuxppc64-2.5/arch/ppc64/kernel/vmlinux.lds.S
--- linux-2.5/arch/ppc64/kernel/vmlinux.lds.S	2004-01-19 06:28:28.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/vmlinux.lds.S	2004-03-05 13:02:02.000000000 +0000
@@ -84,6 +84,10 @@ SECTIONS
 	__con_initcall_end = .;
 	}
 
+  __kdb_initcall_start = .;
+  .kdb_initcall.init : { *(.kdb_initcall.init) }
+  __kdb_initcall_end = .;
+
   SECURITY_INIT
 
   . = ALIGN(4096);
diff -purN linux-2.5/arch/ppc64/kernel/vpurr.c linuxppc64-2.5/arch/ppc64/kernel/vpurr.c
--- linux-2.5/arch/ppc64/kernel/vpurr.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/vpurr.c	2004-03-20 10:56:41.000000000 +0000
@@ -0,0 +1,101 @@
+/*
+ * PPC64 Cpu util performace monitoring.
+ *
+ * Manish Ahuja mahuja@us.ibm.com
+ *    Copyright (c) 2004 Manish Ahuja IBM CORP.
+ *
+ *      This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ *
+ *	This file will also report many of the perf values for 2.6 
+ */
+
+#include <linux/config.h>
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <linux/proc_fs.h>
+#include <linux/init.h>
+#include <asm/uaccess.h>
+#include <asm/hvcall.h>
+#include <asm/cputable.h>
+#include "vpurr.h"
+
+#define SAMPLE_TICK HZ
+
+DEFINE_PER_CPU(struct cpu_util_store, cpu_util_sampler);
+
+static void collect_startpurr(int cpu);
+
+/*
+ * This is a timer handler.  There is on per CPU. It gets scheduled
+ * every SAMPLE_TICK ticks.
+ */
+
+static void util_timer_func(unsigned long data)
+{
+	struct cpu_util_store * cus = &__get_cpu_var(cpu_util_sampler);
+	struct timer_list *tl = &cus->cpu_util_timer;
+
+	cus->current_purr = mfspr(PURR);
+	cus->tb = mftb();
+
+	/*printk(KERN_INFO "PURR VAL %ld %lld %lld\n", data, cus->current_purr, cus->tb);*/
+
+	mod_timer(tl, jiffies + SAMPLE_TICK);
+}
+
+/*
+ * One time function that gets called when all the cpu's are online 
+ * to start collection. It adds the timer to each cpu on the system.
+ * start_purr is collected during smp_init time in __cpu_up code
+ */
+
+static void start_util_timer(int cpu)
+{
+	struct cpu_util_store * cus = &per_cpu(cpu_util_sampler, cpu);
+	struct timer_list *tl = &cus->cpu_util_timer;
+
+	if (tl->function != NULL)
+		return;
+
+	init_timer(tl);
+	tl->expires = jiffies + SAMPLE_TICK;
+	tl->data = cpu;
+	tl->function = util_timer_func;
+	add_timer_on(tl, cpu);
+}
+
+static int __init cpu_util_init(void)
+{
+	int cpu;
+
+	if (PVR_VER(systemcfg->processor) == PV_POWER5) {
+		for_each_online_cpu(cpu){
+			collect_startpurr(cpu);
+			start_util_timer(cpu);
+		}
+	}
+
+	return 0;
+}
+
+__initcall(cpu_util_init);
+
+/* Collect starting purr, to collect starting purr from the
+ * cpu in question, we make a call to get that cpu and then run
+ */
+
+static void collect_startpurr(int cpu)
+{
+	struct cpu_util_store * cus = &per_cpu(cpu_util_sampler, cpu);	
+
+	set_cpus_allowed(current, cpumask_of_cpu(cpu));
+	BUG_ON(smp_processor_id() != cpu);
+
+	cus->start_purr = mfspr(PURR);
+	cus->tb = mftb();
+}
+
diff -purN linux-2.5/arch/ppc64/kernel/vpurr.h linuxppc64-2.5/arch/ppc64/kernel/vpurr.h
--- linux-2.5/arch/ppc64/kernel/vpurr.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/vpurr.h	2004-03-20 10:56:41.000000000 +0000
@@ -0,0 +1,26 @@
+/*
+ *    Copyright (c) 2004 Manish Ahuja <mahuja@us.ibm.com> IBM CORP.
+ *
+ *    Module name: vpurr.h
+ *
+ *    Description:
+ *      Architecture- / platform-specific boot-time initialization code for
+ *      tracking purr utilization and other performace features in coming 
+ * 	releases for splpar/smt machines.
+ *
+ *      This program is free software; you can redistribute it and/or
+ *      modify it under the terms of the GNU General Public License
+ *      as published by the Free Software Foundation; either version
+ *      2 of the License, or (at your option) any later version.
+ */
+
+#define PURR 309
+
+DECLARE_PER_CPU(struct cpu_util_store, cpu_util_sampler);
+
+struct cpu_util_store {
+        struct timer_list cpu_util_timer;
+        u64 start_purr;
+        u64 current_purr;
+        u64 tb;
+};
diff -purN linux-2.5/arch/ppc64/kernel/xics.c linuxppc64-2.5/arch/ppc64/kernel/xics.c
--- linux-2.5/arch/ppc64/kernel/xics.c	2004-03-16 11:30:37.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/kernel/xics.c	2004-03-18 12:26:28.000000000 +0000
@@ -19,6 +19,7 @@
 #include <linux/init.h>
 #include <linux/gfp.h>
 #include <linux/radix-tree.h>
+#include <linux/cpu.h>
 #include <asm/prom.h>
 #include <asm/io.h>
 #include <asm/pgtable.h>
@@ -58,7 +59,6 @@ struct hw_interrupt_type xics_8259_pic =
 static struct radix_tree_root irq_map = RADIX_TREE_INIT(GFP_KERNEL);
 
 #define XICS_IPI		2
-#define XICS_IRQ_OFFSET		0x10
 #define XICS_IRQ_SPURIOUS	0
 
 /* Want a priority other than 0.  Various HW issues require this. */
@@ -91,7 +91,8 @@ static struct xics_ipl *xics_per_cpu[NR_
 static int xics_irq_8259_cascade = 0;
 static int xics_irq_8259_cascade_real = 0;
 static unsigned int default_server = 0xFF;
-static unsigned int default_distrib_server = 0;
+/* also referenced in smp.c... */
+unsigned int default_distrib_server = 0;
 
 /*
  * XICS only has a single IPI, so encode the messages per CPU
@@ -216,7 +217,7 @@ xics_ops pSeriesLP_ops = {
 
 static unsigned int xics_startup(unsigned int virq)
 {
-	virq -= XICS_IRQ_OFFSET;
+	virq = irq_offset_down(virq);
 	if (radix_tree_insert(&irq_map, virt_irq_to_real(virq),
 			      &virt_irq_to_real_map[virq]) == -ENOMEM)
 		printk(KERN_CRIT "Out of memory creating real -> virtual"
@@ -235,26 +236,50 @@ static unsigned int real_irq_to_virt(uns
 	return ptr - virt_irq_to_real_map;
 }
 
+/* XXX Fix this when we clean up large irq support */
+extern cpumask_t get_irq_affinity(unsigned int irq);
+
+static int get_irq_server(unsigned int irq)
+{
+	cpumask_t cpumask = irq_affinity[irq];
+	cpumask_t allcpus = CPU_MASK_ALL;
+	cpumask_t tmp = CPU_MASK_NONE;
+	unsigned int server;
+	
+#ifdef CONFIG_IRQ_ALL_CPUS
+	/* For the moment only implement delivery to all cpus or one cpu */
+	if (smp_threads_ready) {
+		if (cpus_equal(cpumask, allcpus)) {
+			server = default_distrib_server;
+		} else {
+			cpus_and(tmp, cpu_online_map, cpumask);
+
+			if (cpus_empty(tmp))
+				server = default_distrib_server;
+			else
+				server = get_hard_smp_processor_id(first_cpu(tmp));
+		}
+	} else {
+		server = default_server;
+	}
+#else
+	server = default_server;
+#endif
+	return server;
+
+}
+
 static void xics_enable_irq(unsigned int virq)
 {
 	unsigned int irq;
 	long call_status;
 	unsigned int server;
 
-	virq -= XICS_IRQ_OFFSET;
-	irq = virt_irq_to_real(virq);
+	irq = virt_irq_to_real(irq_offset_down(virq));
 	if (irq == XICS_IPI)
 		return;
 
-#ifdef CONFIG_IRQ_ALL_CPUS
-	if (smp_threads_ready)
-		server = default_distrib_server;
-	else
-		server = default_server;
-#else
-	server = default_server;
-#endif
-
+	server = get_irq_server(virq);
 	call_status = rtas_call(ibm_set_xive, 3, 1, NULL, irq, server,
 				DEFAULT_PRIORITY);
 	if (call_status != 0) {
@@ -275,6 +300,7 @@ static void xics_enable_irq(unsigned int
 static void xics_disable_real_irq(unsigned int irq)
 {
 	long call_status;
+	unsigned int server;
 
 	if (irq == XICS_IPI)
 		return;
@@ -286,9 +312,9 @@ static void xics_disable_real_irq(unsign
 		return;
 	}
 
+	server = get_irq_server(irq);
 	/* Have to set XIVE to 0xff to be able to remove a slot */
-	call_status = rtas_call(ibm_set_xive, 3, 1, NULL, irq, default_server,
-				0xff);
+	call_status = rtas_call(ibm_set_xive, 3, 1, NULL, irq, server, 0xff);
 	if (call_status != 0) {
 		printk(KERN_ERR "xics_disable_irq: irq=%x: ibm_set_xive(0xff)"
 		       " returned %lx\n", irq, call_status);
@@ -300,25 +326,25 @@ static void xics_disable_irq(unsigned in
 {
 	unsigned int irq;
 
-	virq -= XICS_IRQ_OFFSET;
-	irq = virt_irq_to_real(virq);
+	irq = virt_irq_to_real(irq_offset_down(virq));
 	xics_disable_real_irq(irq);
 }
 
-static void xics_end_irq(unsigned int	irq)
+static void xics_end_irq(unsigned int irq)
 {
 	int cpu = smp_processor_id();
 
 	iosync();
-	ops->xirr_info_set(cpu, ((0xff<<24) |
-				 (virt_irq_to_real(irq-XICS_IRQ_OFFSET))));
+	ops->xirr_info_set(cpu, ((0xff << 24) |
+				 (virt_irq_to_real(irq_offset_down(irq)))));
+
 }
 
 static void xics_mask_and_ack_irq(unsigned int irq)
 {
 	int cpu = smp_processor_id();
 
-	if (irq < XICS_IRQ_OFFSET) {
+	if (irq < irq_offset_value()) {
 		i8259_pic.ack(irq);
 		iosync();
 		ops->xirr_info_set(cpu, ((0xff<<24) |
@@ -344,7 +370,8 @@ int xics_get_irq(struct pt_regs *regs)
 		irq = i8259_irq(cpu);
 		if (irq == -1) {
 			/* Spurious cascaded interrupt.  Still must ack xics */
-                        xics_end_irq(XICS_IRQ_OFFSET + xics_irq_8259_cascade);
+			xics_end_irq(irq_offset_up(xics_irq_8259_cascade));
+
 			irq = -1;
 		}
 	} else if (vec == XICS_IRQ_SPURIOUS) {
@@ -358,7 +385,7 @@ int xics_get_irq(struct pt_regs *regs)
 			       " disabling it.\n", vec);
 			xics_disable_real_irq(vec);
 		} else
-			irq += XICS_IRQ_OFFSET;
+			irq = irq_offset_up(irq);
 	}
 	return irq;
 }
@@ -372,6 +399,11 @@ irqreturn_t xics_ipi_action(int irq, voi
 	int cpu = smp_processor_id();
 
 	ops->qirr_info(cpu, 0xff);
+	if (cpu_is_offline(cpu))
+		printk("xics_ipi_action %08lx on %i\n",
+		       xics_ipi_message[cpu].value,
+		       cpu);
+
 	while (xics_ipi_message[cpu].value) {
 		if (test_and_clear_bit(PPC_MSG_CALL_FUNCTION,
 				       &xics_ipi_message[cpu].value)) {
@@ -514,6 +546,9 @@ nextnode:
 	if (systemcfg->platform == PLATFORM_PSERIES) {
 #ifdef CONFIG_SMP
 		for_each_cpu(i) {
+			/* FIXME: Do this dynamically! --RR */
+			if (!cpu_present_at_boot(i))
+				continue;
 			xics_per_cpu[i] = __ioremap((ulong)inodes[get_hard_smp_processor_id(i)].addr, 
 						    (ulong)inodes[get_hard_smp_processor_id(i)].size,
 						    _PAGE_NO_CACHE);
@@ -534,9 +569,9 @@ nextnode:
 	xics_8259_pic.enable = i8259_pic.enable;
 	xics_8259_pic.disable = i8259_pic.disable;
 	for (i = 0; i < 16; ++i)
-		irq_desc[i].handler = &xics_8259_pic;
+		get_irq_desc(i)->handler = &xics_8259_pic;
 	for (; i < NR_IRQS; ++i)
-		irq_desc[i].handler = &xics_pic;
+		get_irq_desc(i)->handler = &xics_pic;
 
 	ops->cppr_info(boot_cpuid, 0xff);
 	iosync();
@@ -552,7 +587,7 @@ static int __init xics_setup_i8259(void)
 {
 	if (naca->interrupt_controller == IC_PPC_XIC &&
 	    xics_irq_8259_cascade != -1) {
-		if (request_irq(xics_irq_8259_cascade + XICS_IRQ_OFFSET,
+		if (request_irq(irq_offset_up(xics_irq_8259_cascade),
 				no_action, 0, "8259 cascade", 0))
 			printk(KERN_ERR "xics_init_IRQ: couldn't get 8259 cascade\n");
 		i8259_init();
@@ -567,36 +602,31 @@ void xics_request_IPIs(void)
 	virt_irq_to_real_map[XICS_IPI] = XICS_IPI;
 
 	/* IPIs are marked SA_INTERRUPT as they must run with irqs disabled */
-	request_irq(XICS_IPI + XICS_IRQ_OFFSET, xics_ipi_action, SA_INTERRUPT,
+	request_irq(irq_offset_up(XICS_IPI), xics_ipi_action, SA_INTERRUPT,
 		    "IPI", 0);
-	irq_desc[XICS_IPI+XICS_IRQ_OFFSET].status |= IRQ_PER_CPU;
+	get_irq_desc(irq_offset_up(XICS_IPI))->status |= IRQ_PER_CPU;
 }
 #endif
 
 static void xics_set_affinity(unsigned int virq, cpumask_t cpumask)
 {
-        irq_desc_t *desc = irq_desc + virq;
 	unsigned int irq;
-	unsigned long flags;
 	long status;
 	unsigned long xics_status[2];
 	unsigned long newmask;
 	cpumask_t allcpus = CPU_MASK_ALL;
 	cpumask_t tmp = CPU_MASK_NONE;
 
-	virq -= XICS_IRQ_OFFSET;
-	irq = virt_irq_to_real(virq);
+	irq = virt_irq_to_real(irq_offset_down(virq));
 	if (irq == XICS_IPI)
 		return;
 
-        spin_lock_irqsave(&desc->lock, flags);
-
 	status = rtas_call(ibm_get_xive, 1, 3, (void *)&xics_status, irq);
 
 	if (status) {
 		printk(KERN_ERR "xics_set_affinity: irq=%d ibm,get-xive "
 		       "returns %ld\n", irq, status);
-		goto out;
+		return;
 	}
 
 	/* For the moment only implement delivery to all cpus or one cpu */
@@ -605,8 +635,8 @@ static void xics_set_affinity(unsigned i
 	} else {
 		cpus_and(tmp, cpu_online_map, cpumask);
 		if (cpus_empty(tmp))
-			goto out;
-		newmask = get_hard_smp_processor_id(first_cpu(cpumask));
+			return;
+		newmask = get_hard_smp_processor_id(first_cpu(tmp));
 	}
 
 	status = rtas_call(ibm_set_xive, 3, 1, NULL,
@@ -615,9 +645,86 @@ static void xics_set_affinity(unsigned i
 	if (status) {
 		printk(KERN_ERR "xics_set_affinity irq=%d ibm,set-xive "
 		       "returns %ld\n", irq, status);
-		goto out;
+		return;
+	}
+}
+
+#ifdef CONFIG_HOTPLUG_CPU
+
+/* Interrupts are disabled. */
+void xics_migrate_irqs_away(void)
+{
+	int set_indicator = rtas_token("set-indicator");
+	const unsigned long giqs = 9005UL; /* Global Interrupt Queue Server */
+	unsigned long status = 0;
+	unsigned int irq, cpu = smp_processor_id();
+	unsigned long xics_status[2];
+	unsigned long flags;
+
+	BUG_ON(set_indicator == RTAS_UNKNOWN_SERVICE);
+
+	/* Reject any interrupt that was queued to us... */
+	ops->cppr_info(cpu, 0);
+	iosync();
+
+	/* Refuse any new interrupts... */
+	rtas_call(set_indicator, 3, 1, &status, giqs,
+		  hard_smp_processor_id(), 0UL);
+	WARN_ON(status != 0);
+
+	/* Allow IPIs again... */
+	ops->cppr_info(cpu, DEFAULT_PRIORITY);
+	iosync();
+
+	printk(KERN_WARNING "HOTPLUG: Migrating IRQs away\n");
+	for_each_irq(irq) {
+		irq_desc_t *desc = get_irq_desc(irq);
+
+		/* We need to get IPIs still. */
+		if (irq_offset_down(irq) == XICS_IPI)
+			continue;
+
+		/* We only need to migrate enabled IRQS */
+		if (desc == NULL || desc->handler == NULL
+		    || desc->action == NULL
+		    || desc->handler->set_affinity == NULL)
+			continue;
+
+		spin_lock_irqsave(&desc->lock, flags);
+
+		status = rtas_call(ibm_get_xive, 1, 3, (void *)&xics_status,
+				   irq);
+		if (status) {
+			printk(KERN_ERR "migrate_irqs_away: irq=%d "
+					"ibm,get-xive returns %ld\n",
+					irq, status);
+			goto unlock;
+		}
+
+		/* 
+		 * We only support delivery to all cpus or to one cpu.
+		 * The irq has to be migrated only in the single cpu
+		 * case.
+		 */
+		if (xics_status[0] != get_hard_smp_processor_id(cpu))
+			goto unlock;
+
+		printk(KERN_WARNING "IRQ %d affinity broken off cpu %u\n",
+		       irq, cpu);
+
+		/* Reset affinity to all cpus */
+		xics_status[0] = default_distrib_server;
+
+		status = rtas_call(ibm_set_xive, 3, 1, NULL,
+				irq, xics_status[0], xics_status[1]);
+		if (status)
+			printk(KERN_ERR "migrate_irqs_away irq=%d "
+					"ibm,set-xive returns %ld\n",
+					irq, status);
+
+unlock:
+		spin_unlock_irqrestore(&desc->lock, flags);
 	}
 
-out:
-        spin_unlock_irqrestore(&desc->lock, flags);
 }
+#endif
diff -purN linux-2.5/arch/ppc64/lib/Makefile linuxppc64-2.5/arch/ppc64/lib/Makefile
--- linux-2.5/arch/ppc64/lib/Makefile	2003-06-10 13:09:57.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/lib/Makefile	2004-03-19 03:01:36.000000000 +0000
@@ -4,3 +4,6 @@
 
 lib-y := checksum.o dec_and_lock.o string.o strcase.o
 lib-y += copypage.o memcpy.o copyuser.o
+
+lib-$(CONFIG_PPC_ISERIES) += locks.o
+lib-$(CONFIG_PPC_SPLPAR)  += locks.o
diff -purN linux-2.5/arch/ppc64/lib/locks.c linuxppc64-2.5/arch/ppc64/lib/locks.c
--- linux-2.5/arch/ppc64/lib/locks.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/lib/locks.c	2004-03-19 02:52:48.000000000 +0000
@@ -0,0 +1,90 @@
+/*
+ * Slow paths for spin and read/write lock operations.
+ *
+ * Copyright (C) 2001 Paul Mackerras <paulus@au.ibm.com>, IBM
+ * Copyright (C) 2001 Anton Blanchard <anton@au.ibm.com>, IBM
+ * Copyright (C) 2002 Dave Engebretsen <engebret@us.ibm.com>, IBM
+ *   Rework to support virtual processors
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/config.h>
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+#include <linux/module.h>
+#include <asm/hvcall.h>
+#include <asm/iSeries/HvCall.h>
+
+/*
+ * On a system with shared processors (that is, where a physical
+ * processor is multiplexed between several virtual processors),
+ * there is no point spinning on a lock if the holder of the lock
+ * isn't currently scheduled on a physical processor.  Instead
+ * we detect this situation and ask the hypervisor to give the
+ * rest of our timeslice to the lock holder.
+ */
+
+/* waiting for a spinlock... */
+void __spin_yield(spinlock_t *lock)
+{
+	unsigned int lock_value, holder_cpu, yield_count;
+	struct paca_struct *holder_paca;
+
+	lock_value = lock->lock;
+	if (lock_value == 0)
+		return;
+	holder_cpu = lock_value & 0xffff;
+	BUG_ON(holder_cpu >= NR_CPUS);
+	holder_paca = &paca[holder_cpu];
+	yield_count = holder_paca->xLpPaca.xYieldCount;
+	if ((yield_count & 1) == 0)
+		return;		/* virtual cpu is currently running */
+	rmb();
+	if (lock->lock != lock_value)
+		return;		/* something has changed */
+#ifdef CONFIG_PPC_ISERIES
+	HvCall2(HvCallBaseYieldProcessor, HvCall_YieldToProc,
+		((u64)holder_cpu << 32) | yield_count);
+#else
+	plpar_hcall_norets(H_CONFER, holder_cpu, yield_count);
+#endif
+}
+
+EXPORT_SYMBOL(__spin_yield);
+
+/*
+ * Waiting for a read lock or a write lock on a rwlock...
+ * This turns out to be the same for read and write locks, since
+ * we only know the holder if it is write-locked.
+ */
+void __rw_yield(rwlock_t *rw)
+{
+	int lock_value;
+	unsigned int holder_cpu, yield_count;
+	struct paca_struct *holder_paca;
+
+	lock_value = rw->lock;
+	if (lock_value >= 0)
+		return;		/* no write lock at present */
+	holder_cpu = lock_value & 0xffff;
+	BUG_ON(holder_cpu >= NR_CPUS);
+	holder_paca = &paca[holder_cpu];
+	yield_count = holder_paca->xLpPaca.xYieldCount;
+	if ((yield_count & 1) == 0)
+		return;		/* virtual cpu is currently running */
+	rmb();
+	if (rw->lock != lock_value)
+		return;		/* something has changed */
+#ifdef CONFIG_PPC_ISERIES
+	HvCall2(HvCallBaseYieldProcessor, HvCall_YieldToProc,
+		((u64)holder_cpu << 32) | yield_count);
+#else
+	plpar_hcall_norets(H_CONFER, holder_cpu, yield_count);
+#endif
+}
+
+EXPORT_SYMBOL(__rw_yield);
diff -purN linux-2.5/arch/ppc64/mm/numa.c linuxppc64-2.5/arch/ppc64/mm/numa.c
--- linux-2.5/arch/ppc64/mm/numa.c	2004-03-19 05:59:29.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/mm/numa.c	2004-03-20 11:56:08.000000000 +0000
@@ -71,6 +71,11 @@ static int __init parse_numa_properties(
 		return -1;
 	}
 
+	if (strstr(saved_command_line, "numa=off")) {
+		printk(KERN_WARNING "NUMA disabled by user\n");
+		return -1;
+	}
+
 	cpu = of_find_node_by_type(NULL, "cpu");
 	if (!cpu)
 		goto err;
@@ -133,6 +138,7 @@ static int __init parse_numa_properties(
 		unsigned long size = 0;
 		int numa_domain;
 		int ranges;
+		int propsize;
 
 		tmp1 = (int *)get_property(memory, "reg", NULL);
 		if (!tmp1)
@@ -159,10 +165,21 @@ new_range:
 		if ((start + size) > MAX_MEMORY)
 			BUG();
 
+		/* Some versions of OF sometimes have an empty property for
+		 * associativity, so we need to get the size too.
+		 */
 		tmp2 = (int *)get_property(memory, "ibm,associativity",
-					   NULL);
+					   &propsize);
 		if (!tmp2)
 			continue;
+
+		if (!propsize) {
+			printk(KERN_INFO "Buggy OF? Empty ibm,associativity "
+			       "property for %s. Disabling NUMA.\n",
+			       memory->full_name);
+			goto err;
+		}
+
 		numa_domain = tmp2[depth];
 
 		/* FIXME */
diff -purN linux-2.5/arch/ppc64/xmon/start.c linuxppc64-2.5/arch/ppc64/xmon/start.c
--- linux-2.5/arch/ppc64/xmon/start.c	2004-02-14 11:48:01.000000000 +0000
+++ linuxppc64-2.5/arch/ppc64/xmon/start.c	2004-03-19 21:49:51.000000000 +0000
@@ -19,6 +19,7 @@
 #include <asm/processor.h>
 #include <asm/udbg.h>
 #include <asm/system.h>
+#include "nonstdio.h"
 
 #ifdef CONFIG_MAGIC_SYSRQ
 
@@ -63,9 +64,8 @@ xmon_read_poll(void)
 	return udbg_getc_poll();
 }
  
-void *xmon_stdin;
-void *xmon_stdout;
-void *xmon_stderr;
+FILE *xmon_stdin;
+FILE *xmon_stdout;
 
 int
 xmon_putc(int c, void *f)
diff -purN linux-2.5/drivers/char/Kconfig linuxppc64-2.5/drivers/char/Kconfig
--- linux-2.5/drivers/char/Kconfig	2004-03-16 10:10:34.000000000 +0000
+++ linuxppc64-2.5/drivers/char/Kconfig	2004-03-17 00:46:50.000000000 +0000
@@ -588,6 +588,20 @@ config HVC_CONSOLE
 	  console. This driver allows each pSeries partition to have a console
 	  which is accessed via the HMC.
 
+config HVCS
+	tristate "IBM Hypervisor Virtual Console Server support"
+	depends on PPC_PSERIES
+	help
+	  Partitionable IBM Power5 ppc64 machines allow hosting of
+	  firmware virtual consoles from one Linux partition by
+	  another Linux partition.  This driver allows console data
+	  from Linux partitions to be accessed through TTY device
+	  interfaces in the device tree on a Linux partition running
+	  this driver.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called hvcs.ko.
+
 config PC9800_OLDLP
 	tristate "NEC PC-9800 old-style printer port support"
 	depends on X86_PC9800 && !PARPORT
diff -purN linux-2.5/drivers/char/Makefile linuxppc64-2.5/drivers/char/Makefile
--- linux-2.5/drivers/char/Makefile	2004-03-19 05:59:29.000000000 +0000
+++ linuxppc64-2.5/drivers/char/Makefile	2004-03-20 11:59:31.000000000 +0000
@@ -45,6 +45,7 @@ obj-$(CONFIG_RAW_DRIVER)	+= raw.o
 obj-$(CONFIG_SGI_L1_SERIAL)	+= sn_serial.o
 obj-$(CONFIG_VIOCONS) += viocons.o
 obj-$(CONFIG_VIOTAPE)		+= viotape.o
+obj-$(CONFIG_HVCS)		+= hvcs.o
 
 obj-$(CONFIG_PRINTER) += lp.o
 obj-$(CONFIG_TIPAR) += tipar.o
diff -purN linux-2.5/drivers/char/hvc_console.c linuxppc64-2.5/drivers/char/hvc_console.c
--- linux-2.5/drivers/char/hvc_console.c	2004-03-16 11:30:38.000000000 +0000
+++ linuxppc64-2.5/drivers/char/hvc_console.c	2004-03-19 17:27:30.000000000 +0000
@@ -27,23 +27,18 @@
 #include <linux/tty_flip.h>
 #include <linux/sched.h>
 #include <linux/kbd_kern.h>
-#include <asm/uaccess.h>
 #include <linux/spinlock.h>
 #include <linux/cpumask.h>
-
-extern int hvc_count(int *);
-extern int hvc_get_chars(int index, char *buf, int count);
-extern int hvc_put_chars(int index, const char *buf, int count);
+#include <asm/uaccess.h>
+#include <asm/hvconsole.h>
 
 #define HVC_MAJOR	229
 #define HVC_MINOR	0
 
-#define MAX_NR_HVC_CONSOLES	4
-
 #define TIMEOUT		((HZ + 99) / 100)
 
 static struct tty_driver *hvc_driver;
-static int hvc_offset;
+static int count;
 #ifdef CONFIG_MAGIC_SYSRQ
 static int sysrq_pressed;
 #endif
@@ -52,6 +47,10 @@ static int sysrq_pressed;
 
 #define __ALIGNED__	__attribute__((__aligned__(8)))
 
+/* This driver speaks only in "indexes", i.e. logical consoles starting at 0.
+ * The ppc64 backend converts those indexes (e.g. hvc0) to whatever the
+ * ultimate "vterm number" that the platform understands. */
+
 struct hvc_struct {
 	spinlock_t lock;
 	int index;
@@ -112,7 +111,7 @@ static void hvc_push(struct hvc_struct *
 {
 	int n;
 
-	n = hvc_put_chars(hp->index + hvc_offset, hp->outbuf, hp->n_outbuf);
+	n = hvc_arch_put_chars(hp->index, hp->outbuf, hp->n_outbuf);
 	if (n <= 0) {
 		if (n == 0)
 			return;
@@ -226,9 +225,14 @@ static void hvc_poll(int index)
 		for (;;) {
 			if (TTY_FLIPBUF_SIZE - tty->flip.count < sizeof(buf))
 				break;
-			n = hvc_get_chars(index + hvc_offset, buf, sizeof(buf));
-			if (n <= 0)
+			n = hvc_arch_get_chars(index, buf, sizeof(buf));
+			if (n <= 0) {
+				if (n == -EPIPE) {
+					printk("tty_hangup\n");
+					tty_hangup(tty);
+				}
 				break;
+			}
 			for (i = 0; i < n; ++i) {
 #ifdef CONFIG_MAGIC_SYSRQ		/* Handle the SysRq Hack */
 				if (buf[i] == '\x0f') {	/* ^O -- should support a sequence */
@@ -281,6 +285,30 @@ int khvcd(void *unused)
 	}
 }
 
+static int hvc_tiocmget(struct tty_struct *tty, struct file *file)
+{
+	struct hvc_struct *hp = tty->driver_data;
+	int ret = -EIO;
+
+	if (!file || !tty_hung_up_p(file)) {
+		ret = hvc_arch_tiocmget(hp->index);
+	}
+	return ret;
+}
+
+static int hvc_tiocmset(struct tty_struct *tty, struct file *file,
+	unsigned int set, unsigned int clear)
+{
+	struct hvc_struct *hp = tty->driver_data;
+	int ret = -EIO;
+
+	if (!file || !tty_hung_up_p(file)) {
+		ret = hvc_arch_tiocmset(hp->index, set, clear);
+	}
+
+	return ret;
+}
+
 static struct tty_operations hvc_ops = {
 	.open = hvc_open,
 	.close = hvc_close,
@@ -288,17 +316,13 @@ static struct tty_operations hvc_ops = {
 	.hangup = hvc_hangup,
 	.write_room = hvc_write_room,
 	.chars_in_buffer = hvc_chars_in_buffer,
+	.tiocmget = hvc_tiocmget,
+	.tiocmset = hvc_tiocmset,
 };
 
 int __init hvc_init(void)
 {
-	int num = hvc_count(&hvc_offset);
-	int i;
-
-	if (num > MAX_NR_HVC_CONSOLES)
-		num = MAX_NR_HVC_CONSOLES;
-
-	hvc_driver = alloc_tty_driver(num);
+	hvc_driver = alloc_tty_driver(count);
 	if (!hvc_driver)
 		return -ENOMEM;
 
@@ -312,23 +336,20 @@ int __init hvc_init(void)
 	hvc_driver->init_termios = tty_std_termios;
 	hvc_driver->flags = TTY_DRIVER_REAL_RAW;
 	tty_set_operations(hvc_driver, &hvc_ops);
-	for (i = 0; i < num; i++) {
-		hvc_struct[i].lock = SPIN_LOCK_UNLOCKED;
-		hvc_struct[i].index = i;
-	}
 
 	if (tty_register_driver(hvc_driver))
 		panic("Couldn't register hvc console driver\n");
 
-	if (num > 0)
+	if (count > 0)
 		kernel_thread(khvcd, NULL, CLONE_KERNEL);
+	else
+		printk(KERN_WARNING "no virtual consoles found\n");
 
 	return 0;
 }
+device_initcall(hvc_init);
 
-static void __exit hvc_exit(void)
-{
-}
+/***** console (not tty) code: *****/
 
 void hvc_console_print(struct console *co, const char *b, unsigned count)
 {
@@ -348,7 +369,7 @@ void hvc_console_print(struct console *c
 				--count;
 			}
 		} else {
-			r = hvc_put_chars(co->index + hvc_offset, c, i);
+			r = hvc_arch_put_chars(co->index, c, i);
 			if (r < 0) {
 				/* throw away chars on error */
 				i = 0;
@@ -370,7 +391,7 @@ static struct tty_driver *hvc_console_de
 static int __init hvc_console_setup(struct console *co, char *options)
 {
 	if (co->index < 0 || co->index >= MAX_NR_HVC_CONSOLES
-	    || co->index >= hvc_count(&hvc_offset))
+	    || co->index >= count)
 		return -1;
 	return 0;
 }
@@ -384,12 +405,27 @@ struct console hvc_con_driver = {
 	.index		= -1,
 };
 
+/* hvc_instantiate - called once per discovered vterm by hvc_arch_find_vterms */
+int hvc_instantiate(void)
+{
+	struct hvc_struct *hvc;
+
+	if (count >= MAX_NR_HVC_CONSOLES)
+		return -1;
+
+	hvc = &hvc_struct[count];
+	hvc->lock = SPIN_LOCK_UNLOCKED;
+	hvc->index = count;
+
+	count++;
+
+	return 0;
+}
+
 static int __init hvc_console_init(void)
 {
+	hvc_arch_find_vterms(); /* populate hvc_struct[] early */
 	register_console(&hvc_con_driver);
 	return 0;
 }
 console_initcall(hvc_console_init);
-
-module_init(hvc_init);
-module_exit(hvc_exit);
diff -purN linux-2.5/drivers/char/hvcs.c linuxppc64-2.5/drivers/char/hvcs.c
--- linux-2.5/drivers/char/hvcs.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/char/hvcs.c	2004-02-27 21:16:22.000000000 +0000
@@ -0,0 +1,1106 @@
+/*
+ * IBM eServer Hypervisor Virtual Console Server Device Driver
+ * Copyright (C) 2003, 2004 IBM Corp.
+ *  Ryan S. Arnold (rsa@us.ibm.com)
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307 USA
+ *
+ *
+ * Author(s) :  Ryan S. Arnold <rsa@us.ibm.com>
+ *
+ * This is the device driver for the IBM Hypervisor Virtual Console
+ * Server, "hvcs".  The IBM hvcs provides a TTY interface to allow
+ * Linux user space applications access to the system consoles of
+ * partitioned RPA supported operating systems (Linux and AIX)
+ * running on the same partitioned IBM POWER architecture eServer.
+ * Physical hardware consoles per partition do not exist on these
+ * platforms and system consoles are interacted with through
+ * firmware interfaces utilized by this driver.
+ *
+ * This driver's first responsiblity is for registering itself as the
+ * device driver for the vio bus's firmware defined vty-server@
+ * vdevices.  These devices are virtual terminal devices that are
+ * created and configured by the "Super Admin Authority" using the
+ * IBM Hardware Management Console (which views them as "VSerial
+ * Client Adapters").  A vty-server@ vdevice is architected to allow
+ * "partner" connections to one or more target vty-client vdevices.
+ * Firmware reports these devices as vty@ vdevices ("VSerial Server
+ * Adapters" to the HMC).  The initial release of this driver only
+ * supports a single configured partner connection between a
+ * vty-server@ vdevice and a vty@ vdevice due to current firmware
+ * function limitations.
+ *
+ * Vty@ vdevices are firmware defined virtual terminal devices
+ * that console device drivers recognize as the origin and
+ * destination device of an RPA supported OS's console input and
+ * output.  A vty@ vdevice can be connected to as an output source
+ * and input target by a number of vty-server@ vdevices on the
+ * eServer, such as the Hardware Management Console, a serial console,
+ * and this console server (executing on the same or another Linux
+ * partition).
+ *
+ * Firmware is tasked with exposing vty-server@ and vty@ adapters
+ * to the POWER Linux virtual I/O bus.  On the backend (not user
+ * accessible) this driver is implemented as a vio device driver so
+ * that it can receive notification of vty-server@ vdevice lifetimes
+ * from the vio bus after it registers to handle vty-server@ lifetime
+ * notifications (.probe and .remove).
+ *
+ * On the front end this driver is implemented as a tty driver to
+ * allow it to interact with user space applications via the linux
+ * kernel's tty interface.  A block of major and minor numbers for the
+ * vty-server@ vdevices are generated dynamically by the TTY layer
+ * when this driver is registered as a tty driver.  From this the tty
+ * layer does the work of providing the major and minor numbers to the
+ * user via sysfs attributes.
+ *
+ * According to specification a vty@, while configurably many-to-one,
+ * may only be actively connected to a single vty-server@ at one time.
+ * This means that if the IBM Hardware Management Console is currently
+ * hosting the console for a target Linux or AIX partition then
+ * attempting to open the TTY device to that partition's console
+ * using the hvcs will return -EBUSY with every open attempt until
+ * the IBM Hardware Management Console frees the connection between
+ * its vty-server@ vdevice and the desired partition's vty@ vdevice.
+ *
+ * Similarily, only one vty-server@ adapter on the eServer may have a
+ * connection to a target RPA partition vty@ vdevice at one time.
+ * Conversely, a vty-server@ vdevice may only be connected to a single
+ * vty@ vdevice at one time even though it may have several configured
+ * vty@ partner possibilities.
+ *
+ * Firmware does not provide notification of partner changes to this
+ * driver.  This means that an eServer admin may add or remove
+ * partner vty@ vdevices from a vty-server@ vdevice's partner list
+ * and the changes will not be signaled to the vty-server@ vdevice.
+ * Firmware only notifies the driver when a vty-server@ vdevice is
+ * added to or removed from the system.
+ *
+ * Terminology:
+ *
+ * "vty-server@" and "vty@" are firmware defined device naming
+ * conventions which appear in a Linux device tree concatenated with
+ * a per partition unique unit address.
+ *
+ * "partner" describes a configured vty@ vdevice in respect to the
+ * vty-server@ vdevice that can connect to it.  A vty@ vdevice can be
+ * a partner to more than one vty-server@ vdevice.
+ *
+ * This driver was originally written to cooperate with the 2.6 Linux
+ * kernel as a kernel driver module.  If you notice that this driver
+ * utilizes deprecated techniques please notify the author(s).
+ *
+ */
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/major.h>
+#include <linux/kernel.h>
+#include <linux/moduleparam.h>
+#include <linux/interrupt.h>
+#include <linux/spinlock.h>
+#include <asm/uaccess.h>
+#include <linux/stat.h>
+#include <linux/kobject.h>
+#include <linux/sched.h>
+#include <linux/tty.h>
+#include <linux/tty_flip.h>
+#include <linux/device.h>
+#include <asm/vio.h>
+#include <linux/list.h>
+#include <asm/hvconsole.h>
+
+MODULE_AUTHOR("Ryan S. Arnold <rsa@us.ibm.com>");
+MODULE_DESCRIPTION("IBM hvcs (Hypervisor Virtual Console Server) Driver");
+MODULE_LICENSE("GPL");
+
+/* TODO:
+ * Do I need to grab a kobject ref to the hvcs_struct each time
+ * I use the hvcs_struct instance?
+ *
+ * How many of the function forward declarations are really
+ * required?
+ *
+ * Remove dev_node sysfs entry once I figure out how to inform the
+ * user about what the device node entry is that cooresponds to a
+ * vty-server@3000000* entry.
+ *
+ * Investigate issue surrounding hvcs_throttle() and
+ * hvcs_unthrottle() callbacks and apparent dataloss.  This may
+ * require fixes to hvc_console to determine who (this driver or
+ * hvc_console) is actually dropping data.
+ *
+ * Write Documentation/hvcs.txt help file for this driver.
+ *
+ * Develop test plan.
+ *
+ */
+
+/* Since the Linux TTY code does not currently (2-04-2004) support
+ * dynamic addition of tty derived devices and we shouldn't
+ * allocate thousands of tty_device pointers when the number of
+ * vty-server@ & vty@ partner connections will most often be much
+ * lower than this, we'll arbitrarily allocate HVCS_DEFAULT_SERVER_ADAPTERS
+ * tty_structs and cdev's by default when we register the tty_driver.
+ * This can be overridden using an insmod parameter.
+ */
+#define HVCS_DEFAULT_SERVER_ADAPTERS	64
+
+/* The user can't specify with insmod more than HVCS_MAX_SERVER_ADAPTERS
+ * hvcs device nodes as a sanity check.  Theoretically there can be
+ * over 1 Billion vty-server@ & vty@ partner vdevice connections.
+ */
+#define HVCS_MAX_SERVER_ADAPTERS	1024
+
+/* We let Linux assign us a Major number and we start the minors at
+ * ZERO.  There is no mapping between minor number and the target
+ * partition.  The mapping of minor number is related to the order
+ * the vty-server@ vdevices are exposed to this driver via the
+ * hvcs_probe function.
+ */
+#define HVCS_MINOR_START	0
+
+#define __ALIGNED__	__attribute__((__aligned__(8)))
+
+/* Converged location code string length + 1 null terminator */
+#define CLC_LENGTH		80
+
+/* How much data can firmware send with each hvterm_put_chars()?
+ * Maybe this should be moved into an architecture specific area. */
+#define HVCS_BUFF_LEN	16
+
+#define HVCS_MAX_FROM_USER	4096
+
+/* This value is used to take the place of a command line parameter
+ * when the module is inserted.  It starts as -1 and stays as such if
+ * the user doesn't specify a module insmod parameter.  If they DO
+ * specify one then it is set to the value of the integer passed in.
+ */
+static int hvcs_parm_num_devs = -1;
+module_param(hvcs_parm_num_devs, int, 0);
+
+static const char hvcs_driver_name[] = "hvcs";
+static const char hvcs_device_node[] = "hvcs";
+static const char hvcs_driver_string[]
+	= "IBM hvcs (Hypervisor Virtual Console Server) Driver";
+
+/* Status of partner info rescan triggered via sysfs. */
+static int hvcs_rescan_status = 0;
+
+static struct tty_driver *hvcs_tty_driver;
+
+/* This is used to associate a vty-server@ vdevice as it is exposed to
+ * the driver with a preallocated tty_struct.index.  The dev node and
+ * hvcs index numbers are not re-used after device removal otherwise
+ * removing vdevices and adding a new one would link a /dev/hvcs*
+ * entry to a different vty-server@ vdevice than it did before the
+ * removal.  This means that a newly exposed vty-server@ vdevice will
+ * always map to an incrementally higher /dev/hvcs* entry than last
+ * exposed vty-server@ vdevice.
+ */
+static int hvcs_struct_count = -1;
+
+/* One vty-server@ vdevice per hvcs_struct */
+struct hvcs_struct {
+	struct list_head next; /* list management */
+	struct vio_dev *vdev;
+	struct tty_struct *tty;
+	unsigned int open_count;
+	/* this index identifies this hvcs device as the
+	 * complement to a tty index. */
+	unsigned int index;
+	unsigned int p_unit_address; /* partner unit address */
+	unsigned int p_partition_ID; /* partner partition ID */
+	char p_location_code[CLC_LENGTH];
+	char name[32];
+	int enabled; /* there are tty's open against this device */
+	struct kobject kobj; /* ref count & hvcs_struct lifetime */
+	struct work_struct read_work;
+};
+
+/* Require to back map a kobject to its containing object */
+#define from_kobj(kobj) container_of(kobj, struct hvcs_struct, kobj)
+
+static struct list_head hvcs_structs = LIST_HEAD_INIT(hvcs_structs);
+
+/* Be careful when adding flags to this line discipline.  Don't add
+ * anything that will cause the line discipline to put this driver
+ * into canonical mode.  We can't go into canonical mode because we
+ * need to be able to get THROTTLE and UNTHROTTLE callbacks.
+ */
+struct termios hvcs_tty_termios = {
+	.c_iflag = IGNBRK | IGNPAR,
+	/* Enable implementation-defined output processing. */
+	/* Map NL to CR-NL on output */
+	/*.c_oflag = OPOST, ONLCR,
+	.c_iflag = ICRNL, */
+	.c_cflag = B38400 | CS8 | CREAD | HUPCL,
+	/* NOTICE NO ECHO or we'll go into recursive loop echoing chars
+	 * back and forth with the console drivers. */
+	.c_cc = INIT_C_CC
+};
+
+static void hvcs_read_task(unsigned long data);
+static void hvcs_unthrottle(struct tty_struct *tty);
+static void hvcs_throttle(struct tty_struct *tty);
+static irqreturn_t hvcs_handle_interrupt(int irq, void *dev_instance, struct pt_regs *regs);
+
+static int hvcs_write(struct tty_struct *tty, int from_user, const unsigned char *buf, int count);
+static int hvcs_write_room(struct tty_struct *tty);
+static int hvcs_chars_in_buffer(struct tty_struct *tty);
+
+static int hvcs_has_pi(struct hvcs_struct *hvcsd);
+static void hvcs_set_pi(struct hvcs_partner_info *pi, struct hvcs_struct *hvcsd);
+static int hvcs_get_pi(struct hvcs_struct *hvcsd);
+static int hvcs_rescan_devices_list(void);
+
+static int hvcs_partner_connect(struct hvcs_struct *hvcsd);
+static void hvcs_partner_free(struct hvcs_struct *hvcsd);
+
+static int hvcs_enable_device(struct hvcs_struct *hvcsd);
+static void hvcs_disable_device(struct hvcs_struct *hvcsd);
+
+static void destroy_hvcs_struct(struct kobject *kobj);
+static int hvcs_open(struct tty_struct *tty, struct file *filp);
+static void hvcs_close(struct tty_struct *tty, struct file *filp);
+static void hvcs_hangup(struct tty_struct * tty);
+
+static void hvcs_create_device_attrs(struct hvcs_struct *hvcsd);
+static void hvcs_remove_device_attrs(struct hvcs_struct *hvcsd);
+static void hvcs_create_driver_attrs(void);
+static void hvcs_remove_driver_attrs(void);
+
+static int __devinit hvcs_probe(struct vio_dev *dev, const struct vio_device_id *id);
+static void __devexit hvcs_remove(struct vio_dev *dev);
+static int __init hvcs_module_init(void);
+static void __exit hvcs_module_exit(void);
+
+/* This task is scheduled to execute out of the read data interrupt
+ * handler, the hvcs_unthrottle, and be rescheduled out of itself.
+ * This should never get called from both unthrottle and the
+ * read_task at the same time because if the tty isn't throttled the
+ * unthrottle function will never call this function and if the tty is
+ * throttled then only unthrottle will call this function because vio
+ * interrupts will be turned off until this function enables them.
+ * We only want to enable interrupts when hvterm_get_chars() returns
+ * zero.
+ */
+
+static void hvcs_read_task(unsigned long data)
+{
+	struct hvcs_struct *hvcsd = (struct hvcs_struct *)data;
+	unsigned int unit_address = hvcsd->vdev->unit_address;
+	struct tty_struct *tty = hvcsd->tty;
+	char buf[HVCS_BUFF_LEN] __ALIGNED__;
+	int got;
+	int i;
+
+	/* It is possible that hvcs_close was interrupted and this
+	 * task was scheduled during the middle of hvcs_close().  */
+	if (!hvcsd->enabled || !tty) {
+		return;
+	}
+
+	while ((tty->flip.count + HVCS_BUFF_LEN) < TTY_FLIPBUF_SIZE ) {
+		memset(&buf[0], 0x00, HVCS_BUFF_LEN);
+		got = hvterm_get_chars(unit_address, &buf[0], HVCS_BUFF_LEN);
+
+		if (!got) {
+			if (tty->flip.count)
+				tty_flip_buffer_push(tty);
+			vio_enable_interrupts(hvcsd->vdev);
+			return;
+		}
+		for(i = 0; i < got; i++) {
+			tty_insert_flip_char(tty, buf[i], TTY_NORMAL);
+		}
+
+	}
+	if (tty->flip.count) {
+		tty_flip_buffer_push(tty);
+	}
+	/* reschedule because the flip buffer is full and we may have
+	 * more data to pull from the hypervisor */
+	schedule_delayed_work(&hvcsd->read_work, 1);
+	return;
+}
+
+/* This is the callback from the tty layer that tells us that the flip
+ * buffer has more space.
+ */
+static void hvcs_unthrottle(struct tty_struct *tty)
+{
+	struct hvcs_struct *hvcsd = (struct hvcs_struct *)tty->driver_data;
+	printk(KERN_INFO "HVCS: tty unthrottled, re-scheduling"
+			" read task.\n");
+	schedule_delayed_work(&hvcsd->read_work, 1);
+}
+
+static void hvcs_throttle(struct tty_struct *tty)
+{
+	struct hvcs_struct *hvcsd = (struct hvcs_struct *)tty->driver_data;
+
+	printk(KERN_INFO "HVCS: tty throttled.\n");
+
+	/* We'll re-schedule once the tty executes the
+	 * unthrottle callback. */
+	cancel_delayed_work(&hvcsd->read_work);
+}
+
+/* If the device is being removed we don't have to worry about this
+ * interrupt handler taking any further interrupts because they are
+ * disabled which means the hvcs_struct will always be valid in this
+ * handler.
+ */
+static irqreturn_t hvcs_handle_interrupt(int irq, void *dev_instance, struct pt_regs *regs)
+{
+	struct hvcs_struct *hvcsd = (struct hvcs_struct *)dev_instance;
+
+	vio_disable_interrupts(hvcsd->vdev);
+	schedule_work(&hvcsd->read_work);
+
+	return IRQ_HANDLED;
+}
+
+static struct vio_device_id hvcs_driver_table[] __devinitdata= {
+	{"serial-server", "hvterm2"},
+	{ 0,}
+};
+MODULE_DEVICE_TABLE(vio, hvcs_driver_table);
+
+/* callback when the kboject ref count reaches zero */
+static void destroy_hvcs_struct(struct kobject *kobj)
+{
+	struct hvcs_struct *hvcsd = from_kobj(kobj);
+
+	list_del(&(hvcsd->next));
+
+	kfree(hvcsd);
+
+	printk(KERN_INFO "HVCS: Last kobj to hvcs_struct released,"
+		" hvcs_struct freed.\n");
+}
+
+static struct kobj_type hvcs_kobj_type = {
+	.release = destroy_hvcs_struct,
+};
+
+static int __devinit hvcs_probe(
+	struct vio_dev *dev,
+	const struct vio_device_id *id)
+{
+	struct hvcs_struct *hvcsd;
+
+	if (!dev || !id) {
+		printk(KERN_ERR "hvcs_probe: called with invalid"
+			" device or id pointer.\n");
+	}
+
+	printk(KERN_INFO "HVCS: Added vty-server@%X.\n", dev->unit_address);
+
+	hvcsd = kmalloc(sizeof(*hvcsd), GFP_KERNEL);
+	if (!hvcsd) {
+		return -ENODEV;
+	}
+
+	/* hvcsd->tty is zeroed out with the memset */
+	memset(hvcsd, 0x00, sizeof(*hvcsd));
+
+	/* Automatically incs the refcount the first time */
+	kobject_init(&hvcsd->kobj);
+	/* Set up the callback for terminating the hvcs_struct's life */
+	hvcsd->kobj.ktype = &hvcs_kobj_type;
+
+	hvcsd->vdev = dev;
+	dev->driver_data = hvcsd;
+	sprintf(hvcsd->name,"%X",dev->unit_address);
+
+	hvcsd->index = ++hvcs_struct_count;
+
+	INIT_WORK(&hvcsd->read_work, hvcs_read_task, (unsigned long)hvcsd);
+
+	hvcsd->enabled = 0;
+
+	/* This will populate the hvcs_struct's partner info fields
+	 * for the first time. */
+	if(hvcs_get_pi(hvcsd)) {
+		printk(KERN_ERR "hvcs_probe : Failed to fetch partner"
+			" info for vty-server@%X.\n",
+			hvcsd->vdev->unit_address);
+	}
+
+	/* If a user app opens a tty that corresponds to this vty-server@
+	 * vdevice before the hvcs_struct has been added to the devices
+	 * list then the user app will get -ENODEV.
+	 */
+
+	list_add_tail(&(hvcsd->next), &hvcs_structs);
+
+	hvcs_create_device_attrs(hvcsd);
+
+	/* DON'T enable interrupts here because there is no user */
+	return 0;
+}
+
+static void __devexit hvcs_remove(struct vio_dev *dev)
+{
+	struct hvcs_struct *hvcsd = (struct hvcs_struct *)dev->driver_data;
+
+	if (!hvcsd)
+		return;
+
+	printk(KERN_INFO "HVCS: Removed vty-server@%X.\n",
+		dev->unit_address);
+
+	/* By this time the vty-server@ vdevice won't be getting any
+	 * more interrups but we might get a callback from the tty. */
+	cancel_delayed_work(&hvcsd->read_work);
+	flush_scheduled_work();
+
+	/* If hvcs_remove is called after a tty_hangup has been issued
+	 * (indicating that there are no connections) or before a user
+	 * has attempted to open the device then the device will not be
+	 * enabled and thus we don't need to do any cleanup.  */
+	if (hvcsd->enabled) {
+		hvcs_disable_device(hvcsd);
+	}
+
+	if (hvcsd->tty) {
+		/* This is a scheduled function which will
+		 * auto chain call hvcs_hangup. */
+		tty_hangup(hvcsd->tty);
+	}
+
+	hvcs_remove_device_attrs(hvcsd);
+
+	/* Let the last holder of this object cause it to be removed,
+	 * which would probably be tty_hangup.
+	 */
+	kobject_put (&hvcsd->kobj);
+};
+
+static struct vio_driver hvcs_vio_driver = {
+	.name		= &hvcs_driver_name,
+	.id_table	= hvcs_driver_table,
+	.probe		= hvcs_probe,
+	.remove		= __devexit_p(hvcs_remove),
+};
+
+/* Only called from hvcs_get_pi please */
+static void hvcs_set_pi(struct hvcs_partner_info *pi, struct hvcs_struct *hvcsd)
+{
+	int clclength;
+
+	hvcsd->p_unit_address = pi->unit_address;
+	hvcsd->p_partition_ID  = pi->partition_ID;
+	clclength = strlen(&pi->location_code[0]);
+	if(clclength > CLC_LENGTH - 1) {
+		clclength = CLC_LENGTH - 1;
+	}
+	/* copy the null-term char too */
+	strncpy(&hvcsd->p_location_code[0],
+			&pi->location_code[0], clclength + 1);
+}
+
+/* Traverse the list and add the partner info that is found to the
+ * hvcs_struct struct entry. NOTE: At this time I know that partner
+ * info will return a single entry but in the future there may be
+ * multiple partner info entries per vty-server@ vdevice and you'll
+ * want to zero out that list and reset it.  If for some reason you
+ * have an old version of this driver and there IS more than one
+ * partner info then hvcsd->p_* will have the last partner info
+ * data from the firmware query.
+ */
+static int hvcs_get_pi(struct hvcs_struct *hvcsd)
+{
+	/* struct hvcs_partner_info *head_pi = NULL; */
+	struct hvcs_partner_info *pi = NULL;
+	unsigned int unit_address = hvcsd->vdev->unit_address;
+	struct list_head head;
+	int retval;
+
+	retval = hvcs_get_partner_info(unit_address, &head);
+	if (retval) {
+		printk(KERN_ERR "HVCS: Failed to fetch partner"
+			" info for vty-server@%x.\n",unit_address);
+		return retval;
+	}
+
+	/* nixes the values if the partner vty@ went away */
+	hvcsd->p_unit_address = 0;
+	hvcsd->p_partition_ID = 0;
+
+	list_for_each_entry(pi, &head, node) {
+		hvcs_set_pi(pi,hvcsd);
+	}
+
+	hvcs_free_partner_info(&head);
+	return 0;
+}
+
+/* This function is executed by the driver "rescan" sysfs entry */
+static int hvcs_rescan_devices_list(void)
+{
+	struct hvcs_struct *hvcsd = NULL;
+
+	/* Locking issues? */
+	list_for_each_entry(hvcsd, &hvcs_structs, next) {
+		hvcs_get_pi(hvcsd);
+	}
+
+	return 0;
+}
+
+/* Farm this off into its own function because it could be
+ * more complex once multiple partners support is added. */
+static int hvcs_has_pi(struct hvcs_struct *hvcsd)
+{
+	if ((!hvcsd->p_unit_address) || (!hvcsd->p_partition_ID))
+		return 0;
+	return 1;
+}
+
+/* NOTE: It is possible that the super admin removed
+ * a partner vty@ vdevice and then added a diff vty@
+ * vdevice as the new partner.  At this point this
+ * could connect a console to a different target
+ * partition.
+ */
+static int hvcs_partner_connect(struct hvcs_struct *hvcsd)
+{
+	int retval;
+	unsigned int unit_address = hvcsd->vdev->unit_address;
+
+	/* If there wasn't any pi when the vdevice was added it
+	 * doesn't meant there isn't any now.  This driver isn't
+	 * notified when a new partner vty@ is added to a
+	 * vty-server@ vdevice so we discover changes on our own.
+	 * Please see comments in hvcs_register_connection() for
+	 * justification of this bizarre code. */
+	retval = hvcs_register_connection(unit_address,
+			hvcsd->p_partition_ID,
+			hvcsd->p_unit_address);
+	if (!retval)
+		return 0;
+	else if (retval != -EINVAL)
+		return retval;
+
+	/* As per the spec re-get the pi and try again if -EINVAL
+	 * after the first connection attempt. */
+	if (hvcs_get_pi(hvcsd))
+		return -ENOMEM;
+
+	if (!hvcs_has_pi(hvcsd))
+		return -ENODEV;
+
+	retval = hvcs_register_connection(unit_address,
+			hvcsd->p_partition_ID,
+			hvcsd->p_unit_address);
+	if (retval != -EINVAL)
+		return retval;
+
+	/* EBUSY is the most likely scenario though the vty@ could have
+	 * been removed or there really could be an hcall error due to the
+	 * parameter data. */
+	printk(KERN_INFO "HVCS: vty-server@ or partner"
+			" vty@ is busy.  Try again later.\n");
+	return -EBUSY;
+}
+
+static void hvcs_partner_free(struct hvcs_struct *hvcsd)
+{
+	int retval;
+	do {
+		/* it will return -EBUSY if the operation would take too
+		 * long to complete synchronously.
+		 */
+		retval = hvcs_free_connection(hvcsd->vdev->unit_address);
+	} while (retval == -EBUSY);
+}
+
+static int hvcs_enable_device(struct hvcs_struct *hvcsd)
+{
+	int retval;
+	/* It is possible that the vty-server@ vdevice was removed
+	 * between the time that the conn was registered and now.
+	 */
+	if ((retval = request_irq(hvcsd->vdev->irq,
+			&hvcs_handle_interrupt, SA_INTERRUPT,
+			"ibmhvcs", hvcsd)) != 0) {
+		printk(KERN_ERR "hvcs_enable_device : failed to request"
+			" irq for vty-server@%X with retval :%d.\n",
+			hvcsd->vdev->unit_address, retval);
+		hvcs_partner_free(hvcsd);
+		return -ENODEV;
+	}
+
+	/* It is possible the vty-server@ vdevice was removed
+	 * after the irq was requested but before we have time
+	 * to enabled interrupts.
+	 */
+	if (vio_enable_interrupts(hvcsd->vdev) != H_Success) {
+		printk(KERN_ERR "hvcs_enable_device : failed to enable"
+			" interrtups for vty-server@%X.\n",
+			hvcsd->vdev->unit_address);
+		/* These can fail but we'll just ignore them for now. */
+		free_irq(hvcsd->vdev->irq, hvcsd);
+		hvcs_partner_free(hvcsd);
+		return -ENODEV;
+	}
+
+	return 0;
+}
+
+static void hvcs_disable_device(struct hvcs_struct *hvcsd)
+{
+	if(!hvcsd->enabled)
+		return;
+
+	hvcsd->enabled = 0;
+	/* Any one of these might fail at any time due to the
+	 * vty-server@ vdevice's availability during the call.
+	 */
+	vio_disable_interrupts(hvcsd->vdev);
+	hvcs_partner_free(hvcsd);
+	printk(KERN_INFO "HVCS: Freed vty-server@%X and"
+			" partner vty@%X:%d.\n",
+			hvcsd->vdev->unit_address,
+			hvcsd->p_unit_address,
+			(unsigned int)hvcsd->p_partition_ID);
+	free_irq(hvcsd->vdev->irq, hvcsd);
+}
+
+/* This always increments the kobject ref count if the call is
+ * successful.  Please remember to dec when you are done with
+ * the instance. */
+struct hvcs_struct *hvcs_get_by_index(int index)
+{
+	struct hvcs_struct *hvcsd = NULL;
+	struct list_head *element;
+	struct list_head *safe_temp;
+	/* We can immediately discard OOB requests */
+	if (index >= 0 && index < HVCS_MAX_SERVER_ADAPTERS) {
+		list_for_each_safe(element, safe_temp, &hvcs_structs) {
+			hvcsd = list_entry(element, struct hvcs_struct, next);
+			if (hvcsd->index == index) {
+				kobject_get(&hvcsd->kobj);
+				break;
+			}
+		}
+	}
+	return hvcsd;
+}
+
+/* This is invoked via the tty_open interface when a user app
+ * connects to the /dev node.
+ */
+static int hvcs_open(struct tty_struct *tty, struct file *filp)
+{
+	struct hvcs_struct *hvcsd = NULL;
+	int retval = 0;
+
+	if (tty->driver_data)
+		goto fast_open;
+
+	/* Is there a vty-server@ adapter that shares the same index? */
+	if (!(hvcsd = hvcs_get_by_index(tty->index))) return -ENODEV;
+
+	printk(KERN_INFO "HVCS: First open of vty-server@%X.\n",
+		hvcsd->vdev->unit_address );
+
+	if((retval = hvcs_partner_connect(hvcsd)))
+		goto error_release;
+
+	/* This will free the connection if it fails */
+	if((retval = hvcs_enable_device(hvcsd)))
+		goto error_release;
+
+	hvcsd->open_count = 1;
+	hvcsd->enabled = 1;
+	hvcsd->tty = tty;
+	tty->driver_data = hvcsd;
+
+	goto open_success;
+
+fast_open:
+	hvcsd = (struct hvcs_struct *)tty->driver_data;
+
+	if (!kobject_get(&hvcsd->kobj)) {
+		printk(KERN_ERR "hvcs_open: Kobject of open"
+			" hvcs doesn't exist.\n");
+		return -EFAULT; /* Is this the right return value? */
+	}
+
+	hvcsd->open_count++;
+
+open_success:
+	return 0;
+
+error_release:
+	kobject_put(&hvcsd->kobj);
+	return retval;
+}
+
+static void hvcs_close(struct tty_struct *tty, struct file *filp)
+{
+	struct hvcs_struct *hvcsd;
+
+	/* Is someone trying to close the file associated with
+	 * this device after we have hung up?  If so
+	 * tty->driver_data wouldn't be valid.
+	 */
+	if (tty_hung_up_p(filp)) {
+		printk(KERN_INFO "hvcs_close: tty is hung up.\n");
+		return;
+	}
+
+	/* No driver_data means that this close was probably
+	 * issued after a failed hvcs_open by the tty layer's
+	 * release_dev() api and we can just exit cleanly.
+	 */
+	if (!tty->driver_data) {
+		printk(KERN_INFO "hvcs_close: No tty->driver_data.\n");
+		return;
+	}
+
+	hvcsd = (struct hvcs_struct *)tty->driver_data;
+
+	if (--hvcsd->open_count == 0) {
+		hvcs_disable_device(hvcsd);
+
+		/* This line is important because it tells hvcs_open
+		 * that this device needs to be re-configured the
+		 * next time hvcs_open is called.
+		 */
+		hvcsd->tty->driver_data = NULL;
+		hvcsd->tty = NULL;
+		hvcsd->p_partition_ID = 0;
+		hvcsd->p_unit_address = 0;
+		memset(&hvcsd->p_location_code[0], 0x00, CLC_LENGTH);
+	} else if (hvcsd->open_count < 0) {
+		printk(KERN_ERR "hvcs_close : vty-server@%X"
+			" is missmanaged with open_count: %d.\n",
+			hvcsd->vdev->unit_address, hvcsd->open_count);
+	}
+
+	kobject_put(&hvcsd->kobj);
+}
+
+static void hvcs_hangup(struct tty_struct * tty)
+{
+	struct hvcs_struct *hvcsd = (struct hvcs_struct *)tty->driver_data;
+
+	/* If the vty-server@ vdevice disappeared then the device
+	 * would already be disabled.  Otherwise the hangup was
+	 * indicated from a sighup?
+	 */
+	hvcs_disable_device(hvcsd);
+
+	/* One big fat close regardless of the open_count */
+	hvcsd->tty->driver_data = NULL;
+	hvcsd->tty = NULL;
+	hvcsd->p_partition_ID = 0;
+	hvcsd->p_unit_address = 0;
+	memset(&hvcsd->p_location_code[0], 0x00, CLC_LENGTH);
+
+	/* We need to kobject_put() for every open_count we have
+	 * since the tty_hangup() function doesn't invoke a close
+	 * per open connection on a non-console device. */
+	while(hvcsd->open_count) {
+		--hvcsd->open_count;
+		/* The final put will trigger destruction of the
+		 * hvcs_struct */
+		kobject_put(&hvcsd->kobj);
+	}
+}
+
+/*NOTE: This is almost always from_user since user level apps interact
+ * with the /dev devices. I'm going out on a limb here and trusting
+ * that if hvcs_write gets called and interrupted by hvcs_remove
+ * (which removes the target device and executes tty_hangup()) that
+ * tty_hangup will allow hvcs_write time to complete execution before
+ * it terminates our device. */
+static int hvcs_write(struct tty_struct *tty, int from_user, const unsigned char *buf, int count)
+{
+	struct hvcs_struct *hvcsd = (struct hvcs_struct *)tty->driver_data;
+	unsigned int unit_address;
+	unsigned char *charbuf;
+	int total_sent = 0;
+	int tosend = 0;
+	int sent = 0;
+
+	/* If they don't check the return code off of their open they
+	 * may attempt this even if there is no connected device. */
+	if (!hvcsd) {
+		printk(KERN_ERR "hvcs_write: No enabled vty-server@"
+			" adapter associated with this tty.\n");
+		return -ENODEV;
+	}
+
+	/* Reasonable size to prevent user level flooding */
+	if (count > HVCS_MAX_FROM_USER)
+		count = HVCS_MAX_FROM_USER;
+
+	unit_address = hvcsd->vdev->unit_address;
+
+	/* Somehow an open succeded but the device was removed or the
+	 * connection terminated between the vty-server@ vdevice and
+	 * partner vty@ vdevice during the middle of a write
+	 * operation? */
+	if (!hvcsd->enabled)
+		return -ENODEV;
+
+	if (!from_user) {
+		charbuf = (unsigned char *)buf;
+	} else {
+		/* This isn't so important to do if we don't spinlock
+		 * around the copy_from_user but we'll leave it here
+		 * anyway because there may be locking issues in the
+		 * future. */
+		charbuf = kmalloc(count, GFP_KERNEL);
+		if(!charbuf)
+			return -ENOMEM;
+
+		count -= copy_from_user(charbuf,buf,count);
+	}
+
+	while (count > 0) {
+		tosend = min(count, HVCS_BUFF_LEN);
+
+		/* won't return partial writes */
+		sent = hvterm_put_chars(unit_address, &charbuf[total_sent], tosend);
+		if (sent <= 0) {
+			break;
+		}
+
+		total_sent+=sent;
+		count-=sent;
+	}
+
+	if (from_user)
+		kfree(charbuf);
+
+	if (sent == -1)
+		return -ENODEV;
+	else
+		return total_sent;
+}
+
+static int hvcs_write_room(struct tty_struct *tty)
+{
+	struct hvcs_struct *hvcsd = (struct hvcs_struct *)tty->driver_data;
+
+	if (!hvcsd || !hvcsd->enabled)
+		return 0;
+	else
+		return HVCS_MAX_FROM_USER;
+}
+
+static int hvcs_chars_in_buffer(struct tty_struct *tty)
+{
+	return HVCS_MAX_FROM_USER;
+}
+
+static struct tty_operations hvcs_ops = {
+	.open = hvcs_open,
+	.close = hvcs_close,
+	.hangup = hvcs_hangup,
+	.write = hvcs_write,
+	.write_room = hvcs_write_room,
+	.chars_in_buffer = hvcs_chars_in_buffer,
+	.unthrottle = hvcs_unthrottle,
+	.throttle = hvcs_throttle,
+};
+
+static int __init hvcs_module_init(void)
+{
+	int rc;
+	int num_ttys_to_alloc;
+
+	printk(KERN_INFO "Initializing %s\n", hvcs_driver_string);
+
+	/* Has the user specified an overload with an insmod param? */
+	if( hvcs_parm_num_devs <= 0 ||
+		(hvcs_parm_num_devs > HVCS_MAX_SERVER_ADAPTERS)) {
+		num_ttys_to_alloc = HVCS_DEFAULT_SERVER_ADAPTERS;
+	} else {
+		num_ttys_to_alloc = hvcs_parm_num_devs;
+	}
+
+	hvcs_tty_driver = alloc_tty_driver(num_ttys_to_alloc);
+	if (!hvcs_tty_driver)
+		return -ENOMEM;
+
+	hvcs_tty_driver->owner = THIS_MODULE;
+
+	hvcs_tty_driver->driver_name = hvcs_driver_name;
+	hvcs_tty_driver->name = hvcs_device_node;
+
+	/* We'll let the system assign us a major number,
+	 * indicated by leaving it blank  */
+
+	hvcs_tty_driver->minor_start = HVCS_MINOR_START;
+	hvcs_tty_driver->type = TTY_DRIVER_TYPE_SYSTEM;
+
+	/* We role our own so that we DONT ECHO.  We can't echo
+	 * because the adapter we are connecting to already echoes
+	 * by default and this would throw us into a horrible
+	 * recursive echo-echo-echo loop. */
+	hvcs_tty_driver->init_termios = hvcs_tty_termios;
+	hvcs_tty_driver->flags = TTY_DRIVER_REAL_RAW;
+
+	tty_set_operations(hvcs_tty_driver, &hvcs_ops);
+
+	/* The following call will result in sysfs entries that denote
+	 * the dynamically assigned major and minor numbers for our
+	 * deices. */
+	if(tty_register_driver(hvcs_tty_driver)) {
+		printk(KERN_ERR "hvcs_module_init: registration of"
+			" hvcs as tty driver failed.\n");
+		put_tty_driver(hvcs_tty_driver);
+		return rc;
+	}
+
+	rc = vio_register_driver(&hvcs_vio_driver);
+
+	/* This needs to be done AFTER the vio_register_driver() call
+	 * or else the kobjects won't be initialized properly.
+	 */
+	hvcs_create_driver_attrs();
+
+	printk(KERN_INFO "HVCS: driver module inserted.\n");
+
+	return rc;
+}
+
+static void __exit hvcs_module_exit(void)
+{
+	hvcs_remove_driver_attrs();
+
+	vio_unregister_driver(&hvcs_vio_driver);
+
+	tty_unregister_driver(hvcs_tty_driver);
+
+	put_tty_driver(hvcs_tty_driver);
+
+	printk(KERN_INFO "HVCS: driver module removed.\n");
+}
+
+module_init(hvcs_module_init);
+module_exit(hvcs_module_exit);
+
+static inline struct hvcs_struct *from_vio_dev(struct vio_dev *viod)
+{
+	return (struct hvcs_struct *)viod->driver_data;
+}
+/* The sysfs interface for the driver and devices */
+
+static ssize_t hvcs_partner_vtys_show(struct device *dev, char *buf)
+{
+	struct vio_dev *viod = to_vio_dev(dev);
+	struct hvcs_struct *hvcsd = from_vio_dev(viod);
+	return sprintf(buf, "vty@%X\n", hvcsd->p_unit_address);
+}
+static DEVICE_ATTR(partner_vtys, S_IRUGO, hvcs_partner_vtys_show, NULL);
+
+static ssize_t hvcs_partner_clcs_show(struct device *dev, char *buf)
+{
+	struct vio_dev *viod = to_vio_dev(dev);
+	struct hvcs_struct *hvcsd = from_vio_dev(viod);
+	return sprintf(buf, "%s\n", &hvcsd->p_location_code[0]);
+}
+static DEVICE_ATTR(partner_clcs, S_IRUGO, hvcs_partner_clcs_show, NULL);
+
+static ssize_t hvcs_dev_node_show(struct device *dev, char *buf)
+{
+	struct vio_dev *viod = to_vio_dev(dev);
+	struct hvcs_struct *hvcsd = from_vio_dev(viod);
+	return sprintf(buf, "/dev/%s%d\n",hvcs_device_node, hvcsd->index);
+}
+static DEVICE_ATTR(dev_node, S_IRUGO, hvcs_dev_node_show, NULL);
+
+static ssize_t hvcs_current_vty_store(struct device *dev, const char * buf, size_t count)
+{
+	/* Don't need this feature at the present time. */
+	printk(KERN_INFO "HVCS: Denied current_vty change: -EPERM.\n");
+	return -EPERM;
+}
+
+static ssize_t hvcs_current_vty_show(struct device *dev, char *buf)
+{
+	struct vio_dev *viod = to_vio_dev(dev);
+	struct hvcs_struct *hvcsd = from_vio_dev(viod);
+	return sprintf(buf, "%s\n", &hvcsd->p_location_code[0]);
+}
+
+static DEVICE_ATTR(current_vty,
+	S_IRUGO | S_IWUSR, hvcs_current_vty_show, hvcs_current_vty_store);
+
+static struct attribute *hvcs_attrs[] = {
+	&dev_attr_partner_vtys.attr,
+	&dev_attr_partner_clcs.attr,
+	&dev_attr_dev_node.attr,
+	&dev_attr_current_vty.attr,
+	NULL,
+};
+
+static struct attribute_group hvcs_attr_group = {
+	.attrs = hvcs_attrs,
+};
+
+static void hvcs_create_device_attrs(struct hvcs_struct *hvcsd)
+{
+	struct vio_dev *vdev = hvcsd->vdev;
+	sysfs_create_group(&vdev->dev.kobj, &hvcs_attr_group);
+}
+
+static void hvcs_remove_device_attrs(struct hvcs_struct *hvcsd)
+{
+	struct vio_dev *vdev = hvcsd->vdev;
+	sysfs_remove_group(&vdev->dev.kobj, &hvcs_attr_group);
+}
+
+static ssize_t hvcs_rescan_show(struct device_driver *ddp, char *buf)
+{
+	/* A 1 means it is updating, a 0 means it is done updating */
+	return snprintf(buf, PAGE_SIZE, "%d\n", hvcs_rescan_status);
+}
+
+static ssize_t hvcs_rescan_store(struct device_driver *ddp, const char * buf, size_t count)
+{
+	if ((simple_strtol(buf,NULL,0) != 1)
+		&& (hvcs_rescan_status != 0))
+		return -EINVAL;
+
+	hvcs_rescan_status = 1;
+	printk(KERN_INFO "HVCS: rescanning partner info for all"
+		" vty-server@ vdevices.\n");
+	hvcs_rescan_devices_list();
+	hvcs_rescan_status = 0;
+	return count;
+}
+static DRIVER_ATTR(rescan,
+	S_IRUGO | S_IWUSR, hvcs_rescan_show, hvcs_rescan_store);
+
+static void hvcs_create_driver_attrs(void)
+{
+	struct device_driver *driverfs = &(hvcs_vio_driver.driver);
+	driver_create_file(driverfs, &driver_attr_rescan);
+}
+
+static void hvcs_remove_driver_attrs(void)
+{
+	struct device_driver *driverfs = &(hvcs_vio_driver.driver);
+	driver_remove_file(driverfs, &driver_attr_rescan);
+}
diff -purN linux-2.5/drivers/net/Kconfig linuxppc64-2.5/drivers/net/Kconfig
--- linux-2.5/drivers/net/Kconfig	2004-03-19 06:04:54.000000000 +0000
+++ linuxppc64-2.5/drivers/net/Kconfig	2004-03-20 11:56:10.000000000 +0000
@@ -2104,10 +2104,19 @@ config IXGB_NAPI
 endmenu
 
 
-config VETH
+config ISERIES_VETH
 	tristate "iSeries Virtual Ethernet driver support"
 	depends on NETDEVICES && PPC_ISERIES
 
+config IBMVETH
+	tristate "IBM LAN Virtual Ethernet support"
+	depends on NET_ETHERNET && PPC_PSERIES
+	---help---
+	  This is the Virtual Ethernet adapter driver.  
+
+	  To compile this driver as a module, choose M here and read
+	  <file:Documentation/networking/net-modules.txt>. 
+
 config FDDI
 	bool "FDDI driver support"
 	depends on NETDEVICES && (PCI || EISA)
diff -purN linux-2.5/drivers/net/Makefile linuxppc64-2.5/drivers/net/Makefile
--- linux-2.5/drivers/net/Makefile	2004-03-02 07:35:14.000000000 +0000
+++ linuxppc64-2.5/drivers/net/Makefile	2004-03-13 03:17:48.000000000 +0000
@@ -45,6 +45,7 @@ obj-$(CONFIG_SIS190) += sis190.o
 obj-$(CONFIG_SIS900) += sis900.o
 obj-$(CONFIG_YELLOWFIN) += yellowfin.o
 obj-$(CONFIG_ACENIC) += acenic.o
+obj-$(CONFIG_ISERIES_VETH) += iseries_veth.o
 obj-$(CONFIG_NATSEMI) += natsemi.o
 obj-$(CONFIG_NS83820) += ns83820.o
 obj-$(CONFIG_STNIC) += stnic.o 8390.o
diff -purN linux-2.5/drivers/net/e1000/e1000_main.c linuxppc64-2.5/drivers/net/e1000/e1000_main.c
--- linux-2.5/drivers/net/e1000/e1000_main.c	2004-02-20 22:09:55.000000000 +0000
+++ linuxppc64-2.5/drivers/net/e1000/e1000_main.c	2004-02-26 05:41:58.000000000 +0000
@@ -1582,7 +1582,7 @@ e1000_tx_map(struct e1000_adapter *adapt
 
 	i = tx_ring->next_to_use;
 
-	while(len) {
+	while(len && count < E1000_DESC_UNUSED(&adapter->tx_ring)) {
 		buffer_info = &tx_ring->buffer_info[i];
 		size = min(len, max_per_txd);
 #ifdef NETIF_F_TSO
@@ -1619,7 +1619,7 @@ e1000_tx_map(struct e1000_adapter *adapt
 		len = frag->size;
 		offset = frag->page_offset;
 
-		while(len) {
+		while(len && count < E1000_DESC_UNUSED(&adapter->tx_ring)) {
 			buffer_info = &tx_ring->buffer_info[i];
 			size = min(len, max_per_txd);
 #ifdef NETIF_F_TSO
diff -purN linux-2.5/drivers/net/iseries_veth.c linuxppc64-2.5/drivers/net/iseries_veth.c
--- linux-2.5/drivers/net/iseries_veth.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/net/iseries_veth.c	2004-03-17 05:59:42.000000000 +0000
@@ -0,0 +1,1339 @@
+/* File veth.c created by Kyle A. Lucke on Mon Aug  7 2000. */
+/*
+ * IBM eServer iSeries Virtual Ethernet Device Driver
+ * Copyright (C) 2001 Kyle A. Lucke (klucke@us.ibm.com), IBM Corp.
+ * Substantially cleaned up by:
+ * Copyright (C) 2003 David Gibson <dwg@au1.ibm.com>, IBM Corporation.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *                                                                      
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307
+ * USA
+ *
+ *
+ * This module implements the virtual ethernet device for iSeries LPAR
+ * Linux.  It uses hypervisor message passing to enable an
+ * ethernet-like network device communicating between partitions on
+ * the iSeries.
+ *
+ * The iSeries LPAR hypervisor currently allows for up to 16 different
+ * virtual ethernets.  These are all dynamically configurable on
+ * OS/400 partitions, but dynamic configuration is not supported under
+ * Linux yet.  An ethXX network device will be created for each
+ * virtual ethernet this partition is connected to.
+ *
+ * - This driver is responsible for routing packets to and from other
+ *   partitions.  The MAC addresses used by the virtual ethernets
+ *   contains meaning and must not be modified.
+ *
+ * - Having 2 virtual ethernets to the same remote partition DOES NOT
+ *   double the available bandwidth.  The 2 devices will share the
+ *   available hypervisor bandwidth.
+ *
+ * - If you send a packet to your own mac address, it will just be
+ *   dropped, you won't get it on the receive side.
+ *
+ * - Multicast is implemented by sending the frame frame to every
+ *   other partition.  It is the responsibility of the receiving
+ *   partition to filter the addresses desired.
+ *
+ * Tunable parameters:
+ *
+ * VETH_NUMBUFFERS: This compile time option defaults to 120.  It
+ * controls how much memory Linux will allocate per remote partition
+ * it is communicating with.  It can be thought of as the maximum
+ * number of packets outstanding to a remote partition at a time.
+ */
+
+#include <linux/config.h>
+#include <linux/module.h>
+#include <linux/version.h>
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <linux/ioport.h>
+#include <linux/pci.h>
+#include <linux/kernel.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/skbuff.h>
+#include <linux/init.h>
+#include <linux/delay.h>
+#include <linux/mm.h>
+#include <linux/ethtool.h>
+#include <asm/iSeries/mf.h>
+#include <asm/uaccess.h>
+
+#include <asm/iSeries/HvLpConfig.h>
+#include <asm/iSeries/HvTypes.h>
+#include <asm/iSeries/HvLpEvent.h>
+#include <asm/iommu.h>
+
+#include "iseries_veth.h"
+
+extern struct pci_dev *iSeries_veth_dev;
+
+#define veth_printk(prio, fmt, args...) \
+	printk(prio "%s: " fmt, __FILE__, ## args)
+
+#define veth_error(fmt, args...) \
+	printk(KERN_ERR "(%s:%3.3d) ERROR: " fmt, __FILE__, __LINE__ , ## args)
+
+#define VETH_NUMBUFFERS		120
+
+#if VETH_NUMBUFFERS < 10
+#define ACK_THRESHOLD 1
+#elif VETH_NUMBUFFERS < 20
+#define ACK_THRESHOLD 4
+#elif VETH_NUMBUFFERS < 40
+#define ACK_THRESHOLD 10
+#else
+#define ACK_THRESHOLD 20
+#endif
+
+static int veth_open(struct net_device *dev);
+static int veth_close(struct net_device *dev);
+static int veth_start_xmit(struct sk_buff *skb, struct net_device *dev);
+static int veth_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd);
+static void veth_handle_event(struct HvLpEvent *, struct pt_regs *);
+static void veth_handle_ack(struct VethLpEvent *);
+static void veth_handle_int(struct VethLpEvent *);
+static void veth_init_connection(struct veth_lpar_connection *cnx, u8 rlp);
+static void veth_open_connection(u8);
+static void veth_finish_open_connection(void *parm);
+static void veth_close_connection(u8);
+static void veth_set_multicast_list(struct net_device *dev);
+
+static void veth_take_cap(struct veth_lpar_connection *, struct VethLpEvent *);
+static void veth_take_cap_ack(struct veth_lpar_connection *, struct VethLpEvent *);
+static void veth_take_monitor_ack(struct veth_lpar_connection *,
+				  struct VethLpEvent *);
+static void veth_recycle_msg(struct veth_lpar_connection *, struct veth_msg *);
+static void veth_monitor_ack_task(void *);
+static void veth_receive(struct veth_lpar_connection *, struct VethLpEvent *);
+static struct net_device_stats *veth_get_stats(struct net_device *dev);
+static int veth_change_mtu(struct net_device *dev, int new_mtu);
+static void veth_timed_ack(unsigned long connectionPtr);
+static void veth_failMe(struct veth_lpar_connection *cnx);
+
+static struct VethFabricMgr *mFabricMgr; /* = NULL */
+static struct net_device *veth_devices[HVMAXARCHITECTEDVIRTUALLANS];
+static int veth_num_devices; /* = 0 */
+
+#define VETH_MAX_MTU		9000
+
+MODULE_AUTHOR("Kyle Lucke <klucke@us.ibm.com>");
+MODULE_DESCRIPTION("iSeries Virtual ethernet driver");
+MODULE_LICENSE("GPL");
+
+static int VethModuleReopen = 1;
+
+static inline u64 veth_dma_addr(void *p)
+{
+	return 0x8000000000000000LL | virt_to_absolute((unsigned long) p);
+}
+
+static inline HvLpEvent_Rc 
+veth_signalevent(struct veth_lpar_connection *cnx, u16 subtype, 
+		 HvLpEvent_AckInd ackind, HvLpEvent_AckType acktype,
+		 u64 token,
+		 u64 data1, u64 data2, u64 data3, u64 data4, u64 data5)
+{
+	return HvCallEvent_signalLpEventFast(cnx->remote_lp,
+					     HvLpEvent_Type_VirtualLan,
+					     subtype, ackind, acktype,
+					     cnx->src_inst,
+					     cnx->dst_inst,
+					     token, data1, data2, data3,
+					     data4, data5);
+}
+
+static inline HvLpEvent_Rc veth_signaldata(struct veth_lpar_connection *cnx,
+					   u16 subtype, u64 token, void *data)
+{
+	u64 *p = (u64 *) data;
+
+	return veth_signalevent(cnx, subtype, HvLpEvent_AckInd_NoAck,
+				HvLpEvent_AckType_ImmediateAck,
+				token, p[0], p[1], p[2], p[3], p[4]);
+}
+
+struct veth_allocation {
+	struct completion c;
+	int num;
+};
+
+static void veth_complete_allocation(void *parm, int number)
+{
+	struct veth_allocation *vc = (struct veth_allocation *)parm;
+
+	vc->num = number;
+	complete(&vc->c);
+}
+
+static int veth_allocate_events(HvLpIndex rlp, int number)
+{
+	struct veth_allocation vc = { COMPLETION_INITIALIZER(vc.c), 0 };
+
+	mf_allocateLpEvents(rlp, HvLpEvent_Type_VirtualLan,
+			    sizeof(struct VethLpEvent), number,
+			    &veth_complete_allocation, &vc);
+	wait_for_completion(&vc.c);
+
+	return vc.num;
+}
+
+struct net_device * __init veth_probe_one(int vlan)
+{
+	struct net_device *dev;
+	struct veth_port *port;
+	int i, rc;
+
+	dev = alloc_etherdev(sizeof (struct veth_port));
+	if (! dev) {
+		veth_error("Unable to allocate net_device structure!\n");
+		return NULL;
+	}
+
+	port = (struct veth_port *) dev->priv;
+
+	spin_lock_init(&port->pending_gate);
+	rwlock_init(&port->mcast_gate);
+
+	for (i = 0; i < HVMAXARCHITECTEDLPS; i++) {
+		HvLpVirtualLanIndexMap map;
+
+		if (i == mFabricMgr->this_lp)
+			continue;
+		map = HvLpConfig_getVirtualLanIndexMapForLp(i);
+		if (map & (0x8000 >> vlan))
+			port->lpar_map |= (1 << i);
+	}
+
+	dev->dev_addr[0] = 0x02;
+	dev->dev_addr[1] = 0x01;
+	dev->dev_addr[2] = 0xff;
+	dev->dev_addr[3] = vlan;
+	dev->dev_addr[4] = 0xff;
+	dev->dev_addr[5] = HvLpConfig_getLpIndex_outline();
+
+	dev->mtu = VETH_MAX_MTU;
+
+	memcpy(&port->mac_addr, dev->dev_addr, 6);
+
+	dev->open = veth_open;
+	dev->hard_start_xmit = veth_start_xmit;
+	dev->stop = veth_close;
+	dev->get_stats = veth_get_stats;
+	dev->change_mtu = veth_change_mtu;
+	dev->set_mac_address = NULL;
+	dev->set_multicast_list = veth_set_multicast_list;
+	dev->do_ioctl = veth_ioctl;
+
+	rc = register_netdev(dev);
+	if (rc != 0) {
+		veth_printk(KERN_ERR,
+			    "Failed to register an ethernet device for vlan %d\n",
+			    vlan);
+		free_netdev(dev);
+		return NULL;
+	}
+
+	veth_printk(KERN_DEBUG, "%s attached to iSeries vlan %d (lpar_map=0x%04x)\n",
+		    dev->name, vlan, port->lpar_map);
+
+	return dev;
+}
+
+int __init veth_probe(void)
+{
+	HvLpIndexMap vlan_map = HvLpConfig_getVirtualLanIndexMap();
+	int i;
+
+	memset(veth_devices, 0, sizeof(veth_devices));
+
+	for (i = 0; vlan_map != 0; vlan_map <<= 1, i++) {
+		struct net_device *dev = NULL;
+
+		if (! (vlan_map & 0x8000))
+			continue;
+
+		dev = veth_probe_one(i);
+
+		if (dev) {
+			mFabricMgr->netdev[i] = dev;
+			veth_devices[veth_num_devices] = dev;
+			veth_num_devices++;
+		}
+	}
+
+	if (veth_num_devices == 0)
+		return -ENODEV;
+
+	return 0;
+}
+
+void __exit veth_module_cleanup(void)
+{
+	int i;
+	struct VethFabricMgr *fm = mFabricMgr;
+
+	if (! mFabricMgr)
+		return;
+
+	VethModuleReopen = 0;
+	
+	for (i = 0; i < HVMAXARCHITECTEDLPS; ++i) {
+		struct veth_lpar_connection *cnx = &mFabricMgr->connection[i];
+		unsigned long flags;
+
+		spin_lock_irqsave(&cnx->status_gate, flags);
+		veth_close_connection(i);
+		spin_unlock_irqrestore(&cnx->status_gate, flags);
+	}
+	
+	flush_scheduled_work();
+	
+	HvLpEvent_unregisterHandler(HvLpEvent_Type_VirtualLan);
+	
+	mb();
+	mFabricMgr = NULL;
+	mb();
+	
+	for (i = 0; i < HVMAXARCHITECTEDLPS; ++i) {
+		struct veth_lpar_connection *cnx = &fm->connection[i];
+
+		if (cnx->mNumberAllocated + cnx->mNumberRcvMsgs > 0) {
+			mf_deallocateLpEvents(cnx->remote_lp,
+					      HvLpEvent_Type_VirtualLan,
+					      cnx->mNumberAllocated
+					      + cnx->mNumberRcvMsgs,
+					      NULL, NULL);
+		}
+		
+		if (cnx->msgs)
+			kfree(cnx->msgs);
+	}
+	
+	for (i = 0; i < HVMAXARCHITECTEDVIRTUALLANS; ++i) {
+		struct net_device *dev;
+
+		if (! fm->netdev[i])
+			continue;
+
+		dev = fm->netdev[i];
+		fm->netdev[i] = NULL;
+
+		mb();
+			
+		if (dev) {
+			unregister_netdev(dev);
+			free_netdev(dev);
+		}
+	}
+	
+	kfree(fm);
+}
+
+module_exit(veth_module_cleanup);
+
+int __init veth_module_init(void)
+{
+	int i;
+	int this_lp;
+	int rc;
+
+	mFabricMgr = kmalloc(sizeof (struct VethFabricMgr), GFP_KERNEL);
+	if (! mFabricMgr) {
+		veth_error("Unable to allocate fabric manager\n");
+		return -ENOMEM;
+	}
+
+	memset(mFabricMgr, 0, sizeof (*mFabricMgr));
+
+	this_lp = HvLpConfig_getLpIndex_outline();
+	mFabricMgr->this_lp = this_lp;
+
+	for (i = 0; i < HVMAXARCHITECTEDLPS; ++i) {
+		struct veth_lpar_connection *cnx = &mFabricMgr->connection[i];
+
+		veth_init_connection(cnx, i);
+	}
+
+	rc = veth_probe();
+	if (rc != 0) {
+		veth_module_cleanup();
+		return rc;
+	}
+
+	HvLpEvent_registerHandler(HvLpEvent_Type_VirtualLan, &veth_handle_event);
+
+	/* Run through the active lps and open connections to the ones
+	 * we need to */
+	/* FIXME: is there any reason to do this backwards? */
+	for (i = HVMAXARCHITECTEDLPS - 1; i >= 0; --i) {
+		struct veth_lpar_connection *cnx = &mFabricMgr->connection[i];
+
+		if ( (i == this_lp) 
+		     || ! HvLpConfig_doLpsCommunicateOnVirtualLan(this_lp, i) )
+			continue;
+
+		spin_lock_irq(&cnx->status_gate);
+		veth_open_connection(i);
+		spin_unlock_irq(&cnx->status_gate);
+	}
+
+	return 0;
+}
+
+module_init(veth_module_init);
+
+static int veth_open(struct net_device *dev)
+{
+	struct veth_port *port = (struct veth_port *) dev->priv;
+
+	memset(&port->stats, 0, sizeof (port->stats));
+
+	netif_start_queue(dev);
+
+	return 0;
+}
+
+static int veth_close(struct net_device *dev)
+{
+	netif_stop_queue(dev);
+
+	return 0;
+}
+
+static struct net_device_stats *veth_get_stats(struct net_device *dev)
+{
+	struct veth_port *port = (struct veth_port *) dev->priv;
+
+	return &port->stats;
+}
+
+static int veth_change_mtu(struct net_device *dev, int new_mtu)
+{
+	if ((new_mtu < 68) || (new_mtu > VETH_MAX_MTU))
+		return -EINVAL;
+	dev->mtu = new_mtu;
+	return 0;
+}
+
+static int veth_transmit_to_one(struct sk_buff *skb, HvLpIndex rlp,
+				struct net_device *dev)
+{
+	struct veth_lpar_connection *cnx = mFabricMgr->connection + rlp;
+	HvLpEvent_Rc rc;
+	u32 dma_address, dma_length;
+	struct veth_msg *msg = NULL;
+
+	if (! cnx->status.ready)
+		return 2;
+
+	if ((skb->len - 14) > VETH_MAX_MTU)
+		return 2;
+
+	VETHSTACKPOP(&cnx->msg_stack, msg);
+
+	if (! msg)
+		return 1;
+
+	dma_length = skb->len;
+	dma_address = pci_map_single(iSeries_veth_dev, skb->data,
+				     dma_length, PCI_DMA_TODEVICE);
+	
+	/* Is it really necessary to check the length and address
+	 * fields of the first entry here? */
+	if (dma_address != NO_TCE) {
+		msg->skb = skb;
+		msg->data.addr[0] = dma_address;
+		msg->data.len[0] = dma_length;
+		msg->data.eof = 1;
+		set_bit(0, &(msg->in_use));
+		rc = veth_signaldata(cnx, VethEventTypeFrames,
+				     msg->token, &msg->data);
+	} else {
+		struct veth_port *port = (struct veth_port *) dev->priv;
+		rc = -1;	/* Bad return code */
+		port->stats.tx_errors++;
+	}
+	
+	if (rc != HvLpEvent_Rc_Good) {
+		msg->skb = NULL;
+		/* need to set in use to make veth_recycle_msg in case
+		 * this was a mapping failure */
+		set_bit(0, &msg->in_use);
+		veth_recycle_msg(cnx, msg);
+		return 2;
+	}
+
+	return 0;
+}
+
+static HvLpIndexMap veth_transmit_to_many(struct sk_buff *skb,
+					  HvLpIndexMap lpmask,
+					  struct net_device *dev)
+{
+	struct veth_port *port = (struct veth_port *) dev->priv;
+	int i;
+	int rc;
+
+	for (i = 0; i < HVMAXARCHITECTEDLPS; i++) {
+		struct sk_buff *clone;
+
+		if (! lpmask & (1<<i))
+			continue;
+		
+		clone = skb_clone(skb, GFP_ATOMIC);
+
+		if (! clone) {
+			veth_error("%s: skb_clone failed %p\n",
+				   dev->name, skb);
+			continue;
+		}
+		/* the ack handles deleting the skb */
+		rc = veth_transmit_to_one(clone, i, dev);
+		
+		/* tx failed, we need to free the skb */
+		if (rc != 0)
+			dev_kfree_skb(clone);
+			
+		/* if we didn't fail from lack of buffers, the tx as a
+		 * whole is successful */
+		if (rc != 1)
+			lpmask &= ~(1<<i);
+	}
+
+	if (! lpmask) {
+		port->stats.tx_packets++;
+		port->stats.tx_bytes += skb->len;
+	}
+
+	return lpmask;
+}
+
+static int veth_start_xmit(struct sk_buff *skb, struct net_device *dev)
+{
+	unsigned char *frame = skb->data;
+	struct veth_port *port = (struct veth_port *) dev->priv;
+	unsigned long flags;
+	HvLpIndexMap lpmask;
+
+	BUG_ON(! mFabricMgr);
+
+	if (! (frame[0] & 0x01)) {
+		/* unicast packet */
+		HvLpIndex rlp = frame[5];
+
+		if ( ! ((1 << rlp) & port->lpar_map) ) {
+			dev_kfree_skb(skb);
+			return 0;
+		}
+
+		lpmask = 1 << rlp;
+	} else {
+		lpmask = port->lpar_map;
+	}
+
+	lpmask = veth_transmit_to_many(skb, lpmask, dev);
+
+	if (lpmask) {
+		spin_lock_irqsave(&port->pending_gate, flags);
+		if (port->pending_skb) {
+			veth_error("%s: Tx while skb was pending!\n", dev->name);
+			dev_kfree_skb(skb);
+			return 1;
+		}
+
+		port->pending_skb = skb;
+		port->pending_lpmask = lpmask;
+		netif_stop_queue(dev);
+
+		spin_unlock_irqrestore(&port->pending_gate, flags);
+	}
+
+	dev_kfree_skb(skb);
+	return 0;
+}
+
+static int veth_ioctl(struct net_device *dev, struct ifreq *ifr, int cmd)
+{
+#ifdef SIOCETHTOOL
+	struct ethtool_cmd ecmd;
+
+	if (cmd != SIOCETHTOOL)
+		return -EOPNOTSUPP;
+	if (copy_from_user(&ecmd, ifr->ifr_data, sizeof (ecmd)))
+		return -EFAULT;
+	switch (ecmd.cmd) {
+	case ETHTOOL_GSET:
+		ecmd.supported = (SUPPORTED_1000baseT_Full
+				  | SUPPORTED_Autoneg | SUPPORTED_FIBRE);
+		ecmd.advertising = (SUPPORTED_1000baseT_Full
+				    | SUPPORTED_Autoneg | SUPPORTED_FIBRE);
+
+		ecmd.port = PORT_FIBRE;
+		ecmd.transceiver = XCVR_INTERNAL;
+		ecmd.phy_address = 0;
+		ecmd.speed = SPEED_1000;
+		ecmd.duplex = DUPLEX_FULL;
+		ecmd.autoneg = AUTONEG_ENABLE;
+		ecmd.maxtxpkt = 120;
+		ecmd.maxrxpkt = 120;
+		if (copy_to_user(ifr->ifr_data, &ecmd, sizeof(ecmd)))
+			return -EFAULT;
+		return 0;
+
+	case ETHTOOL_GDRVINFO:{
+			struct ethtool_drvinfo info = { ETHTOOL_GDRVINFO };
+			strncpy(info.driver, "veth", sizeof(info.driver) - 1);
+			info.driver[sizeof(info.driver) - 1] = '\0';
+			strncpy(info.version, "1.0", sizeof(info.version) - 1);
+			if (copy_to_user(ifr->ifr_data, &info, sizeof(info)))
+				return -EFAULT;
+			return 0;
+		}
+		/* get link status */
+	case ETHTOOL_GLINK:{
+			struct ethtool_value edata = { ETHTOOL_GLINK };
+			edata.data = 1;
+			if (copy_to_user(ifr->ifr_data, &edata, sizeof(edata)))
+				return -EFAULT;
+			return 0;
+		}
+
+	default:
+		break;
+	}
+
+#endif
+	return -EOPNOTSUPP;
+}
+
+static void veth_set_multicast_list(struct net_device *dev)
+{
+	struct veth_port *port = (struct veth_port *) dev->priv;
+	unsigned long flags;
+
+	write_lock_irqsave(&port->mcast_gate, flags);
+
+	if (dev->flags & IFF_PROMISC) {	/* set promiscuous mode */
+		printk(KERN_INFO "%s: Promiscuous mode enabled.\n",
+		       dev->name);
+		port->promiscuous = 1;
+	} else if ( (dev->flags & IFF_ALLMULTI)
+		    || (dev->mc_count > VETH_MAX_MCAST) ) {
+		port->all_mcast = 1;
+	} else {
+		struct dev_mc_list *dmi = dev->mc_list;
+		int i;
+
+		/* Update table */
+		port->num_mcast = 0;
+		
+		for (i = 0; i < dev->mc_count; i++) {
+			u8 *addr = dmi->dmi_addr;
+			u64 xaddr = 0;
+
+			if (addr[0] & 0x01) {/* multicast address? */
+				memcpy(&xaddr, addr, ETH_ALEN);
+				port->mcast_addr[port->num_mcast] = xaddr;
+				port->num_mcast++;
+			}
+			dmi = dmi->next;
+		}
+	}
+
+	write_unlock_irqrestore(&port->mcast_gate, flags);
+}
+
+static void veth_handle_event(struct HvLpEvent *event, struct pt_regs *regs)
+{
+	struct VethLpEvent *veth_event = (struct VethLpEvent *)event;
+
+	if (event->xFlags.xFunction == HvLpEvent_Function_Ack)
+		veth_handle_ack(veth_event);
+	else if (event->xFlags.xFunction == HvLpEvent_Function_Int)
+		veth_handle_int(veth_event);
+}
+
+static void veth_handle_ack(struct VethLpEvent *event)
+{
+	HvLpIndex rlp = event->base_event.xTargetLp;
+	struct veth_lpar_connection *cnx = &mFabricMgr->connection[rlp];
+
+	switch (event->base_event.xSubtype) {
+	case VethEventTypeCap:
+		veth_take_cap_ack(cnx, event);
+		break;
+	case VethEventTypeMonitor:
+		veth_take_monitor_ack(cnx, event);
+		break;
+	default:
+		veth_error("Unknown ack type %d from lpar %d\n",
+			   event->base_event.xSubtype, rlp);
+	};
+}
+
+static void veth_handle_int(struct VethLpEvent *event)
+{
+	HvLpIndex rlp = event->base_event.xSourceLp;
+	struct veth_lpar_connection *cnx = &mFabricMgr->connection[rlp];
+	int i;
+
+	switch (event->base_event.xSubtype) {
+	case VethEventTypeCap:
+		veth_take_cap(cnx, event);
+		break;
+	case VethEventTypeMonitor:
+		/* do nothing... this'll hang out here til we're dead,
+		 * and the hypervisor will return it for us. */
+		break;
+	case VethEventTypeFramesAck:
+		for (i = 0; i < VETH_MAX_ACKS_PER_MSG; ++i) {
+			u16 msgnum = event->u.frames_ack_data.token[i];
+
+			if (msgnum < VETH_NUMBUFFERS)
+				veth_recycle_msg(cnx, cnx->msgs + msgnum);
+		}
+		break;
+	case VethEventTypeFrames:
+		veth_receive(cnx, event);
+		break;
+	default:
+		veth_error("Unknown interrupt type %d from lpar %d\n",
+			   event->base_event.xSubtype, rlp);
+	};
+}
+
+static void veth_failMe(struct veth_lpar_connection *cnx)
+{
+	cnx->status.ready = 0;
+}
+
+static void veth_init_connection(struct veth_lpar_connection *cnx, u8 rlp)
+{
+	struct veth_msg *msgs;
+	HvLpIndex this_lp = mFabricMgr->this_lp;
+	int i;
+
+	veth_failMe(cnx);
+
+	cnx->remote_lp = rlp;
+
+	spin_lock_init(&cnx->ack_gate);
+	spin_lock_init(&cnx->status_gate);
+
+	cnx->status.got_cap = 0;
+	cnx->status.got_cap_ack = 0;
+
+	INIT_WORK(&cnx->finish_open_wq, veth_finish_open_connection, cnx);
+	INIT_WORK(&cnx->monitor_ack_wq, veth_monitor_ack_task, cnx);
+
+	init_timer(&cnx->ack_timer);
+	cnx->ack_timer.function = veth_timed_ack;
+	cnx->ack_timer.data = (unsigned long) cnx;
+
+	if ( (rlp == this_lp) 
+	     || ! HvLpConfig_doLpsCommunicateOnVirtualLan(this_lp, rlp) )
+		return;
+
+	msgs = kmalloc(VETH_NUMBUFFERS * sizeof(struct veth_msg), GFP_KERNEL);
+	if (! msgs)
+		return;
+
+	cnx->msgs = msgs;
+	memset(msgs, 0, VETH_NUMBUFFERS * sizeof(struct veth_msg));
+	spin_lock_init(&cnx->msg_stack.lock);
+
+	for (i = 0; i < VETH_NUMBUFFERS; i++) {
+		msgs[i].token = i;
+		VETHSTACKPUSH(&cnx->msg_stack, msgs + i);
+	}
+
+	cnx->mNumberAllocated = veth_allocate_events(rlp, 2);
+
+	if (cnx->mNumberAllocated < 2) {
+		veth_error("Couldn't allocate base msgs for lpar %d, only got %d\n",
+			   cnx->remote_lp, cnx->mNumberAllocated);
+		veth_failMe(cnx);
+		return;
+	}
+
+	cnx->mNumberRcvMsgs = veth_allocate_events(cnx->remote_lp,
+						   VETH_NUMBUFFERS);
+
+	cnx->local_caps.num_buffers = VETH_NUMBUFFERS;
+	cnx->local_caps.ack_threshold = ACK_THRESHOLD;
+	cnx->local_caps.ack_timeout = VETH_ACKTIMEOUT;
+}
+
+static void veth_open_connection(u8 rlp)
+{
+	struct veth_lpar_connection *cnx = &mFabricMgr->connection[rlp];
+	HvLpEvent_Rc rc;
+	u64 *rawcap = (u64 *) &cnx->local_caps;
+
+	if (! cnx->msgs || (cnx->mNumberAllocated < 2)
+	    || ! cnx->mNumberRcvMsgs) {
+		veth_failMe(cnx);
+		return;
+	}
+
+	spin_lock_irq(&cnx->ack_gate);
+
+	memset(&cnx->pending_acks, 0xff, sizeof (cnx->pending_acks));
+	cnx->num_pending_acks = 0;
+
+	HvCallEvent_openLpEventPath(rlp, HvLpEvent_Type_VirtualLan);
+
+	cnx->status.open = 1;
+
+	cnx->src_inst = 
+		HvCallEvent_getSourceLpInstanceId(rlp,
+						  HvLpEvent_Type_VirtualLan);
+	cnx->dst_inst =
+		HvCallEvent_getTargetLpInstanceId(rlp,
+						  HvLpEvent_Type_VirtualLan);
+
+	spin_unlock_irq(&cnx->ack_gate);
+
+	rc = veth_signalevent(cnx, VethEventTypeCap,
+			      HvLpEvent_AckInd_DoAck,
+			      HvLpEvent_AckType_ImmediateAck,
+			      0, rawcap[0], rawcap[1], rawcap[2], rawcap[3],
+			      rawcap[4]);
+
+	if ( (rc == HvLpEvent_Rc_PartitionDead)
+	     || (rc == HvLpEvent_Rc_PathClosed)) {
+		/* Never mind we'll resend out caps when we get the
+		 * caps from the other end comes up and sends
+		 * theirs */
+		return;
+	} else if (rc != HvLpEvent_Rc_Good) {
+		veth_error("Couldn't send capabilities to lpar %d, rc=%x\n",
+				  cnx->remote_lp, (int) rc);
+		veth_failMe(cnx);
+		return;
+	}
+
+	cnx->status.sent_caps = 1;
+}
+
+static void veth_finish_open_connection(void *parm)
+{
+	struct veth_lpar_connection *cnx = (struct veth_lpar_connection *)parm;
+	struct VethCapData *remote_caps = &cnx->remote_caps;
+	u64 num_acks_needed = 0;
+	HvLpEvent_Rc rc;
+
+	spin_lock_irq(&cnx->status_gate);
+
+	memcpy(remote_caps, &cnx->cap_event.u.caps_data,
+	       sizeof(*remote_caps));
+
+	/* Convert timer to jiffies */
+	if (cnx->local_caps.ack_timeout)
+		cnx->ack_timeout = remote_caps->ack_timeout * HZ / 1000000;
+	else
+		cnx->ack_timeout = VETH_ACKTIMEOUT * HZ / 1000000;
+
+	if ( (remote_caps->num_buffers == 0)
+	     || (remote_caps->ack_threshold > VETH_MAX_ACKS_PER_MSG)
+	     || (remote_caps->ack_threshold == 0) 
+	     || (cnx->ack_timeout == 0) ) {
+		veth_error("Received incompatible capabilities from lpar %d\n",
+			   cnx->remote_lp);
+		cnx->cap_event.base_event.xRc = HvLpEvent_Rc_InvalidSubtypeData;
+		HvCallEvent_ackLpEvent((struct HvLpEvent *)&cnx->cap_event);
+
+		veth_failMe(cnx);
+		goto out;
+	}
+
+	num_acks_needed = (remote_caps->num_buffers / remote_caps->ack_threshold) + 1;
+
+	if (cnx->mNumberLpAcksAlloced < num_acks_needed) {
+		int num;
+		
+		num_acks_needed = num_acks_needed - cnx->mNumberLpAcksAlloced;
+		
+		spin_unlock_irq(&cnx->status_gate);
+		
+		num = veth_allocate_events(cnx->remote_lp, num_acks_needed);
+		
+		if (num > 0)
+			cnx->mNumberLpAcksAlloced += num;
+		spin_lock_irq(&cnx->status_gate);
+	}
+	
+	if (cnx->mNumberLpAcksAlloced < num_acks_needed) {
+		veth_error("Couldn't allocate all the frames ack events for lpar %d\n",
+			   cnx->remote_lp);
+
+		cnx->cap_event.base_event.xRc = HvLpEvent_Rc_BufferNotAvailable;
+		HvCallEvent_ackLpEvent((struct HvLpEvent *)&cnx->cap_event);
+
+		veth_failMe(cnx);
+		goto out;
+	}
+
+	rc = HvCallEvent_ackLpEvent((struct HvLpEvent *)&cnx->cap_event);
+	if (rc != HvLpEvent_Rc_Good) {
+		veth_error("Failed to ack remote cap for lpar %d with rc %x\n",
+			   cnx->remote_lp, (int) rc);
+		veth_failMe(cnx);
+		goto out;
+	}
+
+	if (cnx->cap_ack_event.base_event.xRc != HvLpEvent_Rc_Good) {
+		veth_printk(KERN_ERR, "Bad rc(%d) from lpar %d on capabilities\n",
+			    cnx->cap_ack_event.base_event.xRc, cnx->remote_lp);
+		veth_failMe(cnx);
+		goto out;
+	}
+
+	/* Send the monitor */
+	rc = veth_signalevent(cnx, VethEventTypeMonitor,
+			      HvLpEvent_AckInd_DoAck,
+			      HvLpEvent_AckType_DeferredAck,
+			      0, 0, 0, 0, 0, 0);
+	
+	if (rc != HvLpEvent_Rc_Good) {
+		veth_error("Monitor send to lpar %d failed with rc %x\n",
+				  cnx->remote_lp, (int) rc);
+		veth_failMe(cnx);
+		goto out;
+	}
+
+	cnx->status.ready = 1;
+	
+	/* Start the ACK timer */
+	cnx->ack_timer.expires = jiffies + cnx->ack_timeout;
+	add_timer(&cnx->ack_timer);
+
+ out:
+	spin_unlock_irq(&cnx->status_gate);
+}
+
+static void veth_close_connection(u8 rlp)
+{
+	struct veth_lpar_connection *cnx = &mFabricMgr->connection[rlp];
+	unsigned long flags;
+
+	del_timer_sync(&cnx->ack_timer);
+
+	cnx->status.sent_caps = 0;
+	cnx->status.got_cap = 0;
+	cnx->status.got_cap_ack = 0;
+
+	if (cnx->status.open) {
+		int i;		
+
+		HvCallEvent_closeLpEventPath(rlp, HvLpEvent_Type_VirtualLan);
+		cnx->status.open = 0;
+		veth_failMe(cnx);
+
+		/* reset ack data */
+		spin_lock_irqsave(&cnx->ack_gate, flags);
+
+		memset(&cnx->pending_acks, 0xff, sizeof (cnx->pending_acks));
+		cnx->num_pending_acks = 0;
+
+		spin_unlock_irqrestore(&cnx->ack_gate, flags);
+
+		/* Clean up any leftover messages */
+		for (i = 0; i < VETH_NUMBUFFERS; ++i)
+			veth_recycle_msg(cnx, cnx->msgs + i);
+	}
+
+}
+
+static void veth_take_cap(struct veth_lpar_connection *cnx, struct VethLpEvent *event)
+{
+	unsigned long flags;
+	HvLpEvent_Rc rc;
+
+	spin_lock_irqsave(&cnx->status_gate, flags);
+
+	if (cnx->status.got_cap) {
+		veth_error("Received a second capabilities from lpar %d\n",
+			   cnx->remote_lp);
+		event->base_event.xRc = HvLpEvent_Rc_BufferNotAvailable;
+		HvCallEvent_ackLpEvent((struct HvLpEvent *) event);
+		goto out;
+	}
+
+	memcpy(&cnx->cap_event, event, sizeof (cnx->cap_event));
+	/* If we failed to send caps out before (presumably because
+	 * the target lpar was down), send them now. */
+	if (! cnx->status.sent_caps) {
+		u64 *rawcap = (u64 *) &cnx->local_caps;
+
+		cnx->dst_inst =
+			HvCallEvent_getTargetLpInstanceId(cnx->remote_lp,
+							  HvLpEvent_Type_VirtualLan);
+
+		rc = veth_signalevent(cnx, VethEventTypeCap,
+				      HvLpEvent_AckInd_DoAck,
+				      HvLpEvent_AckType_ImmediateAck,
+				      0, rawcap[0], rawcap[1], rawcap[2], rawcap[3],
+				      rawcap[4]);
+		if ( (rc == HvLpEvent_Rc_PartitionDead)
+		     || (rc == HvLpEvent_Rc_PathClosed)) {
+			veth_error("Partition down when resending capabilities!!\n");
+			goto out;
+		} else if (rc != HvLpEvent_Rc_Good) {
+			veth_error("Couldn't send cap to lpar %d, rc %x\n",
+				   cnx->remote_lp, (int) rc);
+			veth_failMe(cnx);
+			goto out;
+		}
+		cnx->status.sent_caps = 1;
+	}
+
+	cnx->status.got_cap = 1;
+	if (cnx->status.got_cap_ack)
+		schedule_work(&cnx->finish_open_wq);
+
+ out:
+	spin_unlock_irqrestore(&cnx->status_gate, flags);
+}
+
+static void veth_take_cap_ack(struct veth_lpar_connection *cnx,
+			      struct VethLpEvent *event)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&cnx->status_gate, flags);
+
+	if (cnx->status.got_cap_ack) {
+		veth_error("Received a second capabilities ack from lpar %d\n",
+			   cnx->remote_lp);
+		goto out;
+	}
+
+	memcpy(&cnx->cap_ack_event, event, sizeof(&cnx->cap_ack_event));
+	cnx->status.got_cap_ack = 1;
+
+	if (cnx->status.got_cap)
+		schedule_work(&cnx->finish_open_wq);
+
+ out:
+	spin_unlock_irqrestore(&cnx->status_gate, flags);
+}
+
+static void veth_take_monitor_ack(struct veth_lpar_connection *cnx,
+				  struct VethLpEvent *event)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&cnx->status_gate, flags);
+
+	veth_printk(KERN_DEBUG, "Monitor ack returned for lpar %d\n", cnx->remote_lp);
+
+	if (cnx->status.monitor_ack_pending) {
+		veth_error("Received a monitor ack from lpar %d while already processing one\n",
+			   cnx->remote_lp);
+		goto out;
+	}
+
+	schedule_work(&cnx->monitor_ack_wq);
+
+ out:
+	spin_unlock_irqrestore(&cnx->status_gate, flags);
+}
+
+static void veth_recycle_msg(struct veth_lpar_connection *cnx,
+			     struct veth_msg *myMsg)
+{
+	u32 dma_address, dma_length;
+	int i;
+
+	if (test_and_clear_bit(0, &myMsg->in_use)) {
+		dma_address = myMsg->data.addr[0];
+		dma_length = myMsg->data.len[0];
+
+		pci_unmap_single(iSeries_veth_dev, dma_address, dma_length,
+				 PCI_DMA_TODEVICE);
+
+		if (myMsg->skb) {
+			dev_kfree_skb_any(myMsg->skb);
+			myMsg->skb = NULL;
+		}
+
+		memset(&myMsg->data, 0, sizeof(myMsg->data));
+		VETHSTACKPUSH(&cnx->msg_stack, myMsg);
+	} else {
+		if (cnx->status.open)
+			veth_error("Bogus frames ack from lpar %d (index=%d)\n",
+				   cnx->remote_lp, myMsg->token);
+	}
+
+	for (i = 0; i < veth_num_devices; i++) {
+		struct net_device *dev = veth_devices[i];
+		struct veth_port *port = (struct veth_port *)dev->priv;
+		unsigned long flags;
+
+		if (! (port->lpar_map & (1<<cnx->remote_lp)))
+			continue;
+
+		spin_lock_irqsave(&port->pending_gate, flags);
+		if (port->pending_skb) {
+			port->pending_lpmask =
+				veth_transmit_to_many(port->pending_skb,
+						      port->pending_lpmask,
+						      dev);
+			if (! port->pending_lpmask) {
+				dev_kfree_skb_any(port->pending_skb);
+				port->pending_skb = NULL;
+				netif_start_queue(dev);
+			}
+		}
+		spin_unlock_irqrestore(&port->pending_gate, flags);
+	}
+}
+
+static void veth_monitor_ack_task(void *parm)
+{
+	struct veth_lpar_connection *cnx = (struct veth_lpar_connection *) parm;
+
+	spin_lock_irq(&cnx->status_gate);
+
+	veth_failMe(cnx);
+
+	if (cnx->status.open) {
+		veth_close_connection(cnx->remote_lp);
+
+		udelay(100);
+	}
+
+	if (VethModuleReopen)
+		veth_open_connection(cnx->remote_lp);
+
+	cnx->status.monitor_ack_pending = 0;
+
+	spin_unlock_irq(&cnx->status_gate);
+}
+
+static inline int veth_frame_wanted(struct veth_port *port, u64 mac_addr)
+{
+	int wanted = 0;
+	int i;
+	unsigned long flags;
+
+	if ( (mac_addr == port->mac_addr)
+	     || (mac_addr == 0xffffffffffff0000)
+	     || port->promiscuous )
+		return 1;
+	
+	if (! (((char *) &mac_addr)[0] & 0x01))
+		return 0;
+
+	read_lock_irqsave(&port->mcast_gate, flags);
+
+	if (port->all_mcast) {
+		wanted = 1;
+		goto out;
+	}
+
+	for (i = 0; i < port->num_mcast; ++i) {
+		if (port->mcast_addr[i] == mac_addr) {
+			wanted = 1;
+			break;
+		}
+	}
+
+ out:
+	read_unlock_irqrestore(&port->mcast_gate, flags);
+
+	return wanted;
+}
+
+struct dma_chunk {
+	u64 addr;
+	u64 size;
+};
+
+#define VETH_MAX_PAGES_PER_FRAME ( (VETH_MAX_MTU+PAGE_SIZE-2)/PAGE_SIZE + 1 )
+
+static inline void veth_build_dma_list(struct dma_chunk *list, unsigned char *p,
+				      unsigned long length)
+{
+	unsigned long done;
+	int i = 1;
+
+	/* FIXME: skbs are continguous in real addresses.  Do we
+	 * really need to break it into PAGE_SIZE chunks, or can we do
+	 * it just at the granularity of iSeries real->absolute
+	 * mapping? */
+	list[0].addr = veth_dma_addr(p);
+	list[0].size = min(length,
+			   PAGE_SIZE - ((unsigned long)p & ~PAGE_MASK));
+
+	done = list[0].size;
+	while (done < length) {
+		list[i].addr = veth_dma_addr(p + done);
+		list[i].size = min(done, PAGE_SIZE);
+		done += list[i].size;
+		i++;
+	}
+}
+
+static void veth_receive(struct veth_lpar_connection *cnx, struct VethLpEvent *event)
+{
+	struct VethFramesData *senddata = &event->u.frames_data;
+	int startchunk = 0;
+	int nchunks;
+	unsigned long flags;
+	HvLpDma_Rc rc;
+
+	do {
+		u16 length = 0;
+		struct sk_buff *skb;
+		struct dma_chunk local_list[VETH_MAX_PAGES_PER_FRAME];
+		struct dma_chunk remote_list[VETH_MAX_FRAMES_PER_MSG];
+		u64 dest;
+		HvLpVirtualLanIndex vlan;
+		struct net_device *dev;
+		struct veth_port *port;
+
+		/* FIXME: do we need this? */
+		memset(local_list, 0, sizeof(local_list));
+		memset(remote_list, 0, sizeof(VETH_MAX_FRAMES_PER_MSG));
+
+		nchunks = 0;
+
+		/* a 0 address marks the end of the valid entries */
+		if (senddata->addr[startchunk] == 0)
+			break;
+
+		/* make sure that we have at least 1 EOF entry in the
+		 * remaining entries */
+		if (! (senddata->eof >> startchunk)) {
+			veth_error("missing EOF frag in event: 0x%x startchunk=%d\n",
+				   (unsigned) senddata->eof, startchunk);
+			break;
+		}
+
+		/* build list of chunks in this frame */
+		do {
+			remote_list[nchunks].addr =
+				(u64) senddata->addr[startchunk + nchunks] << 32;
+			remote_list[nchunks].size =
+				senddata->len[startchunk + nchunks];
+			length += remote_list[nchunks].size;
+		} while (! (senddata->eof & (1 << (startchunk + nchunks++))));
+
+		/* length == total length of all chunks */
+		/* nchunks == # of chunks in this frame */
+
+		if ((length - ETH_HLEN) > VETH_MAX_MTU)
+			continue;
+
+		skb = alloc_skb(length, GFP_ATOMIC);
+		if (!skb)
+			continue;
+
+		veth_build_dma_list(local_list, skb->data, length);
+
+		rc = HvCallEvent_dmaBufList(HvLpEvent_Type_VirtualLan,
+					    event->base_event.xSourceLp,
+					    HvLpDma_Direction_RemoteToLocal,
+					    cnx->src_inst,
+					    cnx->dst_inst,
+					    HvLpDma_AddressType_RealAddress,
+					    HvLpDma_AddressType_TceIndex,
+					    veth_dma_addr(&local_list),
+					    veth_dma_addr(&remote_list),
+					    length);
+		if (rc != HvLpDma_Rc_Good) {
+			dev_kfree_skb_irq(skb);
+			continue;
+		}
+		
+		vlan = skb->data[9];
+		dev = mFabricMgr->netdev[vlan];
+		port = (struct veth_port *)dev->priv;
+		dest = *((u64 *) skb->data) & 0xFFFFFFFFFFFF0000;
+
+		if ((vlan > HVMAXARCHITECTEDVIRTUALLANS) || !port) {
+			dev_kfree_skb_irq(skb);
+			continue;
+		}
+		if (! veth_frame_wanted(port, dest)) {
+			dev_kfree_skb_irq(skb);
+			continue;
+		}
+			
+		skb_put(skb, length);
+		skb->dev = dev;
+		skb->protocol = eth_type_trans(skb, dev);
+		skb->ip_summed = CHECKSUM_NONE;
+		netif_rx(skb);	/* send it up */
+		port->stats.rx_packets++;
+		port->stats.rx_bytes += length;
+	} while (startchunk += nchunks, startchunk < VETH_MAX_FRAMES_PER_MSG);
+
+	/* Ack it */
+	spin_lock_irqsave(&cnx->ack_gate, flags);
+	
+	if (cnx->num_pending_acks < VETH_MAX_ACKS_PER_MSG) {
+		cnx->pending_acks[cnx->num_pending_acks] =
+			event->base_event.xCorrelationToken;
+		++cnx->num_pending_acks;
+		
+		if (cnx->num_pending_acks == cnx->remote_caps.ack_threshold) {
+			rc = veth_signaldata(cnx, VethEventTypeFramesAck,
+					     0, &cnx->pending_acks);
+			
+			if (rc != HvLpEvent_Rc_Good)
+				veth_error("Error 0x%x acking frames from lpar %d!\n",
+					   (unsigned)rc, cnx->remote_lp);
+			
+			cnx->num_pending_acks = 0;
+			memset(&cnx->pending_acks, 0xff, sizeof(cnx->pending_acks));
+		}
+		
+	}
+	
+	spin_unlock_irqrestore(&cnx->ack_gate, flags);
+}
+
+static void veth_timed_ack(unsigned long ptr)
+{
+	unsigned long flags;
+	HvLpEvent_Rc rc;
+	struct veth_lpar_connection *cnx = (struct veth_lpar_connection *) ptr;
+
+	/* Ack all the events */
+	spin_lock_irqsave(&cnx->ack_gate, flags);
+
+	if (cnx->num_pending_acks > 0) {
+		rc = veth_signaldata(cnx, VethEventTypeFramesAck,
+				     0, &cnx->pending_acks);
+		if (rc != HvLpEvent_Rc_Good)
+			veth_error("Error 0x%x acking frames from lpar %d!\n", 
+				   (unsigned) rc, cnx->remote_lp);
+
+		cnx->num_pending_acks = 0;
+		memset(&cnx->pending_acks, 0xff, sizeof(cnx->pending_acks));
+	}
+
+	spin_unlock_irqrestore(&cnx->ack_gate, flags);
+
+	/* Reschedule the timer */
+	cnx->ack_timer.expires = jiffies + cnx->ack_timeout;
+	add_timer(&cnx->ack_timer);
+}
diff -purN linux-2.5/drivers/net/iseries_veth.h linuxppc64-2.5/drivers/net/iseries_veth.h
--- linux-2.5/drivers/net/iseries_veth.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/net/iseries_veth.h	2004-03-17 05:59:42.000000000 +0000
@@ -0,0 +1,140 @@
+/* File veth.h created by Kyle A. Lucke on Mon Aug  7 2000. */
+
+#ifndef _ISERIES_VETH_H
+#define _ISERIES_VETH_H
+
+#define VethEventTypeCap	(0)
+#define VethEventTypeFrames	(1)
+#define VethEventTypeMonitor	(2)
+#define VethEventTypeFramesAck	(3)
+
+#define VETH_MAX_ACKS_PER_MSG	(20)
+#define VETH_MAX_FRAMES_PER_MSG	(6)
+
+struct VethFramesData {
+	u32 addr[VETH_MAX_FRAMES_PER_MSG];
+	u16 len[VETH_MAX_FRAMES_PER_MSG];
+	u32 eof:VETH_MAX_FRAMES_PER_MSG;
+	u32 mReserved:(32-VETH_MAX_FRAMES_PER_MSG);
+};
+
+struct VethFramesAckData {
+	u16 token[VETH_MAX_ACKS_PER_MSG];
+};
+
+struct VethCapData {
+	u8 caps_version;
+	u8 rsvd1;
+	u16 num_buffers;
+	u16 ack_threshold;
+	u16 rsvd2;
+	u32 ack_timeout;
+	u32 rsvd3;
+	u64 rsvd4[3];
+};
+
+struct VethLpEvent {
+	struct HvLpEvent base_event;
+	union {
+		struct VethCapData caps_data;
+		struct VethFramesData frames_data;
+		struct VethFramesAckData frames_ack_data;
+	} u;
+
+};
+
+#define VETH_ACKTIMEOUT 	(1000000) /* microseconds */
+#define VETH_MAX_MCAST		(12)
+
+#define HVMAXARCHITECTEDVIRTUALLANS (16)
+
+#define VETHSTACK(T) \
+	struct VethStack##T { \
+		struct T *head; \
+		spinlock_t lock; \
+	}
+#define VETHSTACKPUSH(s, p) \
+	do { \
+		unsigned long flags; \
+		spin_lock_irqsave(&(s)->lock,flags); \
+		(p)->next = (s)->head; \
+		(s)->head = (p); \
+		spin_unlock_irqrestore(&(s)->lock, flags); \
+	} while (0)
+
+#define VETHSTACKPOP(s,p) \
+	do { \
+		unsigned long flags; \
+		spin_lock_irqsave(&(s)->lock,flags); \
+		(p) = (s)->head; \
+		if ((s)->head) \
+			(s)->head = (s)->head->next; \
+		spin_unlock_irqrestore(&(s)->lock, flags); \
+	} while (0)
+
+struct veth_msg {
+	struct veth_msg *next;
+	struct VethFramesData data;
+	int token;
+	unsigned long in_use;
+	struct sk_buff *skb;
+};
+
+struct veth_lpar_connection {
+	HvLpIndex remote_lp;
+	struct work_struct finish_open_wq;
+	struct work_struct monitor_ack_wq;
+	struct timer_list ack_timer;
+	struct veth_msg *msgs;
+
+	HvLpInstanceId src_inst;
+	HvLpInstanceId dst_inst;
+
+	spinlock_t status_gate;
+	struct {
+		u64 open:1;
+		u64 ready:1;
+		u64 sent_caps:1;
+		u64 got_cap:1;
+		u64 got_cap_ack:1;
+		u64 monitor_ack_pending:1;
+	} status;
+	struct VethLpEvent cap_event, cap_ack_event;
+
+	spinlock_t ack_gate;
+	u16 pending_acks[VETH_MAX_ACKS_PER_MSG];
+	u32 num_pending_acks;
+
+	int mNumberAllocated;
+	int mNumberRcvMsgs;
+	int mNumberLpAcksAlloced;
+	struct VethCapData local_caps;
+	struct VethCapData remote_caps;
+	u32 ack_timeout;
+
+	VETHSTACK(veth_msg) msg_stack;
+};
+
+struct veth_port {
+	struct net_device_stats stats;
+	u64 mac_addr;
+	HvLpIndexMap lpar_map;
+
+	spinlock_t pending_gate;
+	struct sk_buff *pending_skb;
+	HvLpIndexMap pending_lpmask;
+
+	rwlock_t mcast_gate;
+	int promiscuous;
+	int all_mcast;
+	int num_mcast;
+	u64 mcast_addr[VETH_MAX_MCAST];
+};
+
+struct VethFabricMgr {
+	HvLpIndex this_lp;
+	struct veth_lpar_connection connection[HVMAXARCHITECTEDLPS];
+	struct net_device *netdev[HVMAXARCHITECTEDVIRTUALLANS];
+};
+
+#endif	/* _ISERIES_VETH_H */
diff -purN linux-2.5/drivers/pci/hotplug/rpaphp_pci.c linuxppc64-2.5/drivers/pci/hotplug/rpaphp_pci.c
--- linux-2.5/drivers/pci/hotplug/rpaphp_pci.c	2004-03-05 09:45:43.000000000 +0000
+++ linuxppc64-2.5/drivers/pci/hotplug/rpaphp_pci.c	2004-03-20 12:06:20.000000000 +0000
@@ -257,6 +257,7 @@ static struct pci_dev *rpaphp_config_pci
 		}
 
 		dev = rpaphp_pci_config_dn(slot->dn, pci_bus);
+		eeh_add_device(dev);
 	} else {
 		/* slot is not enabled */
 		err("slot doesn't have pci_dev structure\n");
@@ -281,6 +282,7 @@ int rpaphp_unconfig_pci_adapter(struct s
 		goto exit;
 	}
 	/* remove the device from the pci core */
+	eeh_remove_device(slot->dev.pci_dev);
 	pci_remove_bus_device(slot->dev.pci_dev);
 
 	slot->state = NOT_CONFIGURED;
diff -purN linux-2.5/drivers/pci/pci.ids linuxppc64-2.5/drivers/pci/pci.ids
--- linux-2.5/drivers/pci/pci.ids	2004-03-05 20:39:20.000000000 +0000
+++ linuxppc64-2.5/drivers/pci/pci.ids	2004-03-07 23:40:33.000000000 +0000
@@ -1820,6 +1820,8 @@
 	0010  DAC960PX
 	0050  AcceleRAID 352/170/160 support Device
 	b166  Gemstone chipset SCSI controller
+		1014 0266  Dual Channel PCI-X U320 SCSI Adapter
+		1014 0278  Dual Channel PCI-X U320 SCSI RAID Adapter
 	ba55  eXtremeRAID 1100 support Device
 	ba56  eXtremeRAID 2000/3000 support Device
 106a  Aten Research Inc
diff -purN linux-2.5/drivers/scsi/Kconfig linuxppc64-2.5/drivers/scsi/Kconfig
--- linux-2.5/drivers/scsi/Kconfig	2004-03-12 18:54:25.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/Kconfig	2004-03-13 03:17:59.000000000 +0000
@@ -752,6 +752,23 @@ config SCSI_IPS
 	  To compile this driver as a module, choose M here: the
 	  module will be called ips.
 
+config SCSI_IBMVSCSI
+	tristate "IBM Virtual SCSI support"
+	depends on PPC_PSERIES || PPC_ISERIES
+	help
+	  This is the IBM Virtual SCSI Client
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called ibmvscsic.
+
+config SCSI_IBMVSCSIS
+	tristate "IBM Virtual SCSI Server support"
+	depends on PPC_PSERIES
+	help
+	  This is the IBM Virtual SCSI Server
+	  To compile this driver as a module, choose M here: the
+	  module will be called ibmvscsis.
+
 config SCSI_INITIO
 	tristate "Initio 9100U(W) support"
 	depends on PCI && SCSI && BROKEN
@@ -970,6 +987,31 @@ config SCSI_SYM53C8XX_IOMAPPED
 	  the card.  This is significantly slower then using memory
 	  mapped IO.  Most people should answer N.
 
+config SCSI_IPR
+	tristate "IBM Power Linux RAID adapter support"
+	depends on PCI && SCSI
+	enable FW_LOADER
+	---help---
+	  This driver supports the IBM Power Linux family RAID adapters.
+	  This includes IBM pSeries 5712, 5703, 5709, and 570A, as well
+	  as IBM iSeries 5702, 5703, 5709, and 570A.
+
+config SCSI_IPR_TRACE
+	bool "enable driver internal trace"
+	depends on SCSI_IPR
+	help
+	  If you say Y here, the driver will trace all commands issued
+	  to the adapter. Performance impact is minimal. Trace can be
+	  dumped using /sys/bus/class/scsi_host/hostXX/trace.
+
+config SCSI_IPR_DUMP
+	bool "enable adapter dump support"
+	depends on SCSI_IPR
+	help
+	  If you say Y here, the driver will support adapter crash dump.
+	  If you enable this support, the iprdump daemon can be used
+	  to capture adapter failure analysis information.
+
 config SCSI_ZALON
 	tristate "Zalon SCSI support"
 	depends on GSC && SCSI
diff -purN linux-2.5/drivers/scsi/Makefile linuxppc64-2.5/drivers/scsi/Makefile
--- linux-2.5/drivers/scsi/Makefile	2004-03-12 18:54:25.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/Makefile	2004-03-13 03:23:55.000000000 +0000
@@ -117,12 +117,14 @@ obj-$(CONFIG_SCSI_FCAL)		+= fcal.o
 obj-$(CONFIG_SCSI_CPQFCTS)	+= cpqfc.o
 obj-$(CONFIG_SCSI_LASI700)	+= 53c700.o lasi700.o
 obj-$(CONFIG_SCSI_NSP32)	+= nsp32.o
+obj-$(CONFIG_SCSI_IPR)		+= ipr.o
 obj-$(CONFIG_SCSI_SATA_SVW)	+= libata.o sata_svw.o
 obj-$(CONFIG_SCSI_ATA_PIIX)	+= libata.o ata_piix.o
 obj-$(CONFIG_SCSI_SATA_PROMISE)	+= libata.o sata_promise.o
 obj-$(CONFIG_SCSI_SATA_SIL)	+= libata.o sata_sil.o
 obj-$(CONFIG_SCSI_SATA_VIA)	+= libata.o sata_via.o
 obj-$(CONFIG_SCSI_SATA_VITESSE)	+= libata.o sata_vsc.o
+obj-$(CONFIG_SCSI_IBMVSCSI)	+= ibmvscsi/
 
 obj-$(CONFIG_ARM)		+= arm/
 
diff -purN linux-2.5/drivers/scsi/ibmvscsi/Makefile linuxppc64-2.5/drivers/scsi/ibmvscsi/Makefile
--- linux-2.5/drivers/scsi/ibmvscsi/Makefile	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/ibmvscsi/Makefile	2004-02-27 21:38:44.000000000 +0000
@@ -0,0 +1,7 @@
+obj-$(CONFIG_SCSI_IBMVSCSI)	+= ibmvscsic.o
+
+ibmvscsic-y			+= ibmvscsi.o
+ibmvscsic-$(CONFIG_PPC_ISERIES)	+= iseries_vscsi.o 
+ibmvscsic-$(CONFIG_PPC_PSERIES)	+= rpa_vscsi.o 
+ 
+obj-$(CONFIG_SCSI_IBMVSCSIS)	+= ibmvscsis.o
diff -purN linux-2.5/drivers/scsi/ibmvscsi/ibmvscsi.c linuxppc64-2.5/drivers/scsi/ibmvscsi/ibmvscsi.c
--- linux-2.5/drivers/scsi/ibmvscsi/ibmvscsi.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/ibmvscsi/ibmvscsi.c	2004-03-16 20:47:27.000000000 +0000
@@ -0,0 +1,1055 @@
+/* ------------------------------------------------------------
+ * ibmvscsi.c
+ * (C) Copyright IBM Corporation 1994, 2004
+ * Authors: Colin DeVilbiss (devilbis@us.ibm.com)
+ *          Santiago Leon (santil@us.ibm.com)
+ *          Dave Boutcher (sleddog@us.ibm.com)
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307
+ * USA
+ *
+ * ------------------------------------------------------------
+ * Emulation of a SCSI host adapter for Virtual I/O devices
+ *
+ * This driver supports the SCSI adapter implemented by the IBM
+ * Power5 firmware.  That SCSI adapter is not a physical adapter,
+ * but allows Linux SCSI peripheral drivers to directly
+ * access devices in another logical partition on the physical system.
+ *
+ * The virtual adapter(s) are present in the open firmware device
+ * tree just like real adapters.
+ *
+ * One of the capabilities provided on these systems is the ability
+ * to DMA between partitions.  The architecture states that for VSCSI,
+ * the server side is allowed to DMA to and from the client.  The client
+ * is never trusted to DMA to or from the server directly.
+ *
+ * Messages are sent between partitions on a "Command/Response Queue" 
+ * (CRQ), which is just a buffer of 16 byte entries in the receiver's 
+ * Senders cannot access the buffer directly, but send messages by
+ * making a hypervisor call and passing in the 16 bytes.  The hypervisor
+ * puts the message in the next 16 byte space in round-robbin fashion,
+ * turns on the high order bit of the message (the valid bit), and 
+ * generates an interrupt to the receiver (if interrupts are turned on.) 
+ * The receiver just turns off the valid bit when they have copied out
+ * the message.
+ *
+ * The VSCSI client builds a SCSI Remote Protocol (SRP) Information Unit
+ * (IU) (as defined in the T10 standard available at www.t10.org), gets 
+ * a DMA address for the message, and sends it to the server as the
+ * payload of a CRQ message.  The server DMAs the SRP IU and processes it,
+ * including doing any additional data transfers.  When it is done, it
+ * DMAs the SRP response back to the same address as the request came from,
+ * and sends a CRQ message back to inform the client that the request has
+ * completed.
+ *
+ * Note that some of the underlying infrastructure is different between
+ * machines conforming to the "RS/6000 Platform Architecture" (RPA) and
+ * the older iSeries hypervisor models.  To support both, some low level
+ * routines have been broken out into rpa_vscsi.c and iseries_vscsi.c.
+ * The Makefile should pick one, not two, not zero, of these.
+ *
+ * TODO: This is currently pretty tied to the IBM i/pSeries hypervisor
+ * interfaces.  It would be really nice to abstract this above an RDMA
+ * layer.
+ */
+
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/dma-mapping.h>
+#include <scsi/scsi.h>
+#include <scsi/scsi_cmnd.h>
+#include <scsi/scsi_host.h>
+#include <scsi/scsi_device.h>
+#include "ibmvscsi.h"
+
+/* The values below are somewhat arbitrary default values, but 
+ * OS/400 will use 3 busses (disks, CDs, tapes, I think.)
+ * Note that there are 3 bits of channel value, 6 bits of id, and
+ * 5 bits of LUN.
+ */
+static int max_id = 64;
+static int max_channel = 3;
+static int init_timeout = 5;
+
+MODULE_DESCRIPTION("IBM Virtual SCSI");
+MODULE_AUTHOR("Dave Boutcher");
+MODULE_LICENSE("GPL");
+
+module_param_named(max_id, max_id, int, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(max_id, "Largest ID value for each channel");
+module_param_named(max_channel, max_channel, int, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(max_channel, "Largest channel value");
+module_param_named(init_timeout, init_timeout, int, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(init_timeout, "Initialization timeout in seconds");
+
+/* ------------------------------------------------------------
+ * Routines for the event pool and event structs
+ */
+/**
+ * initialize_event_pool: - Allocates and initializes the event pool for a host
+ * @pool:	event_pool to be initialized
+ * @size:	Number of events in pool
+ * @hostdata:	ibmvscsi_host_data who owns the event pool
+ *
+ * Returns zero on success.
+*/
+static int initialize_event_pool(struct event_pool *pool,
+				 int size, struct ibmvscsi_host_data *hostdata)
+{
+	int i;
+
+	pool->size = size;
+	pool->events = kmalloc(pool->size * sizeof(*pool->events), GFP_KERNEL);
+	if (!pool->events)
+		return -ENOMEM;
+	memset(pool->events, 0x00, pool->size * sizeof(*pool->events));
+
+	pool->iu_storage =
+	    dma_alloc_coherent(hostdata->dev,
+			       pool->size * sizeof(*pool->iu_storage),
+			       &pool->iu_token, 0);
+	if (!pool->iu_storage) {
+		kfree(pool->events);
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < pool->size; ++i) {
+		struct srp_event_struct *evt = &pool->events[i];
+		memset(&evt->crq, 0x00, sizeof(evt->crq));
+		evt->crq.valid = 0x80;
+		evt->crq.IU_length = sizeof(*evt->evt);
+		evt->crq.IU_data_ptr = pool->iu_token + sizeof(*evt->evt) * i;
+		evt->evt = pool->iu_storage + i;
+		evt->hostdata = hostdata;
+	}
+
+	return 0;
+}
+
+/**
+ * release_event_pool: - Frees memory of an event pool of a host
+ * @pool:	event_pool to be released
+ * @hostdata:	ibmvscsi_host_data who owns the even pool
+ *
+ * Returns zero on success.
+*/
+static void release_event_pool(struct event_pool *pool,
+			       struct ibmvscsi_host_data *hostdata)
+{
+	int i, in_use = 0;
+	for (i = 0; i < pool->size; ++i)
+		if (pool->events[i].in_use)
+			++in_use;
+	if (in_use)
+		printk(KERN_WARNING
+		       "ibmvscsi: releasing event pool with %d "
+		       "events still in use?\n",
+		       in_use);
+	kfree(pool->events);
+	dma_free_coherent(hostdata->dev,
+			  pool->size * sizeof(*pool->iu_storage),
+			  pool->iu_storage, pool->iu_token);
+}
+
+/**
+ * ibmvscsi_valid_event_struct: - Determines if event is valid.
+ * @pool:	event_pool that contains the event
+ * @evt:	srp_event_struct to be checked for validity
+ *
+ * Returns zero if event is invalid, one otherwise.
+*/
+int ibmvscsi_valid_event_struct(struct event_pool *pool,
+				struct srp_event_struct *evt)
+{
+	int index = evt - pool->events;
+	if (index < 0 || index >= pool->size)	/* outside of bounds */
+		return 0;
+	if (evt != pool->events + index)	/* unaligned */
+		return 0;
+	return 1;
+}
+
+/**
+ * ibmvscsi_free-event_struct: - Changes status of event to "free"
+ * @pool:	event_pool that contains the event
+ * @evt:	srp_event_struct to be modified
+ *
+*/
+static void ibmvscsi_free_event_struct(struct event_pool *pool,
+				       struct srp_event_struct *evt)
+{
+	if (!ibmvscsi_valid_event_struct(pool, evt)) {
+		printk(KERN_ERR
+		       "ibmvscsi: Freeing invalid event_struct %p "
+		       "(not in pool %p)\n",
+		       evt, pool->events);
+		return;
+	}
+	if (!evt->in_use) {
+		printk(KERN_ERR
+		       "ibmvscsi: Freeing event_struct %p "
+		       "which is not in use!\n",
+		       evt);
+		return;
+	}
+	evt->in_use = 0;
+}
+
+/**
+ * ibmvscsi_get_event_struct: - Gets the next free event in pool
+ * @pool:	event_pool that contains the events to be searched
+ *
+ * Returns the next event in "free" state, and NULL if none are free.
+ * Note that no synchronization is done here, we assume the host_lock
+ * will syncrhonze things.
+*/
+static
+struct srp_event_struct *ibmvscsi_get_event_struct(struct event_pool *pool)
+{
+	struct srp_event_struct *cur, *last = pool->events + pool->size;
+
+	for (cur = pool->events; cur < last; ++cur)
+		if (!cur->in_use) {
+			cur->in_use = 1;
+			break;
+		}
+
+	if (cur >= last) {
+		printk(KERN_ERR "ibmvscsi: found no event struct in pool!\n");
+		return NULL;
+	}
+
+	return cur;
+}
+
+/**
+ * evt_struct_for: - Initializes the next free event
+ * @pool:	event_pool that contains events to be searched
+ * @evt:	VIOSRP_IU that the event will point to
+ * @cmnd:	The scsi cmnd object for this event.  Can be NULL
+ * @done:	Callback function when event is processed
+ *
+ * Returns the initialized event, and NULL if there are no free events
+ */
+static
+struct srp_event_struct *evt_struct_for(struct event_pool *pool,
+					union VIOSRP_IU *evt,
+					struct scsi_cmnd *cmnd,
+					void (*done) (struct srp_event_struct
+						      *))
+{
+	struct srp_event_struct *evt_struct = ibmvscsi_get_event_struct(pool);
+	if (!evt_struct)
+		return NULL;
+
+	*evt_struct->evt = *evt;
+	evt_struct->evt->srp.generic.tag = (u64) (unsigned long)evt_struct;
+
+	evt_struct->cmnd = cmnd;
+	evt_struct->done = done;
+	return evt_struct;
+}
+
+/* ------------------------------------------------------------
+ * Routines for receiving SCSI responses from the hosting partition
+ */
+/**
+ * unmap_direct_data: - Unmap address pointed by SRP_CMD
+ * @cmd:	SRP_CMD whose additional_data member will be unmapped
+ * @dev:	device for which the memory is mapped
+ *
+*/
+static void unmap_direct_data(struct SRP_CMD *cmd, struct device *dev)
+{
+	struct memory_descriptor *data =
+	    (struct memory_descriptor *)cmd->additional_data;
+	dma_unmap_single(dev, data->virtual_address, data->length,
+			 DMA_BIDIRECTIONAL);
+}
+
+/**
+ * unmap_direct_data: - Unmap array of address pointed by SRP_CMD
+ * @cmd:	SRP_CMD whose additional_data member will be unmapped
+ * @dev:	device for which the memory is mapped
+ *
+*/
+static void unmap_indirect_data(struct SRP_CMD *cmd, struct device *dev)
+{
+	struct indirect_descriptor *indirect =
+	    (struct indirect_descriptor *)cmd->additional_data;
+	int i, num_mapped = indirect->head.length / sizeof(indirect->list[0]);
+	for (i = 0; i < num_mapped; ++i) {
+		struct memory_descriptor *data = &indirect->list[i];
+		dma_unmap_single(dev,
+				 data->virtual_address,
+				 data->length, DMA_BIDIRECTIONAL);
+	}
+}
+
+/**
+ * unmap_direct_data: - Unmap data pointed in SRP_CMD based on the format
+ * @cmd:	SRP_CMD whose additional_data member will be unmapped
+ * @dev:	device for which the memory is mapped
+ *
+*/
+static void unmap_cmd_data(struct SRP_CMD *cmd, struct device *dev)
+{
+	if ((cmd->data_out_format == SRP_NO_BUFFER) &&
+	    (cmd->data_in_format == SRP_NO_BUFFER))
+		return;
+	else if ((cmd->data_out_format == SRP_DIRECT_BUFFER) ||
+		 (cmd->data_in_format == SRP_DIRECT_BUFFER))
+		unmap_direct_data(cmd, dev);
+	else
+		unmap_indirect_data(cmd, dev);
+}
+
+/**
+ * map_sg_data: - Maps dma for a scatterlist and initializes decriptor fields
+ * @cmd:	Scsi_Cmnd with the scatterlist
+ * @srp_cmd:	SRP_CMD that contains the memory descriptor
+ * @dev:	device for which to map dma memory
+ *
+ * Called by map_data_for_srp_cmd() when building srp cmd from scsi cmd.
+ * Returns 1 on success.
+*/
+static int map_sg_data(struct scsi_cmnd *cmd,
+		       struct SRP_CMD *srp_cmd, struct device *dev)
+{
+
+	int i, sg_mapped;
+	u64 total_length = 0;
+	struct scatterlist *sg = cmd->request_buffer;
+	struct memory_descriptor *data =
+	    (struct memory_descriptor *)srp_cmd->additional_data;
+	struct indirect_descriptor *indirect =
+	    (struct indirect_descriptor *)data;
+	sg_mapped = dma_map_sg(dev, sg, cmd->use_sg, DMA_BIDIRECTIONAL);
+
+	/* special case; we can use a single direct descriptor */
+	if (sg_mapped == 1) {
+		if (cmd->sc_data_direction == DMA_TO_DEVICE)
+			srp_cmd->data_out_format = SRP_DIRECT_BUFFER;
+		else
+			srp_cmd->data_in_format = SRP_DIRECT_BUFFER;
+		data->virtual_address = sg_dma_address(&sg[0]);
+		data->length = sg_dma_len(&sg[0]);
+		data->memory_handle = 0;
+		return 1;
+	}
+
+	if (sg_mapped > MAX_INDIRECT_BUFS) {
+		printk(KERN_ERR
+		       "ibmvscsi: More than %d mapped sg entries, got %d\n",
+		       MAX_INDIRECT_BUFS, sg_mapped);
+		return 0;
+	}
+
+	if (cmd->sc_data_direction == DMA_TO_DEVICE) {
+		srp_cmd->data_out_format = SRP_INDIRECT_BUFFER;
+		srp_cmd->data_out_count = sg_mapped;
+	} else {
+		srp_cmd->data_in_format = SRP_INDIRECT_BUFFER;
+		srp_cmd->data_in_count = sg_mapped;
+	}
+	indirect->head.virtual_address = 0;
+	indirect->head.length = sg_mapped * sizeof(indirect->list[0]);
+	indirect->head.memory_handle = 0;
+	for (i = 0; i < sg_mapped; ++i) {
+		struct memory_descriptor *descr = &indirect->list[i];
+		struct scatterlist *sg_entry = &sg[i];
+		descr->virtual_address = sg_dma_address(sg_entry);
+		descr->length = sg_dma_len(sg_entry);
+		descr->memory_handle = 0;
+		total_length += sg_dma_len(sg_entry);
+	}
+	indirect->total_length = total_length;
+
+	return 1;
+}
+
+/**
+ * map_sg_data: - Maps memory and initializes memory decriptor fields
+ * @cmd:	struct scsi_cmnd with the memory to be mapped
+ * @srp_cmd:	SRP_CMD that contains the memory descriptor
+ * @dev:	device for which to map dma memory
+ *
+ * Called by map_data_for_srp_cmd() when building srp cmd from scsi cmd.
+ * Returns 1 on success.
+*/
+static int map_single_data(struct scsi_cmnd *cmd,
+			   struct SRP_CMD *srp_cmd, struct device *dev)
+{
+	struct memory_descriptor *data =
+	    (struct memory_descriptor *)srp_cmd->additional_data;
+
+	data->virtual_address =
+	    (u64) (unsigned long)dma_map_single(dev, cmd->request_buffer,
+						cmd->request_bufflen,
+						DMA_BIDIRECTIONAL);
+	/* FIXME: one day this should return something other than 
+	 * 0xFFFFFFFF in case of error. currently arch/ppc64 returns
+	 * ((dma_addr_t)-1) in some cases
+	 */
+	if (data->virtual_address == 0xFFFFFFFF) {
+		printk(KERN_ERR
+		       "ibmvscsi: Unable to map request_buffer for command!\n");
+		return 0;
+	}
+	data->length = cmd->request_bufflen;
+	data->memory_handle = 0 /* viopath_sourceinst(viopath_hostLp) */ ;
+
+	if (cmd->sc_data_direction == DMA_TO_DEVICE)
+		srp_cmd->data_out_format = SRP_DIRECT_BUFFER;
+	else
+		srp_cmd->data_in_format = SRP_DIRECT_BUFFER;
+
+	return 1;
+}
+
+/**
+ * map_data_for_srp_cmd: - Calls functions to map data for srp cmds
+ * @cmd:	struct scsi_cmnd with the memory to be mapped
+ * @srp_cmd:	SRP_CMD that contains the memory descriptor
+ * @dev:	dma device for which to map dma memory
+ *
+ * Called by scsi_cmd_to_srp_cmd() when converting scsi cmds to srp cmds 
+ * Returns 1 on success.
+*/
+static int map_data_for_srp_cmd(struct scsi_cmnd *cmd,
+				struct SRP_CMD *srp_cmd, struct device *dev)
+{
+	switch (cmd->sc_data_direction) {
+	case DMA_FROM_DEVICE:
+	case DMA_TO_DEVICE:
+		break;
+	case DMA_NONE:
+		return 1;
+	case DMA_BIDIRECTIONAL:
+		printk(KERN_ERR
+		       "ibmvscsi: Can't map DMA_BIDIRECTIONAL to read/write\n");
+		return 0;
+	default:
+		printk(KERN_ERR
+		       "ibmvscsi: Unknown data direction 0x%02x; can't map!\n",
+		       cmd->sc_data_direction);
+		return 0;
+	}
+
+	if (!cmd->request_buffer)
+		return 1;
+	if (cmd->use_sg)
+		return map_sg_data(cmd, srp_cmd, dev);
+	return map_single_data(cmd, srp_cmd, dev);
+}
+
+/* ------------------------------------------------------------
+ * Routines for sending and receiving SRPs
+ */
+/**
+ * ibmvscsi_send_srp_event: - Transforms event to u64 array and calls send_crq()
+ * @evt_struct:	evt_struct to be sent
+ * @hostdata:	ibmvscsi_host_data of host
+ *
+ * Returns the value returned from ibmvscsi_send_crq(). (Zero for success)
+ * Note that this routine assumes that host_lock is held for synchronization
+*/
+static int ibmvscsi_send_srp_event(struct srp_event_struct *evt_struct,
+				   struct ibmvscsi_host_data *hostdata)
+{
+	struct scsi_cmnd *cmnd;
+	u64 *crq_as_u64 = (u64 *) & evt_struct->crq;
+
+	/* If we have exhausted our request limit, just queue this request.
+	 * Note that there are rare cases involving driver generated requests 
+	 * (such as task management requests) that the mid layer may think we
+	 * can handle more requests (can_queue) when we actually can't
+	 */
+	if (atomic_dec_if_positive(&hostdata->request_limit) < 0) {
+		printk("ibmvscsi: Warning, request_limit exceeded\n");
+		return SCSI_MLQUEUE_HOST_BUSY;
+	}
+
+	/* Add this to the sent list.  We need to do this 
+	 * before we actually send 
+	 * in case it comes back REALLY fast
+	 */
+	list_add_tail(&evt_struct->list, &hostdata->sent);
+
+	if (ibmvscsi_send_crq(hostdata, crq_as_u64[0], crq_as_u64[1]) != 0) {
+		list_del(&evt_struct->list);
+
+		cmnd = evt_struct->cmnd;
+		printk(KERN_ERR "ibmvscsi: failed to send event struct\n");
+		unmap_cmd_data(&evt_struct->evt->srp.cmd, hostdata->dev);
+		ibmvscsi_free_event_struct(&hostdata->pool, evt_struct);
+		cmnd->result = DID_ERROR << 16;
+		evt_struct->cmnd_done(cmnd);
+	}
+
+	return 0;
+}
+
+/**
+ * handle_cmd_rsp: -  Handle responses from commands
+ * @evt_struct:	srp_event_struct to be handled
+ *
+ * Used as a callback by when sending scsi cmds (by scsi_cmd_to_event_struct). 
+ * Gets called by ibmvscsi_handle_crq()
+*/
+static void handle_cmd_rsp(struct srp_event_struct *evt_struct)
+{
+	struct SRP_RSP *rsp = &evt_struct->evt->srp.rsp;
+	struct scsi_cmnd *cmnd = (struct scsi_cmnd *)evt_struct->cmnd;
+
+	if (cmnd) {
+		cmnd->result |= rsp->status;
+		if (((cmnd->result >> 1) & 0x1f) == CHECK_CONDITION)
+			memcpy(cmnd->sense_buffer,
+			       rsp->sense_and_response_data,
+			       rsp->sense_data_list_length);
+		unmap_cmd_data(&evt_struct->cmd, evt_struct->hostdata->dev);
+
+		if (rsp->dounder)
+			cmnd->resid = rsp->data_out_residual_count;
+		else if (rsp->diunder)
+			cmnd->resid = rsp->data_in_residual_count;
+	}
+
+	if (evt_struct->cmnd_done) {
+		evt_struct->cmnd_done(cmnd);
+	}
+}
+
+/* ------------------------------------------------------------
+ * Routines for queuing individual SCSI commands to the hosting partition
+ */
+
+/**
+ * lun_from_dev: - Returns the lun of the scsi device
+ * @dev:	struct scsi_device
+ *
+*/
+static inline u16 lun_from_dev(struct scsi_device *dev)
+{
+	return (0x2 << 14) | (dev->id << 8) | (dev->channel << 5) | dev->lun;
+}
+
+/**
+ * scsi_cmd_to_srp_cmd: - Initializes srp cmd with data from scsi cmd
+ * @cmd:	source struct scsi_cmnd
+ * @srp_cmd:	target SRP_CMD
+ * @hostdata:	ibmvscsi_host_data of host
+ *
+ * Returns 1 on success.
+*/
+static int scsi_cmd_to_srp_cmd(struct scsi_cmnd *cmd,
+			       struct SRP_CMD *srp_cmd,
+			       struct ibmvscsi_host_data *hostdata)
+{
+	u16 lun = lun_from_dev(cmd->device);
+	memset(srp_cmd, 0x00, sizeof(*srp_cmd));
+
+	srp_cmd->type = SRP_CMD_TYPE;
+	memcpy(srp_cmd->cdb, cmd->cmnd, sizeof(cmd->cmnd));
+	srp_cmd->lun = ((u64) lun) << 48;
+
+	return map_data_for_srp_cmd(cmd, srp_cmd, hostdata->dev);
+}
+
+/**
+ * scsi_cmd_to_event_struct: - Initializes a srp_event_struct 
+ *                             with data from scsi cmd
+ * @cmd:	Source struct scsi_cmnd
+ * @done:	Callback function to be called when cmd is completed
+ * @hostdata:	ibmvscsi_host_data of host
+ *
+ * Returns the srp_event_struct to be used or NULL if not successful.
+*/
+static struct srp_event_struct *scsi_cmd_to_event_struct(struct scsi_cmnd *cmd,
+							 void (*done) (struct
+								       scsi_cmnd
+								       *),
+							 struct
+							 ibmvscsi_host_data
+							 *hostdata)
+{
+	struct SRP_CMD srp_cmd;
+	struct srp_event_struct *evt_struct;
+
+	if (!scsi_cmd_to_srp_cmd(cmd, &srp_cmd, hostdata)) {
+		printk(KERN_ERR "ibmvscsi: couldn't convert cmd to SRP_CMD\n");
+		return NULL;
+	}
+
+	evt_struct = evt_struct_for(&hostdata->pool,
+				    (union VIOSRP_IU *)&srp_cmd,
+				    (void *)cmd, handle_cmd_rsp);
+	if (!evt_struct) {
+		printk(KERN_ERR "ibmvscsi: evt_struct_for() returned NULL\n");
+		return NULL;
+	}
+
+	evt_struct->cmd = srp_cmd;
+	evt_struct->cmnd_done = done;
+	evt_struct->crq.timeout = cmd->timeout;
+	return evt_struct;
+}
+
+/**
+ * ibmvscsi_queue: - The queuecommand function of the scsi template 
+ * @cmd:	struct scsi_cmnd to be executed
+ * @done:	Callback function to be called when cmd is completed
+ *
+ * Always returns zero
+*/
+static
+int ibmvscsi_queue(struct scsi_cmnd *cmd, void (*done) (struct scsi_cmnd *))
+{
+	struct ibmvscsi_host_data *hostdata =
+	    (struct ibmvscsi_host_data *)&cmd->device->host->hostdata;
+	struct srp_event_struct *evt_struct =
+	    scsi_cmd_to_event_struct(cmd, done, hostdata);
+
+	if (!evt_struct) {
+		printk(KERN_ERR
+		       "ibmvscsi: can't convert struct scsi_cmnd to event\n");
+		return SCSI_MLQUEUE_HOST_BUSY;
+	}
+
+	evt_struct->crq.format = VIOSRP_SRP_FORMAT;
+
+	return ibmvscsi_send_srp_event(evt_struct, hostdata);
+}
+
+/* ------------------------------------------------------------
+ * Routines for driver initialization
+ */
+/**
+ * login_rsp: - Handle response to SRP login request
+ * @evt_struct:	srp_event_struct with the response
+ *
+ * Used as a "done" callback by when sending srp_login. Gets called
+ * by ibmvscsi_handle_crq()
+*/
+static void login_rsp(struct srp_event_struct *evt_struct)
+{
+	struct ibmvscsi_host_data *hostdata = evt_struct->hostdata;
+	switch (evt_struct->evt->srp.generic.type) {
+	case SRP_LOGIN_RSP_TYPE:	/* it worked! */
+		break;
+	case SRP_LOGIN_REJ_TYPE:	/* refused! */
+		printk(KERN_INFO "ibmvscsi: SRP_LOGIN_REQ rejected\n");
+		/* Login failed.  */
+		atomic_set(&hostdata->request_limit, -1);
+		return;
+	default:
+		printk(KERN_ERR
+		       "ibmvscsi: Invalid login response typecode 0x%02x!\n",
+		       evt_struct->evt->srp.generic.type);
+		/* Login failed.  */
+		atomic_set(&hostdata->request_limit, -1);
+		return;
+	}
+
+	printk(KERN_INFO "ibmvscsi: SRP_LOGIN succeeded\n");
+
+	/* Now we know what the real request-limit is */
+	atomic_set(&hostdata->request_limit,
+		   evt_struct->evt->srp.login_rsp.request_limit_delta);
+
+	hostdata->host->can_queue =
+	    evt_struct->evt->srp.login_rsp.request_limit_delta;
+
+	return;
+}
+
+/**
+ * send_srp_login: - Sends the srp login
+ * @hostdata:	ibmvscsi_host_data of host
+ * 
+ * Returns zero if successful.
+*/
+static int send_srp_login(struct ibmvscsi_host_data *hostdata)
+{
+	int rc;
+	unsigned long flags;
+
+	struct SRP_LOGIN_REQ req = {
+		.type = SRP_LOGIN_REQ_TYPE,
+		.max_requested_initiator_to_target_iulen = sizeof(union SRP_IU),
+		.required_buffer_formats = 0x0002	/* direct and indirect */
+	};
+	struct srp_event_struct *evt_struct = evt_struct_for(&hostdata->pool,
+							     (union VIOSRP_IU *)
+							     &req,
+							     NULL,
+							     login_rsp);
+
+	if (!evt_struct) {
+		printk(KERN_ERR
+		       "ibmvscsi: couldn't allocate an event for login req!\n");
+		return FAILED;
+	}
+
+	/* Start out with a request limit of 1, since this is negotiated in
+	 * the login request we are just sending
+	 */
+	atomic_set(&hostdata->request_limit, 1);
+	evt_struct->crq.format = VIOSRP_SRP_FORMAT;
+
+	spin_lock_irqsave(hostdata->host->host_lock, flags);
+	rc = ibmvscsi_send_srp_event(evt_struct, hostdata);
+	spin_unlock_irqrestore(hostdata->host->host_lock, flags);
+	return rc;
+};
+
+/**
+ * sync_completion: Signal that a synchronous command has completed
+ */
+static void sync_completion(struct srp_event_struct *evt_struct)
+{
+	complete(&evt_struct->comp);
+}
+
+/**
+ * ibmvscsi_abort: Abort a command...from scsi host template
+ * send this over to the server and wait synchronously for the response
+ */
+static int ibmvscsi_abort(struct scsi_cmnd *cmd)
+{
+	struct ibmvscsi_host_data *hostdata =
+	    *(struct ibmvscsi_host_data **)&cmd->device->host->hostdata;
+	union VIOSRP_IU iu;
+	struct SRP_TSK_MGMT *tsk_mgmt = &iu.srp.tsk_mgmt;
+	struct SRP_RSP *rsp;
+	struct srp_event_struct *evt;
+	struct srp_event_struct *tmp_evt, *found_evt;
+	u16 lun = lun_from_dev(cmd->device);
+
+	/* First, find this command in our sent list so we can figure
+	 * out the correct tag
+	 */
+	found_evt = NULL;
+	list_for_each_entry(tmp_evt, &hostdata->sent, list) {
+		if (tmp_evt->cmnd == cmd) {
+			found_evt = tmp_evt;
+			break;
+		}
+	}
+
+	/* Set up an abort SRP command */
+	memset(&iu, 0x00, sizeof(iu));
+	tsk_mgmt->type = SRP_TSK_MGMT_TYPE;
+	tsk_mgmt->lun = ((u64) lun) << 48;
+	tsk_mgmt->task_mgmt_flags = 0x01;	/* ABORT TASK */
+	tsk_mgmt->managed_task_tag = (u64) (unsigned long)found_evt;
+
+	printk(KERN_INFO "ibmvscsi: aborting command. lun 0x%lx, tag 0x%lx\n",
+	       tsk_mgmt->lun, tsk_mgmt->managed_task_tag);
+
+	evt = evt_struct_for(&hostdata->pool, &iu, NULL, sync_completion);
+	if (!evt) {
+		printk(KERN_ERR "ibmvscsi: failed to allocate abort() event\n");
+		return FAILED;
+	}
+
+	init_completion(&evt->comp);
+	if (ibmvscsi_send_srp_event(evt, hostdata) != 0) {
+		printk(KERN_ERR "ibmvscsi: failed to send abort() event\n");
+		ibmvscsi_free_event_struct(&hostdata->pool, evt);
+		return FAILED;
+	}
+
+	wait_for_completion(&evt->comp);
+
+	if (evt->evt->srp.generic.type != SRP_RSP_TYPE) {
+		printk(KERN_ERR "ibmvscsi: bad TSK_MGMT response type 0x%02x\n",
+		       evt->evt->srp.generic.type);
+	}
+
+	rsp = &evt->evt->srp.rsp;
+	if (!rsp->rspvalid || (rsp->response_data_list_length != 4)) {
+		printk(KERN_ERR "ibmvscsi: bad TSK_MGMT response\n");
+	}
+
+	if (rsp->sense_and_response_data[3] != 0x00) {
+		printk(KERN_ERR "ibmvscsi: ABORT failed: rsp_code 0x%02x\n",
+		       rsp->sense_and_response_data[3]);
+	}
+
+	cmd->result = (DID_ABORT << 16);
+	list_del(&tmp_evt->list);
+	unmap_cmd_data(&tmp_evt->cmd, tmp_evt->hostdata->dev);
+	ibmvscsi_free_event_struct(&tmp_evt->hostdata->pool, tmp_evt);
+	atomic_inc(&hostdata->request_limit);
+	printk(KERN_INFO
+	       "ibmvscsi: successfully aborted task tag 0x%lx\n",
+	       tsk_mgmt->managed_task_tag);
+	return SUCCESS;
+}
+
+/**
+ * purge_requests: Our virtual adapter just shut down.  purge any sent requests 
+ * @hostdata:    the adapter
+ */
+static void purge_requests(struct ibmvscsi_host_data *hostdata)
+{
+	struct srp_event_struct *tmp_evt, *pos;
+	unsigned long flags;
+
+	spin_lock_irqsave(hostdata->host->host_lock, flags);
+	list_for_each_entry_safe(tmp_evt, pos, &hostdata->sent, list) {
+		tmp_evt->cmnd->result = (DID_ERROR << 16);
+		list_del(&tmp_evt->list);
+		unmap_cmd_data(&tmp_evt->cmd, tmp_evt->hostdata->dev);
+		ibmvscsi_free_event_struct(&tmp_evt->hostdata->pool, tmp_evt);
+		if (tmp_evt->cmnd_done) {
+			spin_unlock_irqrestore(hostdata->host->host_lock,
+					       flags);
+			tmp_evt->cmnd_done(tmp_evt->cmnd);
+			spin_lock_irqsave(hostdata->host->host_lock, flags);
+		}
+	}
+	spin_unlock_irqrestore(hostdata->host->host_lock, flags);
+}
+
+/**
+ * ibmvscsi_handle_crq: - Handles and frees received events in the CRQ
+ * @crq:	Command/Response queue
+ * @hostdata:	ibmvscsi_host_data of host
+ *
+*/
+void ibmvscsi_handle_crq(struct VIOSRP_CRQ *crq,
+			 struct ibmvscsi_host_data *hostdata)
+{
+	unsigned long flags;
+	struct srp_event_struct *evt_struct =
+	    (struct srp_event_struct *)crq->IU_data_ptr;
+	switch (crq->valid) {
+	case 0xC0:		/* initialization */
+		switch (crq->format) {
+		case 0x01:	/* Initialization message */
+			printk(KERN_INFO "ibmvscsi: partner initialized\n");
+			/* Send back a response */
+			if (ibmvscsi_send_crq(hostdata, 
+					      0xC002000000000000, 0) == 0) {
+				/* Now login */
+				send_srp_login(hostdata);
+			} else {
+				printk(KERN_ERR 
+				       "ibmvscsi: Unable to send init rsp\n");
+			}
+				       
+			break;
+		case 0x02:	/* Initialization response */
+			printk(KERN_INFO
+			       "ibmvscsi: partner initialization complete\n");
+
+			/* Now login */
+			send_srp_login(hostdata);
+			break;
+		default:
+			printk(KERN_ERR "ibmvscsi: unknown crq message type\n");
+		}
+		return;
+	case 0xFF:		/* Hypervisor telling us the connection is closed */
+		printk(KERN_INFO "ibmvscsi: Virtual adapter failed!\n");
+
+		atomic_set(&hostdata->request_limit, -1);
+		purge_requests(hostdata);
+		return;
+	case 0x80:		/* real payload */
+		break;
+	default:
+		printk(KERN_ERR
+		       "ibmvscsi: got an invalid message type 0x%02x\n",
+		       crq->valid);
+		return;
+	}
+
+	/* The only kind of payload CRQs we should get are responses to 
+	 * things we send. Make sure this response is to something we 
+	 * actually sent
+	 */
+	if (!ibmvscsi_valid_event_struct(&hostdata->pool, evt_struct)) {
+		printk(KERN_ERR
+		       "ibmvscsi: returned correlation_token 0x%p is invalid!\n",
+		       (void *)crq->IU_data_ptr);
+		return;
+	}
+
+	if (crq->format == VIOSRP_SRP_FORMAT)
+		atomic_add(evt_struct->evt->srp.rsp.request_limit_delta,
+			   &hostdata->request_limit);
+
+	if (evt_struct->done)
+		evt_struct->done(evt_struct);
+	else
+		printk(KERN_ERR
+		       "ibmvscsi: returned done() is NULL; not running it!\n");
+
+	/*
+	 * Lock the host_lock before messing with these structures, since we
+	 * are running in a task context
+	 */
+	spin_lock_irqsave(evt_struct->hostdata->host->host_lock, flags);
+	list_del(&evt_struct->list);
+	ibmvscsi_free_event_struct(&evt_struct->hostdata->pool, evt_struct);
+	spin_unlock_irqrestore(evt_struct->hostdata->host->host_lock, flags);
+}
+
+/**
+ * ibmvscsi_get_host_config: Send the command to the server to get host
+ * configuration data.  The data is opaque to us.
+ */
+int ibmvscsi_do_host_config(struct ibmvscsi_host_data *hostdata, 
+			    unsigned char *buffer, int length) {
+	struct VIOSRP_HOST_CONFIG host_config;
+	struct srp_event_struct *evt_struct;
+	int rc;
+	
+	memset(&host_config, 0x00, sizeof(host_config));
+	host_config.common.type = VIOSRP_HOST_CONFIG_TYPE;
+	host_config.common.length = length;
+	host_config.buffer = dma_map_single(hostdata->dev, buffer, length,
+					    DMA_BIDIRECTIONAL);
+	
+	evt_struct = evt_struct_for(&hostdata->pool, 
+				    (union VIOSRP_IU *)&host_config, 
+				    NULL, 
+				    sync_completion);
+	
+	if (!evt_struct) {
+		printk(KERN_ERR 
+		       "ibmvscsi: could't allocate event for HOST_CONFIG!\n");
+		rc = -1;
+	} else {
+		evt_struct->crq.format = VIOSRP_MAD_FORMAT;
+		init_completion(&evt_struct->comp);
+		rc =  ibmvscsi_send_srp_event(evt_struct, hostdata);
+		if (rc == 0) {
+			wait_for_completion(&evt_struct->comp);
+		}
+	}
+	
+	dma_unmap_single(hostdata->dev, host_config.buffer, length, 
+			 DMA_BIDIRECTIONAL);
+
+	return rc ? rc : host_config.common.status;
+}
+
+/* ------------------------------------------------------------
+ * SCSI driver registration
+ */
+static struct scsi_host_template driver_template = {
+	.module = THIS_MODULE,
+	.name = "SCSI host adapter emulator for RPA/iSeries Virtual I/O",
+	.proc_name = "ibmvscsi",
+	.queuecommand = ibmvscsi_queue,
+	.eh_abort_handler = ibmvscsi_abort,
+	.can_queue = 1,		/* Updated after SRP_LOGIN */
+	.this_id = -1,
+	.sg_tablesize = MAX_INDIRECT_BUFS,
+	.cmd_per_lun = 1,
+	.use_clustering = DISABLE_CLUSTERING,
+	.emulated = 1
+};
+
+/**
+ * Called by bus code for each adapter
+ */
+struct ibmvscsi_host_data *ibmvscsi_probe(struct device *dev)
+{
+	struct ibmvscsi_host_data *hostdata;
+	struct Scsi_Host *host;
+	unsigned long wait_switch = 0;
+
+	host = scsi_host_alloc(&driver_template, sizeof(*hostdata));
+	if (!host) {
+		printk(KERN_ERR "ibmvscsi: couldn't allocate host data\n");
+		goto scsi_host_alloc_failed;
+	}
+
+	hostdata = (struct ibmvscsi_host_data *)host->hostdata;
+	memset(hostdata, 0x00, sizeof(*hostdata));
+	INIT_LIST_HEAD(&hostdata->sent);
+	hostdata->host = host;
+	hostdata->dev = dev;
+	atomic_set(&hostdata->request_limit, -1);
+
+	if (ibmvscsi_init_crq_queue(&hostdata->queue, hostdata) != 0) {
+		printk(KERN_ERR "ibmvscsi: couldn't initialize crq\n");
+		goto init_crq_failed;
+	}
+	if (initialize_event_pool(&hostdata->pool,
+				  IBMVSCSI_MAX_REQUESTS, hostdata) != 0) {
+		printk(KERN_ERR "ibmvscsi: couldn't initialize event pool\n");
+		goto init_pool_failed;
+	}
+
+	host->max_lun = 8;
+	host->max_id = max_id;
+	host->max_channel = max_channel;
+
+	if (scsi_add_host(hostdata->host, hostdata->dev))
+		goto add_host_failed;
+		
+	/* Try to send an initialization message.  Note that this is allowed
+	 * to fail if the other end is not acive.  In that case we don't
+	 * want to scan
+	 */
+	if (ibmvscsi_send_crq(hostdata, 0xC001000000000000, 0) == 0) {
+		/*
+		 * Wait around max init_timeout secs for the adapter to finish
+		 * initializing. When we are done initializing, we will have a 
+		 * valid request_limit.  We don't want Linux scanning before 
+		 * we are ready.
+		 */
+		for (wait_switch = jiffies + (init_timeout * HZ);
+		     time_before(jiffies,wait_switch) &&
+			     atomic_read(&hostdata->request_limit) < 0;) {
+			
+			set_current_state(TASK_INTERRUPTIBLE);
+			schedule_timeout(5);
+		}
+
+		/* if we now have a valid request_limit, initiate a scan */
+		if (atomic_read(&hostdata->request_limit) > 0) 
+			scsi_scan_host(host);
+	}
+	
+	return hostdata;
+
+      add_host_failed:
+	release_event_pool(&hostdata->pool, hostdata);
+      init_pool_failed:
+	ibmvscsi_release_crq_queue(&hostdata->queue, hostdata);
+      init_crq_failed:
+	scsi_host_put(host);
+      scsi_host_alloc_failed:
+	return NULL;
+}
+
+void ibmvscsi_remove(struct ibmvscsi_host_data *hostdata)
+{
+	release_event_pool(&hostdata->pool, hostdata);
+	ibmvscsi_release_crq_queue(&hostdata->queue, hostdata);
+
+	scsi_remove_host(hostdata->host);
+	scsi_host_put(hostdata->host);
+	return;
+}
diff -purN linux-2.5/drivers/scsi/ibmvscsi/ibmvscsi.h linuxppc64-2.5/drivers/scsi/ibmvscsi/ibmvscsi.h
--- linux-2.5/drivers/scsi/ibmvscsi/ibmvscsi.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/ibmvscsi/ibmvscsi.h	2004-03-16 20:47:27.000000000 +0000
@@ -0,0 +1,112 @@
+/* ------------------------------------------------------------
+ * ibmvscsi.h
+ * (C) Copyright IBM Corporation 1994, 2003
+ * Authors: Colin DeVilbiss (devilbis@us.ibm.com)
+ *          Santiago Leon (santil@us.ibm.com)
+ *          Dave Boutcher (sleddog@us.ibm.com)
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307
+ * USA
+ *
+ * ------------------------------------------------------------
+ * Emulation of a SCSI host adapter for Virtual I/O devices
+ *
+ * This driver allows the Linux SCSI peripheral drivers to directly
+ * access devices in the hosting partition, either on an iSeries
+ * hypervisor system or a converged hypervisor system.
+ */
+#ifndef IBMVSCSI_H
+#define IBMVSCSI_H
+#include <linux/types.h>
+#include <linux/list.h>
+#include <linux/completion.h>
+#include <linux/workqueue.h>
+#include "viosrp.h"
+
+struct scsi_cmnd;
+struct Scsi_Host;
+/**
+ * Work out the number of scatter/gather buffers we support
+ */
+static const struct SRP_CMD *fake_srp_cmd = NULL;
+enum {
+	IBMVSCSI_MAX_REQUESTS = 50,
+	MAX_INDIRECT_BUFS = (sizeof(fake_srp_cmd->additional_data) -
+			     sizeof(struct indirect_descriptor)) /
+	    sizeof(struct memory_descriptor)
+};
+
+/* ------------------------------------------------------------
+ * Data Structures
+ */
+/* an RPA command/response transport queue */
+struct crq_queue {
+	struct VIOSRP_CRQ *msgs;
+	int size, cur;
+	dma_addr_t msg_token;
+	spinlock_t lock;
+};
+
+/* a unit of work for the hosting partition */
+struct srp_event_struct {
+	union VIOSRP_IU *evt;
+	struct scsi_cmnd *cmnd;
+	struct list_head list;
+	void (*done) (struct srp_event_struct *);
+	struct VIOSRP_CRQ crq;
+	struct ibmvscsi_host_data *hostdata;
+	char in_use;
+	struct SRP_CMD cmd;
+	void (*cmnd_done) (struct scsi_cmnd *);
+	struct completion comp;
+};
+
+/* a pool of event structs for use */
+struct event_pool {
+	struct srp_event_struct *events;
+	u32 size;
+	union VIOSRP_IU *iu_storage;
+	dma_addr_t iu_token;
+};
+
+/* all driver data associated with a host adapter */
+struct ibmvscsi_host_data {
+	atomic_t request_limit;
+	struct device *dev;
+	struct event_pool pool;
+	struct crq_queue queue;
+	struct work_struct srp_task;
+	struct list_head sent;
+	struct Scsi_Host *host;
+};
+
+/* routines for managing a command/response queue */
+int ibmvscsi_init_crq_queue(struct crq_queue *queue,
+			    struct ibmvscsi_host_data *hostdata);
+void ibmvscsi_release_crq_queue(struct crq_queue *queue,
+				struct ibmvscsi_host_data *hostdata);
+void ibmvscsi_handle_crq(struct VIOSRP_CRQ *crq,
+			 struct ibmvscsi_host_data *hostdata);
+int ibmvscsi_send_crq(struct ibmvscsi_host_data *hostdata,
+		      u64 word1, u64 word2);
+
+/* Probe/remove routines */
+struct ibmvscsi_host_data *ibmvscsi_probe(struct device *dev);
+void ibmvscsi_remove(struct ibmvscsi_host_data *hostdata);
+
+int ibmvscsi_do_host_config(struct ibmvscsi_host_data *hostdata, 
+			    unsigned char *buffer, int length);
+
+#endif				/* IBMVSCSI_H */
diff -purN linux-2.5/drivers/scsi/ibmvscsi/ibmvscsis.c linuxppc64-2.5/drivers/scsi/ibmvscsi/ibmvscsis.c
--- linux-2.5/drivers/scsi/ibmvscsi/ibmvscsis.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/ibmvscsi/ibmvscsis.c	2004-03-16 22:46:29.000000000 +0000
@@ -0,0 +1,2387 @@
+/**************************************************************************/
+/* -*- -linux- -*-                                                        */
+/* IBM eServer i/pSeries Virtual SCSI Target Driver                       */
+/* Copyright (C) 2003 Dave Boutcher (boutcher@us.ibm.com) IBM Corp.       */
+/*                                                                        */
+/*  This program is free software; you can redistribute it and/or modify  */
+/*  it under the terms of the GNU General Public License as published by  */
+/*  the Free Software Foundation; either version 2 of the License, or     */
+/*  (at your option) any later version.                                   */
+/*                                                                        */
+/*  This program is distributed in the hope that it will be useful,       */
+/*  but WITHOUT ANY WARRANTY; without even the implied warranty of        */
+/*  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the         */
+/*  GNU General Public License for more details.                          */
+/*                                                                        */
+/*  You should have received a copy of the GNU General Public License     */
+/*  along with this program; if not, write to the Free Software           */
+/*  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  */
+/*                                                                   USA  */
+/*                                                                        */
+/* This module contains the eServer virtual SCSI target code.  The driver */
+/* takes SRP requests from the virtual SCSI client (the linux version is  */
+/* int ibmvscsi.c, but there can be other clients, like AIX or OF) and    */
+/* passes them on to real devices in this system.                         */
+/*                                                                        */ 
+/* The basic hierarchy (and somewhat the organization of this file) is    */ 
+/* that SCSI CDBs are in SRPs are in CRQs.                                */ 
+/*                                                                        */ 
+/**************************************************************************/
+/*
+  TODO:
+  - Support redirecting SRP SCSI requests to a real SCSI driver
+*/
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/interrupt.h>
+#include <linux/list.h>
+#include <linux/proc_fs.h>
+#include <linux/pagemap.h>
+#include <linux/sched.h>
+#include <linux/pci.h>
+#include <linux/blkdev.h>
+#include <linux/fs.h>
+#include <linux/bio.h>
+
+#include <asm/hvcall.h>
+#include <asm/vio.h>
+#include <asm/iommu.h>
+
+#include "../scsi.h"
+#include "viosrp.h"
+
+MODULE_DESCRIPTION("IBM Virtual SCSI Target");
+MODULE_AUTHOR("Dave Boutcher");
+MODULE_LICENSE("GPL");
+
+char ibmvscsis_driver_name[] = "ibmvscsis";
+static int ibmvscsis_debug = 0;
+
+/*
+ * Quick macro to enable/disable interrupts
+ * TODO: move to vio.h to be common with ibmvscsi.c
+ */
+#define h_vio_signal(ua, mode) \
+  plpar_hcall_norets(H_VIO_SIGNAL, ua, mode)
+
+/* 
+ * These are indexes into the following table, and have to match!!!
+ */
+#define SENSE_SUCCESS       0
+#define SENSE_ABORT         1
+#define SENSE_INVALID_ID    2
+#define SENSE_DEVICE_FAULT  3
+#define SENSE_DEVICE_BUSY   4 
+#define SENSE_UNIT_OFFLINE  5
+#define SENSE_INVALID_CMD   6
+#define SENSE_INTERMEDIATE  7
+#define SENSE_WRITE_PROT    8
+
+static unsigned char ibmvscsis_sense_data[][3] =
+{
+/*
+ * Sense key lookup table
+ * Format: SenseKey,AdditionalSenseCode,AdditionalSenseCodeQualifier
+ * Adapted from 3w-xxxx.h
+ */
+	{0x00, 0x00, 0x00}, /* Success           */
+	{0x0b, 0x00, 0x00}, /* Aborted command   */
+	{0x0b, 0x14, 0x00}, /* ID not found      */
+	{0x04, 0x00, 0x00}, /* Device fault      */
+	{0x0b, 0x00, 0x00}, /* Device busy       */
+	{0x02, 0x04, 0x00}, /* Unit offline      */
+	{0x20, 0x04, 0x00}, /* Invalid Command   */
+	{0x10, 0x00, 0x00}, /* Intermediate      */
+	{0x07, 0x27, 0x00}, /* Write Protected   */
+};
+
+/*
+ * SCSI defined structure for inquiry data
+ * TODO: Seral number is currently messed up if you do
+ *       scsiinfo.  I'm not sure why and I think it comes out of 
+ *       here
+ */
+struct inquiry_data {                                                                                                            
+	u8 qual_type;                                                                                                            
+	u8 rmb_reserve;                                                                                                          
+	u8 version;                                                                                                              
+	u8 aerc_naca_hisup_format;                                                                                               
+	u8 addl_len;         
+	u8 sccs_reserved;                                                                                                        
+	u8 bque_encserv_vs_multip_mchngr_reserved;                                                                               
+	u8 reladr_reserved_linked_cmdqueue_vs;                                                                                   
+	char vendor[8];
+	char product[16];
+	char revision[4];
+	char vendor_specific[20];
+	char reserved1[2];
+	char version_descriptor[16];
+	char reserved2[22];
+};
+
+/*
+ * Our proc dir entry under /proc/drivers.  We use proc to configure
+ * this driver right now.
+ * TODO: For 2.5 move to sysfs
+ */
+#ifdef CONFIG_PROC_FS
+#define IBMVSCSIS_PROC_DIR "ibmvscsis"
+static struct proc_dir_entry *ibmvscsis_proc_dir;
+#endif
+
+extern int vio_num_address_cells;
+
+/* 
+ * an RPA command/response transport queue.  This is our structure
+ * that points to the actual queue.  feel free to modify this structure
+ * as needed
+ */
+struct crq_queue {
+	struct VIOSRP_CRQ *msgs;
+	int size, cur;
+	dma_addr_t msg_token;
+	spinlock_t lock;
+};
+
+/*
+ * This structure tracks our fundamental unit of work.  Whenever
+ * an SRP Information Unit (IU) arrives, we track all the good stuff
+ * here
+ */
+struct iu_entry {
+	union VIOSRP_IU *iu;
+	struct server_adapter *adapter;
+	struct list_head next;
+	dma_addr_t iu_token;
+	struct {
+		dma_addr_t remote_token;
+		char *data_buffer;
+		dma_addr_t data_token;
+		long data_len;
+		struct vdev *vd;
+		char in_use:1;
+		char diunder:1;
+		char diover:1;
+		char dounder:1;
+		char doover:1;
+		char write:1;
+		char linked:1;
+		int data_out_residual_count;
+		int data_in_residual_count;
+		int ioerr;
+	} req;
+};
+
+/* 
+ * a pool of ius for use 
+ */
+struct iu_pool {
+	spinlock_t lock;
+	struct list_head iu_entries;
+	struct iu_entry *list;
+	union VIOSRP_IU *iu_storage;
+	dma_addr_t iu_token;
+	u32 size;
+};
+
+/*
+ * Represents a single device that someone told us about
+ * that we treat as a LUN
+ */
+struct vdev {
+	struct list_head list;
+	char type; /* 'B' for block, 'S' for SCSI */
+	atomic_t refcount;
+	int disabled;
+	u64 lun;
+	struct {
+		struct block_device *bdev;
+		dev_t dev;
+		long blksize;
+		long lastlba;
+		int ro;
+	} b;
+};
+
+/*
+ * Represents a bus.  target #'s in SCSI are 6 bits long,
+ * so you can have 64 targets per bus
+ */
+#define TARGETS_PER_BUS (64)
+#define BUS_PER_ADAPTER (8)
+struct vbus {
+	struct vdev *vdev[TARGETS_PER_BUS];
+};
+
+/*
+ * Buffer cache
+ */
+struct dma_buffer {
+	dma_addr_t token;
+	char *addr;
+	size_t len;
+};
+#define DMA_BUFFER_CACHE_SIZE (16)
+#define DMA_BUFFER_INIT_COUNT (4)
+#define DMA_BUFFER_INIT_LEN (PAGE_SIZE*16)
+
+/* all driver data associated with a host adapter */
+struct server_adapter {
+	struct vio_dev *dma_dev;     
+	struct crq_queue queue;
+	struct work_struct crq_work;
+	struct work_struct endio_work;
+	atomic_t crq_task_count;  /* TODO: this is only for debugging. get rid of it */ 
+	atomic_t endio_task_count; /* TODO: this is only for debugging. get rid of it */ 
+	struct iu_pool pool;
+	spinlock_t lock;
+	struct bio *bio_done;
+	struct bio *bio_donetail;
+	struct vbus *vbus[8];
+	int nvdevs;
+	char name[32]; 
+	unsigned long liobn;
+	unsigned long riobn;
+	
+	/* This ugly expression allocates a bit array of 
+	 * in-use flags large enough for the number of buffers
+	 */
+	unsigned long dma_buffer_use[(DMA_BUFFER_CACHE_SIZE + 
+				      sizeof (unsigned long) -1)
+				     /sizeof(unsigned long)];
+	struct dma_buffer dma_buffer[DMA_BUFFER_CACHE_SIZE];
+
+	/* Statistics only */
+	atomic_t iu_count;  /* number allocated */
+	atomic_t bio_count;  /* number allocated */
+	atomic_t crq_processed;	
+	atomic_t interrupts;
+	atomic_t read_processed;
+	atomic_t write_processed;
+	atomic_t buffers_allocated;
+	atomic_t errors;
+};
+
+/* 
+ * Forward declarations
+ */
+static long send_rsp(struct iu_entry *iue, int status);
+
+/*
+ * The following are lifted from usb.h
+ */
+#define DEBUG 1
+#ifdef DEBUG
+#define dbg(format, arg...) if (ibmvscsis_debug) printk(KERN_WARNING __FILE__ ": " format , ## arg)
+#else
+#define dbg(format, arg...) do {} while (0)
+#endif
+#define err(format, arg...) printk(KERN_ERR __FILE__ ": " format , ## arg)
+#define info(format, arg...) printk(KERN_INFO __FILE__ ": " format  , ## arg)
+#define warn(format, arg...) printk(KERN_WARNING __FILE__ ": " format , ## arg)
+
+/* ==============================================================
+ * Utility Routines
+ * ==============================================================
+ */
+/*
+ * return an 8 byte lun given a bus, target, lun.  
+ * Today this only supports single level luns.  Should we add a level or a
+ * 64 bit LUN as input to support multi-level luns?
+ */
+u64 make_lun(unsigned int bus, unsigned int target, unsigned int lun) {
+	u16 result = (0x8000 |
+		      ((target & 0x003f) << 8) |
+		      ((bus & 0x0007) << 5) |
+		      (lun & 0x001f));
+	return ((u64)result) << 48;
+}
+
+/*
+ * Given an 8 byte LUN, return the first level bus/target/lun.
+ * Today this doesn't support multi-level LUNs
+ */
+#define GETBUS(x) ((int)((((u64)(x)) >> 53) & 0x0007))
+#define GETTARGET(x) ((int)((((u64)(x)) >> 56) & 0x003f))
+#define GETLUN(x) ((int)((((u64)(x)) >> 48) & 0x001f))
+
+static u8 getcontrolbyte(u8 *cdb) {
+        return cdb[COMMAND_SIZE(cdb[0])-1];
+}
+
+static u8 getlink(struct iu_entry *iue) {
+	return (getcontrolbyte(iue->iu->srp.cmd.cdb) & 0x01);
+}
+
+/*
+ * Given an SRP, figure out the data in length
+ */
+static int did_len(struct SRP_CMD *cmd) {
+	struct memory_descriptor *md;
+	struct indirect_descriptor *id;
+	int offset = cmd->additional_cdb_len*4;
+
+	switch (cmd->data_out_format) {
+	case SRP_NO_BUFFER:
+		offset += 0;
+		break;
+	case SRP_DIRECT_BUFFER:
+		offset += sizeof(struct memory_descriptor);
+		break;
+	case SRP_INDIRECT_BUFFER:
+		offset += sizeof(struct indirect_descriptor) 
+			+ ((cmd->data_out_count - 1) *sizeof(struct memory_descriptor));
+		break;
+	default:
+		err("ibmvscsis: did_len Invalid data_out_format %d\n",cmd->data_out_format);
+		return 0;
+	}
+
+	switch (cmd->data_in_format) {
+	case SRP_NO_BUFFER:
+		return 0;
+	case SRP_DIRECT_BUFFER:
+		md = (struct memory_descriptor *)(cmd->additional_data + offset);
+		return md->length;
+	case SRP_INDIRECT_BUFFER:
+		id = (struct indirect_descriptor *)(cmd->additional_data + offset);
+		return id->total_length;
+	default:
+		err("ibmvscsis: Invalid data_in_format %d\n",cmd->data_in_format);
+		return 0;
+	}
+}
+
+/* 
+ * We keep a pool of IUs, this routine builds the pool.  The pool is 
+ * per-adapter.  The size of the pool is negotiated as part of the SRP
+ * login, where we negotiate the number of requests (IUs) the client
+ * can send us.  This routine is not synchronized.
+ */
+static int initialize_iu_pool(struct server_adapter *adapter, int size)
+{
+	struct iu_pool *pool = &adapter->pool;
+	int i;
+
+	pool->size = size;
+	pool->lock = SPIN_LOCK_UNLOCKED;
+	INIT_LIST_HEAD(&pool->iu_entries);
+
+	pool->list = kmalloc(pool->size * sizeof(*pool->list), GFP_KERNEL);
+	if(!pool->list) {
+		err("Error: no memory for IU list\n");
+		return -ENOMEM;
+	}
+	memset(pool->list, 0x00, pool->size * sizeof(*pool->list));
+
+	pool->iu_storage = vio_alloc_consistent(adapter->dma_dev, pool->size * sizeof(*pool->iu_storage), &pool->iu_token);
+	if(!pool->iu_storage) {
+		err("Error: no memory for IU pool\n");
+		kfree(pool->list);
+		return -ENOMEM;
+	}
+
+	for(i = 0; i < pool->size; ++i) {
+		pool->list[i].iu = pool->iu_storage + i;
+		pool->list[i].iu_token = pool->iu_token + sizeof(*pool->iu_storage) * i;
+		pool->list[i].adapter = adapter;
+		list_add_tail(&pool->list[i].next, &pool->iu_entries);
+	}
+
+	return 0;
+}
+
+/*
+ * Free the pool we allocated in initialize_iu_pool
+ */
+static void release_iu_pool(struct server_adapter *adapter)
+{
+	struct iu_pool *pool = &adapter->pool;
+	int i, in_use = 0;
+	for(i = 0; i < pool->size; ++i)
+		if(pool->list[i].req.in_use)
+			++in_use;
+	if(in_use)
+		err("ibmvscsis: releasing event pool with %d events still in use?\n", in_use);
+	kfree(pool->list);
+	vio_free_consistent(adapter->dma_dev, pool->size * sizeof(*pool->iu_storage), pool->iu_storage, pool->iu_token);
+}
+
+/*
+ * Get an IU from the pool.  Return NULL of the pool is empty.  This
+ * routine is syncronized by a lock.  The routine sets all the important
+ * fields to 0
+ */
+static struct iu_entry *get_iu(struct server_adapter *adapter) {
+	struct iu_entry *e;
+	unsigned long flags;
+
+	spin_lock_irqsave(&adapter->pool.lock,flags);
+	if (!list_empty(&adapter->pool.iu_entries)) {
+		e = list_entry(adapter->pool.iu_entries.next, struct iu_entry, next);
+		list_del(adapter->pool.iu_entries.next);
+
+		if (e->req.in_use) {
+			err("Found in-use iue in pool!");
+		}
+
+		memset(&e->req,0x00,sizeof(e->req));
+
+		e->req.in_use = 1;
+	} else {
+		e = NULL;
+	}
+	
+	spin_unlock_irqrestore(&adapter->pool.lock, flags);
+	atomic_inc(&adapter->iu_count);
+	return e;
+}
+
+/* 
+ * Return an IU to the pool.  This routine is synchronized
+ */
+static void free_iu(struct iu_entry *iue) {
+	unsigned long flags;
+	if (iue->req.vd) {
+		atomic_dec(&iue->req.vd->refcount);
+	}
+
+	spin_lock_irqsave(&iue->adapter->pool.lock,flags);
+	if (iue->req.in_use == 0) {
+		warn("ibmvscsis: Internal error, freeing iue twice!\n");
+	} else {
+		iue->req.in_use = 0;
+		list_add_tail(&iue->next, &iue->adapter->pool.iu_entries);
+	}
+	spin_unlock_irqrestore(&iue->adapter->pool.lock,flags);
+	atomic_dec(&iue->adapter->iu_count);
+}
+
+/*
+ * Get a CRQ from the inter-partition queue.
+ */
+static struct VIOSRP_CRQ *crq_queue_next_crq(struct crq_queue *queue)
+{
+	struct VIOSRP_CRQ *crq;
+	unsigned long flags;
+
+	spin_lock_irqsave(&queue->lock,flags);
+	crq = &queue->msgs[queue->cur];
+	if(crq->valid & 0x80) {
+		if(++queue->cur == queue->size)
+			queue->cur = 0;
+	}
+	else
+		crq = NULL;
+	spin_unlock_irqrestore(&queue->lock,flags);
+
+	return crq;
+}
+
+/* 
+ * Make the RDMA hypervisor call.  There should be a better way to do this
+ * than inline assembler.
+ * TODO: Fix the inline assembler
+ */
+static long h_copy_rdma(long length, 
+			unsigned long sliobn, unsigned long slioba, 
+			unsigned long dliobn, unsigned long dlioba) {
+	long lpar_rc = 0;
+	__asm__ __volatile__(" li 3,0x110 \n\t"
+			     " mr 4, %1 \n\t"
+			     " mr 5, %2 \n\t"
+			     " mr 6, %3 \n\t"
+			     " mr 7, %4 \n\t"
+			     " mr 8, %5 \n\t"
+			     " .long 0x44000022 \n\t"
+			     " mr %0, 3 \n\t"
+			     : "=&r"(lpar_rc)
+			     : "r" (length), "r" (sliobn), "r" (slioba), "r" (dliobn), "r" (dlioba)
+			     : "r0","r3", "r4", "r5", "r6", "r7", "r8", "cr0", "cr1", "ctr", "xer", "memory");
+	return lpar_rc;
+}
+
+/*
+ * Send an SRP to another partition using the CRQ.
+ */
+static int send_srp(struct iu_entry *iue, u64 length)
+{
+	long rc, rc1;
+	union {
+		struct VIOSRP_CRQ cooked;
+		u64 raw[2];
+	} crq;
+
+
+	/* First copy the SRP */
+	rc = h_copy_rdma(length,
+			 iue->adapter->liobn,
+			 iue->iu_token,
+			 iue->adapter->riobn,
+			 iue->req.remote_token);
+
+	if (rc) {
+		err("Error: In send_srp, h_copy_rdma rc %ld\n",rc);
+	}
+
+	crq.cooked.valid      = 0x80;
+	crq.cooked.format     = VIOSRP_SRP_FORMAT;
+	crq.cooked.reserved   = 0x00;
+	crq.cooked.timeout    = 0x00;
+	crq.cooked.IU_length  =length;
+	crq.cooked.IU_data_ptr=iue->iu->srp.generic.tag;
+
+	if (rc == 0) {
+		crq.cooked.status  =0x99; /* TODO: is this right? */
+	} else {
+		crq.cooked.status  =0x00;
+	}
+
+	rc1 = plpar_hcall_norets(H_SEND_CRQ, iue->adapter->dma_dev->unit_address, 
+				 crq.raw[0], crq.raw[1]);
+
+	if (rc1) {
+		err("ibmvscscsis Error: In send_srp, h_send_crq rc %ld\n",rc1);
+		return rc1;
+	}
+
+	return rc;
+}
+
+/*
+ * Send data to a single SRP memory descriptor
+ * Returns amount of data sent, or negative value on error
+ */
+static long send_md_data(dma_addr_t stoken, int len, struct memory_descriptor *md, struct server_adapter *adapter) {
+	int tosend;
+	long rc;
+
+	if (len < md->length)
+		tosend = len;
+	else 
+		tosend = md->length;
+
+	rc =  h_copy_rdma(tosend,
+			  adapter->liobn,
+			  stoken,
+			  adapter->riobn,
+			  md->virtual_address);
+
+	if (rc != H_Success) {
+		err("Error sending data with h_copy_rdma, rc %ld\n", rc);
+		return -1;
+	}
+	
+	return tosend;
+}
+
+/*
+ * Send data to the SRP data_in buffers
+ * Returns amount of data sent, or negative value on error
+ */
+static long send_cmd_data(dma_addr_t stoken, int len, struct iu_entry *iue) {
+	struct SRP_CMD *cmd = &iue->iu->srp.cmd;
+	struct memory_descriptor *md;
+	struct indirect_descriptor *id;
+	int offset = 0;
+	int total_length = 0;
+	int i;
+	int thislen;
+	int bytes;
+	int sentlen = 0;
+
+	offset = cmd->additional_cdb_len*4;
+	
+	switch (cmd->data_out_format) {
+	case SRP_NO_BUFFER:
+		offset += 0;
+		break;
+	case SRP_DIRECT_BUFFER:
+		offset += sizeof(struct memory_descriptor);
+		break;
+	case SRP_INDIRECT_BUFFER:
+		offset += sizeof(struct indirect_descriptor) 
+			+ ((cmd->data_out_count - 1) *sizeof(struct memory_descriptor));
+		break;
+	default:
+		err("Error: did_len Invalid data_out_format %d\n",cmd->data_out_format);
+		return 0;
+	}
+
+	switch (cmd->data_in_format) {
+	case SRP_NO_BUFFER:
+		return 0;
+	case SRP_DIRECT_BUFFER:
+		md = (struct memory_descriptor *)(cmd->additional_data + offset);
+		return send_md_data(stoken, len, md, iue->adapter);
+	}
+
+	if (cmd->data_in_format != SRP_INDIRECT_BUFFER) {
+		err("Error: send_cmd_data Invalid data_in_format %d\n",cmd->data_in_format);
+		return 0;
+	}
+
+	id = (struct indirect_descriptor *)(cmd->additional_data + offset);
+
+	total_length = id->total_length;
+
+	/* Work through the partial memory descriptor list */
+	for (i=0; ((i<cmd->data_in_count) && (len)); i++) {
+		if (len > id->list[i].length) {
+			thislen = id->list[i].length;
+		} else {
+			thislen = len;
+		}
+
+		bytes = send_md_data(stoken+sentlen, thislen, id->list+i, iue->adapter);
+		if (bytes < 0)
+			return bytes;
+
+		if (bytes != thislen) {
+			warn("Error: Tried to send %d, sent %d\n",thislen, bytes);
+		}
+		
+		sentlen += bytes;
+		total_length -= bytes;
+		len -= bytes;
+	}
+
+	if (len) {
+		warn("Left over data sending to indirect buffer\n");
+		iue->req.diover = 1;
+		iue->req.data_in_residual_count = len;
+	}
+
+	return sentlen;
+}
+
+/*
+ * Get data from the other partition from a single SRP memory descriptor
+ * Returns amount of data sent, or negative value on error
+ */
+static long get_md_data(dma_addr_t ttoken, int len, struct memory_descriptor *md, struct server_adapter *adapter) {
+	int toget;
+	long rc;
+
+	if (len < md->length)
+		toget = len;
+	else 
+		toget = md->length;
+
+	rc =  h_copy_rdma(toget,
+			  adapter->riobn,
+			  md->virtual_address,
+			  adapter->liobn,
+			  ttoken);
+					  
+	if (rc != H_Success) {
+		err("Error sending data with h_copy_rdma, rc %ld\n", rc);
+		return -1;
+	}
+	
+	return toget;
+}
+
+/*
+ * Get data from an SRP data in area.
+ * Returns amount of data sent, or negative value on error
+ */
+static long get_cmd_data(dma_addr_t stoken, int len, struct iu_entry *iue) {
+	struct SRP_CMD *cmd = &iue->iu->srp.cmd;
+	struct memory_descriptor *md;
+	struct indirect_descriptor *id;
+	int offset = 0;
+	int total_length = 0;
+	int i;
+	int thislen;
+	int bytes;
+	int sentlen = 0;
+
+	offset = cmd->additional_cdb_len*4;
+	
+	switch (cmd->data_out_format) {
+	case SRP_NO_BUFFER:
+		return 0;
+		break;
+	case SRP_DIRECT_BUFFER:
+		md = (struct memory_descriptor *)(cmd->additional_data + offset);
+		return get_md_data(stoken, len, md, iue->adapter);
+		break;
+	}
+		
+	if (cmd->data_out_format != SRP_INDIRECT_BUFFER) {
+		err("get_cmd_data Invalid data_out_format %d\n",cmd->data_out_format);
+		return 0;
+	}
+	
+	id = (struct indirect_descriptor *)(cmd->additional_data + offset);
+	
+	total_length = id->total_length;
+
+	/* Work through the partial memory descriptor list */
+	for (i=0; ((i<cmd->data_out_count) && (len)); i++) {
+		if (len > id->list[i].length) {
+			thislen = id->list[i].length;
+		} else {
+			thislen = len;
+		}
+
+		bytes = get_md_data(stoken+sentlen, thislen, id->list+i, iue->adapter);
+		if (bytes < 0)
+			return bytes;
+
+		if (bytes != thislen) {
+			err("Tried to send %d, sent %d\n",thislen, bytes);
+		}
+		
+		sentlen += bytes;
+		total_length -= bytes;
+		len -= bytes;
+	}
+
+	if (len) {
+		warn("Left over data get indirect buffer\n");
+	}
+
+	return sentlen;
+}
+
+/*
+ * Get some data buffers to start.  This doesn't lock the adapter structure!
+ */
+static void init_data_buffer(struct server_adapter *adapter) {
+	int i;
+	
+	for (i=0; i<DMA_BUFFER_INIT_COUNT; i++) {
+		if (adapter->dma_buffer[i].addr == NULL) {
+			adapter->dma_buffer[i].addr = (char *)
+				vio_alloc_consistent(adapter->dma_dev, 
+						     DMA_BUFFER_INIT_LEN,
+						     &adapter->dma_buffer[i].token);
+			adapter->dma_buffer[i].len = DMA_BUFFER_INIT_LEN;
+			dbg("data buf %p token %8.8x, len %ld\n",
+			     adapter->dma_buffer[i].addr,
+			     adapter->dma_buffer[i].token,
+			     adapter->dma_buffer[i].len);
+			atomic_inc(&adapter->buffers_allocated);
+		}
+	}
+
+	return;
+}
+/*
+ * Get a memory buffer that includes a mapped TCE.  
+ */
+static void get_data_buffer(char **buffer, dma_addr_t *data_token, size_t len, struct server_adapter *adapter) {
+	int i;
+	
+	for (i=0; i<DMA_BUFFER_CACHE_SIZE; i++) {
+		if ((adapter->dma_buffer[i].addr) &&
+		    (adapter->dma_buffer[i].len >= len) &&
+		    (!test_and_set_bit(i,adapter->dma_buffer_use)))
+		{
+			*buffer = adapter->dma_buffer[i].addr;
+			*data_token = adapter->dma_buffer[i].token;
+			return;
+		}
+	}
+
+	/* Couldn't get a buffer!  Try and get a new one */
+	*buffer = (char *)vio_alloc_consistent(adapter->dma_dev, len,
+					       data_token);
+	atomic_inc(&adapter->buffers_allocated);
+	return;
+}
+
+/*
+ * Free a memory buffer that includes a mapped TCE.  
+ */
+static void free_data_buffer(char *buffer, dma_addr_t data_token, size_t len, struct server_adapter *adapter) {
+	int i;
+	
+	/* First see if this buffer is already in the cache */
+	for (i=0; i<DMA_BUFFER_CACHE_SIZE; i++) {
+		if (adapter->dma_buffer[i].addr == buffer) {
+			if (adapter->dma_buffer[i].token != data_token) {
+				err("Inconsistent data buffer pool info!\n");
+			}
+			if (!test_and_clear_bit(i,adapter->dma_buffer_use)) {
+				err("Freeing data buffer twice!\n");
+			}
+			return;
+		}
+	}
+	
+	/* See if there is an empty slot in our list */
+	for (i=0; i<DMA_BUFFER_CACHE_SIZE; i++) {
+		if (!test_and_set_bit(i,adapter->dma_buffer_use)) {
+			if (adapter->dma_buffer[i].addr == NULL) {
+				adapter->dma_buffer[i].addr = buffer;
+				adapter->dma_buffer[i].token = data_token;
+				clear_bit(i,adapter->dma_buffer_use);
+				return;
+			} else {
+				clear_bit(i,adapter->dma_buffer_use);
+			}
+		}
+	}
+
+	/* Now see if there is a smaller buffer we should throw out */
+	for (i=0; i<DMA_BUFFER_CACHE_SIZE; i++) {
+		if (!test_and_set_bit(i,adapter->dma_buffer_use)) {
+			if (adapter->dma_buffer[i].len < len) {
+				vio_free_consistent(adapter->dma_dev, 
+						    adapter->dma_buffer[i].len,
+						    adapter->dma_buffer[i].addr,
+						    adapter->dma_buffer[i].token);
+			
+				atomic_dec(&adapter->buffers_allocated);
+
+				adapter->dma_buffer[i].addr = buffer;
+				adapter->dma_buffer[i].token = data_token;
+				adapter->dma_buffer[i].len = len;
+				clear_bit(i,adapter->dma_buffer_use);
+				return;
+			} else {
+				clear_bit(i,adapter->dma_buffer_use);
+			}
+		}
+	}
+
+	/* No space to cache this.  Give it back to the kernel */
+	vio_free_consistent(adapter->dma_dev, len,
+			    buffer, data_token);
+	atomic_dec(&adapter->buffers_allocated);
+}
+
+/*
+ * Release all the data buffers
+ */
+static void release_data_buffer(struct server_adapter *adapter) {
+	int i;
+	int free_in_use = 0;
+	
+	for (i=0; i<DMA_BUFFER_INIT_COUNT; i++) {
+		if (adapter->dma_buffer[i].addr != NULL) {
+			if (test_bit(i,adapter->dma_buffer_use)) {
+				free_in_use++;
+			}
+			vio_free_consistent(adapter->dma_dev, 
+					    adapter->dma_buffer[i].len,
+					    adapter->dma_buffer[i].addr,
+					    adapter->dma_buffer[i].token);
+			
+			atomic_dec(&adapter->buffers_allocated);
+		}
+	}
+	
+	if (free_in_use) {
+		err("Freeing %d in-use data buffers\n", free_in_use);
+	}
+	return;
+}
+
+/*
+ * the routine that gets called on end_io of our bios.  We basically
+ * schedule the processing to be done in our task, since we don't want
+ * do things like RDMA in someone else's interrupt handler
+ *
+ * Each iu request may result in multiple bio requests.  only proceed
+ * when all the bio requests have done.
+ */
+static int ibmvscsis_end_io(struct bio *bio, unsigned int nbytes, int error)
+{
+	struct iu_entry *iue = (struct iu_entry *)bio->bi_private;
+	struct server_adapter *adapter = iue->adapter;
+	unsigned long flags;
+
+	if (!test_bit(BIO_UPTODATE, &bio->bi_flags)) {
+		iue->req.ioerr = 1;
+	};
+
+	/* Add the bio to the done queue */
+	spin_lock_irqsave(&adapter->lock, flags);
+	if (adapter->bio_donetail) {
+		adapter->bio_donetail->bi_next = bio;
+		adapter->bio_donetail = bio;
+	} else
+		adapter->bio_done = adapter->bio_donetail = bio;
+	spin_unlock_irqrestore(&adapter->lock, flags);
+
+	/* Schedule the task */
+	schedule_work(&adapter->endio_work);
+
+	return 0;
+}
+
+/*
+ * Find the vdev structure from the LUN field in an SRP IUE
+ * Note that this routine bumps a refcount field in the vdev.
+ * Normally this is done when free_iu is called.
+ */
+static struct vdev *find_device(struct iu_entry *iue) {
+	u16 *lun = (u16 *)&iue->iu->srp.cmd.lun;
+	u32 bus = (lun[0] & 0x00E0) >> 5;
+	u32 target = (lun[0] & 0x3F00) >> 8;
+	u32 slun = (lun[0] & 0x001F);
+	struct vdev *vd;
+	unsigned long flags;
+
+	/* If asking for a lun other than 0, return nope */
+	if (slun) {
+		return NULL;
+	}
+
+	/* Only from SRP CMD */
+	if (iue->iu->srp.generic.type != SRP_CMD_TYPE)
+		return NULL;
+
+	/* if not a recognized LUN format, return NULL */
+	if ((lun[0] & 0xC000) != 0x8000) 
+		return NULL;
+
+	spin_lock_irqsave(&iue->adapter->lock,flags);
+	if (iue->adapter->vbus[bus] == NULL) {
+		spin_unlock_irqrestore(&iue->adapter->lock,flags);
+		return NULL;
+	}
+
+	vd =  iue->adapter->vbus[bus]->vdev[target];
+
+	if ((vd == NULL) || (vd->disabled)) {
+		spin_unlock_irqrestore(&iue->adapter->lock,flags);
+		return NULL;
+	}
+
+	if (vd) {
+		atomic_inc(&vd->refcount);
+	}
+	spin_unlock_irqrestore(&iue->adapter->lock,flags);
+
+	return vd;
+}
+
+/*
+ * Process BH buffer completions.  When the end_io routine gets called 
+ * we queue the bio on an internal queue and start a task to process them
+ */
+static void endio_task(void *data)
+{
+	struct server_adapter *adapter = (struct server_adapter *)data;
+	struct bio *bio;
+	int bytes;
+	unsigned long flags;
+
+	if (atomic_inc_return(&adapter->endio_task_count) > 1) {
+		err("In endio_task twice!!!\n");
+	}
+
+	do {
+		spin_lock_irqsave(&adapter->lock,flags);
+		if ((bio = adapter->bio_done)) {
+			if (bio == adapter->bio_donetail)
+				adapter->bio_donetail = NULL;
+			adapter->bio_done = bio->bi_next;
+			bio->bi_next = NULL;
+		}
+		spin_unlock_irqrestore(&adapter->lock,flags);
+		
+		if (bio) {
+			struct iu_entry *iue = (struct iu_entry *)bio->bi_private;
+
+			if (!iue->req.ioerr ) {
+				/* return data if this was a read */
+				if (!iue->req.write) {
+					bytes = send_cmd_data(iue->req.data_token, iue->req.data_len, iue);
+					if (bytes != iue->req.data_len) {
+						err("Error sending data on response (tried %d, sent %d\n",
+						    bio->bi_size, bytes);
+						send_rsp(iue, SENSE_ABORT);
+					} else {
+						send_rsp(iue, SENSE_SUCCESS);
+					}
+				} else {
+					send_rsp(iue, SENSE_SUCCESS);
+				}
+			} else {
+				err("Block operation failed\n");
+				send_rsp(iue, SENSE_DEVICE_FAULT);
+			}
+
+			free_data_buffer(iue->req.data_buffer, iue->req.data_token, iue->req.data_len, adapter); 
+				
+			free_iu(iue);
+			
+			bio_put(bio);
+			atomic_dec(&adapter->bio_count);
+		}
+	} while (bio);
+	atomic_dec(&adapter->endio_task_count);
+}
+
+/* ==============================================================
+ * SCSI Command Emulation Routines
+ * ==============================================================
+ */
+
+/*
+ * Process an inquiry SCSI Command
+ */
+static void process_inquiry(struct iu_entry *iue) {
+	struct inquiry_data *id;
+	dma_addr_t data_token;
+	int bytes;
+
+	id = (struct inquiry_data *)vio_alloc_consistent(iue->adapter->dma_dev, 
+							sizeof(*id),
+							&data_token);
+	memset(id,0x00,sizeof(*id));
+
+	/* If we have a valid device */
+	if (iue->req.vd) {
+		dbg("  inquiry returning device\n");
+		id->qual_type = 0x00;    /* Direct Access    */
+		id->rmb_reserve = 0x00;  /* TODO: CD is removable  */
+		id->version = 0x84;      /* ISO/IE                 */
+		id->aerc_naca_hisup_format = 0x22; /* naca & format 0x02 */
+		id->addl_len = sizeof(*id)-4;   /* sizeof(*this) - 4 */
+		id->bque_encserv_vs_multip_mchngr_reserved = 0x00; 
+		id->reladr_reserved_linked_cmdqueue_vs = 0x02; /* CMDQ   */
+		memcpy(id->vendor,"IBM     ",8);
+		memcpy(id->product,"VSCSI blkdev    ",16);
+		memcpy(id->revision,"0001",4);
+	} else {
+		dbg("  inquiry returning no device\n");
+		id->qual_type = 0x7F; /* Not supported, no device */
+	}
+		
+	bytes = send_cmd_data(data_token, sizeof(*id), iue);
+		
+	vio_free_consistent(iue->adapter->dma_dev, 
+			    sizeof(*id),
+			    id,
+			    data_token);
+
+	if (bytes < 0) {
+		send_rsp(iue, SENSE_DEVICE_FAULT);
+	} else {
+		send_rsp(iue, SENSE_SUCCESS);
+	}
+	
+	free_iu(iue);
+}
+
+/*
+ * Handle an I/O.  Called by WRITE6, WRITE10, etc
+ */
+static void process_rw(char *cmd, int rw, struct iu_entry *iue, long lba, long len) {
+	char *buffer;
+	struct bio * bio;
+	int bytes;
+	int num_biovec;
+	int cur_biovec;
+
+	dbg("%s %16.16lx[%d:%d:%d][%d:%d] lba %ld len %ld reladr %d link %d\n",
+	    cmd,
+	    iue->iu->srp.cmd.lun,
+	    GETBUS(iue->iu->srp.cmd.lun),
+	    GETTARGET(iue->iu->srp.cmd.lun),
+	    GETLUN(iue->iu->srp.cmd.lun),
+	    MAJOR(iue->req.vd->b.dev),
+	    MINOR(iue->req.vd->b.dev),
+	    lba,
+	    len / iue->req.vd->b.blksize,
+	    iue->iu->srp.cmd.cdb[1] & 0x01,
+	    iue->req.linked);
+
+	if (rw == WRITE) {
+		atomic_inc(&iue->adapter->write_processed);
+	} else if (rw == READ) {
+		atomic_inc(&iue->adapter->read_processed);
+	} else {
+		err("Major internal error...rw not read or write\n");
+		send_rsp(iue, SENSE_DEVICE_FAULT); 
+		
+		free_iu(iue);
+		return;
+	}
+	
+	if (len == 0) {
+		warn("Zero length I/O\n");
+		send_rsp(iue, SENSE_INVALID_CMD); 
+		
+		free_iu(iue);
+		return;
+	}
+	
+	/* Writing to a read-only device */
+	if ((rw == WRITE) && (iue->req.vd->b.ro)) {
+		warn("WRITE op to r/o device\n");
+		send_rsp(iue, SENSE_WRITE_PROT);
+		
+		free_iu(iue);
+		return;
+	}
+
+	get_data_buffer(&buffer, &iue->req.data_token, len, iue->adapter);
+	iue->req.data_buffer = buffer;
+	iue->req.data_len = len;
+	if (buffer == NULL) {
+		err("Not able to get a data buffer (%lu pages)\n", len/PAGE_SIZE);
+		send_rsp(iue, SENSE_DEVICE_FAULT); 
+		
+		free_iu(iue);
+		return;
+	}
+
+	/* if reladr */
+	if (iue->iu->srp.cmd.cdb[1] & 0x01) {
+		lba = lba+iue->req.vd->b.lastlba;
+	}
+
+	/* If this command is linked, Keep this lba */
+	if (iue->req.linked) {
+		iue->req.vd->b.lastlba = lba;
+	} else {
+		iue->req.vd->b.lastlba = 0;
+	}
+
+	if (rw == WRITE) {
+		iue->req.write = 1;
+		/* Get the data */
+		bytes = get_cmd_data(iue->req.data_token, len, iue);
+		if (bytes != len) {
+			err("Error transferring data\n");
+			send_rsp(iue, SENSE_DEVICE_FAULT); 
+			
+			free_iu(iue);
+			return;
+		}
+	}
+	
+	num_biovec = (len-1)/PAGE_CACHE_SIZE + 1;
+
+	bio = bio_alloc(GFP_ATOMIC, num_biovec);
+	if (!bio) {
+		/* Ouch.  couldn't get a bio.  Mark this I/O as 
+		 * in error, then decrement the outstanding bio.
+		 * If there are still outstanding bio, they will send
+		 * the error and free the IU.  If there are none, we
+		 * should do it here
+		 */
+		iue->req.ioerr = 1;
+		err("Not able to get a bio\n");
+		send_rsp(iue, SENSE_DEVICE_FAULT); 
+		free_iu(iue);
+		return;
+	}
+
+	atomic_inc(&iue->adapter->bio_count);
+	bio->bi_size = len;
+	bio->bi_bdev = iue->req.vd->b.bdev;
+	bio->bi_sector = lba;
+	bio->bi_end_io = &ibmvscsis_end_io;
+	bio->bi_private = iue;
+	bio->bi_rw = (rw == WRITE) ? 1 : 0;
+	bio->bi_phys_segments = 1;
+	bio->bi_hw_segments = 1;
+			
+	/* This all assumes that the buffers we get are page-aligned */
+	for (cur_biovec=0; cur_biovec<num_biovec; cur_biovec++) {
+		long thislen;
+
+		if (len > PAGE_CACHE_SIZE) {
+			thislen = PAGE_CACHE_SIZE;
+		} else {
+			thislen = len;
+		}
+			
+		bio->bi_io_vec[cur_biovec].bv_page = virt_to_page(buffer);
+		bio->bi_io_vec[cur_biovec].bv_len = thislen;
+		bio->bi_io_vec[cur_biovec].bv_offset = (unsigned long)buffer & PAGE_OFFSET_MASK;
+		bio->bi_vcnt++;
+
+		len -= thislen;
+		buffer += thislen;
+	}
+	generic_make_request(bio);
+}
+
+/*
+ * Process a READ6
+ */
+static void processRead6(struct iu_entry *iue) {
+	long lba = (*((u32 *)(iue->iu->srp.cmd.cdb))) & 0x001FFFFF;
+	long len = iue->iu->srp.cmd.cdb[4];
+	
+	/* Length of 0 indicates 256 */
+	if (len == 0) {
+		len = 256;
+	}
+	
+	len = len * iue->req.vd->b.blksize;
+	
+	process_rw("Read6", READ, iue,lba,len);
+}
+
+/*
+ * Process a READ10
+ */
+static void processRead10(struct iu_entry *iue) {
+	long lba = *((u32 *)(iue->iu->srp.cmd.cdb+2));
+	long len = *((u16 *)(iue->iu->srp.cmd.cdb+7))*iue->req.vd->b.blksize;
+
+	process_rw("Read10", READ, iue,lba,len);
+}
+
+/*
+ * Process a READ10
+ */
+static void processRead12(struct iu_entry *iue) {
+	long lba = *((u32 *)(iue->iu->srp.cmd.cdb+2));
+	long len = *((u32 *)(iue->iu->srp.cmd.cdb+6))*iue->req.vd->b.blksize;
+
+	process_rw("Read12", READ, iue,lba,len);
+}
+
+
+static void processWrite6(struct iu_entry *iue) {
+	long lba = (*((u32 *)(iue->iu->srp.cmd.cdb))) & 0x001FFFFF;
+	long len = iue->iu->srp.cmd.cdb[4];
+	
+	/* Length of 0 indicates 256 */
+	if (len == 0) {
+		len = 256;
+	}
+	
+	len = len * iue->req.vd->b.blksize;
+	
+	process_rw("Write6", WRITE, iue,lba,len);
+}
+
+static void processWrite10(struct iu_entry *iue) {
+	long lba = *((u32 *)(iue->iu->srp.cmd.cdb+2));
+	long len = *((u16 *)(iue->iu->srp.cmd.cdb+7))*iue->req.vd->b.blksize;
+
+	process_rw("Write10", WRITE, iue,lba,len);
+}
+
+static void processWrite12(struct iu_entry *iue) {
+	long lba = *((u32 *)(iue->iu->srp.cmd.cdb+2));
+	long len = *((u32 *)(iue->iu->srp.cmd.cdb+6))*iue->req.vd->b.blksize;
+
+	process_rw("Write12", WRITE, iue,lba,len);
+}
+
+/*
+ * Handle Read Capacity
+ */
+static void processReadCapacity(struct iu_entry *iue) {
+	struct ReadCapacityData {
+		u32 blocks;
+		u32 blocksize;
+	} *cap;
+	dma_addr_t data_token;
+	int bytes;
+
+	cap = (struct ReadCapacityData *)vio_alloc_consistent(iue->adapter->dma_dev, 
+							      sizeof(*cap),
+							      &data_token);
+
+	cap->blocksize = iue->req.vd->b.blksize;
+	cap->blocks = iue->req.vd->b.bdev->bd_inode->i_size / iue->req.vd->b.blksize;
+
+	info("Reporting capacity as %u block of size %u\n",cap->blocks, cap->blocksize);
+	
+	bytes = send_cmd_data(data_token, sizeof(*cap), iue);
+	
+	vio_free_consistent(iue->adapter->dma_dev, 
+			    sizeof(*cap),
+			    cap,
+			    data_token);
+
+	if (bytes != sizeof(*cap)) {
+		err("Error sending read capacity data. bytes %d, wanted %ld\n",bytes,sizeof(*cap));
+	}
+	
+	send_rsp(iue, SENSE_SUCCESS);
+	
+	free_iu(iue);
+}
+
+/*
+ * Process Mode Sense
+ * TODO: I know scsiinfo asks for a bunch of mode pages not implemented here.
+ *       Also, we need to act differently for virtual disk and virtual CD
+ */
+#define MODE_SENSE_BUFFER_SIZE (512)
+static void processModeSense(struct iu_entry *iue) {
+	dma_addr_t data_token;
+	int bytes;
+	
+	u8 *mode = (u8 *)vio_alloc_consistent(iue->adapter->dma_dev, 
+					      MODE_SENSE_BUFFER_SIZE,
+					      &data_token);
+	/* which page */
+	switch (iue->iu->srp.cmd.cdb[2]) {
+	case 0:
+	case 0x3f:
+		mode[1] = 0x00; /* Default medium */
+		if (iue->req.vd->b.ro) {
+			mode[2] = 0x80; /* device specific  */
+		} else {
+			mode[2] = 0x00; /* device specific  */
+		}
+		/* note the DPOFUA bit is set to zero! */
+		mode[3] = 0x08; /* block descriptor length */
+		*((u32 *)&mode[4]) = iue->req.vd->b.bdev->bd_inode->i_size / iue->req.vd->b.blksize;
+		*((u32 *)&mode[8]) = iue->req.vd->b.blksize;
+		bytes = mode[0] = 12; /* length */
+		break;
+		
+	case 0x08: /* Cache page */
+		/* length should be 4 */
+		if (iue->iu->srp.cmd.cdb[4] != 4 && iue->iu->srp.cmd.cdb[4] != 0x20) {
+			send_rsp(iue, SENSE_INVALID_CMD); 
+			vio_free_consistent(iue->adapter->dma_dev, 
+					    MODE_SENSE_BUFFER_SIZE,
+					    mode,
+					    data_token);
+			free_iu(iue);
+			return;
+		}
+
+		mode[1] = 0x00; /* Default medium */
+		if (iue->req.vd->b.ro) {
+			mode[2] = 0x80; /* device specific */
+		} else {
+			mode[2] = 0x00; /* device specific */
+		}
+		/* note the DPOFUA bit is set to zero! */
+		mode[3] = 0x08; /* block descriptor length */
+		*((u32 *)&mode[4]) = iue->req.vd->b.bdev->bd_inode->i_size / iue->req.vd->b.blksize;
+		*((u32 *)&mode[8]) = iue->req.vd->b.blksize; 
+		
+		/* Cache page */
+		mode[12] = 0x08; /* page */
+		mode[13] = 0x12; /* page length */
+		mode[14] = 0x01; /* no cache (0x04 for read/write cache) */
+		
+		bytes = mode[0] = 12 + mode[13]; /* length */
+		break;
+	default:
+		warn("Request for unknown mode page %d\n",iue->iu->srp.cmd.cdb[2]);
+		send_rsp(iue, SENSE_INVALID_CMD); 
+		vio_free_consistent(iue->adapter->dma_dev, 
+				    MODE_SENSE_BUFFER_SIZE,
+				    mode,
+				    data_token);
+		free_iu(iue);
+		return;
+	}
+		
+	bytes = send_cmd_data(data_token, bytes, iue);
+	
+	vio_free_consistent(iue->adapter->dma_dev, 
+			    MODE_SENSE_BUFFER_SIZE,
+			    mode,
+			    data_token);
+
+	send_rsp(iue, SENSE_SUCCESS);
+	
+	free_iu(iue);
+	return;
+}
+
+/*
+ * Report LUNS command.
+ */
+static void processReportLUNs(struct iu_entry *iue) {
+	int listsize = did_len(&iue->iu->srp.cmd);
+	dma_addr_t data_token;
+	int index = 2;  /* Start after the two entries (length and LUN0) */
+	int bus;
+	int target;
+	int bytes;
+	unsigned long flags;
+
+	u64 *lunlist = (u64 *)vio_alloc_consistent(iue->adapter->dma_dev, 
+						   listsize,
+						   &data_token);
+
+	memset(lunlist,0x00,listsize);
+	
+	/* work out list size in units of u64 */
+	listsize = listsize / 8;
+
+	if (listsize < 1) {
+		warn("report luns buffer too small\n");
+		send_rsp(iue, SENSE_INVALID_CMD); 
+		free_iu(iue);
+	}
+	
+	spin_lock_irqsave(&iue->adapter->lock,flags);
+
+	/* send lunlist of size 1 when requesting lun is not all zeros */
+	if (iue->iu->srp.cmd.lun != 0x0LL) {
+		*lunlist = ((u64)1*8) << 32;
+		goto send_lunlist;
+	}
+
+	/* return the total number of luns plus LUN0 in bytes */
+	*lunlist = (((u64)((iue->adapter->nvdevs+1)*8)) << 32);
+
+	dbg("reporting %d luns\n",iue->adapter->nvdevs+1);
+	/* loop through the bus */
+	for (bus = 0; bus < BUS_PER_ADAPTER; bus++) {
+		/* If this bus exists */
+		if (iue->adapter->vbus[bus]) {
+			/* loop through the targets */
+			for (target = 0; target < TARGETS_PER_BUS; target++)
+			{
+				/* If the target exists */
+				if (iue->adapter->vbus[bus]->vdev[target]) {
+					if ((index < listsize) &&
+					    (!iue->adapter->vbus[bus]->vdev[target]->disabled)) {
+						lunlist[index++] = iue->adapter->vbus[bus]->vdev[target]->lun;
+						dbg("  lun %16.16lx\n",iue->adapter->vbus[bus]->vdev[target]->lun);
+					}
+				}
+			}
+		}
+	}
+	
+send_lunlist:
+	spin_unlock_irqrestore(&iue->adapter->lock,flags);
+
+	bytes = send_cmd_data(data_token, (index * 8), iue);
+	
+	vio_free_consistent(iue->adapter->dma_dev, 
+			    listsize * 8,
+			    lunlist,
+			    data_token);
+
+	if (bytes != (index * 8)) {
+		err("Error sending report luns data. bytes %d, wanted %d\n",bytes,index * 4);
+		send_rsp(iue, SENSE_ABORT);
+	} else {
+		send_rsp(iue, SENSE_SUCCESS);
+	}
+	
+	free_iu(iue);
+	return;}
+
+/*
+ * Process an IU.  
+ *
+ * Note that THIS routine is responsible for returning the IU from the pool
+ * The current assumption is that all the process routines called from here
+ * are, in turn, responsible for freeing the IU
+ */
+static void process_cmd(struct iu_entry *iue)
+{
+	union VIOSRP_IU *iu = iue->iu;
+
+	iue->req.vd = find_device(iue);	
+
+	if ((iue->req.vd == NULL) && 
+	    (iu->srp.cmd.cdb[0] != REPORT_LUNS) &&
+	    (iu->srp.cmd.cdb[0] != INQUIRY)) {
+		dbg("Cmd %2.2x for unknown LUN %16.16lx\n",
+		    iu->srp.cmd.cdb[0],
+		    iue->iu->srp.cmd.lun);
+		send_rsp(iue, SENSE_INVALID_ID);
+		free_iu(iue);
+		return;
+	} 
+
+	iue->req.linked = getlink(iue);
+
+	switch(iu->srp.cmd.cdb[0]) {
+	case READ_6:
+		processRead6(iue);
+		break;
+	case READ_10:
+		processRead10(iue);
+		break;
+	case READ_12:
+		processRead12(iue);
+		break;
+	case WRITE_6:
+		processWrite6(iue);
+		break;
+	case WRITE_10:
+		processWrite10(iue);
+		break;
+	case WRITE_12:
+		processWrite12(iue);
+		break;
+	case REPORT_LUNS:
+		dbg("REPORT LUNS lun %16.16lx\n",iue->iu->srp.cmd.lun);
+		processReportLUNs(iue);
+		break;
+	case INQUIRY:
+		dbg("INQUIRY lun %16.16lx\n",iue->iu->srp.cmd.lun);
+		process_inquiry(iue);
+		break;
+	case READ_CAPACITY:
+		dbg("READ CAPACITY lun %16.16lx\n",iue->iu->srp.cmd.lun);
+		processReadCapacity(iue);
+		break;
+	case MODE_SENSE:
+		dbg("MODE SENSE lun %16.16lx\n",iue->iu->srp.cmd.lun);
+		processModeSense(iue);
+		break;
+	case TEST_UNIT_READY:
+		/* we already know the device exists */
+		dbg("TEST UNIT READY lun %16.16lx\n",iue->iu->srp.cmd.lun);
+		send_rsp(iue, SENSE_SUCCESS);
+		free_iu(iue);
+		break;
+	case START_STOP:
+		/* just respond OK */
+		dbg("START_STOP lun %16.16lx\n",iue->iu->srp.cmd.lun);
+		send_rsp(iue, SENSE_SUCCESS);
+		free_iu(iue);
+		break;
+	default:
+		warn("Unsupported SCSI Command 0x%2.2x\n",iu->srp.cmd.cdb[0]);
+		send_rsp(iue, SENSE_INVALID_CMD);
+		free_iu(iue);
+	}
+}
+
+/* ==============================================================
+ * SRP Processing Routines
+ * ==============================================================
+ */
+/*
+ * Process an incoming SRP Login request
+ */
+static void process_login(struct iu_entry *iue)
+{
+	union VIOSRP_IU *iu = iue->iu;
+	u64 tag = iu->srp.generic.tag;
+
+	/* TODO handle case that requested size is wrong and buffer format is wrong */
+	memset(iu,0x00,sizeof(struct SRP_LOGIN_RSP));
+	iu->srp.login_rsp.type                          = SRP_LOGIN_RSP_TYPE;
+	iu->srp.login_rsp.request_limit_delta           = iue->adapter->pool.size;
+	iu->srp.login_rsp.tag                           = tag;
+	iu->srp.login_rsp.max_initiator_to_target_iulen = sizeof(union SRP_IU);
+	iu->srp.login_rsp.max_target_to_initiator_iulen = sizeof(union SRP_IU);
+	iu->srp.login_rsp.supported_buffer_formats      = 0x0002; /* direct and indirect */
+	iu->srp.login_rsp.multi_channel_result          = 0x00; /* TODO fix if we were already logged in */
+	
+	send_srp(iue,sizeof(iu->srp.login_rsp));
+}
+
+/*
+ * Send an SRP response that includes sense data
+ */
+static long send_rsp(struct iu_entry *iue, int status) {
+	u8 *sense = iue->iu->srp.rsp.sense_and_response_data;
+	u64 tag = iue->iu->srp.generic.tag;
+	union VIOSRP_IU *iu = iue->iu;
+
+	if (status != SENSE_SUCCESS) {
+		atomic_inc(&iue->adapter->errors);
+	}
+
+	/* If the linked bit is on and status is good */
+	if ((iue->req.linked) && (status == SENSE_SUCCESS)) {
+		status = SENSE_INTERMEDIATE;
+	}
+
+	memset(iu,0x00,sizeof(struct SRP_RSP));
+	iu->srp.rsp.type                          = SRP_RSP_TYPE;
+	iu->srp.rsp.request_limit_delta           = 1;
+	iu->srp.rsp.tag                           = tag;
+
+	iu->srp.rsp.diunder                       = iue->req.diunder;
+	iu->srp.rsp.diover                        = iue->req.diover;
+	iu->srp.rsp.dounder                       = iue->req.dounder;
+	iu->srp.rsp.doover                        = iue->req.doover;
+
+	iu->srp.rsp.data_in_residual_count        = iue->req.data_in_residual_count;
+	iu->srp.rsp.data_out_residual_count       = iue->req.data_out_residual_count;
+
+	iu->srp.rsp.rspvalid                      = 0;
+
+	iu->srp.rsp.response_data_list_length     = 0;
+
+	if (status) {
+		iu->srp.rsp.status                        = SAM_STAT_CHECK_CONDITION;
+		iu->srp.rsp.snsvalid                      = 1;
+		iu->srp.rsp.sense_data_list_length        = 18; /* TODO be smarter about this */
+
+		/* Valid bit and 'current errors' */
+		sense[0] = (0x1 << 7 | 0x70);
+		
+		/* Sense key */
+		sense[2] = ibmvscsis_sense_data[status][0];
+		
+		/* Additional sense length */
+		sense[7] = 0xa; /* 10 bytes */
+		
+		/* Additional sense code */
+		sense[12] = ibmvscsis_sense_data[status][1];
+		
+		/* Additional sense code qualifier */
+		sense[13] = ibmvscsis_sense_data[status][2];
+	} else {
+		iu->srp.rsp.status                        = 0;
+	}
+
+	send_srp(iue,sizeof(iu->srp.rsp));
+
+	return 0;
+}
+
+static void process_iu(struct VIOSRP_CRQ *crq, struct server_adapter *adapter)
+{
+	struct iu_entry *iue = get_iu(adapter);
+	union VIOSRP_IU *iu;
+	int queued = 0;
+	long rc;
+
+	if (iue == NULL) {
+		/* TODO Yikes! */
+		warn("Error getting IU from pool, other side exceeded limit\n");
+		return;
+	}
+
+	iue->req.remote_token = crq->IU_data_ptr;
+
+	rc = h_copy_rdma(crq->IU_length,
+			 iue->adapter->riobn,
+			 iue->req.remote_token,
+			 adapter->liobn,
+			 iue->iu_token);
+
+	iu = iue->iu;
+	
+	if (rc) {
+		err("Got rc %ld from h_copy_rdma\n",rc);
+	}
+	
+	if (crq->format == VIOSRP_MAD_FORMAT) {
+		switch (iu->srp.generic.type) {
+		case VIOSRP_EMPTY_IU_TYPE:
+			warn("Unsupported EMPTY MAD IU\n");
+			break;
+		case VIOSRP_ERROR_LOG_TYPE:
+			warn("Unsupported ERROR LOG MAD IU\n");
+			break;
+		case VIOSRP_ADAPTER_INFO_TYPE:
+			warn("Unsupported ADAPTER INFO MAD IU\n");
+			break;
+		default:
+			warn("Unsupported MAD type %d\n",
+			     iu->srp.generic.type);
+		}
+	} else {
+		switch (iu->srp.generic.type) {
+		case SRP_LOGIN_REQ_TYPE:
+			dbg("SRP LOGIN\n");
+			process_login(iue);
+			break;
+		case SRP_LOGIN_RSP_TYPE:
+			warn("Unsupported LOGIN_RSP SRP IU\n");
+			break;
+		case SRP_I_LOGOUT_TYPE:
+			warn("Unsupported I_LOGOUT SRP IU\n");
+			break;
+		case SRP_T_LOGOUT_TYPE:
+			warn("Unsupported T_LOGOUT SRP IU\n");
+			break;
+		case SRP_TSK_MGMT_TYPE:
+			warn("Unsupported TSK_MGMT SRP IU\n");
+			break;
+		case SRP_CMD_TYPE:
+			process_cmd(iue);
+			queued = 1;
+			break;
+		case SRP_RSP_TYPE:
+			warn("Unsupported RSP SRP IU\n");
+			break;
+		case SRP_CRED_REQ_TYPE:
+			warn("Unsupported CRED_REQ SRP IU\n");
+			break;
+		case SRP_CRED_RSP_TYPE:
+			warn("Unsupported CRED_RSP SRP IU\n");
+			break;
+		case SRP_AER_REQ_TYPE:
+			warn("Unsupported AER_REQ SRP IU\n");
+			break;
+		case SRP_AER_RSP_TYPE:
+			warn("Unsupported AER_RSP SRP IU\n");
+			break;
+		default:
+			warn("Unsupported SRP type %d\n",
+			     iu->srp.generic.type);
+		}
+	}
+
+	/* 
+	 * If no one has queued the IU for further work, free it 
+	 * Note that this is kind of an ugly design based on setting
+	 * this variable up above in cases where the routine we call
+	 * is responsible for freeing the IU
+	 */
+	if (!queued) 
+		free_iu(iue);
+}
+
+/* ==============================================================
+ * CRQ Processing Routines
+ * ==============================================================
+ */
+
+/*
+ * Handle a CRQ event
+ */
+static void handle_crq(struct VIOSRP_CRQ *crq, struct server_adapter *adapter)
+{
+	switch(crq->valid) {
+	case 0xC0: /* initialization */
+		switch(crq->format) {
+		case 0x01:
+			info("Partner just initialized\n");
+			plpar_hcall_norets(H_SEND_CRQ, adapter->dma_dev->unit_address, 
+					   0xC002000000000000, 0);
+			break;
+		case 0x02:
+			info("ibmvscsis: partner initialization complete\n");
+			break;
+		default:
+			err("Unknwn CRQ format %d\n",crq->format);
+		}
+		return;
+	case 0xFF: /* transport event */
+		info("ibmvscsis: partner closed\n");
+		return;
+	case 0x80:  /* real payload */
+	{
+		switch (crq->format) {
+		case  VIOSRP_SRP_FORMAT:
+		case VIOSRP_MAD_FORMAT:
+			process_iu(crq, adapter);
+			break;
+		case VIOSRP_OS400_FORMAT:
+			warn("Unsupported OS400 format CRQ\n");
+			break;
+				
+		case VIOSRP_AIX_FORMAT:
+			warn("Unsupported AIX format CRQ\n");
+			break;
+				
+		case VIOSRP_LINUX_FORMAT:
+			warn("Unsupported LINUX format CRQ\n");
+			break;
+				
+		case VIOSRP_INLINE_FORMAT:
+			warn("Unsupported _INLINE_ format CRQ\n");
+			break;
+				
+		default:
+			err("Unsupported CRQ format %d\n",crq->format);
+		}
+	}
+	break;
+	default:
+		err("ibmvscsis: got an invalid message type 0x%02x!?\n", crq->valid);
+		return;
+	}
+	
+}
+
+/*
+ * Task to handle CRQs and completions
+ */
+static void crq_task(void *data) {
+	struct server_adapter *adapter = (struct server_adapter *)data;
+	struct VIOSRP_CRQ *crq;
+	long rc;
+
+	if (atomic_inc_return(&adapter->crq_task_count) > 1) {
+		err("In crq_task twice!!!\n");
+	}
+
+	/* Loop through and process CRQs */
+	while((crq = crq_queue_next_crq(&adapter->queue)) != NULL) {
+		atomic_inc(&adapter->crq_processed);
+		handle_crq(crq, adapter);
+		crq->valid = 0x00;
+	}
+
+	/* If an interrupt comes in after this the task should be
+	 * rescheduled.
+	 */
+	rc = h_vio_signal(adapter->dma_dev->unit_address, 1);
+	if (rc != 0) {
+		err("Error %ld enabling interrupts!!!\n",rc);
+	}
+	atomic_dec(&adapter->crq_task_count);
+}
+
+/*
+ * Handle the interrupt that occurs when something is placed on our CRQ
+ */
+static irqreturn_t handle_interrupt(int irq, void *dev_instance, struct pt_regs *regs)
+{
+	struct server_adapter *adapter = (struct server_adapter *)dev_instance;
+	long rc;
+
+	rc = h_vio_signal(adapter->dma_dev->unit_address, 0);
+	if (rc != 0) {
+		err(" Error %ld disabling interrupts!!!\n",rc);
+	}
+
+	atomic_inc(&adapter->interrupts);
+	
+	schedule_work(&adapter->crq_work);
+
+	return IRQ_HANDLED;
+}
+
+/* 
+ * Initialize our CRQ
+ * return zero on success, non-zero on failure 
+ */
+static int initialize_crq_queue(struct crq_queue *queue, struct server_adapter *adapter)
+{
+	int rc;
+
+	queue->msgs = (struct VIOSRP_CRQ *)get_zeroed_page(GFP_KERNEL);
+	if(!queue->msgs)
+		goto malloc_failed;
+	queue->size = PAGE_SIZE / sizeof(*queue->msgs);
+
+	if((queue->msg_token = vio_map_single(adapter->dma_dev, 
+					      queue->msgs, 
+					      queue->size * sizeof(*queue->msgs), 
+					      PCI_DMA_BIDIRECTIONAL)) == NO_TCE)
+		goto map_failed;
+
+	rc = plpar_hcall_norets(H_REG_CRQ, adapter->dma_dev->unit_address, queue->msg_token, PAGE_SIZE);
+
+	/* If we opened successfully, send an init message */
+	if (rc == 0) {
+		plpar_hcall_norets(H_SEND_CRQ, adapter->dma_dev->unit_address, 
+				   0xC001000000000000, 0);
+	} else if (rc == 2) {
+		/* Other end is still closed.  This is normal */
+		info("connection registered, other end closed\n");
+	} else {
+		err("couldn't register crq--rc 0x%x\n", rc);
+		goto reg_crq_failed;
+	}
+
+	if(request_irq(adapter->dma_dev->irq, &handle_interrupt, SA_INTERRUPT, "ibmvscsis", adapter) != 0) 
+		goto req_irq_failed;
+
+	queue->cur = 0;
+	queue->lock = SPIN_LOCK_UNLOCKED;
+
+	return 0;
+
+ req_irq_failed:
+	plpar_hcall_norets(H_FREE_CRQ, adapter->dma_dev->unit_address);
+ reg_crq_failed:
+	vio_unmap_single(adapter->dma_dev, queue->msg_token, queue->size * sizeof(*queue->msgs), PCI_DMA_BIDIRECTIONAL);
+ map_failed:
+	free_page((unsigned long)queue->msgs);
+ malloc_failed:
+	return -1;
+}
+
+/*
+ * Release the CRQ
+ */
+static void release_crq_queue(struct crq_queue *queue, struct server_adapter *adapter)
+{
+	info("releasing crq\n");
+	free_irq(adapter->dma_dev->irq, adapter);
+	plpar_hcall_norets(H_FREE_CRQ, adapter->dma_dev->unit_address);
+	vio_unmap_single(adapter->dma_dev, queue->msg_token, queue->size * sizeof(*queue->msgs), PCI_DMA_BIDIRECTIONAL);
+	free_page((unsigned long)queue->msgs);
+}
+
+/* ==============================================================
+ * Module Management
+ * ==============================================================
+ */
+/*
+ * Add a block device as a SCSI LUN
+ */
+static void add_block_device(int majo, int mino, int bus, int target, struct server_adapter *adapter, int ro) {
+	struct vdev *vd;
+	struct vbus *newbus = NULL;
+	mode_t mode;
+	unsigned long flags;
+	dev_t kd = MKDEV(majo, mino);
+
+	if (bus >= BUS_PER_ADAPTER) {
+		err("Invalid bus %u specified\n",bus);
+		return;
+	}
+
+	if (target >= TARGETS_PER_BUS) {
+		err("Invalid target %u specified\n",bus);
+		return;
+	}
+
+	vd = (struct vdev *)kmalloc(sizeof(struct vdev), GFP_KERNEL);
+	if (vd == NULL) {
+		err("Unable to allocate memory for vdev structure");
+		return;
+	}
+	
+	memset(vd,0x00,sizeof(*vd));
+	vd->type = 'B';
+	vd->lun = make_lun(bus,target,0);
+	vd->b.dev = kd;
+	vd->b.bdev = bdget(kd);
+	vd->b.blksize = 512; 
+	vd->b.ro = ro;
+
+	if (ro) {
+		mode = FMODE_READ;
+	} else {
+		mode = FMODE_READ | FMODE_WRITE;
+	}
+
+	if (blkdev_get(vd->b.bdev, mode, 0) != 0) {
+		err("Error opening block device\n");
+		kfree(vd);
+		return;
+	} 
+
+	if (adapter->vbus[bus] == NULL) {
+		newbus = (struct vbus *)kmalloc(sizeof(struct vbus), GFP_KERNEL);
+		memset(newbus,0x00,sizeof(*newbus));
+	}
+	
+	spin_lock_irqsave(&adapter->lock,flags);
+	if ((newbus) && (adapter->vbus[bus] == NULL)) {
+		adapter->vbus[bus] = newbus;
+		newbus = NULL;
+	}
+	
+	if (adapter->vbus[bus]->vdev[target] != NULL) {
+		spin_unlock_irqrestore(&adapter->lock,flags);
+		err("Error: Duplicate vdev as lun 0x%lx\n",vd->lun);
+		kfree(vd);
+		return;
+	}
+	
+	adapter->vbus[bus]->vdev[target] = vd;
+	adapter->nvdevs++;
+	
+	spin_unlock_irqrestore(&adapter->lock,flags);
+	
+	if (newbus)
+		kfree(newbus);
+		
+	info("Adding block device %d:%d as %sLUN 0x%lx\n",
+	     majo,
+	     mino,
+	     ro ? "read only " : "",
+	     vd->lun);
+	return;
+}
+
+static void remove_block_device(int bus, int target, struct server_adapter *adapter) {
+	struct vdev *vd;
+	unsigned long flags;
+	
+	spin_lock_irqsave(&adapter->lock,flags);
+
+	if ((!adapter->vbus[bus]) || 
+	    (!adapter->vbus[bus]->vdev[target])) {
+		spin_unlock_irqrestore(&adapter->lock,flags);
+		err("Error removing non-existant device at bus %d, target %d\n",
+		    bus,target);
+		return;
+	}
+	
+	vd = adapter->vbus[bus]->vdev[target];
+	
+	if (vd->disabled) {
+		spin_unlock_irqrestore(&adapter->lock,flags);
+		err("Device at bus %d, target %d removed twice\n",
+		    bus,target);
+		return;
+	}
+
+	adapter->nvdevs--;
+
+	vd->disabled = 1;
+	
+	spin_unlock_irqrestore(&adapter->lock,flags);
+
+	/* Wait while any users of this device finish.  Note there should
+	 * be no new users, since we have marked this disabled
+	 *
+	 * We just poll here, since we are blocking a proc_write
+	 */
+	while (atomic_read(&vd->refcount)) {
+                schedule_timeout(HZ / 4); /* 1/4 second */
+	}
+	
+	spin_lock_irqsave(&adapter->lock,flags);
+	adapter->vbus[bus]->vdev[target] = NULL;
+	spin_unlock_irqrestore(&adapter->lock,flags);
+		
+	if (blkdev_put(vd->b.bdev)) {
+		err("Error closing block device!\n");
+	}
+	kfree(vd);
+
+	info("Removed block device at %d:%d\n",bus,target);
+}
+
+/*
+ * Handle read from our proc system file.  There is one of these
+ * files per adapter
+ */
+static int ibmvscsis_proc_read(char *page, char **start, off_t off,
+			       int count, int *eof, void *data)
+{
+	struct server_adapter *adapter = (struct server_adapter *)data;
+	int len = 0;
+	int bus;
+	int target;
+	struct vdev *vd;
+	unsigned long flags;
+
+	len += sprintf(page + len, "IBM VSCSI Server: %s\n",adapter->name);
+	len += sprintf(page + len, "interrupts: %10d\t\tread ops:   %10d\n",
+		       atomic_read(&adapter->interrupts),
+		       atomic_read(&adapter->read_processed));
+	len += sprintf(page + len, "crq msgs:   %10d\t\twrite ops:  %10d\n",
+		       atomic_read(&adapter->crq_processed),
+		       atomic_read(&adapter->write_processed));
+	len += sprintf(page + len, "iu alloc:   %10d\t\tbio alloc:  %10d\n",
+		       atomic_read(&adapter->iu_count),
+		       atomic_read(&adapter->bio_count));
+
+	len += sprintf(page + len, "buf alloc:  %10d\t\terrors:     %10d\n",
+		       atomic_read(&adapter->buffers_allocated),
+		       atomic_read(&adapter->errors));
+
+	spin_lock_irqsave(&adapter->lock,flags);
+
+	/* loop through the bus */
+	for (bus = 0; bus < BUS_PER_ADAPTER; bus++) {
+		/* If this bus exists */
+		if (adapter->vbus[bus]) {
+			/* loop through the targets */
+			for (target = 0; target < TARGETS_PER_BUS; target++)
+			{
+				/* If the target exists */
+				if (adapter->vbus[bus]->vdev[target]) {
+					vd = adapter->vbus[bus]->vdev[target];
+					if (vd->type == 'B') {
+						len += sprintf(page + len, "Block Device Major %d, Minor %d, Bus %d, Target %d LUN %d\n",
+							       MAJOR(vd->b.dev),
+							       MINOR(vd->b.dev),
+							       GETBUS(vd->lun),
+							       GETTARGET(vd->lun),
+							       GETLUN(vd->lun));
+					}
+				}
+			}
+		}
+	}
+
+	spin_unlock_irqrestore(&adapter->lock,flags);
+
+	*eof = 1;
+	return len;
+}
+
+/*
+ * Handle proc file system write.  One per adapter, currently used just
+ * to add virtual devices to our adapters.
+ */
+static int ibmvscsis_proc_write(struct file *file, const char *buffer,
+				unsigned long count, void *data){
+	int offset = 0;
+	int bytes;
+	char token[10];
+	char type[4];
+	int majo = -1;
+	int mino = -1;
+	unsigned int bus = -1;
+	unsigned int target = -1;
+	int ro = 0;
+
+	struct server_adapter *adapter = (struct server_adapter *)data;
+
+	if (sscanf(buffer+offset,"%9s%n",token,&bytes) != 1) {
+		err("Error on read of proc file\n");
+		return count;
+	}
+	offset += bytes;
+
+	if (strcmp(token,"add") == 0) {
+		
+		if (sscanf(buffer+offset,"%3s%n",type,&bytes) != 1) {
+			err("Error on read of proc file\n");
+			return count;
+		}
+		offset += bytes;
+		
+		sscanf(buffer+offset,"%i%n",&majo,&bytes);
+		offset += bytes;
+		
+		sscanf(buffer+offset,"%i%n",&mino,&bytes);
+		offset += bytes;
+		
+		sscanf(buffer+offset,"%i%n",&bus,&bytes);
+		offset += bytes;
+		
+		sscanf(buffer+offset,"%i%n",&target,&bytes);
+		offset += bytes;
+		
+		if (strcmp(type,"b") == 0) {
+			ro = 0;
+		} else if (strcmp(type,"br") == 0) {
+			ro = 1;
+		} else {
+			err("Invalid type %s on add request\n",type);
+			return count;
+		}
+		
+		if ((majo == -1) || (mino == -1)) {
+			err("Ignoring command %s device %d::%ds type %s\n",
+			    token,majo, mino,type);
+			return count;
+		}
+		add_block_device(majo,mino,bus,target, adapter, ro);
+	} else if (strcmp(token,"remove") == 0) {
+		if (sscanf(buffer+offset,"%3s%n",type,&bytes) != 1) {
+			err("Error on read of proc file\n");
+			return count;
+		}
+		offset += bytes;
+		
+		sscanf(buffer+offset,"%i%n",&bus,&bytes);
+		offset += bytes;
+		
+		sscanf(buffer+offset,"%i%n",&target,&bytes);
+		offset += bytes;
+		
+		if ((strcmp(type,"b") != 0) &&
+		    (strcmp(type,"br") != 0)) {
+			err("Ignoring remove command for invalid type %s\n",type);
+			return count;
+		}
+		
+		if ((bus == -1) || (target == -1)) {
+			err("Ignoring remove command bus %d target%ds\n",
+			    bus,target);
+			return count;
+		}
+		
+		remove_block_device(bus, target, adapter);
+	} else if (strcmp(token,"debug") == 0) {
+		ibmvscsis_debug = 1;
+		dbg("debugging on\n");
+	} else {
+		err("Ignoring command %s\n",token);
+	}
+	
+	return count;
+}
+ 
+static void ibmvscsis_proc_register_driver(void)
+{
+#ifdef CONFIG_PROC_FS
+	ibmvscsis_proc_dir = create_proc_entry(IBMVSCSIS_PROC_DIR, S_IFDIR, proc_root_driver);
+#endif
+}
+
+static void ibmvscsis_proc_unregister_driver(void)
+{
+#ifdef CONFIG_PROC_FS
+	remove_proc_entry(IBMVSCSIS_PROC_DIR, proc_root_driver);
+#endif
+}
+
+static void ibmvscsis_proc_register_adapter(struct server_adapter *adapter)
+{
+#ifdef CONFIG_PROC_FS
+	struct proc_dir_entry *entry;
+	entry = create_proc_entry(adapter->name, S_IFREG, ibmvscsis_proc_dir);
+	entry->data = (void *) adapter;
+	entry->read_proc = ibmvscsis_proc_read;
+	entry->write_proc = ibmvscsis_proc_write;
+#endif
+}
+
+static void ibmvscsis_proc_unregister_adapter(struct server_adapter *adapter)
+{
+#ifdef CONFIG_PROC_FS
+	remove_proc_entry(adapter->name, ibmvscsis_proc_dir);
+#endif
+}
+
+static int ibmvscsis_probe(struct vio_dev *dev, const struct vio_device_id *id)
+{
+	struct server_adapter *adapter;
+	int rc;
+	unsigned int *dma_window;
+	unsigned int dma_window_property_size;
+	
+	info("entering probe for UA 0x%x\n", dev->unit_address);
+
+	adapter = kmalloc(sizeof(*adapter), GFP_KERNEL);
+	if(!adapter) {
+		err("couldn't kmalloc adapter structure\n");
+		return -1;
+	}
+	memset(adapter, 0x00, sizeof(*adapter));
+	adapter->dma_dev = dev;
+	dev->driver_data = adapter;
+	sprintf(adapter->name,"%x",dev->unit_address);
+	adapter->lock = SPIN_LOCK_UNLOCKED;
+
+	dma_window = (unsigned int *)vio_get_attribute(dev, "ibm,my-dma-window", &dma_window_property_size);
+	if(!dma_window) {
+		warn("Couldn't find ibm,my-dma-window property\n");
+	}
+	
+	adapter->liobn = dma_window[0];
+	/* RPA docs say that #address-cells is always 1 for virtual
+	   devices, but some older boxes' OF returns 2.  This should
+	   be removed by GA, unless there is legacy OFs that still
+	   have 2 or 3 for #address-cells */
+	/*adapter->riobn = dma_window[2+vio_num_address_cells];*/
+
+	/* This is just an ugly kludge. Remove as soon as the OF for all
+	   machines actually follow the spec and encodes the offset field
+	   as phys-encode (that is, #address-cells wide)*/
+	if (dma_window_property_size == 24) {
+		adapter->riobn = dma_window[3];
+	} else if (dma_window_property_size == 40) {
+		adapter->riobn = dma_window[5];
+	} else {
+		warn("ibmvscsis: Invalid size of ibm,my-dma-window=%i\n", dma_window_property_size);
+	}
+
+	INIT_WORK(&adapter->crq_work, crq_task, adapter);
+	INIT_WORK(&adapter->endio_work, endio_task, adapter);
+
+	/* Initialize the buffer cache */
+	init_data_buffer(adapter);
+
+	/* Arbitrarily support 16 IUs right now */
+	rc = initialize_iu_pool(adapter, 16);
+	if (rc) {
+		kfree(adapter);
+		return rc;
+	}
+
+	rc = initialize_crq_queue(&adapter->queue, adapter);
+	if (rc != 0) {
+		kfree(adapter);
+		return rc;
+	}
+
+	rc = h_vio_signal(adapter->dma_dev->unit_address, 1);
+	if (rc != 0) {
+		err("Error %d enabling interrupts!!!\n",rc);
+	}
+
+	ibmvscsis_proc_register_adapter(adapter);
+
+	return 0;
+}
+
+
+static int ibmvscsis_remove(struct vio_dev *dev)
+{
+	int bus;
+	int target;
+	
+	struct server_adapter *adapter = (struct server_adapter *)dev->driver_data;
+
+	info("entering remove for UA 0x%x\n", dev->unit_address);
+
+	ibmvscsis_proc_unregister_adapter(adapter);
+
+	/* 
+	 * No one should be adding any devices at this point because we blew 
+	 * away the proc file system entry 
+	 *
+	 * Loop through the bus
+	 */
+	for (bus = 0; bus < BUS_PER_ADAPTER; bus++) {
+		/* If this bus exists */
+		if (adapter->vbus[bus]) {
+			/* loop through the targets */
+			for (target = 0; target < TARGETS_PER_BUS; target++)
+			{
+				/* If the target exists */
+				if (adapter->vbus[bus]->vdev[target]) {
+					remove_block_device(bus,target,adapter);
+				}
+			}
+		}
+	}
+
+	release_crq_queue(&adapter->queue, adapter);
+
+	release_iu_pool(adapter);
+
+	release_data_buffer(adapter);
+
+	kfree(adapter);
+
+	return 0;
+}
+
+static struct vio_device_id ibmvscsis_device_table[] __devinitdata= {
+	{ "v-scsi-host", "IBM,v-scsi-host" },
+	{ 0,}
+};
+
+MODULE_DEVICE_TABLE(vio, ibmvscsis_device_table);
+
+static struct vio_driver ibmvscsis_driver = {
+	.name        = ibmvscsis_driver_name,
+	.id_table    = ibmvscsis_device_table,
+	.probe       = ibmvscsis_probe,
+	.remove      = ibmvscsis_remove,
+};
+
+static int mod_init(void) {
+	int rc;
+
+	info("ibmvscsis initialized\n");
+
+	ibmvscsis_proc_register_driver();
+
+	rc = vio_register_driver(&ibmvscsis_driver);
+	
+	if (rc) {
+		warn("rc %d from vio_register_driver\n",rc);
+	}
+
+	return rc;
+}
+
+static void mod_exit(void) {
+	info("terminated\n");
+
+	vio_unregister_driver(&ibmvscsis_driver); 
+
+	ibmvscsis_proc_unregister_driver(); 
+}
+
+module_init(mod_init);
+module_exit(mod_exit);
diff -purN linux-2.5/drivers/scsi/ibmvscsi/iseries_vscsi.c linuxppc64-2.5/drivers/scsi/ibmvscsi/iseries_vscsi.c
--- linux-2.5/drivers/scsi/ibmvscsi/iseries_vscsi.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/ibmvscsi/iseries_vscsi.c	2004-03-12 14:24:45.000000000 +0000
@@ -0,0 +1,159 @@
+/* ------------------------------------------------------------
+ * iSeries_vscsi.c
+ * (C) Copyright IBM Corporation 1994, 2003
+ * Authors: Colin DeVilbiss (devilbis@us.ibm.com)
+ *          Santiago Leon (santil@us.ibm.com)
+ *          Dave Boutcher (sleddog@us.ibm.com)
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307
+ * USA
+ *
+ * ------------------------------------------------------------
+ * iSeries-specific functions of the SCSI host adapter for Virtual I/O devices
+ *
+ * This driver allows the Linux SCSI peripheral drivers to directly
+ * access devices in the hosting partition, either on an iSeries
+ * hypervisor system or a converged hypervisor system.
+ */
+
+#include <asm/iSeries/vio.h>
+#include <asm/iSeries/HvLpEvent.h>
+#include <asm/iSeries/HvTypes.h>
+#include <asm/iSeries/HvLpConfig.h>
+#include <linux/pci.h>
+#include <linux/device.h>
+#include "ibmvscsi.h"
+
+static void noop_release(struct device *dev) {};
+
+/* global variables */
+extern struct device *iSeries_vio_dev;
+static struct ibmvscsi_host_data *single_host_data;
+static struct pci_dev iseries_vscsi_dev = {
+	.dev.bus = &pci_bus_type,
+	.dev.bus_id = "vscsi",
+	.dev.release = noop_release
+};
+
+/* ------------------------------------------------------------
+ * Routines for direct interpartition interaction
+ */
+struct VIOSRPLpEvent {
+	struct HvLpEvent lpevt;	/* 0x00-0x17          */
+	u32 reserved1;		/* 0x18-0x1B; unused  */
+	u16 version;		/* 0x1C-0x1D; unused  */
+	u16 subtype_rc;		/* 0x1E-0x1F; unused  */
+	struct VIOSRP_CRQ crq;	/* 0x20-0x3F          */
+};
+
+/** 
+ * standard interface for handling logical partition events.
+ */
+static void ibmvscsi_handle_event(struct HvLpEvent *lpevt)
+{
+	struct VIOSRPLpEvent *evt = (struct VIOSRPLpEvent *)lpevt;
+
+	if (!evt) {
+		printk(KERN_ERR "ibmvscsi: received null event\n");
+		return;
+	}
+
+	if (single_host_data == NULL) {
+		printk(KERN_ERR
+		       "ibmvscsi: received event, no adapter present\n");
+		return;
+	}
+
+	ibmvscsi_handle_crq(&evt->crq, single_host_data);
+}
+
+/* ------------------------------------------------------------
+ * Routines for driver initialization
+ */
+int ibmvscsi_init_crq_queue(struct crq_queue *queue,
+			    struct ibmvscsi_host_data *hostdata)
+{
+	int rc;
+
+	rc = viopath_open(viopath_hostLp, viomajorsubtype_scsi, 0);
+	if (rc < 0) {
+		printk("viopath_open failed with rc %d in open_event_path\n",
+		       rc);
+		goto viopath_open_failed;
+	}
+
+	rc = vio_setHandler(viomajorsubtype_scsi, ibmvscsi_handle_event);
+	if (rc < 0) {
+		printk("vio_setHandler failed with rc %d in open_event_path\n",
+		       rc);
+		goto vio_setHandler_failed;
+	}
+	return 0;
+
+      vio_setHandler_failed:
+	viopath_close(viopath_hostLp, viomajorsubtype_scsi,
+		      IBMVSCSI_MAX_REQUESTS);
+      viopath_open_failed:
+	return -1;
+}
+
+void ibmvscsi_release_crq_queue(struct crq_queue *queue,
+				struct ibmvscsi_host_data *hostdata)
+{
+	vio_clearHandler(viomajorsubtype_scsi);
+	viopath_close(viopath_hostLp, viomajorsubtype_scsi,
+		      IBMVSCSI_MAX_REQUESTS);
+}
+
+/**
+ * ibmvscsi_send_crq: - Send a CRQ
+ * @hostdata:	the adapter
+ * @word1:	the first 64 bits of the data
+ * @word2:	the second 64 bits of the data
+ */
+int ibmvscsi_send_crq(struct ibmvscsi_host_data *hostdata, u64 word1, u64 word2)
+{
+	single_host_data = hostdata;
+	return HvCallEvent_signalLpEventFast(viopath_hostLp,
+					     HvLpEvent_Type_VirtualIo,
+					     viomajorsubtype_scsi,
+					     HvLpEvent_AckInd_NoAck,
+					     HvLpEvent_AckType_ImmediateAck,
+					     viopath_sourceinst(viopath_hostLp),
+					     viopath_targetinst(viopath_hostLp),
+					     0,
+					     VIOVERSION << 16, word1, word2, 0,
+					     0);
+}
+
+int __init ibmvscsi_module_init(void)
+{
+	iseries_vscsi_dev.sysdata = to_pci_dev(iSeries_vio_dev)->sysdata;
+	if (device_register(&iseries_vscsi_dev.dev)) {
+		printk(KERN_ERR "ibmvscsi: failed to register device\n");
+		return 1;
+	}
+	single_host_data = ibmvscsi_probe(&iseries_vscsi_dev.dev);
+	return (single_host_data == NULL);
+}
+
+void __exit ibmvscsi_module_exit(void)
+{
+	ibmvscsi_remove(single_host_data);
+	device_unregister(&iseries_vscsi_dev.dev);
+}
+
+module_init(ibmvscsi_module_init);
+module_exit(ibmvscsi_module_exit);
diff -purN linux-2.5/drivers/scsi/ibmvscsi/rpa_vscsi.c linuxppc64-2.5/drivers/scsi/ibmvscsi/rpa_vscsi.c
--- linux-2.5/drivers/scsi/ibmvscsi/rpa_vscsi.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/ibmvscsi/rpa_vscsi.c	2004-03-16 20:47:27.000000000 +0000
@@ -0,0 +1,299 @@
+/* ------------------------------------------------------------
+ * rpa_vscsi.c
+ * (C) Copyright IBM Corporation 1994, 2003
+ * Authors: Colin DeVilbiss (devilbis@us.ibm.com)
+ *          Santiago Leon (santil@us.ibm.com)
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307
+ * USA
+ *
+ * ------------------------------------------------------------
+ * RPA-specific functions of the SCSI host adapter for Virtual I/O devices
+ *
+ * This driver allows the Linux SCSI peripheral drivers to directly
+ * access devices in the hosting partition, either on an iSeries
+ * hypervisor system or a converged hypervisor system.
+ */
+
+#include <asm/vio.h>
+#include <asm/iommu.h>
+#include <asm/hvcall.h>
+#include <linux/dma-mapping.h>
+#include <linux/interrupt.h>
+#include "ibmvscsi.h"
+
+/* ------------------------------------------------------------
+ * Routines for managing the command/response queue
+ */
+/**
+ * ibmvscsi_handle_event: - Interrupt handler for crq events
+ * @irq:	number of irq to handle, not used
+ * @dev_instance: ibmvscsi_host_data of host that received interrupt
+ * @regs:	pt_regs with registers
+ *
+ * Disables interrupts and schedules srp_task
+ * Always returns IRQ_HANDLED
+ */
+static irqreturn_t ibmvscsi_handle_event(int irq,
+					 void *dev_instance,
+					 struct pt_regs *regs)
+{
+	struct ibmvscsi_host_data *hostdata =
+	    (struct ibmvscsi_host_data *)dev_instance;
+	vio_disable_interrupts(to_vio_dev(hostdata->dev));
+	schedule_work(&hostdata->srp_task);
+	return IRQ_HANDLED;
+}
+
+/**
+ * release_crq_queue: - Deallocates data and unregisters CRQ
+ * @queue:	crq_queue to initialize and register
+ * @host_data:	ibmvscsi_host_data of host
+ *
+ * Frees irq, deallocates a page for messages, unmaps dma, and unregisters
+ * the crq with the hypervisor.
+ */
+void ibmvscsi_release_crq_queue(struct crq_queue *queue,
+				struct ibmvscsi_host_data *hostdata)
+{
+	struct vio_dev *vdev = to_vio_dev(hostdata->dev);
+	free_irq(vdev->irq, (void *)hostdata);
+	plpar_hcall_norets(H_FREE_CRQ, vdev->unit_address);
+	dma_unmap_single(hostdata->dev,
+			 queue->msg_token,
+			 queue->size * sizeof(*queue->msgs),
+			 PCI_DMA_BIDIRECTIONAL);
+	free_page((unsigned long)queue->msgs);
+}
+
+/**
+ * crq_queue_next_crq: - Returns the next entry in message queue
+ * @queue:	crq_queue to use
+ *
+ * Returns pointer to next entry in queue, or NULL if there are no new 
+ * entried in the CRQ.
+ */
+static struct VIOSRP_CRQ *crq_queue_next_crq(struct crq_queue *queue)
+{
+	struct VIOSRP_CRQ *crq;
+	unsigned long flags;
+
+	spin_lock_irqsave(&queue->lock, flags);
+	crq = &queue->msgs[queue->cur];
+	if (crq->valid & 0x80) {
+		if (++queue->cur == queue->size)
+			queue->cur = 0;
+	} else
+		crq = NULL;
+	spin_unlock_irqrestore(&queue->lock, flags);
+
+	return crq;
+}
+
+/**
+ * ibmvscsi_send_crq: - Send a CRQ
+ * @hostdata:	the adapter
+ * @word1:	the first 64 bits of the data
+ * @word2:	the second 64 bits of the data
+ */
+int ibmvscsi_send_crq(struct ibmvscsi_host_data *hostdata, u64 word1, u64 word2)
+{
+	struct vio_dev *vdev = to_vio_dev(hostdata->dev);
+
+	return plpar_hcall_norets(H_SEND_CRQ, vdev->unit_address, word1, word2);
+}
+
+/**
+ * ibmvscsi_task: - Process srps asynchronously
+ * @data:	ibmvscsi_host_data of host
+ */
+static void ibmvscsi_task(void *data)
+{
+	struct ibmvscsi_host_data *hostdata = (struct ibmvscsi_host_data *)data;
+	struct vio_dev *vdev = to_vio_dev(hostdata->dev);
+	struct VIOSRP_CRQ *crq;
+	int done = 0;
+
+	while (!done) {
+		/* Pull all the valid messages off the CRQ */
+		while ((crq = crq_queue_next_crq(&hostdata->queue)) != NULL) {
+			ibmvscsi_handle_crq(crq, hostdata);
+			crq->valid = 0x00;
+		}
+
+		vio_enable_interrupts(vdev);
+		if ((crq = crq_queue_next_crq(&hostdata->queue)) != NULL) {
+			vio_disable_interrupts(vdev);
+			ibmvscsi_handle_crq(crq, hostdata);
+			crq->valid = 0x00;
+		} else {
+			done = 1;
+		}
+	}
+}
+
+/**
+ * initialize_crq_queue: - Initializes and registers CRQ with hypervisor
+ * @queue:	crq_queue to initialize and register
+ * @hostdata:	ibmvscsi_host_data of host
+ *
+ * Allocates a page for messages, maps it for dma, and registers
+ * the crq with the hypervisor.
+ * Returns zero on success.
+ */
+int ibmvscsi_init_crq_queue(struct crq_queue *queue,
+			    struct ibmvscsi_host_data *hostdata)
+{
+	int rc;
+	struct vio_dev *vdev = to_vio_dev(hostdata->dev);
+
+	queue->msgs = (struct VIOSRP_CRQ *)get_zeroed_page(GFP_KERNEL);
+
+	if (!queue->msgs)
+		goto malloc_failed;
+	queue->size = PAGE_SIZE / sizeof(*queue->msgs);
+
+	if ((queue->msg_token = dma_map_single(hostdata->dev,
+					       queue->msgs,
+					       queue->size *
+					       sizeof(*queue->msgs),
+					       PCI_DMA_BIDIRECTIONAL)) ==
+	    NO_TCE)
+		goto map_failed;
+
+	rc = plpar_hcall_norets(H_REG_CRQ,
+				vdev->unit_address,
+				queue->msg_token, PAGE_SIZE);
+	if (rc == 2) {
+		/* Adapter is good, but other end is not ready */
+		printk(KERN_WARNING "ibmvscsi: Partner adapter not ready\n");
+	} else if (rc != 0) {
+		printk(KERN_WARNING
+		       "ibmvscsi: couldn't register crq--rc 0x%x\n", rc);
+		goto reg_crq_failed;
+	}
+
+	if (request_irq(vdev->irq,
+			ibmvscsi_handle_event,
+			0, "ibmvscsi", (void *)hostdata) != 0) {
+		printk(KERN_ERR "ibmvscsi: couldn't register irq 0x%x\n",
+		       vdev->irq);
+		goto req_irq_failed;
+	}
+
+	rc = vio_enable_interrupts(vdev);
+	if (rc != 0) {
+		printk(KERN_ERR "ibmvscsi:  Error %d enabling interrupts!!!\n",
+		       rc);
+		goto req_irq_failed;
+	}
+
+	queue->cur = 0;
+	queue->lock = SPIN_LOCK_UNLOCKED;
+
+	INIT_WORK(&hostdata->srp_task, (void *)ibmvscsi_task, hostdata);
+
+	return 0;
+
+      req_irq_failed:
+	plpar_hcall_norets(H_FREE_CRQ, vdev->unit_address);
+      reg_crq_failed:
+	dma_unmap_single(hostdata->dev,
+			 queue->msg_token,
+			 queue->size * sizeof(*queue->msgs),
+			 PCI_DMA_BIDIRECTIONAL);
+      map_failed:
+	free_page((unsigned long)queue->msgs);
+      malloc_failed:
+	return -1;
+}
+
+/**
+ * Return host configuration data.  This interface is exported
+ * because it is used by arch/ppc64 code to retrieve host
+ * configuration information.
+ */
+int ibmvscsi_get_host_config(struct vio_dev *vdev,
+			     unsigned char *buffer, 
+			     int length) {
+	struct ibmvscsi_host_data *hostdata = 
+		(struct ibmvscsi_host_data *) vdev->driver_data;
+
+	if (hostdata == NULL)
+		return -1;
+
+	return ibmvscsi_do_host_config(hostdata,buffer,length);
+}
+EXPORT_SYMBOL(ibmvscsi_get_host_config);
+
+/**
+ * rpa_device_table: Used by vio.c to match devices in the device tree we 
+ * support.
+ */
+static struct vio_device_id rpa_device_table[] __devinitdata = {
+	{"scsi-3", "IBM,v-scsi"},	/* Note: This entry can go away when 
+					   all the firmware is up to date */
+	{"vscsi", "IBM,v-scsi"},
+	{0,}
+};
+
+/**
+ * rpa_probe: The callback from the virtual I/O bus code.
+ * @vdev     : The vio specific device structure
+ * @id       : the device id..we don't currently use it
+ */
+static int rpa_probe(struct vio_dev *vdev, const struct vio_device_id *id)
+{
+	struct ibmvscsi_host_data *hostdata = ibmvscsi_probe(&vdev->dev);
+	if (hostdata) {
+		vdev->driver_data = hostdata;
+		return 0;
+	} else {
+		return -1;
+	}
+}
+
+/**
+ * rpa_remove: The callback from the virtual I/O bus code to remove a device
+ * @vdev     : The vio specific device structure
+ */
+static int rpa_remove(struct vio_dev *vdev)
+{
+	struct ibmvscsi_host_data *hostdata =
+	    (struct ibmvscsi_host_data *)vdev->driver_data;
+	ibmvscsi_remove(hostdata);
+	return 0;
+}
+
+MODULE_DEVICE_TABLE(vio, rpa_device_table);
+static struct vio_driver ibmvscsi_driver = {
+	.name = "ibmvscsi",
+	.id_table = rpa_device_table,
+	.probe = rpa_probe,
+	.remove = rpa_remove
+};
+
+int __init ibmvscsi_module_init(void)
+{
+	return vio_register_driver(&ibmvscsi_driver);
+}
+
+void __exit ibmvscsi_module_exit(void)
+{
+	vio_unregister_driver(&ibmvscsi_driver);
+}
+
+module_init(ibmvscsi_module_init);
+module_exit(ibmvscsi_module_exit);
diff -purN linux-2.5/drivers/scsi/ibmvscsi/srp.h linuxppc64-2.5/drivers/scsi/ibmvscsi/srp.h
--- linux-2.5/drivers/scsi/ibmvscsi/srp.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/ibmvscsi/srp.h	2004-02-25 16:57:38.000000000 +0000
@@ -0,0 +1,225 @@
+/*****************************************************************************/
+/* srp.h -- SCSI RDMA Protocol definitions                                   */
+/*                                                                           */
+/* Written By: Colin Devilbis, IBM Corporation                               */
+/*                                                                           */
+/* Copyright (C) 2003 IBM Corporation                                        */
+/*                                                                           */
+/* This program is free software; you can redistribute it and/or modify      */
+/* it under the terms of the GNU General Public License as published by      */
+/* the Free Software Foundation; either version 2 of the License, or         */
+/* (at your option) any later version.                                       */
+/*                                                                           */
+/* This program is distributed in the hope that it will be useful,           */
+/* but WITHOUT ANY WARRANTY; without even the implied warranty of            */
+/* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the             */
+/* GNU General Public License for more details.                              */
+/*                                                                           */
+/* You should have received a copy of the GNU General Public License         */
+/* along with this program; if not, write to the Free Software               */
+/* Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA */
+/*                                                                           */
+/*                                                                           */
+/* This file contains structures and definitions for the SCSI RDMA Protocol  */
+/* (SRP) as defined in the T10 standard available at www.t10.org.  This      */
+/* file was based on the 16a version of the standard                         */
+/*                                                                           */
+/*****************************************************************************/
+#ifndef SRP_H
+#define SRP_H
+
+#define PACKED __attribute__((packed))
+
+enum SRP_TYPES {
+	SRP_LOGIN_REQ_TYPE = 0x00,
+	SRP_LOGIN_RSP_TYPE = 0xC0,
+	SRP_LOGIN_REJ_TYPE = 0x80,
+	SRP_I_LOGOUT_TYPE = 0x03,
+	SRP_T_LOGOUT_TYPE = 0x80,
+	SRP_TSK_MGMT_TYPE = 0x01,
+	SRP_CMD_TYPE = 0x02,
+	SRP_RSP_TYPE = 0xC1,
+	SRP_CRED_REQ_TYPE = 0x81,
+	SRP_CRED_RSP_TYPE = 0x41,
+	SRP_AER_REQ_TYPE = 0x82,
+	SRP_AER_RSP_TYPE = 0x42
+};
+
+enum SRP_DESCRIPTOR_FORMATS {
+	SRP_NO_BUFFER = 0x00,
+	SRP_DIRECT_BUFFER = 0x01,
+	SRP_INDIRECT_BUFFER = 0x02
+};
+
+struct memory_descriptor {
+	u64 virtual_address;
+	u32 memory_handle;
+	u32 length;
+};
+
+struct indirect_descriptor {
+	struct memory_descriptor head;
+	u64 total_length;
+	struct memory_descriptor list[1];
+};
+
+struct SRP_GENERIC {
+	u8 type;
+	u8 reserved1[7];
+	u64 tag;
+};
+
+struct SRP_LOGIN_REQ {
+	u8 type;
+	u8 reserved1[7];
+	u64 tag;
+	u32 max_requested_initiator_to_target_iulen;
+	u32 reserved2;
+	u16 required_buffer_formats;
+	u8 reserved3:6;
+	u8 multi_channel_action:2;
+	u8 reserved4;
+	u32 reserved5;
+	u8 initiator_port_identifier[16];
+	u8 target_port_identifier[16];
+};
+
+struct SRP_LOGIN_RSP {
+	u8 type;
+	u8 reserved1[3];
+	u32 request_limit_delta;
+	u64 tag;
+	u32 max_initiator_to_target_iulen;
+	u32 max_target_to_initiator_iulen;
+	u16 supported_buffer_formats;
+	u8 reserved2:6;
+	u8 multi_channel_result:2;
+	u8 reserved3;
+	u8 reserved4[24];
+};
+
+struct SRP_LOGIN_REJ {
+	u8 type;
+	u8 reserved1[3];
+	u32 reason;
+	u64 tag;
+	u64 reserved2;
+	u16 supported_buffer_formats;
+	u8 reserved3[6];
+};
+
+struct SRP_I_LOGOUT {
+	u8 type;
+	u8 reserved1[7];
+	u64 tag;
+};
+
+struct SRP_T_LOGOUT {
+	u8 type;
+	u8 reserved1[3];
+	u32 reason;
+	u64 tag;
+};
+
+struct SRP_TSK_MGMT {
+	u8 type;
+	u8 reserved1[7];
+	u64 tag;
+	u32 reserved2;
+	u64 lun PACKED;
+	u8 reserved3;
+	u8 reserved4;
+	u8 task_mgmt_flags;
+	u8 reserved5;
+	u64 managed_task_tag;
+	u64 reserved6;
+};
+
+struct SRP_CMD {
+	u8 type;
+	u32 reserved1 PACKED;
+	u8 data_out_format:4;
+	u8 data_in_format:4;
+	u8 data_out_count;
+	u8 data_in_count;
+	u64 tag;
+	u32 reserved2;
+	u64 lun PACKED;
+	u8 reserved3;
+	u8 reserved4:5;
+	u8 task_attribute:3;
+	u8 reserved5;
+	u8 additional_cdb_len;
+	u8 cdb[16];
+	u8 additional_data[0x100 - 0x30];
+};
+
+struct SRP_RSP {
+	u8 type;
+	u8 reserved1[3];
+	u32 request_limit_delta;
+	u64 tag;
+	u16 reserved2;
+	u8 reserved3:2;
+	u8 diunder:1;
+	u8 diover:1;
+	u8 dounder:1;
+	u8 doover:1;
+	u8 snsvalid:1;
+	u8 rspvalid:1;
+	u8 status;
+	u32 data_in_residual_count;
+	u32 data_out_residual_count;
+	u32 sense_data_list_length;
+	u32 response_data_list_length;
+	u8 sense_and_response_data[18];
+};
+
+struct SRP_CRED_REQ {
+	u8 type;
+	u8 reserved1[3];
+	u32 request_limit_delta;
+	u64 tag;
+};
+
+struct SRP_CRED_RSP {
+	u8 type;
+	u8 reserved1[7];
+	u64 tag;
+};
+
+struct SRP_AER_REQ {
+	u8 type;
+	u8 reserved1[3];
+	u32 request_limit_delta;
+	u64 tag;
+	u32 reserved2;
+	u64 lun;
+	u32 sense_data_list_length;
+	u32 reserved3;
+	u8 sense_data[20];
+};
+
+struct SRP_AER_RSP {
+	u8 type;
+	u8 reserved1[7];
+	u64 tag;
+};
+
+union SRP_IU {
+	struct SRP_GENERIC generic;
+	struct SRP_LOGIN_REQ login_req;
+	struct SRP_LOGIN_RSP login_rsp;
+	struct SRP_LOGIN_REJ login_rej;
+	struct SRP_I_LOGOUT i_logout;
+	struct SRP_T_LOGOUT t_logout;
+	struct SRP_TSK_MGMT tsk_mgmt;
+	struct SRP_CMD cmd;
+	struct SRP_RSP rsp;
+	struct SRP_CRED_REQ cred_req;
+	struct SRP_CRED_RSP cred_rsp;
+	struct SRP_AER_REQ aer_req;
+	struct SRP_AER_RSP aer_rsp;
+};
+
+#endif
diff -purN linux-2.5/drivers/scsi/ibmvscsi/viosrp.h linuxppc64-2.5/drivers/scsi/ibmvscsi/viosrp.h
--- linux-2.5/drivers/scsi/ibmvscsi/viosrp.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/ibmvscsi/viosrp.h	2004-02-25 16:57:38.000000000 +0000
@@ -0,0 +1,126 @@
+/*****************************************************************************/
+/* srp.h -- SCSI RDMA Protocol definitions                                   */
+/*                                                                           */
+/* Written By: Colin Devilbis, IBM Corporation                               */
+/*                                                                           */
+/* Copyright (C) 2003 IBM Corporation                                        */
+/*                                                                           */
+/* This program is free software; you can redistribute it and/or modify      */
+/* it under the terms of the GNU General Public License as published by      */
+/* the Free Software Foundation; either version 2 of the License, or         */
+/* (at your option) any later version.                                       */
+/*                                                                           */
+/* This program is distributed in the hope that it will be useful,           */
+/* but WITHOUT ANY WARRANTY; without even the implied warranty of            */
+/* MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the             */
+/* GNU General Public License for more details.                              */
+/*                                                                           */
+/* You should have received a copy of the GNU General Public License         */
+/* along with this program; if not, write to the Free Software               */
+/* Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA */
+/*                                                                           */
+/*                                                                           */
+/* This file contains structures and definitions for IBM RPA (RS/6000        */
+/* platform architecture) implementation of the SRP (SCSI RDMA Protocol)     */
+/* standard.  SRP is used on IBM iSeries and pSeries platforms to send SCSI  */
+/* commands between logical partitions.                                      */
+/*                                                                           */
+/* SRP Information Units (IUs) are sent on a "Command/Response Queue" (CRQ)  */
+/* between partitions.  The definitions in this file are architected,        */
+/* and cannot be changed without breaking compatibility with other versions  */
+/* of Linux and other operating systems (AIX, OS/400) that talk this protocol*/
+/* between logical partitions                                                */
+/*****************************************************************************/
+#ifndef VIOSRP_H
+#define VIOSRP_H
+#include "srp.h"
+
+enum VIOSRP_CRQ_FORMATS {
+	VIOSRP_SRP_FORMAT = 0x01,
+	VIOSRP_MAD_FORMAT = 0x02,
+	VIOSRP_OS400_FORMAT = 0x03,
+	VIOSRP_AIX_FORMAT = 0x04,
+	VIOSRP_LINUX_FORMAT = 0x06,
+	VIOSRP_INLINE_FORMAT = 0x07
+};
+
+struct VIOSRP_CRQ {
+	u8 valid;		/* used by RPA */
+	u8 format;		/* SCSI vs out-of-band */
+	u8 reserved;
+	u8 status;		/* non-scsi failure? (e.g. DMA failure) */
+	u16 timeout;		/* in seconds */
+	u16 IU_length;		/* in bytes */
+	u64 IU_data_ptr;	/* the TCE for transferring data */
+};
+
+/* MADs are Management requests above and beyond the IUs defined in the SRP
+ * standard.  
+ */
+enum VIOSRP_MAD_TYPES {
+	VIOSRP_EMPTY_IU_TYPE = 0x01,
+	VIOSRP_ERROR_LOG_TYPE = 0x02,
+	VIOSRP_ADAPTER_INFO_TYPE = 0x03,
+	VIOSRP_HOST_CONFIG_TYPE = 0x04
+};
+
+/* 
+ * Common MAD header
+ */
+struct MAD_COMMON {
+	u32 type;
+	u16 status;
+	u16 length;
+	u64 tag;
+};
+
+/*
+ * All SRP (and MAD) requests normally flow from the
+ * client to the server.  There is no way for the server to send
+ * an asynchronous message back to the client.  The Empty IU is used
+ * to hang out a meaningless request to the server so that it can respond
+ * asynchrouously with something like a SCSI AER 
+ */
+struct VIOSRP_EMPTY_IU {
+	struct MAD_COMMON common;
+	u64 buffer;
+	u32 port;
+};
+
+struct VIOSRP_ERROR_LOG {
+	struct MAD_COMMON common;
+	u64 buffer;
+};
+
+struct VIOSRP_ADAPTER_INFO {
+	struct MAD_COMMON common;
+	u64 buffer;
+};
+
+struct VIOSRP_HOST_CONFIG {
+	struct MAD_COMMON common;
+	u64 buffer;
+};
+
+union MAD_IU {
+	struct VIOSRP_EMPTY_IU empty_iu;
+	struct VIOSRP_ERROR_LOG error_log;
+	struct VIOSRP_ADAPTER_INFO adapter_info;
+	struct VIOSRP_HOST_CONFIG host_config;
+};
+
+union VIOSRP_IU {
+	union SRP_IU srp;
+	union MAD_IU mad;
+};
+
+struct MAD_ADAPTER_INFO_DATA {
+	char srp_version[8];
+	char partition_name[96];
+	u32 partition_number;
+	u32 mad_version;
+	u32 os_type;
+	u32 port_max_txu[8];	/* per-port maximum transfer */
+};
+
+#endif
diff -purN linux-2.5/drivers/scsi/ipr.c linuxppc64-2.5/drivers/scsi/ipr.c
--- linux-2.5/drivers/scsi/ipr.c	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/ipr.c	2004-03-01 19:33:23.000000000 +0000
@@ -0,0 +1,5884 @@
+/*
+ * ipr.c -- driver for IBM Power Linux RAID adapters
+ *
+ * Written By: Brian King, IBM Corporation
+ *
+ * Copyright (C) 2003, 2004 IBM Corporation
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ *
+ */
+
+/*
+ * Notes:
+ *
+ * This driver is used to control the following SCSI adapters:
+ *
+ * IBM iSeries: 5702, 5703, 2780, 5709, 570A, 570B
+ *
+ * IBM pSeries: PCI-X Dual Channel Ultra 320 SCSI RAID Adapter
+ *              PCI-X Dual Channel Ultra 320 SCSI Adapter
+ *              PCI-X Dual Channel Ultra 320 SCSI RAID Enablement Card
+ *              Embedded SCSI adapter on p615 and p655 systems
+ *
+ * Supported Hardware Features:
+ *	- Ultra 320 SCSI controller
+ *	- PCI-X host interface
+ *	- Embedded PowerPC RISC Processor and Hardware XOR DMA Engine
+ *	- Non-Volatile Write Cache
+ *	- Supports attachment of non-RAID disks, tape, and optical devices
+ *	- RAID Levels 0, 5, 10
+ *	- Hot spare
+ *	- Background Parity Checking
+ *	- Background Data Scrubbing
+ *	- Ability to increase the capacity of an existing RAID 5 disk array
+ *		by adding disks
+ *
+ * Driver Features:
+ *	- Tagged command queuing
+ *	- Adapter microcode download
+ *	- PCI hot plug
+ *	- SCSI device hot plug
+ *
+ */
+
+#include <linux/config.h>
+#include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/types.h>
+#include <linux/errno.h>
+#include <linux/kernel.h>
+#include <linux/ioport.h>
+#include <linux/delay.h>
+#include <linux/pci.h>
+#include <linux/wait.h>
+#include <linux/spinlock.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/blkdev.h>
+#include <linux/firmware.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <asm/io.h>
+#include <asm/irq.h>
+#include <asm/processor.h>
+#include <scsi/scsi.h>
+#include <scsi/scsi_host.h>
+#include <scsi/scsi_tcq.h>
+#include <scsi/scsi_eh.h>
+#include <scsi/scsi_cmnd.h>
+#include <scsi/scsi_request.h>
+#include "ipr.h"
+
+/*
+ *   Global Data
+ */
+static struct list_head ipr_ioa_head = LIST_HEAD_INIT(ipr_ioa_head);
+static unsigned int ipr_log_level = IPR_DEFAULT_LOG_LEVEL;
+static unsigned int ipr_max_speed = 1;
+static int ipr_testmode = 0;
+static spinlock_t ipr_driver_lock = SPIN_LOCK_UNLOCKED;
+
+/* This table describes the differences between DMA controller chips */
+static const struct ipr_chip_cfg_t ipr_chip_cfg[] = {
+	{ /* Gemstone */
+		.mailbox = 0x0042C,
+		.cache_line_size = 0x20,
+		{
+			.set_interrupt_mask_reg = 0x0022C,
+			.clr_interrupt_mask_reg = 0x00230,
+			.sense_interrupt_mask_reg = 0x0022C,
+			.clr_interrupt_reg = 0x00228,
+			.sense_interrupt_reg = 0x00224,
+			.ioarrin_reg = 0x00404,
+			.sense_uproc_interrupt_reg = 0x00214,
+			.set_uproc_interrupt_reg = 0x00214,
+			.clr_uproc_interrupt_reg = 0x00218
+		}
+	},
+	{ /* Snipe */
+		.mailbox = 0x0052C,
+		.cache_line_size = 0x20,
+		{
+			.set_interrupt_mask_reg = 0x00288,
+			.clr_interrupt_mask_reg = 0x0028C,
+			.sense_interrupt_mask_reg = 0x00288,
+			.clr_interrupt_reg = 0x00284,
+			.sense_interrupt_reg = 0x00280,
+			.ioarrin_reg = 0x00504,
+			.sense_uproc_interrupt_reg = 0x00290,
+			.set_uproc_interrupt_reg = 0x00290,
+			.clr_uproc_interrupt_reg = 0x00294
+		}
+	},
+};
+
+static int ipr_max_bus_speeds [] = {
+	IPR_80MBs_SCSI_RATE, IPR_U160_SCSI_RATE, IPR_U320_SCSI_RATE
+};
+
+MODULE_AUTHOR("Brian King <brking@us.ibm.com>");
+MODULE_DESCRIPTION("IBM Power RAID SCSI Adapter Driver");
+module_param_named(max_speed, ipr_max_speed, uint, 0);
+MODULE_PARM_DESC(max_speed, "Maximum bus speed (0-2). Default: 1=U160. Speeds: 0=80 MB/s, 1=U160, 2=U320");
+module_param_named(log_level, ipr_log_level, uint, 0);
+MODULE_PARM_DESC(log_level, "Set to 0 - 4 for increasing verbosity of device driver");
+module_param_named(testmode, ipr_testmode, int, 0);
+MODULE_PARM_DESC(testmode, "DANGEROUS!!! Allows unsupported configurations");
+MODULE_LICENSE("GPL");
+
+static const char ipr_version[] = {"ipr version=" IPR_DRIVER_VERSION};
+
+static const char *ipr_gpdd_dev_end_states[] = {
+	"Command complete",
+	"Terminated by host",
+	"Terminated by device reset",
+	"Terminated by bus reset",
+	"Unknown",
+	"Command not started"
+};
+
+static const char *ipr_gpdd_dev_bus_phases[] = {
+	"Bus free",
+	"Arbitration",
+	"Selection",
+	"Message out",
+	"Command",
+	"Message in",
+	"Data out",
+	"Data in",
+	"Status",
+	"Reselection",
+	"Unknown"
+};
+
+/*  A constant array of IOASCs/URCs/Error Messages */
+static const
+struct ipr_error_table_t ipr_error_table[] = {
+	{0x00000000, 1, 1,
+	"8155: An unknown error was received"},
+	{0x00330000, 0, 0,
+	"Soft underlength error"},
+	{0x005A0000, 0, 0,
+	"Command to be cancelled not found"},
+	{0x00808000, 0, 0,
+	"Qualified success"},
+	{0x01080000, 1, 1,
+	"FFFE: Soft device bus error recovered by the IOA"},
+	{0x01170600, 0, 1,
+	"FFF9: Device sector reassign successful"},
+	{0x01170900, 0, 1,
+	"FFF7: Media error recovered by device rewrite procedures"},
+	{0x01180200, 0, 1,
+	"7001: IOA sector reassignment successful"},
+	{0x01180500, 0, 1,
+	"FFF9: Soft media error. Sector reassignment recommended"},
+	{0x01180600, 0, 1,
+	"FFF7: Media error recovered by IOA rewrite procedures"},
+	{0x01418000, 0, 1,
+	"FF3D: Soft PCI bus error recovered by the IOA"},
+	{0x01440000, 1, 1,
+	"FFF6: Device hardware error recovered by the IOA"},
+	{0x01448100, 0, 1,
+	"FFF6: Device hardware error recovered by the device"},
+	{0x01448200, 1, 1,
+	"FF3D: Soft IOA error recovered by the IOA"},
+	{0x01448300, 0, 1,
+	"FFFA: Undefined device response recovered by the IOA"},
+	{0x014A0000, 1, 1,
+	"FFF6: Device bus error, message or command phase"},
+	{0x015D0000, 0, 1,
+	"FFF6: Failure prediction threshold exceeded"},
+	{0x015D9200, 0, 1,
+	"8009: Impending cache battery pack failure"},
+	{0x02040400, 0, 0,
+	"34FF: Disk device format in progress"},
+	{0x023F0000, 0, 0,
+	"Synchronization required"},
+	{0x024E0000, 0, 0,
+	"No ready, IOA shutdown"},
+	{0x02670100, 0, 1,
+	"3020: Storage subsystem configuration error"},
+	{0x03110B00, 0, 0,
+	"FFF5: Medium error, data unreadable, recommend reassign"},
+	{0x03110C00, 0, 0,
+	"7000: Medium error, data unreadable, do not reassign"},
+	{0x03310000, 0, 1,
+	"FFF3: Disk media format bad"},
+	{0x04050000, 0, 1,
+	"3002: Addressed device failed to respond to selection"},
+	{0x04080000, 1, 1,
+	"3100: Device bus error"},
+	{0x04080100, 0, 1,
+	"3109: IOA timed out a device command"},
+	{0x04088000, 0, 0,
+	"3120: SCSI bus is not operational"},
+	{0x04118000, 0, 1,
+	"9000: IOA reserved area data check"},
+	{0x04118100, 0, 1,
+	"9001: IOA reserved area invalid data pattern"},
+	{0x04118200, 0, 1,
+	"9002: IOA reserved area LRC error"},
+	{0x04320000, 0, 1,
+	"102E: Out of alternate sectors for disk storage"},
+	{0x04330000, 1, 1,
+	"FFF4: Data transfer underlength error"},
+	{0x04338000, 1, 1,
+	"FFF4: Data transfer overlength error"},
+	{0x043E0100, 0, 1,
+	"3400: Logical unit failure"},
+	{0x04408500, 0, 1,
+	"FFF4: Device microcode is corrupt"},
+	{0x04418000, 1, 1,
+	"8150: PCI bus error"},
+	{0x04430000, 1, 0,
+	"Unsupported device bus message received"},
+	{0x04440000, 1, 1,
+	"FFF4: Disk device problem"},
+	{0x04448200, 1, 1,
+	"8150: Permanent IOA failure"},
+	{0x04448300, 0, 1,
+	"3010: Disk device returned wrong response to IOA"},
+	{0x04448400, 0, 1,
+	"8151: IOA microcode error"},
+	{0x04448500, 0, 0,
+	"Device bus status error"},
+	{0x04448600, 0, 1,
+	"8157: IOA error requiring IOA reset to recover"},
+	{0x04490000, 0, 0,
+	"Message reject received from the device"},
+	{0x04449200, 0, 1,
+	"8008: A permanent cache battery pack failure occurred"},
+	{0x0444A000, 0, 1,
+	"9090: Disk unit has been modified after the last known status"},
+	{0x0444A200, 0, 1,
+	"9081: IOA detected device error"},
+	{0x0444A300, 0, 1,
+	"9082: IOA detected device error"},
+	{0x044A0000, 1, 1,
+	"3110: Device bus error, message or command phase"},
+	{0x04670400, 0, 1,
+	"9091: Incorrect hardware configuration change has been detected"},
+	{0x046E0000, 0, 1,
+	"FFF4: Command to logical unit failed"},
+	{0x05240000, 1, 0,
+	"Illegal request, invalid request type or request packet"},
+	{0x05250000, 0, 0,
+	"Illegal request, invalid resource handle"},
+	{0x05260000, 0, 0,
+	"Illegal request, invalid field in parameter list"},
+	{0x05260100, 0, 0,
+	"Illegal request, parameter not supported"},
+	{0x05260200, 0, 0,
+	"Illegal request, parameter value invalid"},
+	{0x052C0000, 0, 0,
+	"Illegal request, command sequence error"},
+	{0x06040500, 0, 1,
+	"9031: Array protection temporarily suspended, protection resuming"},
+	{0x06040600, 0, 1,
+	"9040: Array protection temporarily suspended, protection resuming"},
+	{0x06290000, 0, 1,
+	"FFFB: SCSI bus was reset"},
+	{0x06290500, 0, 0,
+	"FFFE: SCSI bus transition to single ended"},
+	{0x06290600, 0, 0,
+	"FFFE: SCSI bus transition to LVD"},
+	{0x06298000, 0, 1,
+	"FFFB: SCSI bus was reset by another initiator"},
+	{0x063F0300, 0, 1,
+	"3029: A device replacement has occurred"},
+	{0x064C8000, 0, 1,
+	"9051: IOA cache data exists for a missing or failed device"},
+	{0x06670100, 0, 1,
+	"9025: Disk unit is not supported at its physical location"},
+	{0x06670600, 0, 1,
+	"3020: IOA detected a SCSI bus configuration error"},
+	{0x06678000, 0, 1,
+	"3150: SCSI bus configuration error"},
+	{0x06690200, 0, 1,
+	"9041: Array protection temporarily suspended"},
+	{0x066B0200, 0, 1,
+	"9030: Array no longer protected due to missing or failed disk unit"},
+	{0x07270000, 0, 0,
+	"Failure due to other device"},
+	{0x07278000, 0, 1,
+	"9008: IOA does not support functions expected by devices"},
+	{0x07278100, 0, 1,
+	"9010: Cache data associated with attached devices cannot be found"},
+	{0x07278200, 0, 1,
+	"9011: Cache data belongs to devices other than those attached"},
+	{0x07278400, 0, 1,
+	"9020: Array missing 2 or more devices with only 1 device present"},
+	{0x07278500, 0, 1,
+	"9021: Array missing 2 or more devices with 2 or more devices present"},
+	{0x07278600, 0, 1,
+	"9022: Exposed array is missing a required device"},
+	{0x07278700, 0, 1,
+	"9023: Array member(s) not at required physical locations"},
+	{0x07278800, 0, 1,
+	"9024: Array not functional due to present hardware configuration"},
+	{0x07278900, 0, 1,
+	"9026: Array not functional due to present hardware configuration"},
+	{0x07278A00, 0, 1,
+	"9027: Array is missing a device and parity is out of sync"},
+	{0x07278B00, 0, 1,
+	"9028: Maximum number of arrays already exist"},
+	{0x07278C00, 0, 1,
+	"9050: Required cache data cannot be located for a disk unit"},
+	{0x07278D00, 0, 1,
+	"9052: Cache data exists for a device that has been modified"},
+	{0x07278F00, 0, 1,
+	"9054: IOA resources not available due to previous problems"},
+	{0x07279100, 0, 1,
+	"9092: Disk unit requires initialization before use"},
+	{0x07279200, 0, 1,
+	"9029: Incorrect hardware configuration change has been detected"},
+	{0x07279600, 0, 1,
+	"9060: One or more disk pairs are missing from an array"},
+	{0x07279700, 0, 1,
+	"9061: One or more disks are missing from an array"},
+	{0x07279800, 0, 1,
+	"9062: One or more disks are missing from an array"},
+	{0x07279900, 0, 1,
+	"9063: Maximum number of functional arrays has been exceeded"},
+	{0x0B260000, 0, 0,
+	"Aborted command, invalid descriptor."},
+	{0x0B5A0000, 0, 0,
+	"Command terminated by host"}
+};
+
+static const struct ipr_ses_table_entry ipr_ses_table[] = {
+	{ "2104-DL1        ", "XXXXXXXXXXXXXXXX", 80 },
+	{ "2104-TL1        ", "XXXXXXXXXXXXXXXX", 80 },
+	{ "HSBP07M P U2SCSI", "XXXXXXXXXXXXXXXX", 80 }, /* Hidive 7 slot */
+	{ "HSBP05M P U2SCSI", "XXXXXXXXXXXXXXXX", 80 }, /* Hidive 5 slot */
+	{ "HSBP05M S U2SCSI", "XXXXXXXXXXXXXXXX", 80 }, /* Bowtie */
+	{ "HSBP06E ASU2SCSI", "XXXXXXXXXXXXXXXX", 80 }, /* MartinFenning */
+	{ "2104-DU3        ", "XXXXXXXXXXXXXXXX", 160 },
+	{ "2104-TU3        ", "XXXXXXXXXXXXXXXX", 160 },
+	{ "HSBP04C RSU2SCSI", "XXXXXXX*XXXXXXXX", 160 },
+	{ "HSBP06E RSU2SCSI", "XXXXXXX*XXXXXXXX", 160 },
+	{ "St  V1S2        ", "XXXXXXXXXXXXXXXX", 160 },
+	{ "HSBPD4M  PU3SCSI", "XXXXXXX*XXXXXXXX", 160 },
+	{ "VSBPD1H   U3SCSI", "XXXXXXX*XXXXXXXX", 160 }
+};
+
+/*
+ *  Function Prototypes
+ */
+static int ipr_reset_alert(struct ipr_cmnd *);
+static void ipr_process_ccn(struct ipr_cmnd *);
+static void ipr_process_error(struct ipr_cmnd *);
+static void ipr_reset_ioa_job(struct ipr_cmnd *);
+static void ipr_initiate_ioa_reset(struct ipr_ioa_cfg *,
+				   enum ipr_shutdown_type);
+
+#ifdef CONFIG_SCSI_IPR_TRACE
+/**
+ * ipr_trc_hook - Add a trace entry to the driver trace
+ * @ipr_cmd:	ipr command struct
+ * @type:		trace type
+ * @add_data:	additional data
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_trc_hook(struct ipr_cmnd *ipr_cmd,
+			 u8 type, u32 add_data)
+{
+	struct ipr_trace_entry *trace_entry;
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+
+	trace_entry = &ioa_cfg->trace[ioa_cfg->trace_index++];
+	trace_entry->time = jiffies;
+	trace_entry->op_code = ipr_cmd->ioarcb.cmd_pkt.cdb[0];
+	trace_entry->type = type;
+	trace_entry->cmd_index = ipr_cmd->cmd_index;
+	trace_entry->res_handle = ipr_cmd->ioarcb.res_handle;
+	trace_entry->add_data = add_data;
+}
+#else
+#define ipr_trc_hook(ipr_cmd, type, add_data) do { } while(0)
+#endif
+
+/**
+ * ipr_reinit_ipr_cmnd - Re-initialize an IPR Cmnd block for reuse
+ * @ipr_cmd:	ipr command struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_reinit_ipr_cmnd(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;
+	struct ipr_ioasa *ioasa = &ipr_cmd->ioasa;
+
+	memset(&ioarcb->cmd_pkt, 0, sizeof(struct ipr_cmd_pkt));
+	ioarcb->write_data_transfer_length = 0;
+	ioarcb->read_data_transfer_length = 0;
+	ioarcb->write_ioadl_len = 0;
+	ioarcb->read_ioadl_len = 0;
+	ioasa->ioasc = 0;
+	ioasa->residual_data_len = 0;
+
+	ipr_cmd->scsi_cmd = NULL;
+	ipr_cmd->sense_buffer[0] = 0;
+	ipr_cmd->dma_use_sg = 0;
+}
+
+/**
+ * ipr_init_ipr_cmnd - Initialize an IPR Cmnd block
+ * @ipr_cmd:	ipr command struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_init_ipr_cmnd(struct ipr_cmnd *ipr_cmd)
+{
+	ipr_reinit_ipr_cmnd(ipr_cmd);
+	init_timer(&ipr_cmd->timer);
+}
+
+/**
+ * ipr_get_free_ipr_cmnd - Get a free IPR Cmnd block
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ * 	pointer to ipr command struct
+ **/
+static
+struct ipr_cmnd *ipr_get_free_ipr_cmnd(struct ipr_ioa_cfg *ioa_cfg)
+{
+	struct ipr_cmnd *ipr_cmd;
+
+	ipr_cmd = list_entry(ioa_cfg->free_q.next, struct ipr_cmnd, queue);
+	list_del(&ipr_cmd->queue);
+	ipr_init_ipr_cmnd(ipr_cmd);
+
+	return ipr_cmd;
+}
+
+/**
+ * ipr_unmap_sglist - Unmap scatterlist if mapped
+ * @ioa_cfg:	ioa config struct
+ * @ipr_cmd:	ipr command struct
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_unmap_sglist(struct ipr_ioa_cfg *ioa_cfg,
+			     struct ipr_cmnd *ipr_cmd)
+{
+	struct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;
+
+	if (ipr_cmd->dma_use_sg) {
+		if (scsi_cmd->use_sg > 0) {
+			pci_unmap_sg(ioa_cfg->pdev, scsi_cmd->request_buffer,
+				     scsi_cmd->use_sg,
+				     scsi_cmd->sc_data_direction);
+		} else {
+			pci_unmap_single(ioa_cfg->pdev, ipr_cmd->dma_handle,
+					 scsi_cmd->request_bufflen,
+					 scsi_cmd->sc_data_direction);
+		}
+	}
+}
+
+/**
+ * ipr_mask_and_clear_all_interrupts - Mask and clears all interrupts on the adapter
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_mask_and_clear_all_interrupts(struct ipr_ioa_cfg *ioa_cfg)
+{
+	volatile u32 int_reg;
+
+	/* Stop new interrupts */
+	ioa_cfg->allow_interrupts = 0;
+
+	/* Set interrupt mask to stop all new interrupts */
+	writel(~0, ioa_cfg->regs.set_interrupt_mask_reg);
+
+	/* Clear any pending interrupts */
+	writel(~0, ioa_cfg->regs.clr_interrupt_reg);
+	int_reg = readl(ioa_cfg->regs.sense_interrupt_reg);
+}
+
+/**
+ * ipr_save_pcix_cmd_reg - Save PCI-X command register
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ * 	0 on success / -EIO on failure
+ **/
+static int ipr_save_pcix_cmd_reg(struct ipr_ioa_cfg *ioa_cfg)
+{
+	int pcix_cmd_reg = pci_find_capability(ioa_cfg->pdev, PCI_CAP_ID_PCIX);
+
+	if (pcix_cmd_reg == 0) {
+		dev_err(&ioa_cfg->pdev->dev, "Failed to save PCI-X command register\n");
+		return -EIO;
+	}
+
+	if (pci_read_config_word(ioa_cfg->pdev, pcix_cmd_reg,
+				 &ioa_cfg->saved_pcix_cmd_reg) != PCIBIOS_SUCCESSFUL) {
+		dev_err(&ioa_cfg->pdev->dev, "Failed to save PCI-X command register\n");
+		return -EIO;
+	}
+
+	ioa_cfg->saved_pcix_cmd_reg |= PCI_X_CMD_DPERR_E | PCI_X_CMD_ERO;
+	return 0;
+}
+
+/**
+ * ipr_set_pcix_cmd_reg - Setup PCI-X command register
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ * 	0 on success / -EIO on failure
+ **/
+static int ipr_set_pcix_cmd_reg(struct ipr_ioa_cfg *ioa_cfg)
+{
+	int pcix_cmd_reg = pci_find_capability(ioa_cfg->pdev, PCI_CAP_ID_PCIX);
+
+	if (pcix_cmd_reg) {
+		if (pci_write_config_word(ioa_cfg->pdev, pcix_cmd_reg,
+					  ioa_cfg->saved_pcix_cmd_reg) != PCIBIOS_SUCCESSFUL) {
+			dev_err(&ioa_cfg->pdev->dev, "Failed to setup PCI-X command register\n");
+			return -EIO;
+		}
+	} else {
+		dev_err(&ioa_cfg->pdev->dev,
+			"Failed to setup PCI-X command register\n");
+		return -EIO;
+	}
+
+	return 0;
+}
+
+/**
+ * ipr_scsi_eh_done - mid-layer done function for aborted ops
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function is invoked by the interrupt handler for
+ * ops generated by the SCSI mid-layer which are being aborted.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_scsi_eh_done(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;
+
+	scsi_cmd->result |= (DID_ERROR << 16);
+
+	ipr_unmap_sglist(ioa_cfg, ipr_cmd);
+	scsi_cmd->scsi_done(scsi_cmd);
+	list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+}
+
+/**
+ * ipr_fail_all_ops - Fails all outstanding ops.
+ * @ioa_cfg:	ioa config struct
+ *
+ * This function fails all outstanding ops.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_fail_all_ops(struct ipr_ioa_cfg *ioa_cfg)
+{
+	struct ipr_cmnd *ipr_cmd, *temp;
+
+	ENTER;
+	list_for_each_entry_safe(ipr_cmd, temp, &ioa_cfg->pending_q, queue) {
+		list_del(&ipr_cmd->queue);
+
+		ipr_cmd->ioasa.ioasc = cpu_to_be32(IPR_IOASC_IOA_WAS_RESET);
+		ipr_cmd->ioasa.ilid = cpu_to_be32(IPR_DRIVER_ILID);
+
+		if (ipr_cmd->scsi_cmd)
+			ipr_cmd->done = ipr_scsi_eh_done;
+
+		ipr_trc_hook(ipr_cmd, IPR_TRACE_FINISH, IPR_IOASC_IOA_WAS_RESET);
+		del_timer(&ipr_cmd->timer);
+		ipr_cmd->done(ipr_cmd);
+	}
+
+	LEAVE;
+}
+
+/**
+ * ipr_do_req -  Send driver initiated requests.
+ * @ipr_cmd:		ipr command struct
+ * @done:			done function
+ * @timeout_func:	timeout function
+ * @timeout:		timeout value
+ *
+ * This function sends the specified command to the adapter with the
+ * timeout given. The done function is invoked on command completion.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_do_req(struct ipr_cmnd *ipr_cmd,
+		       void (*done) (struct ipr_cmnd *),
+		       void (*timeout_func) (struct ipr_cmnd *), u32 timeout)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+
+	list_add_tail(&ipr_cmd->queue, &ioa_cfg->pending_q);
+
+	ipr_cmd->done = done;
+
+	ipr_cmd->timer.data = (unsigned long) ipr_cmd;
+	ipr_cmd->timer.expires = jiffies + timeout;
+	ipr_cmd->timer.function = (void (*)(unsigned long))timeout_func;
+
+	add_timer(&ipr_cmd->timer);
+
+	ipr_trc_hook(ipr_cmd, IPR_TRACE_START, 0);
+
+	mb();
+	writel(be32_to_cpu(ipr_cmd->ioarcb.ioarcb_host_pci_addr),
+	       ioa_cfg->regs.ioarrin_reg);
+}
+
+/**
+ * ipr_internal_cmd_done - Op done function for an internally generated op.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function is the op done function for an internally generated,
+ * blocking op. It simply wakes the sleeping thread.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_internal_cmd_done(struct ipr_cmnd *ipr_cmd)
+{
+	complete(&ipr_cmd->completion);
+}
+
+/**
+ * ipr_send_blocking_cmd - Send command and sleep on its completion.
+ * @ipr_cmd:	ipr command struct
+ * @timeout_func:	function to invoke if command times out
+ * @timeout:	timeout
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_send_blocking_cmd(struct ipr_cmnd *ipr_cmd,
+				  void (*timeout_func) (struct ipr_cmnd *ipr_cmd),
+				  u32 timeout)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+
+	init_completion(&ipr_cmd->completion);
+	ipr_do_req(ipr_cmd, ipr_internal_cmd_done, timeout_func, timeout);
+
+	spin_unlock_irq(ioa_cfg->host->host_lock);
+	wait_for_completion(&ipr_cmd->completion);
+	spin_lock_irq(ioa_cfg->host->host_lock);
+}
+
+/**
+ * ipr_send_hcam - Send an HCAM to the adapter.
+ * @ioa_cfg:	ioa config struct
+ * @type:		HCAM type
+ * @hostrcb:	hostrcb struct
+ *
+ * This function will send a Host Controlled Async command to the adapter.
+ * If HCAMs are currently not allowed to be issued to the adapter, it will
+ * place the hostrcb on the free queue.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_send_hcam(struct ipr_ioa_cfg *ioa_cfg, u8 type,
+			  struct ipr_hostrcb *hostrcb)
+{
+	struct ipr_cmnd *ipr_cmd;
+	struct ipr_ioarcb *ioarcb;
+
+	if (ioa_cfg->allow_cmds) {
+		ipr_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);
+		list_add_tail(&ipr_cmd->queue, &ioa_cfg->pending_q);
+		list_add_tail(&hostrcb->queue, &ioa_cfg->hostrcb_pending_q);
+
+		ipr_cmd->hostrcb = hostrcb;
+		ioarcb = &ipr_cmd->ioarcb;
+
+		ioarcb->res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);
+		ioarcb->cmd_pkt.request_type = IPR_RQTYPE_HCAM;
+		ioarcb->cmd_pkt.cdb[0] = IPR_HOST_CONTROLLED_ASYNC;
+		ioarcb->cmd_pkt.cdb[1] = type;
+		ioarcb->cmd_pkt.cdb[7] = (sizeof(struct ipr_hostrcb) >> 8) & 0xff;
+		ioarcb->cmd_pkt.cdb[8] = sizeof(struct ipr_hostrcb) & 0xff;
+
+		ioarcb->read_data_transfer_length =
+			cpu_to_be32(sizeof(struct ipr_hostrcb));
+		ioarcb->read_ioadl_len =
+			cpu_to_be32(sizeof(struct ipr_ioadl_desc));
+		ipr_cmd->ioadl[0].flags_and_data_len =
+			cpu_to_be32(IPR_IOADL_FLAGS_READ_LAST | sizeof(struct ipr_hostrcb));
+		ipr_cmd->ioadl[0].address = cpu_to_be32(hostrcb->hostrcb_dma);
+
+		if (type == IPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE)
+			ipr_cmd->done = ipr_process_ccn;
+		else
+			ipr_cmd->done = ipr_process_error;
+
+		mb();
+		writel(be32_to_cpu(ipr_cmd->ioarcb.ioarcb_host_pci_addr),
+		       ioa_cfg->regs.ioarrin_reg);
+	} else {
+		list_add_tail(&hostrcb->queue, &ioa_cfg->hostrcb_free_q);
+	}
+}
+
+/**
+ * ipr_init_res_entry - Initialize a resource entry struct.
+ * @res:	resource entry struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_init_res_entry(struct ipr_resource_entry *res)
+{
+	res->needs_sync_complete = 1;
+	res->in_erp = 0;
+	res->add_to_ml = 0;
+	res->del_from_ml = 0;
+	res->resetting_device = 0;
+	res->tcq_active = 0;
+	res->qdepth = IPR_MAX_CMD_PER_LUN;
+	res->sdev = NULL;
+}
+
+/**
+ * ipr_handle_config_change - Handle a config change from the adapter
+ * @ioa_cfg:	ioa config struct
+ * @hostrcb:	hostrcb
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_handle_config_change(struct ipr_ioa_cfg *ioa_cfg,
+			      struct ipr_hostrcb *hostrcb)
+{
+	struct ipr_resource_entry *res = NULL;
+	struct ipr_config_table_entry *cfgte;
+	u32 is_ndn = 1;
+
+	cfgte = &hostrcb->ccn.cfgte;
+
+	list_for_each_entry(res, &ioa_cfg->used_res_q, queue) {
+		if (res->cfgte.res_handle == cfgte->res_handle) {
+			is_ndn = 0;
+			break;
+		}
+	}
+
+	if (is_ndn) {
+		if (list_empty(&ioa_cfg->free_res_q)) {
+			ipr_send_hcam(ioa_cfg,
+				      IPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE,
+				      hostrcb);
+			return;
+		}
+
+		res = list_entry(ioa_cfg->free_res_q.next,
+				 struct ipr_resource_entry, queue);
+
+		list_del(&res->queue);
+		ipr_init_res_entry(res);
+		res->add_to_ml = 1;
+		list_add_tail(&res->queue, &ioa_cfg->used_res_q);
+	}
+
+	memcpy(&res->cfgte, cfgte, sizeof(struct ipr_config_table_entry));
+
+	if (hostrcb->notify_type == IPR_HOST_RCB_NOTIF_TYPE_REM_ENTRY) {
+		if (res->sdev) {
+			res->del_from_ml = 1;
+			schedule_work(&ioa_cfg->work_q);
+		} else
+			list_move_tail(&res->queue, &ioa_cfg->free_res_q);
+	} else if (res->add_to_ml)
+		schedule_work(&ioa_cfg->work_q);
+
+	ipr_send_hcam(ioa_cfg, IPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE, hostrcb);
+}
+
+/**
+ * ipr_process_ccn - Op done function for a CCN.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function is the op done function for a configuration
+ * change notification host controlled async from the adapter.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_process_ccn(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_hostrcb *hostrcb = ipr_cmd->hostrcb;
+	u32 ioasc = be32_to_cpu(ipr_cmd->ioasa.ioasc);
+
+	list_del(&hostrcb->queue);
+
+	if (ioasc) {
+		if (ioasc != IPR_IOASC_IOA_WAS_RESET) {
+			dev_err(&ioa_cfg->pdev->dev,
+				"Host RCB failed with IOASC: 0x%08X\n", ioasc);
+		}
+
+		ipr_send_hcam(ioa_cfg, IPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE, hostrcb);
+	} else {
+		ipr_handle_config_change(ioa_cfg, hostrcb);
+	}
+}
+
+/**
+ * ipr_log_vpd - Log the passed VPD to the error log.
+ * @vpids:			vendor/product id struct
+ * @serial_num:		serial number string
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_log_vpd(struct ipr_std_inq_vpids *vpids, u8 *serial_num)
+{
+	char buffer[max_t(int, sizeof(struct ipr_std_inq_vpids),
+			  IPR_SERIAL_NUM_LEN) + 1];
+
+	memcpy(buffer, vpids, sizeof(struct ipr_std_inq_vpids));
+	buffer[sizeof(struct ipr_std_inq_vpids)] = '\0';
+	ipr_err("Vendor/Product ID: %s\n", buffer);
+
+	memcpy(buffer, serial_num, IPR_SERIAL_NUM_LEN);
+	buffer[IPR_SERIAL_NUM_LEN] = '\0';
+	ipr_err("    Serial Number: %s\n", buffer);
+}
+
+/**
+ * ipr_log_cache_error - Log a cache error.
+ * @ioa_cfg:	ioa config struct
+ * @hostrcb:	hostrcb struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_log_cache_error(struct ipr_ioa_cfg *ioa_cfg,
+				struct ipr_hostrcb *hostrcb)
+{
+	struct ipr_hostrcb_type_02_error *error = &hostrcb->error.type_02_error;
+
+	ipr_err("-----Current Configuration-----\n");
+	ipr_err("I/O Processor Information:\n");
+	ipr_log_vpd(&error->ioa_vpids, error->ioa_sn);
+	ipr_err("Cache Adapter Card Information:\n");
+	ipr_log_vpd(&error->cfc_vpids, error->cfc_sn);
+
+	ipr_err("-----Expected Configuration-----\n");
+	ipr_err("I/O Processor Information:\n");
+	ipr_log_vpd(&error->ioa_last_attached_to_cfc_vpids,
+		    error->ioa_last_attached_to_cfc_sn);
+	ipr_err("Cache Adapter Card Information:\n");
+	ipr_log_vpd(&error->cfc_last_attached_to_ioa_vpids,
+		    error->cfc_last_attached_to_ioa_sn);
+
+	ipr_err("Additional IOA Data: %08X %08X %08X\n",
+		     be32_to_cpu(error->ioa_data[0]),
+		     be32_to_cpu(error->ioa_data[1]),
+		     be32_to_cpu(error->ioa_data[2]));
+}
+
+/**
+ * ipr_log_config_error - Log a configuration error.
+ * @ioa_cfg:	ioa config struct
+ * @hostrcb:	hostrcb struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_log_config_error(struct ipr_ioa_cfg *ioa_cfg,
+				 struct ipr_hostrcb *hostrcb)
+{
+	int errors_logged, i;
+	struct ipr_hostrcb_device_data_entry *dev_entry;
+
+	errors_logged = be32_to_cpu(hostrcb->error.type_03_error.errors_logged);
+
+	ipr_err("Device Errors Detected/Logged: %d/%d\n",
+		be32_to_cpu(hostrcb->error.type_03_error.errors_detected),
+		errors_logged);
+
+	dev_entry = hostrcb->error.type_03_error.dev_entry;
+
+	for (i = 0; i < errors_logged; i++, dev_entry++) {
+		ipr_err_separator;
+
+		if (dev_entry->dev_res_addr.bus >= IPR_MAX_NUM_BUSES) {
+			ipr_err("Device %d: missing\n", i + 1);
+		} else {
+			ipr_err("Device %d: %d:%d:%d:%d\n", i + 1,
+				ioa_cfg->host->host_no, dev_entry->dev_res_addr.bus,
+				dev_entry->dev_res_addr.target, dev_entry->dev_res_addr.lun);
+		}
+		ipr_log_vpd(&dev_entry->dev_vpids, dev_entry->dev_sn);
+
+		ipr_err("-----New Device Information-----\n");
+		ipr_log_vpd(&dev_entry->new_dev_vpids, dev_entry->new_dev_sn);
+
+		ipr_err("I/O Processor Information:\n");
+		ipr_log_vpd(&dev_entry->ioa_last_with_dev_vpids,
+			    dev_entry->ioa_last_with_dev_sn);
+
+		ipr_err("Cache Adapter Card Information:\n");
+		ipr_log_vpd(&dev_entry->cfc_last_with_dev_vpids,
+			    dev_entry->cfc_last_with_dev_sn);
+
+		ipr_err("Additional IOA Data: %08X %08X %08X %08X %08X\n",
+			be32_to_cpu(dev_entry->ioa_data[0]),
+			be32_to_cpu(dev_entry->ioa_data[1]),
+			be32_to_cpu(dev_entry->ioa_data[2]),
+			be32_to_cpu(dev_entry->ioa_data[3]),
+			be32_to_cpu(dev_entry->ioa_data[4]));
+	}
+}
+
+/**
+ * ipr_log_array_error - Log an array configuration error.
+ * @ioa_cfg:	ioa config struct
+ * @hostrcb:	hostrcb struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_log_array_error(struct ipr_ioa_cfg *ioa_cfg,
+				struct ipr_hostrcb *hostrcb)
+{
+	int i;
+	struct ipr_hostrcb_type_04_error *error;
+	struct ipr_hostrcb_array_data_entry *array_entry;
+	u8 zero_sn[IPR_SERIAL_NUM_LEN];
+
+	memset(zero_sn, '0', IPR_SERIAL_NUM_LEN);
+
+	error = &hostrcb->error.type_04_error;
+
+	ipr_err_separator;
+
+	ipr_err("RAID %s Array Configuration: %d:%d:%d:%d\n",
+		error->protection_level,
+		ioa_cfg->host->host_no,
+		error->last_func_vset_res_addr.bus,
+		error->last_func_vset_res_addr.target,
+		error->last_func_vset_res_addr.lun);
+
+	ipr_err_separator;
+
+	array_entry = error->array_member;
+
+	for (i = 0; i < 18; i++) {
+		if (!memcmp(array_entry->serial_num, zero_sn, IPR_SERIAL_NUM_LEN))
+			continue;
+
+		if (error->exposed_mode_adn == i) {
+			ipr_err("Exposed Array Member %d:\n", i);
+		} else {
+			ipr_err("Array Member %d:\n", i);
+		}
+
+		ipr_log_vpd(&array_entry->vpids, array_entry->serial_num);
+
+		if (array_entry->dev_res_addr.bus >= IPR_MAX_NUM_BUSES) {
+			ipr_err("Current Location: unknown\n");
+		} else {
+			ipr_err("Current Location: %d:%d:%d:%d\n",
+				ioa_cfg->host->host_no,
+				array_entry->dev_res_addr.bus,
+				array_entry->dev_res_addr.target,
+				array_entry->dev_res_addr.lun);
+		}
+
+		if (array_entry->dev_res_addr.bus >= IPR_MAX_NUM_BUSES) {
+			ipr_err("Expected Location: unknown\n");
+		} else {
+			ipr_err("Expected Location: %d:%d:%d:%d\n",
+				ioa_cfg->host->host_no,
+				array_entry->expected_dev_res_addr.bus,
+				array_entry->expected_dev_res_addr.target,
+				array_entry->expected_dev_res_addr.lun);
+		}
+
+		ipr_err_separator;
+
+		if (i == 9)
+			array_entry = error->array_member2;
+		else
+			array_entry++;
+	}
+}
+
+/**
+ * ipr_log_generic_error - Log an adapter error.
+ * @ioa_cfg:	ioa config struct
+ * @hostrcb:	hostrcb struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_log_generic_error(struct ipr_ioa_cfg *ioa_cfg,
+				  struct ipr_hostrcb *hostrcb)
+{
+	int i;
+	int ioa_data_len = be32_to_cpu(hostrcb->length);
+
+	if (ioa_data_len == 0)
+		return;
+
+	ipr_err("IOA Error Data:\n");
+	ipr_err("Offset    0 1 2 3  4 5 6 7  8 9 A B  C D E F\n");
+
+	for (i = 0; i < ioa_data_len / 4; i += 4) {
+		ipr_err("%08X: %08X %08X %08X %08X\n", i*4,
+			be32_to_cpu(hostrcb->raw.data[i]),
+			be32_to_cpu(hostrcb->raw.data[i+1]),
+			be32_to_cpu(hostrcb->raw.data[i+2]),
+			be32_to_cpu(hostrcb->raw.data[i+3]));
+	}
+}
+
+/**
+ * ipr_get_error - Find the specfied IOASC in the ipr_error_table.
+ * @ioasc:	IOASC
+ *
+ * This function will return the index of into the ipr_error_table
+ * for the specified IOASC. If the IOASC is not in the table,
+ * 0 will be returned, which points to the entry used for unknown errors.
+ *
+ * Return value:
+ * 	index into the ipr_error_table
+ **/
+static u32 ipr_get_error(u32 ioasc)
+{
+	int i;
+
+	for (i = 0; i < ARRAY_SIZE(ipr_error_table); i++)
+		if (ipr_error_table[i].ioasc == ioasc)
+			return i;
+
+	return 0;
+}
+
+/**
+ * ipr_handle_log_data - Log an adapter error.
+ * @ioa_cfg:	ioa config struct
+ * @hostrcb:	hostrcb struct
+ *
+ * This function logs an adapter error to the system.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_handle_log_data(struct ipr_ioa_cfg *ioa_cfg,
+				struct ipr_hostrcb *hostrcb)
+{
+	u32 ioasc;
+	int error_index;
+
+	if (hostrcb->notify_type != IPR_HOST_RCB_NOTIF_TYPE_ERROR_LOG_ENTRY)
+		return;
+
+	if (hostrcb->notifications_lost == IPR_HOST_RCB_NOTIFICATIONS_LOST)
+		dev_err(&ioa_cfg->pdev->dev, "Error notifications lost\n");
+
+	ioasc = be32_to_cpu(hostrcb->error.failing_dev_ioasc);
+
+	if ((ioasc == IPR_IOASC_BUS_WAS_RESET) ||
+	    (ioasc == IPR_IOASC_BUS_WAS_RESET_BY_OTHER)) {
+		/* Tell the midlayer we had a bus reset so it will handle the UA properly */
+		scsi_report_bus_reset(ioa_cfg->host,
+				      hostrcb->error.failing_dev_res_addr.bus);
+	}
+
+	error_index = ipr_get_error(ioasc);
+
+	if (!ipr_error_table[error_index].log_hcam)
+		return;
+
+	if (ipr_is_device(&hostrcb->error.failing_dev_res_addr)) {
+		ipr_res_err(ioa_cfg, hostrcb->error.failing_dev_res_addr,
+			    "%s\n", ipr_error_table[error_index].error);
+	} else {
+		dev_err(&ioa_cfg->pdev->dev, "%s\n",
+			ipr_error_table[error_index].error);
+	}
+
+	/* Set indication we have logged an error */
+	ioa_cfg->errors_logged++;
+
+	switch (hostrcb->overlay_id) {
+	case IPR_HOST_RCB_OVERLAY_ID_1:
+		ipr_log_generic_error(ioa_cfg, hostrcb);
+		break;
+	case IPR_HOST_RCB_OVERLAY_ID_2:
+		ipr_log_cache_error(ioa_cfg, hostrcb);
+		break;
+	case IPR_HOST_RCB_OVERLAY_ID_3:
+		ipr_log_config_error(ioa_cfg, hostrcb);
+		break;
+	case IPR_HOST_RCB_OVERLAY_ID_4:
+	case IPR_HOST_RCB_OVERLAY_ID_6:
+		ipr_log_array_error(ioa_cfg, hostrcb);
+		break;
+	case IPR_HOST_RCB_OVERLAY_ID_DEFAULT:
+		ipr_log_generic_error(ioa_cfg, hostrcb);
+		break;
+	default:
+		dev_err(&ioa_cfg->pdev->dev,
+			"Unknown error received. Overlay ID: %d\n",
+			hostrcb->overlay_id);
+		break;
+	}
+}
+
+/**
+ * ipr_process_error - Op done function for an adapter error log.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function is the op done function for an error log host
+ * controlled async from the adapter. It will log the error and
+ * send the HCAM back to the adapter.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_process_error(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_hostrcb *hostrcb = ipr_cmd->hostrcb;
+	u32 ioasc = be32_to_cpu(ipr_cmd->ioasa.ioasc);
+
+	list_del(&hostrcb->queue);
+
+	if (!ioasc) {
+		ipr_handle_log_data(ioa_cfg, hostrcb);
+	} else if (ioasc != IPR_IOASC_IOA_WAS_RESET) {
+		dev_err(&ioa_cfg->pdev->dev,
+			"Host RCB failed with IOASC: 0x%08X\n", ioasc);
+	}
+
+	ipr_send_hcam(ioa_cfg, IPR_HCAM_CDB_OP_CODE_LOG_DATA, hostrcb);
+}
+
+/**
+ * ipr_timeout -  An internally generated op has timed out.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function blocks host requests and initiates an
+ * adapter reset.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_timeout(struct ipr_cmnd *ipr_cmd)
+{
+	unsigned long lock_flags = 0;
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+
+	ENTER;
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+
+	ioa_cfg->errors_logged++;
+	dev_err(&ioa_cfg->pdev->dev,
+		"Adapter being reset due to command timeout.\n");
+
+	if (WAIT_FOR_DUMP == ioa_cfg->sdt_state)
+		ioa_cfg->sdt_state = GET_DUMP;
+
+	if (!ioa_cfg->in_reset_reload || ioa_cfg->reset_cmd == ipr_cmd)
+		ipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	LEAVE;
+}
+
+/**
+ * ipr_reset_reload - Reset/Reload the IOA
+ * @ioa_cfg:		ioa config struct
+ * @shutdown_type:	shutdown type
+ *
+ * This function resets the adapter and re-initializes it.
+ * This function assumes that all new host commands have been stopped.
+ * Return value:
+ * 	SUCCESS / FAILED
+ **/
+static int ipr_reset_reload(struct ipr_ioa_cfg *ioa_cfg,
+			    enum ipr_shutdown_type shutdown_type)
+{
+	if (ioa_cfg->ioa_is_dead)
+		return FAILED;
+
+	if (ioa_cfg->in_reset_reload) {
+		spin_unlock_irq(ioa_cfg->host->host_lock);
+		wait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);
+		spin_lock_irq(ioa_cfg->host->host_lock);
+
+		/* If we got hit with a host reset while we were already resetting
+		   the adapter for some reason, and the reset failed. */
+		if (ioa_cfg->ioa_is_dead) {
+			ipr_trace;
+			return FAILED;
+		}
+	} else {
+		ipr_initiate_ioa_reset(ioa_cfg, shutdown_type);
+
+		if (ioa_cfg->in_reset_reload) {
+			spin_unlock_irq(ioa_cfg->host->host_lock);
+			wait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);
+			spin_lock_irq(ioa_cfg->host->host_lock);
+		}
+
+		if (ioa_cfg->ioa_is_dead) {
+			ipr_trace;
+			return FAILED;
+		}
+	}
+
+	return SUCCESS;
+}
+
+/**
+ * ipr_find_ses_entry - Find matching SES in SES table
+ * @res:	resource entry struct of SES
+ *
+ * Return value:
+ * 	pointer to SES table entry / NULL on failure
+ **/
+static const struct ipr_ses_table_entry *
+ipr_find_ses_entry(struct ipr_resource_entry *res)
+{
+	int i, j, matches;
+	const struct ipr_ses_table_entry *ste = ipr_ses_table;
+
+	for (i = 0; i < ARRAY_SIZE(ipr_ses_table); i++, ste++) {
+		for (j = 0, matches = 0; j < IPR_PROD_ID_LEN; j++) {
+			if (ste->compare_product_id_byte[j] == 'X') {
+				if (res->cfgte.std_inq_data.vpids.product_id[j] == ste->product_id[j])
+					matches++;
+				else
+					break;
+			} else
+				matches++;
+		}
+
+		if (matches == IPR_PROD_ID_LEN)
+			return ste;
+	}
+
+	return NULL;
+}
+
+/**
+ * ipr_get_max_scsi_speed - Determine max SCSI speed for a given bus
+ * @ioa_cfg:	ioa config struct
+ * @bus:		SCSI bus
+ * @bus_width:	bus width
+ *
+ * Return value:
+ *	SCSI bus speed in units of 100KHz, 1600 is 160 MHz
+ *	For a 2-byte wide SCSI bus, the maximum transfer speed is
+ *	twice the maximum transfer rate (e.g. for a wide enabled bus,
+ *	max 160MHz = max 320MB/sec).
+ **/
+static u32 ipr_get_max_scsi_speed(struct ipr_ioa_cfg *ioa_cfg, u8 bus, u8 bus_width)
+{
+	struct ipr_resource_entry *res;
+	const struct ipr_ses_table_entry *ste;
+	u32 max_xfer_rate = IPR_MAX_SCSI_RATE(bus_width);
+
+	/* Loop through each config table entry in the config table buffer */
+	list_for_each_entry(res, &ioa_cfg->used_res_q, queue) {
+		if (!(IPR_IS_SES_DEVICE(res->cfgte.std_inq_data)))
+			continue;
+
+		if (bus != res->cfgte.res_addr.bus)
+			continue;
+
+		if (!(ste = ipr_find_ses_entry(res)))
+			continue;
+
+		max_xfer_rate = (ste->max_bus_speed_limit * 10) / (bus_width / 8);
+	}
+
+	return max_xfer_rate;
+}
+
+/**
+ * ipr_wait_iodbg_ack - Wait for an IODEBUG ACK from the IOA
+ * @ioa_cfg:		ioa config struct
+ * @max_delay:		max delay in micro-seconds to wait
+ *
+ * Waits for an IODEBUG ACK from the IOA, doing busy looping.
+ *
+ * Return value:
+ * 	0 on success / other on failure
+ **/
+static int ipr_wait_iodbg_ack(struct ipr_ioa_cfg *ioa_cfg, int max_delay)
+{
+	volatile u32 pcii_reg;
+	int delay = 1;
+
+	/* Read interrupt reg until IOA signals IO Debug Acknowledge */
+	while (delay < max_delay) {
+		pcii_reg = readl(ioa_cfg->regs.sense_interrupt_reg);
+
+		if (pcii_reg & IPR_PCII_IO_DEBUG_ACKNOWLEDGE)
+			return 0;
+
+		/* udelay cannot be used if delay is more than a few milliseconds */
+		if ((delay / 1000) > MAX_UDELAY_MS)
+			mdelay(delay / 1000);
+		else
+			udelay(delay);
+
+		delay += delay;
+	}
+	return -EIO;
+}
+
+/**
+ * ipr_get_ldump_data_section - Dump IOA memory
+ * @ioa_cfg:			ioa config struct
+ * @start_addr:			adapter address to dump
+ * @dest:				destination kernel buffer
+ * @length_in_words:	length to dump in 4 byte words
+ *
+ * Return value:
+ * 	0 on success / -EIO on failure
+ **/
+static int ipr_get_ldump_data_section(struct ipr_ioa_cfg *ioa_cfg,
+				      u32 start_addr,
+				      u32 *dest, u32 length_in_words)
+{
+	volatile u32 temp_pcii_reg;
+	int i, delay = 0;
+
+	/* Write IOA interrupt reg starting LDUMP state  */
+	writel((IPR_UPROCI_RESET_ALERT | IPR_UPROCI_IO_DEBUG_ALERT),
+	       ioa_cfg->regs.set_uproc_interrupt_reg);
+	mb();
+
+	/* Wait for IO debug acknowledge */
+	if (ipr_wait_iodbg_ack(ioa_cfg,
+			       IPR_LDUMP_MAX_LONG_ACK_DELAY_IN_USEC)) {
+		dev_err(&ioa_cfg->pdev->dev,
+			"IOA dump long data transfer timeout\n");
+		return -EIO;
+	}
+
+	/* Signal LDUMP interlocked - clear IO debug ack */
+	writel(IPR_PCII_IO_DEBUG_ACKNOWLEDGE,
+	       ioa_cfg->regs.clr_interrupt_reg);
+
+	/* Write Mailbox with starting address */
+	writel(start_addr, ioa_cfg->ioa_mailbox);
+
+	/* Signal address valid - clear IOA Reset alert */
+	writel(IPR_UPROCI_RESET_ALERT,
+	       ioa_cfg->regs.clr_uproc_interrupt_reg);
+	mb();
+
+	for (i = 0; i < length_in_words; i++) {
+		/* Wait for IO debug acknowledge */
+		if (ipr_wait_iodbg_ack(ioa_cfg,
+				       IPR_LDUMP_MAX_SHORT_ACK_DELAY_IN_USEC)) {
+			dev_err(&ioa_cfg->pdev->dev,
+				"IOA dump short data transfer timeout\n");
+			return -EIO;
+		}
+
+		/* Read data from mailbox and increment destination pointer */
+		*dest = cpu_to_be32(readl(ioa_cfg->ioa_mailbox));
+		dest++;
+
+		/* For all but the last word of data, signal data received */
+		if (i < (length_in_words - 1)) {
+			/* Signal dump data received - Clear IO debug Ack */
+			writel(IPR_PCII_IO_DEBUG_ACKNOWLEDGE,
+			       ioa_cfg->regs.clr_interrupt_reg);
+			mb();
+		}
+	}
+
+	/* Signal end of block transfer. Set reset alert then clear IO debug ack */
+	writel(IPR_UPROCI_RESET_ALERT,
+	       ioa_cfg->regs.set_uproc_interrupt_reg);
+
+	writel(IPR_UPROCI_IO_DEBUG_ALERT,
+	       ioa_cfg->regs.clr_uproc_interrupt_reg);
+
+	/* Signal dump data received - Clear IO debug Ack */
+	writel(IPR_PCII_IO_DEBUG_ACKNOWLEDGE,
+	       ioa_cfg->regs.clr_interrupt_reg);
+	mb();
+
+	/* Wait for IOA to signal LDUMP exit - IOA reset alert will be cleared */
+	while (delay < IPR_LDUMP_MAX_SHORT_ACK_DELAY_IN_USEC) {
+		temp_pcii_reg =
+		    readl(ioa_cfg->regs.sense_uproc_interrupt_reg);
+
+		if (!(temp_pcii_reg & IPR_UPROCI_RESET_ALERT))
+			return 0;
+
+		udelay(10);
+		delay += 10;
+	}
+
+	return 0;
+}
+
+#ifdef CONFIG_SCSI_IPR_DUMP
+/**
+ * ipr_sdt_copy - Copy Smart Dump Table to kernel buffer
+ * @ioa_cfg:		ioa config struct
+ * @pci_address:	adapter address
+ * @length:			length of data to copy
+ *
+ * Copy data from PCI adapter to kernel buffer.
+ * Note: length MUST be a 4 byte multiple
+ * Return value:
+ * 	0 on success / other on failure
+ **/
+static int ipr_sdt_copy(struct ipr_ioa_cfg *ioa_cfg,
+			unsigned long pci_address, u32 length)
+{
+	int bytes_copied = 0;
+	int cur_len, rc, rem_len, rem_page_len;
+	u32 *page;
+	unsigned long lock_flags = 0;
+	struct ipr_ioa_dump *ioa_dump = &ioa_cfg->dump->ioa_dump;
+
+	while ((bytes_copied < length) &&
+	       ((ioa_dump->hdr.len + bytes_copied) < IPR_MAX_IOA_DUMP_SIZE)) {
+		if ((ioa_dump->page_offset >= PAGE_SIZE) ||
+		    (ioa_dump->page_offset == 0)) {
+			page = (u32 *)__get_free_page(GFP_ATOMIC);
+
+			if (!page) {
+				ipr_trace;
+				return bytes_copied;
+			}
+
+			ioa_dump->page_offset = 0;
+			ioa_dump->ioa_data[ioa_dump->next_page_index] = page;
+			ioa_dump->next_page_index++;
+		} else
+			page = ioa_dump->ioa_data[ioa_dump->next_page_index - 1];
+
+		rem_len = length - bytes_copied;
+		rem_page_len = PAGE_SIZE - ioa_dump->page_offset;
+		cur_len = min(rem_len, rem_page_len);
+
+		spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+		if (ioa_cfg->sdt_state == ABORT_DUMP) {
+			rc = -EIO;
+		} else {
+			rc = ipr_get_ldump_data_section(ioa_cfg,
+							pci_address + bytes_copied,
+							&page[ioa_dump->page_offset / 4],
+							(cur_len / sizeof(u32)));
+		}
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+
+		if (!rc) {
+			ioa_dump->page_offset += cur_len;
+			bytes_copied += cur_len;
+		} else {
+			ipr_trace;
+			break;
+		}
+		schedule();
+	}
+
+	return bytes_copied;
+}
+
+/**
+ * ipr_init_dump_entry_hdr - Initialize a dump entry header.
+ * @hdr:	dump entry header struct
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_init_dump_entry_hdr(struct ipr_dump_entry_header *hdr)
+{
+	hdr->eye_catcher = IPR_DUMP_EYE_CATCHER;
+	hdr->num_elems = 1;
+	hdr->offset = sizeof(*hdr);
+	hdr->status = IPR_DUMP_STATUS_SUCCESS;
+}
+
+/**
+ * ipr_dump_version_data - Fill in the driver version in the dump.
+ * @ioa_cfg:	ioa config struct
+ * @driver_dump:	driver dump struct
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_dump_version_data(struct ipr_ioa_cfg *ioa_cfg,
+				  struct ipr_driver_dump *driver_dump)
+{
+	ipr_init_dump_entry_hdr(&driver_dump->version_entry.hdr);
+	driver_dump->version_entry.hdr.len =
+		sizeof(struct ipr_dump_version_entry) -
+		sizeof(struct ipr_dump_entry_header);
+	driver_dump->version_entry.hdr.data_type = IPR_DUMP_DATA_TYPE_ASCII;
+	driver_dump->version_entry.hdr.id = IPR_DUMP_DRIVER_VERSION_ID;
+	strcpy(driver_dump->version_entry.version, IPR_DRIVER_VERSION);
+	driver_dump->hdr.num_entries++;
+}
+
+/**
+ * ipr_dump_trace_data - Fill in the IOA trace in the dump.
+ * @ioa_cfg:	ioa config struct
+ * @driver_dump:	driver dump struct
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_dump_trace_data(struct ipr_ioa_cfg *ioa_cfg,
+				   struct ipr_driver_dump *driver_dump)
+{
+	ipr_init_dump_entry_hdr(&driver_dump->trace_entry.hdr);
+	driver_dump->trace_entry.hdr.len =
+		sizeof(struct ipr_dump_trace_entry) -
+		sizeof(struct ipr_dump_entry_header);
+	driver_dump->trace_entry.hdr.data_type = IPR_DUMP_DATA_TYPE_BINARY;
+	driver_dump->trace_entry.hdr.id = IPR_DUMP_TRACE_ID;
+	memcpy(driver_dump->trace_entry.trace, ioa_cfg->trace, IPR_TRACE_SIZE);
+	driver_dump->hdr.num_entries++;
+}
+
+/**
+ * ipr_dump_location_data - Fill in the IOA location in the dump.
+ * @ioa_cfg:	ioa config struct
+ * @driver_dump:	driver dump struct
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_dump_location_data(struct ipr_ioa_cfg *ioa_cfg,
+				   struct ipr_driver_dump *driver_dump)
+{
+	ipr_init_dump_entry_hdr(&driver_dump->location_entry.hdr);
+	driver_dump->location_entry.hdr.len =
+		sizeof(struct ipr_dump_location_entry) -
+		sizeof(struct ipr_dump_entry_header);
+	driver_dump->location_entry.hdr.data_type = IPR_DUMP_DATA_TYPE_ASCII;
+	driver_dump->location_entry.hdr.id = IPR_DUMP_LOCATION_ID;
+	strcpy(driver_dump->location_entry.location, ioa_cfg->pdev->dev.bus_id);
+	driver_dump->hdr.num_entries++;
+}
+
+/**
+ * ipr_get_ioa_dump - Perform a dump of the driver and adapter.
+ * @ioa_cfg:	ioa config struct
+ * @dump:		dump struct
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_get_ioa_dump(struct ipr_ioa_cfg *ioa_cfg, struct ipr_dump *dump)
+{
+	unsigned long start_addr, sdt_word;
+	unsigned long lock_flags = 0;
+	struct ipr_driver_dump *driver_dump = &dump->driver_dump;
+	struct ipr_ioa_dump *ioa_dump = &dump->ioa_dump;
+	u32 num_entries, start_off, end_off;
+	u32 bytes_to_copy, bytes_copied, rc;
+	struct ipr_sdt *sdt;
+	int i;
+
+	ENTER;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+
+	if (ioa_cfg->sdt_state != GET_DUMP)
+		return;
+
+	start_addr = readl(ioa_cfg->ioa_mailbox);
+
+	if (!ipr_sdt_is_fmt2(start_addr)) {
+		dev_err(&ioa_cfg->pdev->dev,
+			"Invalid dump table format: %lx\n", start_addr);
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		return;
+	}
+
+	dev_err(&ioa_cfg->pdev->dev, "Dump of IOA initiated\n");
+
+	driver_dump->hdr.eye_catcher = IPR_DUMP_EYE_CATCHER;
+
+	/* Initialize the overall dump header */
+	driver_dump->hdr.len = sizeof(struct ipr_driver_dump);
+	driver_dump->hdr.num_entries = 1;
+	driver_dump->hdr.first_entry_offset = sizeof(struct ipr_dump_header);
+	driver_dump->hdr.status = IPR_DUMP_STATUS_SUCCESS;
+	driver_dump->hdr.os = IPR_DUMP_OS_LINUX;
+	driver_dump->hdr.driver_name = IPR_DUMP_DRIVER_NAME;
+
+	ipr_dump_version_data(ioa_cfg, driver_dump);
+	ipr_dump_location_data(ioa_cfg, driver_dump);
+	ipr_dump_trace_data(ioa_cfg, driver_dump);
+
+	/* Update dump_header */
+	driver_dump->hdr.len += sizeof(struct ipr_dump_entry_header);
+
+	/* IOA Dump entry */
+	ipr_init_dump_entry_hdr(&ioa_dump->hdr);
+	ioa_dump->format = IPR_SDT_FMT2;
+	ioa_dump->hdr.len = 0;
+	ioa_dump->hdr.data_type = IPR_DUMP_DATA_TYPE_BINARY;
+	ioa_dump->hdr.id = IPR_DUMP_IOA_DUMP_ID;
+
+	/* First entries in sdt are actually a list of dump addresses and
+	 lengths to gather the real dump data.  sdt represents the pointer
+	 to the ioa generated dump table.  Dump data will be extracted based
+	 on entries in this table */
+	sdt = &ioa_dump->sdt;
+
+	rc = ipr_get_ldump_data_section(ioa_cfg, start_addr, (u32 *)sdt,
+					sizeof(struct ipr_sdt) / sizeof(u32));
+
+	/* Smart Dump table is ready to use and the first entry is valid */
+	if (rc || (be32_to_cpu(sdt->hdr.state) != IPR_FMT2_SDT_READY_TO_USE)) {
+		dev_err(&ioa_cfg->pdev->dev,
+			"Dump of IOA failed. Dump table not valid: %d, %X.\n",
+			rc, be32_to_cpu(sdt->hdr.state));
+		driver_dump->hdr.status = IPR_DUMP_STATUS_FAILED;
+		ioa_cfg->sdt_state = DUMP_OBTAINED;
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		return;
+	}
+
+	num_entries = be32_to_cpu(sdt->hdr.num_entries_used);
+
+	if (num_entries > IPR_NUM_SDT_ENTRIES)
+		num_entries = IPR_NUM_SDT_ENTRIES;
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+
+	for (i = 0; i < num_entries; i++) {
+		if (ioa_dump->hdr.len > IPR_MAX_IOA_DUMP_SIZE)
+			break;
+
+		if (sdt->entry[i].flags & IPR_SDT_VALID_ENTRY) {
+			sdt_word = be32_to_cpu(sdt->entry[i].bar_str_offset);
+			start_off = sdt_word & IPR_FMT2_MBX_ADDR_MASK;
+			end_off = be32_to_cpu(sdt->entry[i].end_offset);
+
+			if (ipr_sdt_is_fmt2(sdt_word) && sdt_word) {
+				bytes_to_copy = end_off - start_off;
+				if (bytes_to_copy > IPR_MAX_IOA_DUMP_SIZE) {
+					sdt->entry[i].flags &= ~IPR_SDT_VALID_ENTRY;
+					continue;
+				}
+
+				/* Copy data from adapter to driver buffers */
+				bytes_copied = ipr_sdt_copy(ioa_cfg, sdt_word,
+							    bytes_to_copy);
+
+				ioa_dump->hdr.len += bytes_copied;
+
+				if (bytes_copied != bytes_to_copy) {
+					dev_err(&ioa_cfg->pdev->dev, "Dump of IOA completed.\n");
+					driver_dump->hdr.status = IPR_DUMP_STATUS_QUAL_SUCCESS;
+					driver_dump->hdr.len += ioa_dump->hdr.len;
+					ioa_cfg->sdt_state = DUMP_OBTAINED;
+					return;
+				}
+			}
+		}
+	}
+
+	dev_err(&ioa_cfg->pdev->dev, "Dump of IOA completed.\n");
+
+	/* Update dump_header */
+	driver_dump->hdr.len += ioa_dump->hdr.len;
+	wmb();
+	ioa_cfg->sdt_state = DUMP_OBTAINED;
+	LEAVE;
+}
+
+#else
+#define ipr_get_ioa_dump(ioa_cfg, dump) do { } while(0)
+#endif
+
+/**
+ * ipr_worker_thread - Worker thread
+ * @data:		ioa config struct
+ *
+ * Called at task level from a work thread. This function takes care
+ * of adding and removing device from the mid-layer as configuration
+ * changes are detected by the adapter.
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_worker_thread(void *data)
+{
+	unsigned long lock_flags;
+	struct ipr_resource_entry *res;
+	struct scsi_device *sdev;
+	struct ipr_dump *dump;
+	struct ipr_ioa_cfg *ioa_cfg = data;
+	u8 bus, target, lun;
+	int did_work;
+
+	ENTER;
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+
+	if (ioa_cfg->sdt_state == GET_DUMP) {
+		dump = ioa_cfg->dump;
+		if (!dump || !kobject_get(&dump->kobj)) {
+			spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+			return;
+		}
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		ipr_get_ioa_dump(ioa_cfg, dump);
+		kobject_put(&dump->kobj);
+
+		spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+		if (ioa_cfg->sdt_state == DUMP_OBTAINED)
+			ipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		return;
+	}
+
+restart:
+	do {
+		did_work = 0;
+		if (!ioa_cfg->allow_cmds) {
+			spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+			return;
+		}
+
+		list_for_each_entry(res, &ioa_cfg->used_res_q, queue) {
+			if (res->del_from_ml && res->sdev) {
+				did_work = 1;
+				sdev = res->sdev;
+				if (!scsi_device_get(sdev)) {
+					spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+					scsi_remove_device(sdev);
+					scsi_device_put(sdev);
+					spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+				}
+				break;
+			}
+		}
+	} while(did_work);
+
+	list_for_each_entry(res, &ioa_cfg->used_res_q, queue) {
+		if (res->add_to_ml) {
+			bus = res->cfgte.res_addr.bus;
+			target = res->cfgte.res_addr.target;
+			lun = res->cfgte.res_addr.lun;
+			spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+			scsi_add_device(ioa_cfg->host, bus, target, lun);
+			spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+			goto restart;
+		}
+	}
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	LEAVE;
+}
+
+#ifdef CONFIG_SCSI_IPR_TRACE
+/**
+ * ipr_read_trace - Dump the adapter trace
+ * @kobj:		kobject struct
+ * @buf:		buffer
+ * @off:		offset
+ * @count:		buffer size
+ *
+ * Return value:
+ *	number of bytes printed to buffer
+ **/
+static ssize_t ipr_read_trace(struct kobject *kobj, char *buf,
+			      loff_t off, size_t count)
+{
+	struct class_device *cdev = container_of(kobj,struct class_device,kobj);
+	struct Scsi_Host *shost = class_to_shost(cdev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;
+	unsigned long lock_flags = 0;
+	int size = IPR_TRACE_SIZE;
+	char *src = (char *)ioa_cfg->trace;
+
+	if (off > size)
+		return 0;
+	if (off + count > size) {
+		size -= off;
+		count = size;
+	}
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	memcpy(buf, &src[off], count);
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	return count;
+}
+
+static struct bin_attribute ipr_trace_attr = {
+	.attr =	{
+		.name = "trace",
+		.mode = S_IRUGO,
+	},
+	.size = 0,
+	.read = ipr_read_trace,
+};
+#endif
+
+/**
+ * ipr_show_fw_version - Show the firmware version
+ * @class_dev:	class device struct
+ * @buf:		buffer
+ *
+ * Return value:
+ *	number of bytes printed to buffer
+ **/
+static ssize_t ipr_show_fw_version(struct class_device *class_dev, char *buf)
+{
+	struct Scsi_Host *shost = class_to_shost(class_dev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;
+	struct ipr_inquiry_page3 *ucode_vpd = &ioa_cfg->vpd_cbs->page3_data;
+	unsigned long lock_flags = 0;
+	int len;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	len = snprintf(buf, PAGE_SIZE, "%02X%02X%02X%02X\n",
+		       ucode_vpd->major_release, ucode_vpd->card_type,
+		       ucode_vpd->minor_release[0],
+		       ucode_vpd->minor_release[1]);
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	return len;
+}
+
+static struct class_device_attribute ipr_fw_version_attr = {
+	.attr = {
+		.name =		"fw_version",
+		.mode =		S_IRUGO,
+	},
+	.show = ipr_show_fw_version,
+};
+
+/**
+ * ipr_show_log_level - Show the adapter's error logging level
+ * @class_dev:	class device struct
+ * @buf:		buffer
+ *
+ * Return value:
+ * 	number of bytes printed to buffer
+ **/
+static ssize_t ipr_show_log_level(struct class_device *class_dev, char *buf)
+{
+	struct Scsi_Host *shost = class_to_shost(class_dev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;
+	unsigned long lock_flags = 0;
+	int len;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	len = snprintf(buf, PAGE_SIZE, "%d\n", ioa_cfg->log_level);
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	return len;
+}
+
+/**
+ * ipr_store_log_level - Change the adapter's error logging level
+ * @class_dev:	class device struct
+ * @buf:		buffer
+ *
+ * Return value:
+ * 	number of bytes printed to buffer
+ **/
+static ssize_t ipr_store_log_level(struct class_device *class_dev,
+				   const char *buf, size_t count)
+{
+	struct Scsi_Host *shost = class_to_shost(class_dev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;
+	unsigned long lock_flags = 0;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	ioa_cfg->log_level = simple_strtoul(buf, NULL, 10);
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	return strlen(buf);
+}
+
+static struct class_device_attribute ipr_log_level_attr = {
+	.attr = {
+		.name =		"log_level",
+		.mode =		S_IRUGO | S_IWUSR,
+	},
+	.show = ipr_show_log_level,
+	.store = ipr_store_log_level
+};
+
+/**
+ * ipr_store_diagnostics - IOA Diagnostics interface
+ * @class_dev:	class_device struct
+ * @buf:		buffer
+ * @count:		buffer size
+ *
+ * This function will reset the adapter and wait a reasonable
+ * amount of time for any errors that the adapter might log.
+ *
+ * Return value:
+ * 	count on success / other on failure
+ **/
+static ssize_t ipr_store_diagnostics(struct class_device *class_dev,
+				     const char *buf, size_t count)
+{
+	struct Scsi_Host *shost = class_to_shost(class_dev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;
+	unsigned long lock_flags = 0;
+	int rc = count;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EACCES;
+
+	wait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	ioa_cfg->errors_logged = 0;
+	ipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NORMAL);
+
+	if (ioa_cfg->in_reset_reload) {
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		wait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);
+
+		/* Wait for a second for any errors to be logged */
+		schedule_timeout(HZ);
+	} else {
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		return -EIO;
+	}
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	if (ioa_cfg->in_reset_reload || ioa_cfg->errors_logged)
+		rc = -EIO;
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+
+	return rc;
+}
+
+static struct class_device_attribute ipr_diagnostics_attr = {
+	.attr = {
+		.name =		"run_diagnostics",
+		.mode =		S_IWUSR,
+	},
+	.store = ipr_store_diagnostics
+};
+
+/**
+ * ipr_store_reset_adapter - Reset the adapter
+ * @class_dev:	class_device struct
+ * @buf:		buffer
+ * @count:		buffer size
+ *
+ * This function will reset the adapter.
+ *
+ * Return value:
+ * 	count on success / other on failure
+ **/
+static ssize_t ipr_store_reset_adapter(struct class_device *class_dev,
+				       const char *buf, size_t count)
+{
+	struct Scsi_Host *shost = class_to_shost(class_dev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;
+	unsigned long lock_flags;
+	int result = count;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EACCES;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	if (!ioa_cfg->in_reset_reload)
+		ipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NORMAL);
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	wait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);
+
+	return result;
+}
+
+static struct class_device_attribute ipr_ioa_reset_attr = {
+	.attr = {
+		.name =		"reset_host",
+		.mode =		S_IWUSR,
+	},
+	.store = ipr_store_reset_adapter
+};
+
+/**
+ * ipr_alloc_ucode_buffer - Allocates a microcode download buffer
+ * @buf_len:		buffer length
+ *
+ * Allocates a DMA'able buffer in chunks and assembles a scatter/gather
+ * list to use for microcode download
+ *
+ * Return value:
+ * 	pointer to sglist / NULL on failure
+ **/
+static struct ipr_sglist *ipr_alloc_ucode_buffer(int buf_len)
+{
+	int sg_size, order, bsize_elem, num_elem, i, j;
+	struct ipr_sglist *sglist;
+	struct scatterlist *scatterlist;
+	struct page *page;
+
+	/* Get the minimum size per scatter/gather element */
+	sg_size = buf_len / (IPR_MAX_SGLIST - 1);
+
+	/* Get the actual size per element */
+	order = get_order(sg_size);
+
+	/* Determine the actual number of bytes per element */
+	bsize_elem = PAGE_SIZE * (1 << order);
+
+	/* Determine the actual number of sg entries needed */
+	if (buf_len % bsize_elem)
+		num_elem = (buf_len / bsize_elem) + 1;
+	else
+		num_elem = buf_len / bsize_elem;
+
+	/* Allocate a scatter/gather list for the DMA */
+	sglist = kmalloc(sizeof(struct ipr_sglist) +
+			 (sizeof(struct scatterlist) * (num_elem - 1)),
+			 GFP_KERNEL);
+
+	if (sglist == NULL) {
+		ipr_trace;
+		return NULL;
+	}
+
+	memset(sglist, 0, sizeof(struct ipr_sglist) +
+	       (sizeof(struct scatterlist) * (num_elem - 1)));
+
+	scatterlist = sglist->scatterlist;
+
+	sglist->order = order;
+	sglist->num_sg = num_elem;
+
+	/* Allocate a bunch of sg elements */
+	for (i = 0; i < num_elem; i++) {
+		page = alloc_pages(GFP_KERNEL, order);
+		if (!page) {
+			ipr_trace;
+
+			/* Free up what we already allocated */
+			for (j = i - 1; j >= 0; j--)
+				__free_pages(scatterlist[j].page, order);
+			kfree(sglist);
+			return NULL;
+		}
+
+		scatterlist[i].page = page;
+	}
+
+	return sglist;
+}
+
+/**
+ * ipr_free_ucode_buffer - Frees a microcode download buffer
+ * @p_dnld:		scatter/gather list pointer
+ *
+ * Free a DMA'able ucode download buffer previously allocated with
+ * ipr_alloc_ucode_buffer
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_free_ucode_buffer(struct ipr_sglist *sglist)
+{
+	int i;
+
+	for (i = 0; i < sglist->num_sg; i++)
+		__free_pages(sglist->scatterlist[i].page, sglist->order);
+
+	kfree(sglist);
+}
+
+/**
+ * ipr_copy_ucode_buffer - Copy user buffer to kernel buffer
+ * @sglist:		scatter/gather list pointer
+ * @buffer:		buffer pointer
+ * @len:		buffer length
+ *
+ * Copy a microcode image from a user buffer into a buffer allocated by
+ * ipr_alloc_ucode_buffer
+ *
+ * Return value:
+ * 	0 on success / other on failure
+ **/
+static int ipr_copy_ucode_buffer(struct ipr_sglist *sglist,
+				 u8 *buffer, u32 len)
+{
+	int bsize_elem, i, result = 0;
+	struct scatterlist *scatterlist;
+	void *kaddr;
+
+	/* Determine the actual number of bytes per element */
+	bsize_elem = PAGE_SIZE * (1 << sglist->order);
+
+	scatterlist = sglist->scatterlist;
+
+	for (i = 0; i < (len / bsize_elem); i++, buffer += bsize_elem) {
+		kaddr = kmap(scatterlist[i].page);
+		memcpy(kaddr, buffer, bsize_elem);
+		kunmap(scatterlist[i].page);
+
+		scatterlist[i].length = bsize_elem;
+
+		if (result != 0) {
+			ipr_trace;
+			return result;
+		}
+	}
+
+	if (len % bsize_elem) {
+		kaddr = kmap(scatterlist[i].page);
+		memcpy(kaddr, buffer, len % bsize_elem);
+		kunmap(scatterlist[i].page);
+
+		scatterlist[i].length = len % bsize_elem;
+	}
+
+	sglist->buffer_len = len;
+	return result;
+}
+
+/**
+ * ipr_map_ucode_buffer - Map a microcode download buffer
+ * @ipr_cmd:	ipr command struct
+ * @sglist:		scatter/gather list
+ * @len:		total length of download buffer
+ *
+ * Maps a microcode download scatter/gather list for DMA and
+ * builds the IOADL.
+ *
+ * Return value:
+ * 	0 on success / -EIO on failure
+ **/
+static int ipr_map_ucode_buffer(struct ipr_cmnd *ipr_cmd,
+				struct ipr_sglist *sglist, int len)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;
+	struct ipr_ioadl_desc *ioadl = ipr_cmd->ioadl;
+	struct scatterlist *scatterlist = sglist->scatterlist;
+	int i;
+
+	ipr_cmd->dma_use_sg = pci_map_sg(ioa_cfg->pdev, scatterlist,
+					 sglist->num_sg, DMA_TO_DEVICE);
+
+	ioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;
+	ioarcb->write_data_transfer_length = cpu_to_be32(len);
+	ioarcb->write_ioadl_len =
+		cpu_to_be32(sizeof(struct ipr_ioadl_desc) * ipr_cmd->dma_use_sg);
+
+	for (i = 0; i < ipr_cmd->dma_use_sg; i++) {
+		ioadl[i].flags_and_data_len =
+			cpu_to_be32(IPR_IOADL_FLAGS_WRITE | sg_dma_len(&scatterlist[i]));
+		ioadl[i].address =
+			cpu_to_be32(sg_dma_address(&scatterlist[i]));
+	}
+
+	if (likely(ipr_cmd->dma_use_sg)) {
+		ioadl[i-1].flags_and_data_len |=
+			cpu_to_be32(IPR_IOADL_FLAGS_LAST);
+	}
+	else {
+		dev_err(&ioa_cfg->pdev->dev, "pci_map_sg failed!\n");
+		return -EIO;
+	}
+
+	return 0;
+}
+
+/**
+ * ipr_store_update_fw - Update the firmware on the adapter
+ * @class_dev:	class_device struct
+ * @buf:		buffer
+ * @count:		buffer size
+ *
+ * This function will update the firmware on the adapter.
+ *
+ * Return value:
+ * 	count on success / other on failure
+ **/
+static ssize_t ipr_store_update_fw(struct class_device *class_dev,
+				       const char *buf, size_t count)
+{
+	struct Scsi_Host *shost = class_to_shost(class_dev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;
+	struct ipr_ucode_image_header *image_hdr;
+	const struct firmware *fw_entry;
+	struct ipr_sglist *sglist;
+	unsigned long lock_flags;
+	char fname[100];
+	char *src;
+	int len, result, dnld_size;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EACCES;
+
+	len = snprintf(fname, 99, "%s", buf);
+	fname[len-1] = '\0';
+
+	if(request_firmware(&fw_entry, fname, &ioa_cfg->pdev->dev)) {
+		dev_err(&ioa_cfg->pdev->dev, "Firmware file %s not found\n", fname);
+		return -EIO;
+	}
+
+	image_hdr = (struct ipr_ucode_image_header *)fw_entry->data;
+
+	if (be32_to_cpu(image_hdr->header_length) > fw_entry->size ||
+	    ioa_cfg->vpd_cbs->page3_data.card_type != image_hdr->card_type) {
+		dev_err(&ioa_cfg->pdev->dev, "Invalid microcode buffer\n");
+		release_firmware(fw_entry);
+		return -EINVAL;
+	}
+
+	src = (u8 *)image_hdr + be32_to_cpu(image_hdr->header_length);
+	dnld_size = fw_entry->size - be32_to_cpu(image_hdr->header_length);
+	sglist = ipr_alloc_ucode_buffer(dnld_size);
+
+	if (!sglist) {
+		dev_err(&ioa_cfg->pdev->dev, "Microcode buffer allocation failed\n");
+		release_firmware(fw_entry);
+		return -ENOMEM;
+	}
+
+	result = ipr_copy_ucode_buffer(sglist, src, dnld_size);
+
+	if (result) {
+		dev_err(&ioa_cfg->pdev->dev,
+			"Microcode buffer copy to DMA buffer failed\n");
+		ipr_free_ucode_buffer(sglist);
+		release_firmware(fw_entry);
+		return result;
+	}
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+
+	if (ioa_cfg->ucode_sglist) {
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		dev_err(&ioa_cfg->pdev->dev,
+			"Microcode download already in progress\n");
+		ipr_free_ucode_buffer(sglist);
+		release_firmware(fw_entry);
+		return -EIO;
+	}
+
+	ioa_cfg->ucode_sglist = sglist;
+	ipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NORMAL);
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	wait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	ioa_cfg->ucode_sglist = NULL;
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+
+	ipr_free_ucode_buffer(sglist);
+	release_firmware(fw_entry);
+
+	return count;
+}
+
+static struct class_device_attribute ipr_update_fw_attr = {
+	.attr = {
+		.name =		"update_fw",
+		.mode =		S_IWUSR,
+	},
+	.store = ipr_store_update_fw
+};
+
+static struct class_device_attribute *ipr_ioa_attrs[] = {
+	&ipr_fw_version_attr,
+	&ipr_log_level_attr,
+	&ipr_diagnostics_attr,
+	&ipr_ioa_reset_attr,
+	&ipr_update_fw_attr,
+	NULL,
+};
+
+#ifdef CONFIG_SCSI_IPR_DUMP
+/**
+ * ipr_read_dump - Dump the adapter
+ * @kobj:		kobject struct
+ * @buf:		buffer
+ * @off:		offset
+ * @count:		buffer size
+ *
+ * Return value:
+ *	number of bytes printed to buffer
+ **/
+static ssize_t ipr_read_dump(struct kobject *kobj, char *buf,
+			      loff_t off, size_t count)
+{
+	struct class_device *cdev = container_of(kobj,struct class_device,kobj);
+	struct Scsi_Host *shost = class_to_shost(cdev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;
+	struct ipr_dump *dump;
+	unsigned long lock_flags = 0;
+	char *src;
+	int len;
+	size_t rc = count;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EACCES;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	dump = ioa_cfg->dump;
+
+	if (ioa_cfg->sdt_state != DUMP_OBTAINED || !dump || !kobject_get(&dump->kobj)) {
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		return 0;
+	}
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+
+	if (off > dump->driver_dump.hdr.len) {
+		kobject_put(&dump->kobj);
+		return 0;
+	}
+
+	if (off + count > dump->driver_dump.hdr.len) {
+		count = dump->driver_dump.hdr.len - off;
+		rc = count;
+	}
+
+	if (count && off < sizeof(dump->driver_dump)) {
+		if (off + count > sizeof(dump->driver_dump))
+			len = sizeof(dump->driver_dump) - off;
+		else
+			len = count;
+		src = (u8 *)&dump->driver_dump + off;
+		memcpy(buf, src, len);
+		buf += len;
+		off += len;
+		count -= len;
+	}
+
+	off -= sizeof(dump->driver_dump);
+
+	if (count && off < offsetof(struct ipr_ioa_dump, ioa_data)) {
+		if (off + count > offsetof(struct ipr_ioa_dump, ioa_data))
+			len = offsetof(struct ipr_ioa_dump, ioa_data) - off;
+		else
+			len = count;
+		src = (u8 *)&dump->ioa_dump + off;
+		memcpy(buf, src, len);
+		buf += len;
+		off += len;
+		count -= len;
+	}
+
+	off -= offsetof(struct ipr_ioa_dump, ioa_data);
+
+	while (count) {
+		if ((off & PAGE_MASK) != ((off + count) & PAGE_MASK))
+			len = PAGE_ALIGN(off) - off;
+		else
+			len = count;
+		src = (u8 *)dump->ioa_dump.ioa_data[(off & PAGE_MASK) >> PAGE_SHIFT];
+		src += off & ~PAGE_MASK;
+		memcpy(buf, src, len);
+		buf += len;
+		off += len;
+		count -= len;
+	}
+
+	kobject_put(&dump->kobj);
+	return rc;
+}
+
+/**
+ * ipr_release_dump - Free adapter dump memory
+ * @kobj:	kobject struct
+ *
+ * Return value:
+ *	nothing
+ **/
+static void ipr_release_dump(struct kobject *kobj)
+{
+	struct ipr_dump *dump = container_of(kobj,struct ipr_dump,kobj);
+	struct ipr_ioa_cfg *ioa_cfg = dump->ioa_cfg;
+	unsigned long lock_flags = 0;
+	int i;
+
+	ENTER;
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	ioa_cfg->dump = NULL;
+	ioa_cfg->sdt_state = INACTIVE;
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+
+	for (i = 0; i < dump->ioa_dump.next_page_index; i++)
+		free_page((unsigned long) dump->ioa_dump.ioa_data[i]);
+
+	kfree(dump);
+	LEAVE;
+}
+
+static struct kobj_type ipr_dump_kobj_type = {
+	.release = ipr_release_dump,
+};
+
+/**
+ * ipr_alloc_dump - Prepare for adapter dump
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ *	0 on success / other on failure
+ **/
+static int ipr_alloc_dump(struct ipr_ioa_cfg *ioa_cfg)
+{
+	struct ipr_dump *dump;
+	unsigned long lock_flags = 0;
+
+	ENTER;
+	dump = kmalloc(sizeof(struct ipr_dump), GFP_KERNEL);
+
+	if (!dump) {
+		ipr_err("Dump memory allocation failed\n");
+		return -ENOMEM;
+	}
+
+	memset(dump, 0, sizeof(struct ipr_dump));
+	kobject_init(&dump->kobj);
+	dump->kobj.ktype = &ipr_dump_kobj_type;
+	dump->ioa_cfg = ioa_cfg;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+
+	if (INACTIVE != ioa_cfg->sdt_state) {
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		kfree(dump);
+		return 0;
+	}
+
+	ioa_cfg->dump = dump;
+	ioa_cfg->sdt_state = WAIT_FOR_DUMP;
+	if (ioa_cfg->ioa_is_dead && !ioa_cfg->dump_taken) {
+		ioa_cfg->dump_taken = 1;
+		schedule_work(&ioa_cfg->work_q);
+	}
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+
+	LEAVE;
+	return 0;
+}
+
+/**
+ * ipr_free_dump - Free adapter dump memory
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ *	0 on success / other on failure
+ **/
+static int ipr_free_dump(struct ipr_ioa_cfg *ioa_cfg)
+{
+	struct ipr_dump *dump;
+	unsigned long lock_flags = 0;
+
+	ENTER;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	dump = ioa_cfg->dump;
+	if (!dump) {
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		return 0;
+	}
+
+	ioa_cfg->dump = NULL;
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+
+	kobject_put(&dump->kobj);
+
+	LEAVE;
+	return 0;
+}
+
+/**
+ * ipr_write_dump - Setup dump state of adapter
+ * @kobj:		kobject struct
+ * @buf:		buffer
+ * @off:		offset
+ * @count:		buffer size
+ *
+ * Return value:
+ *	number of bytes printed to buffer
+ **/
+static ssize_t ipr_write_dump(struct kobject *kobj, char *buf,
+			      loff_t off, size_t count)
+{
+	struct class_device *cdev = container_of(kobj,struct class_device,kobj);
+	struct Scsi_Host *shost = class_to_shost(cdev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)shost->hostdata;
+	int rc;
+
+	if (!capable(CAP_SYS_ADMIN))
+		return -EACCES;
+
+	if (buf[0] == '1')
+		rc = ipr_alloc_dump(ioa_cfg);
+	else if (buf[0] == '0')
+		rc = ipr_free_dump(ioa_cfg);
+	else
+		return -EINVAL;
+
+	if (rc)
+		return rc;
+	else
+		return count;
+}
+
+static struct bin_attribute ipr_dump_attr = {
+	.attr =	{
+		.name = "dump",
+		.mode = S_IRUSR | S_IWUSR,
+	},
+	.size = 0,
+	.read = ipr_read_dump,
+	.write = ipr_write_dump
+};
+#else
+static int ipr_free_dump(struct ipr_ioa_cfg *ioa_cfg) { return 0; };
+#endif
+
+/**
+ * ipr_store_queue_depth - Change the device's queue depth
+ * @dev:	device struct
+ * @buf:	buffer
+ *
+ * Return value:
+ * 	number of bytes printed to buffer
+ **/
+static ssize_t ipr_store_queue_depth(struct device *dev,
+				    const char *buf, size_t count)
+{
+	struct scsi_device *sdev = to_scsi_device(dev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)sdev->host->hostdata;
+	struct ipr_resource_entry *res;
+	int qdepth = simple_strtoul(buf, NULL, 10);
+	int tagged = 0;
+	unsigned long lock_flags = 0;
+	ssize_t len = -ENXIO;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	res = (struct ipr_resource_entry *)sdev->hostdata;
+	if (res) {
+		res->qdepth = qdepth;
+
+		if (ipr_is_gscsi(res) && res->tcq_active)
+			tagged = MSG_ORDERED_TAG;
+
+		len = strlen(buf);
+	}
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	scsi_adjust_queue_depth(sdev, tagged, qdepth);
+	return len;
+}
+
+static struct device_attribute ipr_queue_depth_attr = {
+	.attr = {
+		.name = 	"queue_depth",
+		.mode =		S_IRUSR | S_IWUSR,
+	},
+	.store = ipr_store_queue_depth
+};
+
+/**
+ * ipr_show_tcq_enable - Show if the device is enabled for tcqing
+ * @dev:	device struct
+ * @buf:	buffer
+ *
+ * Return value:
+ * 	number of bytes printed to buffer
+ **/
+static ssize_t ipr_show_tcq_enable(struct device *dev, char *buf)
+{
+	struct scsi_device *sdev = to_scsi_device(dev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)sdev->host->hostdata;
+	struct ipr_resource_entry *res;
+	unsigned long lock_flags = 0;
+	ssize_t len = -ENXIO;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	res = (struct ipr_resource_entry *)sdev->hostdata;
+	if (res)
+		len = snprintf(buf, PAGE_SIZE, "%d\n", res->tcq_active);
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	return len;
+}
+
+/**
+ * ipr_store_tcq_enable - Change the device's TCQing state
+ * @dev:	device struct
+ * @buf:	buffer
+ *
+ * Return value:
+ * 	number of bytes printed to buffer
+ **/
+static ssize_t ipr_store_tcq_enable(struct device *dev,
+				    const char *buf, size_t count)
+{
+	struct scsi_device *sdev = to_scsi_device(dev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)sdev->host->hostdata;
+	struct ipr_resource_entry *res;
+	unsigned long lock_flags = 0;
+	int tcq_active = simple_strtoul(buf, NULL, 10);
+	int qdepth = IPR_MAX_CMD_PER_LUN;
+	int tagged = 0;
+	ssize_t len = -ENXIO;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+
+	res = (struct ipr_resource_entry *)sdev->hostdata;
+
+	if (res) {
+		res->tcq_active = 0;
+		qdepth = res->qdepth;
+
+		if (ipr_is_gscsi(res) && sdev->tagged_supported) {
+			if (tcq_active) {
+				tagged = MSG_ORDERED_TAG;
+				res->tcq_active = 1;
+			}
+
+			len = strlen(buf);
+		} else if (tcq_active) {
+			len = -EINVAL;
+		}
+	}
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	scsi_adjust_queue_depth(sdev, tagged, qdepth);
+	return len;
+}
+
+static struct device_attribute ipr_tcqing_attr = {
+	.attr = {
+		.name = 	"tcq_enable",
+		.mode =		S_IRUSR | S_IWUSR,
+	},
+	.store = ipr_store_tcq_enable,
+	.show = ipr_show_tcq_enable
+};
+
+/**
+ * ipr_show_adapter_handle - Show the adapter's resource handle for this device
+ * @dev:	device struct
+ * @buf:	buffer
+ *
+ * Return value:
+ * 	number of bytes printed to buffer
+ **/
+static ssize_t ipr_show_adapter_handle(struct device *dev, char *buf)
+{
+	struct scsi_device *sdev = to_scsi_device(dev);
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)sdev->host->hostdata;
+	struct ipr_resource_entry *res;
+	unsigned long lock_flags = 0;
+	ssize_t len = -ENXIO;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	res = (struct ipr_resource_entry *)sdev->hostdata;
+	if (res)
+		len = snprintf(buf, PAGE_SIZE, "%08X\n", res->cfgte.res_handle);
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	return len;
+}
+
+static struct device_attribute ipr_adapter_handle_attr = {
+	.attr = {
+		.name = 	"adapter_handle",
+		.mode =		S_IRUSR,
+	},
+	.show = ipr_show_adapter_handle
+};
+
+static struct device_attribute *ipr_dev_attrs[] = {
+	&ipr_queue_depth_attr,
+	&ipr_tcqing_attr,
+	&ipr_adapter_handle_attr,
+	NULL,
+};
+
+/**
+ * ipr_biosparam - Return the HSC mapping
+ * @sdev:			scsi device struct
+ * @block_device:	block device pointer
+ * @capacity:		capacity of the device
+ * @parm:			Array containing returned HSC values.
+ *
+ * This function generates the HSC parms that fdisk uses.
+ * We want to make sure we return something that places partitions
+ * on 4k boundaries for best performance with the IOA.
+ *
+ * Return value:
+ * 	0 on success
+ **/
+static int ipr_biosparam(struct scsi_device *sdev,
+			 struct block_device *block_device,
+			 sector_t capacity, int *parm)
+{
+	int heads, sectors, cylinders;
+
+	heads = 128;
+	sectors = 32;
+
+	cylinders = capacity;
+	sector_div(cylinders, (128 * 32));
+
+	/* return result */
+	parm[0] = heads;
+	parm[1] = sectors;
+	parm[2] = cylinders;
+
+	return 0;
+}
+
+/**
+ * ipr_slave_destroy - Unconfigure a SCSI device
+ * @sdev:	scsi device struct
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_slave_destroy(struct scsi_device *sdev)
+{
+	struct ipr_resource_entry *res;
+	struct ipr_ioa_cfg *ioa_cfg;
+	unsigned long lock_flags = 0;
+
+	ioa_cfg = (struct ipr_ioa_cfg *) sdev->host->hostdata;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	res = (struct ipr_resource_entry *) sdev->hostdata;
+	if (res) {
+		if (res->del_from_ml)
+			list_move_tail(&res->queue, &ioa_cfg->free_res_q);
+
+		sdev->hostdata = NULL;
+		res->sdev = NULL;
+	}
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+}
+
+/**
+ * ipr_slave_configure - Configure a SCSI device
+ * @sdev:	scsi device struct
+ *
+ * This function configures the specified scsi device.
+ *
+ * Return value:
+ * 	0 on success
+ **/
+static int ipr_slave_configure(struct scsi_device *sdev)
+{
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *) sdev->host->hostdata;
+	struct ipr_resource_entry *res;
+	unsigned long lock_flags = 0;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	res = sdev->hostdata;
+	if (res) {
+		if (ipr_is_af_dasd_device(res))
+			sdev->type = TYPE_RAID;
+		else if (ipr_is_vset_device(res))
+			sdev->allow_restart = 1;
+		if (ipr_is_af_dasd_device(res) || ipr_is_ioa_resource(res))
+			sdev->scsi_level = 4;
+
+		scsi_adjust_queue_depth(sdev, 0, res->qdepth);
+	}
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	return 0;
+}
+
+/**
+ * ipr_slave_alloc - Prepare for commands to a device.
+ * @sdev:	scsi device struct
+ *
+ * This function saves a pointer to the resource entry
+ * in the scsi device struct if the device exists. We
+ * can then use this pointer in ipr_queuecommand when
+ * handling new commands.
+ *
+ * Return value:
+ * 	0 on success
+ **/
+static int ipr_slave_alloc(struct scsi_device *sdev)
+{
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *) sdev->host->hostdata;
+	struct ipr_resource_entry *res;
+	unsigned long lock_flags;
+
+	sdev->hostdata = NULL;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+
+	list_for_each_entry(res, &ioa_cfg->used_res_q, queue) {
+		if ((res->cfgte.res_addr.bus == sdev->channel) &&
+		    (res->cfgte.res_addr.target == sdev->id) &&
+		    (res->cfgte.res_addr.lun == sdev->lun)) {
+			res->sdev = sdev;
+			res->add_to_ml = 0;
+			sdev->hostdata = res;
+			res->needs_sync_complete = 1;
+			break;
+		}
+	}
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+
+	return 0;
+}
+
+/**
+ * ipr_eh_host_reset - Reset the host adapter
+ * @scsi_cmd:	scsi command struct
+ *
+ * Return value:
+ * 	SUCCESS / FAILED
+ **/
+static int ipr_eh_host_reset(struct scsi_cmnd * scsi_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg;
+	int rc;
+
+	ENTER;
+	ioa_cfg = (struct ipr_ioa_cfg *) scsi_cmd->device->host->hostdata;
+
+	dev_err(&ioa_cfg->pdev->dev,
+		"Adapter being reset as a result of error recovery.\n");
+
+	if (WAIT_FOR_DUMP == ioa_cfg->sdt_state)
+		ioa_cfg->sdt_state = GET_DUMP;
+
+	rc = ipr_reset_reload(ioa_cfg, IPR_SHUTDOWN_ABBREV);
+
+	LEAVE;
+	return rc;
+}
+
+/**
+ * ipr_eh_dev_reset - Reset the device
+ * @scsi_cmd:	scsi command struct
+ *
+ * This function issues a device reset to the affected device.
+ * A LUN reset will be sent to the device first. If that does
+ * not work, a target reset will be sent.
+ *
+ * Return value:
+ *	SUCCESS / FAILED
+ **/
+static int ipr_eh_dev_reset(struct scsi_cmnd * scsi_cmd)
+{
+	struct ipr_cmnd *ipr_cmd;
+	struct ipr_ioa_cfg *ioa_cfg;
+	struct ipr_resource_entry *res;
+	struct ipr_cmd_pkt *cmd_pkt;
+	u32 ioasc;
+
+	ENTER;
+	ioa_cfg = (struct ipr_ioa_cfg *) scsi_cmd->device->host->hostdata;
+	res = scsi_cmd->device->hostdata;
+
+	if (!ipr_is_gscsi(res) && !ipr_is_vset_device(res))
+		return FAILED;
+
+	/*
+	 * If we are currently going through reset/reload, return failed. This will force the
+	 * mid-layer to call ipr_eh_host_reset, which will then go to sleep and wait for the
+	 * reset to complete
+	 */
+	if (ioa_cfg->in_reset_reload)
+		return FAILED;
+	if (ioa_cfg->ioa_is_dead)
+		return FAILED;
+
+	list_for_each_entry(ipr_cmd, &ioa_cfg->pending_q, queue) {
+		if (ipr_cmd->ioarcb.res_handle == res->cfgte.res_handle) {
+			if (ipr_cmd->scsi_cmd)
+				ipr_cmd->done = ipr_scsi_eh_done;
+		}
+	}
+
+	res->resetting_device = 1;
+
+	ipr_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);
+
+	ipr_cmd->ioarcb.res_handle = res->cfgte.res_handle;
+	cmd_pkt = &ipr_cmd->ioarcb.cmd_pkt;
+	cmd_pkt->request_type = IPR_RQTYPE_IOACMD;
+	cmd_pkt->flags_hi |= IPR_FLAGS_HI_SYNC_COMPLETE;
+	cmd_pkt->cdb[0] = IPR_RESET_DEVICE;
+	cmd_pkt->cdb[2] = IPR_RESET_TYPE_SELECT | IPR_LUN_RESET | IPR_TARGET_RESET;
+
+	ipr_send_blocking_cmd(ipr_cmd, ipr_timeout, IPR_DEVICE_RESET_TIMEOUT);
+
+	ioasc = be32_to_cpu(ipr_cmd->ioasa.ioasc);
+
+	res->resetting_device = 0;
+
+	list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+
+	LEAVE;
+	return (IPR_IOASC_SENSE_KEY(ioasc) ? FAILED : SUCCESS);
+}
+
+/**
+ * ipr_cancel_op - Cancel specified op
+ * @scsi_cmd:	scsi command struct
+ *
+ * This function cancels specified op.
+ *
+ * Return value:
+ *	SUCCESS / FAILED
+ **/
+static int ipr_cancel_op(struct scsi_cmnd * scsi_cmd)
+{
+	struct ipr_cmnd *ipr_cmd;
+	struct ipr_ioa_cfg *ioa_cfg;
+	struct ipr_resource_entry *res;
+	struct ipr_cmd_pkt *cmd_pkt;
+	u32 ioasc, ioarcb_addr;
+	int op_found = 0;
+
+	ENTER;
+	ioa_cfg = (struct ipr_ioa_cfg *)scsi_cmd->device->host->hostdata;
+	res = scsi_cmd->device->hostdata;
+
+	if (!res || (!ipr_is_gscsi(res) && !ipr_is_vset_device(res)))
+		return FAILED;
+
+	list_for_each_entry(ipr_cmd, &ioa_cfg->pending_q, queue) {
+		if (ipr_cmd->scsi_cmd == scsi_cmd) {
+			ipr_cmd->done = ipr_scsi_eh_done;
+			op_found = 1;
+			break;
+		}
+	}
+
+	if (!op_found)
+		return SUCCESS;
+
+	ioarcb_addr = be32_to_cpu(ipr_cmd->ioarcb.ioarcb_host_pci_addr);
+
+	ipr_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);
+	ipr_cmd->ioarcb.res_handle = res->cfgte.res_handle;
+	cmd_pkt = &ipr_cmd->ioarcb.cmd_pkt;
+	cmd_pkt->request_type = IPR_RQTYPE_IOACMD;
+	cmd_pkt->cdb[0] = IPR_ABORT_TASK;
+	cmd_pkt->cdb[2] = (ioarcb_addr >> 24) & 0xff;
+	cmd_pkt->cdb[3] = (ioarcb_addr >> 16) & 0xff;
+	cmd_pkt->cdb[4] = (ioarcb_addr >> 8) & 0xff;
+	cmd_pkt->cdb[5] = ioarcb_addr & 0xff;
+
+	ipr_send_blocking_cmd(ipr_cmd, ipr_timeout, IPR_ABORT_TASK_TIMEOUT);
+	ioasc = be32_to_cpu(ipr_cmd->ioasa.ioasc);
+	list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+	res->needs_sync_complete = 1;
+
+	LEAVE;
+	return (IPR_IOASC_SENSE_KEY(ioasc) ? FAILED : SUCCESS);
+}
+
+/**
+ * ipr_eh_abort - Abort a single op
+ * @scsi_cmd:	scsi command struct
+ *
+ * Return value:
+ * 	SUCCESS / FAILED
+ **/
+static int ipr_eh_abort(struct scsi_cmnd * scsi_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg;
+
+	ENTER;
+	ioa_cfg = (struct ipr_ioa_cfg *) scsi_cmd->device->host->hostdata;
+
+	/* If we are currently going through reset/reload, return failed. This will force the
+	   mid-layer to call ipr_eh_host_reset, which will then go to sleep and wait for the
+	   reset to complete */
+	if (ioa_cfg->in_reset_reload)
+		return FAILED;
+	if (ioa_cfg->ioa_is_dead)
+		return FAILED;
+	if (!scsi_cmd->device->hostdata)
+		return FAILED;
+
+	LEAVE;
+	return ipr_cancel_op(scsi_cmd);
+}
+
+/**
+ * ipr_handle_other_interrupt - Handle "other" interrupts
+ * @ioa_cfg:	ioa config struct
+ * @int_reg:	interrupt register
+ *
+ * Return value:
+ * 	IRQ_NONE / IRQ_HANDLED
+ **/
+static irqreturn_t ipr_handle_other_interrupt(struct ipr_ioa_cfg *ioa_cfg,
+					      volatile u32 int_reg)
+{
+	irqreturn_t rc = IRQ_HANDLED;
+
+	if (int_reg & IPR_PCII_IOA_TRANS_TO_OPER) {
+		/* Mask the interrupt */
+		writel(IPR_PCII_IOA_TRANS_TO_OPER, ioa_cfg->regs.set_interrupt_mask_reg);
+
+		/* Clear the interrupt */
+		writel(IPR_PCII_IOA_TRANS_TO_OPER, ioa_cfg->regs.clr_interrupt_reg);
+		int_reg = readl(ioa_cfg->regs.sense_interrupt_reg);
+
+		del_timer(&ioa_cfg->reset_cmd->timer);
+		ipr_reset_ioa_job(ioa_cfg->reset_cmd);
+	} else {
+		if (int_reg & IPR_PCII_IOA_UNIT_CHECKED)
+			ioa_cfg->ioa_unit_checked = 1;
+		else
+			dev_err(&ioa_cfg->pdev->dev,
+				"Permanent IOA failure. 0x%08X\n", int_reg);
+
+		if (WAIT_FOR_DUMP == ioa_cfg->sdt_state)
+			ioa_cfg->sdt_state = GET_DUMP;
+
+		ipr_mask_and_clear_all_interrupts(ioa_cfg);
+		ipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);
+	}
+
+	return rc;
+}
+
+/**
+ * ipr_isr - Interrupt service routine
+ * @irq:	irq number
+ * @devp:	pointer to ioa config struct
+ * @regs:	pt_regs struct
+ *
+ * Return value:
+ * 	IRQ_NONE / IRQ_HANDLED
+ **/
+static irqreturn_t ipr_isr(int irq, void *devp, struct pt_regs *regs)
+{
+	struct ipr_ioa_cfg *ioa_cfg = (struct ipr_ioa_cfg *)devp;
+	unsigned long lock_flags = 0;
+	volatile u32 int_reg, int_mask_reg;
+	u32 ioasc;
+	u16 cmd_index;
+	struct ipr_cmnd *ipr_cmd;
+	irqreturn_t rc = IRQ_NONE;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+
+	/* If interrupts are disabled, ignore the interrupt */
+	if (!ioa_cfg->allow_interrupts) {
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		return IRQ_NONE;
+	}
+
+	int_mask_reg = readl(ioa_cfg->regs.sense_interrupt_mask_reg);
+	int_reg = readl(ioa_cfg->regs.sense_interrupt_reg) & ~int_mask_reg;
+
+	/* If an interrupt on the adapter did not occur, ignore it */
+	if (unlikely((int_reg & IPR_PCII_OPER_INTERRUPTS) == 0)) {
+		spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+		return IRQ_NONE;
+	}
+
+	while (1) {
+		ipr_cmd = NULL;
+
+		while ((be32_to_cpu(*ioa_cfg->hrrq_curr) & IPR_HRRQ_TOGGLE_BIT) ==
+		       ioa_cfg->toggle_bit) {
+
+			cmd_index = (be32_to_cpu(*ioa_cfg->hrrq_curr) &
+				     IPR_HRRQ_REQ_RESP_HANDLE_MASK) >> IPR_HRRQ_REQ_RESP_HANDLE_SHIFT;
+
+			if (unlikely(cmd_index >= IPR_NUM_CMD_BLKS)) {
+				ioa_cfg->errors_logged++;
+				dev_err(&ioa_cfg->pdev->dev, "Invalid response handle from IOA\n");
+
+				if (WAIT_FOR_DUMP == ioa_cfg->sdt_state)
+					ioa_cfg->sdt_state = GET_DUMP;
+
+				ipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);
+				spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+				return IRQ_HANDLED;
+			}
+
+			ipr_cmd = ioa_cfg->ipr_cmnd_list[cmd_index];
+
+			ioasc = be32_to_cpu(ipr_cmd->ioasa.ioasc);
+
+			ipr_trc_hook(ipr_cmd, IPR_TRACE_FINISH, ioasc);
+
+			list_del(&ipr_cmd->queue);
+			del_timer(&ipr_cmd->timer);
+			ipr_cmd->done(ipr_cmd);
+
+			rc = IRQ_HANDLED;
+
+			if (ioa_cfg->hrrq_curr < ioa_cfg->hrrq_end) {
+				ioa_cfg->hrrq_curr++;
+			} else {
+				ioa_cfg->hrrq_curr = ioa_cfg->hrrq_start;
+				ioa_cfg->toggle_bit ^= 1u;
+			}
+		}
+
+		if (ipr_cmd != NULL) {
+			/* Clear the PCI interrupt */
+			writel(IPR_PCII_HRRQ_UPDATED, ioa_cfg->regs.clr_interrupt_reg);
+			int_reg = readl(ioa_cfg->regs.sense_interrupt_reg) & ~int_mask_reg;
+		} else
+			break;
+	}
+
+	if (unlikely(rc == IRQ_NONE))
+		rc = ipr_handle_other_interrupt(ioa_cfg, int_reg);
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	return rc;
+}
+
+/**
+ * ipr_build_ioadl - Build a scatter/gather list and map the buffer
+ * @ioa_cfg:	ioa config struct
+ * @ipr_cmd:	ipr command struct
+ *
+ * Return value:
+ * 	0 on success / -1 on failure
+ **/
+static int ipr_build_ioadl(struct ipr_ioa_cfg *ioa_cfg,
+			   struct ipr_cmnd *ipr_cmd)
+{
+	int i;
+	struct scatterlist *sglist;
+	u32 length;
+	u32 ioadl_flags = 0;
+	struct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;
+	struct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;
+	struct ipr_ioadl_desc *ioadl = ipr_cmd->ioadl;
+
+	length = scsi_cmd->request_bufflen;
+
+	if (length == 0)
+		return 0;
+
+	if (scsi_cmd->use_sg) {
+		ipr_cmd->dma_use_sg = pci_map_sg(ioa_cfg->pdev,
+						 scsi_cmd->request_buffer,
+						 scsi_cmd->use_sg,
+						 scsi_cmd->sc_data_direction);
+
+		if (scsi_cmd->sc_data_direction == DMA_TO_DEVICE) {
+			ioadl_flags = IPR_IOADL_FLAGS_WRITE;
+			ioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;
+			ioarcb->write_data_transfer_length = cpu_to_be32(length);
+			ioarcb->write_ioadl_len =
+				cpu_to_be32(sizeof(struct ipr_ioadl_desc) * ipr_cmd->dma_use_sg);
+		} else if (scsi_cmd->sc_data_direction == DMA_FROM_DEVICE) {
+			ioadl_flags = IPR_IOADL_FLAGS_READ;
+			ioarcb->read_data_transfer_length = cpu_to_be32(length);
+			ioarcb->read_ioadl_len =
+				cpu_to_be32(sizeof(struct ipr_ioadl_desc) * ipr_cmd->dma_use_sg);
+		}
+
+		sglist = scsi_cmd->request_buffer;
+
+		for (i = 0; i < ipr_cmd->dma_use_sg; i++) {
+			ioadl[i].flags_and_data_len =
+				cpu_to_be32(ioadl_flags | sg_dma_len(&sglist[i]));
+			ioadl[i].address =
+				cpu_to_be32(sg_dma_address(&sglist[i]));
+		}
+
+		if (likely(ipr_cmd->dma_use_sg)) {
+			ioadl[i-1].flags_and_data_len |=
+				cpu_to_be32(IPR_IOADL_FLAGS_LAST);
+			return 0;
+		} else
+			dev_err(&ioa_cfg->pdev->dev, "pci_map_sg failed!\n");
+	} else {
+		if (scsi_cmd->sc_data_direction == DMA_TO_DEVICE) {
+			ioadl_flags = IPR_IOADL_FLAGS_WRITE;
+			ioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;
+			ioarcb->write_data_transfer_length = cpu_to_be32(length);
+			ioarcb->write_ioadl_len = cpu_to_be32(sizeof(struct ipr_ioadl_desc));
+		} else if (scsi_cmd->sc_data_direction == DMA_FROM_DEVICE) {
+			ioadl_flags = IPR_IOADL_FLAGS_READ;
+			ioarcb->read_data_transfer_length = cpu_to_be32(length);
+			ioarcb->read_ioadl_len = cpu_to_be32(sizeof(struct ipr_ioadl_desc));
+		}
+
+		ipr_cmd->dma_handle = pci_map_single(ioa_cfg->pdev,
+						     scsi_cmd->buffer, length,
+						     scsi_cmd->sc_data_direction);
+
+		if (likely(!pci_dma_error(ipr_cmd->dma_handle))) {
+			ipr_cmd->dma_use_sg = 1;
+			ioadl[0].flags_and_data_len =
+				cpu_to_be32(ioadl_flags | length | IPR_IOADL_FLAGS_LAST);
+			ioadl[0].address = cpu_to_be32(ipr_cmd->dma_handle);
+			return 0;
+		} else
+			dev_err(&ioa_cfg->pdev->dev, "pci_map_single failed!\n");
+	}
+
+	return -1;
+}
+
+/**
+ * ipr_get_task_attributes - Translate SPI Q-Tag to task attributes
+ * @scsi_cmd:	scsi command struct
+ *
+ * Return value:
+ * 	task attributes
+ **/
+static u8 ipr_get_task_attributes(struct scsi_cmnd *scsi_cmd)
+{
+	u8 tag[2];
+	u8 rc = IPR_FLAGS_LO_UNTAGGED_TASK;
+
+	if (scsi_populate_tag_msg(scsi_cmd, tag)) {
+		switch (tag[0]) {
+		case MSG_SIMPLE_TAG:
+			rc = IPR_FLAGS_LO_SIMPLE_TASK;
+			break;
+		case MSG_HEAD_TAG:
+			rc = IPR_FLAGS_LO_HEAD_OF_Q_TASK;
+			break;
+		case MSG_ORDERED_TAG:
+			rc = IPR_FLAGS_LO_ORDERED_TASK;
+			break;
+		};
+	}
+
+	return rc;
+}
+
+/**
+ * ipr_erp_done - Process completion of ERP for a device
+ * @ipr_cmd:		ipr command struct
+ *
+ * This function copies the sense buffer into the scsi_cmd
+ * struct and pushes the scsi_done function.
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_erp_done(struct ipr_cmnd *ipr_cmd)
+{
+	struct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;
+	struct ipr_resource_entry *res = scsi_cmd->device->hostdata;
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	u32 ioasc = be32_to_cpu(ipr_cmd->ioasa.ioasc);
+
+	if (IPR_IOASC_SENSE_KEY(ioasc) > 0) {
+		scsi_cmd->result |= (DID_ERROR << 16);
+		ipr_sdev_err(scsi_cmd->device,
+			     "Request Sense failed with IOASC: 0x%08X\n", ioasc);
+	} else {
+		memcpy(scsi_cmd->sense_buffer, ipr_cmd->sense_buffer,
+		       SCSI_SENSE_BUFFERSIZE);
+	}
+
+	res->needs_sync_complete = 1;
+	ipr_unmap_sglist(ioa_cfg, ipr_cmd);
+	list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+	scsi_cmd->scsi_done(scsi_cmd);
+}
+
+/**
+ * ipr_reinit_ipr_cmnd_for_erp - Re-initialize a cmnd block to be used for ERP
+ * @ipr_cmd:	ipr command struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_reinit_ipr_cmnd_for_erp(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioarcb *ioarcb;
+	struct ipr_ioasa *ioasa;
+
+	ioarcb = &ipr_cmd->ioarcb;
+	ioasa = &ipr_cmd->ioasa;
+
+	memset(&ioarcb->cmd_pkt, 0, sizeof(struct ipr_cmd_pkt));
+	ioarcb->write_data_transfer_length = 0;
+	ioarcb->read_data_transfer_length = 0;
+	ioarcb->write_ioadl_len = 0;
+	ioarcb->read_ioadl_len = 0;
+	ioasa->ioasc = 0;
+	ioasa->residual_data_len = 0;
+}
+
+/**
+ * ipr_erp_request_sense - Send request sense to a device
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function sends a request sense to a device as a result
+ * of a check condition.
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_erp_request_sense(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_cmd_pkt *cmd_pkt = &ipr_cmd->ioarcb.cmd_pkt;
+
+	ipr_reinit_ipr_cmnd_for_erp(ipr_cmd);
+
+	cmd_pkt->request_type = IPR_RQTYPE_SCSICDB;
+	cmd_pkt->cdb[0] = REQUEST_SENSE;
+	cmd_pkt->cdb[4] = SCSI_SENSE_BUFFERSIZE;
+	cmd_pkt->flags_hi |= IPR_FLAGS_HI_SYNC_OVERRIDE;
+	cmd_pkt->flags_hi |= IPR_FLAGS_HI_NO_ULEN_CHK;
+	cmd_pkt->timeout = cpu_to_be16(IPR_REQUEST_SENSE_TIMEOUT / HZ);
+
+	ipr_cmd->ioadl[0].flags_and_data_len =
+		cpu_to_be32(IPR_IOADL_FLAGS_READ_LAST | SCSI_SENSE_BUFFERSIZE);
+	ipr_cmd->ioadl[0].address =
+		cpu_to_be32(ipr_cmd->sense_buffer_dma);
+
+	ipr_cmd->ioarcb.read_ioadl_len =
+		cpu_to_be32(sizeof(struct ipr_ioadl_desc));
+	ipr_cmd->ioarcb.read_data_transfer_length =
+		cpu_to_be32(SCSI_SENSE_BUFFERSIZE);
+
+	ipr_do_req(ipr_cmd, ipr_erp_done, ipr_timeout,
+		   IPR_REQUEST_SENSE_TIMEOUT * IPR_TIMEOUT_MULTIPLIER);
+}
+
+/**
+ * ipr_erp_cancel_all - Send cancel all to a device
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function sends a cancel all to a device to clear the
+ * queue. If we are running TCQ on the device, QERR is set to 1,
+ * which means all outstanding ops have been dropped on the floor.
+ * Cancel all will return them to us.
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_erp_cancel_all(struct ipr_cmnd *ipr_cmd)
+{
+	struct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;
+	struct ipr_resource_entry *res = scsi_cmd->device->hostdata;
+	struct ipr_cmd_pkt *cmd_pkt;
+
+	res->in_erp = 1;
+
+	ipr_reinit_ipr_cmnd_for_erp(ipr_cmd);
+
+	cmd_pkt = &ipr_cmd->ioarcb.cmd_pkt;
+	cmd_pkt->request_type = IPR_RQTYPE_IOACMD;
+	cmd_pkt->cdb[0] = IPR_CANCEL_ALL_REQUESTS;
+
+	ipr_do_req(ipr_cmd, ipr_erp_request_sense, ipr_timeout,
+		   IPR_CANCEL_ALL_TIMEOUT);
+}
+
+/**
+ * ipr_dump_ioasa - Dump contents of IOASA
+ * @ioa_cfg:	ioa config struct
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function is invoked by the interrupt handler when ops
+ * fail. It will log the IOASA if appropriate. Only called
+ * for GPDD ops.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_dump_ioasa(struct ipr_ioa_cfg *ioa_cfg,
+			   struct ipr_cmnd *ipr_cmd)
+{
+	int i;
+	u16 data_len;
+	u32 ioasc;
+	struct ipr_ioasa *ioasa = &ipr_cmd->ioasa;
+	u32 *ioasa_data = (u32 *)ioasa;
+	int error_index;
+
+	ioasc = be32_to_cpu(ioasa->ioasc) & IPR_IOASC_IOASC_MASK;
+
+	if (0 == ioasc)
+		return;
+
+	if (ioa_cfg->log_level < IPR_DEFAULT_LOG_LEVEL)
+		return;
+
+	error_index = ipr_get_error(ioasc);
+
+	if (ioa_cfg->log_level < IPR_MAX_LOG_LEVEL) {
+		/* Don't log an error if the IOA already logged one */
+		if (ioasa->ilid != 0)
+			return;
+
+		if (ipr_error_table[error_index].log_ioasa == 0)
+			return;
+	}
+
+	ipr_sdev_err(ipr_cmd->scsi_cmd->device, "%s\n",
+		     ipr_error_table[error_index].error);
+
+	if ((ioasa->gpdd.device_end_state <= ARRAY_SIZE(ipr_gpdd_dev_end_states)) &&
+	    (ioasa->gpdd.device_bus_phase <= ARRAY_SIZE(ipr_gpdd_dev_bus_phases))) {
+		ipr_sdev_err(ipr_cmd->scsi_cmd->device,
+			     "Device End state: %s Phase: %s\n",
+			     ipr_gpdd_dev_end_states[ioasa->gpdd.device_end_state],
+			     ipr_gpdd_dev_bus_phases[ioasa->gpdd.device_bus_phase]);
+	}
+
+	if (sizeof(struct ipr_ioasa) < be16_to_cpu(ioasa->ret_stat_len))
+		data_len = sizeof(struct ipr_ioasa);
+	else
+		data_len = be16_to_cpu(ioasa->ret_stat_len);
+
+	ipr_err("IOASA Dump:\n");
+
+	for (i = 0; i < data_len / 4; i += 4) {
+		ipr_err("%08X: %08X %08X %08X %08X\n", i*4,
+			be32_to_cpu(ioasa_data[i]),
+			be32_to_cpu(ioasa_data[i+1]),
+			be32_to_cpu(ioasa_data[i+2]),
+			be32_to_cpu(ioasa_data[i+3]));
+	}
+}
+
+/**
+ * ipr_gen_sense - Generate SCSI sense data from an IOASA
+ * @ioasa:		IOASA
+ * @sense_buf:	sense data buffer
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_gen_sense(struct ipr_cmnd *ipr_cmd)
+{
+	u32 failing_lba;
+	u8 *sense_buf = ipr_cmd->scsi_cmd->sense_buffer;
+	struct ipr_resource_entry *res = ipr_cmd->scsi_cmd->device->hostdata;
+	struct ipr_ioasa *ioasa = &ipr_cmd->ioasa;
+	u32 ioasc = be32_to_cpu(ioasa->ioasc);
+
+	memset(sense_buf, 0, SCSI_SENSE_BUFFERSIZE);
+
+	if (ioasc >= IPR_FIRST_DRIVER_IOASC)
+		return;
+
+	ipr_cmd->scsi_cmd->result = SAM_STAT_CHECK_CONDITION;
+
+	if (ipr_is_vset_device(res) &&
+	    ioasc == IPR_IOASC_MED_DO_NOT_REALLOC &&
+	    ioasa->vset.failing_lba_hi != 0) {
+		sense_buf[0] = 0x72;
+		sense_buf[1] = IPR_IOASC_SENSE_KEY(ioasc);
+		sense_buf[2] = IPR_IOASC_SENSE_CODE(ioasc);
+		sense_buf[3] = IPR_IOASC_SENSE_QUAL(ioasc);
+
+		sense_buf[7] = 12;
+		sense_buf[8] = 0;
+		sense_buf[9] = 0x0A;
+		sense_buf[10] = 0x80;
+
+		failing_lba = be32_to_cpu(ioasa->vset.failing_lba_hi);
+
+		sense_buf[12] = (failing_lba & 0xff000000) >> 24;
+		sense_buf[13] = (failing_lba & 0x00ff0000) >> 16;
+		sense_buf[14] = (failing_lba & 0x0000ff00) >> 8;
+		sense_buf[15] = failing_lba & 0x000000ff;
+
+		failing_lba = be32_to_cpu(ioasa->vset.failing_lba_lo);
+
+		sense_buf[16] = (failing_lba & 0xff000000) >> 24;
+		sense_buf[17] = (failing_lba & 0x00ff0000) >> 16;
+		sense_buf[18] = (failing_lba & 0x0000ff00) >> 8;
+		sense_buf[19] = failing_lba & 0x000000ff;
+	} else {
+		sense_buf[0] = 0x70;
+		sense_buf[2] = IPR_IOASC_SENSE_KEY(ioasc);
+		sense_buf[12] = IPR_IOASC_SENSE_CODE(ioasc);
+		sense_buf[13] = IPR_IOASC_SENSE_QUAL(ioasc);
+
+		/* Illegal request */
+		if ((IPR_IOASC_SENSE_KEY(ioasc) == 0x05) &&
+		    (be32_to_cpu(ioasa->ioasc_specific) & IPR_FIELD_POINTER_VALID)) {
+			sense_buf[7] = 10;	/* additional length */
+
+			/* IOARCB was in error */
+			if (IPR_IOASC_SENSE_CODE(ioasc) == 0x24)
+				sense_buf[15] = 0xC0;
+			else	/* Parameter data was invalid */
+				sense_buf[15] = 0x80;
+
+			sense_buf[16] =
+			    ((IPR_FIELD_POINTER_MASK &
+			      be32_to_cpu(ioasa->ioasc_specific)) >> 8) & 0xff;
+			sense_buf[17] =
+			    (IPR_FIELD_POINTER_MASK &
+			     be32_to_cpu(ioasa->ioasc_specific)) & 0xff;
+		} else {
+			if (ioasc == IPR_IOASC_MED_DO_NOT_REALLOC) {
+				if (ipr_is_vset_device(res))
+					failing_lba = be32_to_cpu(ioasa->vset.failing_lba_lo);
+				else
+					failing_lba = be32_to_cpu(ioasa->dasd.failing_lba);
+
+				sense_buf[0] |= 0x80;	/* Or in the Valid bit */
+				sense_buf[3] = (failing_lba & 0xff000000) >> 24;
+				sense_buf[4] = (failing_lba & 0x00ff0000) >> 16;
+				sense_buf[5] = (failing_lba & 0x0000ff00) >> 8;
+				sense_buf[6] = failing_lba & 0x000000ff;
+			}
+
+			sense_buf[7] = 6;	/* additional length */
+		}
+	}
+}
+
+/**
+ * ipr_erp_start - Process an error response for a SCSI op
+ * @ioa_cfg:	ioa config struct
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function determines whether or not to initiate ERP
+ * on the affected device.
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_erp_start(struct ipr_ioa_cfg *ioa_cfg,
+			      struct ipr_cmnd *ipr_cmd)
+{
+	struct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;
+	struct ipr_resource_entry *res = scsi_cmd->device->hostdata;
+	u32 ioasc = be32_to_cpu(ipr_cmd->ioasa.ioasc);
+
+	if (!res) {
+		ipr_scsi_eh_done(ipr_cmd);
+		return;
+	}
+
+	if (ipr_is_gscsi(res))
+		ipr_dump_ioasa(ioa_cfg, ipr_cmd);
+	else
+		ipr_gen_sense(ipr_cmd);
+
+	switch (ioasc & IPR_IOASC_IOASC_MASK) {
+	case IPR_IOASC_ABORTED_CMD_TERM_BY_HOST:
+		scsi_cmd->result |= (DID_ERROR << 16);
+		break;
+	case IPR_IOASC_IR_RESOURCE_HANDLE:
+		scsi_cmd->result |= (DID_NO_CONNECT << 16);
+		break;
+	case IPR_IOASC_HW_SEL_TIMEOUT:
+		scsi_cmd->result |= (DID_NO_CONNECT << 16);
+		res->needs_sync_complete = 1;
+		break;
+	case IPR_IOASC_SYNC_REQUIRED:
+		if (!res->in_erp) {
+			res->needs_sync_complete = 1;
+			scsi_cmd->result |= (BUSY << 1);
+		} else {
+			scsi_cmd->result |= (DID_ERROR << 16);
+		}
+		break;
+	case IPR_IOASC_MED_DO_NOT_REALLOC: /* prevent retries */
+		scsi_cmd->result |= (DID_PASSTHROUGH << 16);
+		break;
+	case IPR_IOASC_BUS_WAS_RESET:
+	case IPR_IOASC_BUS_WAS_RESET_BY_OTHER:
+		/*
+		 * Report the bus reset and ask for a retry. The device
+		 * will give CC/UA the next command.
+		 */
+		if (!res->resetting_device)
+			scsi_report_bus_reset(ioa_cfg->host, scsi_cmd->device->channel);
+		scsi_cmd->result |= (DID_ERROR << 16);
+		res->needs_sync_complete = 1;
+		break;
+	case IPR_IOASC_HW_DEV_BUS_STATUS:
+		scsi_cmd->result |= IPR_IOASC_SENSE_STATUS(ioasc);
+		if (IPR_IOASC_SENSE_STATUS(ioasc) == SAM_STAT_CHECK_CONDITION) {
+			ipr_erp_cancel_all(ipr_cmd);
+			return;
+		}
+		break;
+	case IPR_IOASC_NR_INIT_CMD_REQUIRED:
+		break;
+	default:
+		scsi_cmd->result |= (DID_ERROR << 16);
+		if (!ipr_is_vset_device(res))
+			res->needs_sync_complete = 1;
+		break;
+	}
+
+	ipr_unmap_sglist(ioa_cfg, ipr_cmd);
+	list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+	scsi_cmd->scsi_done(scsi_cmd);
+}
+
+/**
+ * ipr_scsi_done - mid-layer done function
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function is invoked by the interrupt handler for
+ * ops generated by the SCSI mid-layer
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_scsi_done(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct scsi_cmnd *scsi_cmd = ipr_cmd->scsi_cmd;
+	u32 ioasc = be32_to_cpu(ipr_cmd->ioasa.ioasc);
+
+	scsi_cmd->resid = be32_to_cpu(ipr_cmd->ioasa.residual_data_len);
+
+	if (likely(IPR_IOASC_SENSE_KEY(ioasc) == 0)) {
+		ipr_unmap_sglist(ioa_cfg, ipr_cmd);
+		list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+		scsi_cmd->scsi_done(scsi_cmd);
+	} else
+		ipr_erp_start(ioa_cfg, ipr_cmd);
+}
+
+/**
+ * ipr_save_ioafp_mode_select - Save adapters mode select data
+ * @ioa_cfg:	ioa config struct
+ * @scsi_cmd:	scsi command struct
+ *
+ * This function saves mode select data for the adapter to
+ * use following an adapter reset.
+ *
+ * Return value:
+ *	0 on success / SCSI_MLQUEUE_HOST_BUSY on failure
+ **/
+static int ipr_save_ioafp_mode_select(struct ipr_ioa_cfg *ioa_cfg,
+				       struct scsi_cmnd *scsi_cmd)
+{
+	if (!ioa_cfg->saved_mode_pages) {
+		ioa_cfg->saved_mode_pages  = kmalloc(sizeof(struct ipr_mode_pages),
+						     GFP_ATOMIC);
+		if (!ioa_cfg->saved_mode_pages) {
+			dev_err(&ioa_cfg->pdev->dev,
+				"IOA mode select buffer allocation failed\n");
+			return SCSI_MLQUEUE_HOST_BUSY;
+		}
+	}
+
+	memcpy(ioa_cfg->saved_mode_pages, scsi_cmd->buffer, scsi_cmd->cmnd[4]);
+	ioa_cfg->saved_mode_page_len = scsi_cmd->cmnd[4];
+	return 0;
+}
+
+/**
+ * ipr_queuecommand - Queue a mid-layer request
+ * @scsi_cmd:	scsi command struct
+ * @done:		done function
+ *
+ * This function queues a request generated by the mid-layer.
+ *
+ * Return value:
+ *	0 on success
+ *	SCSI_MLQUEUE_DEVICE_BUSY if device is busy
+ *	SCSI_MLQUEUE_HOST_BUSY if host is busy
+ **/
+static int ipr_queuecommand(struct scsi_cmnd *scsi_cmd,
+			    void (*done) (struct scsi_cmnd *))
+{
+	struct ipr_ioa_cfg *ioa_cfg;
+	struct ipr_resource_entry *res;
+	void (*timeout_func) (struct scsi_cmnd *);
+	struct ipr_ioarcb *ioarcb;
+	struct ipr_cmnd *ipr_cmd;
+	int rc = 0;
+	int timeout;
+
+	scsi_cmd->scsi_done = done;
+	ioa_cfg = (struct ipr_ioa_cfg *)scsi_cmd->device->host->hostdata;
+	res = scsi_cmd->device->hostdata;
+	scsi_cmd->result = (DID_OK << 16);
+
+	/*
+	 * We are currently blocking all devices due to a host reset
+	 * We have told the host to stop giving us new requests, but
+	 * ERP ops don't count. FIXME
+	 */
+	if (unlikely(!ioa_cfg->allow_cmds))
+		return SCSI_MLQUEUE_HOST_BUSY;
+
+	/*
+	 * FIXME - Create scsi_set_device_offline, scsi_set_host_offline
+	 * interfaces and we can remove some of these checks
+	 */
+	if (unlikely(ioa_cfg->ioa_is_dead || !res || res->del_from_ml)) {
+		memset(scsi_cmd->sense_buffer, 0, SCSI_SENSE_BUFFERSIZE);
+		scsi_cmd->result = (DID_NO_CONNECT << 16);
+		scsi_cmd->scsi_done(scsi_cmd);
+		return 0;
+	}
+
+	ipr_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);
+	ioarcb = &ipr_cmd->ioarcb;
+	list_add_tail(&ipr_cmd->queue, &ioa_cfg->pending_q);
+
+	memcpy(ioarcb->cmd_pkt.cdb, scsi_cmd->cmnd, scsi_cmd->cmd_len);
+	ipr_cmd->scsi_cmd = scsi_cmd;
+	ioarcb->res_handle = res->cfgte.res_handle;
+
+	/*
+	 * Double the timeout value to use as we will use the adapter
+	 * as the primary timing mechanism. FIXME
+	 */
+	timeout_func = (void (*)(struct scsi_cmnd *)) scsi_cmd->eh_timeout.function;
+	timeout = scsi_cmd->timeout_per_command;
+	scsi_add_timer(scsi_cmd, timeout * IPR_TIMEOUT_MULTIPLIER, timeout_func);
+
+	ipr_cmd->done = ipr_scsi_done;
+	ipr_trc_hook(ipr_cmd, IPR_TRACE_START, IPR_GET_PHYS_LOC(res->cfgte.res_addr));
+
+	if (ipr_is_gscsi(res) || ipr_is_vset_device(res)) {
+		if ((timeout / HZ) > IPR_MAX_SECOND_RADIX_TIMEOUT) {
+			ioarcb->cmd_pkt.timeout =
+				cpu_to_be16(((timeout / HZ) / 60) | IPR_TIMEOUT_MINUTE_RADIX);
+		} else {
+			ioarcb->cmd_pkt.timeout = cpu_to_be16(timeout / HZ);
+		}
+
+		if (scsi_cmd->underflow == 0)
+			ioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_NO_ULEN_CHK;
+
+		if (res->needs_sync_complete) {
+			ioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_SYNC_COMPLETE;
+			res->needs_sync_complete = 0;
+		}
+
+		ioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_NO_LINK_DESC;
+		ioarcb->cmd_pkt.flags_lo |= IPR_FLAGS_LO_DELAY_AFTER_RST;
+		ioarcb->cmd_pkt.flags_lo |= IPR_FLAGS_LO_ALIGNED_BFR;
+		ioarcb->cmd_pkt.flags_lo |= ipr_get_task_attributes(scsi_cmd);
+	}
+
+	if (!ipr_is_gscsi(res) && scsi_cmd->cmnd[0] >= 0xC0)
+		ioarcb->cmd_pkt.request_type = IPR_RQTYPE_IOACMD;
+
+	if (ipr_is_ioa_resource(res) && scsi_cmd->cmnd[0] == MODE_SELECT)
+		rc = ipr_save_ioafp_mode_select(ioa_cfg, scsi_cmd);
+
+	if (likely(rc == 0))
+		rc = ipr_build_ioadl(ioa_cfg, ipr_cmd);
+
+	if (likely(rc == 0)) {
+		mb();
+		writel(be32_to_cpu(ipr_cmd->ioarcb.ioarcb_host_pci_addr),
+		       ioa_cfg->regs.ioarrin_reg);
+	} else {
+		 list_move_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+		 return SCSI_MLQUEUE_HOST_BUSY;
+	}
+
+	return 0;
+}
+
+/**
+ * ipr_info - Get information about the card/driver
+ * @scsi_host:	scsi host struct
+ *
+ * Return value:
+ * 	pointer to buffer with description string
+ **/
+static const char * ipr_ioa_info(struct Scsi_Host *host)
+{
+	static char buffer[512];
+	struct ipr_ioa_cfg *ioa_cfg;
+	unsigned long lock_flags = 0;
+
+	ioa_cfg = (struct ipr_ioa_cfg *) host->hostdata;
+
+	spin_lock_irqsave(host->host_lock, lock_flags);
+	sprintf(buffer, "IBM %X Storage Adapter", ioa_cfg->type);
+	spin_unlock_irqrestore(host->host_lock, lock_flags);
+
+	return buffer;
+}
+
+static struct scsi_host_template driver_template = {
+	.name = "IPR",
+	.info = ipr_ioa_info,
+	.queuecommand = ipr_queuecommand,
+	.eh_abort_handler = ipr_eh_abort,
+	.eh_device_reset_handler = ipr_eh_dev_reset,
+	.eh_host_reset_handler = ipr_eh_host_reset,
+	.slave_alloc = ipr_slave_alloc,
+	.slave_configure = ipr_slave_configure,
+	.slave_destroy = ipr_slave_destroy,
+	.bios_param = ipr_biosparam,
+	.can_queue = IPR_MAX_COMMANDS,
+	.this_id = -1,
+	.sg_tablesize = IPR_MAX_SGLIST,
+	.max_sectors = IPR_MAX_SECTORS,
+	.cmd_per_lun = IPR_MAX_CMD_PER_LUN,
+	.use_clustering = ENABLE_CLUSTERING,
+	.shost_attrs = ipr_ioa_attrs,
+	.sdev_attrs = ipr_dev_attrs,
+	.proc_name = IPR_NAME
+};
+
+#ifdef CONFIG_PPC_PSERIES
+static const u16 ipr_blocked_processors[] = {
+	PV_NORTHSTAR,
+	PV_PULSAR,
+	PV_POWER4,
+	PV_ICESTAR,
+	PV_SSTAR,
+	PV_POWER4p,
+	PV_630,
+	PV_630p
+};
+
+/**
+ * ipr_invalid_adapter - Determine if this adapter is supported on this hardware
+ * @ioa_cfg:	ioa cfg struct
+ *
+ * Adapters that use Gemstone revision < 3.1 do not work reliably on
+ * certain pSeries hardware. This function determines if the given
+ * adapter is in one of these confgurations or not.
+ *
+ * Return value:
+ * 	1 if adapter is not supported / 0 if adapter is supported
+ **/
+static int ipr_invalid_adapter(struct ipr_ioa_cfg *ioa_cfg)
+{
+	u8 rev_id;
+	int i;
+
+	if (ioa_cfg->type == 0x5702) {
+		if (pci_read_config_byte(ioa_cfg->pdev, PCI_REVISION_ID,
+					 &rev_id) == PCIBIOS_SUCCESSFUL) {
+			if (rev_id < 4) {
+				for (i = 0; i < ARRAY_SIZE(ipr_blocked_processors); i++){
+					if (__is_processor(ipr_blocked_processors[i]))
+						return 1;
+				}
+			}
+		}
+	}
+	return 0;
+}
+#else
+#define ipr_invalid_adapter(ioa_cfg) 0
+#endif
+
+/**
+ * ipr_ioa_bringdown_done - IOA bring down completion.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function processes the completion of an adapter bring down.
+ * It wakes any reset sleepers.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_RETURN
+ **/
+static int ipr_ioa_bringdown_done(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+
+	ENTER;
+	ioa_cfg->in_reset_reload = 0;
+	ioa_cfg->reset_retries = 0;
+	list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+	wake_up_all(&ioa_cfg->reset_wait_q);
+
+	spin_unlock_irq(ioa_cfg->host->host_lock);
+	scsi_unblock_requests(ioa_cfg->host);
+	spin_lock_irq(ioa_cfg->host->host_lock);
+	LEAVE;
+
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_ioa_reset_done - IOA reset completion.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function processes the completion of an adapter reset.
+ * It schedules any necessary mid-layer add/removes and
+ * wakes any reset sleepers.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_RETURN
+ **/
+static int ipr_ioa_reset_done(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_resource_entry *res;
+	struct ipr_hostrcb *hostrcb, *temp;
+	int i = 0;
+
+	ENTER;
+	ioa_cfg->in_reset_reload = 0;
+	ioa_cfg->allow_cmds = 1;
+	ioa_cfg->reset_cmd = NULL;
+
+	list_for_each_entry(res, &ioa_cfg->used_res_q, queue) {
+		if (res->add_to_ml || res->del_from_ml) {
+			ipr_trace;
+			schedule_work(&ioa_cfg->work_q);
+			break;
+		}
+	}
+
+	list_for_each_entry_safe(hostrcb, temp, &ioa_cfg->hostrcb_free_q, queue) {
+		list_del(&hostrcb->queue);
+		if (i++ < IPR_NUM_LOG_HCAMS)
+			ipr_send_hcam(ioa_cfg, IPR_HCAM_CDB_OP_CODE_LOG_DATA, hostrcb);
+		else
+			ipr_send_hcam(ioa_cfg, IPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE, hostrcb);
+	}
+
+	dev_info(&ioa_cfg->pdev->dev, "IOA initialized.\n");
+
+	ioa_cfg->reset_retries = 0;
+	list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+	wake_up_all(&ioa_cfg->reset_wait_q);
+
+	spin_unlock_irq(ioa_cfg->host->host_lock);
+	scsi_unblock_requests(ioa_cfg->host);
+	spin_lock_irq(ioa_cfg->host->host_lock);
+
+	if (!ioa_cfg->allow_cmds)
+		scsi_block_requests(ioa_cfg->host);
+
+	LEAVE;
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_set_sup_dev_dflt - Initialize a Set Supported Device buffer
+ * @supported_dev:	supported device struct
+ * @vpids:			vendor product id struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_set_sup_dev_dflt(struct ipr_supported_device *supported_dev,
+				 struct ipr_std_inq_vpids *vpids)
+{
+	memset(supported_dev, 0, sizeof(struct ipr_supported_device));
+	memcpy(&supported_dev->vpids, vpids, sizeof(struct ipr_std_inq_vpids));
+	supported_dev->num_records = 1;
+	supported_dev->data_length =
+		cpu_to_be16(sizeof(struct ipr_supported_device));
+	supported_dev->reserved = 0;
+}
+
+/**
+ * ipr_set_supported_devs - Send Set Supported Devices for a device
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function send a Set Supported Devices to the adapter
+ *
+ * Return value:
+ * 	IPR_RC_JOB_CONTINUE / IPR_RC_JOB_RETURN
+ **/
+static int ipr_set_supported_devs(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_supported_device *supp_dev = &ioa_cfg->vpd_cbs->supp_dev;
+	struct ipr_ioadl_desc *ioadl = ipr_cmd->ioadl;
+	struct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;
+	struct ipr_resource_entry *res = ipr_cmd->res;
+
+	ipr_cmd->job_step = ipr_ioa_reset_done;
+
+	list_for_each_entry_continue(res, &ioa_cfg->used_res_q, queue) {
+		if (!ipr_is_af_dasd_device(res))
+			continue;
+
+		ipr_cmd->res = res;
+		ipr_set_sup_dev_dflt(supp_dev, &res->cfgte.std_inq_data.vpids);
+
+		ioarcb->res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);
+		ioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;
+		ioarcb->cmd_pkt.request_type = IPR_RQTYPE_IOACMD;
+
+		ioarcb->cmd_pkt.cdb[0] = IPR_SET_SUPPORTED_DEVICES;
+		ioarcb->cmd_pkt.cdb[7] = (sizeof(struct ipr_supported_device) >> 8) & 0xff;
+		ioarcb->cmd_pkt.cdb[8] = sizeof(struct ipr_supported_device) & 0xff;
+
+		ioadl->flags_and_data_len = cpu_to_be32(IPR_IOADL_FLAGS_WRITE_LAST |
+							sizeof(struct ipr_supported_device));
+		ioadl->address = cpu_to_be32(ioa_cfg->vpd_cbs_dma +
+					     offsetof(struct ipr_misc_cbs, supp_dev));
+		ioarcb->write_ioadl_len = cpu_to_be32(sizeof(struct ipr_ioadl_desc));
+		ioarcb->write_data_transfer_length =
+			cpu_to_be32(sizeof(struct ipr_supported_device));
+
+		ipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout,
+			   IPR_SET_SUP_DEVICE_TIMEOUT);
+
+		ipr_cmd->job_step = ipr_set_supported_devs;
+		return IPR_RC_JOB_RETURN;
+	}
+
+	return IPR_RC_JOB_CONTINUE;
+}
+
+/**
+ * ipr_get_mode_page - Locate specified mode page
+ * @mode_pages:	mode page buffer
+ * @page_code:	page code to find
+ * @len:		minimum required length for mode page
+ *
+ * Return value:
+ * 	pointer to mode page / NULL on failure
+ **/
+static void *ipr_get_mode_page(struct ipr_mode_pages *mode_pages,
+			       u32 page_code, u32 len)
+{
+	struct ipr_mode_page_hdr *mode_hdr;
+	u32 page_length;
+	u32 length;
+
+	if (!mode_pages || (mode_pages->hdr.length == 0))
+		return NULL;
+
+	length = (mode_pages->hdr.length + 1) - 4 - mode_pages->hdr.block_desc_len;
+	mode_hdr = (struct ipr_mode_page_hdr *)
+		(mode_pages->data + mode_pages->hdr.block_desc_len);
+
+	while (length) {
+		if (IPR_GET_MODE_PAGE_CODE(mode_hdr) == page_code) {
+			if (mode_hdr->page_length >= (len - sizeof(struct ipr_mode_page_hdr)))
+				return mode_hdr;
+			break;
+		} else {
+			page_length = (sizeof(struct ipr_mode_page_hdr) +
+				       mode_hdr->page_length);
+			length -= page_length;
+			mode_hdr = (struct ipr_mode_page_hdr *)
+				((unsigned long)mode_hdr + page_length);
+		}
+	}
+	return NULL;
+}
+
+/**
+ * ipr_check_term_power - Check for term power errors
+ * @ioa_cfg:	ioa config struct
+ * @mode_pages:	IOAFP mode pages buffer
+ *
+ * Check the IOAFP's mode page 28 for term power errors
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_check_term_power(struct ipr_ioa_cfg *ioa_cfg,
+				 struct ipr_mode_pages *mode_pages)
+{
+	int i;
+	int entry_length;
+	struct ipr_dev_bus_entry *bus;
+	struct ipr_mode_page28 *mode_page;
+
+	mode_page = ipr_get_mode_page(mode_pages, 0x28,
+				      sizeof(struct ipr_mode_page28));
+
+	entry_length = mode_page->entry_length;
+
+	bus = mode_page->bus;
+
+	for (i = 0; i < mode_page->num_entries; i++) {
+		if (bus->flags & IPR_SCSI_ATTR_NO_TERM_PWR) {
+			dev_err(&ioa_cfg->pdev->dev,
+				"Term power is absent on scsi bus %d\n",
+				bus->res_addr.bus);
+		}
+
+		bus = (struct ipr_dev_bus_entry *)((char *)bus + entry_length);
+	}
+}
+
+/**
+ * ipr_scsi_bus_speed_limit - Limit the SCSI speed based on SES table
+ * @ioa_cfg:	ioa config struct
+ *
+ * Looks through the config table checking for SES devices. If
+ * the SES device is in the SES table indicating a maximum SCSI
+ * bus speed, the speed is limited for the bus.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_scsi_bus_speed_limit(struct ipr_ioa_cfg *ioa_cfg)
+{
+	u32 max_xfer_rate;
+	int i;
+
+	for (i = 0; i < IPR_MAX_NUM_BUSES; i++) {
+		max_xfer_rate = ipr_get_max_scsi_speed(ioa_cfg, i,
+						       ioa_cfg->bus_attr[i].bus_width);
+
+		if (max_xfer_rate < ioa_cfg->bus_attr[i].max_xfer_rate)
+			ioa_cfg->bus_attr[i].max_xfer_rate = max_xfer_rate;
+	}
+}
+
+/**
+ * ipr_modify_ioafp_mode_page_28 - Modify IOAFP Mode Page 28
+ * @ioa_cfg:	ioa config struct
+ * @mode_pages:	mode page 28 buffer
+ *
+ * Updates mode page 28 based on driver configuration
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_modify_ioafp_mode_page_28(struct ipr_ioa_cfg *ioa_cfg,
+					  	struct ipr_mode_pages *mode_pages)
+{
+	int i, entry_length;
+	struct ipr_dev_bus_entry *bus;
+	struct ipr_bus_attributes *bus_attr;
+	struct ipr_mode_page28 *mode_page;
+
+	mode_page = ipr_get_mode_page(mode_pages, 0x28,
+				      sizeof(struct ipr_mode_page28));
+
+	entry_length = mode_page->entry_length;
+
+	/* Loop for each device bus entry */
+	for (i = 0, bus = mode_page->bus;
+	     i < mode_page->num_entries;
+	     i++, bus = (struct ipr_dev_bus_entry *)((u8 *)bus + entry_length)) {
+		if (bus->res_addr.bus > IPR_MAX_NUM_BUSES) {
+			dev_err(&ioa_cfg->pdev->dev,
+				"Invalid resource address reported: 0x%08X\n",
+				IPR_GET_PHYS_LOC(bus->res_addr));
+			continue;
+		}
+
+		bus_attr = &ioa_cfg->bus_attr[i];
+		bus->extended_reset_delay = IPR_EXTENDED_RESET_DELAY;
+		bus->bus_width = bus_attr->bus_width;
+		bus->max_xfer_rate = cpu_to_be32(bus_attr->max_xfer_rate);
+		bus->flags &= ~IPR_SCSI_ATTR_QAS_MASK;
+		if (bus_attr->qas_enabled)
+			bus->flags |= IPR_SCSI_ATTR_ENABLE_QAS;
+		else
+			bus->flags |= IPR_SCSI_ATTR_DISABLE_QAS;
+	}
+}
+
+/**
+ * ipr_build_mode_select - Build a mode select command
+ * @ipr_cmd:	ipr command struct
+ * @res_handle:	resource handle to send command to
+ * @parm:		Byte 2 of Mode Sense command
+ * @dma_addr:	DMA buffer address
+ * @xfer_len:	data transfer length
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_build_mode_select(struct ipr_cmnd *ipr_cmd,
+				  u32 res_handle, u8 parm, u32 dma_addr,
+				  u8 xfer_len)
+{
+	struct ipr_ioadl_desc *ioadl = ipr_cmd->ioadl;
+	struct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;
+
+	ioarcb->res_handle = res_handle;
+	ioarcb->cmd_pkt.request_type = IPR_RQTYPE_SCSICDB;
+	ioarcb->cmd_pkt.flags_hi |= IPR_FLAGS_HI_WRITE_NOT_READ;
+	ioarcb->cmd_pkt.cdb[0] = MODE_SELECT;
+	ioarcb->cmd_pkt.cdb[1] = parm;
+	ioarcb->cmd_pkt.cdb[4] = xfer_len;
+
+	ioadl->flags_and_data_len =
+		cpu_to_be32(IPR_IOADL_FLAGS_WRITE_LAST | xfer_len);
+	ioadl->address = cpu_to_be32(dma_addr);
+	ioarcb->write_ioadl_len = cpu_to_be32(sizeof(struct ipr_ioadl_desc));
+	ioarcb->write_data_transfer_length = cpu_to_be32(xfer_len);
+}
+
+/**
+ * ipr_ioafp_mode_select_page28 - Issue Mode Select Page 28 to IOA
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function sets up the SCSI bus attributes and sends
+ * a Mode Select for Page 28 to activate them.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_RETURN
+ **/
+static int ipr_ioafp_mode_select_page28(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_mode_pages *mode_pages = &ioa_cfg->vpd_cbs->mode_pages;
+	int length;
+
+	ENTER;
+	if (ioa_cfg->saved_mode_pages) {
+		memcpy(mode_pages, ioa_cfg->saved_mode_pages,
+		       ioa_cfg->saved_mode_page_len);
+		length = ioa_cfg->saved_mode_page_len;
+	} else {
+		ipr_scsi_bus_speed_limit(ioa_cfg);
+		ipr_check_term_power(ioa_cfg, mode_pages);
+		ipr_modify_ioafp_mode_page_28(ioa_cfg, mode_pages);
+		length = mode_pages->hdr.length + 1;
+		mode_pages->hdr.length = 0;
+	}
+
+	ipr_build_mode_select(ipr_cmd, cpu_to_be32(IPR_IOA_RES_HANDLE), 0x11,
+			      ioa_cfg->vpd_cbs_dma + offsetof(struct ipr_misc_cbs, mode_pages),
+			      length);
+
+	ipr_cmd->job_step = ipr_set_supported_devs;
+	ipr_cmd->res = list_entry(ioa_cfg->used_res_q.next,
+				  struct ipr_resource_entry, queue);
+
+	ipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);
+
+	LEAVE;
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_build_mode_sense - Builds a mode sense command
+ * @ipr_cmd:	ipr command struct
+ * @res:		resource entry struct
+ * @parm:		Byte 2 of mode sense command
+ * @dma_addr:	DMA address of mode sense buffer
+ * @xfer_len:	Size of DMA buffer
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_build_mode_sense(struct ipr_cmnd *ipr_cmd,
+				 u32 res_handle,
+				 u8 parm, u32 dma_addr, u8 xfer_len)
+{
+	struct ipr_ioadl_desc *ioadl = ipr_cmd->ioadl;
+	struct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;
+
+	ioarcb->res_handle = res_handle;
+	ioarcb->cmd_pkt.cdb[0] = MODE_SENSE;
+	ioarcb->cmd_pkt.cdb[2] = parm;
+	ioarcb->cmd_pkt.cdb[4] = xfer_len;
+	ioarcb->cmd_pkt.request_type = IPR_RQTYPE_SCSICDB;
+
+	ioadl->flags_and_data_len =
+		cpu_to_be32(IPR_IOADL_FLAGS_READ_LAST | xfer_len);
+	ioadl->address = cpu_to_be32(dma_addr);
+	ioarcb->read_ioadl_len = cpu_to_be32(sizeof(struct ipr_ioadl_desc));
+	ioarcb->read_data_transfer_length = cpu_to_be32(xfer_len);
+}
+
+/**
+ * ipr_ioafp_mode_sense_page28 - Issue Mode Sense Page 28 to IOA
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function send a Page 28 mode sense to the IOA to
+ * retrieve SCSI bus attributes.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_RETURN
+ **/
+static int ipr_ioafp_mode_sense_page28(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+
+	ENTER;
+	ipr_build_mode_sense(ipr_cmd, cpu_to_be32(IPR_IOA_RES_HANDLE),
+			     0x28, ioa_cfg->vpd_cbs_dma +
+			     offsetof(struct ipr_misc_cbs, mode_pages),
+			     sizeof(struct ipr_mode_pages));
+
+	ipr_cmd->job_step = ipr_ioafp_mode_select_page28;
+
+	ipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);
+
+	LEAVE;
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_init_res_table - Initialize the resource table
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function looks through the existing resource table, comparing
+ * it with the config table. This function will take care of old/new
+ * devices and schedule adding/removing them from the mid-layer
+ * as appropriate.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_CONTINUE
+ **/
+static int ipr_init_res_table(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_resource_entry *res, *temp;
+	struct ipr_config_table_entry *cfgte;
+	int found, i;
+	LIST_HEAD(old_res);
+
+	ENTER;
+	if (ioa_cfg->cfg_table->hdr.flags & IPR_UCODE_DOWNLOAD_REQ)
+		dev_err(&ioa_cfg->pdev->dev, "Microcode download required\n");
+
+	list_for_each_entry_safe(res, temp, &ioa_cfg->used_res_q, queue)
+		list_move_tail(&res->queue, &old_res);
+
+	for (i = 0; i < ioa_cfg->cfg_table->hdr.num_entries; i++) {
+		cfgte = &ioa_cfg->cfg_table->dev[i];
+		found = 0;
+
+		list_for_each_entry_safe(res, temp, &old_res, queue) {
+			if (!memcmp(&res->cfgte.res_addr,
+				    &cfgte->res_addr, sizeof(cfgte->res_addr))) {
+				list_move_tail(&res->queue, &ioa_cfg->used_res_q);
+				found = 1;
+				break;
+			}
+		}
+
+		if (!found) {
+			if (list_empty(&ioa_cfg->free_res_q)) {
+				dev_err(&ioa_cfg->pdev->dev, "Too many devices attached\n");
+				break;
+			}
+
+			found = 1;
+			res = list_entry(ioa_cfg->free_res_q.next,
+					 struct ipr_resource_entry, queue);
+			list_move_tail(&res->queue, &ioa_cfg->used_res_q);
+			ipr_init_res_entry(res);
+			if (ioa_cfg->allow_ml_add_del) {
+				ipr_trace;
+				res->add_to_ml = 1;
+			}
+		}
+
+		if (found)
+			memcpy(&res->cfgte, cfgte, sizeof(struct ipr_config_table_entry));
+	}
+
+	list_for_each_entry_safe(res, temp, &old_res, queue) {
+		if (ioa_cfg->allow_ml_add_del && res->sdev) {
+			ipr_trace;
+			res->del_from_ml = 1;
+			list_move_tail(&res->queue, &ioa_cfg->used_res_q);
+		} else {
+			list_move_tail(&res->queue, &ioa_cfg->free_res_q);
+		}
+	}
+
+	ipr_cmd->job_step = ipr_ioafp_mode_sense_page28;
+
+	LEAVE;
+	return IPR_RC_JOB_CONTINUE;
+}
+
+/**
+ * ipr_ioafp_query_ioa_cfg - Send a Query IOA Config to the adapter.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function sends a Query IOA Configuration command
+ * to the adapter to retrieve the IOA configuration table.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_RETURN
+ **/
+static int ipr_ioafp_query_ioa_cfg(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;
+	struct ipr_ioadl_desc *ioadl = ipr_cmd->ioadl;
+
+	ENTER;
+	ioarcb->cmd_pkt.request_type = IPR_RQTYPE_IOACMD;
+	ioarcb->res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);
+
+	ioarcb->cmd_pkt.cdb[0] = IPR_QUERY_IOA_CONFIG;
+	ioarcb->cmd_pkt.cdb[7] = (sizeof(struct ipr_config_table) >> 8) & 0xff;
+	ioarcb->cmd_pkt.cdb[8] = sizeof(struct ipr_config_table) & 0xff;
+
+	ioarcb->read_ioadl_len = cpu_to_be32(sizeof(struct ipr_ioadl_desc));
+	ioarcb->read_data_transfer_length =
+		cpu_to_be32(sizeof(struct ipr_config_table));
+
+	ioadl->address = cpu_to_be32(ioa_cfg->cfg_table_dma);
+	ioadl->flags_and_data_len =
+		cpu_to_be32(IPR_IOADL_FLAGS_READ_LAST | sizeof(struct ipr_config_table));
+
+	ipr_cmd->job_step = ipr_init_res_table;
+
+	ipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);
+
+	LEAVE;
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_ioafp_inquiry - Send an Inquiry to the adapter.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This utility function sends an inquiry to the adapter.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_ioafp_inquiry(struct ipr_cmnd *ipr_cmd, u8 flags, u8 page,
+			      u32 dma_addr, u8 xfer_len)
+{
+	struct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;
+	struct ipr_ioadl_desc *ioadl = ipr_cmd->ioadl;
+
+	ENTER;
+	ioarcb->cmd_pkt.request_type = IPR_RQTYPE_SCSICDB;
+	ioarcb->res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);
+
+	ioarcb->cmd_pkt.cdb[0] = INQUIRY;
+	ioarcb->cmd_pkt.cdb[1] = flags;
+	ioarcb->cmd_pkt.cdb[2] = page;
+	ioarcb->cmd_pkt.cdb[4] = xfer_len;
+
+	ioarcb->read_ioadl_len = cpu_to_be32(sizeof(struct ipr_ioadl_desc));
+	ioarcb->read_data_transfer_length = cpu_to_be32(xfer_len);
+
+	ioadl->address = cpu_to_be32(dma_addr);
+	ioadl->flags_and_data_len =
+		cpu_to_be32(IPR_IOADL_FLAGS_READ_LAST | xfer_len);
+
+	ipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);
+	LEAVE;
+}
+
+/**
+ * ipr_ioafp_page3_inquiry - Send a Page 3 Inquiry to the adapter.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function sends a Page 3 inquiry to the adapter
+ * to retrieve software VPD information.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_CONTINUE / IPR_RC_JOB_RETURN
+ **/
+static int ipr_ioafp_page3_inquiry(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	char type[5];
+
+	ENTER;
+
+	/* Grab the type out of the VPD and store it away */
+	memcpy(type, ioa_cfg->vpd_cbs->ioa_vpd.std_inq_data.vpids.product_id, 4);
+	type[4] = '\0';
+	ioa_cfg->type = simple_strtoul((char *)type, NULL, 16);
+
+	ipr_cmd->job_step = ipr_ioafp_query_ioa_cfg;
+
+	ipr_ioafp_inquiry(ipr_cmd, 1, 3,
+			  ioa_cfg->vpd_cbs_dma + offsetof(struct ipr_misc_cbs, page3_data),
+			  sizeof(struct ipr_inquiry_page3));
+
+	LEAVE;
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_ioafp_std_inquiry - Send a Standard Inquiry to the adapter.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function sends a standard inquiry to the adapter.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_RETURN
+ **/
+static int ipr_ioafp_std_inquiry(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+
+	ENTER;
+	ipr_cmd->job_step = ipr_ioafp_page3_inquiry;
+
+	ipr_ioafp_inquiry(ipr_cmd, 0, 0,
+			  ioa_cfg->vpd_cbs_dma + offsetof(struct ipr_misc_cbs, ioa_vpd),
+			  sizeof(struct ipr_ioa_vpd));
+
+	LEAVE;
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_ioafp_indentify_hrrq - Send Identify Host RRQ.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function send an Identify Host Request Response Queue
+ * command to establish the HRRQ with the adapter.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_RETURN
+ **/
+static int ipr_ioafp_indentify_hrrq(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_ioarcb *ioarcb = &ipr_cmd->ioarcb;
+
+	ENTER;
+	dev_info(&ioa_cfg->pdev->dev, "Starting IOA initialization sequence.\n");
+
+	ioarcb->cmd_pkt.cdb[0] = IPR_ID_HOST_RR_Q;
+	ioarcb->res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);
+
+	ioarcb->cmd_pkt.request_type = IPR_RQTYPE_IOACMD;
+	ioarcb->cmd_pkt.cdb[2] =
+		((u32) ioa_cfg->host_rrq_dma >> 24) & 0xff;
+	ioarcb->cmd_pkt.cdb[3] =
+		((u32) ioa_cfg->host_rrq_dma >> 16) & 0xff;
+	ioarcb->cmd_pkt.cdb[4] =
+		((u32) ioa_cfg->host_rrq_dma >> 8) & 0xff;
+	ioarcb->cmd_pkt.cdb[5] =
+		((u32) ioa_cfg->host_rrq_dma) & 0xff;
+	ioarcb->cmd_pkt.cdb[7] =
+		((sizeof(u32) * IPR_NUM_CMD_BLKS) >> 8) & 0xff;
+	ioarcb->cmd_pkt.cdb[8] =
+		(sizeof(u32) * IPR_NUM_CMD_BLKS) & 0xff;
+
+	ipr_cmd->job_step = ipr_ioafp_std_inquiry;
+
+	ipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, IPR_INTERNAL_TIMEOUT);
+
+	LEAVE;
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_reset_timer_done - Adapter reset timer function
+ * @ipr_cmd:	ipr command struct
+ *
+ * Description: This function is used in adapter reset processing
+ * for timing events. If the reset_cmd pointer in the IOA
+ * config struct is not this adapter's we are doing nested
+ * resets and fail_all_ops will take care of freeing the
+ * command block.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_reset_timer_done(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	unsigned long lock_flags = 0;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+
+	if (ioa_cfg->reset_cmd == ipr_cmd) {
+		list_del(&ipr_cmd->queue);
+		ipr_cmd->done(ipr_cmd);
+	}
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+}
+
+/**
+ * ipr_reset_start_timer - Start a timer for adapter reset job
+ * @ipr_cmd:	ipr command struct
+ * @timeout:	timeout value
+ *
+ * Description: This function is used in adapter reset processing
+ * for timing events. If the reset_cmd pointer in the IOA
+ * config struct is not this adapter's we are doing nested
+ * resets and fail_all_ops will take care of freeing the
+ * command block.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_reset_start_timer(struct ipr_cmnd *ipr_cmd,
+				  unsigned long timeout)
+{
+	list_add_tail(&ipr_cmd->queue, &ipr_cmd->ioa_cfg->pending_q);
+	ipr_cmd->done = ipr_reset_ioa_job;
+
+	ipr_cmd->timer.data = (unsigned long) ipr_cmd;
+	ipr_cmd->timer.expires = jiffies + timeout;
+	ipr_cmd->timer.function = (void (*)(unsigned long))ipr_reset_timer_done;
+	add_timer(&ipr_cmd->timer);
+}
+
+/**
+ * ipr_init_ioa_mem - Initialize ioa_cfg control block
+ * @ioa_cfg:	ioa cfg struct
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_init_ioa_mem(struct ipr_ioa_cfg *ioa_cfg)
+{
+	memset(ioa_cfg->host_rrq, 0, sizeof(u32) * IPR_NUM_CMD_BLKS);
+
+	/* Initialize Host RRQ pointers */
+	ioa_cfg->hrrq_start = ioa_cfg->host_rrq;
+	ioa_cfg->hrrq_end = &ioa_cfg->host_rrq[IPR_NUM_CMD_BLKS - 1];
+	ioa_cfg->hrrq_curr = ioa_cfg->hrrq_start;
+	ioa_cfg->toggle_bit = 1;
+
+	/* Zero out config table */
+	memset(ioa_cfg->cfg_table, 0, sizeof(struct ipr_config_table));
+}
+
+/**
+ * ipr_reset_enable_ioa - Enable the IOA following a reset.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function reinitializes some control blocks and
+ * enables destructive diagnostics on the adapter.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_RETURN
+ **/
+static int ipr_reset_enable_ioa(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	volatile u32 temp_reg;
+
+	ENTER;
+	ipr_init_ioa_mem(ioa_cfg);
+
+	/* Enable interrupts */
+	ioa_cfg->allow_interrupts = 1;
+	writel(IPR_PCII_OPER_INTERRUPTS, ioa_cfg->regs.clr_interrupt_mask_reg);
+	temp_reg = readl(ioa_cfg->regs.sense_interrupt_mask_reg);
+
+	/* Enable destructive diagnostics on IOA */
+	writel(IPR_DOORBELL, ioa_cfg->regs.set_uproc_interrupt_reg);
+
+	dev_info(&ioa_cfg->pdev->dev, "Initializing IOA.\n");
+
+	ipr_cmd->timer.data = (unsigned long) ipr_cmd;
+	ipr_cmd->timer.expires = jiffies + IPR_OPERATIONAL_TIMEOUT;
+	ipr_cmd->timer.function = (void (*)(unsigned long))ipr_timeout;
+	add_timer(&ipr_cmd->timer);
+	ipr_cmd->job_step = ipr_ioafp_indentify_hrrq;
+
+	LEAVE;
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_reset_wait_for_dump - Wait for a dump to timeout.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function is invoked when an adapter dump has run out
+ * of processing time.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_CONTINUE
+ **/
+static int ipr_reset_wait_for_dump(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+
+	if (ioa_cfg->sdt_state == GET_DUMP)
+		ioa_cfg->sdt_state = ABORT_DUMP;
+
+	ipr_cmd->job_step = ipr_reset_alert;
+
+	return IPR_RC_JOB_CONTINUE;
+}
+
+/**
+ * ipr_unit_check_no_data - Log a unit check/no data error log
+ * @ioa_cfg:		ioa config struct
+ *
+ * Logs an error indicating the adapter unit checked, but for some
+ * reason, we were unable to fetch the unit check buffer.
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_unit_check_no_data(struct ipr_ioa_cfg *ioa_cfg)
+{
+	ioa_cfg->errors_logged++;
+	dev_err(&ioa_cfg->pdev->dev, "IOA unit check with no data\n");
+}
+
+/**
+ * ipr_get_unit_check_buffer - Get the unit check buffer from the IOA
+ * @ioa_cfg:		ioa config struct
+ *
+ * Fetches the unit check buffer from the adapter by clocking the data
+ * through the mailbox register.
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_get_unit_check_buffer(struct ipr_ioa_cfg *ioa_cfg)
+{
+	unsigned long mailbox;
+	struct ipr_hostrcb *hostrcb;
+	struct ipr_uc_sdt sdt;
+	int rc, length;
+
+	mailbox = readl(ioa_cfg->ioa_mailbox);
+
+	if (!ipr_sdt_is_fmt2(mailbox)) {
+		ipr_unit_check_no_data(ioa_cfg);
+		return;
+	}
+
+	memset(&sdt, 0, sizeof(struct ipr_uc_sdt));
+	rc = ipr_get_ldump_data_section(ioa_cfg, mailbox, (u32 *) &sdt,
+					(sizeof(struct ipr_uc_sdt)) / sizeof(u32));
+
+	if (rc || (be32_to_cpu(sdt.hdr.state) != IPR_FMT2_SDT_READY_TO_USE) ||
+	    !(sdt.entry[0].flags & IPR_SDT_VALID_ENTRY)) {
+		ipr_unit_check_no_data(ioa_cfg);
+		return;
+	}
+
+	/* Find length of the first sdt entry (UC buffer) */
+	length = (be32_to_cpu(sdt.entry[0].end_offset) -
+		  be32_to_cpu(sdt.entry[0].bar_str_offset)) & IPR_FMT2_MBX_ADDR_MASK;
+
+	hostrcb = list_entry(ioa_cfg->hostrcb_free_q.next,
+			     struct ipr_hostrcb, queue);
+	list_del(&hostrcb->queue);
+	memset(hostrcb, 0, sizeof(*hostrcb));
+
+	rc = ipr_get_ldump_data_section(ioa_cfg,
+					be32_to_cpu(sdt.entry[0].bar_str_offset),
+					(u32 *)hostrcb,
+					min(length, (int)(sizeof(*hostrcb) / sizeof(u32))));
+
+	if (!rc)
+		ipr_handle_log_data(ioa_cfg, hostrcb);
+	else
+		ipr_unit_check_no_data(ioa_cfg);
+
+	list_add_tail(&hostrcb->queue, &ioa_cfg->hostrcb_free_q);
+}
+
+/**
+ * ipr_reset_restore_cfg_space - Restore PCI config space.
+ * @ipr_cmd:	ipr command struct
+ *
+ * Description: This function restores the saved PCI config space of
+ * the adapter, fails all outstanding ops back to the callers, and
+ * fetches the dump/unit check if applicable to this reset.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_CONTINUE / IPR_RC_JOB_RETURN
+ **/
+static int ipr_reset_restore_cfg_space(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	int rc;
+
+	ENTER;
+	rc = pci_restore_state(ioa_cfg->pdev, ioa_cfg->pci_cfg_buf);
+
+	if (rc != PCIBIOS_SUCCESSFUL) {
+		ipr_cmd->ioasa.ioasc = cpu_to_be32(IPR_IOASC_PCI_ACCESS_ERROR);
+		return IPR_RC_JOB_CONTINUE;
+	}
+
+	if (ipr_set_pcix_cmd_reg(ioa_cfg)) {
+		ipr_cmd->ioasa.ioasc = cpu_to_be32(IPR_IOASC_PCI_ACCESS_ERROR);
+		return IPR_RC_JOB_CONTINUE;
+	}
+
+	ipr_fail_all_ops(ioa_cfg);
+
+	if (ioa_cfg->ioa_unit_checked) {
+		ioa_cfg->ioa_unit_checked = 0;
+		ipr_get_unit_check_buffer(ioa_cfg);
+		ipr_cmd->job_step = ipr_reset_alert;
+		ipr_reset_start_timer(ipr_cmd, 0);
+		return IPR_RC_JOB_RETURN;
+	}
+
+	if (ioa_cfg->in_ioa_bringdown) {
+		ipr_cmd->job_step = ipr_ioa_bringdown_done;
+	} else {
+		ipr_cmd->job_step = ipr_reset_enable_ioa;
+
+		if (GET_DUMP == ioa_cfg->sdt_state) {
+			ipr_reset_start_timer(ipr_cmd, IPR_DUMP_TIMEOUT);
+			ipr_cmd->job_step = ipr_reset_wait_for_dump;
+			schedule_work(&ioa_cfg->work_q);
+			return IPR_RC_JOB_RETURN;
+		}
+	}
+
+	ENTER;
+	return IPR_RC_JOB_CONTINUE;
+}
+
+/**
+ * ipr_reset_start_bist - Run BIST on the adapter.
+ * @ipr_cmd:	ipr command struct
+ *
+ * Description: This function runs BIST on the adapter, then delays 2 seconds.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_CONTINUE / IPR_RC_JOB_RETURN
+ **/
+static int ipr_reset_start_bist(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	int rc;
+
+	ENTER;
+	rc = pci_write_config_byte(ioa_cfg->pdev, PCI_BIST, PCI_BIST_START);
+
+	if (rc != PCIBIOS_SUCCESSFUL) {
+		ipr_cmd->ioasa.ioasc = cpu_to_be32(IPR_IOASC_PCI_ACCESS_ERROR);
+		rc = IPR_RC_JOB_CONTINUE;
+	} else {
+		ipr_cmd->job_step = ipr_reset_restore_cfg_space;
+		ipr_reset_start_timer(ipr_cmd, IPR_WAIT_FOR_BIST_TIMEOUT);
+		rc = IPR_RC_JOB_RETURN;
+	}
+
+	LEAVE;
+	return rc;
+}
+
+/**
+ * ipr_reset_allowed - Query whether or not IOA can be reset
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ * 	0 if reset not allowed / non-zero if reset is allowed
+ **/
+static int ipr_reset_allowed(struct ipr_ioa_cfg *ioa_cfg)
+{
+	volatile u32 temp_reg;
+
+	temp_reg = readl(ioa_cfg->regs.sense_interrupt_reg);
+	return ((temp_reg & IPR_PCII_CRITICAL_OPERATION) == 0);
+}
+
+/**
+ * ipr_reset_wait_to_start_bist - Wait for permission to reset IOA.
+ * @ipr_cmd:	ipr command struct
+ *
+ * Description: This function waits for adapter permission to run BIST,
+ * then runs BIST. If the adapter does not give permission after a
+ * reasonable time, we will reset the adapter anyway. The impact of
+ * resetting the adapter without warning the adapter is the risk of
+ * losing the persistent error log on the adapter. If the adapter is
+ * reset while it is writing to the flash on the adapter, the flash
+ * segment will have bad ECC and be zeroed.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_CONTINUE / IPR_RC_JOB_RETURN
+ **/
+static int ipr_reset_wait_to_start_bist(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	int rc = IPR_RC_JOB_RETURN;
+
+	if (!ipr_reset_allowed(ioa_cfg) && ipr_cmd->time_left) {
+		ipr_cmd->time_left -= IPR_CHECK_FOR_RESET_TIMEOUT;
+		ipr_reset_start_timer(ipr_cmd, IPR_CHECK_FOR_RESET_TIMEOUT);
+	} else {
+		ipr_cmd->job_step = ipr_reset_start_bist;
+		rc = IPR_RC_JOB_CONTINUE;
+	}
+
+	return rc;
+}
+
+/**
+ * ipr_reset_alert_part2 - Alert the adapter of a pending reset
+ * @ipr_cmd:	ipr command struct
+ *
+ * Description: This function alerts the adapter that it will be reset.
+ * If memory space is not currently enabled, proceed directly
+ * to running BIST on the adapter. The timer must always be started
+ * so we guarantee we do not run BIST from ipr_isr.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_RETURN
+ **/
+static int ipr_reset_alert(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	u16 cmd_reg;
+	int rc;
+
+	ENTER;
+	rc = pci_read_config_word(ioa_cfg->pdev, PCI_COMMAND, &cmd_reg);
+
+	if ((rc == PCIBIOS_SUCCESSFUL) && (cmd_reg & PCI_COMMAND_MEMORY)) {
+		ipr_mask_and_clear_all_interrupts(ioa_cfg);
+		writel(IPR_UPROCI_RESET_ALERT, ioa_cfg->regs.set_uproc_interrupt_reg);
+		ipr_cmd->job_step = ipr_reset_wait_to_start_bist;
+	} else {
+		ipr_cmd->job_step = ipr_reset_start_bist;
+	}
+
+	ipr_cmd->time_left = IPR_WAIT_FOR_RESET_TIMEOUT;
+	ipr_reset_start_timer(ipr_cmd, IPR_CHECK_FOR_RESET_TIMEOUT);
+
+	LEAVE;
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_reset_ucode_download_done - Microcode download completion
+ * @ipr_cmd:	ipr command struct
+ *
+ * Description: This function unmaps the microcode download buffer.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_CONTINUE
+ **/
+static int ipr_reset_ucode_download_done(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_sglist *sglist = ioa_cfg->ucode_sglist;
+
+	pci_unmap_sg(ioa_cfg->pdev, sglist->scatterlist,
+		     sglist->num_sg, DMA_TO_DEVICE);
+
+	ipr_cmd->job_step = ipr_reset_alert;
+	return IPR_RC_JOB_CONTINUE;
+}
+
+/**
+ * ipr_reset_ucode_download - Download microcode to the adapter
+ * @ipr_cmd:	ipr command struct
+ *
+ * Description: This function checks to see if it there is microcode
+ * to download to the adapter. If there is, a download is performed.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_CONTINUE / IPR_RC_JOB_RETURN
+ **/
+static int ipr_reset_ucode_download(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	struct ipr_sglist *sglist = ioa_cfg->ucode_sglist;
+
+	ENTER;
+	ipr_cmd->job_step = ipr_reset_alert;
+
+	if (!sglist)
+		return IPR_RC_JOB_CONTINUE;
+
+	ipr_cmd->ioarcb.res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);
+	ipr_cmd->ioarcb.cmd_pkt.request_type = IPR_RQTYPE_SCSICDB;
+	ipr_cmd->ioarcb.cmd_pkt.cdb[0] = WRITE_BUFFER;
+	ipr_cmd->ioarcb.cmd_pkt.cdb[1] = 5;
+	ipr_cmd->ioarcb.cmd_pkt.cdb[6] = (sglist->buffer_len & 0xff0000) >> 16;
+	ipr_cmd->ioarcb.cmd_pkt.cdb[7] = (sglist->buffer_len & 0x00ff00) >> 8;
+	ipr_cmd->ioarcb.cmd_pkt.cdb[8] = sglist->buffer_len & 0x0000ff;
+
+	if (ipr_map_ucode_buffer(ipr_cmd, sglist, sglist->buffer_len)) {
+		dev_err(&ioa_cfg->pdev->dev,
+			"Failed to map microcode download buffer\n");
+		return IPR_RC_JOB_CONTINUE;
+	}
+
+	ipr_cmd->job_step = ipr_reset_ucode_download_done;
+
+	ipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout,
+		   IPR_WRITE_BUFFER_TIMEOUT);
+
+	LEAVE;
+	return IPR_RC_JOB_RETURN;
+}
+
+/**
+ * ipr_reset_shutdown_ioa - Shutdown the adapter
+ * @ipr_cmd:	ipr command struct
+ *
+ * Description: This function issues an adapter shutdown of the
+ * specified type to the specified adapter as part of the
+ * adapter reset job.
+ *
+ * Return value:
+ * 	IPR_RC_JOB_CONTINUE / IPR_RC_JOB_RETURN
+ **/
+static int ipr_reset_shutdown_ioa(struct ipr_cmnd *ipr_cmd)
+{
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+	enum ipr_shutdown_type shutdown_type = ipr_cmd->shutdown_type;
+	unsigned long timeout;
+	int rc = IPR_RC_JOB_CONTINUE;
+
+	ENTER;
+	if (shutdown_type != IPR_SHUTDOWN_NONE && !ioa_cfg->ioa_is_dead) {
+		ipr_cmd->ioarcb.res_handle = cpu_to_be32(IPR_IOA_RES_HANDLE);
+		ipr_cmd->ioarcb.cmd_pkt.request_type = IPR_RQTYPE_IOACMD;
+		ipr_cmd->ioarcb.cmd_pkt.cdb[0] = IPR_IOA_SHUTDOWN;
+		ipr_cmd->ioarcb.cmd_pkt.cdb[1] = shutdown_type;
+
+		if (shutdown_type == IPR_SHUTDOWN_ABBREV)
+			timeout = IPR_ABBREV_SHUTDOWN_TIMEOUT;
+		else if (shutdown_type == IPR_SHUTDOWN_PREPARE_FOR_NORMAL)
+			timeout = IPR_INTERNAL_TIMEOUT;
+		else
+			timeout = IPR_SHUTDOWN_TIMEOUT;
+
+		ipr_do_req(ipr_cmd, ipr_reset_ioa_job, ipr_timeout, timeout);
+
+		rc = IPR_RC_JOB_RETURN;
+		ipr_cmd->job_step = ipr_reset_ucode_download;
+	} else
+		ipr_cmd->job_step = ipr_reset_alert;
+
+	LEAVE;
+	return rc;
+}
+
+/**
+ * ipr_reset_ioa_job - Adapter reset job
+ * @ipr_cmd:	ipr command struct
+ *
+ * Description: This function is the job router for the adapter reset job.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_reset_ioa_job(struct ipr_cmnd *ipr_cmd)
+{
+	u32 rc, ioasc;
+	unsigned long scratch = ipr_cmd->scratch;
+	struct ipr_ioa_cfg *ioa_cfg = ipr_cmd->ioa_cfg;
+
+	do {
+		ioasc = be32_to_cpu(ipr_cmd->ioasa.ioasc);
+
+		if (ioa_cfg->reset_cmd != ipr_cmd) {
+			/*
+			 * We are doing nested adapter resets and this is
+			 * not the current reset job.
+			 */
+			list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+			return;
+		}
+
+		if (ioasc) {
+			dev_err(&ioa_cfg->pdev->dev,
+				"0x%02X failed with IOASC: 0x%08X\n",
+				ipr_cmd->ioarcb.cmd_pkt.cdb[0], ioasc);
+
+			ipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);
+			list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+			return;
+		}
+
+		ipr_reinit_ipr_cmnd(ipr_cmd);
+		ipr_cmd->scratch = scratch;
+		rc = ipr_cmd->job_step(ipr_cmd);
+	} while(rc == IPR_RC_JOB_CONTINUE);
+}
+
+/**
+ * ipr_initiate_ioa_reset - Initiate an adapter reset
+ * @ioa_cfg:		ioa config struct
+ * @shutdown_type:	shutdown type
+ *
+ * Description: This function will initiate the reset of the given adapter.
+ * If the caller needs to wait on the completion of the reset,
+ * the caller must sleep on the reset_wait_q.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_initiate_ioa_reset(struct ipr_ioa_cfg *ioa_cfg,
+				   enum ipr_shutdown_type shutdown_type)
+{
+	struct ipr_cmnd *ipr_cmd;
+
+	if (ioa_cfg->ioa_is_dead)
+		return;
+
+	if (ioa_cfg->in_reset_reload && ioa_cfg->sdt_state == GET_DUMP)
+		ioa_cfg->sdt_state = ABORT_DUMP;
+
+	if (ioa_cfg->reset_retries++ > IPR_NUM_RESET_RELOAD_RETRIES) {
+		dev_err(&ioa_cfg->pdev->dev,
+			"IOA taken offline - error recovery failed.\n");
+
+		ioa_cfg->reset_retries = 0;
+		ioa_cfg->ioa_is_dead = 1;
+
+		if (ioa_cfg->in_ioa_bringdown) {
+			ioa_cfg->reset_cmd = NULL;
+			ioa_cfg->in_reset_reload = 0;
+			ipr_fail_all_ops(ioa_cfg);
+			wake_up_all(&ioa_cfg->reset_wait_q);
+
+			spin_unlock_irq(ioa_cfg->host->host_lock);
+			scsi_unblock_requests(ioa_cfg->host);
+			spin_lock_irq(ioa_cfg->host->host_lock);
+			return;
+		} else {
+			ioa_cfg->in_ioa_bringdown = 1;
+			shutdown_type = IPR_SHUTDOWN_NONE;
+		}
+	}
+
+	ioa_cfg->in_reset_reload = 1;
+	ioa_cfg->allow_cmds = 0;
+	scsi_block_requests(ioa_cfg->host);
+
+	ipr_cmd = ipr_get_free_ipr_cmnd(ioa_cfg);
+	ioa_cfg->reset_cmd = ipr_cmd;
+	ipr_cmd->job_step = ipr_reset_shutdown_ioa;
+	ipr_cmd->shutdown_type = shutdown_type;
+
+	ipr_reset_ioa_job(ipr_cmd);
+}
+
+/**
+ * ipr_probe_ioa_part2 - Initializes IOAs found in ipr_probe_ioa(..)
+ * @ioa_cfg:	ioa cfg struct
+ *
+ * Description: This is the second phase of adapter intialization
+ * This function takes care of initilizing the adapter to the point
+ * where it can accept new commands.
+
+ * Return value:
+ * 	0 on sucess / -EIO on failure
+ **/
+static int __devinit ipr_probe_ioa_part2(struct ipr_ioa_cfg *ioa_cfg)
+{
+	int rc = 0;
+	unsigned long host_lock_flags = 0;
+
+	ENTER;
+	spin_lock_irqsave(ioa_cfg->host->host_lock, host_lock_flags);
+
+	dev_dbg(&ioa_cfg->pdev->dev, "ioa_cfg adx: 0x%p\n", ioa_cfg);
+
+	ipr_initiate_ioa_reset(ioa_cfg, IPR_SHUTDOWN_NONE);
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, host_lock_flags);
+	wait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);
+	spin_lock_irqsave(ioa_cfg->host->host_lock, host_lock_flags);
+
+	if (ioa_cfg->ioa_is_dead) {
+		rc = -EIO;
+	} else if (ipr_invalid_adapter(ioa_cfg)) {
+		if (!ipr_testmode)
+			rc = -EIO;
+
+		dev_err(&ioa_cfg->pdev->dev,
+			"Adapter not supported in this hardware configuration.\n");
+	}
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, host_lock_flags);
+
+	LEAVE;
+	return rc;
+}
+
+/**
+ * ipr_free_cmd_blks - Frees command blocks allocated for an adapter
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_free_cmd_blks(struct ipr_ioa_cfg *ioa_cfg)
+{
+	int i;
+
+	for (i = 0; i < IPR_NUM_CMD_BLKS; i++) {
+		if (ioa_cfg->ipr_cmnd_list[i])
+			pci_pool_free(ioa_cfg->ipr_cmd_pool,
+				      ioa_cfg->ipr_cmnd_list[i],
+				      ioa_cfg->ipr_cmnd_list_dma[i]);
+
+		ioa_cfg->ipr_cmnd_list[i] = NULL;
+	}
+
+	if (ioa_cfg->ipr_cmd_pool)
+		pci_pool_destroy (ioa_cfg->ipr_cmd_pool);
+
+	ioa_cfg->ipr_cmd_pool = NULL;
+}
+
+/**
+ * ipr_free_mem - Frees memory allocated for an adapter
+ * @ioa_cfg:	ioa cfg struct
+ *
+ * Return value:
+ * 	nothing
+ **/
+static void ipr_free_mem(struct ipr_ioa_cfg *ioa_cfg)
+{
+	int i;
+
+	kfree(ioa_cfg->res_entries);
+	pci_free_consistent(ioa_cfg->pdev, sizeof(struct ipr_misc_cbs),
+			    ioa_cfg->vpd_cbs, ioa_cfg->vpd_cbs_dma);
+	ipr_free_cmd_blks(ioa_cfg);
+	pci_free_consistent(ioa_cfg->pdev, sizeof(u32) * IPR_NUM_CMD_BLKS,
+			    ioa_cfg->host_rrq, ioa_cfg->host_rrq_dma);
+	pci_free_consistent(ioa_cfg->pdev, sizeof(struct ipr_config_table),
+			    ioa_cfg->cfg_table,
+			    ioa_cfg->cfg_table_dma);
+
+	for (i = 0; i < IPR_NUM_HCAMS; i++) {
+		pci_free_consistent(ioa_cfg->pdev,
+				    sizeof(struct ipr_hostrcb),
+				    ioa_cfg->hostrcb[i],
+				    ioa_cfg->hostrcb_dma[i]);
+	}
+
+	ipr_free_dump(ioa_cfg);
+	kfree(ioa_cfg->saved_mode_pages);
+	kfree(ioa_cfg->trace);
+}
+
+/**
+ * ipr_free_all_resources - Free all allocated resources for an adapter.
+ * @ipr_cmd:	ipr command struct
+ *
+ * This function frees all allocated resources for the
+ * specified adapter.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_free_all_resources(struct ipr_ioa_cfg *ioa_cfg)
+{
+	ENTER;
+	free_irq(ioa_cfg->pdev->irq, ioa_cfg);
+	iounmap((void *) ioa_cfg->hdw_dma_regs);
+	release_mem_region(ioa_cfg->hdw_dma_regs_pci,
+			   pci_resource_len(ioa_cfg->pdev, 0));
+	ipr_free_mem(ioa_cfg);
+	scsi_host_put(ioa_cfg->host);
+	LEAVE;
+}
+
+/**
+ * ipr_alloc_cmd_blks - Allocate command blocks for an adapter
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ * 	0 on success / -ENOMEM on allocation failure
+ **/
+static int __devinit ipr_alloc_cmd_blks(struct ipr_ioa_cfg *ioa_cfg)
+{
+	struct ipr_cmnd *ipr_cmd;
+	struct ipr_ioarcb *ioarcb;
+	u32 dma_addr;
+	int i;
+
+	ioa_cfg->ipr_cmd_pool = pci_pool_create (IPR_NAME, ioa_cfg->pdev,
+						 sizeof(struct ipr_cmnd), 8, 0);
+
+	if (!ioa_cfg->ipr_cmd_pool)
+		return -ENOMEM;
+
+	for (i = 0; i < IPR_NUM_CMD_BLKS; i++) {
+		ipr_cmd = pci_pool_alloc (ioa_cfg->ipr_cmd_pool, SLAB_KERNEL, &dma_addr);
+
+		if (!ipr_cmd) {
+			ipr_free_cmd_blks(ioa_cfg);
+			return -ENOMEM;
+		}
+
+		memset(ipr_cmd, 0, sizeof(*ipr_cmd));
+		ioa_cfg->ipr_cmnd_list[i] = ipr_cmd;
+		ioa_cfg->ipr_cmnd_list_dma[i] = dma_addr;
+
+		ioarcb = &ipr_cmd->ioarcb;
+		ioarcb->ioarcb_host_pci_addr = cpu_to_be32(dma_addr);
+		ioarcb->host_response_handle = cpu_to_be32(i << 2);
+		ioarcb->write_ioadl_addr =
+			cpu_to_be32(dma_addr + offsetof(struct ipr_cmnd, ioadl));
+		ioarcb->read_ioadl_addr = ioarcb->write_ioadl_addr;
+		ioarcb->ioasa_host_pci_addr =
+			cpu_to_be32(dma_addr + offsetof(struct ipr_cmnd, ioasa));
+		ioarcb->ioasa_len = cpu_to_be16(sizeof(struct ipr_ioasa));
+		ipr_cmd->cmd_index = i;
+		ipr_cmd->ioa_cfg = ioa_cfg;
+		ipr_cmd->sense_buffer_dma = dma_addr +
+			offsetof(struct ipr_cmnd, sense_buffer);
+
+		list_add_tail(&ipr_cmd->queue, &ioa_cfg->free_q);
+	}
+
+	return 0;
+}
+
+/**
+ * ipr_alloc_mem - Allocate memory for an adapter
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ * 	0 on success / non-zero for error
+ **/
+static int __devinit ipr_alloc_mem(struct ipr_ioa_cfg *ioa_cfg)
+{
+	int i;
+
+	ENTER;
+	ioa_cfg->res_entries = kmalloc(sizeof(struct ipr_resource_entry) *
+				       IPR_MAX_PHYSICAL_DEVS, GFP_KERNEL);
+
+	if (!ioa_cfg->res_entries)
+		goto cleanup;
+
+	memset(ioa_cfg->res_entries, 0,
+	       sizeof(struct ipr_resource_entry) * IPR_MAX_PHYSICAL_DEVS);
+
+	for (i = 0; i < IPR_MAX_PHYSICAL_DEVS; i++)
+		list_add_tail(&ioa_cfg->res_entries[i].queue, &ioa_cfg->free_res_q);
+
+	ioa_cfg->vpd_cbs = pci_alloc_consistent(ioa_cfg->pdev,
+						sizeof(struct ipr_misc_cbs),
+						&ioa_cfg->vpd_cbs_dma);
+
+	if (!ioa_cfg->vpd_cbs)
+		goto cleanup;
+
+	if (ipr_alloc_cmd_blks(ioa_cfg))
+		goto cleanup;
+
+	ioa_cfg->host_rrq = pci_alloc_consistent(ioa_cfg->pdev,
+						 sizeof(u32) * IPR_NUM_CMD_BLKS,
+						 &ioa_cfg->host_rrq_dma);
+
+	if (!ioa_cfg->host_rrq)
+		goto cleanup;
+
+	ioa_cfg->cfg_table = pci_alloc_consistent(ioa_cfg->pdev,
+						  sizeof(struct ipr_config_table),
+						  &ioa_cfg->cfg_table_dma);
+
+	if (!ioa_cfg->cfg_table)
+		goto cleanup;
+
+	for (i = 0; i < IPR_NUM_HCAMS; i++) {
+		ioa_cfg->hostrcb[i] = pci_alloc_consistent(ioa_cfg->pdev,
+							   sizeof(struct ipr_hostrcb),
+							   &ioa_cfg->hostrcb_dma[i]);
+
+		if (!ioa_cfg->hostrcb[i])
+			goto cleanup;
+
+		memset(ioa_cfg->hostrcb[i], 0, sizeof(struct ipr_hostrcb));
+		ioa_cfg->hostrcb[i]->hostrcb_dma = ioa_cfg->hostrcb_dma[i];
+		list_add_tail(&ioa_cfg->hostrcb[i]->queue, &ioa_cfg->hostrcb_free_q);
+	}
+
+	ioa_cfg->trace = kmalloc(sizeof(struct ipr_trace_entry) *
+				 IPR_NUM_TRACE_ENTRIES, GFP_KERNEL);
+
+	if (!ioa_cfg->trace)
+		goto cleanup;
+
+	memset(ioa_cfg->trace, 0,
+	       sizeof(struct ipr_trace_entry) * IPR_NUM_TRACE_ENTRIES);
+
+	LEAVE;
+	return 0;
+
+cleanup:
+	ipr_free_mem(ioa_cfg);
+
+	LEAVE;
+	return -ENOMEM;
+}
+
+/**
+ * ipr_initialize_bus_attr - Initialize SCSI bus attributes to default values
+ * @ioa_cfg:	ioa config struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void __devinit ipr_initialize_bus_attr(struct ipr_ioa_cfg *ioa_cfg)
+{
+	int i;
+
+	for (i = 0; i < IPR_MAX_NUM_BUSES; i++) {
+		ioa_cfg->bus_attr[i].bus = i;
+		ioa_cfg->bus_attr[i].qas_enabled = 0;
+		ioa_cfg->bus_attr[i].bus_width = IPR_DEFAULT_BUS_WIDTH;
+		if (ipr_max_speed < ARRAY_SIZE(ipr_max_bus_speeds))
+			ioa_cfg->bus_attr[i].max_xfer_rate = ipr_max_bus_speeds[ipr_max_speed];
+		else
+			ioa_cfg->bus_attr[i].max_xfer_rate = IPR_U160_SCSI_RATE;
+	}
+}
+
+/**
+ * ipr_init_ioa_cfg - Initialize IOA config struct
+ * @ioa_cfg:	ioa config struct
+ * @host:		scsi host struct
+ * @pdev:		PCI dev struct
+ *
+ * Return value:
+ * 	none
+ **/
+static void __devinit ipr_init_ioa_cfg(struct ipr_ioa_cfg *ioa_cfg,
+				       struct Scsi_Host *host, struct pci_dev *pdev)
+{
+	ioa_cfg->host = host;
+	ioa_cfg->pdev = pdev;
+	ioa_cfg->log_level = ipr_log_level;
+	sprintf(ioa_cfg->eye_catcher, IPR_EYECATCHER);
+	sprintf(ioa_cfg->trace_start, IPR_TRACE_START_LABEL);
+	sprintf(ioa_cfg->ipr_free_label, IPR_FREEQ_LABEL);
+	sprintf(ioa_cfg->ipr_pending_label, IPR_PENDQ_LABEL);
+	sprintf(ioa_cfg->cfg_table_start, IPR_CFG_TBL_START);
+	sprintf(ioa_cfg->resource_table_label, IPR_RES_TABLE_LABEL);
+	sprintf(ioa_cfg->ipr_hcam_label, IPR_HCAM_LABEL);
+	sprintf(ioa_cfg->ipr_cmd_label, IPR_CMD_LABEL);
+
+	INIT_LIST_HEAD(&ioa_cfg->free_q);
+	INIT_LIST_HEAD(&ioa_cfg->pending_q);
+	INIT_LIST_HEAD(&ioa_cfg->hostrcb_free_q);
+	INIT_LIST_HEAD(&ioa_cfg->hostrcb_pending_q);
+	INIT_LIST_HEAD(&ioa_cfg->free_res_q);
+	INIT_LIST_HEAD(&ioa_cfg->used_res_q);
+	INIT_WORK(&ioa_cfg->work_q, ipr_worker_thread, ioa_cfg);
+	init_waitqueue_head(&ioa_cfg->reset_wait_q);
+	ioa_cfg->sdt_state = INACTIVE;
+
+	ipr_initialize_bus_attr(ioa_cfg);
+
+	host->max_id = IPR_MAX_NUM_TARGETS_PER_BUS;
+	host->max_lun = IPR_MAX_NUM_LUNS_PER_TARGET;
+	host->max_channel = IPR_MAX_BUS_TO_SCAN;
+	host->unique_id = host->host_no;
+	host->max_cmd_len = IPR_MAX_CDB_LEN;
+	pci_set_drvdata(pdev, ioa_cfg);
+
+	memcpy(&ioa_cfg->regs, &ioa_cfg->chip_cfg->regs, sizeof(ioa_cfg->regs));
+
+	ioa_cfg->regs.set_interrupt_mask_reg += ioa_cfg->hdw_dma_regs;
+	ioa_cfg->regs.clr_interrupt_mask_reg += ioa_cfg->hdw_dma_regs;
+	ioa_cfg->regs.sense_interrupt_mask_reg += ioa_cfg->hdw_dma_regs;
+	ioa_cfg->regs.clr_interrupt_reg += ioa_cfg->hdw_dma_regs;
+	ioa_cfg->regs.sense_interrupt_reg += ioa_cfg->hdw_dma_regs;
+	ioa_cfg->regs.ioarrin_reg += ioa_cfg->hdw_dma_regs;
+	ioa_cfg->regs.sense_uproc_interrupt_reg += ioa_cfg->hdw_dma_regs;
+	ioa_cfg->regs.set_uproc_interrupt_reg += ioa_cfg->hdw_dma_regs;
+	ioa_cfg->regs.clr_uproc_interrupt_reg += ioa_cfg->hdw_dma_regs;
+}
+
+/**
+ * ipr_probe_ioa - Allocates memory and does first stage of initialization
+ * @pdev:		PCI device struct
+ * @dev_id:		PCI device id struct
+ *
+ * Return value:
+ * 	0 on success / non-zero on failure
+ **/
+static int __devinit ipr_probe_ioa(struct pci_dev *pdev,
+				   const struct pci_device_id *dev_id)
+{
+	struct ipr_ioa_cfg *ioa_cfg;
+	struct Scsi_Host *host;
+	unsigned long ipr_regs, ipr_regs_pci;
+	u32 rc = PCIBIOS_SUCCESSFUL;
+
+	ENTER;
+
+	if ((rc = pci_enable_device(pdev))) {
+		dev_err(&pdev->dev, "Cannot enable adapter\n");
+		return rc;
+	}
+
+	dev_info(&pdev->dev, "Found IOA with IRQ: %d\n", pdev->irq);
+
+	host = scsi_host_alloc(&driver_template, sizeof(*ioa_cfg));
+
+	if (!host) {
+		dev_err(&pdev->dev, "call to scsi_host_alloc failed!\n");
+		return -ENOMEM;
+	}
+
+	ioa_cfg = (struct ipr_ioa_cfg *)host->hostdata;
+	memset(ioa_cfg, 0, sizeof(struct ipr_ioa_cfg));
+
+	ioa_cfg->chip_cfg = (const struct ipr_chip_cfg_t *)dev_id->driver_data;
+
+	ipr_regs_pci = pci_resource_start(pdev, 0);
+
+	if (!request_mem_region(ipr_regs_pci,
+				pci_resource_len(pdev, 0), IPR_NAME)) {
+		dev_err(&pdev->dev,
+			"Couldn't register memory range of registers\n");
+		scsi_host_put(host);
+		return -ENOMEM;
+	}
+
+	ipr_regs = (unsigned long)ioremap(ipr_regs_pci,
+					  pci_resource_len(pdev, 0));
+
+	if (!ipr_regs) {
+		dev_err(&pdev->dev,
+			"Couldn't map memory range of registers\n");
+		release_mem_region(ipr_regs_pci, pci_resource_len(pdev, 0));
+		scsi_host_put(host);
+		return -ENOMEM;
+	}
+
+	ioa_cfg->hdw_dma_regs = ipr_regs;
+	ioa_cfg->hdw_dma_regs_pci = ipr_regs_pci;
+	ioa_cfg->ioa_mailbox = ioa_cfg->chip_cfg->mailbox + ipr_regs;
+
+	ipr_init_ioa_cfg(ioa_cfg, host, pdev);
+
+	pci_set_master(pdev);
+	rc = pci_set_dma_mask(pdev, 0xffffffff);
+
+	if (rc != PCIBIOS_SUCCESSFUL) {
+		dev_err(&pdev->dev, "Failed to set PCI DMA mask\n");
+		rc = -EIO;
+		goto cleanup_nomem;
+	}
+
+	rc = pci_write_config_byte(pdev, PCI_CACHE_LINE_SIZE,
+				   ioa_cfg->chip_cfg->cache_line_size);
+
+	if (rc != PCIBIOS_SUCCESSFUL) {
+		dev_err(&pdev->dev, "Write of cache line size failed\n");
+		rc = -EIO;
+		goto cleanup_nomem;
+	}
+
+	/* Save away PCI config space for use following IOA reset */
+	rc = pci_save_state(pdev, ioa_cfg->pci_cfg_buf);
+
+	if (rc != PCIBIOS_SUCCESSFUL) {
+		dev_err(&pdev->dev, "Failed to save PCI config space\n");
+		rc = -EIO;
+		goto cleanup_nomem;
+	}
+
+	if ((rc = ipr_save_pcix_cmd_reg(ioa_cfg)))
+		goto cleanup_nomem;
+
+	if ((rc = ipr_alloc_mem(ioa_cfg)))
+		goto cleanup;
+
+	ipr_mask_and_clear_all_interrupts(ioa_cfg);
+
+	rc = request_irq(pdev->irq, ipr_isr, SA_SHIRQ, IPR_NAME, ioa_cfg);
+
+	if (rc) {
+		dev_err(&pdev->dev, "Couldn't register IRQ %d! rc=%d\n",
+			pdev->irq, rc);
+		goto cleanup_nolog;
+	}
+
+	spin_lock(&ipr_driver_lock);
+	list_add_tail(&ioa_cfg->queue, &ipr_ioa_head);
+	spin_unlock(&ipr_driver_lock);
+
+	LEAVE;
+	return 0;
+
+cleanup:
+	dev_err(&pdev->dev, "Couldn't allocate enough memory for device driver!\n");
+cleanup_nolog:
+	ipr_free_mem(ioa_cfg);
+cleanup_nomem:
+	iounmap((void *) ipr_regs);
+	release_mem_region(ipr_regs_pci, pci_resource_len(pdev, 0));
+	scsi_host_put(host);
+
+	return rc;
+}
+
+/**
+ * ipr_scan_vsets - Scans for VSET devices
+ * @ioa_cfg:	ioa config struct
+ *
+ * Description: Since the VSET resources do not follow SAM in that we can have
+ * sparse LUNs with no LUN 0, we have to scan for these ourselves.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_scan_vsets(struct ipr_ioa_cfg *ioa_cfg)
+{
+	int target, lun;
+
+	for (target = 0; target < IPR_MAX_NUM_TARGETS_PER_BUS; target++)
+		for (lun = 0; lun < IPR_MAX_NUM_VSET_LUNS_PER_TARGET; lun++ )
+			scsi_add_device(ioa_cfg->host, IPR_VSET_BUS, target, lun);
+}
+
+/**
+ * ipr_initiate_ioa_bringdown - Bring down an adapter
+ * @ioa_cfg:		ioa config struct
+ * @shutdown_type:	shutdown type
+ *
+ * Description: This function will initiate bringing down the adapter.
+ * This consists of issuing an IOA shutdown to the adapter
+ * to flush the cache, and running BIST.
+ * If the caller needs to wait on the completion of the reset,
+ * the caller must sleep on the reset_wait_q.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_initiate_ioa_bringdown(struct ipr_ioa_cfg *ioa_cfg,
+				       enum ipr_shutdown_type shutdown_type)
+{
+	ENTER;
+	if (ioa_cfg->sdt_state == WAIT_FOR_DUMP)
+		ioa_cfg->sdt_state = ABORT_DUMP;
+	ioa_cfg->reset_retries = 0;
+	ioa_cfg->in_ioa_bringdown = 1;
+	ipr_initiate_ioa_reset(ioa_cfg, shutdown_type);
+	LEAVE;
+}
+
+/**
+ * __ipr_remove - Remove a single adapter
+ * @pdev:	pci device struct
+ *
+ * Adapter hot plug remove entry point.
+ *
+ * Return value:
+ * 	none
+ **/
+static void __ipr_remove(struct pci_dev *pdev)
+{
+	unsigned long host_lock_flags = 0;
+	struct ipr_ioa_cfg *ioa_cfg = pci_get_drvdata(pdev);
+	ENTER;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, host_lock_flags);
+	ipr_initiate_ioa_bringdown(ioa_cfg, IPR_SHUTDOWN_NORMAL);
+
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, host_lock_flags);
+	wait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);
+	spin_lock_irqsave(ioa_cfg->host->host_lock, host_lock_flags);
+
+	spin_lock(&ipr_driver_lock);
+	list_del(&ioa_cfg->queue);
+	spin_unlock(&ipr_driver_lock);
+
+	if (ioa_cfg->sdt_state == ABORT_DUMP)
+		ioa_cfg->sdt_state = WAIT_FOR_DUMP;
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, host_lock_flags);
+
+	ipr_free_all_resources(ioa_cfg);
+
+	LEAVE;
+}
+
+/**
+ * ipr_remove - IOA hot plug remove entry point
+ * @pdev:	pci device struct
+ *
+ * Adapter hot plug remove entry point.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_remove(struct pci_dev *pdev)
+{
+	struct ipr_ioa_cfg *ioa_cfg = pci_get_drvdata(pdev);
+
+	ENTER;
+
+	ioa_cfg->allow_cmds = 0;
+	flush_scheduled_work();
+	ipr_remove_trace_file(&ioa_cfg->host->shost_classdev.kobj,
+			      &ipr_trace_attr);
+	ipr_remove_dump_file(&ioa_cfg->host->shost_classdev.kobj,
+			     &ipr_dump_attr);
+	scsi_remove_host(ioa_cfg->host);
+
+	__ipr_remove(pdev);
+
+	LEAVE;
+}
+
+/**
+ * ipr_probe - Adapter hot plug add entry point
+ *
+ * Return value:
+ * 	0 on success / non-zero on failure
+ **/
+static int __devinit ipr_probe(struct pci_dev *pdev,
+			       const struct pci_device_id *dev_id)
+{
+	struct ipr_ioa_cfg *ioa_cfg;
+	int rc;
+
+	rc = ipr_probe_ioa(pdev, dev_id);
+
+	if (rc)
+		return rc;
+
+	ioa_cfg = pci_get_drvdata(pdev);
+	rc = ipr_probe_ioa_part2(ioa_cfg);
+
+	if (rc) {
+		__ipr_remove(pdev);
+		return rc;
+	}
+
+	rc = scsi_add_host(ioa_cfg->host, &pdev->dev);
+
+	if (rc) {
+		__ipr_remove(pdev);
+		return rc;
+	}
+
+	rc = ipr_create_trace_file(&ioa_cfg->host->shost_classdev.kobj,
+				   &ipr_trace_attr);
+
+	if (rc) {
+		scsi_remove_host(ioa_cfg->host);
+		__ipr_remove(pdev);
+		return rc;
+	}
+
+	rc = ipr_create_dump_file(&ioa_cfg->host->shost_classdev.kobj,
+				   &ipr_dump_attr);
+
+	if (rc) {
+		ipr_remove_trace_file(&ioa_cfg->host->shost_classdev.kobj,
+				      &ipr_trace_attr);
+		scsi_remove_host(ioa_cfg->host);
+		__ipr_remove(pdev);
+		return rc;
+	}
+
+	ioa_cfg->allow_ml_add_del = 1;
+	scsi_scan_host(ioa_cfg->host);
+	ipr_scan_vsets(ioa_cfg);
+	scsi_add_device(ioa_cfg->host, IPR_IOA_BUS, IPR_IOA_TARGET, IPR_IOA_LUN);
+	return 0;
+}
+
+/**
+ * ipr_shutdown - Shutdown handler.
+ * @dev:	device struct
+ *
+ * This function is invoked upon system shutdown/reboot. It will issue
+ * an adapter shutdown to the adapter to flush the write cache.
+ *
+ * Return value:
+ * 	none
+ **/
+static void ipr_shutdown(struct device *dev)
+{
+	struct ipr_ioa_cfg *ioa_cfg = pci_get_drvdata(to_pci_dev(dev));
+	unsigned long lock_flags = 0;
+
+	spin_lock_irqsave(ioa_cfg->host->host_lock, lock_flags);
+	ipr_initiate_ioa_bringdown(ioa_cfg, IPR_SHUTDOWN_NORMAL);
+	spin_unlock_irqrestore(ioa_cfg->host->host_lock, lock_flags);
+	wait_event(ioa_cfg->reset_wait_q, !ioa_cfg->in_reset_reload);
+}
+
+static struct pci_device_id ipr_pci_table[] __devinitdata = {
+	{ PCI_VENDOR_ID_MYLEX, PCI_DEVICE_ID_IBM_GEMSTONE,
+		PCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_5702,
+		0, 0, (kernel_ulong_t)&ipr_chip_cfg[0] },
+	{ PCI_VENDOR_ID_MYLEX, PCI_DEVICE_ID_IBM_GEMSTONE,
+		PCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_5703,
+	      0, 0, (kernel_ulong_t)&ipr_chip_cfg[0] },
+	{ PCI_VENDOR_ID_IBM, PCI_DEVICE_ID_IBM_SNIPE,
+		PCI_VENDOR_ID_IBM, IPR_SUBS_DEV_ID_2780,
+		0, 0, (kernel_ulong_t)&ipr_chip_cfg[1] },
+	{ }
+};
+MODULE_DEVICE_TABLE(pci, ipr_pci_table);
+
+static struct pci_driver ipr_driver = {
+	.name = IPR_NAME,
+	.id_table = ipr_pci_table,
+	.probe = ipr_probe,
+	.remove = ipr_remove,
+	.driver = {
+		.shutdown = ipr_shutdown,
+	},
+};
+
+/**
+ * ipr_init - Module entry point
+ *
+ * Return value:
+ * 	0 on success / non-zero on failure
+ **/
+static int __init ipr_init(void)
+{
+	ipr_info("IBM Power RAID SCSI Device Driver version: %s\n",
+		 IPR_DRIVER_VERSION);
+
+	pci_register_driver(&ipr_driver);
+
+	return 0;
+}
+
+/**
+ * ipr_exit - Module unload
+ *
+ * Module unload entry point.
+ *
+ * Return value:
+ * 	none
+ **/
+static void __exit ipr_exit(void)
+{
+	pci_unregister_driver(&ipr_driver);
+}
+
+module_init(ipr_init);
+module_exit(ipr_exit);
diff -purN linux-2.5/drivers/scsi/ipr.h linuxppc64-2.5/drivers/scsi/ipr.h
--- linux-2.5/drivers/scsi/ipr.h	1970-01-01 00:00:00.000000000 +0000
+++ linuxppc64-2.5/drivers/scsi/ipr.h	2004-03-01 19:33:24.000000000 +0000
@@ -0,0 +1,1245 @@
+/*
+ * ipr.h -- driver for IBM Power Linux RAID adapters
+ *
+ * Written By: Brian King, IBM Corporation
+ *
+ * Copyright (C) 2003, 2004 IBM Corporation
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ *
+ */
+
+#ifndef _IPR_H
+#define _IPR_H
+
+#include <linux/types.h>
+#include <linux/completion.h>
+#include <linux/list.h>
+#include <scsi/scsi.h>
+#include <scsi/scsi_cmnd.h>
+#ifdef CONFIG_KDB
+#include <asm/kdb.h>
+#endif
+
+/*
+ * Literals
+ */
+#define IPR_DRIVER_VERSION "2.0.0-7 (February 27, 2004)"
+
+/*
+ * IPR_DBG_TRACE: Setting this to 1 will turn on some general function tracing
+ *			resulting in a bunch of extra debugging printks to the console
+ *
+ * IPR_DEBUG:	Setting this to 1 will turn on some error path tracing.
+ *			Enables the ipr_trace macro.
+ */
+#ifdef IPR_DEBUG_ALL
+#define IPR_DEBUG				1
+#define IPR_DBG_TRACE			1
+#else
+#define IPR_DEBUG				0
+#define IPR_DBG_TRACE			0
+#endif
+
+/*
+ * IPR_MAX_CMD_PER_LUN: This defines the maximum number of outstanding
+ *	ops per device for devices not running tagged command queuing.
+ *	This can be adjusted at runtime through sysfs device attributes.
+ */
+#define IPR_MAX_CMD_PER_LUN				6
+
+/*
+ * IPR_NUM_BASE_CMD_BLKS: This defines the maximum number of
+ *	ops the mid-layer can send to the adapter.
+ */
+#define IPR_NUM_BASE_CMD_BLKS				100
+
+#define IPR_SUBS_DEV_ID_2780	0x0264
+#define IPR_SUBS_DEV_ID_5702	0x0266
+#define IPR_SUBS_DEV_ID_5703	0x0278
+
+#define IPR_NAME				"ipr"
+
+/*
+ * Return codes
+ */
+#define IPR_RC_JOB_CONTINUE		1
+#define IPR_RC_JOB_RETURN		2
+
+/* FIXME - these should exist on all architectures */
+#ifndef pci_dma_error
+#if defined(CONFIG_PPC64)
+#define pci_dma_error(x) ((x) == ((dma_addr_t)-1))
+#else
+#define pci_dma_error(x) (0)
+#endif
+#endif
+
+/*
+ * IOASCs
+ */
+#define IPR_IOASC_NR_INIT_CMD_REQUIRED		0x02040200
+#define IPR_IOASC_SYNC_REQUIRED			0x023f0000
+#define IPR_IOASC_MED_DO_NOT_REALLOC		0x03110C00
+#define IPR_IOASC_HW_SEL_TIMEOUT			0x04050000
+#define IPR_IOASC_HW_DEV_BUS_STATUS			0x04448500
+#define	IPR_IOASC_IOASC_MASK			0xFFFFFF00
+#define	IPR_IOASC_SCSI_STATUS_MASK		0x000000FF
+#define IPR_IOASC_IR_RESOURCE_HANDLE		0x05250000
+#define IPR_IOASC_BUS_WAS_RESET			0x06290000
+#define IPR_IOASC_BUS_WAS_RESET_BY_OTHER		0x06298000
+#define IPR_IOASC_ABORTED_CMD_TERM_BY_HOST	0x0B5A0000
+
+#define IPR_FIRST_DRIVER_IOASC			0x10000000
+#define IPR_IOASC_IOA_WAS_RESET			0x10000001
+#define IPR_IOASC_PCI_ACCESS_ERROR			0x10000002
+
+#define IPR_NUM_LOG_HCAMS				2
+#define IPR_NUM_CFG_CHG_HCAMS				2
+#define IPR_NUM_HCAMS	(IPR_NUM_LOG_HCAMS + IPR_NUM_CFG_CHG_HCAMS)
+#define IPR_MAX_NUM_TARGETS_PER_BUS			0x10
+#define IPR_MAX_NUM_LUNS_PER_TARGET			256
+#define IPR_MAX_NUM_VSET_LUNS_PER_TARGET	8
+#define IPR_VSET_BUS					0xff
+#define IPR_IOA_BUS						0xff
+#define IPR_IOA_TARGET					0xff
+#define IPR_IOA_LUN						0xff
+#define IPR_MAX_NUM_BUSES				4
+#define IPR_MAX_BUS_TO_SCAN				IPR_MAX_NUM_BUSES
+
+#define IPR_NUM_RESET_RELOAD_RETRIES		3
+
+/* We need resources for HCAMS, IOA reset, IOA bringdown, and ERP */
+#define IPR_NUM_INTERNAL_CMD_BLKS	(IPR_NUM_HCAMS + \
+                                     ((IPR_NUM_RESET_RELOAD_RETRIES + 1) * 2) + 3)
+
+#define IPR_MAX_COMMANDS		IPR_NUM_BASE_CMD_BLKS
+#define IPR_NUM_CMD_BLKS		(IPR_NUM_BASE_CMD_BLKS + \
+						IPR_NUM_INTERNAL_CMD_BLKS)
+
+#define IPR_MAX_PHYSICAL_DEVS				192
+
+#define IPR_MAX_SGLIST					64
+#define IPR_MAX_SECTORS					512
+#define IPR_MAX_CDB_LEN					16
+
+#define IPR_DEFAULT_BUS_WIDTH				16
+#define IPR_80MBs_SCSI_RATE		((80 * 10) / (IPR_DEFAULT_BUS_WIDTH / 8))
+#define IPR_U160_SCSI_RATE	((160 * 10) / (IPR_DEFAULT_BUS_WIDTH / 8))
+#define IPR_U320_SCSI_RATE	((320 * 10) / (IPR_DEFAULT_BUS_WIDTH / 8))
+#define IPR_MAX_SCSI_RATE(width) ((320 * 10) / ((width) / 8))
+
+#define IPR_IOA_RES_HANDLE				0xffffffff
+#define IPR_IOA_RES_ADDR				0x00ffffff
+
+/*
+ * Adapter Commands
+ */
+#define IPR_RESET_DEVICE				0xC3
+#define	IPR_RESET_TYPE_SELECT				0x80
+#define	IPR_LUN_RESET					0x40
+#define	IPR_TARGET_RESET					0x20
+#define	IPR_BUS_RESET					0x10
+#define IPR_ID_HOST_RR_Q				0xC4
+#define IPR_QUERY_IOA_CONFIG				0xC5
+#define IPR_ABORT_TASK					0xC7
+#define IPR_CANCEL_ALL_REQUESTS			0xCE
+#define IPR_HOST_CONTROLLED_ASYNC			0xCF
+#define	IPR_HCAM_CDB_OP_CODE_CONFIG_CHANGE	0x01
+#define	IPR_HCAM_CDB_OP_CODE_LOG_DATA		0x02
+#define IPR_SET_SUPPORTED_DEVICES			0xFB
+#define IPR_IOA_SHUTDOWN				0xF7
+#define	IPR_WR_BUF_DOWNLOAD_AND_SAVE			0x05
+
+/*
+ * Timeouts
+ */
+#define IPR_TIMEOUT_MULTIPLIER		2
+#define IPR_MAX_SECOND_RADIX_TIMEOUT	0x3fff
+#define IPR_TIMEOUT_MINUTE_RADIX		0x4000
+#define IPR_SHUTDOWN_TIMEOUT			(10 * 60 * HZ)
+#define IPR_ABBREV_SHUTDOWN_TIMEOUT		(10 * HZ)
+#define IPR_DEVICE_RESET_TIMEOUT		(30 * HZ)
+#define IPR_CANCEL_ALL_TIMEOUT		(30 * HZ)
+#define IPR_ABORT_TASK_TIMEOUT		(30 * HZ)
+#define IPR_INTERNAL_TIMEOUT			(30 * HZ)
+#define IPR_WRITE_BUFFER_TIMEOUT		(10 * 60 * HZ)
+#define IPR_SET_SUP_DEVICE_TIMEOUT		(2 * 60 * HZ)
+#define IPR_REQUEST_SENSE_TIMEOUT		(30 * HZ)
+#define IPR_OPERATIONAL_TIMEOUT		(5 * 60 * HZ)
+#define IPR_WAIT_FOR_RESET_TIMEOUT		(2 * HZ)
+#define IPR_CHECK_FOR_RESET_TIMEOUT		(HZ / 10)
+#define IPR_WAIT_FOR_BIST_TIMEOUT		(2 * HZ)
+#define IPR_DUMP_TIMEOUT			(15 * HZ)
+
+/*
+ * SCSI Literals
+ */
+#define IPR_VENDOR_ID_LEN			8
+#define IPR_PROD_ID_LEN				16
+#define IPR_SERIAL_NUM_LEN			8
+
+/*
+ * Hardware literals
+ */
+#define IPR_FMT2_MBX_ADDR_MASK				0x0fffffff
+#define IPR_FMT2_MBX_BAR_SEL_MASK			0xf0000000
+#define IPR_FMT2_MKR_BAR_SEL_SHIFT			28
+#define IPR_GET_FMT2_BAR_SEL(mbx) \
+(((mbx) & IPR_FMT2_MBX_BAR_SEL_MASK) >> IPR_FMT2_MKR_BAR_SEL_SHIFT)
+#define IPR_SDT_FMT2_BAR0_SEL				0x0
+#define IPR_SDT_FMT2_BAR1_SEL				0x1
+#define IPR_SDT_FMT2_BAR2_SEL				0x2
+#define IPR_SDT_FMT2_BAR3_SEL				0x3
+#define IPR_SDT_FMT2_BAR4_SEL				0x4
+#define IPR_SDT_FMT2_BAR5_SEL				0x5
+#define IPR_SDT_FMT2_EXP_ROM_SEL			0x8
+#define IPR_FMT2_SDT_READY_TO_USE			0xC4D4E3F2
+#define IPR_DOORBELL					0x82800000
+
+#define IPR_PCII_IOA_TRANS_TO_OPER			(0x80000000 >> 0)
+#define IPR_PCII_IOARCB_XFER_FAILED			(0x80000000 >> 3)
+#define IPR_PCII_IOA_UNIT_CHECKED			(0x80000000 >> 4)
+#define IPR_PCII_NO_HOST_RRQ				(0x80000000 >> 5)
+#define IPR_PCII_CRITICAL_OPERATION			(0x80000000 >> 6)
+#define IPR_PCII_IO_DEBUG_ACKNOWLEDGE		(0x80000000 >> 7)
+#define IPR_PCII_IOARRIN_LOST				(0x80000000 >> 27)
+#define IPR_PCII_MMIO_ERROR				(0x80000000 >> 28)
+#define IPR_PCII_PROC_ERR_STATE			(0x80000000 >> 29)
+#define IPR_PCII_HRRQ_UPDATED				(0x80000000 >> 30)
+#define IPR_PCII_CORE_ISSUED_RST_REQ		(0x80000000 >> 31)
+
+#define IPR_PCII_ERROR_INTERRUPTS \
+(IPR_PCII_IOARCB_XFER_FAILED | IPR_PCII_IOA_UNIT_CHECKED | \
+IPR_PCII_NO_HOST_RRQ | IPR_PCII_IOARRIN_LOST | IPR_PCII_MMIO_ERROR)
+
+#define IPR_PCII_OPER_INTERRUPTS \
+(IPR_PCII_ERROR_INTERRUPTS | IPR_PCII_HRRQ_UPDATED | IPR_PCII_IOA_TRANS_TO_OPER)
+
+#define IPR_UPROCI_RESET_ALERT			(0x80000000 >> 7)
+#define IPR_UPROCI_IO_DEBUG_ALERT			(0x80000000 >> 9)
+
+#define IPR_LDUMP_MAX_LONG_ACK_DELAY_IN_USEC		200000	/* 200 ms */
+#define IPR_LDUMP_MAX_SHORT_ACK_DELAY_IN_USEC		200000	/* 200 ms */
+
+/*
+ * Dump literals
+ */
+#define IPR_MAX_IOA_DUMP_SIZE				(4 * 1024 * 1024)
+#define IPR_NUM_SDT_ENTRIES				511
+#define IPR_MAX_NUM_DUMP_PAGES	((IPR_MAX_IOA_DUMP_SIZE / PAGE_SIZE) + 1)
+
+/*
+ * Misc literals
+ */
+#define IPR_NUM_IOADL_ENTRIES			IPR_MAX_SGLIST
+
+/*
+ * Adapter interface types
+ */
+
+struct ipr_res_addr {
+	u8 reserved;
+	u8 bus;
+	u8 target;
+	u8 lun;
+#define IPR_GET_PHYS_LOC(res_addr) \
+	(((res_addr).bus << 16) | ((res_addr).target << 8) | (res_addr).lun)
+}__attribute__((packed, aligned (4)));
+
+struct ipr_std_inq_vpids {
+	u8 vendor_id[IPR_VENDOR_ID_LEN];
+	u8 product_id[IPR_PROD_ID_LEN];
+}__attribute__((packed));
+
+struct ipr_std_inq_data {
+	u8 peri_qual_dev_type;
+#define IPR_STD_INQ_PERI_QUAL(peri) ((peri) >> 5)
+#define IPR_STD_INQ_PERI_DEV_TYPE(peri) ((peri) & 0x1F)
+
+	u8 removeable_medium_rsvd;
+#define IPR_STD_INQ_REMOVEABLE_MEDIUM 0x80
+
+#define IPR_IS_DASD_DEVICE(std_inq) \
+((IPR_STD_INQ_PERI_DEV_TYPE((std_inq).peri_qual_dev_type) == TYPE_DISK) && \
+!(((std_inq).removeable_medium_rsvd) & IPR_STD_INQ_REMOVEABLE_MEDIUM))
+
+#define IPR_IS_SES_DEVICE(std_inq) \
+(IPR_STD_INQ_PERI_DEV_TYPE((std_inq).peri_qual_dev_type) == TYPE_ENCLOSURE)
+
+	u8 version;
+	u8 aen_naca_fmt;
+	u8 additional_len;
+	u8 sccs_rsvd;
+	u8 bq_enc_multi;
+	u8 sync_cmdq_flags;
+
+	struct ipr_std_inq_vpids vpids;
+
+	u8 ros_rsvd_ram_rsvd[4];
+
+	u8 serial_num[IPR_SERIAL_NUM_LEN];
+}__attribute__ ((packed));
+
+struct ipr_config_table_entry {
+	u8 service_level;
+	u8 array_id;
+	u8 flags;
+#define IPR_IS_IOA_RESOURCE	0x80
+#define IPR_IS_ARRAY_MEMBER 0x20
+#define IPR_IS_HOT_SPARE	0x10
+
+	u8 rsvd_subtype;
+#define IPR_RES_SUBTYPE(res) (((res)->cfgte.rsvd_subtype) & 0x0f)
+#define IPR_SUBTYPE_AF_DASD			0
+#define IPR_SUBTYPE_GENERIC_SCSI	1
+#define IPR_SUBTYPE_VOLUME_SET		2
+
+	struct ipr_res_addr res_addr;
+	u32 res_handle;
+	u32 reserved4[2];
+	struct ipr_std_inq_data std_inq_data;
+}__attribute__ ((packed, aligned (4)));
+
+struct ipr_config_table_hdr {
+	u8 num_entries;
+	u8 flags;
+#define IPR_UCODE_DOWNLOAD_REQ	0x10
+	u16 reserved;
+}__attribute__((packed, aligned (4)));
+
+struct ipr_config_table {
+	struct ipr_config_table_hdr hdr;
+	struct ipr_config_table_entry dev[IPR_MAX_PHYSICAL_DEVS];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_hostrcb_cfg_ch_not {
+	struct ipr_config_table_entry cfgte;
+	u8 reserved[936];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_supported_device {
+	u16 data_length;
+	u8 reserved;
+	u8 num_records;
+	struct ipr_std_inq_vpids vpids;
+	u8 reserved2[16];
+}__attribute__((packed, aligned (4)));
+
+/* Command packet structure */
+struct ipr_cmd_pkt {
+	u16 reserved;		/* Reserved by IOA */
+	u8 request_type;
+#define IPR_RQTYPE_SCSICDB		0x00
+#define IPR_RQTYPE_IOACMD		0x01
+#define IPR_RQTYPE_HCAM			0x02
+
+	u8 luntar_luntrn;
+
+	u8 flags_hi;
+#define IPR_FLAGS_HI_WRITE_NOT_READ		0x80
+#define IPR_FLAGS_HI_NO_ULEN_CHK		0x20
+#define IPR_FLAGS_HI_SYNC_OVERRIDE		0x10
+#define IPR_FLAGS_HI_SYNC_COMPLETE		0x08
+#define IPR_FLAGS_HI_NO_LINK_DESC		0x04
+
+	u8 flags_lo;
+#define IPR_FLAGS_LO_ALIGNED_BFR		0x20
+#define IPR_FLAGS_LO_DELAY_AFTER_RST	0x10
+#define IPR_FLAGS_LO_UNTAGGED_TASK		0x00
+#define IPR_FLAGS_LO_SIMPLE_TASK		0x02
+#define IPR_FLAGS_LO_ORDERED_TASK		0x04
+#define IPR_FLAGS_LO_HEAD_OF_Q_TASK		0x06
+#define IPR_FLAGS_LO_ACA_TASK			0x08
+
+	u8 cdb[16];
+	u16 timeout;
+}__attribute__ ((packed, aligned(4)));
+
+/* IOA Request Control Block    128 bytes  */
+struct ipr_ioarcb {
+	u32 ioarcb_host_pci_addr;
+	u32 reserved;
+	u32 res_handle;
+	u32 host_response_handle;
+	u32 reserved1;
+	u32 reserved2;
+	u32 reserved3;
+
+	u32 write_data_transfer_length;
+	u32 read_data_transfer_length;
+	u32 write_ioadl_addr;
+	u32 write_ioadl_len;
+	u32 read_ioadl_addr;
+	u32 read_ioadl_len;
+
+	u32 ioasa_host_pci_addr;
+	u16 ioasa_len;
+	u16 reserved4;
+
+	struct ipr_cmd_pkt cmd_pkt;
+
+	u32 add_cmd_parms_len;
+	u32 add_cmd_parms[10];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_ioadl_desc {
+	u32 flags_and_data_len;
+#define IPR_IOADL_FLAGS_MASK		0xff000000
+#define IPR_IOADL_GET_FLAGS(x) (be32_to_cpu(x) & IPR_IOADL_FLAGS_MASK)
+#define IPR_IOADL_DATA_LEN_MASK		0x00ffffff
+#define IPR_IOADL_GET_DATA_LEN(x) (be32_to_cpu(x) & IPR_IOADL_DATA_LEN_MASK)
+#define IPR_IOADL_FLAGS_READ		0x48000000
+#define IPR_IOADL_FLAGS_READ_LAST	0x49000000
+#define IPR_IOADL_FLAGS_WRITE		0x68000000
+#define IPR_IOADL_FLAGS_WRITE_LAST	0x69000000
+#define IPR_IOADL_FLAGS_LAST		0x01000000
+
+	u32 address;
+}__attribute__((packed, aligned (8)));
+
+struct ipr_ioasa_vset {
+	u32 failing_lba_hi;
+	u32 failing_lba_lo;
+	u32 ioa_data[22];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_ioasa_af_dasd {
+	u32 failing_lba;
+}__attribute__((packed, aligned (4)));
+
+struct ipr_ioasa_gpdd {
+	u8 device_end_state;
+	u8 device_bus_phase;
+	u16 reserved;
+	u32 ioa_data[23];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_ioasa_raw {
+	u32 ioa_data[24];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_ioasa {
+	u32 ioasc;
+#define IPR_IOASC_SENSE_KEY(ioasc) ((ioasc) >> 24)
+#define IPR_IOASC_SENSE_CODE(ioasc) (((ioasc) & 0x00ff0000) >> 16)
+#define IPR_IOASC_SENSE_QUAL(ioasc) (((ioasc) & 0x0000ff00) >> 8)
+#define IPR_IOASC_SENSE_STATUS(ioasc) ((ioasc) & 0x000000ff)
+
+	u16 ret_stat_len;	/* Length of the returned IOASA */
+
+	u16 avail_stat_len;	/* Total Length of status available. */
+
+	u32 residual_data_len;	/* number of bytes in the host data */
+	/* buffers that were not used by the IOARCB command. */
+
+	u32 ilid;
+#define IPR_NO_ILID			0
+#define IPR_DRIVER_ILID		0xffffffff
+
+	u32 fd_ioasc;
+
+	u32 fd_phys_locator;
+
+	u32 fd_res_handle;
+
+	u32 ioasc_specific;	/* status code specific field */
+#define IPR_IOASC_SPECIFIC_MASK		0x00ffffff
+#define IPR_FIELD_POINTER_VALID		(0x80000000 >> 8)
+#define IPR_FIELD_POINTER_MASK		0x0000ffff
+
+	union {
+		struct ipr_ioasa_vset vset;
+		struct ipr_ioasa_af_dasd dasd;
+		struct ipr_ioasa_gpdd gpdd;
+		struct ipr_ioasa_raw raw;
+	};
+}__attribute__((packed, aligned (4)));
+
+struct ipr_mode_parm_hdr {
+	u8 length;
+	u8 medium_type;
+	u8 device_spec_parms;
+	u8 block_desc_len;
+}__attribute__((packed));
+
+struct ipr_mode_pages {
+	struct ipr_mode_parm_hdr hdr;
+	u8 data[255 - sizeof(struct ipr_mode_parm_hdr)];
+}__attribute__((packed));
+
+struct ipr_mode_page_hdr {
+	u8 ps_page_code;
+#define IPR_MODE_PAGE_PS	0x80
+#define IPR_GET_MODE_PAGE_CODE(hdr) ((hdr)->ps_page_code & 0x3F)
+	u8 page_length;
+}__attribute__ ((packed));
+
+struct ipr_dev_bus_entry {
+	struct ipr_res_addr res_addr;
+	u8 flags;
+#define IPR_SCSI_ATTR_ENABLE_QAS			0x80
+#define IPR_SCSI_ATTR_DISABLE_QAS			0x40
+#define IPR_SCSI_ATTR_QAS_MASK				0xC0
+#define IPR_SCSI_ATTR_ENABLE_TM				0x20
+#define IPR_SCSI_ATTR_NO_TERM_PWR			0x10
+#define IPR_SCSI_ATTR_TM_SUPPORTED			0x08
+#define IPR_SCSI_ATTR_LVD_TO_SE_NOT_ALLOWED	0x04
+
+	u8 scsi_id;
+	u8 bus_width;
+	u8 extended_reset_delay;
+#define IPR_EXTENDED_RESET_DELAY	7
+
+	u32 max_xfer_rate;
+
+	u8 spinup_delay;
+	u8 reserved3;
+	u16 reserved4;
+}__attribute__((packed, aligned (4)));
+
+struct ipr_mode_page28 {
+	struct ipr_mode_page_hdr hdr;
+	u8 num_entries;
+	u8 entry_length;
+	struct ipr_dev_bus_entry bus[0];
+}__attribute__((packed));
+
+struct ipr_ioa_vpd {
+	struct ipr_std_inq_data std_inq_data;
+	u8 ascii_part_num[12];
+	u8 reserved[40];
+	u8 ascii_plant_code[4];
+}__attribute__((packed));
+
+struct ipr_inquiry_page3 {
+	u8 peri_qual_dev_type;
+	u8 page_code;
+	u8 reserved1;
+	u8 page_length;
+	u8 ascii_len;
+	u8 reserved2[3];
+	u8 load_id[4];
+	u8 major_release;
+	u8 card_type;
+	u8 minor_release[2];
+	u8 ptf_number[4];
+	u8 patch_number[4];
+}__attribute__((packed));
+
+struct ipr_hostrcb_device_data_entry {
+	struct ipr_std_inq_vpids dev_vpids;
+	u8 dev_sn[IPR_SERIAL_NUM_LEN];
+	struct ipr_res_addr dev_res_addr;
+	struct ipr_std_inq_vpids new_dev_vpids;
+	u8 new_dev_sn[IPR_SERIAL_NUM_LEN];
+	struct ipr_std_inq_vpids ioa_last_with_dev_vpids;
+	u8 ioa_last_with_dev_sn[IPR_SERIAL_NUM_LEN];
+	struct ipr_std_inq_vpids cfc_last_with_dev_vpids;
+	u8 cfc_last_with_dev_sn[IPR_SERIAL_NUM_LEN];
+	u32 ioa_data[5];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_hostrcb_array_data_entry {
+	struct ipr_std_inq_vpids vpids;
+	u8 serial_num[IPR_SERIAL_NUM_LEN];
+	struct ipr_res_addr expected_dev_res_addr;
+	struct ipr_res_addr dev_res_addr;
+}__attribute__((packed, aligned (4)));
+
+struct ipr_hostrcb_type_ff_error {
+	u32 ioa_data[246];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_hostrcb_type_01_error {
+	u32 seek_counter;
+	u32 read_counter;
+	u8 sense_data[32];
+	u32 ioa_data[236];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_hostrcb_type_02_error {
+	struct ipr_std_inq_vpids ioa_vpids;
+	u8 ioa_sn[IPR_SERIAL_NUM_LEN];
+	struct ipr_std_inq_vpids cfc_vpids;
+	u8 cfc_sn[IPR_SERIAL_NUM_LEN];
+	struct ipr_std_inq_vpids ioa_last_attached_to_cfc_vpids;
+	u8 ioa_last_attached_to_cfc_sn[IPR_SERIAL_NUM_LEN];
+	struct ipr_std_inq_vpids cfc_last_attached_to_ioa_vpids;
+	u8 cfc_last_attached_to_ioa_sn[IPR_SERIAL_NUM_LEN];
+	u32 ioa_data[3];
+	u8 reserved[844];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_hostrcb_type_03_error {
+	struct ipr_std_inq_vpids ioa_vpids;
+	u8 ioa_sn[IPR_SERIAL_NUM_LEN];
+	struct ipr_std_inq_vpids cfc_vpids;
+	u8 cfc_sn[IPR_SERIAL_NUM_LEN];
+	u32 errors_detected;
+	u32 errors_logged;
+	u8 ioa_data[12];
+	struct ipr_hostrcb_device_data_entry dev_entry[3];
+	u8 reserved[444];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_hostrcb_type_04_error {
+	struct ipr_std_inq_vpids ioa_vpids;
+	u8 ioa_sn[IPR_SERIAL_NUM_LEN];
+	struct ipr_std_inq_vpids cfc_vpids;
+	u8 cfc_sn[IPR_SERIAL_NUM_LEN];
+	u8 ioa_data[12];
+	struct ipr_hostrcb_array_data_entry array_member[10];
+	u32 exposed_mode_adn;
+	u32 array_id;
+	struct ipr_std_inq_vpids incomp_dev_vpids;
+	u8 incomp_dev_sn[IPR_SERIAL_NUM_LEN];
+	u32 ioa_data2;
+	struct ipr_hostrcb_array_data_entry array_member2[8];
+	struct ipr_res_addr last_func_vset_res_addr;
+	u8 vset_serial_num[IPR_SERIAL_NUM_LEN];
+	u8 protection_level[8];
+	u8 reserved[124];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_hostrcb_error {
+	u32 failing_dev_ioasc;
+	struct ipr_res_addr failing_dev_res_addr;
+	u32 failing_dev_res_handle;
+	u32 prc;
+	union {
+		struct ipr_hostrcb_type_ff_error type_ff_error;
+		struct ipr_hostrcb_type_01_error type_01_error;
+		struct ipr_hostrcb_type_02_error type_02_error;
+		struct ipr_hostrcb_type_03_error type_03_error;
+		struct ipr_hostrcb_type_04_error type_04_error;
+	};
+}__attribute__((packed, aligned (4)));
+
+struct ipr_hostrcb_raw {
+	u32 data[sizeof(struct ipr_hostrcb_error)/sizeof(u32)];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_hostrcb {
+	u8 op_code;
+#define IPR_HOST_RCB_OP_CODE_CONFIG_CHANGE			0xE1
+#define IPR_HOST_RCB_OP_CODE_LOG_DATA				0xE2
+
+	u8 notify_type;
+#define IPR_HOST_RCB_NOTIF_TYPE_EXISTING_CHANGED	0x00
+#define IPR_HOST_RCB_NOTIF_TYPE_NEW_ENTRY			0x01
+#define IPR_HOST_RCB_NOTIF_TYPE_REM_ENTRY			0x02
+#define IPR_HOST_RCB_NOTIF_TYPE_ERROR_LOG_ENTRY		0x10
+#define IPR_HOST_RCB_NOTIF_TYPE_INFORMATION_ENTRY	0x11
+
+	u8 notifications_lost;
+#define IPR_HOST_RCB_NO_NOTIFICATIONS_LOST			0
+#define IPR_HOST_RCB_NOTIFICATIONS_LOST				0x80
+
+	u8 flags;
+#define IPR_HOSTRCB_INTERNAL_OPER	0x80
+#define IPR_HOSTRCB_ERR_RESP_SENT	0x40
+
+	u8 overlay_id;
+#define IPR_HOST_RCB_OVERLAY_ID_1				0x01
+#define IPR_HOST_RCB_OVERLAY_ID_2				0x02
+#define IPR_HOST_RCB_OVERLAY_ID_3				0x03
+#define IPR_HOST_RCB_OVERLAY_ID_4				0x04
+#define IPR_HOST_RCB_OVERLAY_ID_6				0x06
+#define IPR_HOST_RCB_OVERLAY_ID_DEFAULT			0xFF
+
+	u8 reserved1[3];
+	u32 ilid;
+	u32 time_since_last_ioa_reset;
+	u32 reserved2;
+	u32 length;
+
+	union {
+		struct ipr_hostrcb_error error;
+		struct ipr_hostrcb_cfg_ch_not ccn;
+		struct ipr_hostrcb_raw raw;
+	};
+
+	/* Driver added data */
+	u32 hostrcb_dma;
+	struct list_head queue;
+}__attribute__((packed, aligned (4)));
+
+/* IPR smart dump table structures */
+struct ipr_sdt_entry {
+	u32 bar_str_offset;
+	u32 end_offset;
+	u8 entry_byte;
+	u8 reserved[3];
+
+	u8 flags;
+#define IPR_SDT_ENDIAN		0x80
+#define IPR_SDT_VALID_ENTRY	0x20
+
+	u8 resv;
+	u16 priority;
+}__attribute__((packed, aligned (4)));
+
+struct ipr_sdt_header {
+	u32 state;
+	u32 num_entries;
+	u32 num_entries_used;
+	u32 dump_size;
+}__attribute__((packed, aligned (4)));
+
+struct ipr_sdt {
+	struct ipr_sdt_header hdr;
+	struct ipr_sdt_entry entry[IPR_NUM_SDT_ENTRIES];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_uc_sdt {
+	struct ipr_sdt_header hdr;
+	struct ipr_sdt_entry entry[1];
+}__attribute__((packed, aligned (4)));
+
+/*
+ * Driver types
+ */
+struct ipr_bus_attributes {
+	u8 bus;
+	u8 qas_enabled;
+	u8 bus_width;
+	u8 reserved;
+	u32 max_xfer_rate;
+};
+
+struct ipr_resource_entry {
+	struct ipr_config_table_entry cfgte;
+	u8 needs_sync_complete:1;
+	u8 in_erp:1;
+	u8 add_to_ml:1;
+	u8 del_from_ml:1;
+	u8 resetting_device:1;
+	u8 tcq_active:1;
+
+	int qdepth;
+	struct scsi_device *sdev;
+	struct list_head queue;
+};
+
+struct ipr_resource_hdr {
+	u16 num_entries;
+	u16 reserved;
+};
+
+struct ipr_resource_table {
+	struct ipr_resource_hdr hdr;
+	struct ipr_resource_entry dev[IPR_MAX_PHYSICAL_DEVS];
+};
+
+struct ipr_misc_cbs {
+	struct ipr_ioa_vpd ioa_vpd;
+	struct ipr_inquiry_page3 page3_data;
+	struct ipr_mode_pages mode_pages;
+	struct ipr_supported_device supp_dev;
+};
+
+struct ipr_interrupts {
+	unsigned long set_interrupt_mask_reg;
+	unsigned long clr_interrupt_mask_reg;
+	unsigned long sense_interrupt_mask_reg;
+	unsigned long clr_interrupt_reg;
+
+	unsigned long sense_interrupt_reg;
+	unsigned long ioarrin_reg;
+	unsigned long sense_uproc_interrupt_reg;
+	unsigned long set_uproc_interrupt_reg;
+	unsigned long clr_uproc_interrupt_reg;
+};
+
+struct ipr_chip_cfg_t {
+	u32 mailbox;
+	u8 cache_line_size;
+	struct ipr_interrupts regs;
+};
+
+enum ipr_shutdown_type {
+	IPR_SHUTDOWN_NORMAL = 0x00,
+	IPR_SHUTDOWN_PREPARE_FOR_NORMAL = 0x40,
+	IPR_SHUTDOWN_ABBREV = 0x80,
+	IPR_SHUTDOWN_NONE = 0x100
+};
+
+struct ipr_trace_entry {
+	u32 time;
+
+	u8 op_code;
+	u8 type;
+#define IPR_TRACE_START			0x00
+#define IPR_TRACE_FINISH		0xff
+	u16 cmd_index;
+
+	u32 res_handle;
+	union {
+		u32 ioasc;
+		u32 add_data;
+		u32 res_addr;
+	};
+};
+
+struct ipr_sglist {
+	u32 order;
+	u32 num_sg;
+	u32 buffer_len;
+	struct scatterlist scatterlist[1];
+};
+
+enum ipr_sdt_state {
+	INACTIVE,
+	WAIT_FOR_DUMP,
+	GET_DUMP,
+	ABORT_DUMP,
+	DUMP_OBTAINED
+};
+
+/* Per-controller data */
+struct ipr_ioa_cfg {
+	char eye_catcher[8];
+#define IPR_EYECATCHER			"iprcfg"
+
+	struct list_head queue;
+
+	u8 allow_interrupts:1;
+	u8 in_reset_reload:1;
+	u8 in_ioa_bringdown:1;
+	u8 ioa_unit_checked:1;
+	u8 ioa_is_dead:1;
+	u8 dump_taken:1;
+	u8 allow_cmds:1;
+	u8 allow_ml_add_del:1;
+
+	u16 type; /* CCIN of the card */
+
+	u8 log_level;
+#define IPR_MAX_LOG_LEVEL			4
+#define IPR_DEFAULT_LOG_LEVEL		2
+
+#define IPR_NUM_TRACE_INDEX_BITS	8
+#define IPR_NUM_TRACE_ENTRIES		(1 << IPR_NUM_TRACE_INDEX_BITS)
+#define IPR_TRACE_SIZE	(sizeof(struct ipr_trace_entry) * IPR_NUM_TRACE_ENTRIES)
+	char trace_start[8];
+#define IPR_TRACE_START_LABEL			"trace"
+	struct ipr_trace_entry *trace;
+	u32 trace_index:IPR_NUM_TRACE_INDEX_BITS;
+
+	/*
+	 * Queue for free command blocks
+	 */
+	char ipr_free_label[8];
+#define IPR_FREEQ_LABEL			"free-q"
+	struct list_head free_q;
+
+	/*
+	 * Queue for command blocks outstanding to the adapter
+	 */
+	char ipr_pending_label[8];
+#define IPR_PENDQ_LABEL			"pend-q"
+	struct list_head pending_q;
+
+	char cfg_table_start[8];
+#define IPR_CFG_TBL_START		"cfg"
+	struct ipr_config_table *cfg_table;
+	u32 cfg_table_dma;
+
+	char resource_table_label[8];
+#define IPR_RES_TABLE_LABEL		"res_tbl"
+	struct ipr_resource_entry *res_entries;
+	struct list_head free_res_q;
+	struct list_head used_res_q;
+
+	char ipr_hcam_label[8];
+#define IPR_HCAM_LABEL			"hcams"
+	struct ipr_hostrcb *hostrcb[IPR_NUM_HCAMS];
+	u32 hostrcb_dma[IPR_NUM_HCAMS];
+	struct list_head hostrcb_free_q;
+	struct list_head hostrcb_pending_q;
+
+	u32 *host_rrq;
+	u32 host_rrq_dma;
+#define IPR_HRRQ_REQ_RESP_HANDLE_MASK	0xfffffffc
+#define IPR_HRRQ_RESP_BIT_SET			0x00000002
+#define IPR_HRRQ_TOGGLE_BIT				0x00000001
+#define IPR_HRRQ_REQ_RESP_HANDLE_SHIFT	2
+	volatile u32 *hrrq_start;
+	volatile u32 *hrrq_end;
+	volatile u32 *hrrq_curr;
+	volatile u32 toggle_bit;
+
+	struct ipr_bus_attributes bus_attr[IPR_MAX_NUM_BUSES];
+
+	const struct ipr_chip_cfg_t *chip_cfg;
+
+	unsigned long hdw_dma_regs;	/* iomapped PCI memory space */
+	unsigned long hdw_dma_regs_pci;	/* raw PCI memory space */
+	unsigned long ioa_mailbox;
+	struct ipr_interrupts regs;
+
+	u32 pci_cfg_buf[64];
+	u16 saved_pcix_cmd_reg;
+	u16 reset_retries;
+
+	u32 errors_logged;
+
+	struct Scsi_Host *host;
+	struct pci_dev *pdev;
+	struct ipr_sglist *ucode_sglist;
+	struct ipr_mode_pages *saved_mode_pages;
+	u8 saved_mode_page_len;
+
+	struct work_struct work_q;
+
+	wait_queue_head_t reset_wait_q;
+
+	struct ipr_dump *dump;
+	enum ipr_sdt_state sdt_state;
+
+	struct ipr_misc_cbs *vpd_cbs;
+	u32 vpd_cbs_dma;
+
+	struct pci_pool *ipr_cmd_pool;
+
+	struct ipr_cmnd *reset_cmd;
+
+	char ipr_cmd_label[8];
+#define IPR_CMD_LABEL		"ipr_cmnd"
+	struct ipr_cmnd *ipr_cmnd_list[IPR_NUM_CMD_BLKS];
+	u32 ipr_cmnd_list_dma[IPR_NUM_CMD_BLKS];
+};
+
+struct ipr_cmnd {
+	struct ipr_ioarcb ioarcb;
+	struct ipr_ioasa ioasa;
+	struct ipr_ioadl_desc ioadl[IPR_NUM_IOADL_ENTRIES];
+	struct list_head queue;
+	struct scsi_cmnd *scsi_cmd;
+	struct completion completion;
+	struct timer_list timer;
+	void (*done) (struct ipr_cmnd *);
+	int (*job_step) (struct ipr_cmnd *);
+	u16 cmd_index;
+	u8 sense_buffer[SCSI_SENSE_BUFFERSIZE];
+	dma_addr_t sense_buffer_dma;
+	unsigned short dma_use_sg;
+	dma_addr_t dma_handle;
+	union {
+		enum ipr_shutdown_type shutdown_type;
+		struct ipr_hostrcb *hostrcb;
+		unsigned long time_left;
+		unsigned long scratch;
+		struct ipr_resource_entry *res;
+	};
+
+	struct ipr_ioa_cfg *ioa_cfg;
+};
+
+struct ipr_ses_table_entry {
+	char product_id[17];
+	char compare_product_id_byte[17];
+	u32 max_bus_speed_limit;	/* MB/sec limit for this backplane */
+};
+
+struct ipr_dump_header {
+	u32 eye_catcher;
+#define IPR_DUMP_EYE_CATCHER		0xC5D4E3F2
+	u32 len;
+	u32 num_entries;
+	u32 first_entry_offset;
+	u32 status;
+#define IPR_DUMP_STATUS_SUCCESS			0
+#define IPR_DUMP_STATUS_QUAL_SUCCESS		2
+#define IPR_DUMP_STATUS_FAILED			0xffffffff
+	u32 os;
+#define IPR_DUMP_OS_LINUX	0x4C4E5558
+	u32 driver_name;
+#define IPR_DUMP_DRIVER_NAME	0x49505232
+}__attribute__((packed, aligned (4)));
+
+struct ipr_dump_entry_header {
+	u32 eye_catcher;
+#define IPR_DUMP_EYE_CATCHER		0xC5D4E3F2
+	u32 len;
+	u32 num_elems;
+	u32 offset;
+	u32 data_type;
+#define IPR_DUMP_DATA_TYPE_ASCII	0x41534349
+#define IPR_DUMP_DATA_TYPE_BINARY	0x42494E41
+	u32 id;
+#define IPR_DUMP_IOA_DUMP_ID		0x494F4131
+#define IPR_DUMP_LOCATION_ID		0x4C4F4341
+#define IPR_DUMP_TRACE_ID		0x54524143
+#define IPR_DUMP_DRIVER_VERSION_ID	0x44525652
+#define IPR_DUMP_IOA_CTRL_BLK		0x494F4342
+#define IPR_DUMP_PEND_OPS		0x414F5053
+	u32 status;
+}__attribute__((packed, aligned (4)));
+
+struct ipr_dump_location_entry {
+	struct ipr_dump_entry_header hdr;
+	u8 location[BUS_ID_SIZE];
+}__attribute__((packed));
+
+struct ipr_dump_trace_entry {
+	struct ipr_dump_entry_header hdr;
+	u32 trace[IPR_TRACE_SIZE / sizeof(u32)];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_dump_version_entry {
+	struct ipr_dump_entry_header hdr;
+	u8 version[sizeof(IPR_DRIVER_VERSION)];
+};
+
+struct ipr_driver_dump {
+	struct ipr_dump_header hdr;
+	struct ipr_dump_version_entry version_entry;
+	struct ipr_dump_location_entry location_entry;
+	struct ipr_dump_trace_entry trace_entry;
+}__attribute__((packed));
+
+struct ipr_ioa_dump {
+	struct ipr_dump_entry_header hdr;
+	u32 next_page_index;
+	u32 page_offset;
+	u32 format;
+#define IPR_SDT_FMT2		2
+#define IPR_SDT_UNKNOWN		3
+	u32 reserved;
+	struct ipr_sdt sdt;
+	u32 *ioa_data[IPR_MAX_NUM_DUMP_PAGES];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_dump {
+	struct kobject kobj;
+	struct ipr_ioa_cfg *ioa_cfg;
+	struct ipr_driver_dump driver_dump;
+	struct ipr_ioa_dump ioa_dump;
+};
+
+struct ipr_error_table_t {
+	u32 ioasc;
+	int log_ioasa;
+	int log_hcam;
+	char *error;
+};
+
+struct ipr_software_inq_lid_info {
+    u32  load_id;
+    u32  timestamp[3];
+}__attribute__((packed, aligned (4)));
+
+struct ipr_ucode_image_header {
+    u32 header_length;
+    u32 lid_table_offset;
+    u8 major_release;
+    u8 card_type;
+    u8 minor_release[2];
+    u8 reserved[20];
+    char eyecatcher[16];
+    u32 num_lids;
+    struct ipr_software_inq_lid_info lid[1];
+}__attribute__((packed, aligned (4)));
+
+/*
+ * Macros
+ */
+#if IPR_DEBUG
+#define IPR_DBG_CMD(CMD) { (CMD); }
+#else
+#define IPR_DBG_CMD(CMD)
+#endif
+
+#define ipr_breakpoint_data KERN_ERR IPR_NAME\
+": %s: %s: Line: %d ipr_cfg: %p\n", __FILE__, \
+__FUNCTION__, __LINE__, ipr_cfg
+
+#if defined(CONFIG_KDB) && !defined(CONFIG_PPC_ISERIES)
+#define ipr_breakpoint {printk(ipr_breakpoint_data); KDB_ENTER();}
+#define ipr_breakpoint_or_die {printk(ipr_breakpoint_data); KDB_ENTER();}
+#else
+#define ipr_breakpoint
+#define ipr_breakpoint_or_die panic(ipr_breakpoint_data)
+#endif
+
+#ifdef CONFIG_SCSI_IPR_TRACE
+#define ipr_create_trace_file(kobj, attr) sysfs_create_bin_file(kobj, attr)
+#define ipr_remove_trace_file(kobj, attr) sysfs_remove_bin_file(kobj, attr)
+#else
+#define ipr_create_trace_file(kobj, attr) 0
+#define ipr_remove_trace_file(kobj, attr) do { } while(0)
+#endif
+
+#ifdef CONFIG_SCSI_IPR_DUMP
+#define ipr_create_dump_file(kobj, attr) sysfs_create_bin_file(kobj, attr)
+#define ipr_remove_dump_file(kobj, attr) sysfs_remove_bin_file(kobj, attr)
+#else
+#define ipr_create_dump_file(kobj, attr) 0
+#define ipr_remove_dump_file(kobj, attr) do { } while(0)
+#endif
+
+/*
+ * Error logging macros
+ */
+#define ipr_err(...) printk(KERN_ERR IPR_NAME ": "__VA_ARGS__);
+#define ipr_info(...) printk(KERN_INFO IPR_NAME ": "__VA_ARGS__);
+#define ipr_crit(...) printk(KERN_CRIT IPR_NAME ": "__VA_ARGS__);
+#define ipr_warn(...) printk(KERN_WARNING IPR_NAME": "__VA_ARGS__);
+#define ipr_dbg(...) IPR_DBG_CMD(printk(KERN_DEBUG IPR_NAME ": "__VA_ARGS__));
+
+#define ipr_sdev_printk(level, sdev, fmt, ...) \
+	printk(level IPR_NAME ": %d:%d:%d:%d: " fmt, sdev->host->host_no, \
+		sdev->channel, sdev->id, sdev->lun, ##__VA_ARGS__)
+
+#define ipr_sdev_err(sdev, fmt, ...) \
+	ipr_sdev_printk(KERN_ERR, sdev, fmt, ##__VA_ARGS__)
+
+#define ipr_sdev_info(sdev, fmt, ...) \
+	ipr_sdev_printk(KERN_INFO, sdev, fmt, ##__VA_ARGS__)
+
+#define ipr_res_printk(level, ioa_cfg, res, fmt, ...) \
+	printk(level IPR_NAME ": %d:%d:%d:%d: " fmt, ioa_cfg->host->host_no, \
+		res.bus, res.target, res.lun, ##__VA_ARGS__)
+
+#define ipr_res_err(ioa_cfg, res, fmt, ...) \
+	ipr_res_printk(KERN_ERR, ioa_cfg, res, fmt, ##__VA_ARGS__);
+#define ipr_res_dbg(ioa_cfg, res, fmt, ...) \
+	ipr_res_printk(KERN_DEBUG, ioa_cfg, res, fmt, ##__VA_ARGS__);
+
+#define ipr_trace ipr_dbg("%s: %s: Line: %d\n",\
+	__FILE__, __FUNCTION__, __LINE__);
+
+#if IPR_DBG_TRACE
+#define ENTER printk(KERN_DEBUG IPR_NAME": Entering %s\n", __FUNCTION__);
+#define LEAVE printk(KERN_DEBUG IPR_NAME": Leaving %s\n", __FUNCTION__);
+#else
+#define ENTER
+#define LEAVE
+#endif
+
+#define ipr_err_separator \
+ipr_err("----------------------------------------------------------\n")
+
+
+/*
+ * Inlines
+ */
+
+/**
+ * ipr_is_ioa_resource - Determine if a resource is the IOA
+ * @res:	resource entry struct
+ *
+ * Return value:
+ * 	1 if IOA / 0 if not IOA
+ **/
+static inline int ipr_is_ioa_resource(struct ipr_resource_entry *res)
+{
+	return (res->cfgte.flags & IPR_IS_IOA_RESOURCE) ? 1 : 0;
+}
+
+/**
+ * ipr_is_af_dasd_device - Determine if a resource is an AF DASD
+ * @res:	resource entry struct
+ *
+ * Return value:
+ * 	1 if AF DASD / 0 if not AF DASD
+ **/
+static inline int ipr_is_af_dasd_device(struct ipr_resource_entry *res)
+{
+	if (IPR_IS_DASD_DEVICE(res->cfgte.std_inq_data) &&
+	    !ipr_is_ioa_resource(res) &&
+	    IPR_RES_SUBTYPE(res) == IPR_SUBTYPE_AF_DASD)
+		return 1;
+	else
+		return 0;
+}
+
+/**
+ * ipr_is_vset_device - Determine if a resource is a VSET
+ * @res:	resource entry struct
+ *
+ * Return value:
+ * 	1 if VSET / 0 if not VSET
+ **/
+static inline int ipr_is_vset_device(struct ipr_resource_entry *res)
+{
+	if (IPR_IS_DASD_DEVICE(res->cfgte.std_inq_data) &&
+	    !ipr_is_ioa_resource(res) &&
+	    IPR_RES_SUBTYPE(res) == IPR_SUBTYPE_VOLUME_SET)
+		return 1;
+	else
+		return 0;
+}
+
+/**
+ * ipr_is_gscsi - Determine if a resource is a generic scsi resource
+ * @res:	resource entry struct
+ *
+ * Return value:
+ * 	1 if GSCSI / 0 if not GSCSI
+ **/
+static inline int ipr_is_gscsi(struct ipr_resource_entry *res)
+{
+	if (!ipr_is_ioa_resource(res) &&
+	    IPR_RES_SUBTYPE(res) == IPR_SUBTYPE_GENERIC_SCSI)
+		return 1;
+	else
+		return 0;
+}
+
+/**
+ * ipr_is_device - Determine if resource address is that of a device
+ * @res_addr:	resource address struct
+ *
+ * Return value:
+ * 	1 if AF / 0 if not AF
+ **/
+static inline int ipr_is_device(struct ipr_res_addr *res_addr)
+{
+	if ((res_addr->bus < IPR_MAX_NUM_BUSES) &&
+	    (res_addr->target < IPR_MAX_NUM_TARGETS_PER_BUS))
+		return 1;
+
+	return 0;
+}
+
+/**
+ * ipr_sdt_is_fmt2 - Determine if a SDT address is in format 2
+ * @sdt_word:	SDT address
+ *
+ * Return value:
+ * 	1 if format 2 / 0 if not
+ **/
+static inline int ipr_sdt_is_fmt2(u32 sdt_word)
+{
+	u32 bar_sel = IPR_GET_FMT2_BAR_SEL(sdt_word);
+
+	switch (bar_sel) {
+	case IPR_SDT_FMT2_BAR0_SEL:
+	case IPR_SDT_FMT2_BAR1_SEL:
+	case IPR_SDT_FMT2_BAR2_SEL:
+	case IPR_SDT_FMT2_BAR3_SEL:
+	case IPR_SDT_FMT2_BAR4_SEL:
+	case IPR_SDT_FMT2_BAR5_SEL:
+	case IPR_SDT_FMT2_EXP_ROM_SEL:
+		return 1;
+	};
+
+	return 0;
+}
+
+#endif
diff -purN linux-2.5/include/asm-ppc/unistd.h linuxppc64-2.5/include/asm-ppc/unistd.h
--- linux-2.5/include/asm-ppc/unistd.h	2004-03-16 10:29:21.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc/unistd.h	2004-03-17 00:47:00.000000000 +0000
@@ -280,6 +280,7 @@
 		register unsigned long __sc_5  __asm__ ("r5");		\
 		register unsigned long __sc_6  __asm__ ("r6");		\
 		register unsigned long __sc_7  __asm__ ("r7");		\
+		register unsigned long __sc_8  __asm__ ("r8");		\
 									\
 		__sc_loadargs_##nr(name, args);				\
 		__asm__ __volatile__					\
@@ -288,10 +289,10 @@
 			: "=&r" (__sc_0),				\
 			  "=&r" (__sc_3),  "=&r" (__sc_4),		\
 			  "=&r" (__sc_5),  "=&r" (__sc_6),		\
-			  "=&r" (__sc_7)				\
+			  "=&r" (__sc_7),  "=&r" (__sc_8)		\
 			: __sc_asm_input_##nr				\
 			: "cr0", "ctr", "memory",			\
-			  "r8", "r9", "r10","r11", "r12");		\
+			  "r9", "r10","r11", "r12");			\
 		__sc_ret = __sc_3;					\
 		__sc_err = __sc_0;					\
 	}								\
@@ -319,6 +320,9 @@
 #define __sc_loadargs_5(name, arg1, arg2, arg3, arg4, arg5)		\
 	__sc_loadargs_4(name, arg1, arg2, arg3, arg4);			\
 	__sc_7 = (unsigned long) (arg5)
+#define __sc_loadargs_6(name, arg1, arg2, arg3, arg4, arg5, arg6)	\
+	__sc_loadargs_5(name, arg1, arg2, arg3, arg4, arg5);		\
+	__sc_8 = (unsigned long) (arg6)
 
 #define __sc_asm_input_0 "0" (__sc_0)
 #define __sc_asm_input_1 __sc_asm_input_0, "1" (__sc_3)
@@ -326,6 +330,7 @@
 #define __sc_asm_input_3 __sc_asm_input_2, "3" (__sc_5)
 #define __sc_asm_input_4 __sc_asm_input_3, "4" (__sc_6)
 #define __sc_asm_input_5 __sc_asm_input_4, "5" (__sc_7)
+#define __sc_asm_input_6 __sc_asm_input_5, "6" (__sc_8)
 
 #define _syscall0(type,name)						\
 type name(void)								\
@@ -363,6 +368,12 @@ type name(type1 arg1, type2 arg2, type3 
 	__syscall_nr(5, type, name, arg1, arg2, arg3, arg4, arg5);	\
 }
 
+#define _syscall6(type,name,type1,arg1,type2,arg2,type3,arg3,type4,arg4,type5,arg5,type6,arg6) \
+type name(type1 arg1, type2 arg2, type3 arg3, type4 arg4, type5 arg5, type6 arg6) \
+{									\
+	__syscall_nr(6, type, name, arg1, arg2, arg3, arg4, arg5, arg6); \
+}
+
 #ifdef __KERNEL__
 
 #define __NR__exit __NR_exit
diff -purN linux-2.5/include/asm-ppc64/hardirq.h linuxppc64-2.5/include/asm-ppc64/hardirq.h
--- linux-2.5/include/asm-ppc64/hardirq.h	2003-09-11 04:07:58.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/hardirq.h	2004-02-18 06:28:00.000000000 +0000
@@ -80,7 +80,8 @@ typedef struct {
 
 #define irq_enter()		(preempt_count() += HARDIRQ_OFFSET)
 
-#ifdef CONFIG_PREEMPT
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_DEBUG_SPINLOCK_SLEEP)
+#include <linux/smp_lock.h>
 # define in_atomic()	((preempt_count() & ~PREEMPT_ACTIVE) != kernel_locked())
 # define IRQ_EXIT_OFFSET (HARDIRQ_OFFSET-1)
 #else
diff -purN linux-2.5/include/asm-ppc64/hvconsole.h linuxppc64-2.5/include/asm-ppc64/hvconsole.h
--- linux-2.5/include/asm-ppc64/hvconsole.h	2004-02-05 21:10:52.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/hvconsole.h	2004-03-19 17:27:30.000000000 +0000
@@ -22,9 +22,41 @@
 #ifndef _PPC64_HVCONSOLE_H
 #define _PPC64_HVCONSOLE_H
 
-extern int hvc_get_chars(int index, char *buf, int count);
-extern int hvc_put_chars(int index, const char *buf, int count);
-extern int hvc_count(int *start_termno);
+#include <linux/list.h>
+
+#define MAX_NR_HVC_CONSOLES	4
+
+extern int hvc_arch_get_chars(int index, char *buf, int count);
+extern int hvc_arch_put_chars(int index, const char *buf, int count);
+extern int hvc_arch_tiocmset(int index, unsigned int set, unsigned int clear);
+extern int hvc_arch_tiocmget(int index);
+extern int hvc_arch_find_vterms(void);
+
+extern int hvc_instantiate(void);
+
+/* hvterm_get/put_chars() do not work with HVSI console protocol; present only
+ * for HVCS console server driver */
+extern int hvterm_get_chars(uint32_t vtermno, char *buf, int count);
+extern int hvterm_put_chars(uint32_t vtermno, const char *buf, int count);
+
+/* Converged Location Code length */
+#define HVCS_CLC_LENGTH	79
+
+struct hvcs_partner_info {
+	/* list management */
+	struct list_head node;
+	/* partner unit address */
+	unsigned int unit_address;
+	/*partner partition ID */
+	unsigned int partition_ID;
+	/* CLC (79 chars) + 1 Null-term char */
+	char location_code[HVCS_CLC_LENGTH + 1];
+};
+
+extern int hvcs_free_partner_info(struct list_head *head);
+extern int hvcs_get_partner_info(unsigned int unit_address, struct list_head *head);
+extern int hvcs_register_connection(unsigned int unit_address, unsigned int p_partition_ID, unsigned int p_unit_address);
+extern int hvcs_free_connection(unsigned int unit_address);
 
 #endif /* _PPC64_HVCONSOLE_H */
 
diff -purN linux-2.5/include/asm-ppc64/hw_irq.h linuxppc64-2.5/include/asm-ppc64/hw_irq.h
--- linux-2.5/include/asm-ppc64/hw_irq.h	2004-01-21 01:50:56.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/hw_irq.h	2004-01-24 06:44:51.000000000 +0000
@@ -75,9 +75,24 @@ static inline void __do_save_and_cli(uns
 
 #endif /* CONFIG_PPC_ISERIES */
 
-#define mask_irq(irq) ({if (irq_desc[irq].handler && irq_desc[irq].handler->disable) irq_desc[irq].handler->disable(irq);})
-#define unmask_irq(irq) ({if (irq_desc[irq].handler && irq_desc[irq].handler->enable) irq_desc[irq].handler->enable(irq);})
-#define ack_irq(irq) ({if (irq_desc[irq].handler && irq_desc[irq].handler->ack) irq_desc[irq].handler->ack(irq);})
+#define mask_irq(irq)						\
+	({							\
+	 	irq_desc_t *desc = get_irq_desc(irq);		\
+		if (desc->handler && desc->handler->disable)	\
+			desc->handler->disable(irq);		\
+	})
+#define unmask_irq(irq)						\
+	({							\
+	 	irq_desc_t *desc = get_irq_desc(irq);		\
+		if (desc->handler && desc->handler->enable)	\
+			desc->handler->enable(irq);		\
+	})
+#define ack_irq(irq)						\
+	({							\
+	 	irq_desc_t *desc = get_irq_desc(irq);		\
+		if (desc->handler && desc->handler->ack)	\
+			desc->handler->ack(irq);		\
+	})
 
 /* Should we handle this via lost interrupts and IPIs or should we don't care like
  * we do now ? --BenH.
diff -purN linux-2.5/include/asm-ppc64/irq.h linuxppc64-2.5/include/asm-ppc64/irq.h
--- linux-2.5/include/asm-ppc64/irq.h	2004-02-25 02:54:12.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/irq.h	2004-02-26 10:56:03.000000000 +0000
@@ -11,6 +11,11 @@
 
 #include <asm/atomic.h>
 
+/*
+ * Maximum number of interrupt sources that we can handle.
+ */
+#define NR_IRQS		512
+
 extern void disable_irq(unsigned int);
 extern void disable_irq_nosync(unsigned int);
 extern void enable_irq(unsigned int);
@@ -18,12 +23,11 @@ extern void enable_irq(unsigned int);
 /* this number is used when no interrupt has been assigned */
 #define NO_IRQ			(-1)
 
-/*
- * this is the maximum number of virtual irqs we will use.
- */
-#define NR_IRQS			512
+#define get_irq_desc(irq) (&irq_desc[(irq)])
 
-#define NUM_8259_INTERRUPTS	16
+/* Define a way to iterate across irqs. */
+#define for_each_irq(i) \
+	for ((i) = 0; (i) < NR_IRQS; ++(i))
 
 /* Interrupt numbers are virtual in case they are sparsely
  * distributed by the hardware.
@@ -41,12 +45,35 @@ static inline unsigned int virt_irq_to_r
 	return virt_irq_to_real_map[virt_irq];
 }
 
+/*
+ * Because many systems have two overlapping names spaces for
+ * interrupts (ISA and XICS for example), and the ISA interrupts
+ * have historically not been easy to renumber, we allow ISA
+ * interrupts to take values 0 - 15, and shift up the remaining
+ * interrupts by 0x10.
+ */
+#define NUM_ISA_INTERRUPTS	0x10
+extern int __irq_offset_value;
+
+static inline int irq_offset_up(int irq)
+{
+	return(irq + __irq_offset_value);
+}
+
+static inline int irq_offset_down(int irq)
+{
+	return(irq - __irq_offset_value);
+}
+
+static inline int irq_offset_value(void)
+{
+	return __irq_offset_value;
+}
+
 static __inline__ int irq_canonicalize(int irq)
 {
 	return irq;
 }
 
-#define NR_MASK_WORDS	((NR_IRQS + 63) / 64)
-
 #endif /* _ASM_IRQ_H */
 #endif /* __KERNEL__ */
diff -purN linux-2.5/include/asm-ppc64/machdep.h linuxppc64-2.5/include/asm-ppc64/machdep.h
--- linux-2.5/include/asm-ppc64/machdep.h	2004-02-27 23:02:35.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/machdep.h	2004-03-17 15:42:02.000000000 +0000
@@ -11,6 +11,7 @@
 
 #include <linux/config.h>
 #include <linux/seq_file.h>
+#include <linux/irq.h>
 
 struct pt_regs;
 struct pci_bus;	
@@ -81,6 +82,7 @@ struct machdep_calls {
 	void		(*restart)(char *cmd);
 	void		(*power_off)(void);
 	void		(*halt)(void);
+	void		(*panic)(char *str);
 
 	int		(*set_rtc_time)(struct rtc_time *);
 	void		(*get_rtc_time)(struct rtc_time *);
diff -purN linux-2.5/include/asm-ppc64/rtas.h linuxppc64-2.5/include/asm-ppc64/rtas.h
--- linux-2.5/include/asm-ppc64/rtas.h	2004-03-16 11:30:38.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/rtas.h	2004-03-17 16:17:20.000000000 +0000
@@ -175,6 +175,7 @@ extern void call_rtas_display_status(cha
 extern void rtas_restart(char *cmd);
 extern void rtas_power_off(void);
 extern void rtas_halt(void);
+extern void rtas_os_term(char *str);
 extern int rtas_get_sensor(int sensor, int index, int *state);
 extern int rtas_get_power_level(int powerdomain, int *level);
 extern int rtas_set_power_level(int powerdomain, int level, int *setlevel);
@@ -219,6 +220,8 @@ extern void pSeries_log_error(char *buf,
 extern spinlock_t rtas_data_buf_lock;
 extern char rtas_data_buf[RTAS_DATA_BUF_SIZE];
 
+extern void rtas_stop_self(void);
+
 /* RMO buffer reserved for user-space RTAS use */
 extern unsigned long rtas_rmo_buf;
 
diff -purN linux-2.5/include/asm-ppc64/siginfo.h linuxppc64-2.5/include/asm-ppc64/siginfo.h
--- linux-2.5/include/asm-ppc64/siginfo.h	2003-09-11 04:03:18.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/siginfo.h	2004-03-19 18:33:59.000000000 +0000
@@ -10,6 +10,7 @@
 
 #define __ARCH_SI_PREAMBLE_SIZE	(4 * sizeof(int))
 #define SI_PAD_SIZE32		((SI_MAX_SIZE/sizeof(int)) - 3)
+#define __ARCH_SI_BAND_T 	long
 
 #include <asm-generic/siginfo.h>
 
diff -purN linux-2.5/include/asm-ppc64/smp.h linuxppc64-2.5/include/asm-ppc64/smp.h
--- linux-2.5/include/asm-ppc64/smp.h	2004-02-23 16:39:10.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/smp.h	2004-03-09 23:39:56.000000000 +0000
@@ -67,9 +67,14 @@ extern cpumask_t cpu_available_map;
 #endif
 #define PPC_MSG_DEBUGGER_BREAK  3
 
+extern cpumask_t irq_affinity[];
+
 void smp_init_iSeries(void);
 void smp_init_pSeries(void);
 
+extern int __cpu_disable(void);
+extern void __cpu_die(unsigned int cpu);
+extern void cpu_die(void) __attribute__((noreturn));
 #endif /* !(CONFIG_SMP) */
 
 #define get_hard_smp_processor_id(CPU) (paca[(CPU)].xHwProcNum)
diff -purN linux-2.5/include/asm-ppc64/spinlock.h linuxppc64-2.5/include/asm-ppc64/spinlock.h
--- linux-2.5/include/asm-ppc64/spinlock.h	2002-09-10 09:50:15.000000000 +0000
+++ linuxppc64-2.5/include/asm-ppc64/spinlock.h	2004-03-19 03:01:36.000000000 +0000
@@ -4,7 +4,7 @@
 /*
  * Simple spin lock operations.  
  *
- * Copyright (C) 2001 Paul Mackerras <paulus@au.ibm.com>, IBM
+ * Copyright (C) 2001-2004 Paul Mackerras <paulus@au.ibm.com>, IBM
  * Copyright (C) 2001 Anton Blanchard <anton@au.ibm.com>, IBM
  *
  * Type of int is used as a full 64b word is not necessary.
@@ -14,56 +14,72 @@
  * as published by the Free Software Foundation; either version
  * 2 of the License, or (at your option) any later version.
  */
+#include <linux/config.h>
+#include <linux/compiler.h>
+#include <linux/threads.h>
+#include <asm/memory.h>
+
 typedef struct {
 	volatile unsigned int lock;
 } spinlock_t;
 
+/*
+ * When we have shared processors, we need to be able to tell
+ * which virtual processor holds the lock, so we can tell the
+ * hypervisor to give our timeslice to them if they are not
+ * currently scheduled on a real processor.  To do this,
+ * we put paca->xPacaIndex + 0x10000 in the lock when it is
+ * held.
+ */
+
 #ifdef __KERNEL__
 #define SPIN_LOCK_UNLOCKED	(spinlock_t) { 0 }
 
 #define spin_is_locked(x)	((x)->lock != 0)
 
-static __inline__ int _raw_spin_trylock(spinlock_t *lock)
+/*
+ * This returns the old value in the lock, so we succeeded
+ * in getting the lock if the return value is 0.
+ */
+static __inline__ int __spin_trylock(spinlock_t *lock)
 {
-	unsigned int tmp;
+	unsigned int tmp, tmp2;
 
 	__asm__ __volatile__(
-"1:	lwarx		%0,0,%1		# spin_trylock\n\
+"1:	lwarx		%0,0,%2		# __spin_trylock\n\
 	cmpwi		0,%0,0\n\
-	li		%0,0\n\
 	bne-		2f\n\
-	li		%0,1\n\
-	stwcx.		%0,0,%1\n\
+	lhz		%1,24(13)\n\
+	oris		%1,%1,1\n\
+	stwcx.		%1,0,%2\n\
 	bne-		1b\n\
 	isync\n\
-2:"	: "=&r"(tmp)
-	: "r"(&lock->lock)
+2:"	: "=&r" (tmp), "=&r" (tmp2)
+	: "r" (&lock->lock)
 	: "cr0", "memory");
 
 	return tmp;
 }
 
-static __inline__ void _raw_spin_lock(spinlock_t *lock)
+static __inline__ int _raw_spin_trylock(spinlock_t *lock)
 {
-	unsigned int tmp;
+	return __spin_trylock(lock) == 0;
+}
 
-	__asm__ __volatile__(
-	"b		2f		# spin_lock\n\
-1:"
-	HMT_LOW
-"	lwzx		%0,0,%1\n\
-	cmpwi		0,%0,0\n\
-	bne+		1b\n"
-	HMT_MEDIUM
-"2:	lwarx		%0,0,%1\n\
-	cmpwi		0,%0,0\n\
-	bne-		1b\n\
-	stwcx.		%2,0,%1\n\
-	bne-		2b\n\
-	isync"
-	: "=&r"(tmp)
-	: "r"(&lock->lock), "r"(1)
-	: "cr0", "memory");
+extern void __spin_yield(spinlock_t *);
+#if !defined(CONFIG_PPC_ISERIES) && !defined(CONFIG_PPC_SPLPAR)
+#define __spin_yield(x)		do { } while (0)
+#endif
+
+static __inline__ void _raw_spin_lock(spinlock_t *lock)
+{
+	while (unlikely(__spin_trylock(lock) != 0)) {
+		do {
+			HMT_low();
+			__spin_yield(lock);
+		} while (likely(lock->lock != 0));
+		HMT_medium();
+	}
 }
 
 static __inline__ void _raw_spin_unlock(spinlock_t *lock)
@@ -81,6 +97,10 @@ static __inline__ void _raw_spin_unlock(
  * can "mix" irq-safe locks - any writer needs to get a
  * irq-safe write-lock, but readers can get non-irqsafe
  * read-locks.
+ *
+ * For a write lock, we store 0x80000000 | paca->xPacaIndex,
+ * so that we can tell who holds the lock, which we need
+ * to know if we are running on shared processors.
  */
 typedef struct {
 	volatile signed int lock;
@@ -88,50 +108,48 @@ typedef struct {
 
 #define RW_LOCK_UNLOCKED (rwlock_t) { 0 }
 
-static __inline__ int _raw_read_trylock(rwlock_t *rw)
+/*
+ * This returns the old value in the lock + 1,
+ * so we got a read lock if the return value is > 0.
+ */
+static __inline__ int __read_trylock(rwlock_t *rw)
 {
-	unsigned int tmp;
-	unsigned int ret;
+	int tmp;
 
 	__asm__ __volatile__(
-"1:	lwarx		%0,0,%2		# read_trylock\n\
-	li		%1,0\n\
+"1:	lwarx		%0,0,%1		# read_trylock\n\
 	extsw		%0,%0\n\
 	addic.		%0,%0,1\n\
 	ble-		2f\n\
-	stwcx.		%0,0,%2\n\
+	stwcx.		%0,0,%1\n\
 	bne-		1b\n\
-	li		%1,1\n\
 	isync\n\
-2:"	: "=&r"(tmp), "=&r"(ret)
-	: "r"(&rw->lock)
-	: "cr0", "memory");
+2:"	: "=&r" (tmp)
+	: "r" (&rw->lock)
+	: "cr0", "xer", "memory");
 
-	return ret;
+	return tmp;
 }
 
-static __inline__ void _raw_read_lock(rwlock_t *rw)
+static __inline__ int _raw_read_trylock(rwlock_t *rw)
 {
-	unsigned int tmp;
+	return __read_trylock(rw) > 0;
+}
 
-	__asm__ __volatile__(
-	"b		2f		# read_lock\n\
-1:"
-	HMT_LOW
-"	lwax		%0,0,%1\n\
-	cmpwi		0,%0,0\n\
-	blt+		1b\n"
-	HMT_MEDIUM
-"2:	lwarx		%0,0,%1\n\
-	extsw		%0,%0\n\
-	addic.		%0,%0,1\n\
-	ble-		1b\n\
-	stwcx.		%0,0,%1\n\
-	bne-		2b\n\
-	isync"
-	: "=&r"(tmp)
-	: "r"(&rw->lock)
-	: "cr0", "memory");
+extern void __rw_yield(rwlock_t *);
+#if !defined(CONFIG_PPC_ISERIES) && !defined(CONFIG_PPC_SPLPAR)
+#define __rw_yield(x)		do { } while (0)
+#endif
+
+static __inline__ void _raw_read_lock(rwlock_t *rw)
+{
+	while (unlikely(__read_trylock(rw) <= 0)) {
+		do {
+			HMT_low();
+			__rw_yield(rw);
+		} while (likely(rw->lock < 0));
+		HMT_medium();
+	}
 }
 
 static __inline__ void _raw_read_unlock(rwlock_t *rw)
@@ -149,48 +167,44 @@ static __inline__ void _raw_read_unlock(
 	: "cr0", "memory");
 }
 
-static __inline__ int _raw_write_trylock(rwlock_t *rw)
+/*
+ * This returns the old value in the lock,
+ * so we got the write lock if the return value is 0.
+ */
+static __inline__ int __write_trylock(rwlock_t *rw)
 {
-	unsigned int tmp;
-	unsigned int ret;
+	int tmp, tmp2;
 
 	__asm__ __volatile__(
 "1:	lwarx		%0,0,%2		# write_trylock\n\
 	cmpwi		0,%0,0\n\
-	li		%1,0\n\
 	bne-		2f\n\
-	stwcx.		%3,0,%2\n\
+	lhz		%1,24(13)\n\
+	oris		%1,%1,0x8000\n\
+	stwcx.		%1,0,%2\n\
 	bne-		1b\n\
-	li		%1,1\n\
 	isync\n\
-2:"	: "=&r"(tmp), "=&r"(ret)
-	: "r"(&rw->lock), "r"(-1)
+2:"	: "=&r" (tmp), "=&r" (tmp2)
+	: "r" (&rw->lock)
 	: "cr0", "memory");
 
-	return ret;
+	return tmp;
 }
 
-static __inline__ void _raw_write_lock(rwlock_t *rw)
+static __inline__ int _raw_write_trylock(rwlock_t *rw)
 {
-	unsigned int tmp;
+	return __write_trylock(rw) == 0;
+}
 
-	__asm__ __volatile__(
-	"b		2f		# write_lock\n\
-1:"
-	HMT_LOW
-	"lwax		%0,0,%1\n\
-	cmpwi		0,%0,0\n\
-	bne+		1b\n"
-	HMT_MEDIUM
-"2:	lwarx		%0,0,%1\n\
-	cmpwi		0,%0,0\n\
-	bne-		1b\n\
-	stwcx.		%2,0,%1\n\
-	bne-		2b\n\
-	isync"
-	: "=&r"(tmp)
-	: "r"(&rw->lock), "r"(-1)
-	: "cr0", "memory");
+static __inline__ void _raw_write_lock(rwlock_t *rw)
+{
+	while (unlikely(__write_trylock(rw) != 0)) {
+		do {
+			HMT_low();
+			__rw_yield(rw);
+		} while (likely(rw->lock != 0));
+		HMT_medium();
+	}
 }
 
 static __inline__ void _raw_write_unlock(rwlock_t *rw)
diff -purN linux-2.5/include/linux/pci_ids.h linuxppc64-2.5/include/linux/pci_ids.h
--- linux-2.5/include/linux/pci_ids.h	2004-03-16 20:49:40.000000000 +0000
+++ linuxppc64-2.5/include/linux/pci_ids.h	2004-03-17 01:54:00.000000000 +0000
@@ -435,7 +435,9 @@
 #define PCI_DEVICE_ID_IBM_CPC710_PCI64	0x00fc
 #define PCI_DEVICE_ID_IBM_CPC710_PCI32	0x0105
 #define	PCI_DEVICE_ID_IBM_405GP		0x0156
+#define PCI_DEVICE_ID_IBM_SNIPE		0x0180
 #define PCI_DEVICE_ID_IBM_SERVERAIDI960	0x01bd
+#define PCI_DEVICE_ID_IBM_GEMSTONE		0xB166
 #define PCI_DEVICE_ID_IBM_MPIC_2	0xffff
 
 #define PCI_VENDOR_ID_COMPEX2		0x101a // pci.ids says "AT&T GIS (NCR)"
diff -purN linux-2.5/include/linux/preempt.h linuxppc64-2.5/include/linux/preempt.h
--- linux-2.5/include/linux/preempt.h	2004-02-25 10:42:02.000000000 +0000
+++ linuxppc64-2.5/include/linux/preempt.h	2004-02-26 05:42:10.000000000 +0000
@@ -25,6 +25,17 @@ do { \
 
 asmlinkage void preempt_schedule(void);
 
+#define preempt_check_resched() \
+do { \
+	if (unlikely(test_thread_flag(TIF_NEED_RESCHED))) \
+		preempt_schedule(); \
+} while (0)
+#else
+#define preempt_check_resched()		do { } while (0)
+#endif
+
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_DEBUG_SPINLOCK_SLEEP)
+
 #define preempt_disable() \
 do { \
 	inc_preempt_count(); \
@@ -37,12 +48,6 @@ do { \
 	dec_preempt_count(); \
 } while (0)
 
-#define preempt_check_resched() \
-do { \
-	if (unlikely(test_thread_flag(TIF_NEED_RESCHED))) \
-		preempt_schedule(); \
-} while (0)
-
 #define preempt_enable() \
 do { \
 	preempt_enable_no_resched(); \
diff -purN linux-2.5/include/linux/sched.h linuxppc64-2.5/include/linux/sched.h
--- linux-2.5/include/linux/sched.h	2004-03-17 02:10:10.000000000 +0000
+++ linuxppc64-2.5/include/linux/sched.h	2004-03-19 00:32:40.000000000 +0000
@@ -549,6 +549,8 @@ extern void node_nr_running_init(void);
 
 /* Move tasks off this (offline) CPU onto another. */
 extern void migrate_all_tasks(void);
+/* Try to move me here, if possible. */
+void migrate_to_cpu(int dest_cpu);
 extern void set_user_nice(task_t *p, long nice);
 extern int task_prio(task_t *p);
 extern int task_nice(task_t *p);
diff -purN linux-2.5/kernel/cpu.c linuxppc64-2.5/kernel/cpu.c
--- linux-2.5/kernel/cpu.c	2004-03-18 23:51:18.000000000 +0000
+++ linuxppc64-2.5/kernel/cpu.c	2004-03-20 11:59:31.000000000 +0000
@@ -161,7 +161,7 @@
 	int ret;
 	void *hcpu = (void *)(long)cpu;
 
-	if ((ret = down_interruptible(&cpucontrol)) != 0)
+	if ((ret = lock_cpu_hotplug_interruptible()) != 0)
 		return ret;
 
 	if (cpu_online(cpu)) {
@@ -195,6 +195,6 @@
 	if (ret != 0)
 		notifier_call_chain(&cpu_chain, CPU_UP_CANCELED, hcpu);
 out:
-	up(&cpucontrol);
+	unlock_cpu_hotplug();
 	return ret;
 }
diff -purN linux-2.5/kernel/sched.c linuxppc64-2.5/kernel/sched.c
--- linux-2.5/kernel/sched.c	2004-03-19 04:54:57.000000000 +0000
+++ linuxppc64-2.5/kernel/sched.c	2004-03-20 11:59:31.000000000 +0000
@@ -724,7 +724,7 @@ void fastcall sched_fork(task_t *p)
 	INIT_LIST_HEAD(&p->run_list);
 	p->array = NULL;
 	spin_lock_init(&p->switch_lock);
-#ifdef CONFIG_PREEMPT
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_DEBUG_SPINLOCK_SLEEP)
 	/*
 	 * During context-switch we hold precisely one spinlock, which
 	 * schedule_tail drops. (in the common case it's this_rq()->lock,
@@ -2676,7 +2676,7 @@ void __init init_idle(task_t *idle, int 
 	local_irq_restore(flags);
 
 	/* Set the preempt count _outside_ the spinlocks! */
-#ifdef CONFIG_PREEMPT
+#if defined(CONFIG_PREEMPT) || defined(CONFIG_DEBUG_SPINLOCK_SLEEP)
 	idle->thread_info->preempt_count = (idle->lock_depth >= 0);
 #else
 	idle->thread_info->preempt_count = 0;
@@ -2884,6 +2884,9 @@ static int migration_call(struct notifie
 	case CPU_ONLINE:
 		/* Strictly unneccessary, as first user will wake it. */
 		wake_up_process(cpu_rq(cpu)->migration_thread);
+		/* Make sure it's hit high prio before we return. */
+		while (!rt_task(cpu_rq(cpu)->migration_thread))
+			yield();
 		break;
 #ifdef CONFIG_HOTPLUG_CPU
 	case CPU_UP_CANCELED:
