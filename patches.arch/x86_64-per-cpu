Index: linux/arch/x86_64/kernel/ioport.c
===================================================================
--- linux.orig/arch/x86_64/kernel/ioport.c	2004-09-12 21:08:44.%N +0200
+++ linux/arch/x86_64/kernel/ioport.c	2004-09-12 22:11:50.%N +0200
@@ -61,7 +61,7 @@
 	 * do it in the per-thread copy and in the TSS ...
 	 */
 	set_bitmap(t->io_bitmap_ptr, from, num, !turn_on);
-	tss = init_tss + get_cpu();
+	tss = &per_cpu(init_tss, get_cpu());
 	if (tss->io_bitmap_base == IO_BITMAP_OFFSET) { /* already active? */
 		set_bitmap(tss->io_bitmap, from, num, !turn_on);
 	} else {
Index: linux/arch/x86_64/kernel/suspend.c
===================================================================
--- linux.orig/arch/x86_64/kernel/suspend.c	2004-08-15 19:45:14.%N +0200
+++ linux/arch/x86_64/kernel/suspend.c	2004-09-12 22:13:01.%N +0200
@@ -129,7 +129,7 @@
 void fix_processor_context(void)
 {
 	int cpu = smp_processor_id();
-	struct tss_struct * t = init_tss + cpu;
+	struct tss_struct *t = &per_cpu(init_tss, cpu);
 
 	set_tss_desc(cpu,t);	/* This just modifies memory; should not be neccessary. But... This is neccessary, because 386 hardware has concept of busy TSS or some similar stupidity. */
 
Index: linux/arch/x86_64/kernel/process.c
===================================================================
--- linux.orig/arch/x86_64/kernel/process.c	2004-09-12 21:08:44.%N +0200
+++ linux/arch/x86_64/kernel/process.c	2004-09-12 22:13:34.%N +0200
@@ -251,7 +251,7 @@
 {
 	struct task_struct *me = current;
 	if (me->thread.io_bitmap_ptr) { 
-		struct tss_struct *tss = init_tss + get_cpu();
+		struct tss_struct *tss = &per_cpu(init_tss,get_cpu());
 		kfree(me->thread.io_bitmap_ptr); 
 		me->thread.io_bitmap_ptr = NULL;
 		tss->io_bitmap_base = INVALID_IO_BITMAP_OFFSET;
@@ -404,7 +404,7 @@
 	struct thread_struct *prev = &prev_p->thread,
 				 *next = &next_p->thread;
 	int cpu = smp_processor_id();  
-	struct tss_struct *tss = init_tss + cpu;
+	struct tss_struct *tss = &per_cpu(init_tss, cpu);
 
 	unlazy_fpu(prev_p);
 
Index: linux/arch/x86_64/kernel/setup64.c
===================================================================
--- linux.orig/arch/x86_64/kernel/setup64.c	2004-09-12 21:08:44.%N +0200
+++ linux/arch/x86_64/kernel/setup64.c	2004-09-12 22:12:40.%N +0200
@@ -237,7 +237,7 @@
 #else
 	int cpu = smp_processor_id();
 #endif
-	struct tss_struct * t = &init_tss[cpu];
+	struct tss_struct *t = &per_cpu(init_tss, cpu);
 	unsigned long v; 
 	char *estacks = NULL; 
 	struct task_struct *me;
Index: linux/arch/x86_64/kernel/traps.c
===================================================================
--- linux.orig/arch/x86_64/kernel/traps.c	2004-09-12 21:08:44.%N +0200
+++ linux/arch/x86_64/kernel/traps.c	2004-09-12 22:11:31.%N +0200
@@ -110,9 +110,10 @@
 { 
 	int k;
 	for (k = 0; k < N_EXCEPTION_STACKS; k++) {
-		unsigned long end = init_tss[cpu].ist[k] + EXCEPTION_STKSZ; 
+		struct tss_struct *tss = &per_cpu(init_tss, cpu);
+		unsigned long end = tss->ist[k] + EXCEPTION_STKSZ; 
 
-		if (stack >= init_tss[cpu].ist[k]  && stack <= end) 
+		if (stack >= tss->ist[k]  && stack <= end) 
 			return (unsigned long *)end;
 	}
 	return NULL;
Index: linux/arch/x86_64/kernel/smpboot.c
===================================================================
--- linux.orig/arch/x86_64/kernel/smpboot.c	2004-09-12 21:09:34.%N +0200
+++ linux/arch/x86_64/kernel/smpboot.c	2004-09-12 22:14:15.%N +0200
@@ -572,7 +572,7 @@
 	start_rip = setup_trampoline();
 
 	init_rsp = idle->thread.rsp; 
-	init_tss[cpu].rsp0 = init_rsp;
+	per_cpu(init_tss,cpu).rsp0 = init_rsp;
 	initial_code = start_secondary;
 	clear_ti_thread_flag(idle->thread_info, TIF_FORK);
 
Index: linux/arch/x86_64/kernel/init_task.c
===================================================================
--- linux.orig/arch/x86_64/kernel/init_task.c	2004-08-15 19:45:14.%N +0200
+++ linux/arch/x86_64/kernel/init_task.c	2004-09-12 22:22:02.%N +0200
@@ -44,8 +44,7 @@
  * section. Since TSS's are completely CPU-local, we want them
  * on exact cacheline boundaries, to eliminate cacheline ping-pong.
  */ 
-struct tss_struct init_tss[NR_CPUS] __cacheline_aligned;
-
+DEFINE_PER_CPU(struct tss_struct, init_tss) ____cacheline_maxaligned_in_smp;
 
 #define ALIGN_TO_4K __attribute__((section(".data.init_task")))
 
Index: linux/include/asm-x86_64/processor.h
===================================================================
--- linux.orig/include/asm-x86_64/processor.h	2004-09-12 21:08:46.%N +0200
+++ linux/include/asm-x86_64/processor.h	2004-09-12 22:19:53.%N +0200
@@ -18,6 +18,7 @@
 #include <asm/current.h>
 #include <asm/system.h>
 #include <asm/mmsegment.h>
+#include <asm/percpu.h>
 #include <linux/personality.h>
 
 #define TF_MASK		0x00000100
@@ -77,9 +78,6 @@
 #define X86_VENDOR_NUM 8
 #define X86_VENDOR_UNKNOWN 0xff
 
-extern struct cpuinfo_x86 boot_cpu_data;
-extern struct tss_struct init_tss[NR_CPUS];
-
 #ifdef CONFIG_SMP
 extern struct cpuinfo_x86 cpu_data[];
 #define current_cpu_data cpu_data[smp_processor_id()]
@@ -229,6 +227,9 @@
 	unsigned long io_bitmap[IO_BITMAP_LONGS + 1];
 } __attribute__((packed)) ____cacheline_aligned;
 
+extern struct cpuinfo_x86 boot_cpu_data;
+DECLARE_PER_CPU(struct tss_struct,init_tss);
+
 #define ARCH_MIN_TASKALIGN	16
 
 struct thread_struct {
