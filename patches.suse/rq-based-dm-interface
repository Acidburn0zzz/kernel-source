From da987d3df0947d2fd35e923b556bfcdafcf3af68 Mon Sep 17 00:00:00 2001
From: Hannes Reinecke <hare@suse.de>
Date: Fri, 8 Aug 2008 08:32:02 +0200
Subject: rq-based multipathing: device-mapper interface
References: FATE#302108

Add the required functions to the device-mapper core.

Signed-off-by: Hannes Reinecke <hare@suse.de>
---
 drivers/md/dm-table.c         |   66 +++
 drivers/md/dm.c               |  836 ++++++++++++++++++++++++++++++++++++++----
 drivers/md/dm.h               |   18 
 include/linux/device-mapper.h |   14 
 4 files changed, 866 insertions(+), 68 deletions(-)

--- a/drivers/md/dm-table.c
+++ b/drivers/md/dm-table.c
@@ -108,6 +108,8 @@ static void combine_restrictions_low(str
 	lhs->bounce_pfn = min_not_zero(lhs->bounce_pfn, rhs->bounce_pfn);
 
 	lhs->no_cluster |= rhs->no_cluster;
+
+	lhs->no_request_stacking |= rhs->no_request_stacking;
 }
 
 /*
@@ -437,6 +439,12 @@ static int __table_get_device(struct dm_
 			return r;
 	}
 
+	r = dm_init_md(t->md);
+	if (r) {
+		DMWARN("Cannot initialize device %s, error %d", path, r);
+		return r;
+	}
+
 	dd = find_device(&t->devices, dev);
 	if (!dd) {
 		dd = kmalloc(sizeof(*dd), GFP_KERNEL);
@@ -522,6 +530,9 @@ void dm_set_device_limits(struct dm_targ
 	rs->bounce_pfn = min_not_zero(rs->bounce_pfn, q->bounce_pfn);
 
 	rs->no_cluster |= !test_bit(QUEUE_FLAG_CLUSTER, &q->queue_flags);
+
+	if (!q->request_fn)
+		rs->no_request_stacking = 1;
 }
 EXPORT_SYMBOL_GPL(dm_set_device_limits);
 
@@ -648,8 +659,12 @@ int dm_split_args(int *argc, char ***arg
 	return 0;
 }
 
-static void check_for_valid_limits(struct io_restrictions *rs)
+static int check_for_valid_limits(struct io_restrictions *rs,
+				  struct mapped_device *md)
 {
+	int r = 0;
+	struct dm_table *t;
+
 	if (!rs->max_sectors)
 		rs->max_sectors = SAFE_MAX_SECTORS;
 	if (!rs->max_hw_sectors)
@@ -666,6 +681,39 @@ static void check_for_valid_limits(struc
 		rs->seg_boundary_mask = -1;
 	if (!rs->bounce_pfn)
 		rs->bounce_pfn = -1;
+
+	if (!dm_request_based(md))
+		return 0;
+
+	/* Allows to load only request stackable tables */
+	if (rs->no_request_stacking) {
+		DMERR("table load rejected: including non-request-stackable "
+		      "devices");
+		return -EINVAL;
+	}
+
+	t = dm_get_table(md);
+
+	/* Initial table loading must be allowed */
+	if (!t)
+		return 0;
+
+	if ((rs->max_sectors < t->limits.max_sectors) ||
+	    (rs->max_hw_sectors < t->limits.max_hw_sectors) ||
+	    (rs->max_phys_segments < t->limits.max_phys_segments) ||
+	    (rs->max_hw_segments < t->limits.max_hw_segments) ||
+	    (rs->hardsect_size > t->limits.hardsect_size) ||
+	    (rs->max_segment_size < t->limits.max_segment_size) ||
+	    (rs->seg_boundary_mask < t->limits.seg_boundary_mask) ||
+	    (rs->bounce_pfn < t->limits.bounce_pfn) ||
+	    (rs->no_cluster && !t->limits.no_cluster)) {
+		DMERR("table load rejected: shrinking current restriction");
+		r = -EINVAL;
+	}
+
+	dm_table_put(t);
+
+	return r;
 }
 
 int dm_table_add_target(struct dm_table *t, const char *type,
@@ -731,6 +779,16 @@ int dm_table_add_target(struct dm_table
 	return r;
 }
 
+void dm_table_set_request_based(struct dm_table *t)
+{
+	dm_set_request_based(t->md);
+}
+
+int dm_table_request_based(struct dm_table *t)
+{
+	return dm_request_based(t->md);
+}
+
 static int setup_indexes(struct dm_table *t)
 {
 	int i;
@@ -765,7 +823,9 @@ int dm_table_complete(struct dm_table *t
 	int r = 0;
 	unsigned int leaf_nodes;
 
-	check_for_valid_limits(&t->limits);
+	r = check_for_valid_limits(&t->limits, t->md);
+	if (r)
+		return r;
 
 	/* how many indexes will the btree have ? */
 	leaf_nodes = dm_div_up(t->num_targets, KEYS_PER_NODE);
@@ -978,3 +1038,5 @@ EXPORT_SYMBOL(dm_table_get_md);
 EXPORT_SYMBOL(dm_table_put);
 EXPORT_SYMBOL(dm_table_get);
 EXPORT_SYMBOL(dm_table_unplug_all);
+EXPORT_SYMBOL(dm_table_request_based);
+EXPORT_SYMBOL(dm_table_set_request_based);
--- a/drivers/md/dm.c
+++ b/drivers/md/dm.c
@@ -52,6 +52,27 @@ struct dm_target_io {
 	union map_info info;
 };
 
+/*
+ * For request based dm.
+ * One of these is allocated per request.
+ *
+ * Since assuming "original request : cloned request = 1 : 1" and
+ * a counter for number of clones like struct dm_io.io_count isn't needed,
+ * struct dm_io and struct target_io can be merged.
+ */
+struct dm_rq_target_io {
+	struct mapped_device *md;
+	struct dm_target *ti;
+	struct request *orig, clone;
+	int error;
+	union map_info info;
+};
+
+struct dm_clone_bio_info {
+	struct bio *orig;
+	struct request *rq;
+};
+
 union map_info *dm_get_mapinfo(struct bio *bio)
 {
 	if (bio && bio->bi_private)
@@ -59,6 +80,14 @@ union map_info *dm_get_mapinfo(struct bi
 	return NULL;
 }
 
+union map_info *dm_get_rq_mapinfo(struct request *rq)
+{
+	if (rq && rq->end_io_data)
+		return &((struct dm_rq_target_io *)rq->end_io_data)->info;
+	return NULL;
+}
+EXPORT_SYMBOL_GPL(dm_get_rq_mapinfo);
+
 #define MINOR_ALLOCED ((void *)-1)
 
 /*
@@ -70,6 +99,9 @@ union map_info *dm_get_mapinfo(struct bi
 #define DMF_FREEING 3
 #define DMF_DELETING 4
 #define DMF_NOFLUSH_SUSPENDING 5
+#define DMF_REQUEST_BASED 6
+#define DMF_BIO_BASED 7
+#define DMF_INITIALIZED 8
 
 /*
  * Work processed by per-device workqueue.
@@ -143,52 +175,71 @@ struct mapped_device {
 
 	/* forced geometry settings */
 	struct hd_geometry geometry;
+
+	/* For saving the address of __make_request for request based dm */
+	make_request_fn *saved_make_request_fn;
 };
 
 #define MIN_IOS 256
 static struct kmem_cache *_io_cache;
 static struct kmem_cache *_tio_cache;
+static struct kmem_cache *_rq_tio_cache;
+static struct kmem_cache *_bio_info_cache;
 
 static int __init local_init(void)
 {
-	int r;
+	int r = -ENOMEM;
 
 	/* allocate a slab for the dm_ios */
 	_io_cache = KMEM_CACHE(dm_io, 0);
 	if (!_io_cache)
-		return -ENOMEM;
+		return r;
 
 	/* allocate a slab for the target ios */
 	_tio_cache = KMEM_CACHE(dm_target_io, 0);
-	if (!_tio_cache) {
-		kmem_cache_destroy(_io_cache);
-		return -ENOMEM;
-	}
+	if (!_tio_cache)
+		goto out_free_io_cache;
+
+	_rq_tio_cache = KMEM_CACHE(dm_rq_target_io, 0);
+	if (!_rq_tio_cache)
+		goto out_free_tio_cache;
+
+	_bio_info_cache = KMEM_CACHE(dm_clone_bio_info, 0);
+	if (!_bio_info_cache)
+		goto out_free_rq_tio_cache;
 
 	r = dm_uevent_init();
-	if (r) {
-		kmem_cache_destroy(_tio_cache);
-		kmem_cache_destroy(_io_cache);
-		return r;
-	}
+	if (r)
+		goto out_free_bio_info_cache;
 
 	_major = major;
 	r = register_blkdev(_major, _name);
-	if (r < 0) {
-		kmem_cache_destroy(_tio_cache);
-		kmem_cache_destroy(_io_cache);
-		dm_uevent_exit();
-		return r;
-	}
+	if (r < 0)
+		goto out_uevent_exit;
 
 	if (!_major)
 		_major = r;
 
 	return 0;
+
+out_uevent_exit:
+	dm_uevent_exit();
+out_free_bio_info_cache:
+	kmem_cache_destroy(_bio_info_cache);
+out_free_rq_tio_cache:
+	kmem_cache_destroy(_rq_tio_cache);
+out_free_tio_cache:
+	kmem_cache_destroy(_tio_cache);
+out_free_io_cache:
+	kmem_cache_destroy(_io_cache);
+
+	return r;
 }
 
 static void local_exit(void)
 {
+	kmem_cache_destroy(_bio_info_cache);
+	kmem_cache_destroy(_rq_tio_cache);
 	kmem_cache_destroy(_tio_cache);
 	kmem_cache_destroy(_io_cache);
 	unregister_blkdev(_major, _name);
@@ -380,6 +431,28 @@ static void free_tio(struct mapped_devic
 	mempool_free(tio, md->tio_pool);
 }
 
+static inline struct dm_rq_target_io *alloc_rq_tio(struct mapped_device *md)
+{
+	return mempool_alloc(md->tio_pool, GFP_ATOMIC);
+}
+
+static inline void free_rq_tio(struct mapped_device *md,
+			       struct dm_rq_target_io *tio)
+{
+	mempool_free(tio, md->tio_pool);
+}
+
+static inline struct dm_clone_bio_info *alloc_bio_info(struct mapped_device *md)
+{
+	return mempool_alloc(md->io_pool, GFP_ATOMIC);
+}
+
+static inline void free_bio_info(struct mapped_device *md,
+				 struct dm_clone_bio_info *info)
+{
+	mempool_free(info, md->io_pool);
+}
+
 static void start_io_acct(struct dm_io *io)
 {
 	struct mapped_device *md = io->md;
@@ -568,6 +641,204 @@ static void clone_endio(struct bio *bio,
 	free_tio(md, tio);
 }
 
+/*
+ * Partial completion handling for request-based dm
+ */
+static void end_clone_bio(struct bio *bio, int error)
+{
+	struct dm_clone_bio_info *info = bio->bi_private;
+	struct dm_rq_target_io *tio = info->rq->end_io_data;
+	struct bio *orig_bio = info->orig;
+	unsigned int nr_bytes = info->orig->bi_size;
+
+	free_bio_info(tio->md, info);
+	bio->bi_private = tio->md->bs;
+	bio_put(bio);
+
+	if (tio->error) {
+		/*
+		 * An error has already been detected on the request.
+		 * Once error occurred, just let clone->end_io() handle
+		 * the remainder.
+		 */
+		return;
+	} else if (error) {
+		/*
+		 * Don't notice the error to the upper layer yet.
+		 * The error handling decision is made by the target driver,
+		 * when the request is completed.
+		 */
+		tio->error = error;
+		return;
+	}
+
+	/*
+	 * I/O for the bio successfully completed.
+	 * Notice the data completion to the upper layer.
+	 */
+
+	/*
+	 * bios are processed from the head of the list.
+	 * So the completing bio should always be rq->bio.
+	 * If it's not, something wrong is happening.
+	 */
+	if (tio->orig->bio != orig_bio)
+		DMWARN("bio completion is going in the middle of the request");
+
+	/*
+	 * Update the original request.
+	 * Do not use blk_end_request() here, because it may complete
+	 * the original request before the clone, and break the ordering.
+	 */
+	blk_update_request(tio->orig, 0, nr_bytes);
+}
+
+static void free_bio_clone(struct request *clone)
+{
+	struct dm_rq_target_io *tio = clone->end_io_data;
+	struct mapped_device *md = tio->md;
+	struct bio *bio;
+	struct dm_clone_bio_info *info;
+
+	while ((bio = clone->bio) != NULL) {
+		clone->bio = bio->bi_next;
+
+		info = bio->bi_private;
+		free_bio_info(md, info);
+
+		bio->bi_private = md->bs;
+		bio_put(bio);
+	}
+}
+
+static void dec_rq_pending(struct dm_rq_target_io *tio)
+{
+	if (!atomic_dec_return(&tio->md->pending))
+		/* nudge anyone waiting on suspend queue */
+		wake_up(&tio->md->wait);
+}
+
+static void __requeue_request(struct request_queue *q, struct request *rq)
+{
+	if (elv_queue_empty(q))
+		blk_plug_device(q);
+	blk_requeue_request(q, rq);
+}
+
+static void requeue_request(struct request_queue *q, struct request *rq)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(q->queue_lock, flags);
+	__requeue_request(q, rq);
+	spin_unlock_irqrestore(q->queue_lock, flags);
+}
+
+/*
+ * Complete the clone and the original request
+ */
+void dm_end_request(struct request *clone, int error)
+{
+	struct dm_rq_target_io *tio = clone->end_io_data;
+	struct request *orig = tio->orig;
+	struct request_queue *q_orig = orig->q;
+	unsigned int nr_bytes = blk_rq_bytes(orig);
+
+	if (error == DM_ENDIO_REQUEUE) {
+		/*
+		 * Requeue the original request of the clone.
+		 * Don't invoke blk_run_queue() so that the requeued request
+		 * won't be dispatched again soon.
+		 */
+		free_bio_clone(clone);
+		dec_rq_pending(tio);
+		free_rq_tio(tio->md, tio);
+
+		requeue_request(q_orig, orig);
+		return;
+	}
+
+	if (blk_pc_request(orig)) {
+		orig->errors = clone->errors;
+		orig->data_len = clone->data_len;
+
+		if (orig->sense)
+			/*
+			 * We are using the sense buffer of the original
+			 * request.
+			 * So setting the length of the sense data is enough.
+			 */
+			orig->sense_len = clone->sense_len;
+	}
+
+	free_bio_clone(clone);
+	dec_rq_pending(tio);
+	free_rq_tio(tio->md, tio);
+
+	if (unlikely(blk_end_request(orig, error, nr_bytes)))
+		BUG();
+
+	blk_run_queue(q_orig);
+}
+EXPORT_SYMBOL_GPL(dm_end_request);
+
+/*
+ * Request completion handler for request-based dm
+ */
+static void dm_softirq_done(struct request *orig)
+{
+	struct request *clone = orig->completion_data;
+	struct dm_rq_target_io *tio = clone->end_io_data;
+	dm_request_endio_fn rq_end_io = tio->ti->type->rq_end_io;
+	int error = tio->error, r;
+
+	if (rq_end_io) {
+		r = rq_end_io(tio->ti, clone, error, &tio->info);
+		if (r <= 0 || r == DM_ENDIO_REQUEUE)
+			/* The target wants to complete or requeue the I/O */
+			error = r;
+		else if (r == DM_ENDIO_INCOMPLETE)
+			/* The target will handle the I/O */
+			return;
+		else {
+			DMWARN("unimplemented target endio return value: %d",
+			       r);
+			BUG();
+		}
+	}
+
+	dm_end_request(clone, error);
+}
+
+/*
+ * Called with the queue lock held
+ */
+static void end_clone_request(struct request *clone, int error)
+{
+	struct dm_rq_target_io *tio = clone->end_io_data;
+	struct request *orig = tio->orig;
+
+	/*
+	 * For just cleaning up the information of the queue in which
+	 * the clone was dispatched.
+	 * The clone is *NOT* freed actually here because it is alloced from
+	 * dm own mempool and REQ_ALLOCED isn't set in clone->cmd_flags.
+	 */
+	__blk_put_request(clone->q, clone);
+
+	/*
+	 * Actual request completion is done in a softirq context which doesn't
+	 * hold the queue lock.  Otherwise, deadlock could occur because:
+	 *     - another request may be submitted by the upper level driver
+	 *       of the stacking during the completion
+	 *     - the submission which requires queue lock may be done
+	 *       against this queue
+	 */
+	tio->error = error;
+	orig->completion_data = clone;
+	blk_complete_request(orig);
+}
+
 static sector_t max_io_len(struct mapped_device *md,
 			   sector_t sector, struct dm_target *ti)
 {
@@ -882,7 +1153,7 @@ static int dm_merge_bvec(struct request_
  * The request function that just remaps the bio built up by
  * dm_merge_bvec.
  */
-static int dm_request(struct request_queue *q, struct bio *bio)
+static int _dm_request(struct request_queue *q, struct bio *bio)
 {
 	int r = -EIO;
 	int rw = bio_data_dir(bio);
@@ -932,12 +1203,267 @@ out_req:
 	return 0;
 }
 
+static int dm_make_request(struct request_queue *q, struct bio *bio)
+{
+	struct mapped_device *md = (struct mapped_device *)q->queuedata;
+
+	if (unlikely(bio_barrier(bio))) {
+		bio_endio(bio, -EOPNOTSUPP);
+		return 0;
+	}
+
+	if (unlikely(!md->map)) {
+		bio_endio(bio, -EIO);
+		return 0;
+	}
+
+	return md->saved_make_request_fn(q, bio); /* call __make_request() */
+}
+
+static int dm_request(struct request_queue *q, struct bio *bio)
+{
+	struct mapped_device *md = q->queuedata;
+
+	if (test_bit(DMF_REQUEST_BASED, &md->flags))
+		return dm_make_request(q, bio);
+
+	return _dm_request(q, bio);
+}
+
+void dm_dispatch_request(struct request *rq)
+{
+	rq->start_time = jiffies;
+	blk_submit_request(rq->q, rq);
+}
+EXPORT_SYMBOL_GPL(dm_dispatch_request);
+
+static void copy_request_info(struct request *clone, struct request *orig)
+{
+	INIT_LIST_HEAD(&clone->queuelist);
+	INIT_LIST_HEAD(&clone->donelist);
+	clone->q = NULL;
+	clone->cmd_flags = (rq_data_dir(orig) | REQ_NOMERGE);
+	clone->cmd_type = orig->cmd_type;
+	clone->sector = orig->sector;
+	clone->hard_sector = orig->hard_sector;
+	clone->nr_sectors = orig->nr_sectors;
+	clone->hard_nr_sectors = orig->hard_nr_sectors;
+	clone->current_nr_sectors = orig->current_nr_sectors;
+	clone->hard_cur_sectors = orig->hard_cur_sectors;
+	INIT_HLIST_NODE(&clone->hash);
+	clone->completion_data = NULL;
+	clone->elevator_private = NULL;
+	clone->elevator_private2 = NULL;
+	clone->rq_disk = NULL;
+	clone->start_time = jiffies;
+	clone->nr_phys_segments = orig->nr_phys_segments;
+	clone->nr_hw_segments = orig->nr_hw_segments;
+	clone->ioprio = orig->ioprio;
+	clone->special = NULL;
+	clone->buffer = orig->buffer;
+	clone->tag = -1;
+	clone->errors = 0;
+	clone->ref_count = 1;
+	clone->cmd_len = orig->cmd_len;
+	WARN_ON(orig->cmd != orig->__cmd);
+	clone->cmd = clone->__cmd;
+	if (orig->cmd_len) {
+		memcpy(clone->cmd, orig->cmd, sizeof(orig->cmd));
+	}
+	clone->data_len = orig->data_len;
+	clone->sense_len = orig->sense_len;
+	clone->data = orig->data;
+	clone->sense = orig->sense;
+	clone->timeout = 0;
+	clone->retries = 0;
+	clone->end_io = end_clone_request;
+	clone->next_rq = NULL;
+}
+
+static int clone_request_bios(struct request *clone, struct request *orig)
+{
+	struct dm_rq_target_io *tio = clone->end_io_data;
+	struct mapped_device *md = tio->md;
+	struct bio *bio, *orig_bio;
+	struct dm_clone_bio_info *info;
+
+	for (orig_bio = orig->bio; orig_bio; orig_bio = orig_bio->bi_next) {
+		info = alloc_bio_info(md);
+		if (!info)
+			goto free_and_out;
+
+		bio = bio_alloc_bioset(GFP_ATOMIC, orig_bio->bi_max_vecs,
+				       md->bs);
+		if (!bio) {
+			free_bio_info(md, info);
+			goto free_and_out;
+		}
+
+		__bio_clone(bio, orig_bio);
+		bio->bi_destructor = dm_bio_destructor;
+		bio->bi_end_io = end_clone_bio;
+		info->rq = clone;
+		info->orig = orig_bio;
+		bio->bi_private = info;
+
+		if (clone->bio) {
+			clone->biotail->bi_next = bio;
+			clone->biotail = bio;
+		} else
+			clone->bio = clone->biotail = bio;
+	}
+
+	return 0;
+
+free_and_out:
+	free_bio_clone(clone);
+
+	return -ENOMEM;
+}
+
+static int setup_clone(struct request *clone, struct request *orig)
+{
+	int r;
+
+	r = clone_request_bios(clone, orig);
+	if (r)
+		return r;
+
+	copy_request_info(clone, orig);
+
+	return 0;
+}
+
+static int clone_and_map_request(struct dm_target *ti, struct request *rq,
+				 struct mapped_device *md)
+{
+	int r;
+	struct request *clone;
+	struct dm_rq_target_io *tio;
+
+	tio = alloc_rq_tio(md); /* Only one for each original request */
+	if (!tio)
+		/* -ENOMEM */
+		goto requeue;
+	tio->md = md;
+	tio->orig = rq;
+	tio->error = 0;
+	tio->ti = ti;
+	memset(&tio->info, 0, sizeof(tio->info));
+
+	clone = &tio->clone;
+	clone->end_io_data = tio;
+	clone->bio = clone->biotail = NULL;
+	if (setup_clone(clone, rq))
+		/* -ENOMEM */
+		goto free_rq_tio_and_requeue;
+
+	atomic_inc(&md->pending);
+	r = ti->type->map_rq(ti, clone, &tio->info);
+	switch (r) {
+	case DM_MAPIO_SUBMITTED:
+		/* The target has taken the request to submit by itself */
+		break;
+	case DM_MAPIO_REMAPPED:
+		/* The clone has been remapped so dispatch it */
+		dm_dispatch_request(clone);
+		break;
+	case DM_MAPIO_REQUEUE:
+		/* The target wants to requeue the original request */
+		goto free_bio_clone_and_requeue;
+	default:
+		if (r > 0) {
+			DMWARN("unimplemented target map return value: %d", r);
+			BUG();
+		}
+
+		/*
+		 * The target wants to complete the original request.
+		 * Avoid printing "I/O error" message, since we didn't I/O.
+		 */
+		rq->cmd_flags |= REQ_QUIET;
+		dm_end_request(clone, r);
+		break;
+	}
+
+	return 0;
+
+free_bio_clone_and_requeue:
+	free_bio_clone(clone);
+	dec_rq_pending(tio);
+
+free_rq_tio_and_requeue:
+	free_rq_tio(md, tio);
+
+requeue:
+	/*
+	 * Actual requeue is done in dm_request_fn() after queue lock is held
+	 * so that we can avoid to get extra queue lock for the requeue
+	 */
+	return 1;
+}
+
+/*
+ * q->request_fn for request-based dm.
+ * Called with the queue lock held
+ */
+static void dm_request_fn(struct request_queue *q)
+{
+	int r;
+	struct mapped_device *md = (struct mapped_device *)q->queuedata;
+	struct dm_table *map = dm_get_table(md);
+	struct dm_target *ti;
+	dm_congested_fn congested;
+	struct request *rq;
+
+	/*
+	 * The check for blk_queue_stopped() needs here, because:
+	 *     - device suspend uses blk_stop_queue() and expects that
+	 *       no I/O will be dispatched any more after the queue stop
+	 *     - generic_unplug_device() doesn't call q->request_fn()
+	 *       when the queue is stopped, so no problem
+	 *     - but underlying device drivers may call q->request_fn()
+	 *       without the check through blk_run_queue()
+	 */
+	while (!blk_queue_plugged(q) && !blk_queue_stopped(q)) {
+		rq = elv_next_request(q);
+		if (!rq)
+			break;
+
+		ti = dm_table_find_target(map, rq->sector);
+		congested = ti->type->congested;
+		if (congested && congested(ti))
+			break;
+
+		blkdev_dequeue_request(rq);
+		spin_unlock(q->queue_lock);
+		r = clone_and_map_request(ti, rq, md);
+		spin_lock_irq(q->queue_lock);
+
+		if (r)
+			__requeue_request(q, rq);
+	}
+
+	dm_table_put(map);
+
+	return;
+}
+
+int dm_underlying_device_congested(struct request_queue *q)
+{
+	return blk_lld_busy(q);
+}
+EXPORT_SYMBOL_GPL(dm_underlying_device_congested);
+
 static void dm_unplug_all(struct request_queue *q)
 {
 	struct mapped_device *md = q->queuedata;
 	struct dm_table *map = dm_get_table(md);
 
 	if (map) {
+		if (test_bit(DMF_REQUEST_BASED, &md->flags))
+			generic_unplug_device(q);
+
 		dm_table_unplug_all(map);
 		dm_table_put(map);
 	}
@@ -951,6 +1477,9 @@ static int dm_any_congested(void *conges
 
 	if (!map || test_bit(DMF_BLOCK_IO, &md->flags))
 		r = bdi_bits;
+	else if (test_bit(DMF_REQUEST_BASED, &md->flags))
+		/* Request-based dm cares about only own queue */
+		r = md->queue->backing_dev_info.state & bdi_bits;
 	else
 		r = dm_table_any_congested(map, bdi_bits);
 
@@ -1033,6 +1562,121 @@ out:
 	return r;
 }
 
+static void init_queue(struct request_queue *q, struct mapped_device *md)
+{
+	q->queuedata = md;
+	q->backing_dev_info.congested_fn = dm_any_congested;
+	q->backing_dev_info.congested_data = md;
+	blk_queue_make_request(q, dm_request);
+	blk_queue_bounce_limit(q, BLK_BOUNCE_ANY);
+	q->unplug_fn = dm_unplug_all;
+	blk_queue_merge_bvec(q, dm_merge_bvec);
+}
+
+int dm_set_md_request_based(struct mapped_device *md)
+{
+	int r = 0;
+
+	if (test_bit(DMF_INITIALIZED, &md->flags))
+		/* Initialization is already done */
+		return 0;
+
+	md->io_pool = mempool_create_slab_pool(MIN_IOS, _bio_info_cache);
+	if (!md->io_pool)
+		return -ENOMEM;
+
+	md->tio_pool = mempool_create_slab_pool(MIN_IOS, _rq_tio_cache);
+	if (!md->tio_pool) {
+		r = -ENOMEM;
+		goto out_free_io_pool;
+	}
+
+	md->bs = bioset_create(MIN_IOS, MIN_IOS);
+	if (!md->bs) {
+		r = -ENOMEM;
+		goto out_free_tio_pool;
+	}
+
+	md->queue = blk_init_queue(dm_request_fn, NULL);
+	if (!md->queue) {
+		DMERR("request queue initialization for request-based failed");
+		r = -ENOMEM;
+		goto out_free_bs;
+	}
+
+	DMINFO("%s: activating request-based I/O", md->disk->disk_name);
+	md->saved_make_request_fn = md->queue->make_request_fn;
+	init_queue(md->queue, md);
+	blk_queue_softirq_done(md->queue, dm_softirq_done);
+	md->disk->queue = md->queue;
+	add_disk(md->disk);
+
+	return 0;
+
+out_cleanup_queue:
+	blk_cleanup_queue(md->queue);
+	md->disk->queue = md->queue = NULL;
+	md->saved_make_request_fn = NULL;
+
+out_free_bs:
+	bioset_free(md->bs);
+	md->bs = NULL;
+
+out_free_tio_pool:
+	mempool_destroy(md->tio_pool);
+	md->tio_pool = NULL;
+
+out_free_io_pool:
+	mempool_destroy(md->io_pool);
+	md->io_pool = NULL;
+
+	return r;
+}
+
+int dm_set_md_bio_based(struct mapped_device *md)
+{
+	if (test_bit(DMF_INITIALIZED, &md->flags)) {
+		/* Initialization is already done */
+		return 0;
+	}
+
+	md->io_pool = mempool_create_slab_pool(MIN_IOS, _io_cache);
+	if (!md->io_pool)
+		goto out;
+
+	md->tio_pool = mempool_create_slab_pool(MIN_IOS, _tio_cache);
+	if (!md->tio_pool)
+		goto out_free_io_pool;
+
+	md->bs = bioset_create(16, 16);
+	if (!md->bs)
+		goto out_free_tio_pool;
+
+	md->queue = blk_alloc_queue(GFP_KERNEL);
+	if (!md->queue) {
+		DMERR("request queue initialization for bio-based failed");
+		goto out_free_bs;
+	}
+
+	init_queue(md->queue, md);
+	md->disk->queue = md->queue;
+	add_disk(md->disk);
+
+	return 0;
+
+out_free_bs:
+	bioset_free(md->bs);
+	md->bs = NULL;
+out_free_tio_pool:
+	mempool_destroy(md->tio_pool);
+	md->tio_pool = NULL;
+out_free_io_pool:
+	mempool_destroy(md->io_pool);
+	md->io_pool = NULL;
+out:
+	return -ENOMEM;
+}
+
 static struct block_device_operations dm_blk_dops;
 
 /*
@@ -1070,30 +1714,10 @@ static struct mapped_device *alloc_dev(i
 	atomic_set(&md->uevent_seq, 0);
 	INIT_LIST_HEAD(&md->uevent_list);
 	spin_lock_init(&md->uevent_lock);
+	/* Defaults to BIO based */
+	set_bit(DMF_BIO_BASED, &md->flags);
 
-	md->queue = blk_alloc_queue(GFP_KERNEL);
-	if (!md->queue)
-		goto bad_queue;
-
-	md->queue->queuedata = md;
-	md->queue->backing_dev_info.congested_fn = dm_any_congested;
-	md->queue->backing_dev_info.congested_data = md;
-	blk_queue_make_request(md->queue, dm_request);
-	blk_queue_bounce_limit(md->queue, BLK_BOUNCE_ANY);
-	md->queue->unplug_fn = dm_unplug_all;
-	blk_queue_merge_bvec(md->queue, dm_merge_bvec);
-
-	md->io_pool = mempool_create_slab_pool(MIN_IOS, _io_cache);
-	if (!md->io_pool)
-		goto bad_io_pool;
-
-	md->tio_pool = mempool_create_slab_pool(MIN_IOS, _tio_cache);
-	if (!md->tio_pool)
-		goto bad_tio_pool;
-
-	md->bs = bioset_create(16, 16);
-	if (!md->bs)
-		goto bad_no_bioset;
+	/* md's queue and mempools will be allocated after the 1st table load */
 
 	md->disk = alloc_disk(1);
 	if (!md->disk)
@@ -1106,10 +1730,8 @@ static struct mapped_device *alloc_dev(i
 	md->disk->major = _major;
 	md->disk->first_minor = minor;
 	md->disk->fops = &dm_blk_dops;
-	md->disk->queue = md->queue;
 	md->disk->private_data = md;
 	sprintf(md->disk->disk_name, "dm-%d", minor);
-	add_disk(md->disk);
 	format_dev_t(md->name, MKDEV(_major, minor));
 
 	md->wq = create_singlethread_workqueue("kdmflush");
@@ -1128,14 +1750,6 @@ static struct mapped_device *alloc_dev(i
 bad_thread:
 	put_disk(md->disk);
 bad_disk:
-	bioset_free(md->bs);
-bad_no_bioset:
-	mempool_destroy(md->tio_pool);
-bad_tio_pool:
-	mempool_destroy(md->io_pool);
-bad_io_pool:
-	blk_cleanup_queue(md->queue);
-bad_queue:
 	free_minor(minor);
 bad_minor:
 	module_put(THIS_MODULE);
@@ -1144,6 +1758,26 @@ bad_module_get:
 	return NULL;
 }
 
+int dm_init_md(struct mapped_device *md)
+{
+	int r = 0;
+
+	if (test_bit(DMF_INITIALIZED, &md->flags))
+		return 0;
+
+	if(test_bit(DMF_REQUEST_BASED, &md->flags))
+		r = dm_set_md_request_based(md);
+	else if (test_bit(DMF_BIO_BASED, &md->flags))
+		r = dm_set_md_bio_based(md);
+	else
+		r = -EINVAL;
+
+	if (!r)
+		set_bit(DMF_INITIALIZED, &md->flags);
+
+	return r;
+}
+
 static void unlock_fs(struct mapped_device *md);
 
 static void free_dev(struct mapped_device *md)
@@ -1155,9 +1789,12 @@ static void free_dev(struct mapped_devic
 		bdput(md->suspended_bdev);
 	}
 	destroy_workqueue(md->wq);
-	mempool_destroy(md->tio_pool);
-	mempool_destroy(md->io_pool);
-	bioset_free(md->bs);
+	if (md->tio_pool)
+		mempool_destroy(md->tio_pool);
+	if (md->io_pool)
+		mempool_destroy(md->io_pool);
+	if (md->bs)
+		bioset_free(md->bs);
 	del_gendisk(md->disk);
 	free_minor(minor);
 
@@ -1166,7 +1803,8 @@ static void free_dev(struct mapped_devic
 	spin_unlock(&_minor_lock);
 
 	put_disk(md->disk);
-	blk_cleanup_queue(md->queue);
+	if (md->queue)
+		blk_cleanup_queue(md->queue);
 	module_put(THIS_MODULE);
 	kfree(md);
 }
@@ -1447,6 +2085,25 @@ out:
 	return r;
 }
 
+static void stop_queue(struct request_queue *q)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(q->queue_lock, flags);
+	blk_stop_queue(q);
+	spin_unlock_irqrestore(q->queue_lock, flags);
+}
+
+static void start_queue(struct request_queue *q)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(q->queue_lock, flags);
+	if (blk_queue_stopped(q))
+		blk_start_queue(q);
+	spin_unlock_irqrestore(q->queue_lock, flags);
+}
+
 /*
  * Functions to lock and unlock any filesystem running on the
  * device.
@@ -1522,7 +2179,7 @@ int dm_suspend(struct mapped_device *md,
 		if (!md->suspended_bdev) {
 			DMWARN("bdget failed in dm_suspend");
 			r = -ENOMEM;
-			goto flush_and_out;
+			goto out;
 		}
 
 		/*
@@ -1545,6 +2202,16 @@ int dm_suspend(struct mapped_device *md,
 	add_wait_queue(&md->wait, &wait);
 	up_write(&md->io_lock);
 
+	/*
+	 * In request-based dm, stopping request_queue prevents mapping.
+	 * Even after stopping the request_queue, submitted requests from
+	 * upper-layer can be inserted to the request_queue.
+	 * So original (unmapped) requests are kept in the request_queue
+	 * during suspension.
+	 */
+	if (test_bit(DMF_REQUEST_BASED, &md->flags))
+		stop_queue(md->queue);
+
 	/* unplug */
 	if (map)
 		dm_table_unplug_all(map);
@@ -1557,14 +2224,23 @@ int dm_suspend(struct mapped_device *md,
 	down_write(&md->io_lock);
 	remove_wait_queue(&md->wait, &wait);
 
-	if (noflush)
-		__merge_pushback_list(md);
+	if (noflush) {
+		if (test_bit(DMF_REQUEST_BASED, &md->flags))
+			/* Request-based dm uses md->queue for noflush */
+			clear_bit(DMF_NOFLUSH_SUSPENDING, &md->flags);
+		else
+			__merge_pushback_list(md);
+	}
 	up_write(&md->io_lock);
 
 	/* were we interrupted ? */
 	if (r < 0) {
 		dm_queue_flush(md, DM_WQ_FLUSH_DEFERRED, NULL);
 
+		if (test_bit(DMF_REQUEST_BASED, &md->flags))
+			/* Request-based dm uses md->queue for deferred I/Os */
+			start_queue(md->queue);
+
 		unlock_fs(md);
 		goto out; /* pushback list is already flushed, so skip flush */
 	}
@@ -1573,14 +2249,6 @@ int dm_suspend(struct mapped_device *md,
 
 	set_bit(DMF_SUSPENDED, &md->flags);
 
-flush_and_out:
-	if (r && noflush)
-		/*
-		 * Because there may be already I/Os in the pushback list,
-		 * flush them before return.
-		 */
-		dm_queue_flush(md, DM_WQ_FLUSH_ALL, NULL);
-
 out:
 	if (r && md->suspended_bdev) {
 		bdput(md->suspended_bdev);
@@ -1611,6 +2279,18 @@ int dm_resume(struct mapped_device *md)
 	if (r)
 		goto out;
 
+	/*
+	 * Flushing deferred I/Os must be done after targets are resumed
+	 * so that mapping of targets can work correctly.
+	 *
+	 * Resuming request_queue earlier than clear_bit(DMF_BLOCK_IO) means
+	 * starting to flush requests before upper-layer starts to submit bios.
+	 * It may be better because llds should be empty and no need to wait
+	 * for bio merging so strictly at this time.
+	 */
+	if (test_bit(DMF_REQUEST_BASED, &md->flags))
+		start_queue(md->queue);
+
 	dm_queue_flush(md, DM_WQ_FLUSH_DEFERRED, NULL);
 
 	unlock_fs(md);
@@ -1683,6 +2363,30 @@ int dm_suspended(struct mapped_device *m
 	return test_bit(DMF_SUSPENDED, &md->flags);
 }
 
+int dm_request_based(struct mapped_device *md)
+{
+	return test_bit(DMF_REQUEST_BASED, &md->flags);
+}
+
+int dm_bio_based(struct mapped_device *md)
+{
+	return test_bit(DMF_BIO_BASED, &md->flags);
+}
+
+void dm_set_request_based(struct mapped_device *md)
+{
+	if (test_bit(DMF_REQUEST_BASED, &md->flags))
+		return;
+
+	if (test_bit(DMF_INITIALIZED, &md->flags)) {
+		DMERR("Cannot change to request based, already initialized");
+		return;
+	}
+
+	set_bit(DMF_REQUEST_BASED, &md->flags);
+	clear_bit(DMF_BIO_BASED, &md->flags);
+}
+
 int dm_noflush_suspending(struct dm_target *ti)
 {
 	struct mapped_device *md = dm_table_get_md(ti->table);
--- a/drivers/md/dm.h
+++ b/drivers/md/dm.h
@@ -50,6 +50,8 @@ void dm_table_postsuspend_targets(struct
 int dm_table_resume_targets(struct dm_table *t);
 int dm_table_any_congested(struct dm_table *t, int bdi_bits);
 void dm_table_unplug_all(struct dm_table *t);
+void dm_table_set_request_based(struct dm_table *t);
+int dm_table_request_based(struct dm_table *t);
 
 /*
  * To check the return value from dm_table_find_target().
@@ -67,6 +69,13 @@ int dm_target_iterate(void (*iter_func)(
 					void *param), void *param);
 
 /*-----------------------------------------------------------------
+ * Helper for block layer and dm core operations
+ *---------------------------------------------------------------*/
+void dm_dispatch_request(struct request *rq);
+void dm_end_request(struct request *rq, int error);
+int dm_underlying_device_congested(struct request_queue *q);
+
+/*-----------------------------------------------------------------
  * Useful inlines.
  *---------------------------------------------------------------*/
 static inline int array_too_big(unsigned long fixed, unsigned long obj,
@@ -95,6 +104,7 @@ void dm_stripe_exit(void);
 
 void *dm_vcalloc(unsigned long nmemb, unsigned long elem_size);
 union map_info *dm_get_mapinfo(struct bio *bio);
+union map_info *dm_get_rq_mapinfo(struct request *rq);
 int dm_open_count(struct mapped_device *md);
 int dm_lock_for_deletion(struct mapped_device *md);
 
@@ -103,4 +113,12 @@ void dm_kobject_uevent(struct mapped_dev
 int dm_kcopyd_init(void);
 void dm_kcopyd_exit(void);
 
+/*
+ * Initializer for request-based/bio-based device
+ */
+int dm_set_md_request_based(struct mapped_device *md);
+int dm_set_md_bio_based(struct mapped_device *md);
+void dm_set_request_based(struct mapped_device *md);
+int dm_init_md(struct mapped_device *md);
+
 #endif
--- a/include/linux/device-mapper.h
+++ b/include/linux/device-mapper.h
@@ -11,6 +11,7 @@
 #include <linux/bio.h>
 #include <linux/blkdev.h>
 
+struct request;
 struct dm_target;
 struct dm_table;
 struct dm_dev;
@@ -47,6 +48,9 @@ typedef void (*dm_dtr_fn) (struct dm_tar
 typedef int (*dm_map_fn) (struct dm_target *ti, struct bio *bio,
 			  union map_info *map_context);
 
+typedef int (*dm_map_request_fn) (struct dm_target *ti, struct request *clone,
+				  union map_info *map_context);
+
 /*
  * Returns:
  * < 0 : error (currently ignored)
@@ -59,6 +63,10 @@ typedef int (*dm_endio_fn) (struct dm_ta
 			    struct bio *bio, int error,
 			    union map_info *map_context);
 
+typedef int (*dm_request_endio_fn) (struct dm_target *ti,
+				    struct request *clone, int error,
+				    union map_info *map_context);
+
 typedef void (*dm_flush_fn) (struct dm_target *ti);
 typedef void (*dm_presuspend_fn) (struct dm_target *ti);
 typedef void (*dm_postsuspend_fn) (struct dm_target *ti);
@@ -73,6 +81,7 @@ typedef int (*dm_message_fn) (struct dm_
 typedef int (*dm_ioctl_fn) (struct dm_target *ti, struct inode *inode,
 			    struct file *filp, unsigned int cmd,
 			    unsigned long arg);
+typedef int (*dm_congested_fn) (struct dm_target *ti);
 
 typedef int (*dm_merge_fn) (struct dm_target *ti, struct bvec_merge_data *bvm,
 			    struct bio_vec *biovec, int max_size);
@@ -103,7 +112,9 @@ struct target_type {
 	dm_ctr_fn ctr;
 	dm_dtr_fn dtr;
 	dm_map_fn map;
+	dm_map_request_fn map_rq;
 	dm_endio_fn end_io;
+	dm_request_endio_fn rq_end_io;
 	dm_flush_fn flush;
 	dm_presuspend_fn presuspend;
 	dm_postsuspend_fn postsuspend;
@@ -113,6 +124,7 @@ struct target_type {
 	dm_message_fn message;
 	dm_ioctl_fn ioctl;
 	dm_merge_fn merge;
+	dm_congested_fn congested;
 };
 
 struct io_restrictions {
@@ -125,6 +137,7 @@ struct io_restrictions {
 	unsigned short max_hw_segments;
 	unsigned short max_phys_segments;
 	unsigned char no_cluster; /* inverted so that 0 is default */
+	unsigned char no_request_stacking; /* inverted so that 0 is default */
 };
 
 struct dm_target {
@@ -201,6 +214,7 @@ const char *dm_device_name(struct mapped
 int dm_copy_name_and_uuid(struct mapped_device *md, char *name, char *uuid);
 struct gendisk *dm_disk(struct mapped_device *md);
 int dm_suspended(struct mapped_device *md);
+int dm_request_based(struct mapped_device *md);
 int dm_noflush_suspending(struct dm_target *ti);
 
 /*
