diff -urN linux.orig/drivers/char/Kconfig linux/drivers/char/Kconfig
--- linux.orig/drivers/char/Kconfig	2004-05-03 10:53:01.000000000 -0700
+++ linux/drivers/char/Kconfig	2004-05-11 04:56:19.636496345 -0700
@@ -1019,6 +1019,13 @@
 	  The mmtimer device memory gives userspace access to the
 	  Altix realtime clock.
 
+config VTUNE
+	tristate "Intel VTune sampling driver"
+	depends on X86 || X86_64 || IA64
+	default n
+	help
+	  Intel VTune sampling driver.
+
 endmenu
 
 source "drivers/char/lirc/Kconfig"
diff -urN linux.orig/drivers/char/Makefile linux/drivers/char/Makefile
--- linux.orig/drivers/char/Makefile	2004-05-03 10:53:01.000000000 -0700
+++ linux/drivers/char/Makefile	2004-05-11 04:56:19.637472908 -0700
@@ -85,6 +85,7 @@
 obj-$(CONFIG_DRM) += drm/
 obj-$(CONFIG_PCMCIA) += pcmcia/
 obj-$(CONFIG_IPMI_HANDLER) += ipmi/
+obj-$(CONFIG_VTUNE) += vtune/
 
 obj-$(CONFIG_HANGCHECK_TIMER) += hangcheck-timer.o
 
diff -urN linux.orig/drivers/char/vtune/apic32_64.c linux/drivers/char/vtune/apic32_64.c
--- linux.orig/drivers/char/vtune/apic32_64.c	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/apic32_64.c	2004-03-29 10:06:04.000000000 -0800
@@ -0,0 +1,404 @@
+/*
+ *  apic32_64.c
+ *
+ *  Copyright (C) 2003-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: apic32_64.c
+ *
+ *	Description: handle APIC manipulations done by sampling driver
+ *	             on Pentium(R) 4 processors
+ *	             with Intel(R) Extended Memory 64 Technology
+ *
+ *	Author(s): Charles Spirakis, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <asm/processor.h>
+#include <asm/fixmap.h>
+#include <asm/apic.h>
+
+#include "vtdef.h"
+#include "vtypes.h"
+#include "vtasm32_64.h"
+#include "apic.h"
+
+//
+// Linux* already defines the apic virtual address for us at boot time
+// and it is the same one each time...
+//
+#define  apicVirtAddr   (APIC_BASE)
+
+#define APIC_BASE_PHYS_ADDR         (0xfffffffffffff000)
+
+#if defined(DEBUG)
+IDTGDT_DESC gIdtDesc;
+#endif
+
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void ShortDelay (IN __u32 wantSerialize)
+ * @brief       A small delay loop with optional serialization
+ *
+ * @param       wantSerialize  IN  - True if you want to serialize at the end
+ *
+ * @return      none
+ *
+ * A small delay loop that can also serialize the processor at the end
+ *
+ * <I>Special Notes:</I>
+ * 
+ *      Uses the cpuid instruction to do the serialization
+ *
+ * @todo    This routine is currently untested!
+ *
+ */
+void
+ShortDelay(
+        __u32 wantSerialize)
+{
+    volatile char t;
+    int i;
+
+    VDK_PRINT_WARNING("You are running UNTESTED CODE: %s line %d\n", __FILE__, __LINE__);
+
+    for (i = 0; i < 10; i++) {
+        t = 0;
+    }
+    if (wantSerialize) {
+        // CPUID is a serializing instruction
+        cpuid_eax(1);
+    }
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void SetVirtualWireMode(void)
+ * @brief       Put the APIC in virtual wire mode
+ *
+ * @return      none
+ *
+ * This routine assumes the APIC exists.
+ *
+ * <I>Special Notes:</I>
+ * 
+ *      none
+ *
+ * @todo    This routine is currently untested!
+ *
+ */
+void
+SetVirtualWireMode(
+        void)
+{
+    unsigned long oldFlags;
+    unsigned char pic1Mask;
+    unsigned char pic2Mask;
+    __u64 apicBaseValue;
+
+    VDK_PRINT_WARNING("You are running UNTESTED CODE: %s line %d\n", __FILE__, __LINE__);
+
+    VDK_PRINT("Setting virtual wire mode\n");
+
+    //
+    // Interrupts are DISABLED HERE
+    //
+    local_irq_save(oldFlags);
+
+    //
+    // Mask the 8259's to prevent interrupts from being trapped
+    // while switching to virtual wire mode
+    //
+    pic2Mask = __inbyte(0xa1);      // save old mask
+    ShortDelay(FALSE);
+
+    __outbyte(0xa1, 0xff);          // mask off everything
+    ShortDelay(FALSE);
+
+    pic1Mask = __inbyte(0x21);      // save old mask
+    ShortDelay(FALSE);
+
+    __outbyte(0x21, 0xff);          // mask off everything
+    ShortDelay(FALSE);
+
+    //
+    // About to RE-ENABLE interrupts HERE
+    //
+    local_irq_restore(oldFlags);
+
+    //
+    // Delay to let any pending interrupts get handled...
+    //
+    // Shouldn't get many interrupts - only the pending ones since the 
+    // 8259's are masked off
+    //
+    ShortDelay(TRUE);
+    ShortDelay(TRUE);
+
+    //
+    // Interrupts are DISABLED HERE
+    //
+    local_irq_save(oldFlags);
+
+    //
+    // Turn on the global enable
+    //
+    ReadMsr(IA32_APIC_BASE, &apicBaseValue);
+    apicBaseValue |= APIC_BASE_GLOBAL_ENABLE;
+    WriteMsr(IA32_APIC_BASE, apicBaseValue);
+
+    // Enable virtual wire mode
+    APIC_WRITE(apicVirtAddr, APIC_SPIV, 0x1ff);
+
+    // Block interrupts (should be done already above, but...)
+    APIC_WRITE(apicVirtAddr, APIC_TASKPRI, APIC_VALUE_TPR_HI);
+
+    // local id is 0
+    APIC_WRITE(apicVirtAddr, APIC_ID, 0);
+
+    // logical unit id is 0
+    APIC_WRITE(apicVirtAddr, APIC_LDR, 0);
+
+    // destination format
+    APIC_WRITE(apicVirtAddr, APIC_DFR, -1);
+
+    // mask local timer
+    APIC_WRITE(apicVirtAddr, APIC_LVTT, APIC_LVT_MASKED);
+
+    // 8259->lint0 - edge sensitive
+    APIC_WRITE(apicVirtAddr, APIC_LVT0, SET_APIC_DELIVERY_MODE(0, APIC_MODE_EXINT));
+
+    // nmi->lint1 - level sensitive
+    APIC_WRITE(apicVirtAddr, APIC_LVT1, SET_APIC_DELIVERY_MODE(APIC_LVT_LEVEL_TRIGGER, APIC_MODE_NMI));
+
+    // Set PMU interrupt priority
+    APIC_WRITE(apicVirtAddr, APIC_LVTPC, PERF_MON_VECTOR);
+
+    // Allow interrupts through apic
+    APIC_WRITE(apicVirtAddr, APIC_TASKPRI, APIC_VALUE_TPR_LO);
+
+    
+    __outbyte(0xa1, pic2Mask);          // put back pic2 mask
+    ShortDelay(FALSE);
+
+    __outbyte(0x21, pic1Mask);          // put back pic1 mask
+    ShortDelay(TRUE);
+
+    //
+    // About to RE-ENABLE interrupts HERE
+    //
+    local_irq_restore(oldFlags);
+
+    VDK_PRINT("Setting virtual wire mode - DONE\n");
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void DoApicInit (void)
+ * @brief       Check for and initialize the APIC if necessary
+ *
+ * @return      none
+ *
+ * <I>Special Notes:</I>
+ *
+ *          none
+ *
+ */
+void
+DoApicInit(
+        void)
+{
+    __u64 msrValue;
+
+    void *apicPhysAddr;
+
+#if defined(DEBUG)
+    //
+    // Find out where the IDT is hiding...
+    //
+    GetIDTDesc(&gIdtDesc);
+#endif
+
+    ReadMsr(IA32_APIC_BASE, &msrValue);
+
+    apicPhysAddr = (void *) (msrValue & APIC_BASE_PHYS_ADDR);
+
+    VDK_PRINT_DEBUG("Apic base (msr 0x%x) has 0x%p\n", IA32_APIC_BASE, (void *) msrValue);
+    VDK_PRINT_DEBUG("Physical apic base address is 0x%p\n", apicPhysAddr);
+    VDK_PRINT_DEBUG("Virtual apic base address is 0x%p\n", (void *) apicVirtAddr);
+
+    if (msrValue & APIC_BASE_GLOBAL_ENABLE) {
+        VDK_PRINT_DEBUG("Apic is already enabled\n");
+    }
+    else {
+        VDK_PRINT_DEBUG("Putting APIC in virtual wire mode\n");
+        SetVirtualWireMode();
+    }
+
+#if defined(DEBUG)
+    {
+        __u32 idtPerfOffset;
+        LONG_IDT_ENTRY *pIdtEntry;
+        void *targetOffset;
+
+        VDK_PRINT("APIC id 0x%x\n", APIC_READ(apicVirtAddr, APIC_ID));
+        VDK_PRINT("APIC version 0x%x\n", APIC_READ(apicVirtAddr, APIC_LVR));
+        VDK_PRINT("APIC lvl timer 0x%x\n", APIC_READ(apicVirtAddr, APIC_LVTT));
+        VDK_PRINT("APIC lvl perf 0x%x\n", APIC_READ(apicVirtAddr, APIC_LVTPC));
+        VDK_PRINT("APIC lvl lint0 %x\n", APIC_READ(apicVirtAddr, APIC_LVT0));
+        VDK_PRINT("APIC lvl lint1 %x\n", APIC_READ(apicVirtAddr, APIC_LVT1));
+
+        idtPerfOffset = APIC_READ(apicVirtAddr, APIC_LVTT) & 0xff;
+
+        pIdtEntry = ((LONG_IDT_ENTRY *) gIdtDesc.idtgdt_base) + idtPerfOffset;
+
+        VDK_PRINT("timer - idt base: 0x%p, idtPerfOffset: 0x%x, pIdtEntry: 0x%p, *pIdtEntry: 0x%p, (*p+1): 0x%p\n",
+                (void *)gIdtDesc.idtgdt_base,
+                idtPerfOffset,
+                (void *)pIdtEntry,
+                (void *)pIdtEntry->lowQuad,
+                (void *)pIdtEntry->highQuad);
+
+        targetOffset = (void *) (pIdtEntry->offsetLow + (pIdtEntry->offsetMid << 16) + (((__u64) pIdtEntry->offsetHigh) << 32));
+
+        VDK_PRINT("Idt targetOffset: 0x%p, sel: 0x%x, P: 0x%x, dpl: 0x%x, S: 0x%x type: 0x%x, ist: 0x%x\n",
+                    (void *)targetOffset,
+                    pIdtEntry->selector,
+                    pIdtEntry->pres,
+                    pIdtEntry->dpl,
+                    pIdtEntry->sysUsr,
+                    pIdtEntry->type,
+                    pIdtEntry->ist);
+
+        idtPerfOffset = APIC_READ(apicVirtAddr, APIC_LVTPC) & 0xff;
+
+        pIdtEntry = ((LONG_IDT_ENTRY *) gIdtDesc.idtgdt_base) + idtPerfOffset;
+
+        VDK_PRINT("perf - idt base: 0x%p, idtPerfOffset: 0x%x, pIdtEntry: 0x%p, *pIdtEntry: 0x%p, (*p+1): 0x%p\n",
+                (void *)gIdtDesc.idtgdt_base,
+                idtPerfOffset,
+                (void *)pIdtEntry,
+                (void *)pIdtEntry->lowQuad,
+                (void *)pIdtEntry->highQuad);
+
+        targetOffset = (void *) (pIdtEntry->offsetLow + (pIdtEntry->offsetMid << 16) + (((__u64) pIdtEntry->offsetHigh) << 32));
+
+        VDK_PRINT("Idt targetOffset: 0x%p, sel: 0x%x, P: 0x%x, dpl: 0x%x, S: 0x%x type: 0x%x, ist: 0x%x\n",
+                    (void *)targetOffset,
+                    pIdtEntry->selector,
+                    pIdtEntry->pres,
+                    pIdtEntry->dpl,
+                    pIdtEntry->sysUsr,
+                    pIdtEntry->type,
+                    pIdtEntry->ist);
+    }
+#endif
+}
+
+__u32
+IsApicEnabled(
+        void
+        )
+{
+    __u64 msrValue;
+
+    ReadMsr(IA32_APIC_BASE, &msrValue);
+
+    return (msrValue & APIC_BASE_GLOBAL_ENABLE) ? TRUE : FALSE;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void EnablePerfVect(__u32 wantEnable)
+ * @brief       Re-enable the perf interrupts
+ *
+ * @param       wantEnable  IN  - TRUE to enable, FALSE to disable
+ *
+ * @return      none
+ *
+ * Mask or unmask the perf interrupt through the local vector table (lvt)
+ *
+ * <I>Special Notes:</I>
+ *
+ *          none
+ *
+ */
+void
+EnablePerfVect(
+        __u32 wantEnable
+        )
+{
+    __u32 perfLevel;
+
+    perfLevel = APIC_READ(apicVirtAddr, APIC_LVTPC);
+
+    VDK_PRINT_DEBUG("APIC PERF called. old perfValue is %x\n", perfLevel);
+
+    if (wantEnable) {
+        perfLevel &= ~APIC_LVT_MASKED;      // zero means enabled
+    }
+    else {
+        perfLevel |= APIC_LVT_MASKED;       // one means disabled
+    }
+
+    VDK_PRINT_DEBUG("APIC PERF called. new perfValue is %x\n", perfLevel);
+
+    APIC_WRITE(apicVirtAddr, APIC_LVTPC, perfLevel);
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          __u32 SetApicPerfLevel(__u32 newValue)
+ * @brief       Set the new LVT for the perf counters
+ *
+ * @param       newValue - the new value to be used
+ *
+ * @return      the value that was there before
+ *
+ * Put in a new value in the APIC LVT entry for the performance counter
+ *
+ * <I>Special Notes:</I>
+ *
+ *          none
+ *
+ */
+__u32
+SetApicPerfLevel(__u32 newValue)
+{
+    __u32 perfLevel;
+
+    perfLevel = APIC_READ(apicVirtAddr, APIC_LVTPC);
+
+    VDK_PRINT_DEBUG("SetApicPerfLevel(%x), old value %x\n", newValue, perfLevel);
+
+    APIC_WRITE(apicVirtAddr, APIC_LVTPC, newValue);
+
+    return (perfLevel);
+}
diff -urN linux.orig/drivers/char/vtune/apic32.c linux/drivers/char/vtune/apic32.c
--- linux.orig/drivers/char/vtune/apic32.c	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/apic32.c	2004-03-29 10:06:04.000000000 -0800
@@ -0,0 +1,246 @@
+/*
+ *  apic32.c
+ *
+ *  Copyright (C) 2003-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: apic32.c
+ *
+ *	Description: handle APIC manipulations done by sampling driver
+ *
+ *	Author(s): Charles Spirakis, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#include <asm/uaccess.h>
+#include <linux/vmalloc.h>
+#include <linux/slab.h>     /*malloc */
+#include <asm/io.h>
+#include <asm/desc.h>
+
+#include "vtuneshared.h"
+#include "vtdef.h"
+#include "vtasm32.h"
+#include "vtproto.h"
+#include "vtextern.h"
+#include "apic.h"
+
+#define APIC_BASE_PHYS_ADDR         (0xfffff000)
+
+
+/*!
+ * @fn          void *map_apic(void)
+ * @brief       Map the local apic registers into virtual memory
+ *
+ * @return	non-zero - virtual address of the apic, zero - apic not mapped
+ *
+ *
+ * Local apic is only available on Pentium(R) Pro processor and beyond.
+ *
+ * <I>Special Notes:</I>
+ *
+ *	Some laptops using newer processors don't include the apic. This
+ * code could have problems on laptop machines...
+ *
+ * @todo	Need a better test for apic existance rather than cpu family
+ *
+ */
+void *
+map_apic(void)
+{
+    __u32 msr_low, msr_high;
+
+    if (!IA32_family6 && !IA32_familyF) {
+        return ((void *) 0);
+    }
+    rdmsr(APIC_BASE_MSR, msr_low, msr_high);
+
+    return ((void *) ioremap_nocache(msr_low & 0xFFFFF000, 4096));
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void un_map_apic(void *apic_virt_addr)
+ * @brief       Unmap the local apic from virtual memory
+ *
+ * @param       apic_virt_addr	IN - address of be unmapped
+ *
+ * @return	none
+ *
+ * <I>Special Notes:</I>
+ *
+ *	None
+ *
+ */
+void
+un_map_apic(void *apic_virt_addr)
+{
+    if (apic_virt_addr) {
+        iounmap(apic_virt_addr);
+    }
+
+    return;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          BOOLEAN enable_apic(void) 
+ * @brief       Enables the apic if it exists and is not already enabled
+ *
+ * @return	true if apic was enabled, false otherwise
+ *
+ * Calls an assembly routine to do the actual work. If the apic isn't
+ * enabled, the assembly routine enables it and puts it into
+ * "virtual wire mode" which bascially makes the apic emulate
+ * the older 8249 interrupt structure but still allow for the processor
+ * to use processor specific interrupts (like the PMU overflow).
+ *
+ * <I>Special Notes:</I>
+ *
+ *	Same apic existance problem as map_apic()
+ *
+ * @todo        Same apic existance problem as map_apic()
+ *
+ */
+BOOLEAN
+enable_apic(void)
+{
+    __u32 msr_low, msr_high;
+
+    if (!IA32_family6 && !IA32_familyF) {
+        return (FALSE);
+    }
+
+    SAMP_Set_Apic_Virtual_Wire_Mode();
+    rdmsr(APIC_BASE_MSR, msr_low, msr_high);
+
+    return ((msr_low & APIC_BASE_MSR_ENABLE) ? TRUE : FALSE);
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void DoApicInit (void)
+ * @brief       Check for and initialize the APIC if necessary
+ *
+ * @return      none
+ *
+ * <I>Special Notes:</I>
+ *
+ *          none
+ *
+ */
+void
+DoApicInit(
+        void)
+{
+    ULARGE_INTEGER msrValue;
+
+    BOOLEAN apic_enabled = FALSE;
+    void *apicPhysAddr;
+    
+    apic_local_addr = map_apic();
+    apic_enabled = enable_apic();
+
+    if (!apic_enabled) {
+        un_map_apic(apic_local_addr);
+        apic_local_addr = 0;
+    }
+    
+    msrValue = samp_read_msr(IA32_APIC_BASE);
+    apicPhysAddr = (void *) (msrValue.low_part & APIC_BASE_PHYS_ADDR);
+
+    VDK_PRINT_DEBUG("Apic base (msr 0x%x) has 0x%p\n", IA32_APIC_BASE, (void *) msrValue);
+    VDK_PRINT_DEBUG("Physical apic base address is 0x%p\n", apicPhysAddr);
+    VDK_PRINT_DEBUG("Virtual apic base address is 0x%p\n", (void *) apic_local_addr);
+
+    return;
+}
+
+__u32
+IsApicEnabled(
+        void
+        )
+{
+    ULARGE_INTEGER  msrValue;
+
+    msrValue = samp_read_msr(IA32_APIC_BASE);
+
+    return (msrValue.low_part & APIC_BASE_GLOBAL_ENABLE) ? TRUE : FALSE;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void EnablePerfVect(__u32 wantEnable)
+ * @brief       Re-enable the perf interrupts
+ *
+ * @param       wantEnable  IN  - TRUE to enable, FALSE to disable
+ *
+ * @return      none
+ *
+ * Mask or unmask the perf interrupt through the local vector table (lvt)
+ *
+ * <I>Special Notes:</I>
+ *
+ *          none
+ *
+ */
+void
+EnablePerfVect(
+        __u32 wantEnable
+        )
+{
+    if (wantEnable) {
+        samp_apic_clear_perf_lvt_int_mask();       
+    }
+    else {
+        samp_apic_set_perf_lvt_int_mask();
+    }
+
+    return;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          __u32 SetApicPerfLevel(__u32 newValue)
+ * @brief       Set the new LVT for the perf counters
+ *
+ * @param       newValue - the new value to be used
+ *
+ * @return      the value that was there before
+ *
+ * Put in a new value in the APIC LVT entry for the performance counter
+ *
+ * <I>Special Notes:</I>
+ *
+ *          none
+ *
+ */
+__u32
+SetApicPerfLevel(__u32 newValue)
+{
+    return SAMP_Set_apic_perf_lvt(newValue);
+}
diff -urN linux.orig/drivers/char/vtune/apic.h linux/drivers/char/vtune/apic.h
--- linux.orig/drivers/char/vtune/apic.h	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/apic.h	2004-05-11 05:01:21.150164527 -0700
@@ -0,0 +1,111 @@
+/*
+ *  apic.h
+ *
+ *  Copyright (C) 2002-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: apic.h
+ *
+ *	Description: header file for apic on IA32 platforms 
+ *
+ *	Author(s): Charles Spirakis, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#if !defined(__VTUNE_APIC_H_)
+#define __VTUNE_APIC_H_
+
+//
+// How big is the apic area that is mapped into physical memory
+//
+#define APIC_PHYSICAL_LENGTH    4096
+
+//
+// The feature flags from CPUID instruction...
+//
+#define CPUID_EDX_HASAPIC     (1 << 9)
+
+
+//
+// APIC related MSRs
+//
+
+#define IA32_APIC_BASE      0x1b
+
+//
+// And ia32_apic_base bit info
+//
+#define APIC_BASE_BSP               (1 << 8)
+#define APIC_BASE_GLOBAL_ENABLE     (1 << 11)
+
+//
+//
+// The linux apic headers files handle the read and make sure
+// the reads go to the correct area of memory. So, the base is
+// unused in this case...
+//
+#define APIC_READ(base, offset)         (apic_read(offset))
+#define APIC_WRITE(base, offset, value) (apic_write(offset, value))
+
+//
+// Priority levels for use in the TASKPRI register
+#define APIC_VALUE_TPR_HI       0xf0
+#define APIC_VALUE_TPR_LO       0x00
+
+//
+// at least try to be nice...
+//
+#ifndef PERF_MON_VECTOR
+#define PERF_MON_VECTOR 0xF0
+#endif
+
+//
+// Routines that can be called that are apic related...
+//
+void EnablePerfVect(
+        __u32 wantEnable
+        );
+
+__u32
+SetApicPerfLevel(
+        __u32 newValue
+        );
+
+void
+SetVirtualWireMode(
+        void
+        );
+__u32
+IsApicEnabled(
+        void
+        );
+
+void DoApicInit(
+        void
+        );
+
+#endif // __VTUNE_APIC_H_
diff -urN linux.orig/drivers/char/vtune/familyf_msr.h linux/drivers/char/vtune/familyf_msr.h
--- linux.orig/drivers/char/vtune/familyf_msr.h	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/familyf_msr.h	2004-04-29 14:11:27.000000000 -0700
@@ -0,0 +1,659 @@
+/*
+ *  familyf_msr.h 
+ *
+ *  Copyright (C) 2002-2004 Intel Corporation 
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: familyf_msr.h 
+ *
+ *	Description: Definition file for Intel(R) Pentium(R) 4 processor
+ *                   performance event counters.
+ *                   The presence of the Debug Trace and precise EMON features
+ *                   can be determined via the DTS bit (bit 21) in the feature
+ *                   flags returned by the CPUID instruction. 
+ *
+ *	Author(s): George Artz, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#if !defined(_VTUNE_FAMILYF_MSR_H)
+#define _VTUNE_FAMILYF_MSR_H
+
+#define CCCR_REG  1
+#define COUNT_REG 2
+#define PEBS_REG  3
+#define ESCR_REG  4
+#define UNKNOWN_REG 0
+
+#define CCCR_FIRST 864
+#define CCCR_LAST  881
+#define COUNT_FIRST (CCCR_FIRST - 0x60)
+#define COUNT_LAST  (CCCR_LAST  - 0x60)
+#define ESCR_FIRST 928
+#define ESCR_LAST  993
+
+#define NUM_ESCRS (ESCR_LAST - ESCR_FIRST +1)
+#define NUM_CCCRS (ESCR_LAST - ESCR_FIRST +1)
+
+#define NUMBER_OF_WMT_EVENT_ESCRS     2
+#define NUMBER_OF_WMT_COUNTERS        18
+#define NUMBER_OF_WMT_CONTROLS        18
+#define NUMBER_OF_WMT_PUBS            4
+#define NUMBER_OF_WMT_UNITS           23
+#define NUMBER_OF_WMT_SELECTORS       (NUMBER_OF_WMT_EVENT_ESCRS*NUMBER_OF_WMT_UNITS)
+#define NUMBER_OF_WMT_MASKS           16
+#define NUMBER_OF_WMT_CRUS            3
+#define MAX_WMTEVENTS                 18    //18 counters
+#define MAX_WMT_COUNTER_VALUE         0x10000000000
+
+#define WMT_RATE                      0x10000
+#define WMT_CC_USER                   1
+#define WMT_CC_SUP                    2
+#define WMT_CC_ALL                    3
+
+/*
+
+    Event Selection control register: ESCR
+
+    +------+-----------+---------+--------+--------+--------+-------+--------+
+    |  31  |   30-25   |  24-09  | 08-04  |   03   |   02   |   01  |   00   |
+    +------+-----------+---------+--------+--------+--------+-------+--------+
+    |Rsvd  |EventSelect|Eventmask|  Rsvd  | T0_OS  | T0_USR | T1_OS | T1_USR |
+    +------+-----------+---------+--------+--------+--------+-------+--------+
+
+    +------------------------------------------------------------------------+
+    | 63-32                                                                  |
+    +------------------------------------------------------------------------+
+    |Rsvd                                                                    |
+    +------------------------------------------------------------------------+
+
+    Event Select control register (ESCR) field encodings
+
+    +------------+-----------------------------------------------------------+
+    |  Field     | description                                               |
+    +------------+-----------------------------------------------------------+
+    |T1_USR      | If (Thread == T1) and  (T1_CPL=1|2|3), output event count |
+    |T1_OS       | If (Thread == T1) and  (T1_CPL=0), output event count     |
+    |T0_USR      | If (Thread == T0) and  (T0_CPL=1|2|3), output event count |
+    |T0_OS       | If (Thread == T0) and  (T0_CPL=0), output event count     |
+    |Event mask  | Selects an event-specific mask                            |
+    |Event Select| Selects an unit-specific event                            |
+    +------------+-----------------------------------------------------------+
+
+*/
+
+//
+//      ESCR                        MSR           performance Counters
+//
+#define BSU_CR_ESCR0                0x3A0   //Counter 0, 1
+#define BSU_CR_ESCR1                0x3A1   //Counter 2, 3
+#define FSB_CR_ESCR0                0x3A2   //Counter 0, 1
+#define FSB_CR_ESCR1                0x3A3   //Counter 2, 3
+#define FIRM_CR_ESCR0               0x3A4   //Counter 8, 9
+#define FIRM_CR_ESCR1               0x3A5   //Counter 10, 11
+#define FLAME_CR_ESCR0              0x3A6   //Counter 8, 9
+#define FLAME_CR_ESCR1              0x3A7   //Counter 10, 11
+#define DAC_CR_ESCR0                0x3A8   //Counter 8, 9
+#define DAC_CR_ESCR1                0x3A9   //Counter 10, 11
+#define MOB_CR_ESCR0                0x3AA   //Counter 0, 1
+#define MOB_CR_ESCR1                0x3AB   //Counter 2, 3
+#define PMH_CR_ESCR0                0x3AC   //Counter 0, 1
+#define PMH_CR_ESCR1                0x3AD   //Counter 2, 3
+#define SAAT_CR_ESCR0               0x3AE   //Counter 8, 9
+#define SAAT_CR_ESCR1               0x3AF   //Counter 10, 11
+#define U2L_CR_ESCR0                0x3B0   //Counter 8, 9
+#define U2L_CR_ESCR1                0x3B1   //Counter 10, 11
+#define BPU_CR_ESCR0                0x3B2   //Counter 0, 1
+#define BPU_CR_ESCR1                0x3B3   //Counter 2, 3
+#define IS_CR_ESCR0                 0x3B4   //Counter 0, 1
+#define IS_CR_ESCR1                 0x3B5   //Counter 2, 3
+#define ITLB_CR_ESCR0               0x3B6   //Counter 0, 1
+#define ITLB_CR_ESCR1               0x3B7   //Counter 2, 3
+#define CRU_CR_ESCR0                0x3B8   //Counter 12, 13, 16
+#define CRU_CR_ESCR1                0x3B9   //Counter 14, 15, 17
+#define IQ_CR_ESCR0                 0x3BA   //Counter 12, 13, 16
+#define IQ_CR_ESCR1                 0x3BB   //Counter 14, 15, 17
+#define RAT_CR_ESCR0                0x3BC   //Counter 12, 13, 16
+#define RAT_CR_ESCR1                0x3BD   //Counter 14, 15, 17
+#define SSU_CR_ESCR0                0x3BE   //Counter 12, 13, 16
+#define MS_CR_ESCR0                 0x3C0   //Counter 4, 5
+#define MS_CR_ESCR1                 0x3C1   //Counter 6, 7
+#define TBPU_CR_ESCR0               0x3C2   //Counter 4, 5
+#define TBPU_CR_ESCR1               0x3C3   //Counter 6, 7
+#define TC_CR_ESCR0                 0x3C4   //Counter 4, 5
+#define TC_CR_ESCR1                 0x3C5   //Counter 6, 7
+#define IX_CR_ESCR0                 0x3C8   //Counter 0, 1
+#define IX_CR_ESCR1                 0x3C9   //Counter 2, 3
+#define ALF_CR_ESCR0                0x3CA   //Counter 12, 13, 16
+#define ALF_CR_ESCR1                0x3CB   //Counter 14, 15, 17
+#define CRU_CR_ESCR2                0x3CC   //Counter 12, 13, 16
+#define CRU_CR_ESCR3                0x3CD   //Counter 14, 15, 17
+#define CRU_CR_ESCR4                0x3E0   //Counter 12, 13, 16
+#define CRU_CR_ESCR5                0x3E1   //Counter 14, 15, 17
+
+#define ESCR_INTERNAL_MASK          0x80000000  //pos: 31
+#define ESCR_EventSelect_MASK       0x7e000000  //pos: 30-25
+#define ESCR_MASK_MASK              0x01fffe00  //pos: 24-09
+#define ESCR_Tagvalue_MASK          0x000001e0  //pos: 08-05
+#define ESCR_TagUop_MASK            0x00000010  //pos: 04
+#define ESCR_T0_OS_MASK             0x00000008  //pos: 03
+#define ESCR_T0_USR_MASK            0x00000004  //pos: 02
+#define ESCR_T0_MASK                0x0000000c
+#define ESCR_T1_OS_MASK             0x00000002  //pos: 01
+#define ESCR_T1_USR_MASK            0x00000001  //pos: 00
+#define ESCR_T1_MASK                0x00000003
+
+#define WMT_CR_DTES_AREA    0x0600  //New MSR defined for Pentium(R) 4 processor
+#define DEBUG_CTL_MSR       0x1d9
+#define debug_ctl_msr_DTS     0x8000    //enable store record to memory
+#define debug_ctl_msr_LBR     0x0001 	//enable lbr
+
+#define CPUID_DTS_MASK      0x00200000  //Debug Trace Sampling
+
+/*
+
+    Counter Configuration and control registers: CCCR
+    
+    +---+----+-----+------+------+------+----+------+-----+----+-------+------+------+-----+
+    |31 |30  |28-29| 27   | 26   | 25   | 24 |23-20 | 19  | 18 | 17-16 |15-13 |  12  |11-00|
+    +---+----+-----+------+------+------+----+------+-----+----+-------+------+------+-----+
+    |OVF|CAS |Rsvd |OVF_  |OVF_  |FORCE_|EDGE|THRESH|COMPL|COMP|ACTIVE_|SELECT|ENABLE|Rsvd |
+    |   |CADE|     |PMI_T1|PMI_T0|OVF   |    |HOLD  |EMENT|ARE |THREADS|      |      |     |
+    +---+----+-----+------+------+------+----+------+-----+----+-------+------+------+-----+
+
+    +--------------------------------------------------------------------------------------+
+    | 63-32                                                                                |
+    +--------------------------------------------------------------------------------------+
+    |Rsvd                                                                                  |
+    +--------------------------------------------------------------------------------------+
+
+    Counter Configuration and control registers (CCCR) field encodings
+
+    +--------------+-----------------------------------------------------------------------+
+    |  Field       | description                                                           |
+    +--------------+-----------------------------------------------------------------------+
+    |ENABLE        |If set counting is enabled. Bit cleared on reset                       |
+    +--------------+-----------------------------------------------------------------------+
+    |SELECT        |Identifies the ESCR to be used for selecting event to be counted in    |
+    |              |  this counter                                                         |
+    +--------------+-----------------------------------------------------------------------+
+    |ACTIVE_THREADS|Select when counting should occur based on which threads are active    |
+    +--------------+-----------------------------------------------------------------------+
+    |COMPARE       |If set, enable filtering using the THRESHOLD value                     |
+    +--------------+-----------------------------------------------------------------------+
+    |COMPLEMENT    |If clear, use > comparison; if set use the <= comparison               |
+    +--------------+-----------------------------------------------------------------------+
+    |THRESHOLD     |threshold value to be used for comparison                              |
+    +--------------+-----------------------------------------------------------------------+
+    |EDGE          |If set, edge detect; increment counter by 1 if the previous input value|
+    |              |  was zero and the current input is non-zero                           |
+    +--------------+-----------------------------------------------------------------------+
+    |FORCE_OVF     |If set, force an overflow on every counter increment.                  |
+    +--------------+-----------------------------------------------------------------------+
+    |OVF_PMI_T0    |If set, cause a performance Monitor interrupt (PMI) to logical         |
+    |              | processor T0 on counter overflow.                                     |
+    +--------------+-----------------------------------------------------------------------+
+    |OVF_PMI_T1    |If set, cause a performance Monitor interrupt (PMI) to logical         | 
+    |              |  processor T1 on counter overflow.                                    |
+    +--------------+-----------------------------------------------------------------------+
+    |CASCADE       |If set, enable counting when the "alternate" counter of the counter    |
+    |              |  pair overflows.                                                      |
+    +--------------+-----------------------------------------------------------------------+
+    |OVF           |A status bit that indicates that the counter has overflowed. This bit  |
+    |              |  is reset by software.                                                |
+    +--------------+-----------------------------------------------------------------------+
+
+    Note: The counter can also be enabled via the cascading feature if the CASCADE bit is set. 
+    
+    +-------------+------------------------------------------------------------------------+
+    |ACTIVE_THREAD| Abbrev  |         description                                          |
+    +-------------+------------------------------------------------------------------------+
+    |    00       |    NT   | No logical processor mode                                    |
+    |             |         | (only count when neither T0 or T1 is active)                 |
+    +-------------+------------------------------------------------------------------------+
+    |    01       |    ST   | Single logical processor mode                                |
+    |             |         | (count when only one logical processor -                     |
+    |             |         |  either T0 or T1 - is active)                                |
+    +-------------+------------------------------------------------------------------------+
+    |    10       |    DT   | Dual logical processor mode                                  |
+    |             |         | (count when both T0 and T1 are active)                       |       
+    +-------------+------------------------------------------------------------------------+
+    |    11       |   ANY   | Count when either logical processor is active                |
+    +-------------+------------------------------------------------------------------------+
+
+
+*/
+
+#define ACTIVE_THREAD_NT            0
+#define ACTIVE_THREAD_ST            1
+#define ACTIVE_THREAD_DT            2
+#define ACTIVE_THREAD_ANY           3
+
+//
+//      CCCR                        MSR           performance Counters
+//
+#define BPU_CR_PUB_CCCR0            0x360   //BPU_CR_PUB_COUNTER0
+#define BPU_CR_PUB_CCCR1            0x361   //BPU_CR_PUB_COUNTER1
+#define BPU_CR_PUB_CCCR2            0x362   //BPU_CR_PUB_COUNTER2
+#define BPU_CR_PUB_CCCR3            0x363   //BPU_CR_PUB_COUNTER3
+#define MS_CR_PUB_CCCR0             0x364   //MS_CR_PUB_COUNTER0
+#define MS_CR_PUB_CCCR1             0x365   //MS_CR_PUB_COUNTER1
+#define MS_CR_PUB_CCCR2             0x366   //MS_CR_PUB_COUNTER2
+#define MS_CR_PUB_CCCR3             0x367   //MS_CR_PUB_COUNTER3
+#define FLAME_CR_PUB_CCCR0          0x368   //FLAME_CR_PUB_COUNTER0
+#define FLAME_CR_PUB_CCCR1          0x369   //FLAME_CR_PUB_COUNTER1
+#define FLAME_CR_PUB_CCCR2          0x36a   //FLAME_CR_PUB_COUNTER2
+#define FLAME_CR_PUB_CCCR3          0x36b   //FLAME_CR_PUB_COUNTER3
+#define IQ_CR_PUB_CCCR0             0x36c   //IQ_CR_PUB_COUNTER0
+#define IQ_CR_PUB_CCCR1             0x36d   //IQ_CR_PUB_COUNTER1
+#define IQ_CR_PUB_CCCR2             0x36e   //IQ_CR_PUB_COUNTER2
+#define IQ_CR_PUB_CCCR3             0x36f   //IQ_CR_PUB_COUNTER3
+#define IQ_CR_PUB_CCCR4             0x370   //IQ_CR_PUB_COUNTER4
+#define IQ_CR_PUB_CCCR5             0x371   //IQ_CR_PUB_COUNTER5
+
+#define CCCR_Overflow_MASK          0x80000000  //pos: 31
+#define CCCR_Cascade_MASK           0x40000000  //pos: 30
+#define CCCR_OVF_UBP1_MASK          0x20000000  //pos: 29
+#define CCCR_OVF_UBP0_MASK          0x10000000  //pos: 28
+#define CCCR_OVF_PMI_T1_MASK        0x08000000  //pos: 27
+#define CCCR_OVF_PMI_T0_MASK        0x04000000  //pos: 26
+#define CCCR_ForceOverflow_MASK     0x02000000  //pos: 25
+#define CCCR_Edge_MASK              0x01000000  //pos: 24
+#define CCCR_threshold_MASK         0x00f00000  //pos: 23-20
+#define CCCR_Complement_MASK        0x00080000  //pos: 19
+#define CCCR_Compare_MASK           0x00040000  //pos: 18
+#define CCCR_active_threads_MASK     0x00030000 //pos: 17-16
+#define CCCR_Select_MASK            0x0000e000  //pos: 15-13
+#define CCCR_Enable_MASK            0x00001000  //pos: 12
+
+//
+//  CCCR Select encoding
+//
+#define CCCR_SELECT_BSU             7
+#define CCCR_SELECT_FSB             6
+#define CCCR_SELECT_FIRM            1
+#define CCCR_SELECT_FLAME           0
+#define CCCR_SELECT_DAC             5
+#define CCCR_SELECT_MOB             2
+#define CCCR_SELECT_PMH             4
+#define CCCR_SELECT_SAAT            2
+#define CCCR_SELECT_U2L             3
+#define CCCR_SELECT_BPU             0
+#define CCCR_SELECT_IS              1
+#define CCCR_SELECT_ITLB            3
+#define CCCR_SELECT_CRUD            4
+#define CCCR_SELECT_CRUN            5
+#define CCCR_SELECT_CRUR            6
+#define CCCR_SELECT_IQ              0
+#define CCCR_SELECT_RAT             2
+#define CCCR_SELECT_SSU             3
+#define CCCR_SELECT_MS              0
+#define CCCR_SELECT_TBPU            2
+#define CCCR_SELECT_TC              1
+#define CCCR_SELECT_IX              5
+#define CCCR_SELECT_ALF             1
+
+/*
+
+    Each performance counter is 40-bits wide. The RDPMC instruction has been 
+    enhanced to enable reading of either the full counter-width (40-bits) 
+    or the lower 32-bits of the counter. 
+
+    +------------------------------------------------------------------------+
+    | 39-0                                                                   |
+    +------------------------------------------------------------------------+
+    | performance Counter register                                           |
+    +------------------------------------------------------------------------+
+
+    +------------------------------------------------------------------------+
+    | 63-40                                                                  |
+    +------------------------------------------------------------------------+
+    |Rsvd                                                                    |
+    +------------------------------------------------------------------------+
+
+*/
+
+//
+//      COUNTERS                    MSR       
+//
+#define BPU_CR_PUB_COUNTER0         0x300   //Counter #0
+#define BPU_CR_PUB_COUNTER1         0x301   //Counter #1
+#define BPU_CR_PUB_COUNTER2         0x302   //Counter #2
+#define BPU_CR_PUB_COUNTER3         0x303   //Counter #3
+#define MS_CR_PUB_COUNTER0          0x304   //Counter #4
+#define MS_CR_PUB_COUNTER1          0x305   //Counter #5
+#define MS_CR_PUB_COUNTER2          0x306   //Counter #6
+#define MS_CR_PUB_COUNTER3          0x307   //Counter #7
+#define FLAME_CR_PUB_COUNTER0       0x308   //Counter #8
+#define FLAME_CR_PUB_COUNTER1       0x309   //Counter #9
+#define FLAME_CR_PUB_COUNTER2       0x30a   //Counter #10
+#define FLAME_CR_PUB_COUNTER3       0x30b   //Counter #11
+#define IQ_CR_PUB_COUNTER0          0x30c   //Counter #12
+#define IQ_CR_PUB_COUNTER1          0x30d   //Counter #13
+#define IQ_CR_PUB_COUNTER2          0x30e   //Counter #14
+#define IQ_CR_PUB_COUNTER3          0x30f   //Counter #15
+#define IQ_CR_PUB_COUNTER4          0x310   //Counter #16
+#define IQ_CR_PUB_COUNTER5          0x311   //Counter #17
+
+#define NUM_CCCRS_BPU               4
+#define NUM_CCCRS_MS                4
+#define NUM_CCCRS_FLAME             4
+#define NUM_CCCRS_IQ                6
+
+#define NUM_ESCRS_BSU               2
+#define NUM_ESCRS_FSB               2
+#define NUM_ESCRS_FIRM              2
+#define NUM_ESCRS_FLAME             2
+#define NUM_ESCRS_DAC               2
+#define NUM_ESCRS_MOB               2
+#define NUM_ESCRS_PMH               2
+#define NUM_ESCRS_SAAT              2
+#define NUM_ESCRS_U2L               2
+#define NUM_ESCRS_BPU               2
+#define NUM_ESCRS_IS                2
+#define NUM_ESCRS_ITLB              2
+#define NUM_ESCRS_CRUD              2
+#define NUM_ESCRS_CRUN              2
+#define NUM_ESCRS_CRUR              2
+#define NUM_ESCRS_IQ                2
+#define NUM_ESCRS_RAT               2
+#define NUM_ESCRS_SSU               1
+#define NUM_ESCRS_MS                2
+#define NUM_ESCRS_TBPU              2
+#define NUM_ESCRS_TC                2
+#define NUM_ESCRS_IX                2
+#define NUM_ESCRS_ALF               2
+
+/*
+    Hyper-Threading Technology Support
+*/
+#define CPUID_ACPI_MASK                 0x00400000
+#define CPUID_ATHROT_MASK               0x20000000
+#define CPUID_HT_MASK                   0x10000000
+
+/*
+
+    precise event-based sampling control register: CRU_CR_PEBS_MATRIX_HORIZ 
+
+    +------------------+-----------------+------------------------------------+
+    |       26         |      25         |               24-00                |
+    +------------------+-----------------+------------------------------------+
+    |ENABLE_EBS_OTH_THR|ENABLE_EBS_MY_THR|              Rsvd                  |
+    +------------------+-----------------+------------------------------------+
+
+    +-------------------------------------------------------------------------+
+    | 63-27                                                                   |
+    +-------------------------------------------------------------------------+
+    |Rsvd                                                                     |
+    +-------------------------------------------------------------------------+
+
+
+*/
+
+#define TC_CR_PRECISE_EVENT             0x3f0
+#define CRU_CR_PEBS_MATRIX_HORIZ        0x3f1
+#define CRU_CR_PEBS_MATRIX_VERT         0x3f2
+
+#define ENABLE_EBS_OTH_THR              0x04000000
+#define ENABLE_EBS_MY_THR               0x02000000
+
+/*
+
+    Configuration Area for Debug Trace and precise event: WMT_CR_DTES_ATEA
+
+    +------------------------------------------------------------------------+
+    | 31-0                                                                   |
+    +------------------------------------------------------------------------+
+    | Configuration Area Linear Address                                      |
+    +------------------------------------------------------------------------+
+
+    +------------------------------------------------------------------------+
+    | 63-32                                                                  |
+    +------------------------------------------------------------------------+
+    |Rsvd                                                                    |
+    +------------------------------------------------------------------------+
+
+    MSR Address: 0600h, defined in dtsdrv.h
+
+*/
+
+/*
+
+    The DTES memory storage has three parts: 
+        the save area that has the buffer management information common to DTS 
+            and precise EMON, 
+        the debug trace store (DTS) for the branch trace messages, 
+        and the precise event-based sampling store. 
+        
+    DTES buffer format:
+    +------------+----------------------------------------------------------------+
+    |  Offset    | Double Word Contents                                           |
+    +------------+----------------------------------------------------------------+
+    |   0x00     | DTS buffer base                                                |   
+    +------------+----------------------------------------------------------------+ 
+    |   0x04     | DTS index: the location to write the next record to            |
+    +------------+----------------------------------------------------------------+
+    |   0x08     | DTS absolute max: next byte past the end of the buffer         |
+    +------------+----------------------------------------------------------------+
+    |   0x0C     | DTS interrupt threshold. Must be < absolute maximum.           |
+    |            |   Must be at an offset from the base that is a multiple of     |
+    |            |   the branch trace message record size                         |
+    +------------+----------------------------------------------------------------+
+    |   0x10     | precise event-based sampling buffer base                       |
+    +------------+----------------------------------------------------------------+
+    |   0x14     | precise event-based sampling index                             |
+    +------------+----------------------------------------------------------------+
+    |   0x18     | precise event-based sampling absolute max                      |
+    +------------+----------------------------------------------------------------+ 
+    |   0x1C     | precise event-based sampling interrupt threshold.              |
+    |            |   Must be < absolute maximum. Must be at an offset from        |
+    |            |   the base that is a multiple of the branch trace message      |
+    |            |   record size.                                                 |
+    +------------+----------------------------------------------------------------+
+    |   0x20     | precise event-based sampling counter reset value, lower 32 bits|
+    +------------+----------------------------------------------------------------+
+    |   0x24     | precise event-based sampling counter reset value, upper 8-bits |
+    +------------+----------------------------------------------------------------+
+    |   0x30     | Reserved - 128 bytes                                           |
+    +------------+----------------------------------------------------------------+
+
+    Each precise Event-based Sampling Record:
+    
+    +------------+----------------------------+
+    |  Offset    | Contents                   |
+    +------------+----------------------------+
+    |   0x00     | Eflags                     |
+    +------------+----------------------------+
+    |   0x04     | Linear IP                  |
+    +------------+----------------------------+
+    |   0x08     | EAX                        |
+    +------------+----------------------------+
+    |   0x0C     | EBX                        |
+    +------------+----------------------------+
+    |   0x10     | ECX                        |
+    +------------+----------------------------+
+    |   0x14     | EDX                        |
+    +------------+----------------------------+
+    |   0x18     | ESI                        |
+    +------------+----------------------------+ 
+    |   0x1C     | EDI                        |
+    +------------+----------------------------+
+    |   0x20     | EBP                        |
+    +------------+----------------------------+
+    |   0x24     | ESP                        |
+    +------------+----------------------------+
+
+*/
+
+typedef struct _DTS_RECORD {
+    void_ptr from_address;
+    void_ptr to_address;
+    void_ptr branch_prediction; // Only for Pentium(R) 4 processor
+} DTS_RECORD, *PDTS_RECORD;
+
+typedef struct _DTS_BUFFER {
+    __u32 base;
+    __u32 index;
+    __u32 max;
+    __u32 threshold;
+    __u32 PEBS_base;
+    __u32 PEBS_index;
+    __u32 PEBS_max;
+    __u32 PEBS_threshold;
+    __u32 PEBS_counter_reset_value1;    //lower 32 bits
+    __u32 PEBS_counter_reset_value2;    //Upper 8 bits
+    __u32 buffer[32];   // 128 bytes Pentium(R) 4 processor OSWG Manual
+} DTS_BUFFER, *PDTS_BUFFER;
+
+typedef enum _buffer_mode {
+    buf_circular,
+    buf_interrupt
+} buffer_mode;
+
+typedef struct _DTS_USER_HEADER {
+    char processor_number;
+    __u32 number_of_branches;
+    __u32 buffer_mode;  // buf_circular, buf_interrupt or Emon
+    __u32 reserved1;
+} DTS_USER_HEADER, *PDTS_USER_HEADER;
+
+#define DTS_RECORD_SIZE             sizeof(DTS_RECORD)
+#define DTS_SAVE_AREA               sizeof(DTS_BUFFER)
+
+#define OVERFLOW_RECORDS_SIZE       (OVERFLOW_RECORDS*DTS_RECORD_SIZE)
+#define DTS_INFO_AREA               (DTS_SAVE_AREA+OVERFLOW_RECORDS_SIZE+DTS_RECORD_SIZE)
+#define DEFAULT_BUFFER_SIZE         (MINIMUM_BRANCHES*DTS_RECORD_SIZE+DTS_INFO_AREA)
+
+/*
+    __u32 linear_IP;
+*/
+
+//
+// For PEBS
+//
+typedef struct _PEBS_RECORD {
+    __u32 E_flags;
+    __u32 linear_IP;
+    __u32 EAX1;
+    __u32 EBX1;
+    __u32 ECX1;
+    __u32 EDX1;
+    __u32 ESI1;
+    __u32 EDI1;
+    __u32 EBP1;
+    __u32 ESP1;
+} PEBS_RECORD, *PPEBS_RECORD;
+
+typedef struct _PEBS_USER_HEADER {
+    char processor_number;
+    __u32 number_of_samples;
+    __u32 buffer_mode;
+    __u32 reserved1;
+} PEBS_USER_HEADER, *PPEBS_USER_HEADER;
+
+#define PEBS_RECORD_SIZE            sizeof(PEBS_RECORD)
+#define PEBS_INFO_AREA              (DTS_SAVE_AREA+PEBS_OVERFLOW_SIZE+PEBS_RECORD_SIZE)
+
+//
+// Where the LBRs are... at least for now...
+// no guarantee they will remain here or remain sequentially ordered...
+//
+#define MSR_LASTBRANCH_TOS  	0x01da
+#define MSR_LASTBRANCH_0    	(MSR_LASTBRANCH_TOS + 1)
+#define MSR_LASTBRANCH_1    	(MSR_LASTBRANCH_0 + 1)
+#define MSR_LASTBRANCH_2    	(MSR_LASTBRANCH_0 + 2)
+#define MSR_LASTBRANCH_3    	(MSR_LASTBRANCH_0 + 3)
+
+#define LBR_SAVE_SIZE		(sizeof(ULARGE_INTEGER) * 5)
+
+typedef struct _IA32EM64T_DTS_BUFFER {
+    __u64 base;                   // Offset 0x00
+    __u64 index;                  // Offset 0x08
+    __u64 max;                    // Offset 0x10
+    __u64 threshold;              // Offset 0x18
+    __u64 PEBS_base;               // Offset 0x20
+    __u64 PEBS_index;              // Offset 0x28
+    __u64 PEBS_max;                // Offset 0x30
+    __u64 PEBS_threshold;          // Offset 0x38
+    __u64 PEBS_counter_addr;        // Offset 0x40
+    __u64 PEBS_counter_reset;       // Offset 0x48
+} IA32EM64T_DTS_BUFFER, *PIA32EM64T_DTS_BUFFER;
+
+
+//
+// For PEBS
+//
+typedef struct _IA32EM64T_PEBS_RECORD {
+    __u64 E_flags;            // Offset 0x00
+    __u64 linear_IP;          // Offset 0x08
+    __u64 rax;                // Offset 0x10
+    __u64 rbx;                // Offset 0x18
+    __u64 rcx;                // Offset 0x20
+    __u64 rdx;                // Offset 0x28
+    __u64 rsi;                // Offset 0x30
+    __u64 rdi;                // Offset 0x38
+    __u64 rbp;                // Offset 0x40
+    __u64 rsp;                // Offset 0x48
+    __u64 r8;                 // Offset 0x50
+    __u64 r9;                 // Offset 0x58
+    __u64 r10;                // Offset 0x60
+    __u64 r11;                // Offset 0x68
+    __u64 r12;                // Offset 0x70
+    __u64 r13;                // Offset 0x78
+    __u64 r14;                // Offset 0x80
+    __u64 r15;                // Offset 0x88
+} IA32EM64T_PEBS_RECORD, *PIA32EM64T_PEBS_RECORD;
+
+//
+// This is the same for all PEBS enabled processoes (so far) and would
+// go in a "generic x86" PMU header file...
+//
+#define MSR_CR_DTES_AREA        0x600
+
+//
+// this is also common with all PEBS enabled processors (so far)
+//
+#define DEBUG_CTRL_TR_BIT   (1 << 2)
+
+//
+// Will need to move this to a processor specific spot... Since we need
+// to deal with Intel(R) Pentium(R) 4 processors with Streaming SIMD 
+// Extensions 3 (SSE3) vs. those without at runtime, we need this
+// information in an array that is generated as needed.  So, use base
+// base and counts instead of the full list spelled out.
+//
+// LBR msr's for IA-32 processors.  NOT ARCHITECTURAL so they can change!
+//
+#define MSR_LBR_PSC_TOS             474
+
+#define MSR_LBR_PSC_SRC_BASE        1664
+#define MSR_LBR_PSC_SRC_NUMBER      16
+
+#define MSR_LBR_PSC_TARG_BASE       1728
+#define MSR_LBR_PSC_TARG_NUMBER     16
+
+#endif /* _VTUNE_FAMILYF_MSR_H */
diff -urN linux.orig/drivers/char/vtune/include/driversharedarea.h linux/drivers/char/vtune/include/driversharedarea.h
--- linux.orig/drivers/char/vtune/include/driversharedarea.h	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/include/driversharedarea.h	2004-01-14 13:57:27.000000000 -0800
@@ -0,0 +1,290 @@
+/*
+ *  driversharedarea.h
+ *
+ *  Copyright (C) 2002-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: driversharedarea.h
+ *
+ *	Description: sampling data structures shared between Windows*
+ *                   and Linux*
+ *
+ *	Author(s): George Artz, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#ifndef _DRIVERSHAREDAREA_H
+#define _DRIVERSHAREDAREA_H
+
+#include "sampfile.h"
+
+#define AREA_VERSION    12  // for area_version (read as 0.12)
+
+/*
+******************************************************************************
+*           Windows* and Linux* shared structs
+******************************************************************************
+*/
+
+/*
+  The following structure is created and mapped by TBS Sampler device drivers,
+  Windows*.  This area is pagefixed and can be globally mapped.
+  The mapping address may be different in each process, so care must be taken
+  to use only relative offsets within this area, no absolute pointers.
+*/
+
+typedef struct driver_shared_area_s {
+  __u32 length;                  // length of this entire area
+  __u32 area_version;            // version of this mapping area
+  __u32 driver_version;          // driver version number  (e.g. 400 read as 4.00)
+  __u32 offset_marks_global_area;// offset of marks global area
+  __u32 sample_skip_count;       // count of samples skipped due to pause mode or some other     01-15-98
+                                 // ..reason. This is a total for all cpus all counters          01-15-98
+  __u32 suspend_start_count;     // count of times sampling suspended during a sampling session  01-15-98
+                                 // ..suspension is usually due to buffer full write             01-15-98
+  __u32 suspend_end_count;       // count of times sampling suspend ended during a sampling session  01-15-98
+  __u32 reserved[17];            // other offsets are coming
+                                 // BK 11-14-97 changed pause_count from __u32 to __s32 to be able to use Interlocked... APIs correctly
+  __u32 pause_count;             // pause / resume count.  Sampling paused if greater than 0
+  __u32 sample_session_count;    // incremented for each TBSStart()
+  __u32 sample_count;            // current sample count
+  __u32 lock_owner;              // pid of current sampler lock owner (no lock if NULL)
+  __u32 num_open;                // number of current open's to device driver
+  char vtune_output_file[256];   // fully qualified name of sample file output
+  __u32 cpu_ID[32];              // id's of cpu's (num cpu in sys_info.dw_number_of_processors)
+  __u32 mega_hertz;              // approximate speed of this machine in megahertz (used for TSC conversion)
+  __u32 vtune_filename_offset;   // offset into path of filename for vtune_output_file
+  __u32 reserved2[11];
+  //
+  union {
+    __u32 flags;                 // status flags as one __u32
+    struct {                     // options broken out by bit fields
+      __u32 running:1;           // sampling is running
+      __u32 start_delay:1;       // start delay in effect (loads being tracked, no sampling)
+      __u32 pid_set:1;           // the sampler device driver set pid's in the sample records
+      __u32 tid_set:1;           // the sampler device driver set tid's in the sample records
+      __u32 module_tracking:1;   // driver tracks module loads
+      __u32 module_rec2and5:1;   // driver can build either 2.0 or TB5 format module records.  01-05-02
+      // This flag is was defined for Windows* 95 VXD. The VXD builds
+      // the 2.0 format module records for VTune analyzer version 5
+      // and VTune analyzer version 6 beta.  TB5 format module records
+      // will be built for VTune analyzer version 6 FCS.
+      // The TB5 module record option is activated for a sampling 
+      // session via a command in the samp_parm_ex configuration structure
+      __u32 pid_creates_tracked_in_module_recs:1;  // if this flag is set, the
+      //  1stModuleRecInProcess flag in the module_record 
+      //  struct indicates that a process 
+      //  was created 
+      __u32:25;                  // flags to come
+    };
+  };
+  //
+  //  following fields set by driver after sampling complete
+  __u32 duration;                // total sample time in milliseconds
+  __u32 tot_samples;             // total samples taken (sample_count at end of session)
+  __u16 sample_rec_length;       // length of each sample record
+  __u16 reserved3;
+  __u32 tot_idt_ints;            // total physical interrupts taken for this sampling session
+  __u32 tot_profile_ints;        // total number of calls by OS to our profile interrupt routine
+  __u32 tot_skip_samp;           // total samples skipped during file I/O
+  union {
+    __u32 driver_flags;          // supported samplings methods on this machine
+    struct {
+      __u32 method_RTC:1;        // real time clock
+      __u32 method_VTD:1;        // OS Virtual Timer Device
+      __u32 method_NMI:1;        // non-maskable interrupt time based
+      __u32 method_EBS:1;        // event based
+      __u32:28;                  // flags to come
+    };
+  };
+  __u16 num_event_counters;  // number of event counters supported
+  __u16 reserved4;
+  __u32 reserved5[6];
+} driver_shared_area;
+
+/*
+  Sample Parameter area.  Passed to driver to start / configure sampling.
+*/
+
+#define MAX_ESR_SETS 16      // Pentium(R) 4 processor  05-31-00
+
+typedef struct _samp_parm_header {
+  char eye_catcher[4];           // eyecatcher "SPRM"
+  __u16 length;                  // length of this header
+  __u16 sp_version;              // sampparm version
+  __u32 sp_offset;               // offset to sampparm structure from the beginning of this structure
+  __u32 sp_length;               // length of the sampparm struct
+} samp_parm_header;
+
+typedef struct samp_parm3_s {
+  union {
+    __u32 options;               // options for sampling as one __u32
+    struct {                     // options broken out by bit fields
+      __u32 sample_TSC:1;        // record TimeStampCounter in samples
+      __u32 start_in_pause_mode:1; // if set, start sampling in Pause mode  BK 3-10-98
+      // the next two bits are designed to remain backwards compatible with other users who
+      // are using the TBS subsystem DLLs without VTune analyzer BK 9-10-98
+      __u32 PC_sampling_off:1;  // reset (i.e. 0) if traditional (TBS/EBS) sampling is supposed to be launched
+      __u32 chronologies_on:1;  // set if chronologies are to be collected
+      __u32 EBS_chronologies_on:1; // set if EBS chronologies are to be collected
+      __u32 stop_returns_result:1; // if set, TBSstop returns error codes, else TBSstop returns sample count
+      __u32:26;                   // options to come
+    } sp3_o;
+  } sp3_options;
+  __u16 method;                 // sampling type, see SM_ defines in SampFile.h
+  __u16 reserved;
+  __u32 samp_rate;              // sample rate in microseconds
+  __u32 samps_per_buffer;       // samples per buffer
+  __u32 maximum_samples;        // stop sampling when this number reached (0 = infinite)
+  __u32 max_interval;           // maximum time to sample in seconds (0 = infinite)
+  __u32 start_delay;            // seconds to delay start of sampling
+  char *module_info_file_name;  // name of file to write module records to
+  char *raw_sample_file_name;   // name of file to write samples to (before merging is done)
+  __u32 num_event_reg_set;      // number of event_reg_set structures
+  event_reg_set esr_set[MAX_ESR_SETS]; // used to set Event registers   Pentium(R) 4 processor 05-31-00
+  __u32 reserved2[9];
+} samp_parm3;
+
+typedef struct samp_parm5_s {
+  union {
+    __u32 options;              // options for sampling as one __u32
+    struct {                    // options broken out by bit fields
+      __u32 sample_TSC:1;       // record TimeStampCounter in samples
+      __u32 track_pid_creates:1;
+      __u32 track_tid_creates:1;
+      __u32 start_in_pause_mode:1;  // if set, start sampling in Pause mode  BK 3-10-98
+      __u32:28; // options to come
+    } sp5_o;
+  } sp5_options;
+  __u32 samps_per_buffer;       // samples per buffer
+  __u32 maximum_samples;        // stop sampling when this number reached (0 = infinite)
+  __u32 max_interval;           // maximum time to sample in seconds (0 = infinite)
+  __u32 start_delay;            // seconds to delay start of sampling
+  char *module_info_file_name;  // name of file to write module records to
+  char *raw_sample_file_name;   // name of file to write samples to (before merging is done)
+  __u32 reserved2[16];
+  __u32 num_event_reg_set;      // number of event_reg_set structures
+  event_reg_set_ex esr_set[1];  // used to set Event registers
+} samp_parm5;
+
+typedef struct samp_parm6_s {
+  union {
+    __u32 options;              // options for sampling as one __u32
+    struct {                    // options broken out by bit fields
+      __u32 sample_TSC:1;       // record TimeStampCounter in samples
+      __u32 track_pid_creates:1;
+      __u32 track_tid_creates:1;
+      __u32 start_in_pause_mode:1;  // if set, start sampling in Pause mode  BK 3-10-98
+      __u32 calibration:1;      // calibration... don't collect PC samples or module info
+      __u32 count_events:1;     // keep event totals
+      __u32 ptrs_are_offsets:1; // all pointer fields in this structure are offsets
+      __u32 create_final_file:1;//
+      __u32 use_marks_collector:1; //
+      __u32:23;                 // options to come
+    };
+  };
+  __u32 samps_per_buffer;       // samples per buffer
+  __u32 maximum_samples;        // stop sampling when this number reached (0 = infinite)
+  __u32 max_interval;           // maximum time to sample in seconds (0 = infinite)
+  __u32 start_delay;            // seconds to delay start of sampling
+  __u32 reserved1[3];           // reserved for future use and for 8 byte alignment of the following 64-bit fields
+  union {
+    ULARGE_INTEGER module_info_file_name64;
+    char *module_info_file_name; // name of file to write module records to
+  };
+  union {
+    ULARGE_INTEGER raw_sample_file_name64;
+    char *raw_sample_file_name; // name of file to write samples to (before merging is done)
+  };
+  __u32 reserved2[15];          // reserved for future use and 8 byte alignment of event_reg_set_ex array
+  __u32 num_event_reg_set;      // number of event_reg_set structures
+  event_reg_set_ex esr_set[1];  // used to set Event registers
+} samp_parm6;
+
+typedef struct samp_parm_ex {
+  samp_parm_header hdr;         // sampparms header
+  union {
+    samp_parm3 sp3;     // sampparms
+    samp_parm5 sp5;     // ..
+    samp_parm6 sp6;     // ..
+  } _spex;
+} samp_parm_ex, *P_samp_parm_ex;
+
+//
+// Event totals
+//  
+// 
+//  
+typedef struct _EVENT_TOTALS {
+  __u32 version;        // version of event totals
+  __u32 length;         // length of this event totals buffer
+  __u32 flags;          // flags
+  __u32 num_events;     // number of events in the event totals arrays
+  __u32 num_cpus;       // number of cpus represented in the event totals arrays
+  __u32 offset_event_I_ds;    // offset of array of event IDs (__u32S). numEventIDs is number of entries in the array.
+  __u32 offset_event_counts;  // offset of array of event counts (64 bit unsigned). numEventIDs is number of entries in the array.
+  __u32 offset_event_counts_per_cpu;    // offset of two dimensional array of event counts (ULARGE_INTEGERs... 64 bit unsigned).
+  // Can be indexed by [event number, cpu] where event number and cpu is current cpu number relative to 0.
+  // is index of event_Ids array. numEventIDs and num_cpus are the two dimensions of the array/matrix.
+  __u32 offset_tsc_at_start;  // time stamp counter when event counting started on a cpu. Two dimensional array with bounds [ num_events, num_cpus]
+  __u32 offset_tsc_at_last_update;  // time stamp counter when event count last updated per cpu. Two dimensional array with bounds [ num_events, num_cpus]
+  __u32 reserved[3];  // reverved (8 byte alignment)
+} EVENT_TOTALS, *PEVENT_TOTALS;
+
+//
+// Definitions for RDPMC Driver API
+//
+
+#define RDPMC_MAX_CPUS 32
+#define RDPMC_MAX_PMCS 64
+
+typedef struct _PMC_VALUE {
+  ULARGE_INTEGER pmc_current;   // current pmc value
+  ULARGE_INTEGER pmc_total;
+  ULARGE_INTEGER tsc_last_update;
+} PMC_VALUE, *PPMC_VALUE;
+
+typedef struct _PMC_VALUES_PER_CPU {
+  PMC_VALUE pmc_val[RDPMC_MAX_CPUS];
+} PMC_VALUES_PER_CPU, *PPMC_VALUES_PER_CPU;
+
+typedef struct _RDPMC_BUF {
+  __u32 version;        // version of rdpmc buf
+  __u32 cpu_mask_in;    // cpu's to collect data on
+  __u32 cpu_mask_out;   // cpu's on which data was collected
+  __u32 reserved;       // reserved
+  __u32 duration;       // total time in milliseconds, obtained from the DSA
+  __u32 reserved1;      // reserved
+  ULARGE_INTEGER pmc_mask;  // each bit represents a logical performance counter register. 
+  // For x86, this specifies logical cpu perfrmance counter to be read (ecx for rdpmc... pmc_mask.bit0= mov ecx,0; rdpmc)
+  // For Itanium(R) processor, this specifies the PMD registers to be read (ex pmc_mask.bit4 = mov 4 = t15; mov pmd[t15] = a1;)
+  // reading of x86 pmc's on Itanium(R)-based processors is not supported
+  PMC_VALUES_PER_CPU pmc_values[RDPMC_MAX_PMCS]; // one entry for each pmc
+} RDPMC_BUF, *PRDPMC_BUF;
+#define RDPMC_BUF_SIZE sizeof(RDPMC_BUF)
+
+#endif  // _DRIVERSHAREDAREA_H
diff -urN linux.orig/drivers/char/vtune/include/sampfile.h linux/drivers/char/vtune/include/sampfile.h
--- linux.orig/drivers/char/vtune/include/sampfile.h	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/include/sampfile.h	2004-01-14 13:57:27.000000000 -0800
@@ -0,0 +1,338 @@
+/*
+ *  sampfile.h
+ *
+ *  Copyright (C) 2002-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: sampfile.h
+ *
+ *	Description: sampling data structures and constants
+ *
+ *	Author(s): George Artz, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#ifndef _SAMPFILE_H
+#define _SAMPFILE_H
+
+#include "vtypes.h"
+
+// sampling methods
+#define SM_RTC          2020  // real time clock
+#define SM_VTD          2021  // OS Virtual Timer Device
+#define SM_NMI          2022  // non-maskable interrupt time based
+#define SM_EBS          2023  // event based
+
+// sampling mechanism bitmap definitions
+#define INTERRUPT_RTC   0x1
+#define INTERRUPT_VTD   0x2
+#define INTERRUPT_NMI   0x4
+#define INTERRUPT_EBS   0x8
+
+// eflags defines
+#define EFLAGS_VM       0x00020000  // V86 mode
+#define EFLAGS_IOPL0    0
+#define EFLAGS_IOPL1    0x00001000
+#define EFLAGS_IOPL2    0x00002000
+#define EFLAGS_IOPL3    0x00003000
+
+typedef struct cpu_map_s {
+  __u32 ul_cpu_num;
+  __u32 ul_package_num;
+  __u32 ul_hardware_thread_num;
+} cpu_map;
+
+// added by Bob Knight, 08-17-2001
+typedef struct cpuid_output_s {
+  __u32 ul_cpu_num;
+  __u32 ul_EAX_input;
+  __u32 ul_EAX_output;
+  __u32 ul_EBX_output;
+  __u32 ul_ECX_output;
+  __u32 ul_EDX_output;
+} cpuid_output;
+
+/*
+  X86 processor code descriptor
+*/
+typedef struct code_descriptor_s {
+  union {
+    __u32 low_word;       // low dword of descriptor
+    struct {              // low broken out by fields
+      __u16 limit_low;    // segment limit 15:00
+      __u16 base_low;     // segment base 15:00
+    };
+  };
+  union {
+    __u32 high_word;      // high word of descriptor
+    struct {              // high broken out by bit fields
+      __u32 base_mid:8;   // base 23:16
+      __u32 accessed:1;   // accessed
+      __u32 readable:1;   // readable
+      __u32 conforming:1; // conforming code segment
+      __u32 one_one:2;    // always 11
+      __u32 dpl:2;        // dpl
+      __u32 pres:1;       // present bit
+      __u32 limit_hi:4;   // limit 19:16
+      __u32 sys:1;        // available for use by system
+      __u32 reserved_0:1; // reserved, always 0
+      __u32 default_size:1;  // default operation size (1=32bit, 0=16bit)
+      __u32 granularity:1;   // granularity (1=32 bit, 0=20 bit)
+      __u32 base_hi:8;    // base hi 31:24
+    };
+  };
+} code_descriptor;
+
+typedef struct sample_record_PC_s {
+  union {
+    struct {
+      ULARGE_INTEGER iip;  // Itanium(R) processor interrupt instruction pointer
+      ULARGE_INTEGER ipsr; // Itanium(R) processor interrupt processor status register
+    };
+    struct {
+      __u32 eip;           // x86 instruction pointer
+      __u32 eflags;        // x86 eflags
+      code_descriptor csd; // x86 code seg descriptor (8 bytes)
+    };
+  };
+  __u16 cs;                // x86 cs (0 for Itanium(R) processor)
+  union {
+    __u16 cpu_and_OS;      // cpu and OS info as one word
+    struct {               // cpu and OS info broken out
+      __u16 cpu_num:8;     // cpu number (0 - 255)
+      __u16 resrvd:4;      // reserved bits
+      __u16 not_vmid0:1;   // Windows* 95, vmid0 flag (1 means NOT vmid 0)
+      __u16 code_mode:2;   // processor mode, see MODE_ defines
+      __u16:1;             // reserved
+    };
+  };
+  __u32 tid;               // thread ID  (from OS, may get reused, a problem, see tid_is_raw)   06-25-99
+  __u32 pid_rec_index;     // process ID rec index (index into start of pid record section)
+                           // .. can validly be 0 if not raw (array index).  Use returnPid() to
+                           // ..access this field
+                           // .. (see pid_rec_index_raw)
+  union {
+    __u32 bit_fields2;
+    struct {
+      __u32 mr_index:20;    // module record index (index into start of module rec section)
+      // .. (see mr_index_none)
+      __u32 event_index:8;  // index into the events section
+      __u32 tid_is_raw:1;   // tid is raw OS tid                                       06-25-99
+      __u32 itp_pc:1;       // PC sample (TRUE=this is a Itanium(R) processor PC sample record)
+      __u32 pid_rec_index_raw:1;  // pid_rec_index is raw OS pid
+      __u32 mr_index_none:1;  // no mr_index (unknown module)
+    };
+  };
+} sample_record_PC, *P_sample_record_PC;
+
+// time stamp counter section   (see sample_record_section_flags in sample_file_header)
+typedef struct sample_record_TSC_s {
+  ULARGE_INTEGER tsc; // processor timestamp counter
+} sample_record_TSC;
+
+typedef struct module_record_s {
+  __u16 rec_length;       // total length of this record (including this length, always __u32 multiple)
+                          // ..output from sampler is variable length (pathname at end of record)
+                          // ..sampfile builder moves path names to a separate "literal pool" area
+                          // ..so that these records become fixed length, and can be treated as an array
+                          // .. see modrec_fixed_len in header
+  
+  __u16 segment_type:2;   // V86, 16, 32, 64 (see MODE_ defines), maybe inaccurate for Windows* 95
+                          // .. a 16 bit module may become a 32 bit module, inferred by
+                          // ..looking at 1st sample record that matches the module selector
+  __u16 load_event:1;     // 0 for load, 1 for unload
+  __u16:13;               // reserved
+  
+  __u16 selector;         // code selector or V86 segment
+  union {
+    __u16 segment_name_length;  // length of the segment name if the segment_name_set bit is set
+    __u16 reserved;
+  };
+  //   __u16    reserved;
+  __u32 segment_number;   // segment number, Windows* 95 (and now Java*) can have multiple pieces for one module
+  union {
+    __u32 flags;          // all the flags as one dword
+    struct {
+      __u32 exe:1;        // this module is an exe
+                          // __u32 global_moduleTB3 : 1;  // globally loaded module.  There may be multiple module records
+      __u32 global_module:1;  // globally loaded module.  There may be multiple module records
+                          // ..for a global module, but the samples will only point to the
+                          // ..1st one, the others will be ignored.  NT's Kernel32
+                          // ..is an example of this.  REVISIT this??
+      __u32 bogus_win95:1;    // "bogus" Windows* 95 module.  By bogus, we mean a module that has
+                          // ..a pid of 0, no length and no base.  selector actually used
+                          // ..as a 32 bit module.
+      __u32 pid_rec_index_raw:1;  // pid_rec_index is raw OS pid
+      __u32 sample_found:1;   // at least one sample referenced this module
+      __u32 tsc_used:1;   // tsc set when record written
+      __u32 duplicate:1;  // 1st pass analysis has determined this is a duplicate load
+      __u32 global_module_Tb5:1;  // module mapped into all processes on system
+      __u32 segment_name_set:1;   // set if the segment name was collected (initially done for Xbox* collections)
+      __u32 first_module_rec_in_process:1;    // if the pid_creates_tracked_in_module_recs flag is set 
+                          //  in the SampleHeaderEx struct and this flag 
+                          //  is set, the associated module indicates 
+                          //  the beginning of a new process 
+      __u32:22;           // reserved
+    };
+  };
+  union {
+    ULARGE_INTEGER length64; // module length
+    __u32 length;         // module length (not used for 16 bit, bytes (mult of 16) for V86, bytes for 32 bit)
+  };
+  union {
+    ULARGE_INTEGER load_addr64; // load address
+    __u32 load_addr;      // load address (0 for 16 bit, bytes (mul of 16) for V86, bytes for 32 bit)
+  };
+  __u32 pid_rec_index;    // process ID rec index (index into  start of pid record section).
+                          // .. (see pid_rec_index_raw).  If pid_rec_index == 0 and pid_rec_index_raw == 1
+                          // ..then this is a kernel or global module.  Can validly
+                          // ..be 0 if not raw (array index).  Use returnPid() to access this
+                          // ..field
+  __u32 load_sample_count; // sample count when module loaded (add 1 to this when comparing)
+                          // .. if 0, loaded prior to sampling start
+  __u32 unload_sample_count;  // sample count when module unloaded
+  __u32 path;             // module path name (section offset on disk)
+                          // ..when initally written by sampler name is at end of this
+                          // ..struct, when merged with main file names are pooled at end
+                          // ..of module_record Section so ModulesRecords can be
+                          // ..fixed length
+  __u16 path_length;      // path name length (inludes terminating \0)
+  __u16 filename_offset;  // offset into path name of base filename
+  union {
+    __u32 segment_name;   // offset to the segment_name from the beginning of the 
+                          //  module section in a processed module section 
+                          //  (s/b 0 in a raw module record)
+                          // in a raw module record, the segment name will follow the 
+                          //  module name and the module name's terminating NULL char
+    __u32 reserved2;
+  } mr_sn;
+  ULARGE_INTEGER tsc;     // time stamp counter when record written (see tsc_used)
+} module_record;
+
+typedef struct event_reg_set_ex_s {
+  union {
+    __u32 counter_number; // counter number to set
+    struct {              // 
+      __u32 data:24;      // depends on command
+      __u32 command:8;    // see below
+    };
+  };
+  __u32 event_ID;
+  __u32 reserved;         // added to get the structure on an 8 byte boundary
+  __u32 esr_value;        // value to set register with.  If 2 regs are set with
+                          // ..1 esr_value (e.g. Pentium(R) processor) 2 event_reg_set entries are needed
+                          // ..with the esr_value the same in both of them
+  ULARGE_INTEGER esr_count;   // counter value (may be transformed depending on esr value)
+} event_reg_set_ex;
+
+// event_reg_set command definitions
+#define ERS_CMD_SET_CONFIG_AND_COUNTER_REGS 0   // data field is logical counter number                          
+                                                // esr_value is value of config register (max 32 bits)
+                                                // esr_count is initial value of count register (value will be negated before write to counter if counter is count up)
+#define ERS_CMD_WRITE_MSR  1   // data field is unused     
+                               // esr_value is msr number       
+                               // esr_count is msr value        
+#define ERS_CMD_NOP        2   // driver can ignore this entry      
+
+#define ERS_CMD_OS_EVENT   3   // sample based on OS event          
+                               // ..esr_value = OS event ID (On Windows NT*, See KeProfileinterruptWithSource and enum in NTDDK.H)
+                               // ..esr_count = event frequency
+
+#define ERS_CMD_TBS_VTD    4   // esr_count field of event_reg_set_ex struct contains the sampling interval in microseconds
+#define ERS_CMD_TBS_RTC    5   // esr_count field of event_reg_set_ex struct contains the sampling inteval in microseconds
+#define ERS_CMD_TBS_STATCARD 6 // esr_count field of event_reg_set_ex struct contains the sampling inteval in microseconds
+//#define ERS_CMD_EBS      7   // for consistency with other method commands (EBS is assumed if the other method
+                               // commands not used - in other words this command is not required)
+#define ERS_CMD_WRITE_PMC  8   //      
+
+typedef struct event_reg_set_s {
+  union {
+    __u32 counter_number;      // counter number to set
+    struct {                   // 
+      __u32 data:24;           // depends on command
+      __u32 command:8;         // see below
+    } ers_data;
+  } ers_counter_number;
+  __u32 esr_value;             // value to set register with.  If 2 regs are s et with
+                               // ..1 esr_value (e.g. Pentium(R) processor) 2 event_reg_set entries are needed
+                               // ..with the esr_value the same in both of them
+  ULARGE_INTEGER esr_count;    // counter value (may be transformed depending on esr value)
+} event_reg_set;
+
+typedef struct _INT_FRAME {
+  __u32 eip;
+  __u32 seg_cs;
+  __u32 E_flags;
+  __u32 reserved;
+  __u64 csd;                   // unscrambled code segment descriptor
+  ULARGE_INTEGER iip;
+  ULARGE_INTEGER ipsr;
+} INT_FRAME, *PINT_FRAME;
+
+/*
+  Pid Record.  Windows NT* and Windows* 95 can resue a pid number very rapidly.
+  This presents a uniqueness problem for sampling of any significant duration, 
+  especially if measurements are being taken on a system where lots of 
+  process create / terminates have deliberately been induced.  One of these
+  records is written for every process create and terminate.  Post analysis
+  is used to convert the raw system pid in the sample records into section
+  offset for these records, so that views by process can be meaningful even
+  if the process id is rapidly reused by the system.
+*/
+typedef struct pid_record_s {
+  __u16 rec_length;            // total length of this record (including this length, always __u32 multiple)
+                               // ..output from sampler is variable length (pathname at end of record)
+                               // ..sampfile builder moves path names to a separate "literal pool" area
+                               // ..so that these records become fixed length, and can be treated as an array
+  __u16 pid_event:1;           // 0 means created, 1 means destroyed (destroys not currently recorded)
+  __u16 pid_manufactured:1;    // 1 means that pid record was "manufactured" by analyzer, the sampler  
+                               // ..wrote no pid create record, its existence was inferred.  This
+                               // ..is usually for pids in existence before sampling starts (no enum)
+  __u16 name_generated:1;      // the name of this pid was generated                            
+  __u16:13;                    // reserved
+  __u32 os_pid;                // OS assigned pid value
+  __u32 os_pid_instance;       // "instance" of OS pid, a duplication count of how many different pids  
+                               // ..with the same os pid have occurred                                
+  __u32 index_num;             // the index value for this record.  Stored in sample and module records. 
+                               // ..saved in case this area needs sorting                             
+  __u32 sample_count;          // sample count at pid creation
+  __u32 sample_count_term;     // sample count at pid termination (inferred by analyzer)
+  ULARGE_INTEGER tsc;          // time stamp counter at pid creation
+  __u32 path;                  // exe path name (section offset on disk)
+  __u16 path_length;           // exe path name length (inludes terminating \0)
+  __u16 filename_offset;       // offset into path name of base exe filename
+} pid_record;
+
+// processor execution modes
+#define MODE_UNKNOWN    99
+// the following defines must start at 0
+#define MODE_64BIT      3
+#define MODE_32BIT      2
+#define MODE_16BIT      1
+#define MODE_V86        0
+
+#endif   // _SAMPFILE_H
diff -urN linux.orig/drivers/char/vtune/include/vtuneshared.h linux/drivers/char/vtune/include/vtuneshared.h
--- linux.orig/drivers/char/vtune/include/vtuneshared.h	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/include/vtuneshared.h	2004-04-12 20:39:33.000000000 -0700
@@ -0,0 +1,152 @@
+/*
+ *  vtuneshared.h
+ *
+ *  Copyright (C) 2002-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: vtuneshared.h
+ *
+ *	Description: sampling data structures shared between driver and library
+ *
+ *	Author(s): George Artz, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#ifndef _VTUNESHARED_H
+#define _VTUNESHARED_H
+
+#include "driversharedarea.h"
+
+/*
+******************************************************************************
+*           Linux* driver shared defs
+******************************************************************************
+*/
+
+#define DEV_SAMPLING_DRIVER     "/dev/vtune_d"  // device 0
+#define DEV_MODULES             "/dev/vtune_m"  // device 1
+#define DEV_PID_CREATES         "/dev/vtune_p"  // device 2
+
+#define VLIB_MAPFUNC            50
+
+/* Error Codes: general */
+#define LIB_RET_SUCCESS            0
+#define LIBERR_DEVICE_FAIL         100
+#define LIBERR_IOCTL_FAIL          102
+#define LIBERR_DEVICE_CONFIG       104
+#define LIBERR_CPUINFO_FAIL        106
+#define LIBERR_SYSINFO_FAIL        108
+#define LIB_SAMP_CANCEL            110
+/* Error Codes: sampling record related */
+#define LIBERR_VTEMP_CREATE        200
+#define LIBERR_SAMPF_CREATE        202
+#define LIBERR_PCWRITE             204
+#define LIBERR_FTEMPSEEK           206
+#define LIBERR_SAMPSEEK            208
+#define LIBERR_SAMPREAD            210
+#define LIBERR_SAMPWRITE           212
+#define LIBERR_USERSAMPALLOC_FAIL  214
+/* Error Codes: module record related */
+#define LIBERR_DIRPROC_FAIL        300
+#define LIBERR_MODF_CREATE         302
+#define LIBERR_MODDEV_FAIL         304
+#define LIBERR_MODQUERY            306
+#define LIBERR_MODOPEN             308
+#define LIBERR_MODINFO             310
+#define LIBERR_MODWRITE            312
+#define LIBERR_USERMODALLOC_FAIL   314
+/* Error Codes: pid record related */
+#define LIBERR_PIDDEV_OPEN_FAIL    400
+#define LIBERR_PIDFILE_CREATE_FAIL 402
+#define LIBERR_PIDFILE_OPEN_FAIL   404
+#define LIBERR_PIDREC_WRITE_FAIL   406
+
+/* Sampling Driver State */
+#define SINFO_STARTED             0x08  // sampling started
+#define SINFO_WRITE               0x04  // write procedure scheduled
+#define SINFO_STOPPING            0x02  // driver is stopping sampling
+#define SINFO_STOP_COMPLETE       0x01  // driver completed "stop sampling" cmd
+#define SINFO_EBS_RESTART         0x20  // restart EBS
+#define SINFO_DO_STOP             0x40  // schedule stop
+#define SINFO_DO_WRITE            0x80  // schedule write
+
+/* NMI type */
+#define EISA_NMI               1
+#define QUIX_NMI               2
+#define STAT_NMI               3
+#define EMON_NMI               4
+
+/* Driver IOCTL Defs */
+#define VTUNE_CONFIG           1  // return # of processors
+#define VTUNE_START            2  // return 0 or 1, is prf enabled?
+#define VTUNE_STOP             3  // return number of text symbols loaded
+#define VTUNE_STAT             4  // return size of symbol names
+#define VTUNE_ABORT            5  // users abort collection data
+#define VTUNE_PARM             6  // return user configure pararameters
+#define VTUNE_DUMP             7  // dump system debug info
+#define VTUNE_SYSINFO          9  // return os system info
+#define VTUNE_CONFIG_EX       12  // new model config
+#define VTUNE_START_EX        15  // new model start
+#define VTUNE_READPERF        18  // read perf data
+#define VTUNE_GETCPUINFO      19  // get cpu info
+#ifdef ENABLE_TGID
+#define VTUNE_GETTGRP         21  // get thread group info	
+#endif
+
+#define PS_VM                 0x00020000  // virtual 86 mode flag
+
+#define MAXNAMELEN            1024  // BUG: should not be fixed value (Windows* limit is 256) (and 2048 is too large for some kernels)
+
+/*
+ *  Sample information structure... maintained by Linux* driver
+ */
+typedef struct _Samp_info {
+  int sample_time;        // total sample time in milleseconds
+  int profile_ints_idt;   // total physical profile interrupts during  sampling
+  int profile_ints;       // total profile interrupts while sampling  (callback from OS)
+  int sample_count;       // total samples taken. Current sample number.
+  int test_IO_errs;       // number of times that VxD IO request was delayed (Windows* 95)
+  int sampling_active;    // pc sampling active (0 = pc sampling suspended)
+  int flags;              // flags
+  int sample_rec_length;  // length of sample record (to be deprecated ... use DSA instead)
+  int rsvd2;              // reserved 2
+  char cpu_family;        // cpu family
+  char cpu_model;         // cpu model
+  char cpu_stepping;      // cpu stepping
+  char rsvd3;             // reserved 3
+} sampinfo_t;
+
+#define FSET_DRV_PROCESS_CREATES_TRACKED_IN_MODULE_RECS 0x1
+
+#ifdef ENABLE_TGID
+typedef struct thread_info_s
+{
+  int tgrp_id;
+} thread_info;
+#endif
+
+#endif   // _VTUNESHARED_H
diff -urN linux.orig/drivers/char/vtune/include/vtypes.h linux/drivers/char/vtune/include/vtypes.h
--- linux.orig/drivers/char/vtune/include/vtypes.h	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/include/vtypes.h	2004-03-29 10:06:06.000000000 -0800
@@ -0,0 +1,84 @@
+/*
+ *  vtypes.h
+ *
+ *  Copyright (C) 2002-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: vtypes.h
+ *
+ *	Description: translation of types and macros from Windows* to Linux*
+ *
+ *	Author(s): George Artz, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#ifndef _VTYPES_H
+#define _VTYPES_H
+
+#include <linux/types.h>
+
+#define TEXT(txt) #txt
+
+/*
+typedef __u8 __u8;	// unsigned  8-bit integer (1 byte)
+typedef __u16 __u16;	// unsigned 16-bit integer (2 bytes)
+typedef __u32 __u32;	// unsigned 32-bit integer (4 bytes)
+typedef __s32 __s32;	//   signed 32-bit integer (4 bytes)
+typedef __u64 __u64;	// unsigned 64-bit integer (8 bytes)
+
+typedef __u16 __u16;
+typedef __u32 __u32;
+typedef char char;
+typedef void void;
+*/
+typedef char *char_ptr;
+typedef void *void_ptr;
+
+#ifdef linux32
+#define __u32_PTR __u32
+#endif
+
+#if defined(linux64) || defined(linux32_64)
+#define __u32_PTR __u64
+#endif
+
+typedef union _ULARGE_INTEGER {	// unsigned 64-bit integer (8 bytes)
+  struct {
+    __u32 low_part;
+    __u32 high_part;
+  };
+  __u64 quad_part;
+} ULARGE_INTEGER;
+
+typedef ULARGE_INTEGER *PULARGE_INTEGER;
+
+#define TRUE      1
+#define FALSE     0
+
+typedef enum boolean { B_FALSE, B_TRUE } BOOLEAN;
+
+#endif	// _VTYPES_H
diff -urN linux.orig/drivers/char/vtune/Makefile linux/drivers/char/vtune/Makefile
--- linux.orig/drivers/char/vtune/Makefile	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/Makefile	2004-05-11 13:15:53.476949478 -0700
@@ -0,0 +1,25 @@
+flags-$(CONFIG_X86)	+= -Dlinux32
+flags-$(CONFIG_X86_64)	+= -Dlinux32_64
+flags-$(CONFIG_IA64)	+= -Dlinux64
+#flags-$(CONFIG_PERFMON)	+= -DPERFMON_SUPPORTED
+
+EXTRA_CFLAGS += -I$(src)/include \
+		-DKERNEL_26X \
+		-DUSE_TYPE_WAIT_QUEUE_NEW \
+		-DUSE_TYPE_FILE_OPS_NEW \
+		-DUSE_MM_HEADER \
+		-DEXPORTED_FOR_EACH_PROCESS \
+		-DUSE_PARENT_TASK_FIELD \
+		-DREMAP_PAGE_RANGE_REQUIRES_EXTRA_ARGS \
+		-DDSA_SUPPORT_MMAP \
+		-DENABLE_TGID \
+		-DALLOW_LBRS \
+		$(flags-y)
+
+obj-$(CONFIG_VTUNE)	:= vtune_drv.o
+vtune_drv-y		:= vtune.o sys_call_table.o
+vtune_drv-$(CONFIG_X86)	+= vtlib32.o vtoshooks32.o vtxsys32.o \
+			   vtlibcommon32.o apic32.o
+vtune_drv-$(CONFIG_X86_64) += vtlib32_64.o vtoshooks32_64.o vtxsys32_64.o \
+			      apic32_64.o vtlibcommon32.o
+vtune_drv-$(CONFIG_IA64) += vtlib64.o vtoshooks64.o vtxsys64.o
diff -urN linux.orig/drivers/char/vtune/sys_call_table.c linux/drivers/char/vtune/sys_call_table.c
--- linux.orig/drivers/char/vtune/sys_call_table.c	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/sys_call_table.c	2004-04-08 13:28:56.000000000 -0700
@@ -0,0 +1,96 @@
+/*
+ *  sys_call_table.c
+ *
+ *  Copyright (C) 2003-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: sys_call_table.c
+ *
+ *	Description: scan for sys_call_table symbol and return pointer to it
+ *	             if found (null otherwise)
+ *
+ *	Author(s): Juan Villacis, George Artz, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <asm/unistd.h>
+
+#include "vtdef.h"
+
+#ifdef USE_SYSCALLTABLE_ADDRESS
+/*
+ * use_sys_call_table_address() : returns address of sys_call_table obtained
+ *                                by one of the following commands:
+ *
+ *              "grep sys_call_table /boot/System.map-2.x.y"
+ *      or,
+ *              "nm /boot/vmlinux-2.x.y | grep sys_call_table"
+ *
+ * the address used below is expected to be of the form "0xNNNNNN"
+ */
+void *use_sys_call_table_address(void)
+{
+  void *sys_call_table = 0;
+  unsigned long address = USE_SYSCALLTABLE_ADDRESS;
+
+  if (address>0x1)
+    sys_call_table = (void *)address;
+
+  return (sys_call_table);
+}
+#endif /* USE_SYSCALLTABLE_ADDRESS */
+
+/*
+ * find_sys_call_table_symbol() : if not exported, then scan for sys_call_table
+ *                                symbol and returns pointer to it if found
+ *                                (NULL otherwise)
+ */
+void *find_sys_call_table_symbol(int verbose)
+{
+  void *sys_call_table = 0;
+
+#ifdef USE_SYSCALLTABLE_ADDRESS
+  if (! sys_call_table)
+    sys_call_table = use_sys_call_table_address();
+#endif
+
+  if (sys_call_table) 
+  {
+    if (verbose)
+      VDK_PRINT("sys_call_table symbol found at address 0x%lx\n",(long)sys_call_table);
+  }
+  else
+  {
+    if (verbose)
+      VDK_PRINT_WARNING("failed to find address of sys_call_table!\n");
+  }
+
+  return (sys_call_table);
+}
diff -urN linux.orig/drivers/char/vtune/vtasm32_64.h linux/drivers/char/vtune/vtasm32_64.h
--- linux.orig/drivers/char/vtune/vtasm32_64.h	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/vtasm32_64.h	2004-03-29 10:06:05.000000000 -0800
@@ -0,0 +1,105 @@
+/*
+ *  vtasm32_64.h
+ *
+ *  Copyright (C) 2003-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: vtasm32_64.h
+ *
+ *	Description: type definitions for Pentium(R) 4 processors
+ *	             with Intel(R) Extended Memory 64 Technology
+ *
+ *	Author(s): Charles Spirakis, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#if !defined(__VTUNE_ASM32_64_H__)
+#define __VTUNE_ASM32_64_H__
+
+//
+// Define the various HW structures. To do that, 
+// we need to avoid having the compiler add padding...
+//
+#pragma pack(push, 1)
+//
+// For the GDTR and IDTR regsiters
+//
+typedef struct _idtgdtDesc {
+    __u16 idtgdt_limit;
+    void *idtgdt_base;
+} IDTGDT_DESC;
+
+//
+// A "long" IDT entry...
+//
+typedef struct _longIdtEntry {
+    union {
+        __u64 lowQuad;
+        struct {
+            __u64 offsetLow : 16;     // Offset 15:00
+            __u64 selector : 16;      // segment selector value
+
+            __u64 ist : 3;            // interrupt stack switch
+            __u64 reserved0 : 5;      // reserved/ignored
+            __u64 type : 4;           // Type should == 0xe for int descriptor
+            __u64 sysUsr: 1;          // system/user should == 0 for int descriptor
+            __u64 dpl : 2;            // Dpl
+            __u64 pres : 1;           // present bit
+            __u64 offsetMid : 16;     // offset 31:16
+        };
+    };
+
+    union {
+        __u64 highQuad;
+        struct {
+            __u64 offsetHigh : 32;    // offset 63:32
+
+            __u64 reserved1 : 32;     // reserved
+        };
+    };
+} LONG_IDT_ENTRY;
+#pragma pack(pop)
+
+#define GDT_SEGMENT_SHIFT       3
+
+//
+// These functions were all written in assembly and make an assumption
+// regarding the calling convention... Spell out that assumption...
+//
+
+void ReadTsc(__u64 *pTsc);
+
+void GetIDTDesc(IDTGDT_DESC *pIdtDesc);
+void GetGDTDesc(IDTGDT_DESC *pGdtDesc);
+
+void ReadMsr(__u32 msr, __u64 *pResult);
+void WriteMsr(__u32 msr, __u64 value);
+
+__u8 __inbyte(__u32 port);
+void __outbyte(__u32 port, __u8 val);
+
+#endif // __VTUNE_ASM32_64_H__
diff -urN linux.orig/drivers/char/vtune/vtasm32.h linux/drivers/char/vtune/vtasm32.h
--- linux.orig/drivers/char/vtune/vtasm32.h	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/vtasm32.h	2004-03-29 10:06:05.000000000 -0800
@@ -0,0 +1,103 @@
+/*
+ *  vtasm32.h
+ *
+ *  Copyright (C) 2003-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: vtasm32.h
+ *
+ *	Description: type definitions for IA32 based platforms
+ *
+ *	Author(s): Charles Spirakis, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#if !defined(__VTUNE_ASM32_H__)
+#define __VTUNE_ASM32_H__
+
+//
+// Define the various HW structures. To do that, 
+// we need to avoid having the compiler add padding...
+//
+#pragma pack(push, 1)
+//
+// For the GDTR and IDTR regsiters
+//
+typedef struct _idtgdtDesc {
+    __u16 idtgdt_limit;
+    void *idtgdt_base;
+} IDTGDT_DESC;
+
+//
+// A "long" IDT entry...
+//
+typedef struct _IdtEntry {
+    union {
+        __u64 lowQuad;
+        struct {
+            __u64 offsetLow : 16;     // Offset 15:00
+            __u64 selector : 16;      // segment selector value
+
+            __u64 ist : 3;            // interrupt stack switch
+            __u64 reserved0 : 5;      // reserved/ignored
+            __u64 type : 4;           // Type should == 0xe for int descriptor
+            __u64 sysUsr: 1;          // system/user should == 0 for int descriptor
+            __u64 dpl : 2;            // Dpl
+            __u64 pres : 1;           // present bit
+            __u64 offsetMid : 16;     // offset 31:16
+        };
+    };
+} IDT_ENTRY;
+
+/* NOTE:  also defined in include/asm-x86_64/desc.h */
+struct gate_struct {
+    u16 a;
+    u16 b;
+    u16 c;
+    u16 d;
+} ;
+#pragma pack(pop)
+
+#define GDT_SEGMENT_SHIFT       3
+
+//
+// These functions were all written in assembly and make an assumption
+// regarding the calling convention... Spell out that assumption...
+//
+
+void ReadTsc(__u64 *pTsc);
+
+void GetIDTDesc(IDTGDT_DESC *pIdtDesc);
+void GetGDTDesc(IDTGDT_DESC *pGdtDesc);
+
+void ReadMsr(__u32 msr, __u64 *pResult);
+void WriteMsr(__u32 msr, __u64 value);
+
+__u8 __inbyte(__u32 port);
+void __outbyte(__u32 port, __u8 val);
+
+#endif // __VTUNE_ASM32_H__
diff -urN linux.orig/drivers/char/vtune/vtdef.h linux/drivers/char/vtune/vtdef.h
--- linux.orig/drivers/char/vtune/vtdef.h	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/vtdef.h	2004-05-11 05:04:43.496841735 -0700
@@ -0,0 +1,498 @@
+/*
+ *  vtdef.h
+ *
+ *  Copyright (C) 2002-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: vtdef.h
+ *
+ *	Description: 
+ *
+ *	Author(s): George Artz, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#if !defined(_VTUNE_DEF_H)
+#define _VTUNE_DEF_H
+
+#include "vtypes.h"
+#ifdef linux64
+#ifdef PERFMON_SUPPORTED
+#include <linux/interrupt.h>
+#include <asm/hw_irq.h>
+#endif // PERFMON_SUPPORTED
+#else
+#include "familyf_msr.h"
+#endif
+#ifdef KERNEL_26X
+#include <linux/timer.h>
+#endif
+
+#define SYS_VERSION_MAJOR               0
+#define SYS_VERSION_MINOR               921002
+
+#define  PCMINOR                        0   /* /dev/vtune minor number */
+#define  MDMINOR                        1   /* /dev/vtune_m */
+#define  PIDMINOR                       2   /* /dev/vtune_p */
+
+#define VFLAG_OFF                       0
+#define VFLAG_ON                        1
+#define SAMPMODE_MANUAL                 -1
+
+#define XHDRM_SAMPLE_QTY                2
+#define P6_FAMILY                       6
+
+#ifdef PERFMON_SUPPORTED
+#define VTUNE_PERFMON_IRQ               IA64_PERFMON_VECTOR
+#else
+#define VTUNE_PERFMON_IRQ               0xED
+#endif
+
+#define CPUFAMILY                       0xF00
+#define CPUMODEL                        0xF0
+
+#define emask_t unsigned long
+
+/*  NEW */
+#define APIC_PERFLVT_FIXED_ENA          0x00000 /* fixed mode; "should" be 0 according to Pentium(R) 4 processor manual (was 0x02000) */
+#define APIC_PERFLVT_INT_MASK           0x10000 /* mask perf interrupt 0=ints not masked, 1=perf ints masked */
+#define APIC_TIMERLVT_ENA               0x22000 /* local apic timer enable */
+
+#define APIC_BASE_MSR        0x1B   // apic base msr
+#define APIC_BASE_MSR_ENABLE 0x800  // apic enable/disable bit
+
+/*---------------------------------------------------------------*/
+/*
+ *         performance Monitor Model Specific registers
+ */
+/*---------------------------------------------------------------*/
+
+#define EM_MSR_TSC         0x10 /* Time Stamp Counter */
+#define EM_MSR_CESR        0x11 /* control and Event Select */
+#define EM_MSR_CTR0        0x12 /* Counter 0 */
+#define EM_MSR_CTR1        0x13 /* Counter 1 */
+/*** P6 Code ***/
+/** P6 MSR's **/
+#define EM_MSR_PESR0        0x186   /* performance Event Select */
+#define EM_MSR_PESR1        0x187   /* performance Event Select */
+#define EM_MSR_PCTR0        0x0C1   /* P6 Counter 0 */
+#define EM_MSR_PCTR1        0x0C2   /* P6 Counter 1 */
+
+/* 
+ *  PESR mask controls for Intel(R) Pentium(R) Pro, Pentium(R) II, 
+ *  and Pentium(R) III processors
+ *
+ *        33222222 2 2 2 2 1 1 1 1 11111100 00000000
+ *        10987654 3 2 1 0 9 8 7 6 54321098 76543210
+ *       +--------+-+-+-+-+-+-+-+-+--------+--------+
+ *       |        |I|E|r|I|P|E|O|U|        |        |
+ *       |  CMSK  |N|N|s|N|C| |S|S|  UMSK  |   ES   |
+ *       |        |V| |v|T| | | |E|        |        |
+ *       |        | | |d| | | | |R|        |        |
+ *       +--------+-+-+-+-+-+-+-+-+--------+--------+
+ */
+
+#define EM_PERF_DISABLE         0x0003FFFF
+#define EM_ES_MASK              0x000000FF
+#define EM_UMSK_MASK            0x0000FF00
+#define EM_USER_MASK            0x00010000
+#define EM_OS_MASK              0x00020000
+#define EM_E_MASK               0x00040000
+#define EM_PC_MASK              0x00080000
+#define EM_INT_MASK             0x00100000
+#define EM_EN_MASK              0x00400000
+#define EM_INV_MASK             0x00800000
+#define EM_CMSK_MASK            0xFF000000
+
+#ifdef linux64
+
+#define itp_read_reg_pmc(a)     ia64_get_pmc(a)
+#define itp_read_reg_pmd(a)     ia64_get_pmd(a)
+#define itp_read_reg_pmv()      itp_get_pmv()
+
+#define itp_write_reg_pmc(a, b) ia64_set_pmc(a, b)
+#define itp_write_reg_pmd(a, b) ia64_set_pmd(a, b)
+#ifdef KERNEL_26X
+#define itp_write_reg_pmv(a) \
+    asm volatile ("mov cr.pmv=%0" :: "r"(a) : "memory")
+#else
+#define itp_write_reg_pmv(a)    ia64_set_pmv(a)
+#endif
+
+#define itp_get_cpuid(a)        ia64_get_cpuid(a)
+#define itp_srlz_d()            ia64_srlz_d()
+
+#ifdef KERNEL_26X
+static inline __u64
+ia64_get_dcr (void)
+{
+	__u64 r;
+	asm volatile ("mov %0=cr.dcr" : "=r"(r));
+	return r;
+}
+
+static inline void
+ia64_set_dcr (__u64 val)
+{
+	asm volatile ("mov cr.dcr=%0;;" :: "r"(val) : "memory");
+	ia64_srlz_d();
+}
+#endif
+#define itp_get_dcr()           ia64_get_dcr()
+#define itp_set_dcr(a)          ia64_set_dcr(a)
+
+static __inline__ unsigned long
+itp_get_itc(void)
+{
+    unsigned long result;
+
+    __asm__ __volatile__("mov %0=ar.itc":"=r"(result)::"memory");
+    return result;
+}
+
+static inline __u64
+itp_get_pmv(void)
+{
+    __u64 r;
+      __asm__("mov %0=cr.pmv":"=r"(r));
+    return r;
+}
+
+//
+// GDT entry
+//
+
+typedef struct _KGDTENTRY {
+    __u16 limit_low;
+    __u16 base_low;
+    union {
+        struct {
+            char base_mid;
+            char flags1;    // Declare as bytes to avoid alignment
+            char flags2;    // Problems.
+            char base_hi;
+        } bytes;
+        struct {
+            __u32 base_mid:8;
+            __u32 type:5;
+            __u32 dpl:2;
+            __u32 pres:1;
+            __u32 limit_hi:4;
+            __u32 sys:1;
+            __u32 reserved_0:1;
+            __u32 default_big:1;
+            __u32 granularity:1;
+            __u32 base_hi:8;
+        } bits;
+    } high_word;
+} KGDTENTRY, *PKGDTENTRY;
+
+#define TYPE_TSS    0x01    // 01001 = NonBusy TSS
+#define TYPE_LDT    0x02    // 00010 = LDT
+
+//
+// UnScrambled Descriptor format
+//
+typedef struct _KDESCRIPTOR_UNSCRAM {
+    union {
+        __u64 descriptor_words;
+        struct {
+            __u64 base:32;
+            __u64 limit:20;
+            __u64 type:5;
+            __u64 dpl:2;
+            __u64 pres:1;
+            __u64 sys:1;
+            __u64 reserved_0:1;
+            __u64 default_big:1;
+            __u64 granularity:1;
+        } bits;
+    } words;
+} KXDESCRIPTOR, *PKXDESCRIPTOR;
+
+#define TYPE_CODE_USER                0x1A  // 0x11011 = Code, Readable, Accessed
+#define TYPE_DATA_USER                0x13  // 0x10011 = Data, Readwrite, Accessed
+
+#define DESCRIPTOR_EXPAND_DOWN        0x14
+#define DESCRIPTOR_DATA_READWRITE     (0x8|0x2) // Data, Read/write
+
+#define DPL_USER    3
+#define DPL_SYSTEM  0
+
+#define GRAN___u8   0
+#define GRAN_PAGE   1
+
+#define SELECTOR_TABLE_INDEX 0x04
+
+#endif  // linux64
+
+
+// Defines for Linux* OS list functions
+#define LIST_ENTRY   struct list_head
+#define PLIST_ENTRY  LIST_ENTRY *
+
+#define non_paged_pool 0
+
+/*
+ * ===========================================================================
+ *      5-04-01 - TVK (New Model Port ) 
+ * ===========================================================================
+ */
+
+#define STATUS_SUCCESS           0
+#define STATUS_INVALID_PARAMETER -EINVAL
+#define STATUS_NOT_SUPPORTED     -ENOSYS
+#define STATUS_DEVICE_BUSY       -EBUSY
+
+#define MAX_ACTIVE_EVENTS        32
+
+#define MAX_PROCESSORS           64
+
+#define MAX_REG_SET_ENTRIES      120
+
+#define IA32_FAMILY5_MAX_COUNTER 0x0ffffffffff  // max counter on P5... cpu counters are 40 bits
+#define IA32_FAMILY6_MAX_COUNTER 0x0ffffffffff  // max counter on P6... cpu counters are 40 bits
+#define IA32_FAMILYF_MAX_COUNTER 0x0ffffffffff  // max counter on Pentium(R) 4 processor ... cpu counters are 40 bits
+
+typedef struct _REG_SET {
+    __u8 options;
+    __u8 pmc_num;
+    union {
+        __u8 c_index;   // if this is a CCCR, then ctrindex is the
+        // index for the associated counter... 
+        // reg_setx[ctrindex] is associated counter
+
+        __u8 event_I_dindex;    // index of this event in the event ID table
+    };
+
+    __u8 event_ID;      // event ID passed to SampConfigure routine
+    // in event_reg_set_ex structure
+    __u32 reg_num;
+
+    __u32 c_ovf[MAX_PROCESSORS];    // counter overflow status for
+    // each cpu... non zero = xounter
+    // has overflowed on that cpu
+    ULARGE_INTEGER reg_val;
+
+    ULARGE_INTEGER event_inc;   // value added to event_total on counter
+    // overflow interrupt      03-30-01
+
+    ULARGE_INTEGER event_total[MAX_PROCESSORS]; // event total from start of
+    // sampling session   03-30-01
+} REG_SET, *PREG_SET;
+
+// Defines for REG_SET options field
+#define REG_SET_CCCR            0x01    // this entry is a CCCR
+#define REG_SET_COUNTER         0x02    // this entry is a counter
+#define REG_SET_CINDEX_VALID    0x04    // indexToCtr is set
+
+// Defines for bits of sample method                              09-10-00
+#define METHOD_VTD     0x01
+#define METHOD_RTC     0x02
+#define METHOD_EBS     0x04
+
+/* User allocation req's for SP structs */
+typedef struct {
+    unsigned int size_user_Sp3;
+    unsigned int strlen_raw_file_name_Sp3;
+    unsigned int strlen_mod_info_file_name_Sp3;
+} samp_user_config_stat_t;
+
+#define ITP_CPUID_REG3_FAMILY   0xFF000000
+#define ITP_CPUID_REG3_MODEL   0xFF0000
+#define ITP_CPU_FAMILY_ITANIUM  7
+#define ITP_CPU_FAMILY_ITANIUM2 0x1F
+
+//
+// These correspond to the slot for the bundle that was interrupted
+// OS/Apps are supposed to look here for slot info, but often the assumption
+// is that the slot is the bottom 2 bits of the IIP... Sometimes driver updates
+// the IIP to have that. Sometimes it doesn't. We're pretty good about keeping
+// the RI bits accurate though...
+//
+#define ITP_IPSR_RI_SHIFT           41
+#define ITP_IPSR_RI_MASK            (((__u64) 3) << ITP_IPSR_RI_SHIFT)
+
+#define ITANIUM_INSTRUCTION_EAR_PMC 10
+#define ITANIUM_DATA_EAR_PMC        11
+#define ITANIUM_BRANCH_TRACE_BUFFER_PMC 10
+#define ITANIUM_MAXCOUNTER          0xFFFFFFFF
+#define ITANIUM_MAX_CONFIG_PMC      13
+#define ITANIUM_PMD17_SLOT_MASK     0xC
+#define ITANIUM_PMD17_SLOT_SHIFT    2
+#define ITANIUM_PMD17_BUNDLE_MASK   0
+#define ITANIUM_PMC_ES_MASK         0x7F00
+#define ITANIUM_PMC_ES_SHIFT        8
+#define ITANIUM_DEAR_EVENT_CODE     0x67
+#define ITANIUM_IEAR_EVENT_CODE     0x23
+#define ITANIUM_BTRACE_EVENT_CODE   0x11
+
+#define ITANIUM2_MAXCOUNTER         0x7FFFFFFFFFFF
+#define ITANIUM2_MAX_CONFIG_PMC     15
+#define ITANIUM2_PMD17_SLOT_MASK    0x3
+#define ITANIUM2_PMD17_SLOT_SHIFT   0
+#define ITANIUM2_PMD17_BUNDLE_MASK  0x4
+#define ITANIUM2_PMC_ES_MASK        0xFF00
+#define ITANIUM2_PMC_ES_SHIFT       8
+#define ITANIUM2_DEAR_EVENT_CODE    0xC8
+#define ITANIUM2_IEAR_EVENT_CODE    0x43
+#define ITANIUM2_BTRACE_EVENT_CODE  0x11
+
+#define ITANIUM2_PMC12_DS_MASK      (1 << 7)
+
+#define ITANIUM2_PMD16_BBI_MASK     0x07
+#define ITANIUM2_PMD16_FULL_MASK    0x08
+
+#define ITANIUM2_BTB_SLOT_SHIFT     2
+#define ITANIUM2_BTB_SLOT_MASK      (3 << ITANIUM2_BTB_SLOT_SHIFT)
+
+#define ITANIUM2_BTB_MP_MASK        0x02
+#define ITANIUM2_BTB_B_MASK         0x01
+
+#define ITANIUM2_NUM_BTBREGS        8
+
+#define PMC0_FREEZE   1
+#define PMC0_OFLOW0   0x10  // perf counter 4 overflow status bit
+#define PMC0_OFLOW1   0x20  // perf counter 5 overflow status bit
+#define PMC0_OFLOW2   0x40  // perf counter 6 overflow status bit
+#define PMC0_OFLOW3   0x80  // perf counter 7 overflow status bit
+#define PMC0_OFLOWALL (PMC0_OFLOW0 | PMC0_OFLOW1 | PMC0_OFLOW2 | PMC0_OFLOW3)   // perf counter 4-7 overflow status bit
+
+#define PMV_MASK_BIT 0x10000    // pmv.m  0 = unmask counter overflow interrrupts
+                //                        1 = mask counter overflow interrupts
+                // Note: 
+                // Counter overflow interrupt is edge triggered so
+                // is an counter overflow interrupt signal is generated while
+                // pmv.m is set, then the interrupt is lost
+
+#define PMC4_PLM  0x0F      // privilege level mask
+#define PMC4_OI   0x20      // overflow interrupt 0 = no interrupt, 1 = generate counter overflow interrupt
+
+// TBS global data
+
+/* CPU information (originally in include/sampfile.h) */
+
+// in the future, we should store the multiple register values returned from 
+//  an EAX input of 2 when trying to determine the L2 cache size of a processor
+
+// added by Bob Knight, 08-17-2001
+typedef struct cpu_information_s {
+  __u32 ul_cpu_type_from_shell;   // the CTF enum from the shell
+  __u32 ul_num_EAX_inputs;    // the highest number of EAX inputs supported by the cpu (determined by setting EAX to 0 and executing cpuid AND by execuing the cpuid instruction with an input of 2 to discover how many times to execute cpuid with an input of 2 to get the L2 cache size)
+  __u32 ul_num_EAX_extended_inputs;   // the highest number of extended EAX inputs supported by the cpu (determined by setting EAX to 0x80000000 and executing cpuid)
+  __u32 ul_num_cpus_available;   // the maximum number of physical cpus (not packages) available to be used by the OS (this number may be less than the number of physical logical cpus installed on the system if the system is configured to use less than the total number of available physical cpus)
+  __u32 ul_cpu_speed_in_M_hz[MAX_PROCESSORS]; // raw speed of the cpu in MHz (may not be politically correct speed advertised to users - e.g. a 299 MHz cpu would be called a 300MHz cpu publically)
+  __u32 ul_cpul2_cache_size[MAX_PROCESSORS];  // in Kilobytes
+  __u32 ul_offset_to_cpu_map_array;   // offset from the beginning of this structure to an array of ul_num_CP_us_available cpu_map structures
+  __u32 ul_offset_to_cpuid_output_array;  // offset from the beginning of this structure to an array of ul_num_CP_us_available*(ul_num_EAX_inputs+ul_num_EAX_extended_inputs) cpuid_output structures
+  __u32 ul_reserved[8];
+} cpu_information;
+
+/* Sampling system info (originally in include/vtuneshared.h) */
+typedef struct _sys_samp_info {
+  int cpu_I_dmap[MAX_PROCESSORS];  // CPU ID info for all  processors
+  int cpu_feature_bits;       // feature bits, global for all now
+  int num_processors;         // Total CPUs detected
+  int cpu_speed;              // CPU Speed
+  int lib_version_major;      // Sampling library Version Major
+  int lib_version_minor;      // Sampling library Version Minor
+  int sysdrv_version_major;   // Sampling driver Version Major
+  int sysdrv_version_minor;   // Sampling driver Version Minor
+  int feature_set;            // features supported by CPU and OS
+  int kernel_cs;              // kernel selector (see __KERNEL_CS in asm-i386/segment.h)
+  int reservedC;              // Reserved Field C (TBD)
+  int reservedD;              // Reserved Field D (TBD)
+  int reservedE;              // Reserved Field E (TBD)
+} sys_samp_info;
+
+typedef struct _PER_CPU {
+  __u32 processor_status;
+  __u32 processor_EBS_status;
+  BOOLEAN start_all;
+#ifdef linux64
+  __u64 original_apic_perf_local_vector;
+#else
+  __u32 unused_ntprocessorcontrolregister;
+  __u32 original_apic_perf_local_vector;
+#ifdef linux32
+  __u64 original_EBS_idt_entry;
+  PDTS_BUFFER DTES_buf;
+#elif defined(linux32_64)
+  void *original_EBS_idt_entry;
+  PIA32EM64T_DTS_BUFFER DTES_buf;
+#else
+#error Unknown architecture
+#endif
+  BOOLEAN resume_sampling;
+  void *samp_EBS_idt_routine;
+  ULARGE_INTEGER org_dbg_ctl_msr_val;
+  ULARGE_INTEGER org_DTES_area_msr_val;
+  PULARGE_INTEGER p_emon_reg_save_area; // pointer to reg save area per cpu.        05-31-00
+                                        // used to save/restore emon regs across
+                                        // an ebs session
+  __u32 unused_emonints;
+#endif
+} PER_CPU, *PPER_CPU;
+
+extern PER_CPU eachCPU[MAX_PROCESSORS];
+
+#if defined(linux32) || defined(linux32_64)
+extern __u32 package_status[MAX_PROCESSORS + 1];   // add 1 since packageNumber[] is
+                                            // indexed by the package number
+                                            // which is relative to 1
+#endif
+
+#if defined(DEBUG)
+#define VDK_PRINT_DEBUG(fmt,args...) { printk(KERN_INFO "VSD: [DEBUG] " fmt,##args); }
+#else
+#define VDK_PRINT_DEBUG(fmt,args...) {;}
+#endif
+
+#define VDK_PRINT(fmt,args...) { printk(KERN_INFO "VSD: " fmt,##args); }
+
+#define VDK_PRINT_WARNING(fmt,args...) { printk(KERN_INFO "VSD: [Warning] " fmt,##args); }
+
+#define VDK_PRINT_ERROR(fmt,args...) { printk(KERN_INFO "VSD: [ERROR] " fmt,##args); }
+
+#define VDK_PRINT_TITLE(t,l) VDK_PRINT("VTune(TM) Performance Analyzer %s sampling driver v%d.%d has been %s.\n", t, SYS_VERSION_MAJOR, SYS_VERSION_MINOR, l)
+
+#ifdef CONFIG_SMP
+#define VDK_PRINT_BANNER(l) VDK_PRINT_TITLE("SMP",l)
+#else
+#define VDK_PRINT_BANNER(l) VDK_PRINT_TITLE("UP",l)
+#endif
+
+//
+// Depending on how the task_struct is defined in .../include/linux/sched.h
+//
+#if defined(USE_PARENT_TASK_FIELD)
+#define VT_GET_PARENT(x)  ((x)->parent)
+#else	
+#define VT_GET_PARENT(x)  ((x)->p_opptr)
+#endif	
+
+#endif /* _VTUNE_DEF_H */
diff -urN linux.orig/drivers/char/vtune/vtextern.h linux/drivers/char/vtune/vtextern.h
--- linux.orig/drivers/char/vtune/vtextern.h	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/vtextern.h	2004-05-09 22:26:03.000000000 -0700
@@ -0,0 +1,113 @@
+/*
+ *  vtextern.h
+ *
+ *  Copyright (C) 2002-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: vtextern.h
+ *
+ *	Description: extern declarations
+ *
+ *	Author(s): George Artz, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#if !defined(_VTUNE_EXTERN_H)
+#define _VTUNE_EXTERN_H
+
+extern void *apic_local_addr;
+extern BOOLEAN DTES_supported;
+extern sys_samp_info vtune_sys_info;
+extern samp_parm6 samp_parms;
+extern sampinfo_t samp_info;
+extern __u32 apic_perf_lvt;
+
+extern driver_shared_area *pdsa;    /* DSA ptr */
+#ifdef DSA_SUPPORT_MMAP
+extern driver_shared_area *create_dsa(void);
+extern int destroy_dsa(void);
+#endif
+
+/* PC Sample record buf */
+extern void *buf_start;         /* start of sample buffer */
+extern void *buf_end;           /* end of sample buffer */
+extern __u32 buf_length;        /* size of sampling buffer */
+extern void *p_sample_buf;      /* ptr to next sample in sample buf */
+
+extern int g_max_samp_timer_ON;     /* Global max Timer status */
+extern int g_start_delay_timer_ON;  /* Global Start delay Timer status */
+extern BOOLEAN signal_thread_event;   /* flag to signal sampler thread */
+extern int num_mod_rec;
+extern __u32 ebs_irq;
+
+extern struct timer_list delay_tmr; /* user specifies delay time */
+extern struct timer_list time_out_tmr;  /* max sampling time */
+extern spinlock_t sample_int_lock;
+extern spinlock_t sample_exec_lock;
+extern spinlock_t reg3f1_write_lock;
+extern spinlock_t rdpmc_lock;
+
+extern __u32 sample_max_samples;
+extern __u32 sample_version;
+extern __u32 sample_method;
+extern __u32 sample_tsc;
+extern __u32 sample_tsc_offset;
+extern __u32 sample_rec_length;
+
+extern unsigned long start_time;
+extern unsigned long sample_delay;
+extern unsigned long sample_interval;
+
+extern unsigned long Samples_Per_Buf;
+extern unsigned long sample_rate_us;
+extern unsigned long sample_rate_ms;
+extern unsigned long sample_rate;
+
+extern RDPMC_BUF rdpmc_buf;
+extern int rdpmc_msr_base;
+extern BOOLEAN pebs_option;
+extern __u32 reset_and_power_on_pubs;   
+extern ULARGE_INTEGER max_counter;
+extern __u32 g_CPU_family;
+extern __u32 g_CPU_model;
+
+extern REG_SET reg_set[MAX_REG_SET_ENTRIES];
+extern REG_SET reg_set0[MAX_REG_SET_ENTRIES];
+extern REG_SET reg_set1[MAX_REG_SET_ENTRIES];
+extern REG_SET reg_set_init[MAX_REG_SET_ENTRIES];
+extern void *samp_EBS_idt_routine[MAX_PROCESSORS];
+
+extern int event_count;
+extern char event_Ids[MAX_ACTIVE_EVENTS];
+
+extern BOOLEAN IA32_family5;
+extern BOOLEAN IA32_family6;
+extern BOOLEAN IA32_familyF;
+
+#define PSTATUS_EMON_REGS_SAVED 2   
+
+#endif /* _VTUNE_EXTERN_H */
diff -urN linux.orig/drivers/char/vtune/vtglobal.h linux/drivers/char/vtune/vtglobal.h
--- linux.orig/drivers/char/vtune/vtglobal.h	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/vtglobal.h	2004-04-21 14:44:23.000000000 -0700
@@ -0,0 +1,235 @@
+/*
+ *  vtglobal.h
+ *
+ *  Copyright (C) 2002-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: vtglobal.h
+ *
+ *	Description: globally used/useful variables, constants
+ *
+ *	Author(s): George Artz, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#if !defined(_VTUNE_GLOBAL_H)
+#define _VTUNE_GLOBAL_H
+
+#include <asm/hw_irq.h>
+
+#include "vtypes.h"
+#include "familyf_msr.h"
+
+int vtune_major = 0;            /* Major Device ID  stored in /dev */
+
+unsigned int glob_sample_record_size = sizeof (sample_record_PC);
+
+#if defined(linux32)
+void *apic_local_addr;
+unsigned long apic_paddr, apic_physical_addr;
+unsigned long apic_timer_lvt;       /* local apic timer LVT (0x320 on P6) */
+unsigned long current_apic_perf_lvt;    /* current local apic performance LVT (0x340 on P6) */
+#endif
+
+#if defined(linux32) || defined(linux32_64)
+long apic_perf_lvt;         /* local apic performance LVT (0x340 on P6) */ 
+#endif
+
+#define PSTATUS_EMON_REGS_SAVED 2   // emon regs have been saved
+
+#define LINUX32_APP_BASE_LOW   0x08000000
+#define LINUX32_APP_BASE_HIGH  0x40000000
+
+#define LINUX64_APP_BASE_LOW   0x4000000000000000
+#define LINUX64_APP_BASE_HIGH  0x6000000000000000
+
+__u32_PTR app_base_low;     // these fields are used to determine if a loaded module is an "exe" (primary binary for a process)
+__u32_PTR app_base_high;        // ..
+
+short my_cs, my_ds;
+unsigned long *my_idt;
+
+int p6_def_pesr = 0x530000;
+int local_timer_icount = 0x50d4f;
+
+char intelid[] = "Genuineintel";    /* intel processor ID */
+unsigned long g_this_CPUID = 0;
+unsigned long g_this_CPU_features = 0;
+__u32 g_CPU_family = 0;
+__u32 g_CPU_model = 0;
+
+#ifdef USE_TYPE_WAIT_QUEUE_OLD
+struct wait_queue *pc_write;        /* pc buffer full write event */
+#endif
+
+#ifdef USE_TYPE_WAIT_QUEUE_NEW
+static DECLARE_WAIT_QUEUE_HEAD(pc_write);
+static DECLARE_WAIT_QUEUE_HEAD(samp_delay);
+#endif
+
+/* sample file beginning size, 8 entries in sample_file_sections */
+#define sfb_len  ((sizeof(sample_file_beginning)  + (7 * sizeof(sample_file_sections))) + 15) & ~15
+
+spinlock_t sample_exec_lock = SPIN_LOCK_UNLOCKED;
+spinlock_t sample_int_lock = SPIN_LOCK_UNLOCKED;
+spinlock_t reg3f1_write_lock = SPIN_LOCK_UNLOCKED;
+spinlock_t rdpmc_lock = SPIN_LOCK_UNLOCKED;
+
+/* pc sample recored buf */
+void *buf_start = NULL;     /* start of sample buffer */
+void *buf_end = NULL;       /* end of sample buffer */
+unsigned long buf_length;   /* size of sampling buffer */
+void *p_sample_buf;     /* ptr to next sample in sample buf */
+
+unsigned long current_buffer_count = 0; /* Current Sample buffer Count */
+int continuous_sampling = 0;    /* flag for non-stop sampling mode */
+driver_shared_area *pdsa = 0;   /* pointer to driver_shared_area */
+
+__u32 sample_rec_length;    // length of sample record
+__u32 sample_tsc;           // record cpu time stamp with sample
+__u32 sample_tsc_offset;    // offset to tsc in sample record
+__u32 sample_rec_length;    // length ofr sample record
+
+unsigned long start_time = 0;   /* time sampling started */
+unsigned long sample_delay = 0; /* sample start delay */
+unsigned long sample_interval = 0;  /* total time to sample */
+
+//unsigned long sample_method = 0;      /* sampling method */
+unsigned long Samples_Per_Buf = 0;  /* samples per buffer */
+unsigned long sample_rate_us = 0;   /* sample rate in microseconds */
+unsigned long sample_rate_ms = 0;   /* sample rate in milliseconds */
+unsigned long sample_rate = 0;      /* sample rate in clock ticks */
+
+int num_mod_rec = 0;
+int num_pid_create_rec = 0;
+unsigned long current_sample_loop_count;
+
+struct timer_list delay_tmr;        /* user specifies delay time */
+struct timer_list time_out_tmr;     /* max sampling time */
+
+int g_max_samp_timer_ON = B_FALSE;  /* Global max Timer status */
+int g_start_delay_timer_ON = B_FALSE;   /* Global Start delay Timer status */
+
+BOOLEAN signal_thread_event;  /* flag to signal sampler thread */
+BOOLEAN terminate_thread;
+
+sys_samp_info vtune_sys_info;   /* system level info */
+sampinfo_t samp_info;       /* sample information area */
+samp_parm6 samp_parms;      /* sample parameters */
+
+module_record *mrlist;
+module_record *last = (module_record *) 0;
+
+static int file_count = 0;
+
+#ifdef USE_TYPE_FILE_OPS_NEW
+static struct file_operations vtune_fops = {
+    read:vtune_read,
+    write:vtune_write,
+    open:vtune_open,
+    ioctl:vtune_ioctl,
+    release:vtune_release,
+#ifdef DSA_SUPPORT_MMAP
+    mmap:vtune_mmap,
+#endif // DSA_SUPPORT_MMAP
+};
+#endif
+
+#ifdef USE_TYPE_FILE_OPS_OLD
+struct file_operations vtune_fops = {
+    NULL,           /* lseek */
+    vtune_read,     /* read */
+    vtune_write,    /* write */
+    NULL,           /* readdir */
+    NULL,           /* poll */
+    vtune_ioctl,    /* ioctl */
+#ifdef DSA_SUPPORT_MMAP
+    vtune_mmap,     /* mmap */
+#else
+    NULL,           /* mmap */
+#endif // DSA_SUPPORT_MMAP
+    vtune_open,     /* open */
+    NULL,           /* flush */
+    vtune_release,  /* release */
+    NULL,           /* fsync */
+    NULL,           /* sfsync */
+    NULL,           /* check_media_change */
+    NULL,           /* revalidate */
+    NULL,           /* lock */
+};
+#endif
+
+__u32 ebs_irq = 0;
+
+/*
+==========================================================================================
+    5-04-01 - TVK (New Model Port ) 
+==========================================================================================
+*/
+
+samp_user_config_stat_t samp_user_config_stat;
+
+//
+// Sampling method. Multiple methods can be active
+// for a sampling session
+//
+__u32 sample_method = 0;        // sample method
+
+void *sample_ptr;       // sample buf pointer
+
+__u32 sample_max_samples;       // maximum samples to collect
+int sample_version;     // sample parms version
+                // ..for backward compatibility
+ULARGE_INTEGER max_counter;
+
+/* Note: These registers are written once on each OS cpu at the
+         start of sampling. They are not written at during the
+         EBS counter overflow ISR 
+*/
+
+REG_SET reg_set[MAX_REG_SET_ENTRIES];   // registers for configure
+REG_SET reg_set0[MAX_REG_SET_ENTRIES];  // registers for Pentium(R) 4 processor thread 0
+REG_SET reg_set1[MAX_REG_SET_ENTRIES];  // registers for Pentium(R) 4 processor thread 1
+REG_SET reg_set_init[MAX_REG_SET_ENTRIES];  // registers for all cpu's
+
+int event_count;            // number of events in event_Ids array
+char event_Ids[MAX_ACTIVE_EVENTS];  // event IDs for active for sampling session
+
+BOOLEAN IA32_family5 = B_FALSE;   // 
+BOOLEAN IA32_family6 = B_FALSE;   // 
+BOOLEAN IA32_familyF = B_FALSE;   // 
+
+__u32 rdpmc_msr_base;               // msr of 1st cpu perf counter (IA32)
+
+BOOLEAN DTES_supported = FALSE;     // CPU supports DTES feature
+BOOLEAN HT_supported = FALSE;       // CPU supports Hyper-Threading Technology
+BOOLEAN pebs_option = B_FALSE;    // precise event sampling active
+__u32 pebs_err;                     // error during precise EBS ISR
+__u32 logical_processors_per_package;   // Pentium(R) 4 processor: logical processors per phycal processor
+__u32 logical_processors_shift;     
+
+#endif /* _VTUNE_GLOBAL_H */
diff -urN linux.orig/drivers/char/vtune/vtlib32_64.c linux/drivers/char/vtune/vtlib32_64.c
--- linux.orig/drivers/char/vtune/vtlib32_64.c	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/vtlib32_64.c	2004-04-29 14:11:27.000000000 -0700
@@ -0,0 +1,724 @@
+/*
+ *  vtlib32_64.c
+ *
+ *  Copyright (C) 2002-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: vtlib32_64.c
+ *
+ *	Description: functions for manipulating hardware registers on
+ *	             Pentium(R) 4 processors with 
+ *                   Intel(R) Extended Memory 64 Technology
+ *
+ *	Author(s): George Artz, Intel Corp.
+ *	           Charles Spirakis, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#include <asm/uaccess.h>
+#include <linux/vmalloc.h>
+#include <linux/slab.h>     /*malloc */
+
+#include "vtuneshared.h"
+
+#include "apic.h"
+#include "vtdef.h"
+#include "vtproto.h"
+#include "vtextern.h"
+#include <asm/io.h>
+#include <asm/msr.h>
+#include <asm/desc.h>
+#include <asm/segment.h>
+
+#include "vtasm32_64.h"
+
+#include "vtlibcommon32.h"
+
+//
+// Seems like these should be in OS include files somewhere, but
+// didn't see them...
+//
+#define IS_LDT_BIT      0x4
+#define SEGMENT_SHIFT   3
+
+#define EFLAGS_V86_MASK       0x00020000L
+
+
+#if defined(ALLOW_LBRS)
+extern BOOLEAN	capture_lbrs;	// should we capture LBRs on this run?
+#endif
+
+extern BOOLEAN pebs_option; // precise event sampling active    05-31-00
+extern __u32 pebs_err;      // error during precise EBS ISR     05-31-00
+
+
+/*++
+Routine description:
+
+  Arguments:
+
+return value:
+    TRUE... DTES buffer setup for PEBS
+
+--*/
+PIA32EM64T_DTS_BUFFER
+PEBS_alloc_DTES_buf(PIA32EM64T_DTS_BUFFER p_DTES_buf)
+{
+    __u32 record_size;
+    __u32 buf_size;
+    __u64 PEBS_base;
+    PIA32EM64T_DTS_BUFFER p_DTS_buffer;
+
+    //
+    // event though we only want one PEBS record... need 2 records so that
+    // threshold can be less than absolute max
+    //
+    buf_size = sizeof (IA32EM64T_DTS_BUFFER) + (sizeof(IA32EM64T_PEBS_RECORD)* 2);
+
+    buf_size += 32;     // allow for allingment of PEBS base to cache line
+
+    if (p_DTES_buf) {
+        p_DTS_buffer = p_DTES_buf;
+    } else {
+        p_DTS_buffer = allocate_pool(non_paged_pool, buf_size);
+    }
+
+    if (!p_DTS_buffer) {
+        return p_DTS_buffer;
+    }
+
+    memset(p_DTS_buffer, 0, buf_size);
+
+    //
+    // The PEBS will start after the DTS area
+    //
+    PEBS_base = ((__u64) p_DTS_buffer) + sizeof(IA32EM64T_DTS_BUFFER);
+
+    //
+    // Force 32 byte alignment here
+    //
+    PEBS_base = (PEBS_base + 31) & ~((__u64) 0x1f);
+
+
+    record_size = sizeof(IA32EM64T_PEBS_RECORD);
+
+    //
+    // Program the DTES buffer for precise EBS.
+    // Set PEBS buffer for one PEBS record...
+    //
+
+    p_DTS_buffer->base = 0;
+    p_DTS_buffer->index = 0;
+    p_DTS_buffer->max = 0;
+    p_DTS_buffer->threshold = 0;
+    p_DTS_buffer->PEBS_base = PEBS_base;
+    p_DTS_buffer->PEBS_index = p_DTS_buffer->PEBS_base;
+    p_DTS_buffer->PEBS_max = p_DTS_buffer->PEBS_base + (record_size * 2);
+    p_DTS_buffer->PEBS_threshold = p_DTS_buffer->PEBS_max - record_size;
+
+    VDK_PRINT_DEBUG("DTES buf allocated for precise EBS. DTES buf %p\n", p_DTS_buffer);
+
+    return (p_DTS_buffer);
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void samp_build_csip_sample(PINT_FRAME int_frame, P_sample_record_PC p_sample)
+ * @brief       Actualy files the sample buffer with values
+ *
+ * @param       int_frame IN  - small interrupt frame with minimal values
+ * @param       p_sample  OUT - Buffer to be filled
+ *
+ * @return	none
+ *
+
+ * This routine actually fills in the sample record. In the case of PEBS,
+ * it can also grab the PEBS entry and use that in place of the eip. This
+ * routine also samples the LBRs if requested and appends them to the
+ * end of the sample record.
+ *
+ * <I>Special Notes:</I>
+ *
+ *	The MSR's to use to get the LBRs is hard coded. Works for current
+ * Pentium(R) 4 processor, but likely to be a problem in the future..
+ *
+ * @todo Update the LBR code to use a lookup array instead of hardcoded MSRs
+ *
+ */
+void
+samp_build_csip_sample(PINT_FRAME int_frame, P_sample_record_PC p_sample)
+{
+    ULARGE_INTEGER *nextOpenData;
+    __u32 is_64bit_addr;
+    __u8 tmp;
+
+    p_sample->cpu_and_OS = 0;   // init bit fields
+    p_sample->cpu_num = (__u16) smp_processor_id();
+    p_sample->bit_fields2 = 0;
+    p_sample->pid_rec_index_raw = 1;    // pid contains OS pid
+    p_sample->cs = (__u16) int_frame->seg_cs;
+
+    get_CSD(p_sample->cs, &p_sample->csd.low_word, &p_sample->csd.high_word);
+
+    //save off the dpl
+    tmp =p_sample->csd.dpl;
+    //
+    // The reserved_0 bit == 1 means we have a 64-bit descriptor
+    // based on the cs of the trap frame we just received...
+    //
+    is_64bit_addr = (1 == p_sample->csd.reserved_0);
+
+    if (is_64bit_addr) {
+        p_sample->iip = int_frame->iip;
+        p_sample->ipsr = int_frame->ipsr;
+	//BSBS this is a complete hack.  we are sticking the cpl in bits
+	//32 and 33 of the ipsr because the user mode code is expecting it
+	//there and currently RFLAGS(extended EFLAGS) is only 32 bits (upper
+	//32 bits all set to 0)
+	p_sample->ipsr.high_part =  tmp;
+        p_sample->itp_pc = TRUE;
+    }
+    else {
+        p_sample->eip = (__u32) (int_frame->iip.quad_part & 0xffffffff);
+        p_sample->eflags = (__u32) (int_frame->ipsr.quad_part & 0xffffffff);
+        p_sample->itp_pc = FALSE;
+
+        //
+        // CSS: TODO: Do we care about V86 mode? If so, do we need to do
+        // more here?  My initial reading of the manual says V86 mode isn't
+        // supported under "long" os'es... Is that true? Need to verify...
+        //
+        if (p_sample->eflags & EFLAGS_V86_MASK) {
+            VDK_PRINT_ERROR("Unexpected V86 mode sample captured!\n");
+        }
+    }
+
+#ifdef ENABLE_TGID
+    p_sample->pid_rec_index = get_thread_group_id(current);
+#else
+    p_sample->pid_rec_index = current->pid;
+#endif
+
+    p_sample->tid = current->pid;
+
+    //
+    // If precise EBS, overwrite the eflags and ip usig the ones from DTES buf.
+    //
+    // NOTE:
+    // There is no indication in the PEBS buffer of the CS used
+    // at the time of the sample. So, we need to assume that the PEBS
+    // sample and the current interrupt frame are "close" in time -
+    // close enough, in fact, to use the CS from the current interrupt
+    // to determine things like 32-bit vs. 64-bit mode...
+    //
+    // If that assumption is false, the data can be misattributed (at
+    // least mode wise)
+    //
+    //
+    if (pebs_option) {
+        PIA32EM64T_DTS_BUFFER p_DTES;
+        PIA32EM64T_PEBS_RECORD p_rec;
+        __u32 cpu;
+
+        cpu = p_sample->cpu_num;
+        if (cpu < MAX_PROCESSORS) {
+            p_DTES = eachCPU[cpu].DTES_buf;
+            if (p_DTES) {
+                if (p_DTES->PEBS_index != p_DTES->PEBS_base) {
+                    p_rec = (PIA32EM64T_PEBS_RECORD) p_DTES->PEBS_base;
+                    //
+                    // Rest of 64/32 bit information should have
+                    // been done above when we were assuming we weren't
+                    // pebs...
+                    //
+                    if (is_64bit_addr) {
+                        p_sample->iip.quad_part = p_rec->linear_IP;
+                        p_sample->ipsr.quad_part = p_rec->E_flags;
+                    }
+                    else {
+                        p_sample->eip = (__u32) (p_rec->linear_IP & 0xffffffff);
+                        p_sample->eflags = (__u32) (p_rec->E_flags & 0xffffffff);
+                    }
+
+                    //
+                    // reset index to next PEBS record to base of buffer
+                    //
+                    p_DTES->PEBS_index = p_DTES->PEBS_base;
+                } else {
+                    pebs_err++;
+                }
+            } else {
+                pebs_err++;
+            }
+        } else {
+            pebs_err++;
+        }
+    }
+
+    nextOpenData = (ULARGE_INTEGER *) (((char *) p_sample) + sizeof(sample_record_PC));
+
+    if (sample_tsc) {
+#if defined(DEBUG)
+        if (((char *) nextOpenData) != ((char *) p_sample) + sample_tsc_offset) {
+            VDK_PRINT_ERROR("TSC offest has unexpected value (%d, %p, %p).\n", sample_tsc_offset, p_sample, nextOpenData);
+            return;
+        }
+	
+        if (sample_tsc_offset + sizeof(ULARGE_INTEGER) > sample_rec_length) {
+            VDK_PRINT_ERROR("TSC offest (%d) larger than record size (%d)\n", sample_tsc_offset, sample_rec_length);
+            return;
+        }
+#endif
+        rdtsc(nextOpenData->low_part, nextOpenData->high_part);
+        nextOpenData++;
+    }
+
+#if defined(ALLOW_LBRS)
+    if (capture_lbrs) {
+
+#if defined(DEBUG)
+	{
+	    // Makes the if statments below easier...
+	    long samp_start, samp_current;
+            long samp_available;
+	    long samp_needed;
+
+	    samp_start = (long) p_sample;
+	    samp_current = (long) nextOpenData;
+            samp_available = sample_rec_length - (samp_current - samp_start);
+
+	    // Need to make sure we have space for 4 LBRs plus a TOS indicator
+	    samp_needed = LBR_SAVE_SIZE;
+
+	    if (samp_available < samp_needed) {
+	        VDK_PRINT_ERROR("sample_rec_length (%d) not big enough to hold samples.\n",
+			sample_rec_length);
+	        VDK_PRINT_ERROR("p_sample: 0x%p next: 0x%p, available %ld needed %ld\n",
+			p_sample,
+			nextOpenData,
+			samp_available,
+			samp_needed);
+	        return;
+	    }
+	}
+#endif
+	//
+	// For now, the MSR to use is a constant, but it is not
+	// architectural so the HW people could change this.
+	// BEWARE!
+	// Longer term, these values should be in an array so
+	// you can choose what to grab (and how many) based on the
+	// processor.
+	//
+	*nextOpenData = samp_read_msr(MSR_LASTBRANCH_TOS);
+	nextOpenData++;
+
+	*nextOpenData = samp_read_msr(MSR_LASTBRANCH_0);
+	nextOpenData++;
+	*nextOpenData = samp_read_msr(MSR_LASTBRANCH_1);
+	nextOpenData++;
+	*nextOpenData = samp_read_msr(MSR_LASTBRANCH_2);
+	nextOpenData++;
+	*nextOpenData = samp_read_msr(MSR_LASTBRANCH_3);
+    }
+#endif    
+
+    VDK_PRINT_DEBUG("returning sample iip: %p, eflags %p, itp_pc: %d, cs: %x\n",
+                (void *) p_sample->iip.quad_part,
+                (void *) p_sample->ipsr.quad_part,
+                p_sample->itp_pc,
+                p_sample->cs);
+
+    return;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void get_idt_func(__u32 vect, void **func)
+ * @brief       Get the function currently associated with an idt vector
+ *
+ * @param       vect - the idt vector of interest
+ * @param       func - where the resulting function address goes
+ *
+ * @return	none
+ *
+ * Done this way in case we want to capture the full vector (16-bytes) instead
+ * of just the function at some point in the future...
+ *
+ * <I>Special Notes:</I>
+ *
+ *      Since we are only reading the idt info, don't bother
+ *      blocking interrupts
+ *
+ */
+
+void
+get_idt_func(
+    __u32 vect,
+    void **func
+    )
+{
+    IDTGDT_DESC idtDesc;
+    struct gate_struct oldGate;
+    struct gate_struct *idt;
+
+    GetIDTDesc(&idtDesc);
+
+    idt = idtDesc.idtgdt_base;
+
+    copy_16byte(&oldGate, &idt[vect]);
+
+    *func = (void *) ((((__u64) oldGate.offset_high) << 32) | (((__u64) oldGate.offset_middle) << 16) | ((__u64) oldGate.offset_low));
+
+    VDK_PRINT_DEBUG("idt base %p, vector %x has address %p\n", idt, vect, *func);
+
+    return;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void set_idt_func(__u32 vect, void *func)
+ * @brief       Set the function currently associated with an idt vector
+ *
+ * @param       vect - the idt vector of interest
+ * @param       func - function to be called when the interrupt occurs
+ *
+ * @return	none
+ *
+ *
+ * <I>Special Notes:</I>
+ *      None
+ *
+ */
+
+void
+set_idt_func(
+    __u32 vect,
+    void *func
+    )
+{
+    IDTGDT_DESC idtDesc;
+    struct gate_struct *idt;
+
+    GetIDTDesc(&idtDesc);
+
+    idt = idtDesc.idtgdt_base;
+
+    VDK_PRINT_DEBUG("Setting idt %p, vector %x with address %p\n", idt, vect, func);
+
+    _set_gate(&idt[vect], GATE_INTERRUPT, (unsigned long) func, 3, 0);
+
+    return;
+}
+
+/*
+**********************************************************************************************************
+
+    START/STOP EBS FUNCTIONS
+
+**********************************************************************************************************
+*/
+
+void
+samp_start_ints(void)
+{
+    __u32 cpu;
+    ULARGE_INTEGER val;
+    PIA32EM64T_DTS_BUFFER our_DTES_buf;
+
+    if (sample_method & METHOD_EBS) {
+        cpu = smp_processor_id();
+        if (cpu < MAX_PROCESSORS) {
+            // InterlockedIncrement(&processor_EBS_status[cpu]);
+            interlocked_exchange(&eachCPU[cpu].processor_EBS_status, 1);
+            if (eachCPU[cpu].original_EBS_idt_entry == 0) {
+                //
+                // CSS: TODO: When we have a routine that can handle
+                // the interrupt, then we write this routine to insert it...
+                //
+                // CSS: Think this is DONE now unless we want to make it
+                // a single routine instead of three calls...
+                //
+                get_idt_func(PERF_MON_VECTOR,&eachCPU[cpu].original_EBS_idt_entry);
+                set_idt_func(PERF_MON_VECTOR, eachCPU[cpu].samp_EBS_idt_routine);
+                eachCPU[cpu].original_apic_perf_local_vector = SetApicPerfLevel(apic_perf_lvt);
+
+                //
+                // If precise EBS active, then allocate a DTES buffer and
+                // install it for current logical processor (HW thread)
+                //
+                if (pebs_option) {
+                    eachCPU[cpu].DTES_buf = PEBS_alloc_DTES_buf(eachCPU[cpu].DTES_buf);
+                    our_DTES_buf = eachCPU[cpu].DTES_buf;
+                    //
+                    // If we could not get a DTES buffer, return without
+                    // starting...
+                    //
+                    // EBS ints for this cpu.
+                    // NEED TO FIX!! if this happens, we should stop
+                    // sampling and report an error.
+                    //
+                    if (!our_DTES_buf) {
+                        VDK_PRINT_ERROR("unable to allocate DTES buffer for cpu %d\n", cpu);
+                        return;
+                    }
+
+                    //
+                    // Save original DTES and DBG_CTL regs if we have not
+                    // already saved them
+                    //
+                    val = samp_read_msr(WMT_CR_DTES_AREA);
+                    if (val.quad_part != (__u64) our_DTES_buf) {
+                        eachCPU[cpu].org_DTES_area_msr_val = val;
+                        eachCPU[cpu].org_dbg_ctl_msr_val = samp_read_msr(DEBUG_CTL_MSR);
+                    }
+
+                    //
+                    // Disable PEBS and DTS for current thread
+                    // and install our DTES buf for PEBS
+                    //
+                    samp_disable_PEB_sand_DTS();
+                    val.quad_part = (__u64) our_DTES_buf;
+                    samp_write_msr(WMT_CR_DTES_AREA, val);
+                }
+            }
+            eachCPU[cpu].start_all = TRUE;
+        }
+        samp_start_emon(NULL);  // start EBS on current cpu
+    }
+
+    return;
+}
+
+
+/*++
+
+Routine description:
+
+    This routine is restores the IDT and Apic perf vectors
+    on the current cpu.
+    This routine is called by the Samp_KeUpdatesystemTimeCleanup or
+    SampKeUpdateRunTimeCleanup so it is eventually runs on each cpu.
+
+Arguments:
+
+return value:
+
+--*/
+void
+samp_restore_cpu_vectors(void)
+{
+    __u32 i;
+
+    i = smp_processor_id();
+
+    //
+    //  restore Apic perf vector for current cpu. We leave perf masked off
+    //
+    if (eachCPU[i].original_apic_perf_local_vector) {
+        EnablePerfVect(FALSE);
+
+        SetApicPerfLevel(eachCPU[i].original_apic_perf_local_vector);
+
+        eachCPU[i].original_apic_perf_local_vector = 0;
+    }
+
+    //
+    //  restore IDT vector for current cpu
+    //
+    if (eachCPU[i].original_EBS_idt_entry) {
+        //
+        // CSS: TODO: When we have a routine that can handle the interrupt
+        // and we write the routine to insert that handler in the IDT
+        // then we can worry about writing this routine to clean up...
+        //
+        set_idt_func(PERF_MON_VECTOR, eachCPU[i].original_EBS_idt_entry);
+        eachCPU[i].original_EBS_idt_entry = 0;
+    }
+
+    return;
+}
+
+/*
+**********************************************************************************************************
+
+                S A M P L I N G    C O N F I G U R A T I O N    F U N C T I O N S  
+
+**********************************************************************************************************
+*/
+
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn __u32 get_CSD(__u32 seg, __u32 *low, __u32 *high)
+ * @brief       Get the csd for a corresponding segment if available
+ *
+ * @param       seg IN - the segment
+ * @param       low OUT - the low part of the csd
+ * @param       high OUT - the high part of the csd
+ *
+ * @return	TRUE if the seg has a correspond CSD, FALSE for any errors
+ *
+ * Get the CSD entry for the corresponding segment number. In theory
+ * each processor can have its own GDT so, in theory, if we cached this
+ * info, we would need to do it per CPU. In reality for linux, it looks
+ * like they all point to the same spot...
+ *
+ * <I>Special Notes:</I>
+ *      Executed at interrupt time...
+ *
+ *
+ * @todo Really should save these since they are the same all the time...
+ *
+ */
+__u32
+get_CSD(
+    __u32 seg,
+    __u32 *low,
+    __u32 *high
+    )
+{
+    IDTGDT_DESC gdt_desc;
+    void *gdt_max_addr;
+    struct desc_struct *gdt;
+    code_descriptor *csd;
+
+    //
+    // These could be pre-init'ed values
+    //
+    GetGDTDesc(&gdt_desc);
+    gdt_max_addr = (void *) (((__u64) gdt_desc.idtgdt_base) + gdt_desc.idtgdt_limit);
+    gdt = gdt_desc.idtgdt_base;
+
+    //
+    // end pre-init potential...
+    //
+
+    //
+    // We don't do ldt's
+    //
+    if (seg & IS_LDT_BIT) {
+        *low = 0;
+        *high =0;
+        return FALSE;
+    }
+
+    //
+    // segment offset is based on dropping the bottom 3 bits...
+    //
+    csd = (code_descriptor *) &(gdt[seg >> SEGMENT_SHIFT]);
+
+    if (((void *) csd) >= gdt_max_addr) {
+        VDK_PRINT_ERROR("segment too big in get_CSD(%x)\n", seg);
+        return FALSE;
+    }
+
+    *low = csd->low_word;
+    *high = csd->high_word;
+
+    VDK_PRINT_DEBUG("get_CSD - seg %x, low %08x, high %08x, reserved_0: %d\n",
+            seg,
+            *low,
+            *high,
+            csd->reserved_0);
+
+    return TRUE;
+}
+
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void ReadTsc(__u64 *pTsc)
+ * @brief       Read the tsc store the result
+ *
+ * @return	none
+ *
+ * Use the inline assembly from msr.h to do the actual tsc read
+ *
+ * <I>Special Notes:</I>
+ *
+ *	None
+ *
+ * @todo Could inline this too...
+ *
+ */
+void
+ReadTsc(__u64 *pTsc)
+{
+    rdtscll(*pTsc);
+
+    return;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void ReadMsr(__u32 msr, __u64 *pResult)
+ * @brief       Read a value from the msr
+ *
+ * @return	none
+ *
+ * Use the inline assembly from msr.h to do the actual msr read
+ *
+ * <I>Special Notes:</I>
+ *
+ *	None
+ *
+ * @todo Could inline this too...
+ *
+ */
+void
+ReadMsr(__u32 msr, __u64 *pResult)
+{
+    rdmsrl(msr, *pResult);
+
+    return;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void WriteMsr(__u32 msr, __u64 value)
+ * @brief       Write a value to the msr
+ *
+ * @return	none
+ *
+ * Use the inline assembly from msr.h to do the actual msr write
+ *
+ * <I>Special Notes:</I>
+ *
+ *	None
+ *
+ * @todo Could inline this too...
+ *
+ */
+void
+WriteMsr(__u32 msr, __u64 value)
+{
+    wrmsrl(msr, value);
+
+    return;
+}
+
diff -urN linux.orig/drivers/char/vtune/vtlib32.c linux/drivers/char/vtune/vtlib32.c
--- linux.orig/drivers/char/vtune/vtlib32.c	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/vtlib32.c	2004-05-04 10:29:08.000000000 -0700
@@ -0,0 +1,472 @@
+/*
+ *  vtlib32.c
+ *
+ *  Copyright (C) 2002-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: vtlib32.c
+ *
+ *	Description: functions for manipulating the hardware registers on
+ *	             IA32 platforms
+ *
+ *	Author(s): George Artz, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#include <asm/uaccess.h>
+#include <linux/vmalloc.h>
+#include <linux/slab.h>     /*malloc */
+
+#include "vtuneshared.h"
+#include "apic.h"
+#include "vtdef.h"
+#include "vtproto.h"
+#include "vtextern.h"
+#include <asm/io.h>
+#ifdef KERNEL_26X
+#include <linux/sched.h>
+#include <asm/current.h>
+#include <linux/pid.h>
+#endif
+#include <asm/msr.h>
+#include <asm/desc.h>
+#include <asm/segment.h>
+
+#include "vtasm32.h"
+#include "vtlibcommon32.h"
+// Note: 07-17-00
+// CJ does not want driver to init all emon regs
+// at the start of a sampling session.
+// See init_emon_regs varilable below.
+//
+
+//
+// Data to save/restore Cpu event monitoring registers          // 05-31-00
+// accross an EBS session
+//
+
+
+__u32 OS_base = 0xC0000000; // starting address of OS ring0 code 05-31-00
+
+__u16 ring0cs = 0x10;       // standard ring0 code segment
+__u16 ring3cs = 0x23;       // standard ring3 code segment
+#define EFLAGS_V86_MASK       0x00020000L
+
+#if defined(ALLOW_LBRS)
+extern BOOLEAN	capture_lbrs ;	// should we capture LBRs on this run?
+#endif
+
+extern BOOLEAN pebs_option;
+extern __u32 pebs_err;
+
+
+/*++
+Routine description:
+
+  Arguments:
+
+return value:
+    TRUE... DTES buffer setup for PEBS
+
+--*/
+PDTS_BUFFER
+PEBS_alloc_DTES_buf(PDTS_BUFFER p_DTES_buf)
+{
+
+    __u32 PEBS_base, lost_size = 0, record_size, buf_size;
+    PDTS_BUFFER p_DTS_buffer;
+
+    buf_size = sizeof (DTS_BUFFER) + (PEBS_RECORD_SIZE * 2);    // one PEBS record... need 2 records so that
+    // threshold can be less than absolute max
+    buf_size += 32;     // allow for allingment of PEBS base to cache line
+
+    if (p_DTES_buf) {
+        p_DTS_buffer = p_DTES_buf;
+    } else {
+        p_DTS_buffer = allocate_pool(non_paged_pool, buf_size);
+    }
+
+    if (!p_DTS_buffer) {
+        return p_DTS_buffer;
+    }
+
+    record_size = PEBS_RECORD_SIZE;
+
+    memset(p_DTS_buffer, 0, buf_size);
+
+    // Let us move down to actual place where the data buffer
+    // begins.
+
+    PEBS_base = (__u32) p_DTS_buffer + sizeof (DTS_BUFFER); //Add of data buffer
+
+    //Check for 32 byte  alignment here
+    if ((PEBS_base & 0x000001F) != 0x0) {
+        lost_size = PEBS_base % 32; //gets the records to be cut to make it aligned
+        PEBS_base += lost_size;
+    }
+    //
+    // Program the DTES buffer for precise EBS. Set PEBS buffer for one PEBS record
+    //
+
+    p_DTS_buffer->base = 0;
+    p_DTS_buffer->index = 0;
+    p_DTS_buffer->max = 0;
+    p_DTS_buffer->threshold = 0;
+    p_DTS_buffer->PEBS_base = PEBS_base;
+    p_DTS_buffer->PEBS_index = p_DTS_buffer->PEBS_base;
+    p_DTS_buffer->PEBS_max = p_DTS_buffer->PEBS_base + (record_size * 2);
+    p_DTS_buffer->PEBS_threshold = p_DTS_buffer->PEBS_max - record_size;
+
+    VDK_PRINT_DEBUG("DTES buf allocated for precise EBS. DTES buf %p\n", p_DTS_buffer);
+
+    return (p_DTS_buffer);
+}
+
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn void samp_build_csip_sample(PINT_FRAME int_frame, P_sample_record_PC p_sample)
+ * @brief       Actualy files the sample buffer with values
+ *
+ * @param       int_frame IN  - small interrupt frame with minimal values
+ * @param       p_sample  OUT - Buffer to be filled
+ *
+ * @return	none
+ *
+
+ * This routine actually fills in the sample record. In the case of PEBS,
+ * it can also grab the PEBS entry and use that in place of the eip. This
+ * routine also samples the LBRs if requested and appends them to the
+ * end of the sample record.
+ *
+ * <I>Special Notes:</I>
+ *
+ *	The MSR's to use to get the LBRs is hard coded. Works for current
+ * Pentium(R) 4 processor, but likely to be a problem in the future..
+ *
+ * @todo Update the LBR code to use a lookup array instead of hardcoded MSRs
+ *
+ */
+void
+samp_build_csip_sample(PINT_FRAME int_frame, P_sample_record_PC p_sample)
+{
+#if defined(ALLOW_LBRS)
+    ULARGE_INTEGER *nextOpenData;
+#endif
+
+    __u32 csdlo;        // low  half code seg descriptor
+    __u32 csdhi;        // high half code seg descriptor
+    __u32 seg_cs;       // code seg selector
+
+#ifdef USE_NMI
+    // temporarily disable if event is precise
+    if (pebs_option)
+      return;
+#endif
+
+    p_sample->cs = (__u16) int_frame->seg_cs;
+    p_sample->eip = int_frame->eip;
+    p_sample->eflags = int_frame->E_flags;
+    p_sample->cpu_and_OS = 0;   // init bit fields
+    p_sample->cpu_num = (__u16) smp_processor_id();
+    p_sample->bit_fields2 = 0;
+    p_sample->pid_rec_index_raw = 1;    // pid contains OS pid
+#ifdef ENABLE_TGID
+    p_sample->pid_rec_index = get_thread_group_id(current);
+#else
+    p_sample->pid_rec_index = current->pid;
+#endif
+    p_sample->tid = current->pid;
+
+    //
+    // If Pentium(R) 4 processor precise EBS, get eflags 
+    // and ip from Pentium(R) 4 processor DTES buf.
+    //
+    // !!! NOTE !!!
+    // There is a potential problem here.
+    /// @todo is there still a problem here?
+    //
+    if (pebs_option) {
+        PDTS_BUFFER p_DTES;
+        PPEBS_RECORD p_rec;
+        __u32 cpu;
+
+        cpu = p_sample->cpu_num;
+        if (cpu < MAX_PROCESSORS) {
+            p_DTES = eachCPU[cpu].DTES_buf;
+            if (p_DTES) {
+                if (p_DTES->PEBS_index != p_DTES->PEBS_base) {
+                    p_rec = (PPEBS_RECORD) p_DTES->PEBS_base;
+                    p_sample->eflags = p_rec->E_flags;
+                    p_sample->eip = p_rec->linear_IP;
+                    //
+                    // If stack says V86 mode but PEBS record says protected mode (32 or 16 bit),
+                    // then we assume that the EBS event occured in ring 0 code with standard NT ring 0 CS
+                    //
+                    if (int_frame->E_flags & EFLAGS_V86_MASK) {
+                        if (!(p_rec->E_flags & EFLAGS_V86_MASK)) {
+                            p_sample->cs = ring0cs;
+                        }
+                    } else {
+                        if (OS_base) {
+                            if (p_sample->eip >= OS_base) {
+                                p_sample->cs = ring0cs;
+                            } else {
+                                p_sample->cs = ring3cs;
+                            }
+                        }
+                    }
+                    p_DTES->PEBS_index = p_DTES->PEBS_base; // reset index to next PEBS record to base of buffer
+                } else {
+                    pebs_err++;
+                }
+            } else {
+                pebs_err++;
+            }
+        } else {
+            pebs_err++;
+        }
+    }
+    //
+    //  Save current code segment descriptor in cs:ip sample
+    //
+
+    ///
+    ///  @todo Is the LDT pagable? If so, this could cause a problem on V86 apps
+    ///
+
+    if (p_sample->eflags & EFLAGS_V86_MASK) {   // changed to use eflags from sample record     05-31-00 Pentium(R) 4 procesor
+        csdlo = 0;
+        csdhi = 0;
+    } else {
+
+        seg_cs = p_sample->cs;  // changed to use CS from sample record         05-31-00 Pentium(R) 4 processor
+        get_CSD(seg_cs, &csdlo, &csdhi);
+    }
+    p_sample->csd.low_word = csdlo;
+    p_sample->csd.high_word = csdhi;
+
+#if defined(ALLOW_LBRS)
+    nextOpenData = (ULARGE_INTEGER *) (((char *) p_sample) + sizeof(sample_record_PC));
+
+
+    if (sample_tsc)
+    {
+#if defined(DEBUG)
+	if (((char *) nextOpenData) != ((char *) p_sample) + sample_tsc_offset) {
+	    VDK_PRINT_ERROR("tsc offest has unexpected value (%d, %p, %p).\n", sample_tsc_offset, p_sample, nextOpenData);
+	    return;
+	}
+	
+	if (sample_tsc_offset + sizeof(ULARGE_INTEGER) > sample_rec_length) {
+	    VDK_PRINT_ERROR("tsc offest (%d) larger than record size (%d)\n", sample_tsc_offset, sample_rec_length);
+	    return;
+	}
+#endif
+	rdtsc(nextOpenData->low_part, nextOpenData->high_part);
+	nextOpenData++;
+    }
+
+#else
+    if (sample_tsc)
+    {
+        u32 *tsc;
+
+        tsc = (void *) p_sample + sample_tsc_offset;
+        rdtsc(tsc[0], tsc[1]);
+    }
+#endif
+
+#if defined(ALLOW_LBRS)
+    if (capture_lbrs) {
+
+#if defined(DEBUG)
+	{
+	    // Makes the if statments below easier...
+	    long samp_start, samp_current;
+            long samp_available;
+	    long samp_needed;
+
+	    samp_start = (long) p_sample;
+	    samp_current = (long) nextOpenData;
+            samp_available = sample_rec_length - (samp_current - samp_start);
+
+	    // Need to make sure we have space for 4 LBRs plus a TOS indicator
+	    samp_needed = LBR_SAVE_SIZE;
+
+	    if (samp_available < samp_needed) {
+	        VDK_PRINT_ERROR("sample_rec_length (%d) not big enough to hold samples.\n", 
+			sample_rec_length);
+	        VDK_PRINT_ERROR("p_sample: 0x%p next: 0x%p, available %ld needed %ld\n",
+			p_sample,
+			nextOpenData,
+			samp_available,
+			samp_needed);
+	        return;
+	    }
+	}
+#endif
+	//
+	// For now, the MSR to use is a constant, but it is not
+	// architectural so the HW people could change this.
+	// BEWARE!
+	// Longer term, these values should be in an array so
+	// you can choose what to grab (and how many) based on the
+	// processor.
+	//
+	*nextOpenData = samp_read_msr(MSR_LASTBRANCH_TOS);
+	nextOpenData++;
+
+	*nextOpenData = samp_read_msr(MSR_LASTBRANCH_0);
+	nextOpenData++;
+	*nextOpenData = samp_read_msr(MSR_LASTBRANCH_1);
+	nextOpenData++;
+	*nextOpenData = samp_read_msr(MSR_LASTBRANCH_2);
+	nextOpenData++;
+	*nextOpenData = samp_read_msr(MSR_LASTBRANCH_3);
+    }
+#endif
+    
+    return;
+}
+
+
+/*
+**********************************************************************************************************
+
+    START/STOP EBS FUNCTIONS
+
+**********************************************************************************************************
+*/
+
+void
+samp_start_ints(void)
+{
+    __u32 cpu;
+    ULARGE_INTEGER val;
+    PDTS_BUFFER our_DTES_buf;
+
+#ifdef USE_NMI
+    // temporarily disable if event is precise
+    if (pebs_option)
+      return;
+#endif
+
+    if (sample_method & METHOD_EBS) {
+        cpu = smp_processor_id();
+        if (cpu < MAX_PROCESSORS) {
+            // InterlockedIncrement(&processor_EBS_status[cpu]);
+            interlocked_exchange(&eachCPU[cpu].processor_EBS_status, 1);
+            if (eachCPU[cpu].original_EBS_idt_entry == 0) {
+#ifndef USE_NMI
+                samp_get_set_idt_entry(PERF_MON_VECTOR, (__u32)
+                               eachCPU[cpu].samp_EBS_idt_routine, &eachCPU[cpu].original_EBS_idt_entry);
+#endif
+                eachCPU[cpu].original_apic_perf_local_vector = SetApicPerfLevel(apic_perf_lvt); 
+
+                //
+                // If Pentium(R) 4 processor precise EBS is active, then allocate a DTES buffer
+                // and install it for current logical processor (HW thread)
+                //
+                if (pebs_option) {  // 05-31-00 Pentium(R) 4 processor
+                    eachCPU[cpu].DTES_buf = PEBS_alloc_DTES_buf(eachCPU[cpu].DTES_buf);
+                    our_DTES_buf = eachCPU[cpu].DTES_buf;
+                    //
+                    // If we could not get a DTES buffer, return without starting
+                    // EBS ints for this cpu.
+                    // NEED TO FIX!! if this happens, we should stop sampling and report
+                    // an error.
+                    //
+                    if (!our_DTES_buf) {
+                        VDK_PRINT("unable to allocate DTES buffer for cpu %d\n", cpu);
+                        return;
+                    }
+                    //
+                    // Save original DTES and DBG_CTL regs if we have not
+                    // already saved them
+                    //
+                    val = samp_read_msr(WMT_CR_DTES_AREA);
+                    if (val.low_part != (__u32) our_DTES_buf) {
+                        eachCPU[cpu].org_DTES_area_msr_val = val;
+                        eachCPU[cpu].org_dbg_ctl_msr_val = samp_read_msr(DEBUG_CTL_MSR);
+                    }
+                    //
+                    // Disable PEBS and DTS for current thread and install our DTES buf for PEBS
+                    //
+                    samp_disable_PEB_sand_DTS();
+                    val.high_part = 0;
+                    val.low_part = (__u32) our_DTES_buf;
+                    samp_write_msr(WMT_CR_DTES_AREA, val);  // install our DTES bufer on current thread
+                }
+            }
+            eachCPU[cpu].start_all = TRUE;
+        }
+        samp_start_emon(NULL);  // start EBS on current cpu
+    }
+
+    return;
+}
+
+
+/*++
+
+Routine description:
+
+    This routine is restores the IDT and Apic perf vectors
+    on the current cpu.
+    This routine is called by the Samp_KeUpdatesystemTimeCleanup or
+    SampKeUpdateRunTimeCleanup so it is eventually runs on each cpu.
+
+Arguments:
+
+return value:
+
+--*/
+void
+samp_restore_cpu_vectors(void)
+{
+    __u32 i;
+
+    i = smp_processor_id();
+
+    //
+    //  restore Apic perf vector for current cpu
+    //
+    if (eachCPU[i].original_apic_perf_local_vector) {
+        EnablePerfVect(FALSE);
+        eachCPU[i].original_apic_perf_local_vector = 0;
+    }
+    //
+    //  restore IDT vector for current cpu
+    //
+    if (eachCPU[i].original_EBS_idt_entry) {
+        samp_restore_idt_entry(PERF_MON_VECTOR, &eachCPU[i].original_EBS_idt_entry);
+        eachCPU[i].original_EBS_idt_entry = 0;
+    }
+
+    return;
+}
diff -urN linux.orig/drivers/char/vtune/vtlib64.c linux/drivers/char/vtune/vtlib64.c
--- linux.orig/drivers/char/vtune/vtlib64.c	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/vtlib64.c	2004-04-08 13:28:56.000000000 -0700
@@ -0,0 +1,1879 @@
+/*
+ * vtlib64.c
+ *
+ *  Copyright (C) 2002-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: vtlib64.c
+ *
+ *	Description: functions for manipulating the hardware registers on
+ *	             Itanium(R) processor family platforms
+ *
+ *	Author(s): George Artz, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#include <linux/vmalloc.h>
+#include <linux/slab.h>     /*malloc */
+#include <linux/module.h>
+#include <asm/uaccess.h>
+
+#include "vtuneshared.h"
+#include "vtdef.h"
+#include "vtproto.h"
+#include "vtextern.h"
+
+void samp_start_ints(BOOLEAN startall);
+
+void samp_stop_ints(void);
+
+#define non_paged_pool 0
+#define profile_time  1
+
+__u32 cpu_family;       // cpu family
+__u32 EBS_vector;       // vector for counter overflow interrupt
+__u32 init_driver_var = 1;      // driver initialized flag
+
+__u16 ring0cs = 0x10;       // standard ring0 code segment  
+__u16 ring3cs = 0x23;       // standard ring3 code segment  
+__u32 valid_counters;       // bit map to indicate which counters we are using
+
+#define EFLAGS_V86_MASK       0x00020000L
+
+// PMC and PMD arrays for sampling session                     
+
+#define NUMBER_OF_ITANIUM_GENERIC_COUNTERS 4    //  Need to fix!! should get PMC/PMD info from PAL       
+__u32 counter_options[NUMBER_OF_ITANIUM_GENERIC_COUNTERS];  // counter overflow options          
+#define COUNTER_OPTS_COLLECT_IEAR_REGS 0x1
+#define COUNTER_OPTS_COLLECT_DEAR_REGS 0x2
+#define COUNTER_OPTS_COLLECT_BTRACE_REGS 0x4
+#define COUNTER_OPTS_EAR  (COUNTER_OPTS_COLLECT_IEAR_REGS | COUNTER_OPTS_COLLECT_DEAR_REGS | COUNTER_OPTS_COLLECT_BTRACE_REGS)
+
+__u64 pmd17_slot_mask;
+__u64 pmd17_slot_shift;
+__u64 pmd17_bundle_mask;
+
+__u64 pmc_es_mask;
+__u64 pmc_es_shift;
+
+__u64 dear_event_code;
+__u64 iear_event_code;
+__u64 btrace_event_code;
+
+__u64 dear_offset;
+__u64 iear_offset;
+__u64 btrace_offset;
+
+__u64 sample_options;
+#define SAMPLE_OPTS_DEAR   1
+#define SAMPLE_OPTS_IEAR   2
+#define SAMPLE_OPTS_BTRACE 4
+
+typedef struct PMC_SPEC_s {
+    __u32 number;       // register
+    __u32 reserved;     //
+    ULARGE_INTEGER value;   // value
+} PMC_SPEC;
+
+typedef struct PMD_SPEC_s {
+    __u32 number;       // register
+    __u32 reserved;     //
+    ULARGE_INTEGER value;   // value
+    ULARGE_INTEGER event_inc;   // value added to event_total on counter overflow interrupt 
+    ULARGE_INTEGER event_total[MAX_PROCESSORS]; // event total from start of sampling session   
+} PMD_SPEC;
+
+#define MAX_PMCS 20     // 10 entries to support setting of PMCs 4-13 Itanium(R) processor, 4-15 Itanium(R) 2 processor
+#define MAX_PMDS 4      // 4  entries to support setting of PMDs 4-7 (4 counters)
+
+PMC_SPEC pmcs[MAX_PMCS + 1];    // pmc regs for a sampling session
+PMD_SPEC pmds[MAX_PMDS + 1];    // pmc regs for a sampling session
+
+__u32 max_config_pmc = 15;  // maximum pmc which can be written for PMU configuration
+
+#define MAX_OS_EVENTS 32
+__s32 OS_events[MAX_OS_EVENTS]; //
+
+void samp_save_set_cpu_vectors(void);
+
+void samp_restore_cpu_vectors(void);
+
+__u32
+get_cpu_family(void)
+{
+    __u64 cpuid = 0;
+
+    cpuid = itp_get_cpuid(3);  // get cpuid reg 3...contains version info
+    // 63:40.... reverved
+    // 39:32.... architecture revision
+    // 31:24.... family
+    // 23:16.... model
+    // 15:8..... revision
+    //  7:0..... max cpuid reg index
+
+    return ((__u32) (cpuid & ITP_CPUID_REG3_FAMILY) >> 24);
+}
+
+void
+set_pmv_mask(void)
+{
+    __u64 pmv;
+
+    pmv = itp_read_reg_pmv();
+    pmv |= PMV_MASK_BIT;
+    itp_write_reg_pmv(pmv);
+
+    return;
+}
+
+void
+clear_PMV_mask(void)
+{
+    __u64 pmv;
+
+    pmv = itp_read_reg_pmv();
+    pmv &= ~PMV_MASK_BIT;
+    itp_write_reg_pmv(pmv);
+
+    return;
+}
+
+__u32
+tbs_get_current_process_id(void)
+{
+    return (current->pid);
+}
+
+__u32
+tbs_get_current_thread_id(void)
+{
+    return (current->pid);
+}
+
+__u32
+get_current_processor_number(void)
+{
+    return (smp_processor_id());
+}
+
+void_ptr
+allocate_pool(__u32 pool_type, __u32 i)
+{
+    return (kmalloc(i, GFP_ATOMIC));
+}
+
+void
+free_pool(void_ptr i)
+{
+    kfree(i);
+
+    return;
+}
+
+void
+rtl_zero_memory(void_ptr dest, __u32 length)
+{
+    memset(dest, 0, length);
+
+    return;
+}
+
+__u64
+interlocked_exchange(__u32 * dest, __u32 val)
+{
+    return (xchg(dest, val));
+}
+
+void
+add_to_counter_total_on_overflow(ULARGE_INTEGER pmc0, __u32 cpu)
+{
+    __u32 i, pmd_num;
+
+    for (i = 0; i < MAX_PMDS; i++) {
+        pmd_num = pmds[i].number;
+        if ((pmd_num >= 4) && (pmd_num <= 7)) {
+            //
+            // If counter overflowed, add to event total
+            //
+            if (pmc0.low_part & (1 << pmd_num)) {
+                pmds[i].event_total[cpu].quad_part +=
+                    pmds[i].event_inc.quad_part;
+            }
+        }
+    }
+
+    return;
+}
+
+//
+// Add the current PMC value to the counter total 
+//
+// This routine should be called once per cpu at the end of a sampling session.
+// The counters should be stopped before calling this routine.
+//
+//
+
+void
+add_to_counter_total_on_stop(void)
+{
+    __u32 i, cpu, pmd_num;
+    ULARGE_INTEGER val;
+
+    cpu = get_current_processor_number();
+    for (i = 0; i < MAX_PMDS; i++) {
+        pmd_num = pmds[i].number;
+        if ((pmd_num < 4) || (pmd_num > 7)) {
+            continue;
+        }
+        val.quad_part = itp_read_reg_pmd(pmd_num);
+        val.quad_part &= max_counter.quad_part;
+        if (val.quad_part >= pmds[i].value.quad_part) {
+            val.quad_part -= pmds[i].value.quad_part;
+        }
+        pmds[i].event_total[cpu].quad_part += val.quad_part;
+
+    }
+
+    return;
+}
+
+
+
+//
+// Read CPU Performance Counters for the current cpu
+//
+// Fill in the "read pmc" buffer with current counter value and 
+// accumlated total for each PMC.
+//
+void 
+read_cpu_perf_counters_for_current_cpu(
+    void *info
+    )
+{
+    __u32 i, cpu, tmp_cpu_num, cpu_mask, pmc_num;
+    ULARGE_INTEGER pmc_val, start_total, pmc_mask, pmc_mask_read;
+    RDPMC_BUF *pr_buf;
+
+    pr_buf = (RDPMC_BUF *) info;
+
+    if (!pr_buf)
+    {
+        return;
+    }
+
+    cpu = get_current_processor_number();
+
+    if (cpu >= RDPMC_MAX_CPUS)
+    {
+      return;
+    }
+
+    cpu_mask = 1 << cpu;
+
+    pmc_mask_read.quad_part = 0;
+    //
+    // Update RDPMC data for current cpu. Skip update if we already did it
+    // 
+    if ((pr_buf->cpu_mask_in & cpu_mask) && !(pr_buf->cpu_mask_out & cpu_mask))   
+    {
+        //
+        // Fill in RDPMC buffer for current cpu
+        //
+        for (i=0; i < MAX_PMDS; i++) 
+        {
+            pmc_num = pmds[i].number;
+            if ((pmc_num < 4) || (pmc_num > 7))
+            {
+                continue;
+            }
+            pmc_mask.quad_part = 1 << pmc_num;
+            //
+            // If pmc is not in regSet, skip it.
+            //
+            if (!(pr_buf->pmc_mask.quad_part & pmc_mask.quad_part))
+            {
+                continue;
+            }
+
+            //
+            // Since the routine can't handle larger than RDPMC_MAX_CPUS,
+            // we need to add the extra CPU data into the results
+            //
+	    for (tmp_cpu_num = cpu ; tmp_cpu_num < MAX_PROCESSORS; tmp_cpu_num += RDPMC_MAX_CPUS)
+	    {
+	      //
+	      // Read current pmc and update pmc total. If the total changed accross the read,
+	      // then a counteroverflow must have occured so we redo the read until we get a
+	      // valid pair of values for pmc current/total.
+	      //
+	      do
+	      {
+                start_total = pmds[i].event_total[tmp_cpu_num];
+                pmc_val.quad_part = itp_read_reg_pmd(pmc_num);
+	      } while (start_total.quad_part != pmds[i].event_total[tmp_cpu_num].quad_part);
+
+	      //
+	      // Only update the current value once with a CPU within the normal
+	      // 0->RDPMC_MAX_CPUS limit
+	      //
+	      if (tmp_cpu_num < RDPMC_MAX_CPUS) {
+		pr_buf->pmc_values[pmc_num].pmc_val[cpu].pmc_current.quad_part = pmc_val.quad_part & max_counter.quad_part;
+	      }
+
+	      pr_buf->pmc_values[pmc_num].pmc_val[cpu].pmc_total.quad_part   += start_total.quad_part;
+	    }
+
+            pmc_mask_read.quad_part |= pmc_mask.quad_part;
+        }
+
+        //
+        // Return current value for pmc's that were not in the RegSet
+        //
+        // Note!! 
+        // For now, only support reading pmd's 4-7. Do other pmds when we have 
+        // have a version of I64ReadRegPMD that uses try/except logic.
+        //
+        pmc_mask.quad_part = 1;
+        for (i=0; i < RDPMC_MAX_PMCS; i++, pmc_mask.quad_part = pmc_mask.quad_part << 1)
+        {
+            if (!(pr_buf->pmc_mask.quad_part & pmc_mask.quad_part) ||
+                 (pmc_mask_read.quad_part & pmc_mask.quad_part))
+            {
+                continue;
+            }
+            if ((i >= 4) && (i <= 7))
+            {
+                pmc_val.quad_part = itp_read_reg_pmd(i);
+                pr_buf->pmc_values[i].pmc_val[cpu].pmc_current.quad_part = pmc_val.quad_part & max_counter.quad_part;
+            }
+            else
+            {
+                pr_buf->pmc_values[i].pmc_val[cpu].pmc_current.quad_part = 0;
+            }
+        }
+
+        spin_lock(&rdpmc_lock);
+        pr_buf->cpu_mask_out |= cpu_mask;
+        spin_unlock(&rdpmc_lock);    
+    }
+
+    return;
+}
+
+void
+init_emon_regs_itanium(void)
+/*++
+
+Routine description:
+
+    Initialize emon regs to a know state at the start of an EBS session.
+
+Arguments:
+
+return value:
+
+    none
+
+--*/
+{
+    __u32 i;
+    __u64 val;
+
+    // Freeze all counters and clear all overflow status bits
+
+    set_pmv_mask();
+    val = itp_read_reg_pmc(0);
+    val = ~(PMC0_OFLOWALL); // clear all overflow bits before setting pmc0.fr to avoid problems on Itanium(R) 2 processor
+    val |= PMC0_FREEZE; // set freeze
+    itp_write_reg_pmc(0, val);
+    clear_PMV_mask();
+
+    //
+    // Clear overflow interrupt and privilege level in all generic PMCs (4-7). 
+    // Clearing oi prevents interrupts, and setting plm=0 effectively stops counting for that pmc/pmd pair.
+    //
+    for (i = 4; i < 8; i++) {
+        val = itp_read_reg_pmc(i);
+        val &= ~(PMC4_OI | PMC4_PLM);   // turn off pmc.oi to prevent counter overflow interrupt. Clear plm to stop counting
+        itp_write_reg_pmc(i, val);  // ..
+        itp_write_reg_pmd(i, 0);    // clear counter
+    }
+
+    //
+    // Set PMC8 and PMC9 to disable address range and opcode matching
+    //
+    {
+        __u64 pmc8, pmc8new;
+        __u64 pmc9, pmc9new;
+
+        pmc8 = itp_read_reg_pmc(8);
+        itp_write_reg_pmc(8, -1);
+        pmc8new = itp_read_reg_pmc(8);
+
+        // init pmc9                                                                    10-22-00
+        pmc9 = itp_read_reg_pmc(9);
+        itp_write_reg_pmc(9, -1);
+        pmc9new = itp_read_reg_pmc(9);
+    }
+
+    //
+    // Clear PMC10.plm to disable Instruntion EAR monitoring
+    //
+    i = 10;
+    val = itp_read_reg_pmc(i);
+    val &= ~0xf;        // set plm = 0 to disable monitoring
+    itp_write_reg_pmc(i, val);
+
+    //
+    // Clear PMC11.plm to disable Data EAR monitoring
+    //
+    i = 11;
+    val = itp_read_reg_pmc(i);
+    val &= ~0xf;        // set plm = 0 to disable monitoring
+    val |= 1 << 28;     // set pt = 1 to disable data address range checking    10-22-00
+    itp_write_reg_pmc(i, val);
+
+    //
+    // Clear PMC12.plm to disable Branch Trace buffer collection
+    //
+    i = 12;
+    val = itp_read_reg_pmc(i);
+    val &= ~0xf;        // set plm = 0 to disable monitoring
+    itp_write_reg_pmc(i, val);
+
+    //
+    // Set "Tag All" bit in pmc13 to disable address range matching
+    //
+    {
+        __u64 pmc13, pmc13new;
+
+        pmc13 = itp_read_reg_pmc(13);
+        pmc13new = pmc13 | 1;   // set "Tag All" bit
+        itp_write_reg_pmc(13, pmc13new);
+        pmc13new = itp_read_reg_pmc(13);
+    }
+
+    return;
+}
+
+void
+init_emon_regs_itanium2(void)
+/*++
+
+Routine description:
+
+    Initialize emon regs to a know state at the start of an EBS session.
+
+Arguments:
+
+return value:
+
+    none
+
+--*/
+{
+    __u32 i;
+    __u64 val;
+
+    //
+    // Enable performance monitoring HW on Itanium(R) 2 processor (PMC4 bit 23 = Enable PM)
+    //
+    {
+        ULARGE_INTEGER pmc4, pmc4new;
+
+        pmc4.quad_part = itp_read_reg_pmc(4);
+        if (!(pmc4.low_part & (1 << 23))) {
+            pmc4new = pmc4;
+            pmc4new.low_part |= (1 << 23);
+            itp_write_reg_pmc(4, pmc4new.quad_part);
+            pmc4new.quad_part = itp_read_reg_pmc(4);
+        }
+    }
+
+    // Freeze all counters and clear all overflow status bits
+
+    set_pmv_mask();
+    val = itp_read_reg_pmc(0);
+    val = ~(PMC0_OFLOWALL); // clear all overflow bits before setting pmc0.fr to avoid problems on Itanium(R) 2 processor
+    val |= PMC0_FREEZE; // set freeze
+    itp_write_reg_pmc(0, val);
+    clear_PMV_mask();
+
+    //
+    // Clear overflow interrupt and privilege level in all generic PMCs (4-7). 
+    // Clearing oi prevents interrupts, and setting plm=0 effectively stops counting for that pmc/pmd pair.
+    //
+    for (i = 4; i < 8; i++) {
+        val = itp_read_reg_pmc(i);
+        val &= ~(PMC4_OI | PMC4_PLM);   // turn off pmc.oi to prevent counter overflow interrupt. Clear plm to stop counting
+        itp_write_reg_pmc(i, val);  // ..
+        itp_write_reg_pmd(i, 0);    // clear counter
+    }
+
+    //
+    // Reset the global pmc's to power on values
+    //
+
+    itp_write_reg_pmc(8, -1);
+    itp_write_reg_pmc(9, -1);
+
+    itp_write_reg_pmc(10, 0);
+    itp_write_reg_pmc(11, 0);
+    itp_write_reg_pmc(12, 0);
+
+    itp_write_reg_pmc(13, 0x2078fefefefe);
+    itp_write_reg_pmc(14, 0xdb6);
+    itp_write_reg_pmc(15, 0xfffffff0);
+
+    return;
+}
+
+void
+init_emon_regs(void)
+/*++
+
+Routine description:
+
+    Initialize emon regs to a know state at the start of an EBS session.
+
+Arguments:
+
+return value:
+
+    none
+
+--*/
+{
+    switch (cpu_family) {
+    case ITP_CPU_FAMILY_ITANIUM:
+        init_emon_regs_itanium();
+        break;
+    case ITP_CPU_FAMILY_ITANIUM2:
+        init_emon_regs_itanium2();
+        break;
+    default:
+        break;
+    }
+
+    return;
+}
+
+__u32
+init_driver(void)
+{
+    init_driver_OS();
+
+    /* Initialize system info storage */
+    memset(&samp_info, 0, sizeof (sampinfo_t));
+    memset(&samp_parms, 0, sizeof (samp_parm6));
+
+    samp_info.flags = SINFO_STOP_COMPLETE;
+    cpu_family = get_cpu_family();
+
+    //
+    // Set max counter value and max wr4iteable pmc.
+    // Need to fix!! 
+    // Counter width should be determined from Itanium(R) processor HW regs.
+    //
+    max_counter.quad_part = ITANIUM2_MAXCOUNTER;
+    max_config_pmc = ITANIUM2_MAX_CONFIG_PMC;
+    pmd17_slot_mask = ITANIUM2_PMD17_SLOT_MASK;
+    pmd17_slot_shift = ITANIUM2_PMD17_SLOT_SHIFT;
+    pmd17_bundle_mask = ITANIUM2_PMD17_BUNDLE_MASK;
+
+    pmc_es_mask = ITANIUM2_PMC_ES_MASK;
+    pmc_es_shift = ITANIUM2_PMC_ES_SHIFT;
+
+    dear_event_code = ITANIUM2_DEAR_EVENT_CODE;
+    iear_event_code = ITANIUM2_IEAR_EVENT_CODE;
+    btrace_event_code = ITANIUM2_BTRACE_EVENT_CODE;
+
+    //
+    // Set EAR data offsets to 0 to prevent collection of 
+    // ear data until we have a way to pass size of 
+    // SampleRecord to user mode code. (Now done using DSA)
+    //
+    dear_offset = iear_offset = btrace_offset = 0;
+
+    if (cpu_family == ITP_CPU_FAMILY_ITANIUM) {
+        max_counter.quad_part = ITANIUM_MAXCOUNTER;
+        max_config_pmc = ITANIUM_MAX_CONFIG_PMC;
+        pmd17_slot_mask = ITANIUM_PMD17_SLOT_MASK;
+        pmd17_slot_shift = ITANIUM_PMD17_SLOT_SHIFT;
+        pmd17_bundle_mask = ITANIUM_PMD17_BUNDLE_MASK;
+        pmc_es_mask = ITANIUM_PMC_ES_MASK;
+        pmc_es_shift = ITANIUM_PMC_ES_SHIFT;
+        dear_event_code = ITANIUM_DEAR_EVENT_CODE;
+        iear_event_code = ITANIUM_IEAR_EVENT_CODE;
+        btrace_event_code = ITANIUM_BTRACE_EVENT_CODE;
+    }
+
+    /* Initialize global sampling control flags */
+    samp_info.flags = SINFO_STOP_COMPLETE;
+    samp_info.sampling_active = FALSE;
+    signal_thread_event = FALSE;
+
+    /*   Initialize timers  and resources */
+    g_max_samp_timer_ON = FALSE;
+    g_start_delay_timer_ON = FALSE;
+    init_timer(&delay_tmr);
+    init_timer(&time_out_tmr);
+
+    memset(&eachCPU[0], 0, sizeof(eachCPU)); // zero out entire array of PER_CPU entries
+
+    //
+    // Allocate Driver Shared Area (DSA) in non paged pool.
+    //
+
+#ifdef DSA_SUPPORT_MMAP
+    pdsa = create_dsa();
+#endif
+    if(pdsa == NULL)
+    {
+      VDK_PRINT_ERROR("init_driver: could not create DSA\n");
+      return (STATUS_INVALID_PARAMETER);
+    }
+    
+    pdsa->num_event_counters = 4;   // four counters on Itanium(R) processor   08-06-00
+    pdsa->method_EBS = TRUE;
+    pdsa->module_tracking = TRUE;   // driver can do module tracking
+
+    return (STATUS_SUCCESS);
+}
+
+/*
+**********************************************************************************************************
+
+    START/STOP EBS FUNCTIONS       
+
+**********************************************************************************************************
+*/
+
+void
+samp_start_emon_itp(BOOLEAN startall)
+/*++
+
+Routine description:
+
+Arguments:
+
+return value:
+
+    none
+
+--*/
+{
+    __u32 i, pmc, pmd;
+    __u64 pmc0, pmc0w;
+
+    if (sample_method & METHOD_EBS) {
+
+        //
+        // mask pmi before setting pmc0.fr=1 to workaround 
+        // Itanium(R) 2 processor errata... setting pmc0.fr=1
+        // on Itanium(R) 2 processor can cause a pmi.
+        // While pmi is masked via the pmv, pmi requests will
+        // be ignored (not latched).
+        //
+        set_pmv_mask();
+        pmc0 = itp_read_reg_pmc(0);
+        pmc0w = pmc0;
+        pmc0w &= ~(PMC0_OFLOWALL);
+        pmc0w |= PMC0_FREEZE;
+        itp_write_reg_pmc(0, pmc0w);
+
+        //
+        // If "startall" option is set, then set all pmc/pmd regs for EBS.
+        // If "startall" is not set, then we are probably restarting sampling
+        // after a counter overflow interrupt, and we only want to set the
+        // pmd regs for the counter(s) that overflowed.
+        //
+        if (startall) {
+            //
+            // Set counters (pmds 4-7) to 0 before setting pmcs 4-7 with pmc.fr=0.
+            // This is done to avoid possible pmis but it's not really necessary
+            // since we masked pmi via the pmv.
+            //
+            for (i = 4; i <= 7; i++) {
+                itp_write_reg_pmd(i, 0);
+            }
+
+            // 
+            // Program pmc's with pmc0.fr=0 to workaround Itanium(R) processor errata. 
+            // pmc0.fr must be unfrozen when programing pmcs for bus events.
+            //
+            pmc0w &= ~PMC0_FREEZE;
+            itp_write_reg_pmc(0, pmc0w);
+
+            for (i = 0; (i < MAX_PMCS) && (pmcs[i].number != 0);
+                 i++) {
+                pmc = pmcs[i].number;
+                if ((pmc >= 4) && (pmc <= 7)) {
+                    itp_write_reg_pmc(pmc,
+                               pmcs[i].value.quad_part);
+                }
+            }
+
+            pmc0w |= PMC0_FREEZE;
+            itp_write_reg_pmc(0, pmc0w);
+
+            for (i = 0; (i < MAX_PMCS) && (pmcs[i].number != 0);
+                 i++) {
+                pmc = pmcs[i].number;
+                if ((pmc < 4) || (pmc > 7)) {
+                    itp_write_reg_pmc(pmc,
+                               pmcs[i].value.quad_part);
+                }
+            }
+        }
+        //
+        // We unmask after writing pmcs to workaround Itanium(R) 2 processor
+        // errata that can caues pmi when writing pmc0.fr=1.
+        //
+        clear_PMV_mask();
+        for (i = 0; (i < MAX_PMDS) && (pmds[i].number != 0); i++) {
+            pmd = pmds[i].number;
+            if (startall
+                || (((pmd >= 4) && (pmd <= 7))
+                && (pmc0 & (1 << pmd)))) {
+                itp_write_reg_pmd(pmd, pmds[i].value.quad_part);
+            }
+        }
+        pmc0w = itp_read_reg_pmc(0);
+        pmc0w &= ~(PMC0_FREEZE | PMC0_OFLOWALL);
+        itp_write_reg_pmc(0, pmc0w);
+    }
+
+    return;
+}
+
+void
+samp_stop_emon_itp(void)
+{
+    __u64 pmr;
+
+    if (sample_method & METHOD_EBS) {
+        set_pmv_mask();
+        pmr = itp_read_reg_pmc(0);
+        pmr |= PMC0_FREEZE;
+        itp_write_reg_pmc(0, pmr);
+        clear_PMV_mask();
+    }
+
+    return;
+}
+
+void
+samp_start_emon(void * info)
+{
+    samp_start_emon_itp(FALSE);
+
+    return;
+}
+
+void
+samp_stop_emon(void)
+{
+    samp_stop_emon_itp();
+
+    return;
+}
+
+void
+samp_start_profile_interrupt(void * info)
+{
+    __u32 processor;    // processor number (0 through MAX_PROCESSORS)
+
+    processor = get_current_processor_number();
+
+    VDK_PRINT_DEBUG("start entered cpu %d ebs_irq %d \n",
+		    processor, ebs_irq);
+    if ((samp_info.flags & SINFO_STARTED) && (sample_method & METHOD_EBS)
+        && (processor < MAX_PROCESSORS) && ebs_irq) {
+
+        if (!xchg(&eachCPU[processor].processor_EBS_status, 1)) {
+            VDK_PRINT_DEBUG("starting profile ints cpu %d \n", processor);
+            samp_save_set_cpu_vectors();
+            init_emon_regs();
+            samp_start_ints(TRUE);
+        }
+    }
+
+    return;
+}
+
+void
+samp_stop_profile_interrupt(void * info)
+{
+    __u32 processor;    // processor number (0 through MAX_PROCESSORS)
+
+    processor = get_current_processor_number();
+
+    if (xchg(&eachCPU[processor].processor_EBS_status, 0)) {
+        VDK_PRINT_DEBUG("stopping profile ints cpu %d \n", processor);
+        samp_stop_ints();
+        add_to_counter_total_on_stop();
+        init_emon_regs();
+        samp_restore_cpu_vectors();
+    }
+
+    return;
+}
+
+void
+driver_load(void)
+{
+    return;
+}
+
+void
+driver_open(void)
+{
+    if (xchg(&init_driver_var, 0)) {
+        init_driver();
+    }
+
+    return;
+}
+
+void
+driver_unload(void)
+{
+#ifdef DSA_SUPPORT_MMAP
+  int ret;
+  ret = destroy_dsa();
+#endif
+  pdsa = NULL;
+
+  return;
+}
+
+void
+samp_start_ints(BOOLEAN startall)
+/*++
+
+Routine description:
+
+    This routine sets up the cpu perfomance counter registers for event
+    based sampling. At the start of a sampling session, this routine is called
+    once on each cpu. When a counter overflow interrupt occurs, this
+    routine is called to reset the cpu performance counter registers
+    for the counter that overflowed.
+
+Arguments:
+
+return value:
+
+    none
+
+--*/
+{
+    if (sample_method & METHOD_EBS) {
+        samp_start_emon_itp(startall);
+    }
+
+    return;
+}
+
+void
+samp_stop_ints(void)
+{
+    __u64 pmr;
+
+    if (sample_method & METHOD_EBS) {
+        set_pmv_mask();
+        pmr = itp_read_reg_pmc(0);
+        pmr |= PMC0_FREEZE;
+        itp_write_reg_pmc(0, pmr);
+        clear_PMV_mask();
+    }
+
+    return;
+}
+
+void
+samp_restore_vectors(void)
+/*++
+
+Routine description:
+
+    This routine is restores the IDT and Apic perf vectors
+    for each cpu.
+
+Arguments:
+
+return value:
+
+--*/
+{
+
+    //
+    // restore IDT and Apic perf vectors on each cpu. This
+    // is done by installing a cleanup routine which runs
+    //
+    //
+
+    return;
+}
+
+void
+samp_save_set_cpu_vectors(void)
+/*++
+
+Routine description:
+
+    This routine is restores the IDT and Apic perf vectors
+    on the current cpu.
+    This routine is called by the Samp_KeUpdatesystemTimeCleanup or
+    SampKeUpdateRunTimeCleanup so it is eventually runs on each cpu.
+
+Arguments:
+
+return value:
+
+--*/
+{
+    __u32 i;
+
+    i = get_current_processor_number();
+
+    //
+    //  restore Apic perf vector for current cpu
+    //
+    if (i < MAX_PROCESSORS) {
+        eachCPU[i].original_apic_perf_local_vector = itp_get_pmv();
+        itp_write_reg_pmv(PMV_MASK_BIT | ebs_irq);
+        itp_srlz_d();
+    }
+
+    return;
+}
+
+void
+samp_restore_cpu_vectors(void)
+/*++
+
+Routine description:
+
+    This routine is restores the IDT and Apic perf vectors
+    on the current cpu.
+    This routine is called by the Samp_KeUpdatesystemTimeCleanup or
+    SampKeUpdateRunTimeCleanup so it is eventually runs on each cpu.
+
+Arguments:
+
+return value:
+
+--*/
+{
+    __u32 i;
+
+    i = get_current_processor_number();
+
+    //
+    //  restore Apic perf vector for current cpu
+    //
+    if (i < MAX_PROCESSORS) {
+        VDK_PRINT_DEBUG("restoring pmv cpu %d pmv %p \n",
+			i, eachCPU[i].original_apic_perf_local_vector);
+        itp_write_reg_pmv(eachCPU[i].original_apic_perf_local_vector);
+        itp_srlz_d();
+    }
+    return;
+}
+
+/*++
+
+Routine description:
+
+    get address of next free byte in Sample buffer.
+
+Arguments:
+
+    length = # of bytes to be allocated in sample buf.
+
+return value:
+
+--*/
+void_ptr
+samp_get_buf_space(__u32 length, u32 *wake_up_thread)
+{
+    void_ptr buf_ptr;
+
+    buf_ptr = NULL;
+    *wake_up_thread = FALSE;
+
+    spin_lock(&sample_int_lock);
+    if (samp_info.sampling_active) {
+        if ((p_sample_buf >= buf_start) && (p_sample_buf < buf_end)) {
+            buf_ptr = p_sample_buf;
+            (void_ptr) p_sample_buf += length;
+            if (p_sample_buf >= buf_end) {   
+                if (p_sample_buf > buf_end) {   // room for sample?
+                    p_sample_buf = buf_ptr;     // ..no
+                    buf_ptr = NULL;
+                }
+                samp_info.flags |= SINFO_DO_WRITE;
+                samp_info.sampling_active = FALSE;
+                *wake_up_thread = TRUE;
+            }
+            if (buf_ptr) {
+                //
+                // If max samples taken, then set flag to stop sampling
+                //
+                if (sample_max_samples) {
+                    __u32 i;
+                    i = pdsa->sample_count + 1;
+                    if (i >= sample_max_samples) {
+                        samp_info.flags |= (SINFO_DO_WRITE | SINFO_DO_STOP);
+                        samp_info.sampling_active = FALSE;
+                        *wake_up_thread = TRUE;
+                        if (i > sample_max_samples) {
+                            p_sample_buf = buf_ptr;     
+                            buf_ptr = NULL;
+                        }
+                    }
+                }
+                if (buf_ptr) {
+                    pdsa->sample_count++;  
+                    samp_info.sample_count = pdsa->sample_count;
+                }
+            }
+        }
+    }
+    spin_unlock(&sample_int_lock);
+
+    return (buf_ptr);
+}
+
+/*
+**********************************************************************************************************
+
+                S A M P L I N G    C O N F I G U R A T I O N    F U N C T I O N S  
+
+**********************************************************************************************************
+*/
+
+/*
+ *
+ *  Function: validate_samp_parm6 
+ *
+ *  description: 
+ *  Check to make sure the size of the samp_parm6 buffer is reasonable.
+ *
+ *  Parms:
+ *      entry:      samp_parm6
+ *                  length
+ *  
+ *      return:     0... samp_par6 length Ok
+ *
+ */
+int
+validate_samp_parm6(samp_parm6 * sp6, int sp6_len)
+{
+    int status = 0, ers_num, ers_len;
+
+    if (sp6_len < sizeof (samp_parm6)) {
+        VDK_PRINT_ERROR("Invalid samp_parm6 configuration\n");
+        status = -EINVAL;
+        return (status);
+    }
+
+    ers_num = sp6->num_event_reg_set;
+    if (ers_num) {
+        ers_len = (ers_num - 1) * sizeof(event_reg_set_ex); // subtract 1 for event_reg_set_ex that is included in samp_par6 structure
+        if (sp6_len < (sizeof(samp_parm6) + ers_len)) {
+            status = -EINVAL;
+        }
+    }
+
+    return (status);
+}
+
+void
+get_options_error(__u32 num_event_reg_set,  // number of entries in event_reg_set array
+        event_reg_set_ex * ers  // event_reg_set array
+    )
+{
+    __u32 i;
+
+    for (i = 0; i < num_event_reg_set; i++) {
+        VDK_PRINT_DEBUG("eventregSet %d cmd 0x%x \n",
+			num_event_reg_set, ers[i].counter_number);
+    }
+    return;
+}
+
+//
+// Setup PMCs and PMDs for sampling session             
+//
+
+BOOLEAN
+get_options_from_event_reg_set(__u32 num_event_reg_set, // number of entries in event_reg_set array
+              event_reg_set_ex * ers    // event_reg_set array
+    )
+{
+    __u32 ersx, pmcx, pmdx, osex, counter;
+    ULARGE_INTEGER pmc_val, pmd_val;
+
+    //
+    //  Init PMC and PMD regs for sampling session
+    //
+
+    memset((char *) &pmcs, 0, sizeof (pmcs));
+    memset((char *) &pmds, 0, sizeof (pmds));
+    memset((char *) &OS_events, 0, sizeof (OS_events));
+    memset((char *) &counter_options, 0, sizeof (counter_options));
+    memset((char *) &event_Ids, 0, sizeof (event_Ids));
+    valid_counters = 0;
+    sample_options = 0;
+
+    //
+    // Treat counter_number of 0 as default counter  (PMC4/PMD4)
+    //
+
+    VDK_PRINT_DEBUG("num_event_reg_set %d  event_reg_set addr %p \n",
+		    num_event_reg_set, ers);
+
+    if ((ers[0].counter_number == 0) && (ers[0].esr_value != 0)) {
+        ers[0].counter_number = 4;
+        if (num_event_reg_set == 0) {
+            num_event_reg_set = 1;
+        }
+    }
+    //
+    // get PMC and PMD regs from event_reg_set
+    //
+
+    for (ersx = 0, pmcx = 0, pmdx = 0, osex = 0; ersx < num_event_reg_set;
+         ++ersx) {
+
+        pmc_val.quad_part = 0;
+        pmd_val.quad_part = 0;
+
+        switch (ers[ersx].command) {
+        case ERS_CMD_SET_CONFIG_AND_COUNTER_REGS:
+            counter = ers[ersx].counter_number;
+            pmc_val.low_part = ers[ersx].esr_value;
+            pmd_val = ers[ersx].esr_count;
+            break;
+        case ERS_CMD_WRITE_PMC:
+            counter = ers[ersx].esr_value;
+            pmc_val = ers[ersx].esr_count;
+            break;
+        case ERS_CMD_TBS_VTD:
+            VDK_PRINT_DEBUG("ERS_CMD_TBS_VTD... dsa.method_VTD %d \n",
+			    (pdsa->method_VTD) ? 1 : 0);
+            if (pdsa->method_VTD) { // 08-18-00
+                if (osex < MAX_OS_EVENTS) {
+                    VDK_PRINT_DEBUG("profileTime set\n");
+                    OS_events[osex] = profile_time;
+                    osex++;
+                    continue;
+                }
+            }
+            get_options_error(num_event_reg_set, ers);
+            return FALSE;
+        case ERS_CMD_OS_EVENT:
+            /*
+               if (pdsa->method_OS_events) {  // 08-18-00
+               if (osex < MAX_OS_EVENTS) {
+               OS_events[osex] = ers[ersx].esr_value;
+               osex++;
+               continue;
+               }
+               }
+             */
+            get_options_error(num_event_reg_set, ers);
+            return FALSE;
+        case ERS_CMD_NOP:
+            continue;
+        default:
+            get_options_error(num_event_reg_set, ers);
+            return FALSE;
+        }
+
+        //
+        // get PMC/PMD combo for generic PMCs (PMC/PMD 4-7)
+        //
+
+        if ((counter >= 4) && (counter <= 7)) {
+            if (pmcx == MAX_PMCS) {
+                get_options_error(num_event_reg_set, ers);
+                return FALSE;
+            }
+            if (pmdx == MAX_PMDS) {
+                get_options_error(num_event_reg_set, ers);
+                return FALSE;
+            }
+            pmcs[pmcx].number = counter;
+            pmcs[pmcx].value = pmc_val;
+            pmds[pmdx].number = counter;
+            pmds[pmdx].event_inc = pmd_val;
+            if (!pmd_val.quad_part) {
+                pmds[pmdx].event_inc.quad_part =
+                    max_counter.quad_part;
+            }
+            if (pmd_val.quad_part) {
+                pmds[pmdx].value.quad_part =
+                    ~pmd_val.quad_part + 1;
+                pmds[pmdx].value.quad_part &=
+                    max_counter.quad_part;
+            }
+            if ((pmcs[pmcx].number == 4)
+                && (cpu_family == ITP_CPU_FAMILY_ITANIUM2)) {
+                pmcs[pmcx].value.low_part |= (1 << 23); // set PMC4 bit 23 to enable performance monitoring HW on Itanium(R) 2 processor
+            }
+            {
+                __u64 event_code;
+
+                event_code =
+                    (pmc_val.quad_part & pmc_es_mask) >> pmc_es_shift;
+
+                if (event_code == dear_event_code) {
+                    counter_options[counter - 4] |=
+                        COUNTER_OPTS_COLLECT_DEAR_REGS;
+                    sample_options |= SAMPLE_OPTS_DEAR;
+                }
+
+                if (event_code == iear_event_code) {
+                    counter_options[counter - 4] |=
+                        COUNTER_OPTS_COLLECT_IEAR_REGS;
+                    sample_options |= SAMPLE_OPTS_IEAR;
+                }
+
+                if (event_code == btrace_event_code) {
+                    counter_options[counter - 4] |=
+                        COUNTER_OPTS_COLLECT_BTRACE_REGS;
+                    sample_options |= SAMPLE_OPTS_BTRACE;
+                }
+            }
+            event_Ids[counter - 4] = ers[ersx].event_ID;
+            valid_counters |= 1 << (counter);
+            pmcx++;
+            pmdx++;
+            continue;
+        }
+        //
+        // Check for PMC 8-13/15 ... Opcode matching (pmc8 and 9), IEAR (pmc10), DEAR (pmc11), and Branch Trace configurtaion 
+        //
+        // Note: pmc 8 and 9 added 10-22-00 to support opcode matching 
+        //
+        if ((counter >= 8) && (counter <= max_config_pmc)) {
+            if (pmcx == MAX_PMCS) {
+                get_options_error(num_event_reg_set, ers);
+                return FALSE;
+            }
+
+            pmcs[pmcx].number = counter;
+            pmcs[pmcx].value = pmc_val;
+            pmcx++;
+            continue;
+        }
+        get_options_error(num_event_reg_set, ers);
+        return FALSE;
+    }
+
+    return TRUE;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          int samp_configure6(samp_parm6 *sp, int sp6_len)
+ * @brief       Configure sampling based on samp_parm6 structure
+ *
+ * @param       sp      IN - configuration information
+ * @param       sp6_len IN - length of config structure
+ *
+ * @return	success/error status
+ *
+ * Does a small amount of configuration. Sets some pointers
+ * in the sp6 structure, sets some global variables indicating the
+ * sampling method, gets the event set and sets the size of the
+ * sampling record based on what events are requested.
+ *
+ * <I>Special Notes:</I>
+ *
+ *	Implies VTD works (TBS), but it isn't actually supported.
+ *
+ * @todo enable VTD for real...
+ */
+int
+samp_configure6(samp_parm6 * sp6, int sp6_len)
+{
+    __u32 i;
+
+    if (samp_info.flags & (SINFO_STARTED | SINFO_STOPPING | SINFO_WRITE)) {
+        return (STATUS_DEVICE_BUSY);
+    }
+
+    if (!sp6 || validate_samp_parm6(sp6, sp6_len)) {
+        return (STATUS_INVALID_PARAMETER);
+    }
+
+
+    if (sp6->ptrs_are_offsets) {
+        sp6->module_info_file_name =
+            (char *) ((void_ptr) sp6 +
+                  (__u64) sp6->module_info_file_name);
+        sp6->raw_sample_file_name =
+            (char *) ((void_ptr) sp6 + (__u64) sp6->raw_sample_file_name);
+        sp6->ptrs_are_offsets = FALSE;
+    }
+
+    sample_method = 0;
+    for (i = 0; i < sp6->num_event_reg_set; i++) {
+        switch (sp6->esr_set[i].command) {
+        case ERS_CMD_TBS_VTD:
+            sample_rate_us = sp6->esr_set[i].esr_count.low_part;
+            sample_method |= METHOD_VTD;
+            break;
+        case ERS_CMD_SET_CONFIG_AND_COUNTER_REGS:
+        case ERS_CMD_WRITE_MSR:
+        case ERS_CMD_WRITE_PMC:
+            if (!(pdsa->method_EBS)) {
+                return (STATUS_INVALID_PARAMETER);
+            }
+            sample_method |= METHOD_EBS;
+            break;
+        case ERS_CMD_OS_EVENT:
+            /*
+               if (!(pdsa->method_OS_events)) {
+               return (STATUS_INVALID_PARAMETER);
+               }
+               sample_method |= METHOD_OSEVENTS;
+               break;
+             */
+            return (STATUS_INVALID_PARAMETER);
+        case ERS_CMD_NOP:
+            break;
+        default:
+            return (STATUS_INVALID_PARAMETER);
+        }
+    }
+
+    //
+    // get EBS regs from user parameters                            
+    //
+
+    if (!get_options_from_event_reg_set(sp6->num_event_reg_set, &sp6->esr_set[0])) {
+        return (STATUS_NOT_SUPPORTED);
+    }
+
+    pdsa->sample_rec_length = sizeof(sample_record_PC);
+    sample_tsc = FALSE;
+    if (sp6->sample_TSC)
+    {
+        sample_tsc_offset = pdsa->sample_rec_length;
+        sample_tsc = TRUE;
+        pdsa->sample_rec_length += 8;
+    }
+
+    dear_offset = iear_offset = btrace_offset = 0;
+    if (sample_options & SAMPLE_OPTS_IEAR)
+    {
+        iear_offset = pdsa->sample_rec_length;
+        pdsa->sample_rec_length += 8*2;
+        VDK_PRINT_DEBUG("Instruction EAR support enabled. PC sample record size %d \n", pdsa->sample_rec_length);
+    }
+    if (sample_options & SAMPLE_OPTS_DEAR)
+    {
+        dear_offset = pdsa->sample_rec_length;
+        pdsa->sample_rec_length += 8*3;
+        VDK_PRINT_DEBUG("Data EAR support enabled. PC sample record size %d \n", pdsa->sample_rec_length);
+    }
+    if (sample_options & SAMPLE_OPTS_BTRACE)
+    {
+        btrace_offset = pdsa->sample_rec_length;
+        pdsa->sample_rec_length += 8*9;
+        VDK_PRINT_DEBUG("Branch Event support enabled. PC sample record size %d \n", pdsa->sample_rec_length);
+    }
+    sample_rec_length = pdsa->sample_rec_length;
+
+    return (STATUS_SUCCESS);
+}
+
+void
+samp_build_PC_sample(PINT_FRAME int_frame, P_sample_record_PC p_sample)
+/*++
+
+Routine description:
+
+    Build a PC sample record containing...
+    cs,eip,eflags,code seg descriptor, pid, cpu#
+
+Arguments:
+
+return value:
+
+--*/
+{
+    KXDESCRIPTOR ucsd;  // unscrambled code segment descriptor
+    PKGDTENTRY pcsd;    // IA32 scrambled format code segment descriptor
+
+    p_sample->cpu_and_OS = 0;   // init bit fields
+    p_sample->bit_fields2 = 0;
+
+    //
+    //  Build PC sample record based on addressing mode at the time
+    //  of the profile interrupt.
+    //
+    //  IPSR.is... 0=Itanium(R) processor, 1=IA32
+    //
+    if (!(int_frame->ipsr.quad_part & IA64_PSR_IS)) {   // check addressing mode at time of profile int (Itanium(R) instructions or IA32)
+        //
+        // Build PC sample record for Itanium(R) instructions ...
+        //                                                  // save Itanium(R) processor cpu state at time of profile interrupt
+        p_sample->iip.quad_part = int_frame->iip.quad_part; // Itanium(R) processor ip...... IIP
+        p_sample->ipsr.quad_part = int_frame->ipsr.quad_part;   // Itanium(R) processor psr..... IPSR
+        p_sample->itp_pc = TRUE;
+    } else {
+        //
+        // Build IA32 PC sample
+        //
+        p_sample->eip = int_frame->iip.low_part;    // IA32 eip..... IIP
+        p_sample->cs = (__u16) int_frame->seg_cs;   // IA32 cs...... GR17 15:0
+        p_sample->eflags = int_frame->E_flags;  // IA32 eflags.. AR24
+        //
+        //  Store 8 byte scrambled IA32 code segment descriptor (CSD) in the PC sample record.
+        //
+        //  AR25 = unscrambled CSD
+        //  p_sample->csd = scrambled CSD
+        //
+        ucsd.words.descriptor_words = int_frame->csd;   // IA32 unscrambled code seg descriptor... AR25
+        pcsd = (PKGDTENTRY) & p_sample->csd;    // pcsd = address of CSD in PC sample record
+        pcsd->limit_low = (__u16) ucsd.words.bits.limit;    // copy unscrambled CSD to scrambled CSD in PC sample record
+        pcsd->base_low = (__u16) ucsd.words.bits.base;  // ..
+        pcsd->high_word.bits.base_mid = (char) (ucsd.words.bits.base >> 16);    // ..
+        pcsd->high_word.bits.type = (__u32) ucsd.words.bits.type;   // ..
+        pcsd->high_word.bits.dpl = (__u32) ucsd.words.bits.dpl; // ..
+        pcsd->high_word.bits.pres = (__u32) ucsd.words.bits.pres;   // ..
+        pcsd->high_word.bits.sys = (__u32) ucsd.words.bits.sys; // ..
+        pcsd->high_word.bits.limit_hi = (__u32) (ucsd.words.bits.limit >> 16);  // ..
+        pcsd->high_word.bits.reserved_0 = (__u32) ucsd.words.bits.reserved_0;   // ..
+        pcsd->high_word.bits.default_big = (__u32) ucsd.words.bits.default_big; // ..
+        pcsd->high_word.bits.granularity = (__u32) ucsd.words.bits.granularity; // ..
+        pcsd->high_word.bits.base_hi = (char) (ucsd.words.bits.base >> 28); // ..
+    }
+
+    p_sample->cpu_num = (__u16) get_current_processor_number();
+    p_sample->pid_rec_index_raw = 1;    // pid contains OS pid
+#ifdef ENABLE_TGID
+    p_sample->pid_rec_index = get_thread_group_id(current);
+#else
+    p_sample->pid_rec_index = tbs_get_current_process_id();
+#endif
+    p_sample->tid = tbs_get_current_thread_id();
+
+    return;
+}
+
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void samp_add_ear_data(P_sample_record_PC p_sample, __u32 counter)
+ * @brief       Add ear data (data, instruction, branch) to end of sample record
+ *
+ * @param       p_sample	IN - address sample record
+ * @param       counter	    IN - index of counter that caused interrupt
+ *
+ * @return	none
+ *
+ * <I>Special Notes:</I>
+ *
+ *	Plays with the actual IIP and IPSR to make those returned values
+ *  have particular meaning for EAR events. However, we don't
+ *  always do it in a uniform manner...
+ *
+ */
+void
+samp_add_ear_data(P_sample_record_PC p_sample, __u32 counter)
+{
+    __u64 *p_data, pmd;
+    __u32 i;
+
+    //
+    // KLUDGE WARNING:
+    //
+    // The IIP and IPSR are already captured at this point based
+    // on the trap frame. However, for EAR data, the trap frame
+    // doesn't have interesting information since the EAR event
+    // happened earlier.
+    //
+    // Alas, the downstram tools are dependant on the IIP and IPSR
+    // having information for the particular event. So... This routine
+    // modifies the IIP and IPSR to make the downstream code happy. A
+    // side effect of this is an inconsistant view from the downstream code...
+    // For most events, the IIP does not include the slot information (since
+    // the slot information is in the ipsr, not the iip). But, for the data
+    // ear event, the code puts the slot information into both the ipsr
+    // AND the iip... 
+    // 
+    // Thus, downstram tools should be looking at the iip and ipsr to get
+    // the correct information, but any tool that blindly prints out the iip
+    // and uses the ipsr for the slot will show the slot data twice... Tools
+    // like sfdump, for example.
+    //
+    // The reason for not always putting the slot in the iip is so that
+    // the driver is providing the actual trap frame iip under normal
+    // circumstances.
+    //
+    // The reason for never putting the slot in the iip is because some
+    // tools only look at the iip and to avoid rewriting those tools, the
+    // slot is put in the iip for the data ears (which is where it tends
+    // to be the most useful due to "skid" for normal events).
+    //
+    // So, we have the inconsistant situation... A strong argument can be made that
+    // the downstram tools should change so that if they care about the slot they
+    // should look for it in the right place...
+    //
+
+    if (counter_options[counter] & COUNTER_OPTS_COLLECT_IEAR_REGS) {    
+        //
+        //  Treat Instruction cache line address as IIP
+        //  Copy cache line address
+        //
+        pmd = itp_read_reg_pmd(0);
+        p_sample->iip.quad_part = pmd & (~0x1f);    // clear low order bits (bits 4-2 ignored, 1 tlb, 0 v)  05-23-00
+        p_sample->ipsr.quad_part &= ~((__u64) 3 << 41); // Itanium(R) processor psr..... IPSR. Clear ri bits in ipsr
+        // this is a kludge since the trapframe ipsr is not related
+        // to the cache line address (pmd0)
+        p_sample->itp_pc = TRUE;
+
+        if (iear_offset) {
+            p_data =
+                (__u64 *) ((__u64) p_sample + iear_offset);
+            p_data[0] = pmd;
+            p_data[1] = itp_read_reg_pmd(1);
+        }
+    }
+
+    if (counter_options[counter] & COUNTER_OPTS_COLLECT_DEAR_REGS) {    
+        //
+        //  Copy Data EAR Instruction address to IIP field and
+        //  treat it as a 64 bit sample
+        //
+        pmd = itp_read_reg_pmd(17);
+        
+        //
+        // See kludge warning above. This is setting the iip and ipsr
+        // to make downstream tools happy...
+        //
+
+        //
+        // set instruction address but clear slot and v bits
+        //
+        p_sample->iip.quad_part = pmd & (~0xf);
+
+        //
+        // set slot number in instruction address
+        // (pmd17 bits 3-2 Itanium(R) processor, bits 1-0 Itanium(R) 2 processor)
+        //
+        // KLUDGE WARNING:
+        // This is DIFFERENT than all other IIP!
+        // All other IIP have just bundle information and
+        // no slot information, but for data
+        // EARs we are also adding the slot into the IIP.
+        //
+        p_sample->iip.quad_part |= (pmd & (pmd17_slot_mask)) >> pmd17_slot_shift;
+
+        //
+        // If the bundle bit is set, we need to bump the
+        // instruction address by one bundle (16 bytes)
+        //
+        if (pmd & pmd17_bundle_mask) {
+            p_sample->iip.quad_part += 16;
+        }
+
+        //
+        // Per the kludge warning above,  the data ear does have an exact
+        // bundle and slot but the tools are expecting the bundle to be
+        // in iip and the slot to be in psr. The code above put the bundle
+        // (plus more) into the iip, the code below is updating the
+        // psr that was grabbed in the trapframe to have the correct slot
+        // information for the data ear event.
+        // NOTE: the rest of the information in the PSR may not be accurate
+        // since it was captured at the time of the interrupt, not at
+        // the time of the data ear event...
+        //
+
+        //
+        // Clear the RI bits (slot information)
+        //
+        p_sample->ipsr.quad_part &= ~ITP_IPSR_RI_MASK;
+
+        //
+        // Then put in the slot information we have from
+        // the actual data ear event
+        //
+        p_sample->ipsr.quad_part |= ((pmd & (pmd17_slot_mask)) >>
+                                            pmd17_slot_shift) <<
+                                            ITP_IPSR_RI_SHIFT;
+
+        p_sample->itp_pc = TRUE;    // treat as 64 bit
+
+        if (dear_offset) {
+            p_data =
+                (__u64 *) ((__u64) p_sample + dear_offset);
+            p_data[0] = itp_read_reg_pmd(2);
+            p_data[1] = itp_read_reg_pmd(3);
+            p_data[2] = pmd;
+        }
+    }
+
+    if (counter_options[counter] & COUNTER_OPTS_COLLECT_BTRACE_REGS) {  
+        if (btrace_offset) {
+            p_data =
+                (__u64 *) ((__u64) p_sample + btrace_offset);
+            p_data[0] = itp_read_reg_pmd(8);
+            p_data[1] = itp_read_reg_pmd(9);
+            p_data[2] = itp_read_reg_pmd(10);
+            p_data[3] = itp_read_reg_pmd(11);
+            p_data[4] = itp_read_reg_pmd(12);
+            p_data[5] = itp_read_reg_pmd(13);
+            p_data[6] = itp_read_reg_pmd(14);
+            p_data[7] = itp_read_reg_pmd(15);
+            p_data[8] = itp_read_reg_pmd(16);
+
+            //
+            // To make the IIP from the BRANCH_EVENT have more
+            // meaning, we are substituting the IIP
+            // under certain specific circumstances...
+            //
+            // 1) Must be on a Itanium(R) 2 processor (or other system with the DS bit)
+            // 2) the buffer must have overflowed (this is done to prevent
+            //      using bad/stale information and is likely to happen
+            //      all the time unless the sampleafter value is unusually
+            //      small...
+            // 3) The pmc12.ds bit is zero.
+            //
+            // When all the conditions are met, we search for a valid
+            // target address (mp == 1, b ==0) starting with PMD8. For
+            // histogram type information, any PMD is as valid as any
+            // other (statistically speaking) so we can start with PMD8
+            // all the time and not worry about parsing the pmd16 first/last
+            // information.
+            //
+            // Finally, when we copy the PMD on top of the IIP, we do
+            // no need to worry about the PMD16.ext bits since we are only
+            // working with valid target address. This also means
+            // the slot information will always be zero since you can't
+            // branch into the middle of a bundle, so we can
+            // avoid doing slot fixups and just zero the slot areas...
+            //
+            // TODO: Revisit this when we have a follow on processor
+            // to the Itanium(R) 2 processor ...
+            //
+            // TODO: Possible changes
+            //          use global variable instead of reading
+            //          pmc12 (help with PMU latency)
+            //
+            //          Zero the BTB PMD's (8-15) and skip the
+            //          full check to get all data... Probably
+            //          not worth the cycles for the rare case this
+            //          algorithm would miss...
+            //
+
+            if ((ITP_CPU_FAMILY_ITANIUM2 == cpu_family) &&
+                        (ITANIUM2_PMD16_FULL_MASK & p_data[8])) {
+                pmd = itp_read_reg_pmc(12);
+                if (0 == (pmd & ITANIUM2_PMC12_DS_MASK)) {
+                    //
+                    // See if we have a better choice for the iip
+                    // than the current one by looking at the
+                    // pmd registers we just saved
+                    //
+                    for (i = 0; i < ITANIUM2_NUM_BTBREGS; i++) {
+                        //
+                        // Need branch bit (bit 0) == 0 and Mispredict Bit
+                        // (bit 1) == 1 to have a valid target address
+                        //
+                        if ((p_data[i] & (ITANIUM2_BTB_MP_MASK | ITANIUM2_BTB_B_MASK)) == 0x2) {
+                            //
+                            // Start with the target address
+                            //
+                            p_sample->iip.quad_part = p_data[i];
+
+                            //
+                            // Strip out the crud at the bottom (slot, mp, b)
+                            //
+                            p_sample->iip.quad_part &= ~((__u64) ITANIUM2_BTB_SLOT_MASK |
+                                    ITANIUM2_BTB_MP_MASK |
+                                    ITANIUM2_BTB_B_MASK);
+
+#if defined(DEBUG)
+                            //
+                            // As noted before, because we are using
+                            // only valid target address, the slot bits
+                            // should be zero
+                            //
+                            if (p_data[i] & ITANIUM2_BTB_SLOT_MASK) {
+                                VDK_PRINT_WARNING("valid target address had a non-zero slot (0x%p)\n", p_data[i]);
+                            }
+#endif
+
+                            //
+                            // And make sure we update the ipsr to have the
+                            // correct slot info as well in case someone
+                            // is looking there for information (which
+                            // is actually where it is supposed to be)
+                            //
+                            p_sample->ipsr.quad_part &= ~ITP_IPSR_RI_MASK;
+
+                            break;
+                        }
+                    }
+                }
+
+                //
+                // Now that we've looked through the BTB for something
+                // interesting, reset PMD16 since the full bit is sticky...
+                //
+                // TODO: Should we be resetting this all the time
+                // after the BRANCH_EVENT interrupt? My initial reaction
+                // is yes, but need to think about it further...
+                //
+                pmd = p_data[8] & ~((__u64) ITANIUM2_PMD16_BBI_MASK | ITANIUM2_PMD16_FULL_MASK);
+                itp_write_reg_pmd(16, pmd);
+            }
+        }
+    }
+
+    return;
+}
+
+void
+samp_build_PC_sample_ex(PINT_FRAME int_frame, P_sample_record_PC p_sample)
+/*++
+
+Routine description:
+
+    This routine is called to build a PC sample and save
+    it in the sample buffer. A spinlock is used to serialize
+    between multiple profile interrupt handlers.
+
+Arguments:
+
+return value:
+
+    none
+
+--*/
+{
+    __u64 *p_ITC;
+
+    if (p_sample) {
+        p_ITC = 0;  
+        samp_build_PC_sample(int_frame, p_sample);
+        if (sample_tsc) {
+            p_ITC = (__u64 *) ((__u32_PTR) p_sample + sample_tsc_offset);  
+            *p_ITC = itp_get_itc();   
+        }
+    }
+
+    return;
+}
+
+u32
+samp_emon_interrupt(PINT_FRAME int_frame)
+/*++
+
+Routine description:
+
+    Handle cpu counter overflow interrupt. This routine is called
+    with the SamplLock held and cpu emon counters stopped.
+
+Arguments:
+
+return value:
+
+    none
+
+--*/
+{
+    __u32 i, events = 0, wake_up_thread = FALSE;
+    P_sample_record_PC p_sample, p_sample2;
+    ULARGE_INTEGER pmc0;
+    __u32 my_cpu = get_current_processor_number();
+
+    if (my_cpu >= MAX_PROCESSORS)  // ignore if sample is on a CPU larger than MAX_PROCESSORS
+    {
+      return FALSE;
+    }
+
+    pmc0.quad_part = itp_read_reg_pmc(0);   // read counter overflow status register
+    events = ((pmc0.low_part & valid_counters) >> 4) & 0xf;
+
+    add_to_counter_total_on_overflow(pmc0, my_cpu);
+
+    // samp_get_sample_lock();
+
+    if (check_pause_mode()) {
+        sample_skipped();
+        events = 0;
+    }
+
+    p_sample = 0;
+    if (events) {
+        p_sample = samp_get_buf_space(sample_rec_length, &wake_up_thread);
+    }
+    if (p_sample) {
+        samp_build_PC_sample_ex(int_frame, p_sample);
+        for (i = 0; i < MAX_ACTIVE_EVENTS; i++, events = events >> 1) {
+            if (events & 1) {
+                break;
+            }
+        }
+        p_sample->event_index = event_Ids[i];
+        if (sample_options && (counter_options[i] & COUNTER_OPTS_EAR)) {
+            samp_add_ear_data(p_sample, i);
+        }
+        events = events >> 1;
+        i++;
+
+        for (; events && (i < MAX_ACTIVE_EVENTS);
+             i++, events = events >> 1) {
+            if (events & 1) {
+                p_sample2 = samp_get_buf_space(sample_rec_length, &wake_up_thread);
+                if (p_sample2) {
+                    memcpy(p_sample2, p_sample,
+                           sample_rec_length);
+                    p_sample2->event_index = event_Ids[i];
+                    if (counter_options[i] &
+                        COUNTER_OPTS_EAR) {
+                        samp_add_ear_data(p_sample2, i);
+                    }
+                } else {
+                    break;
+                }
+            }
+        }
+
+    }
+
+    return(wake_up_thread);
+}
diff -urN linux.orig/drivers/char/vtune/vtlibcommon32.c linux/drivers/char/vtune/vtlibcommon32.c
--- linux.orig/drivers/char/vtune/vtlibcommon32.c	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/vtlibcommon32.c	2004-04-21 14:44:23.000000000 -0700
@@ -0,0 +1,2802 @@
+/*
+ *  vtlibcommon32.c
+ *
+ *  Copyright (C) 2002-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: vtlibcommon32.c
+ *
+ *	Description: functions for manipulating the hardware registers on
+ *	             IA32 platforms
+ *
+ *	Author(s): George Artz, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#include <linux/vmalloc.h>
+#include <linux/slab.h>   
+#include <asm/uaccess.h>
+#include <asm/io.h>
+#include <asm/msr.h>
+#include <asm/desc.h>
+#include <asm/segment.h>
+
+#include "vtuneshared.h"
+#include "apic.h"
+#include "vtdef.h"
+#include "vtproto.h"
+#include "vtextern.h"
+#if defined(linux32)
+#include "vtasm32.h"
+#elif defined(linux32_64)
+#include "vtasm32_64.h"
+#endif
+#include "vtlibcommon32.h"
+
+#define _INIT_EMON_REGS_AT_START_OF_SAMPLING_SESSION 1
+
+#define PACKAGE_INIT_COMPLETE -1
+
+//
+// Data to save/restore Cpu event monitoring registers          // 05-31-00
+// accross an EBS session
+//
+__u32 num_emon_regs = 0;    // number of Emon regs
+__u32 *p_emon_regs = 0;     // emon regs to save/restore
+
+#if defined(ALLOW_LBRS)
+__u32 quick_freeze_msr = 0; // non-zero is msr used to freeze LBRs. Want
+			    //  this aligned on a 4-byte boundary if possible
+BOOLEAN	capture_lbrs = FALSE;	// should we capture LBRs on this run?
+#endif
+
+__u32 cccr_index = 0;       // index of first cccr in Emonregs array
+__u32 cccr_index_last = 0;  // index of last cccr in Emonregs array
+__u32 cccr_count = 0;       // number of cccrs in Emonregs array
+__u32 init_driver_var = 1;  // driver initialized flag
+
+char emon_options[] = "INITEMON";   // eyecatcher for patching
+__u32 init_emon_regs = 1;   // init all Emon regs at start of a sampling session
+__u32 reset_and_power_on_pubs = 1;
+
+
+
+extern BOOLEAN DTES_supported;  // cpu supports DTES feature        05-31-00
+extern BOOLEAN pebs_option; // precise event sampling active    05-31-00
+
+extern __u32 logical_processors_shift;
+
+extern BOOLEAN HT_supported;    // cpu supports Hyper-Threading Technology  05-31-00
+extern __u32 logical_processors_per_package;    // Pentium(R) 4 processor: logical processors per phycal processor
+extern __u32 pebs_err;      // error during precise EBS ISR     05-31-00
+
+void_ptr
+allocate_pool(__u32 pool_type, __u32 i)
+{
+    return (kmalloc(i, GFP_ATOMIC));
+}
+
+void
+free_pool(void_ptr i)
+{
+    kfree(i);
+
+    return;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          __u32 IA32_family_F_reg_type(__u32 reg)
+ * @brief       Checks if the msr provided is a legitmate PMU msr
+ *
+ * @param       reg	IN  - msr of interest
+ *
+ * @return	type of register or UNKNOWN_REG if not PMU msr
+ *
+ *
+ * <I>Special Notes:</I>
+ *
+ *	Tied to current Pentium(R) 4 processor. Will likely need to be
+ *      changed on future processors.
+ *
+ * @todo        Convert to use ptr to array of legit msr's chosen at boot time
+ *
+ */
+__u32
+IA32_family_F_reg_type(__u32 reg)
+{
+    if ((reg >= ESCR_FIRST) && (reg <= ESCR_LAST)) {
+        //
+        // On Linux*, skip regs that are in the ESCR range but are not implemented
+        // on Pentium(R) 4 processors
+        if ((reg == 0x3BF) || (reg == 0x3C6) || (reg == 0x3C7)
+            || (reg == 0x3CE) || (reg == 0x3CF) || ((reg >= 0x3D0)
+                                && (reg <= 0x3DF))) {
+            return (UNKNOWN_REG);
+        }
+        return ESCR_REG;
+    }
+
+    if ((reg >= CCCR_FIRST) && (reg <= CCCR_LAST)) {
+        return CCCR_REG;
+    }
+
+    if ((reg >= COUNT_FIRST) && (reg <= COUNT_LAST)) {
+        return COUNT_REG;
+    }
+
+    if (reg == CRU_CR_PEBS_MATRIX_HORIZ) {
+        return PEBS_REG;
+    }
+
+    if (reg == CRU_CR_PEBS_MATRIX_VERT) {
+        return PEBS_REG;
+    }
+
+    return UNKNOWN_REG;
+}
+
+inline
+ULARGE_INTEGER
+samp_read_msr(__u32 reg)
+{
+    ULARGE_INTEGER val;
+
+    rdmsr(reg, val.low_part, val.high_part);
+
+    return (val);
+}
+
+inline
+void
+samp_write_msr(__u32 reg, ULARGE_INTEGER val)
+{
+    wrmsr(reg, val.low_part, val.high_part);
+
+    return;
+}
+
+__u32
+DTS_get_CPU_features(void)
+/*++
+Routine description:
+    This routine is called by the DTSValidateprocessor()
+    Calls the cpuid and returns the processor features
+
+  Arguments:
+    none
+
+return value:
+    The feature bits
+
+--*/
+{
+    return (cpuid_edx(1));
+}
+
+
+BOOLEAN
+known_reg(__u32 reg)
+{
+    if (IA32_family6) {
+        return ((reg == 0xC1) || (reg == 0xC2)) ? TRUE : FALSE;
+    }
+
+    if (IA32_familyF) {
+        return ((IA32_family_F_reg_type(reg) == UNKNOWN_REG) ? FALSE : TRUE);
+    }
+
+    return (FALSE);
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          BOOLEAN OS_safe_RDMSR(__u32 reg, PULARGE_INTEGER pui64_ret_value)
+ * @brief       See if the msr is a PMU msr before doing an msr read
+ *
+ * @param       reg		IN  - msr of interest
+ * @param       pui64_ret_value OUT - stores the result of the msr read
+ *
+ * @return	true if the msr is allowed, false otherwise
+ *
+ * Routine doesn't touch pui64_ret_value if the msr is not valid
+ *
+ * <I>Special Notes:</I>
+ *
+ *	None
+ *
+ */
+BOOLEAN
+OS_safe_RDMSR(__u32 reg, PULARGE_INTEGER pui64_ret_value)
+{
+    if (!known_reg(reg)) {
+        return (FALSE);
+    }
+
+    *pui64_ret_value = samp_read_msr(reg);
+
+    return (TRUE);
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          BOOLEAN OS_safe_WRMSR_direct(__u32 reg, PULARGE_INTEGER pui64_ret_value)
+ * @brief       See if the msr is a PMU msr before doing an msr read
+ *
+ * @param       reg		IN - msr of interest
+ * @param       pui64_ret_value IN - the value to write
+ *
+ * @return	true if the msr is allowed, false otherwise
+ *
+ * <I>Special Notes:</I>
+ *
+ *	None
+ *
+ */
+BOOLEAN
+OS_safe_WRMSR_direct(__u32 reg, PULARGE_INTEGER pui64_new_value)
+{
+    if (!known_reg(reg)) {
+        return (FALSE);
+    }
+
+    samp_write_msr(reg, *pui64_new_value);
+
+    return (TRUE);
+}
+
+
+/*++
+Routine description:
+    This routine is called by the Mapprocessors function.
+    It calls the CPUID and returns the information in
+    the EBX register which includes the number of logical
+    processors per physical package and initial APIC ID
+
+  Arguments:
+    none
+
+return value:
+    The feature bits
+
+--*/
+__u32
+get_APICID(void)
+{
+    __u32 features_ebx;
+
+    features_ebx = cpuid_ebx(1);
+
+    return features_ebx;
+}
+
+void
+samp_set_PEBS_this_thread(BOOLEAN enable)
+{
+    ULARGE_INTEGER val;
+
+    // 
+    // CRU_CR_PEBS_MATRIX_HORIZ is per package and 
+    // writes need to be serialized by spinlock. 
+    // It would be more efficient to have a seperate 
+    // spinlock per package, but this will do for now. 
+    //
+    // Note:
+    // We don't want to change the PEBS state for the 
+    // other thread. Bit 26 is controls and reflects
+    // PEBS state of the other thread.
+    //
+    spin_lock(&reg3f1_write_lock);
+
+    val = samp_read_msr(CRU_CR_PEBS_MATRIX_HORIZ);
+    if (enable) {
+        val.low_part |= ENABLE_EBS_MY_THR;
+    } else {
+        val.low_part &= ~ENABLE_EBS_MY_THR;
+    }
+
+    samp_write_msr(CRU_CR_PEBS_MATRIX_HORIZ, val);
+    spin_unlock(&reg3f1_write_lock);
+
+    return;
+}
+
+void
+samp_disable_PEB_sand_DTS(void)
+{
+    ULARGE_INTEGER val;
+
+    samp_set_PEBS_this_thread(FALSE);
+
+    val = samp_read_msr(DEBUG_CTL_MSR);
+    val.low_part &= ~(debug_ctl_msr_DTS);
+    samp_write_msr(DEBUG_CTL_MSR, val);
+
+    return;
+}
+
+/*++
+Routine description:
+    This routine clears the enable and overflow bits in the CCCR.
+    It handles both methods of clearing the overflow bit on Pentium(R) 4 processor.
+    Write of CCCR.OVF=0 clears OVF on Pentium(R) 4 processors with stepping C0 and newer.
+    Write of CCCR.OVF=1 clears OVF on Pentium(R) 4 processors with pre C0 stepping.
+
+  Arguments:
+    cccr_reg
+
+return value:
+    TRUE... if overflow bit was set before counting disabled
+
+--*/
+BOOLEAN
+CCCR_disable_counting_and_clear_ovf(__u32 cccr_reg)
+{
+    BOOLEAN ovf;
+    ULARGE_INTEGER cccr, cccr2;
+
+    ovf = FALSE;
+
+    //
+    // Disable counting and clear overflow interrupt status bit
+    // The method of clearing the CCCR OVF bit changed between 
+    // Pentium(R) 4 processor steppings...
+    // steppings up to C0 - OVF is cleared by writing a 1 to OVF bit
+    // steppings C0 and newer - OVF is cleared by writing a 0 to OVF bit
+    //
+    // On C0 and newer parts, write of the CCCR OVF bit is latached.
+    // On pre C0 parts, write of a 0 to CCCR OVF is a don't care and does not 
+    // change the CCCR OVF bit if it is 1
+    //
+    cccr = samp_read_msr(cccr_reg);
+    cccr2 = cccr;
+    cccr2.low_part &= ~(CCCR_Overflow_MASK | CCCR_Enable_MASK); // disable counting on all Pentium(R) 4 processors.  Clear OVF on all Pentium(R) 4 processors except those with pre C0 stepping
+    samp_write_msr(cccr_reg, cccr2);
+    if (cccr.low_part & CCCR_Overflow_MASK) {
+        ovf = TRUE;
+    }
+    //
+    // Clear CCCR OVF bit on pre C0 parts by writing CCCR OVF as a 1
+    //
+    // Note: GA Feb 10, 2002
+    // Just discovered that the CCCR.Enable bit must be on to clear 
+    // OVF bit on Pentium(R) 4 processor pre C0 parts. To handle this counting is disabled by
+    // setting CCCR.Complement=0, CCCR.Compare=1 and CCCR.threshold=F. This effectively stops
+    // counting but leaves the CCCR/PUB enabled so that CCCR.OVF can be cleared.
+    //
+    cccr = samp_read_msr(cccr_reg);
+    if (cccr.low_part & CCCR_Overflow_MASK) {
+        cccr2 = cccr;
+        cccr2.low_part &= ~CCCR_Complement_MASK;    // set compare=1, complement=0, and threashold=F to stop counting 
+        cccr2.low_part |= (CCCR_threshold_MASK | CCCR_Compare_MASK | CCCR_Enable_MASK); // ..(complement=0 means use > comparison with threshold which an't happen)
+        samp_write_msr(cccr_reg, cccr2);    // ..write OVF=1 to clear OVF on Pentium(R) 4 processor pre C0
+        cccr.low_part &= ~(CCCR_Overflow_MASK | CCCR_Enable_MASK);  // disable counting and write OVF=0 to make sure OVF is clear.
+        samp_write_msr(cccr_reg, cccr); // ..for Pentium(R) 4 processor with C0 stepping or newer, there is a very small window where could have come on during initial write to disable (above this if statement)
+        ovf = TRUE;
+    }
+
+    return (ovf);
+}
+
+void
+reset_and_power_on_pub_counter(__u32 cccr_reg)
+{
+    ULARGE_INTEGER cccr;
+
+    if (reset_and_power_on_pubs) {
+        CCCR_disable_counting_and_clear_ovf(cccr_reg);  // disable counting and clear overflow bit
+        cccr = samp_read_msr(cccr_reg); // enable CCCR but disable counting by setting
+        cccr.low_part &= ~CCCR_Complement_MASK; // ..set compare=1, complement=0, and threashold=F 
+        cccr.low_part |= (CCCR_threshold_MASK | CCCR_Compare_MASK | CCCR_Enable_MASK);  // ..write OVF=1 to clear OVF on Pentium(R) 4 processor pre C0
+        samp_write_msr(cccr_reg, cccr); // ..
+    }
+
+    return;
+}
+
+void
+init_EBS_regs_for_package(void)
+{
+    __u32 i;
+
+    //
+    // write EBS regs that are written once per package
+    //
+    for (i = 0; (i < MAX_REG_SET_ENTRIES) && (reg_set_init[i].reg_num); i++) {
+        VDK_PRINT_DEBUG("init_EBS_regs_for_package: reg=%x  val=( 0x%x 0x%0x ) \n",
+			reg_set_init[i].reg_num,
+			reg_set_init[i].reg_val.high_part, reg_set_init[i].reg_val.low_part);
+        samp_write_msr(reg_set_init[i].reg_num, reg_set_init[i].reg_val);
+    }
+
+    return;
+}
+
+/*++
+
+Routine description:
+
+    Save Pentium(R) 4 processor emon regs that are shared by logical processors within a package.
+
+Arguments:
+
+return value:
+
+    none
+
+--*/
+void
+save_clear_init_emon_regs_for_package(BOOLEAN init_EBS_regs_for_package_bool)
+{
+    __u32 i, cpu, initial_apic_ID, package;
+    ULARGE_INTEGER val;
+    ULARGE_INTEGER *p_reg_SA;
+
+    if (!IA32_familyF) {
+        return;
+    }
+    if ((num_emon_regs == 0) || (p_emon_regs == 0)) {
+        return;
+    }
+    //
+    // get physical package ID. A package consists of 1 or more logical cpus
+    //
+    // On return from CPUID eax=1, ebx bits 31-24 contain the initial apic ID for the
+    // logical processor. The initial apic id is physical package id relative to 0
+    // with the low order bits containing the logical processor number within the package.
+    //
+    cpu = smp_processor_id();
+    if (cpu >= MAX_PROCESSORS) {
+        return;
+    }
+    initial_apic_ID = (get_APICID() >> 24) & 1; // NEED TO FIX!! assuming max of 2 logical processors for now
+    package = ((get_APICID() >> 24) >> logical_processors_shift) + 1;   // shift locigal proceoor number and add
+    // 1 to get package ID relative to 1
+
+    //
+    // We only want save and clear ESCR/CCCR regs once for a package, since these
+    // regs are shared between all logical processors in a package.
+    //
+    // The cpu number is saved in the package_status array to indicate
+    // that the save has been done for this package.
+    //
+    i = interlocked_exchange(&package_status[package], cpu + 1);    // add 1 to cpu to make sure it is > 0
+    if (i) {
+        while (TRUE) {
+            if (i == PACKAGE_INIT_COMPLETE) {
+                interlocked_exchange(&package_status[package], PACKAGE_INIT_COMPLETE);
+                return;
+            }
+            i = package_status[package];
+        }
+    }
+    EnablePerfVect(FALSE);
+
+    //
+    // Save Emon regs for this package
+    //
+
+    i = interlocked_exchange(&eachCPU[cpu].processor_status, PSTATUS_EMON_REGS_SAVED);
+    if (i == 0) {
+        if (!eachCPU[cpu].p_emon_reg_save_area) {
+            i = num_emon_regs * 8;
+            eachCPU[cpu].p_emon_reg_save_area = allocate_pool(non_paged_pool, i);
+            if (eachCPU[cpu].p_emon_reg_save_area) {
+                memset(eachCPU[cpu].p_emon_reg_save_area, 0, i);
+            }
+        }
+        if (eachCPU[cpu].p_emon_reg_save_area) {
+            p_reg_SA = eachCPU[cpu].p_emon_reg_save_area;
+            for (i = 0; i < num_emon_regs; i++) {
+                p_reg_SA[i] = samp_read_msr(p_emon_regs[i]);
+            }
+            VDK_PRINT_DEBUG("%d emon regs saved for cpu %d package %d \n", i, cpu, package);
+        } else {
+            VDK_PRINT_DEBUG("unable to allocate emon regSaveArea for cpu %d package %d\n", cpu, package);
+        }
+    } else {
+        VDK_PRINT_DEBUG("emon regs already saved for cpu %d package %d \n", cpu, package);
+    }
+
+#ifdef _INIT_EMON_REGS_AT_START_OF_SAMPLING_SESSION
+    //
+    // Clear EMON regs
+    //
+
+    VDK_PRINT_DEBUG("clearing Emon regs for cpu %d package %d \n", cpu, package);
+
+    //
+    // Make sure all pubs are powered on before clearing escr's.
+    //
+    // Note: 07/12/00
+    // Michael Cranford (MD6) says pubs can power off which can cause units/subunits to power off.
+    // If a pub is powered off when an escr is written, then the write may not take effect.
+    // For example, if an escr was set for tagging and the pub was powered off when the escr is cleared,
+    // then the tagging may continue after the clear of the escr. Sounds wierd.
+    //
+    if (init_emon_regs) {
+        //
+        // Make sure PUBS and Units are powered on when writing ESCRs
+        //
+        if (cccr_count) {
+            for (i = cccr_index; i <= cccr_index_last; i++) {
+                reset_and_power_on_pub_counter(p_emon_regs[i]);
+            }
+        }
+        //
+        // Zero all emon regs except CCCRs
+        //
+        val.quad_part = 0;
+        for (i = 0; i < num_emon_regs && p_emon_regs[i]; i++) {
+            //
+            // Skip CCCRs
+            //
+            if (cccr_count && (i >= cccr_index)
+                && (i <= cccr_index_last)) {
+                continue;
+            }
+            samp_write_msr(p_emon_regs[i], val);
+        }
+
+        //
+        // Init ESCRs while CCCR's are powered on
+        //
+        if (init_EBS_regs_for_package_bool) {
+            init_EBS_regs_for_package();
+            init_EBS_regs_for_package_bool = FALSE;
+        }
+        //
+        // Zero CCCRs that are not set by init_EBS_regs_for_package()
+        //
+        if (cccr_count) {
+            __u32 j;
+            BOOLEAN ebs_CCCR;
+
+            val.quad_part = 0;
+            for (i = cccr_index; i <= cccr_index_last; i++) {
+                ebs_CCCR = FALSE;
+                for (j = 0; (j < MAX_REG_SET_ENTRIES)
+                     && (reg_set_init[j].reg_num); j++) {
+                    if (reg_set_init[j].reg_num == p_emon_regs[i]) {
+                        ebs_CCCR = TRUE;
+                        break;
+                    }
+                }
+                if (ebs_CCCR) {
+                    continue;
+                }
+                samp_write_msr(p_emon_regs[i], val);
+            }
+        }
+
+    }
+#endif              // _INIT_EMON_REGS_AT_START_OF_SAMPLING_SESSION
+
+    if (init_EBS_regs_for_package_bool) {
+        init_EBS_regs_for_package();
+    }
+
+    interlocked_exchange(&package_status[package], PACKAGE_INIT_COMPLETE);
+
+    return;
+}
+
+__u32
+init_driver(void)
+{
+    __u32 i;
+    int ret_val;
+
+    ret_val = init_driver_OS();
+    if (ret_val != STATUS_SUCCESS) {
+        return ret_val;
+    }
+
+    /* Initialize system info storage */
+    memset(&samp_parms, 0, sizeof (samp_parm6));
+
+    max_counter.quad_part = IA32_FAMILY6_MAX_COUNTER;
+    switch (g_CPU_family) {
+    case 5:
+        IA32_family5 = TRUE;
+        break;
+    case 6:
+        IA32_family6 = TRUE;
+        break;
+    case 0xf:
+        IA32_familyF = TRUE;
+        break;
+    default:
+        break;
+    }
+    if (IA32_familyF) {
+        __u32 features;
+
+        //
+        // get CPU features
+        //
+        features = DTS_get_CPU_features();
+        if (features & CPUID_DTS_MASK) {    //21st Bit should be set
+            DTES_supported = TRUE;
+        } else {
+            DTES_supported = FALSE;
+        }
+        //
+        // Check if Hyper-Threading Technology is supported
+        //
+        if (features & CPUID_HT_MASK) { // 28th Bit should be set
+            HT_supported = TRUE;
+        } else {
+            HT_supported = FALSE;
+        }
+        logical_processors_per_package = 1;
+        logical_processors_shift = 1;
+        if (HT_supported) {
+            __u32 i;
+            logical_processors_per_package = (get_APICID() & 0xff0000) >> 16;
+            if (logical_processors_per_package == 0) {
+                logical_processors_per_package = 1;
+            }
+            i = logical_processors_per_package;
+            logical_processors_shift = 0;
+            i--;
+            while (1) {
+                logical_processors_shift++;
+                i >>= 1;
+                if (!i) {
+                    break;
+                }
+            }
+        }
+        VDK_PRINT_DEBUG("cpu features...  DTES=%d HT=%d logical_processors_per_package %d  Shift %d \n",
+			(DTES_supported) ? 1 : 0, (HT_supported) ? 1 : 0,
+			logical_processors_per_package, logical_processors_shift);
+        validate_emon_regs();
+    }
+
+    /* Initialize register Sets */
+    memset(reg_set, 0, sizeof (REG_SET) * MAX_REG_SET_ENTRIES);
+    memset(reg_set0, 0, sizeof (REG_SET) * MAX_REG_SET_ENTRIES);
+    memset(reg_set1, 0, sizeof (REG_SET) * MAX_REG_SET_ENTRIES);
+    memset(reg_set_init, 0, sizeof (REG_SET) * MAX_REG_SET_ENTRIES);
+
+    /* Initialize CPU Specific Sampling Defaults */
+    if (IA32_family6) {
+        rdpmc_msr_base = 0xC1;
+        set_IA32_family6_emon_defaults();
+    }
+    if (IA32_familyF) {
+        rdpmc_msr_base = 0x300; // 03-31-01
+        // set_IA32_family_F_emon_defaults();
+        // 03-31-00 Pentium(R) 4 processor
+    }
+
+    /* Initialize global sampling control flags */
+    samp_info.flags = SINFO_STOP_COMPLETE;
+    samp_info.sampling_active = FALSE;
+    signal_thread_event = FALSE;
+
+    /*   Initialize timers  and resources */
+    g_max_samp_timer_ON = FALSE;
+    g_start_delay_timer_ON = FALSE;
+    init_timer(&delay_tmr);
+    init_timer(&time_out_tmr);
+
+
+    for (i = 0; i < MAX_PROCESSORS; i++) {
+        eachCPU[i].DTES_buf = 0;    // 05-31-00 Pentium(R) 4 processor
+        eachCPU[i].org_dbg_ctl_msr_val.quad_part = 0;
+        eachCPU[i].org_DTES_area_msr_val.quad_part = 0;
+        eachCPU[i].p_emon_reg_save_area = 0;
+    }
+
+    for (i = 0; i < MAX_PROCESSORS; i++) {  //                             12-21-97
+        eachCPU[i].samp_EBS_idt_routine = (void *) t_ebs;
+        eachCPU[i].original_EBS_idt_entry = 0;
+        eachCPU[i].original_apic_perf_local_vector = 0;
+    }
+
+    //
+    // Allocate Driver Shared Area (DSA) in non paged pool.
+    //
+
+    pdsa = create_dsa();
+    if(pdsa == NULL)
+    {
+      VDK_PRINT_ERROR("init_driver: could not create DSA\n");
+      return (STATUS_INVALID_PARAMETER);
+    }
+    pdsa->method_EBS = TRUE;    // NEED TO FIX!! EBS should be set based on cpu features and EBS test
+    pdsa->module_tracking = TRUE;   // driver can do module tracking
+
+#ifdef USE_NMI
+    //
+    // Use nmi for counter overflow interrupt
+    //
+    //  apic perf lvt... PMI delivered as NMI
+    //
+    apic_perf_lvt = 0x400; // mode 100 : see page 8-16 in Intel(R) Pentium 4 Processor Manual Vol 3
+    VDK_PRINT("using NMI with apic_perf_lvt=0x%x\n",apic_perf_lvt);
+#else
+    apic_perf_lvt = APIC_PERFLVT_FIXED_ENA | PERF_MON_VECTOR;
+#endif
+
+    DoApicInit ();
+
+    return (STATUS_SUCCESS);
+}
+
+/*++
+
+Routine description:
+
+    restore Pentium(R) 4 processor emon regs for the current cpu.
+
+Arguments:
+
+return value:
+
+    none
+
+--*/
+void
+restore_emon_regs(void)
+{
+    __u32 i, j, cpu;
+    PULARGE_INTEGER p_reg_SA;
+
+    if (!IA32_familyF) {
+        return;
+    }
+    if ((num_emon_regs == 0) || (p_emon_regs == 0)) {
+        return;
+    }
+
+    cpu = smp_processor_id();
+    if (cpu >= MAX_PROCESSORS) {
+        return;
+    }
+
+    i = interlocked_exchange(&eachCPU[cpu].processor_status, 0);
+    if (i != PSTATUS_EMON_REGS_SAVED) {
+        VDK_PRINT_DEBUG("restore_emon_regs... no restore, regs not saved  cpu %d \n", cpu);
+        return;
+    }
+
+    p_reg_SA = eachCPU[cpu].p_emon_reg_save_area;
+    if (!p_reg_SA) {
+        VDK_PRINT_DEBUG("restore_emon_regs... no restore, no save area cpu %d \n", cpu);
+        return;
+    }
+    EnablePerfVect(FALSE);
+
+#ifdef _INIT_EMON_REGS_AT_START_OF_SAMPLING_SESSION
+    //
+    // Make sure all pubs are powered on before clearing escr's.
+    //
+    if (init_emon_regs) {
+        if (cccr_count) {
+            for (i = cccr_index; i <= cccr_index_last; i++) {
+                reset_and_power_on_pub_counter(p_emon_regs[i]);
+            }
+        }
+    }
+#endif              // _INIT_EMON_REGS_AT_START_OF_SAMPLING_SESSION
+
+    //
+    // restore all regs except CCCR's... restore escrs and other msrs
+    //
+    j = 0;
+    for (i = 0; i < num_emon_regs && p_emon_regs[i]; i++) {
+        if (cccr_count && (i >= cccr_index) && (i <= cccr_index_last)) {
+            continue;
+        }
+        samp_write_msr(p_emon_regs[i], p_reg_SA[i]);
+        j++;
+    }
+
+    //
+    // restore CCCRs last
+    //
+    if (cccr_count) {
+        for (i = cccr_index; i <= cccr_index_last; i++) {
+            samp_write_msr(p_emon_regs[i], p_reg_SA[i]);
+            j++;
+        }
+    }
+
+    {
+        __u32 initial_apic_ID, package;
+
+        initial_apic_ID = (get_APICID() >> 24) & 1; // NEED TO FIX!! assuming max of 2 logical processors for now
+        package = ((get_APICID() >> 24) >> logical_processors_shift) + 1;   // shift locigal proceoor number and add
+        VDK_PRINT_DEBUG("%d emon regs restored for package %d cpu %d \n", j, package, cpu);
+    }
+
+    return;
+}
+
+PREG_SET
+get_reg_set_table(void)
+{
+    REG_SET *p_reg_set;
+
+    p_reg_set = reg_set;
+    if (IA32_familyF) {
+        if (get_APICID() & (1 << 24)) {
+            p_reg_set = reg_set1;
+        } else {
+            p_reg_set = reg_set0;
+        }
+    }
+
+    return (p_reg_set);
+}
+
+void
+add_to_counter_total_on_overflow(PREG_SET p_reg_set, __u32 cpu)
+{
+    p_reg_set->event_total[cpu].quad_part += p_reg_set->event_inc.quad_part;
+
+    return;
+}
+
+//
+// Add the current PMC value to the counter total 
+//
+// This routine should be called once per cpu at the end of a sampling session.
+// The counters should be stopped before calling this routine.
+//
+//
+void
+add_to_counter_total_on_stop(void)
+{
+    __u32 i, cpu;
+    REG_SET *p_reg_set;
+    ULARGE_INTEGER val;
+
+    p_reg_set = get_reg_set_table();
+
+    cpu = smp_processor_id();
+    for (i = 0; i < MAX_REG_SET_ENTRIES && p_reg_set[i].reg_num; i++) {
+        if (!(p_reg_set[i].options & REG_SET_COUNTER)) {
+            continue;
+        }
+        val = samp_read_msr(p_reg_set[i].reg_num);
+        val.quad_part &= max_counter.quad_part;
+        if (val.quad_part >= p_reg_set[i].reg_val.quad_part) {
+            val.quad_part -= p_reg_set[i].reg_val.quad_part;
+        }
+        p_reg_set[i].event_total[cpu].quad_part += val.quad_part;
+        VDK_PRINT_DEBUG("event id %d pmc_msr %x inc %d\n",
+			p_reg_set[i].event_ID, p_reg_set[i].reg_num, p_reg_set[i].event_inc);
+        VDK_PRINT_DEBUG("event total 0x%x %08x\n",
+			p_reg_set[i].event_total[cpu].high_part, p_reg_set[i].event_total[cpu].low_part);
+
+    }
+
+    return;
+}
+
+/*
+ *
+ *  Function: read_cpu_perf_counters_for_current_cpu
+ *
+ *  Description: 
+ *  Put cpu perf counter data in caller's buffer.
+ *  Save current counter value and event total the session.
+ *
+ *  Parms:
+ *      Entry:      address of counter buffer
+ *  
+ *      Return:    
+ *
+ */
+void
+read_cpu_perf_counters_for_current_cpu(
+    void *info
+    )
+{
+    __u32 i, cpu, tmp_cpu_num, pmc_msr, cpu_mask, pmc_num;
+    PREG_SET preg_set;
+    ULARGE_INTEGER pmc_val, start_total, pmc_mask, pmc_mask_read;
+    RDPMC_BUF *pr_buf;
+
+    pr_buf = (RDPMC_BUF *)info;
+
+    if (!pr_buf)
+    {
+        return;
+    }
+
+    cpu = smp_processor_id();
+
+    if (cpu >= RDPMC_MAX_CPUS)
+    {
+      return;
+    }
+
+    cpu_mask = 1 << cpu;
+
+    pmc_mask_read.quad_part = 0;
+    //
+    // Update RDPMC data for current cpu. Skip update if we already did it
+    // 
+    if ((pr_buf->cpu_mask_in & cpu_mask) && !(pr_buf->cpu_mask_out & cpu_mask))   
+    {
+        //
+        // Fill in RDPMC buffer for current cpu
+        //
+        pmc_msr = rdpmc_msr_base;
+        preg_set = get_reg_set_table();
+        for (i=0; i < MAX_REG_SET_ENTRIES; i++, preg_set++) 
+        {
+            if (!(preg_set->options & REG_SET_COUNTER)) 
+            {
+                continue;
+            }
+            pmc_num = preg_set->reg_num - rdpmc_msr_base;
+            pmc_mask.quad_part = 1 << pmc_num;
+            //
+            // If pmc is not in regSet, skip it.
+            //
+            pmc_msr = preg_set->reg_num;
+            if (!(pr_buf->pmc_mask.quad_part & pmc_mask.quad_part))
+            {
+                continue;
+            }
+
+            //
+            // Since the routine can't handle larger than RDPMC_MAX_CPUS,
+            // we need to add the extra CPU data into the results
+            //
+	    for (tmp_cpu_num = cpu ; tmp_cpu_num < MAX_PROCESSORS; tmp_cpu_num += RDPMC_MAX_CPUS)
+	    {
+
+	      //
+	      // Read current pmc and update pmc total. If the total changed accross the read,
+	      // then a counteroverflow must have occured so we redo the read until we get a 
+	      // valid pair of values for pmc current/total.
+	      //
+	      do
+	      {
+                start_total = preg_set->event_total[tmp_cpu_num];
+                pmc_val = samp_read_msr(pmc_msr);
+	      } while (start_total.quad_part != preg_set->event_total[tmp_cpu_num].quad_part);
+	      //
+	      // Only update the current value once with a CPU within the normal
+	      // 0->RDPMC_MAX_CPUS limit
+	      //
+	      if (tmp_cpu_num < RDPMC_MAX_CPUS) {
+		pr_buf->pmc_values[pmc_num].pmc_val[cpu].pmc_current.quad_part = pmc_val.quad_part & max_counter.quad_part;
+	      }
+
+	      pr_buf->pmc_values[pmc_num].pmc_val[cpu].pmc_total.quad_part   += start_total.quad_part;
+	    }
+
+            pmc_mask_read.quad_part |= pmc_mask.quad_part;
+        }
+
+        //
+        // Return current value for pmc's that were not in the RegSet
+        //
+        // Note!! 
+        // On JT, the caller needs to be aware that the cpu perf counters are
+        // shared within a package. This will lead to pmc being read twice
+        // once for each logical cpu. The caller needs to take care not to double 
+        // count pmc values for a JT enabled package.
+        //
+        pmc_msr = rdpmc_msr_base;
+        pmc_mask.quad_part = 1;
+        for (i=0; i < RDPMC_MAX_PMCS; i++, pmc_msr++, pmc_mask.quad_part = pmc_mask.quad_part << 1)
+        {
+            if (!(pr_buf->pmc_mask.quad_part & pmc_mask.quad_part) ||
+                 (pmc_mask_read.quad_part & pmc_mask.quad_part))
+            {
+                continue;
+            }
+            if (OS_safe_RDMSR(pmc_msr, &pmc_val))
+            {
+                pr_buf->pmc_values[i].pmc_val[cpu].pmc_current.quad_part = pmc_val.quad_part & max_counter.quad_part;
+            }
+            else
+            {
+                pr_buf->pmc_values[i].pmc_val[cpu].pmc_current.quad_part = 0;
+            }
+	    pr_buf->pmc_values[i].pmc_val[cpu].pmc_total.quad_part = 0;
+        }
+
+        spin_lock(&rdpmc_lock);
+        pr_buf->cpu_mask_out |= cpu_mask;
+        spin_unlock(&rdpmc_lock);    
+    }
+
+    return;
+}
+
+
+/*
+**********************************************************************************************************
+
+    START/STOP EBS FUNCTIONS
+
+**********************************************************************************************************
+*/
+
+__u32
+samp_check_emon_counter_overflow_IA32_family5(void)
+{
+    return (0);     // Pentium(R) processor EBS not supported
+}
+
+void
+samp_stop_emon_IA32_family6(void)
+{
+    ULARGE_INTEGER esr0;
+
+    //
+    //  Disable counting for both counters
+    //
+    esr0 = samp_read_msr(EM_MSR_PESR0);
+    esr0.low_part &= ~EM_EN_MASK;
+    samp_write_msr(EM_MSR_PESR0, esr0);
+
+    return;
+}
+
+void
+samp_start_emon_IA32_family6(u32 do_stop)
+{
+    int i, cpu;
+    REG_SET *p_reg_set;
+    BOOLEAN enable_counters = FALSE, start_all_counters = FALSE;
+    ULARGE_INTEGER val;
+
+    if (do_stop) {
+        samp_stop_emon_IA32_family6();
+    }
+
+    cpu = smp_processor_id();
+    if (eachCPU[cpu].start_all) {
+        eachCPU[cpu].start_all = FALSE;
+        start_all_counters = TRUE;
+    }
+    //
+    //  Disable counting for both counters
+    //
+    val = samp_read_msr(EM_MSR_PESR0);
+    val.low_part &= ~EM_EN_MASK;
+    samp_write_msr(EM_MSR_PESR0, val);
+
+    //
+    // Set msrs for sampling
+    //
+    p_reg_set = reg_set;
+    for (i = 0; (i < MAX_REG_SET_ENTRIES) && (p_reg_set->reg_num); i++, p_reg_set++) {
+        val = p_reg_set->reg_val;
+        if (p_reg_set->reg_num == EM_MSR_PESR1) {
+            enable_counters = TRUE;
+        }
+        if (p_reg_set->reg_num == EM_MSR_PESR0) {
+            enable_counters = TRUE;
+            val.low_part &= ~EM_EN_MASK;
+        }
+        if (p_reg_set->options & REG_SET_COUNTER) {
+            if ((p_reg_set->c_ovf[cpu]) || start_all_counters) {
+                p_reg_set->c_ovf[cpu] = 0;
+            } else {
+                continue;
+            }
+        }
+        samp_write_msr(p_reg_set->reg_num, val);
+    }
+
+    //
+    // Fix EBS on new mobile processors
+    //
+    EnablePerfVect(TRUE);
+
+    //
+    // If either PESR0 or PESR1 was written, enable both counters
+    //
+    if (enable_counters) {
+        val = samp_read_msr(EM_MSR_PESR0);
+        val.low_part |= EM_EN_MASK;
+        samp_write_msr(EM_MSR_PESR0, val);
+    }
+
+    return;
+}
+
+__u32
+samp_check_emon_counter_overflow_IA32_family6(void)
+{
+    int i, cpu;
+    BOOLEAN ovf;
+    __u32 events = 0;
+    REG_SET *p_reg_set;
+    ULARGE_INTEGER val, esr0;
+
+    //
+    //  Disable counting for both counters
+    //
+    esr0 = samp_read_msr(EM_MSR_PESR0);
+    esr0.low_part &= ~EM_EN_MASK;
+    samp_write_msr(EM_MSR_PESR0, esr0);
+
+    cpu = smp_processor_id();
+    p_reg_set = reg_set;
+    for (i = 0; (i < MAX_REG_SET_ENTRIES) && (p_reg_set->reg_num); i++, p_reg_set++) {
+        ovf = FALSE;
+        if (p_reg_set->reg_num == EM_MSR_PCTR0) {
+            if (esr0.low_part & EM_INT_MASK) {
+                val = samp_read_msr(EM_MSR_PCTR0);
+                val.high_part &= max_counter.high_part;
+                if (val.quad_part < p_reg_set->reg_val.quad_part) {
+                    ovf = TRUE;
+                }
+            }
+        } else {
+            if (p_reg_set->reg_num == EM_MSR_PCTR1) {
+                val = samp_read_msr(EM_MSR_PESR1);
+                if (val.low_part & EM_INT_MASK) {
+                    val = samp_read_msr(EM_MSR_PCTR1);
+                    val.high_part &= max_counter.high_part;
+                    if (val.quad_part < p_reg_set->reg_val.quad_part) {
+                        ovf = TRUE;
+                    }
+                }
+            }
+
+        }
+        if (ovf) {
+            if (interlocked_exchange(&p_reg_set->c_ovf[cpu], 1) == 0) {
+                add_to_counter_total_on_overflow(p_reg_set, cpu);
+            }
+            events |= 1 << p_reg_set->event_I_dindex;
+        }
+    }
+
+    return (events);
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void samp_stop_emon_IA32_familyF(void)
+ * @brief       Stop the PMU for the Pentium(R) 4 processor family
+ *
+ * @return	none
+ *
+ * Heavily hard coded for early Pentium(R) 4 processors. Likely to change 
+ * on later Pentium(R) 4 processors...
+ *
+ * <I>Special Notes:</I>
+ *
+ *	None
+ *
+ */
+void
+samp_stop_emon_IA32_familyF(void)
+{
+    int i, cccr_reg, count_reg = 0, cpu;
+    BOOLEAN cindex_valid, ovf;
+    ULARGE_INTEGER cccr, cccr2, count, count2;
+    REG_SET *p_reg_set, *p_reg_set_tbl;
+#if defined(linux32)
+    PDTS_BUFFER p_DTES;
+#elif defined(linux32_64)
+    PIA32EM64T_DTS_BUFFER p_DTES;
+#endif
+
+#if defined(ALLOW_LBRS)
+    ULARGE_INTEGER val;
+#endif
+
+#ifdef USE_NMI
+    // temporarily disable if event is precise
+    if (pebs_option)
+      return;
+#endif
+
+    if (get_APICID() & (1 << 24)) {
+        p_reg_set = reg_set1;
+    } else {
+        p_reg_set = reg_set0;
+    }
+
+    p_reg_set_tbl = p_reg_set;
+    cpu = smp_processor_id();
+    for (i = 0; (i < MAX_REG_SET_ENTRIES) && (p_reg_set->reg_num); i++, p_reg_set++) {
+        if (p_reg_set->options & REG_SET_CCCR) {
+            ovf = FALSE;
+            cccr_reg = p_reg_set->reg_num;
+            if (p_reg_set->options & REG_SET_CINDEX_VALID) {
+                cindex_valid = TRUE;
+                count_reg = p_reg_set_tbl[p_reg_set->c_index].reg_num;
+                count = samp_read_msr(count_reg);
+            } else {
+                cindex_valid = FALSE;
+            }
+
+            //
+            // Disable counting and clear overflow interrupt status bit
+            // The method of clearing the CCCR OVF bit changed...
+            // steppings up to C0 - OVF is cleared by writing a 1 to OVF
+            // steppings C0 and newer - OVF is cleared by writing a 0 to OVF bit
+            //
+            // On C0 and newer parts, write of the CCCR OVF bit is latached.
+            // On pre C0 parts, write of a 0 to CCCR OVF is a don't care and does not 
+            // change the CCCR OVF bit if it is 1
+            //
+            // Note: GA Feb 10, 2002
+            // Just discovered that the CCCR.Enable bit must be on to clear 
+            // OVF bit on Pentium(R) 4 processor pre C0 parts. To handle this counting 
+            // is disabled by setting CCCR.Complement=0 and CCCR.threshold=F. 
+            // This effectively stops counting but leaves the CCCR/PUB enabled.
+            //
+            cccr = samp_read_msr(cccr_reg);
+            if (!(cccr.low_part & CCCR_Enable_MASK)) {
+                continue;
+            }
+
+            ovf = CCCR_disable_counting_and_clear_ovf(cccr_reg);
+
+            //
+            // At this point counting is stopped and the CCCR OVF bit has 
+            // been cleared. 
+            // 
+            // The logical overflow flag ("ovf") has been based on the CCCR OVF bit, but we need
+            // to treat the following two cases as overflow
+            //
+            // 1) C0 or newer cpu and counter overflowed between CCCR read and CCCR write that we just did to disable counting.
+            //    When this happens, the write to the CCCR to disable counting clears the CCCR OVF bit. To detect overflow in this 
+            //    case, we compare the counter before and after the CCCR write.
+            //
+            // 2) CCCR FOVF bit is set and the counter is different than the initial counter value
+            //
+            // Another special case:
+            //
+            //    If the CCCR overflow bit was set but counter is 0, then it's not time to take a sample.
+            //    We want to take a sample on the next occurence of the event. To do this we 
+            //    do not treat as overflow, but instead set the force overflow flag to get an interrupt 
+            //    on the next occurence of the event.
+            // 
+            //    Note:
+            //    On Pentium(R) 4 processor, the CCCR OVF bit is set when the counter increments
+	    //    from the max value to 0.
+            //    This does not cause an interrupt to be generated. A counter overflow interrupt is generated
+            //    when the event counter increments and either the CCCR OVF or FOVF bits are set.
+            //
+            //    
+            // 
+            if (cindex_valid) {
+                count2 = samp_read_msr(count_reg);
+                count2.high_part &= max_counter.high_part;
+                if (!ovf) {
+                    count.high_part &= max_counter.high_part;
+                    //
+                    // We need to handle the case where the counter may have overflowed between
+                    // the CCCR read and the CCCR write that we just did to disable counting.
+                    // 
+                    // To detect the overflow in this small window, we compare the event count reg
+                    // before and after disabling counting. If the current count is less than the count 
+                    // before sampling was stopped, then assume counter has overflowed.
+                    //
+                    if (count2.quad_part < count.quad_part) {
+                        ovf = TRUE;
+                    }
+                }
+                //
+                // If CCCR force overflag is set and the current value of the counter is not
+                // equal to the initial counter value, then treat it as if the counter overflowed.
+                //
+                if (cccr.low_part & CCCR_ForceOverflow_MASK) {
+                    if (count2.quad_part != p_reg_set_tbl[p_reg_set->c_index].reg_val.quad_part) {
+                        ovf = TRUE;
+                        //
+                        // Clear force overflow bit, if force overflow is set in the CCCR register
+                        // and the force overflow bit is not set in the user's original CCCR value.
+                        //
+                        if (cccr.low_part & CCCR_ForceOverflow_MASK) {
+                            if (!(p_reg_set->reg_val.low_part & CCCR_Overflow_MASK)) {
+                                cccr2 = samp_read_msr(cccr_reg);
+                                cccr2.low_part &= ~CCCR_ForceOverflow_MASK;
+                                samp_write_msr(cccr_reg, cccr2);
+                            }
+                        }
+                    }
+                } else {
+                    //
+                    // If the counter has overflowed but the counter is 0 and FOVF not set,
+                    // then it is not time to take a sample. Instead we set the 
+                    // force overflow bit in the CCCR and we will take a sample
+                    // next time the counter is incremented.
+                    //
+                    // Note: 
+                    // For PEBS, we should never hit this case, because 
+                    // we only support one event when doing PEBS.
+                    //
+                    if (ovf) {
+                        if (count2.quad_part == 0) {
+                            ovf = FALSE;
+                            cccr2 = samp_read_msr(cccr_reg);
+                            cccr2.low_part |= CCCR_ForceOverflow_MASK;
+                            samp_write_msr(cccr_reg, cccr2);
+                        }
+                    }
+                }
+            }
+            if (pebs_option) {
+                p_DTES = eachCPU[cpu].DTES_buf;
+                if (p_DTES) {
+                    if (p_DTES->PEBS_index != p_DTES->PEBS_base) {
+                        ovf = TRUE;
+                    }
+                }
+            }
+            if (ovf && cindex_valid) {
+                if (interlocked_exchange(&p_reg_set_tbl[p_reg_set->c_index].c_ovf[cpu], 1) == 0) {
+                    add_to_counter_total_on_overflow(&p_reg_set_tbl[p_reg_set->c_index], cpu);
+                }
+            }
+        }
+    }
+
+    if (pebs_option) {
+        samp_disable_PEB_sand_DTS();
+    }
+#if defined(ALLOW_LBRS)
+    // LBRs are per logical processor so need to clear per cpu
+    if (capture_lbrs) {
+	val = samp_read_msr(DEBUG_CTL_MSR);
+	val.quad_part &= ~debug_ctl_msr_LBR;
+	samp_write_msr(DEBUG_CTL_MSR, val);
+    }
+#endif
+
+    return;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void samp_start_emon_IA32_familyF(u32 do_stop)
+ * @brief       Stop the PMU for the Pentium(R) 4 processor family
+ *
+ * @return	none
+ *
+ * Heavily hard coded for early Pentium(R) 4 processors.  Likely to change
+ * on later Pentium(R) 4 processors...
+ *
+ * <I>Special Notes:</I>
+ *
+ *	None
+ *
+ * @todo        Get rid of some of the useless types (like u32 vs. __u32).
+ *
+ */
+void
+samp_start_emon_IA32_familyF(u32 do_stop)
+{
+    int i, cpu;
+#if defined(linux32)
+    PDTS_BUFFER p_DTES;
+#elif defined(linux32_64)
+    PIA32EM64T_DTS_BUFFER p_DTES;
+#endif
+    REG_SET *p_reg_set, *p_regs;
+    BOOLEAN start_all_counters = FALSE;
+
+#ifdef USE_NMI
+    // temporarily disable if event is precise
+    if (pebs_option)
+      return;
+#endif
+
+    if (do_stop) {
+        samp_stop_emon_IA32_familyF();
+    }
+    EnablePerfVect(FALSE);
+
+    cpu = smp_processor_id();
+    if (eachCPU[cpu].start_all) {
+        eachCPU[cpu].start_all = FALSE;
+        start_all_counters = TRUE;
+    }
+
+    if (pebs_option) {
+        if (cpu < MAX_PROCESSORS) {
+            p_DTES = eachCPU[cpu].DTES_buf;
+            if (p_DTES) {
+                p_DTES->PEBS_index = p_DTES->PEBS_base; // reset index to next PEBS record to base of buffer
+            }
+            samp_set_PEBS_this_thread(TRUE);
+        }
+    }
+
+    if (get_APICID() & (1 << 24)) {
+        p_reg_set = reg_set1;
+    } else {
+        p_reg_set = reg_set0;
+    }
+
+    p_regs = p_reg_set;
+    for (i = 0; (i < MAX_REG_SET_ENTRIES) && (p_reg_set->reg_num); i++, p_reg_set++) {
+        if (p_reg_set->options & REG_SET_CCCR) {
+            continue;
+        }
+        if (p_reg_set->options & REG_SET_COUNTER) {
+            if ((p_reg_set->c_ovf[cpu]) || start_all_counters) {
+                p_reg_set->c_ovf[cpu] = 0;
+            } else {
+                continue;
+            }
+        }
+        samp_write_msr(p_reg_set->reg_num, p_reg_set->reg_val);
+    }
+
+    p_reg_set = p_regs;
+    for (i = 0; (i < MAX_REG_SET_ENTRIES) && (p_reg_set->reg_num); i++, p_reg_set++) {
+        if (p_reg_set->options & REG_SET_CCCR) {
+            __u32 cccr_reg;
+            ULARGE_INTEGER cccr_val, cccr;
+
+            cccr_reg = p_reg_set->reg_num;
+            cccr_val = p_reg_set->reg_val;
+            if (!start_all_counters) {
+                cccr = samp_read_msr(cccr_reg);
+                if (cccr.low_part & CCCR_ForceOverflow_MASK) {
+                    cccr_val.low_part |= CCCR_ForceOverflow_MASK;
+                }
+            }
+            samp_write_msr(cccr_reg, cccr_val);
+        }
+    }
+#if defined(ALLOW_LBRS)
+    // LBRs are per logical processor so need to set per cpu
+    if (capture_lbrs) {
+	ULARGE_INTEGER val;
+
+	val = samp_read_msr(DEBUG_CTL_MSR);
+	val.quad_part |= debug_ctl_msr_LBR;
+	samp_write_msr(DEBUG_CTL_MSR, val);
+    }
+#endif
+    EnablePerfVect(TRUE);
+
+    return;
+}
+
+/*++
+Routine description:
+    This routine is returns
+
+  Arguments:
+    none
+
+return value:
+    A __u32 representing events for which a counter overflow
+    has occured. Each bit of the __u32 corresponds to an 
+    entry in the event_Ids[] table. 
+    Example: if bit0 of the returned __u32 is 1, then a counter
+    overflow has occured for the 1st event in the event_Ids[] table.
+
+--*/
+__u32
+samp_check_emon_counter_overflow_IA32_familyF(void)
+{
+    int i, cpu;
+    __u32 events = 0;
+    REG_SET *p_reg_set;
+
+    if (get_APICID() & (1 << 24)) {
+        p_reg_set = reg_set1;
+    } else {
+        p_reg_set = reg_set0;
+    }
+
+    cpu = smp_processor_id();
+    for (i = 0; (i < MAX_REG_SET_ENTRIES) && (p_reg_set->reg_num); i++, p_reg_set++) {
+        if (p_reg_set->options & REG_SET_COUNTER) {
+            if (p_reg_set->c_ovf[cpu]) {
+                events |= 1 << p_reg_set->event_I_dindex;
+            }
+        }
+    }
+
+    return (events);
+}
+
+void
+samp_start_emon(void *info)
+{
+    u32 do_stop;
+
+    do_stop = (info) ? TRUE : FALSE;
+
+    if (IA32_familyF) {
+        samp_start_emon_IA32_familyF(do_stop);
+    } else {
+        if (IA32_family6) {
+            samp_start_emon_IA32_family6(do_stop);
+        }
+    }
+
+    return;
+}
+
+void
+samp_stop_emon(void)
+{
+    if (IA32_familyF) {
+        samp_stop_emon_IA32_familyF();
+    } else {
+        if (IA32_family6) {
+            samp_stop_emon_IA32_family6();
+        }
+    }
+    return;
+}
+
+BOOLEAN
+validate_reg_RW(__u32 reg, ULARGE_INTEGER val)
+{
+    BOOLEAN ret = TRUE;
+    ULARGE_INTEGER org_val;
+
+    if (!OS_safe_RDMSR(reg, &org_val)) {
+        return (FALSE);
+    }
+    if (!OS_safe_WRMSR_direct(reg, &val)) {
+        ret = FALSE;
+    }
+    if (!OS_safe_WRMSR_direct(reg, &org_val)) {
+        ret = FALSE;
+    }
+
+    return (ret);
+}
+
+/*++
+
+Routine description:
+
+    Build array of Pentium(R) 4 processor emon registers that will be saved/restored
+    across an EBS sampling session. These are registers which are shared between
+    logical processors within a package. They are only saved once per package
+    when sampling is started.
+
+Arguments:
+
+return value:
+
+    none
+
+--*/
+void
+validate_emon_regs(void)
+{
+    __u32 reg, i, max_regs;
+    ULARGE_INTEGER val;
+
+    if (!IA32_familyF) {
+        return;
+    }
+
+    max_regs = NUM_ESCRS + NUM_CCCRS + 2 + 1;   // max emon regs to save/restore per package
+    // add two for CRU_CR_PEBS_MATRIX_HORIZ and CRU_CR_PEBS_MARTRIX_VERT
+    // add 1 for array termination
+    num_emon_regs = 0;
+    i = max_regs * 4;
+    p_emon_regs = allocate_pool(non_paged_pool, i);
+    if (!p_emon_regs) {
+        return;
+    }
+    memset(p_emon_regs, 0, i);
+
+    //
+    // Add valid ESCRs to emon reg array
+    //
+    val.quad_part = 0;
+    for (i = 0, reg = ESCR_FIRST; reg <= ESCR_LAST; reg++) {
+    //MSRs 0x3BA and 0x3BB have been removed in model 3
+        if ((g_CPU_model == 3) && ((reg == 0x3BA) || (reg == 0x3BB))){
+                continue;
+        }
+        if (validate_reg_RW(reg, val)) {
+            p_emon_regs[i] = reg;
+            i++;
+        } else {
+            VDK_PRINT_DEBUG("invalid ESCR %x \n", reg);
+        }
+    }
+
+    //
+    // Add valid CCCRs to emon reg array
+    //
+    VDK_PRINT_DEBUG("validate_emon_regs: validating CCCR's \n");
+    cccr_index = i;
+    cccr_count = 0;
+    for (reg = CCCR_FIRST; reg <= CCCR_LAST; reg++) {
+        if (validate_reg_RW(reg, val)) {
+            p_emon_regs[i] = reg;
+            i++;
+            cccr_count++;
+        } else {
+            VDK_PRINT_DEBUG("invalid CCCR %x \n", reg);
+        }
+    }
+    cccr_index_last = (cccr_count) ? (cccr_index + cccr_count - 1) : 0;
+
+    //
+    // Add PEBS_MATRIX regs to emon reg array
+    //
+
+    reg = CRU_CR_PEBS_MATRIX_HORIZ;
+    if (validate_reg_RW(reg, val)) {
+        p_emon_regs[i] = reg;
+        i++;
+    } else {
+        VDK_PRINT_DEBUG("invalid CRU_CR_PEBS_MATRIX_HORIZ %x \n", reg);
+    }
+    reg = CRU_CR_PEBS_MATRIX_VERT;
+    if (validate_reg_RW(reg, val)) {
+        p_emon_regs[i] = reg;
+        i++;
+    } else {
+        VDK_PRINT_DEBUG("invalid CRU_CR_PEBS_MATRIX_VERT %x \n", reg);
+    }
+    num_emon_regs = i;
+
+    return;
+}
+
+/*++
+
+Routine description:
+
+    This ensures that each entry in the regSet array is a vaild register.
+
+Arguments:
+
+return value:
+
+    0.......... all registers in the regSet array are valid
+    non-zero... index relative to one of the first invalid register in the regSet array.
+                (i.e. 1 means first entry in the regSet array)
+
+--*/
+__u32
+validate_EBS_regs(void)
+{
+    __u32 i, j;
+    REG_SET *p_reg_set = NULL;
+
+    j = 0;
+    while (j < 3) {
+        //
+        // Validate EBS regs
+        //
+        if (j == 0) {
+            //
+            // On IA32_familyF (Pentium(R) 4 processor), regset0, reg_set1 and regsetInit are
+            // used to hold reg for EBS.
+            // On other processors, regset and regsetInit are used and regset0 and regset1 will
+            // be empty and initialized to zeros.
+            //
+            if (IA32_familyF) {
+                p_reg_set = reg_set0;
+            } else {
+                p_reg_set = reg_set;    // regset is used on non-Pentium(R) 4 processors
+            }
+        }
+        if (j == 1) {
+            p_reg_set = reg_set1;
+        }
+        if (j == 2) {
+            p_reg_set = reg_set_init;
+        }
+        j++;
+
+        for (i = 0; (i < MAX_REG_SET_ENTRIES) && (p_reg_set->reg_num); i++, p_reg_set++) {
+            if (!validate_reg_RW(p_reg_set->reg_num, p_reg_set->reg_val)) {
+                VDK_PRINT_DEBUG("invalid reg for EBS... reg %x %p \n",
+				p_reg_set->reg_num, p_reg_set->reg_val);
+                return (++i);
+            }
+        }
+    }
+
+    return 0;       // all regs valid
+}
+
+void
+driver_load(void)
+{
+    return;
+}
+
+void
+driver_open(void)
+{
+    if (xchg(&init_driver_var, 0)) {
+        init_driver();
+    }
+
+    return;
+}
+
+void
+driver_unload(void)
+{
+    __u32 i;
+    void_ptr p;
+    int ret;
+//BSBS this is only for linux32 because older kernels (not x86-64) 
+//dont expose the virtual address of the APIC so we have to map it
+//in DoApicInit
+#if defined(linux_32) 
+    if ((p = xchg(&apic_local_addr, 0))) {
+        un_map_apic(p);
+    }
+#endif
+    
+    if ((p = xchg(&p_emon_regs, 0))) {
+        free_pool(p);
+    }
+
+    for (i = 0; i < MAX_PROCESSORS; i++) {
+        if ((p = xchg(&eachCPU[i].p_emon_reg_save_area, 0))) {
+            free_pool(p);
+        }
+    }
+
+    ret = destroy_dsa();
+    pdsa = NULL;
+    
+    return;
+}
+
+void
+samp_init_emon_regs(void *info)
+{
+    if (IA32_familyF) {
+        //
+        // Save and clear emon regs for this package. We also PASS "TRUE"
+        // to have EBS regs written that only get written once per package
+        //
+        save_clear_init_emon_regs_for_package(TRUE);    // 05-31-00
+    }
+
+    return;
+}
+
+void
+samp_start_profile_interrupt(void *info)
+{
+    __u32 processor;    // processor number (0-32)
+
+    processor = smp_processor_id();
+
+    //
+    // Turn on PCE in Cr4 on this processor
+    //
+    /*
+       if (IA32_family5 || IA32_family6 || IA32_familyF ) {
+       _asm {
+       pushfd
+       cli
+       push    eax
+       _emit   0x0f   ;get current cr4 to eax
+       _emit   0x20
+       _emit   0x0e0
+       or      eax, CR4_PCE
+       _emit   0x0f   ;move eax to current cr4
+       _emit   0x22
+       _emit   0x0e0
+       pop     eax
+       popfd
+       }
+       }
+     */
+
+    if ((samp_info.flags & SINFO_STARTED) && (sample_method & METHOD_EBS)) {
+        if (processor < MAX_PROCESSORS) {
+            // On Linux*, emon regs are initialized before calling the start routine.
+            // This avoids a timing problem in SaveClearIni
+            samp_start_ints();
+        }
+        return;
+    }
+    // HaldStartProfileinterrupt (Source);
+
+    return;
+}
+
+void
+samp_stop_profile_interrupt(void *info)
+{
+    __u32 cpu;
+    ULARGE_INTEGER val;
+
+#ifdef USE_NMI
+    // temporarily disable if event is precise
+    if (pebs_option)
+      return;
+#endif
+
+    cpu = smp_processor_id();
+
+    if (cpu < MAX_PROCESSORS) {
+        if (interlocked_exchange(&eachCPU[cpu].processor_EBS_status, 0)) {
+            samp_stop_ints();
+            add_to_counter_total_on_stop();
+            if (pebs_option) {
+                //
+                // Disable PEBS and DTS for current thread
+                //
+                samp_disable_PEB_sand_DTS();
+
+                //
+                // Clear buffer address that we wrote to DTES reg
+                //
+                val = samp_read_msr(WMT_CR_DTES_AREA);
+#if defined(linux32) 
+                if (val.low_part == (__u32) eachCPU[cpu].DTES_buf) {
+#elif defined(linux32_64)
+                if (val.quad_part == (__u64) eachCPU[cpu].DTES_buf) {
+#endif
+                    samp_write_msr(WMT_CR_DTES_AREA, eachCPU[cpu].org_DTES_area_msr_val);
+                    samp_write_msr(DEBUG_CTL_MSR, eachCPU[cpu].org_dbg_ctl_msr_val);
+                    eachCPU[cpu].org_DTES_area_msr_val.quad_part = 0;
+                    eachCPU[cpu].org_dbg_ctl_msr_val.quad_part = 0;
+                }
+            }
+            return;
+        }
+    }
+    // HaldstopProfileinterrupt (Source);
+
+    return;
+}
+
+void
+samp_stop_ints(void)
+{
+    samp_stop_emon();
+
+    if (sample_method & METHOD_EBS) {
+        samp_restore_cpu_vectors(); // 10-02-96
+        restore_emon_regs();    // 05-31-00
+    }
+
+    return;
+}
+
+/*
+ *
+ *  Function: set_IA32_family_F_emon_defaults 
+ *
+ *  description: 
+ *  Set Emon Defaults for IA32 Family F
+ *
+ *  Parms:
+ *      entry:      None
+ *  
+ *      return:     None
+ *
+ */
+void
+set_IA32_family_F_emon_defaults(void)
+{
+    ULARGE_INTEGER i64;
+ 
+    memset(reg_set, 0,  sizeof (REG_SET) * MAX_REG_SET_ENTRIES);
+    memset(reg_set0, 0, sizeof (REG_SET) * MAX_REG_SET_ENTRIES);
+    memset(reg_set1, 0,  sizeof (REG_SET) * MAX_REG_SET_ENTRIES);
+    memset(reg_set_init, 0,  sizeof (REG_SET) * MAX_REG_SET_ENTRIES);
+    max_counter.quad_part = IA32_FAMILYF_MAX_COUNTER;
+
+    //
+    // regs to count Clockticks on T0 and
+    // get counter overflow interrupt on T0
+    //
+
+    reg_set0[0].reg_num = ITLB_CR_ESCR0;    // ITLB_CR_ESCR0 0x3b6
+    reg_set0[0].reg_val.low_part = 0x3000040c;  //
+
+    reg_set0[1].options = REG_SET_COUNTER;
+    reg_set0[1].reg_num = BPU_CR_PUB_COUNTER0;  // BPU_CR_PUB_COUNTER0 0x300
+    i64.quad_part = 10000;  // arbitrary rate
+    reg_set0[1].reg_val.quad_part = ~i64.quad_part;
+
+    reg_set0[2].options = REG_SET_CCCR | REG_SET_CINDEX_VALID;
+    reg_set0[2].c_index = 1;    // index of associated counter
+    reg_set0[2].reg_num = BPU_CR_PUB_CCCR0; // BPU_CR_PUB_CCCR0 0x360
+    reg_set0[2].reg_val.low_part = 0x4ff7000;   // OVF_PMI_T0
+
+    //
+    // regs to count Clockticks on T1 and
+    // get counter overflow interrupt on T1
+    //
+    reg_set1[0].reg_num = ITLB_CR_ESCR1;    // ITLB_CR_ESCR1 0x3b7
+    reg_set1[0].reg_val.low_part = 0x30000403;  // T1-OS, T1-USER
+
+    reg_set1[1].options = REG_SET_COUNTER;
+    reg_set1[1].reg_num = BPU_CR_PUB_COUNTER2;  // BPU_CR_PUB_COUNTER2 0x302
+    i64.quad_part = 10000;  // arbitrary rate
+    reg_set1[1].reg_val.quad_part = ~i64.quad_part;
+
+    reg_set1[2].options = REG_SET_CCCR | REG_SET_CINDEX_VALID;
+    reg_set1[2].c_index = 1;    // index of associated counter
+    reg_set1[2].reg_num = BPU_CR_PUB_CCCR2; // BPU_CR_PUB_CCCR2 0x362
+    reg_set1[2].reg_val.low_part = 0x8ff7000;   // OVF_PMI_T0
+
+    return;
+}
+
+/*
+ *
+ *  Function: set_IA32_family6_emon_defaults 
+ *
+ *  description: 
+ *  Set Emon Defaults for IA32 Family 6.
+ *
+ *  Parms:
+ *      entry:      None
+ *  
+ *      return:     None
+ *
+ */
+void
+set_IA32_family6_emon_defaults(void)
+{
+    memset(reg_set, 0, sizeof (REG_SET) * MAX_REG_SET_ENTRIES);
+    memset(reg_set0, 0, sizeof (REG_SET) * MAX_REG_SET_ENTRIES);
+    memset(reg_set1, 0, sizeof (REG_SET) * MAX_REG_SET_ENTRIES);
+    memset(reg_set_init, 0, sizeof (REG_SET) * MAX_REG_SET_ENTRIES);
+
+    /*
+     * regs to count Clockticks
+     */
+
+    max_counter.quad_part = IA32_FAMILY6_MAX_COUNTER;   /* !!!! BUG_FIX_NEW !!!! */
+
+    reg_set[0].reg_num = EM_MSR_PESR0;  /* EventSel0 */
+    reg_set[0].reg_val.low_part = 0x530079; /* cpu cycles unhalted */
+
+    reg_set[1].reg_num = EM_MSR_PCTR0;  /* interval = 60000... arbitrary */
+    reg_set[1].reg_val.low_part = 60000;
+    reg_set[1].reg_val.low_part = ~reg_set[1].reg_val.low_part;
+    reg_set[1].reg_val.quad_part = ~reg_set[1].reg_val.quad_part;
+
+    return;
+}
+
+BOOLEAN
+set_event_ids(PREG_SET p_reg_set)
+{
+    __u32 i, j;
+
+    //
+    // Build table of unique event ID's for the sampling session. Save
+    // the index to the event_ID table in the counter regset entry to
+    // speed up build of event bit map when a profile interrupt occurs.
+    //
+    for (i = 0; i < MAX_REG_SET_ENTRIES; i++) {
+        if (p_reg_set[i].options & REG_SET_COUNTER) {
+            for (j = 0; j < event_count; j++) {
+                if (p_reg_set[i].event_ID == event_Ids[j]) {
+                    p_reg_set[i].event_I_dindex = (char) j;
+                    break;
+                }
+            }
+            //
+            // Add new event_index to event_Ids table
+            //
+            if (j >= event_count) {
+                if (event_count == MAX_ACTIVE_EVENTS) {
+                    return (FALSE);
+                }
+                event_Ids[j] = p_reg_set[i].event_ID;
+                p_reg_set[i].event_I_dindex = (char) event_count;
+                event_count++;
+            }
+        }
+    }
+
+    return (TRUE);
+}
+
+
+/*
+**********************************************************************************************************
+
+                S A M P L I N G    C O N F I G U R A T I O N    F U N C T I O N S  
+
+**********************************************************************************************************
+*/
+
+
+/*
+ *
+ *  Function: validate_samp_parm6 
+ *
+ *  description: 
+ *  Check to make sure the size of the samp_parm6 buffer is reasonable.
+ *
+ *  Parms:
+ *      entry:      samp_parm6
+ *                  length
+ *  
+ *      return:     0... samp_par6 length Ok
+ *
+ */
+int
+validate_samp_parm6(samp_parm6 * sp6, int sp6_len)
+{
+    int status = 0, ers_num, ers_len;
+
+    if (sp6_len < sizeof (samp_parm6)) {
+        VDK_PRINT_ERROR("Invalid samp_parm6 configuration\n");
+        status = -EINVAL;
+        return (status);
+    }
+
+    ers_num = sp6->num_event_reg_set;
+    if (ers_num) {
+        ers_len = (ers_num - 1) * sizeof(event_reg_set_ex); // subtract 1 for event_reg_set_ex that is included in samp_par6 structure
+        if (sp6_len < (sizeof(samp_parm6) + ers_len)) {
+            status = -EINVAL;
+        }
+    }
+
+    return (status);
+}
+
+int
+samp_configure6(samp_parm6 * sp6, int sp6_len)
+{
+    __u32 i;
+
+    if (samp_info.flags & (SINFO_STARTED | SINFO_STOPPING | SINFO_WRITE)) {
+        return (STATUS_DEVICE_BUSY);
+    }
+
+    if (!sp6 || validate_samp_parm6(sp6, sp6_len)) {
+        return (STATUS_INVALID_PARAMETER);
+    }
+
+    if (sp6->ptrs_are_offsets) {
+        sp6->module_info_file_name = (char *) ((__u32_PTR) sp6 + (__u32_PTR) sp6->module_info_file_name);
+        sp6->raw_sample_file_name = (char *) ((__u32_PTR) sp6 + (__u32_PTR) sp6->raw_sample_file_name);
+        sp6->ptrs_are_offsets = FALSE;
+    }
+
+    sample_method = 0;
+    for (i = 0; i < sp6->num_event_reg_set; i++) {
+        switch (sp6->esr_set[i].command) {
+        case ERS_CMD_TBS_VTD:
+            if (!(pdsa->method_VTD)) {
+                return (STATUS_INVALID_PARAMETER);
+            }
+            sample_rate_us = sp6->esr_set[i].esr_count.low_part;
+            sample_method |= METHOD_VTD;
+            break;
+        case ERS_CMD_SET_CONFIG_AND_COUNTER_REGS:
+        case ERS_CMD_WRITE_MSR:
+            if (!(pdsa->method_EBS)) {
+                return (STATUS_INVALID_PARAMETER);
+            }
+            // sample_method = SM_EBS;
+            sample_method |= METHOD_EBS;
+            break;
+        case ERS_CMD_NOP:
+            break;
+        default:
+            return (STATUS_INVALID_PARAMETER);
+        }
+    }
+
+    pdsa->sample_rec_length = sizeof(sample_record_PC);
+    sample_tsc = FALSE;
+    if (sp6->sample_TSC)
+    {
+      sample_tsc_offset = pdsa->sample_rec_length;
+      sample_tsc = TRUE;
+      pdsa->sample_rec_length += 8;
+    }
+
+#if defined(ALLOW_LBRS)
+
+    //
+    // Only thing left now is figuring out how the user code
+    // will pass the request to make capture_lbrs become true...
+    //
+    if (capture_lbrs) {
+	pdsa->sample_rec_length += LBR_SAVE_SIZE;
+	quick_freeze_msr = DEBUG_CTL_MSR;
+    }
+#endif
+
+    sample_rec_length = pdsa->sample_rec_length;
+
+    //
+    //  Zero the reg_set array
+    //
+    memset(reg_set, 0, sizeof (REG_SET) * MAX_REG_SET_ENTRIES);
+    memset(reg_set0, 0, sizeof (REG_SET) * MAX_REG_SET_ENTRIES);
+    memset(reg_set1, 0, sizeof (REG_SET) * MAX_REG_SET_ENTRIES);
+    memset(reg_set_init, 0, sizeof (REG_SET) * MAX_REG_SET_ENTRIES);
+    pebs_option = FALSE;
+
+    if (sample_method & METHOD_EBS) {
+        __u32 i, j, tot, t0, t1, init, msr;
+        ULARGE_INTEGER msr_val; // value to be written to msr register
+
+        if (IA32_familyF) {
+            if (sp6->num_event_reg_set) {
+
+                //
+                // Copy registers from samp_parms to local regSet array
+                //
+
+                for (i = 0, j = 0, t0 = 0, t1 = 0, init = 0, tot = 0; (i < sp6->num_event_reg_set)
+                     && (tot < MAX_REG_SET_ENTRIES); ++i) {
+                    switch (sp6->esr_set[i].command) {
+                    case ERS_CMD_WRITE_MSR:
+                        VDK_PRINT_DEBUG("esr_set[%d]  msr 0x%x  val 0x%x \n", i, sp6->esr_set[i].esr_value, sp6->esr_set[i].esr_count);
+                        tot++;
+                        msr = sp6->esr_set[i].esr_value;
+                        msr_val = sp6->esr_set[i].esr_count;
+                        switch (IA32_family_F_reg_type(msr)) {
+                        case ESCR_REG:
+                            break;  // write ESCR's only once per package
+                        case CCCR_REG:
+                            // the below if - else if statement ensures that the IP collected 
+                            //  relates to the h/w thread that caused the interrupt
+                            // other models could be supported in the future
+                            if (msr_val.low_part & CCCR_OVF_PMI_T0_MASK) {
+                                reg_set0[t0].reg_num = msr;
+                                reg_set0[t0].reg_val = msr_val;
+                                reg_set0[t0].options = REG_SET_CCCR;
+                                t0++;
+                            } else {
+                                if (msr_val.low_part & CCCR_OVF_PMI_T1_MASK) {
+                                    reg_set1[t1].reg_num = msr;
+                                    reg_set1[t1].reg_val = msr_val;
+                                    reg_set1[t1].options = REG_SET_CCCR;
+                                    t1++;
+                                } else {
+                                    //
+                                    // If neither OVF_PMI_T0 or OVF_PMI_T1 is on then
+                                    // it might be a precise event.
+                                    // IQ_CR_PUB_CCCR4 is for T0 precise events and
+                                    // IQ_CR_PUB_CCCR5 is for T1 precise events.
+                                    //
+                                    // If other CCCR reg is specified without either OVF T0/T1 set,
+                                    // then add it to the "Init" regs which will only be written once
+                                    // on each cpu when sampling is started.
+                                    //
+                                    if (msr == IQ_CR_PUB_CCCR4) {
+                                        reg_set0[t0].reg_num = msr;
+                                        reg_set0[t0].reg_val = msr_val;
+                                        reg_set0[t0].options = REG_SET_CCCR;
+                                        t0++;
+                                    } else {
+                                        if (msr == IQ_CR_PUB_CCCR5) {
+                                            reg_set1[t1].reg_num = msr;
+                                            reg_set1[t1].reg_val = msr_val;
+                                            reg_set1
+                                                [t1].options = REG_SET_CCCR;
+                                            t1++;
+                                        } else {
+                                            reg_set_init
+                                                [init].reg_num = msr;
+                                            reg_set_init
+                                                [init].reg_val = msr_val;
+                                            init++;
+                                        }
+                                    }
+                                }
+                            }
+                            continue;
+                            break;
+                        case COUNT_REG:
+                            reg_set[j].reg_num = msr;
+                            reg_set[j].event_ID = (char) sp6->esr_set[i].event_ID;
+                            reg_set[j].event_inc = msr_val;
+                            if (!msr_val.quad_part) {
+                                reg_set[j].
+                                    event_inc.quad_part = IA32_FAMILYF_MAX_COUNTER;
+                            }
+                            //
+                            // Negate the initial count value so that we get a counter
+                            // overflow interrupt (or PEBS record) after the requested
+                            // number of occurences of the event.
+                            //
+                            // On Pentium(R) 4 processor, the counter overflow interrupt
+			    // takes place on the next occurrence of the event following
+                            // the counter overflow.
+                            // To account for this we add 2 to the negated value.
+                            //
+                            // To sample one each event, we should set the FORCE_OVF bit
+                            // in the associated CCCR. This will cause an onverflow
+                            // in on each increment of the counter.
+                            // We'll do this in the future... 07-21-00 (already done now?)
+                            //
+
+                            if (msr_val.quad_part) {
+                                if (msr_val.quad_part == 1) {
+                                    msr_val.quad_part = (~msr_val.quad_part)
+                                        + 1;
+                                } else {
+                                    msr_val.quad_part = (~msr_val.quad_part)
+                                        + 2;
+                                }
+                            }
+                            msr_val.quad_part &= IA32_FAMILYF_MAX_COUNTER;
+                            reg_set[j].reg_val = msr_val;
+                            j++;
+                            continue;
+                            break;
+                        case PEBS_REG:
+                            if (msr == CRU_CR_PEBS_MATRIX_HORIZ) {
+                                if (msr_val.low_part & ENABLE_EBS_MY_THR) {
+                                    if (DTES_supported) {
+                                        pebs_option = TRUE;
+                                    } else {
+                                        return (STATUS_NOT_SUPPORTED);
+                                    }
+                                }
+				//BSBS this is not a bug
+				break;
+                                reg_set0[t0].reg_num = msr;
+                                reg_set0[t0].reg_val = msr_val;
+                                t0++;
+                                reg_set1[t1].reg_num = msr;
+                                reg_set1[t1].reg_val = msr_val;
+                                t1++;
+                                continue;
+                            }
+                            break;
+                        default:
+                            break;
+                        }
+                        //
+                        // If we could not determine that an msr was thread specific,
+                        // then put the msr in the reSetInit array which contains
+                        // registers that will only be written once per OS cpu at the
+                        // start of sampling
+                        //
+                        reg_set_init[init].reg_num = msr;
+                        reg_set_init[init].reg_val = msr_val;
+                        init++;
+                        break;
+                    case ERS_CMD_NOP:
+                    default:
+                        break;
+                    }
+                }
+                if ((i < sp6->num_event_reg_set)
+                    && (tot == MAX_REG_SET_ENTRIES)) {
+                    return (STATUS_NOT_SUPPORTED);
+                }
+                //
+                // At this point the reg_set array has only counter type registers
+                // and we don't know whether they belong in the T0 array or T1 array.
+                //
+                // Look for the corresponding CCCR for the counter and put the counter in
+                // same array (T0 or T1) as the CCCR.
+                //
+
+                for (i = 0; (i < j) && reg_set[i].reg_num; i++) {
+                    __u32 k;
+                    BOOLEAN CCCR_FOUND;
+
+                    CCCR_FOUND = FALSE;
+                    msr = reg_set[i].reg_num;   // msr = count reg
+                    msr += 0x60;    // msr = CCCR reg that corresponds to count reg
+                    for (k = 0; k < MAX_REG_SET_ENTRIES; k++) {
+                        if (msr == reg_set0[k].reg_num) {
+                            reg_set0[t0].reg_num = reg_set[i].reg_num;
+                            reg_set0[t0].reg_val = reg_set[i].reg_val;
+                            reg_set0[t0].event_ID = reg_set[i].event_ID;
+                            reg_set0[t0].event_inc = reg_set[i].event_inc;
+                            reg_set0[t0].options = REG_SET_COUNTER;
+                            reg_set0[k].c_index = (char) t0;
+                            reg_set0[k].options |= REG_SET_CINDEX_VALID;
+                            t0++;
+                            CCCR_FOUND = TRUE;
+                            break;
+                        }
+                    }
+                    for (k = 0; k < MAX_REG_SET_ENTRIES; k++) {
+                        if (msr == reg_set1[k].reg_num) {
+                            reg_set1[t1].reg_num = reg_set[i].reg_num;
+                            reg_set1[t1].reg_val = reg_set[i].reg_val;
+                            reg_set1[t1].event_ID = reg_set[i].event_ID;
+                            reg_set1[t1].event_inc = reg_set[i].event_inc;
+                            reg_set1[t1].options = REG_SET_COUNTER;
+                            reg_set1[k].c_index = (char) t1;
+                            reg_set1[k].options |= REG_SET_CINDEX_VALID;
+                            t1++;
+                            CCCR_FOUND = TRUE;
+                            break;
+                        }
+                    }
+                    for (k = 0; k < MAX_REG_SET_ENTRIES; k++) {
+                        if (msr == reg_set_init[k].reg_num) {
+                            reg_set_init[init].reg_num = reg_set[i].reg_num;
+                            reg_set_init[init].reg_val = reg_set[i].reg_val;
+                            reg_set_init[init].event_ID = reg_set[i].event_ID;
+                            reg_set_init[init].event_inc = reg_set[i].event_inc;
+                            reg_set_init[init].options = REG_SET_COUNTER;
+                            reg_set_init[k].c_index = (char) init;
+                            reg_set_init[k].options |= REG_SET_CINDEX_VALID;
+                            init++;
+                            CCCR_FOUND = TRUE;
+                            break;
+                        }
+                    }
+                    if (!CCCR_FOUND) {
+                        return (STATUS_NOT_SUPPORTED);
+                    }
+                }
+                //
+                // If we put the PEBS enable reg in a thread array with
+                // no other registers, then it is probably a mistake and
+                // the caller only intended to enable PEBS on the other thread.
+                //
+                if (t0 == 1) {
+                    if (reg_set0[0].reg_num == CRU_CR_PEBS_MATRIX_HORIZ) {
+			memset(reg_set0, 0, sizeof (REG_SET) * MAX_REG_SET_ENTRIES);
+                    }
+                }
+                if (t1 == 1) {
+                    if (reg_set1[0].reg_num == CRU_CR_PEBS_MATRIX_HORIZ) {
+		        memset(reg_set1, 0, sizeof (REG_SET) * MAX_REG_SET_ENTRIES);
+                    }
+                }
+            }
+        } else {
+            __u32 event_sel_reg, perf_ctr_reg;
+            // emon_cesr = sp6->esr_set[0].esr_value;
+            // emon_rate = sp6->esr_set[0].esr_count.low_part;
+            if (IA32_family6) {
+
+                if (sp6->num_event_reg_set) {
+                    for (i = 0, j = 0, tot = 0; (i < sp6->num_event_reg_set)
+                         && (tot < (MAX_REG_SET_ENTRIES - 1)); ++i) {
+                        switch (sp6->esr_set[i].command) {
+                        case ERS_CMD_WRITE_MSR:
+                            reg_set[j].reg_num = sp6->esr_set[i].esr_value;
+                            reg_set[j].reg_val = sp6->esr_set[i].esr_count;
+                            reg_set[j].event_ID = (char) sp6->esr_set[i].event_ID;
+                            if ((reg_set[j].reg_num == EM_MSR_PCTR0)
+                                || (reg_set[j].reg_num == EM_MSR_PCTR1)) {
+                                reg_set[j].event_inc = reg_set[j].reg_val;
+                                if (reg_set[j].reg_val.quad_part) {
+                                    reg_set
+                                        [j].
+                                        reg_val.
+                                        quad_part
+                                        =
+                                        ((~reg_set[j].reg_val.quad_part) +
+                                         1) & IA32_FAMILY6_MAX_COUNTER;
+                                } else {
+                                    reg_set
+                                        [j].
+                                        event_inc.
+                                        quad_part = IA32_FAMILY6_MAX_COUNTER;
+                                }
+                                reg_set[j].options = REG_SET_COUNTER;
+                            }
+                            j++;
+                            tot++;
+                            break;
+                        case ERS_CMD_SET_CONFIG_AND_COUNTER_REGS:
+                            if (sp6->esr_set[i].data == 0) {
+                                event_sel_reg = EM_MSR_PESR0;
+                                perf_ctr_reg = EM_MSR_PCTR0;
+                            } else {
+                                if (sp6->esr_set[i].data == 1) {
+                                    event_sel_reg = EM_MSR_PESR1;
+                                    perf_ctr_reg = EM_MSR_PCTR1;
+                                } else {
+                                    return (STATUS_NOT_SUPPORTED);
+                                }
+                            }
+                            reg_set[j].reg_num = event_sel_reg;
+                            reg_set[j].reg_val.low_part = sp6->esr_set[i].esr_value;
+                            j++;
+                            tot++;
+                            reg_set[j].reg_num = perf_ctr_reg;
+                            reg_set[j].reg_val = sp6->esr_set[i].esr_count;
+                            reg_set[j].event_ID = (char) sp6->esr_set[i].event_ID;
+                            reg_set[j].event_inc = reg_set[j].reg_val;
+                            if (reg_set[j].reg_val.quad_part) {
+                                reg_set[j].
+                                    reg_val.quad_part = ((~reg_set[j].reg_val.quad_part)
+                                             +
+                                             1) & IA32_FAMILY6_MAX_COUNTER;
+                            } else {
+                                reg_set[j].
+                                    event_inc.quad_part = IA32_FAMILY6_MAX_COUNTER;
+                            }
+                            reg_set[j].options = REG_SET_COUNTER;
+                            j++;
+                            tot++;
+                            break;
+                        case ERS_CMD_NOP:
+                            break;
+                        default:
+                            return (STATUS_NOT_SUPPORTED);
+                        }
+                    }
+                    if ((i < sp6->num_event_reg_set)
+                        && (tot == MAX_REG_SET_ENTRIES)) {
+                        return (STATUS_NOT_SUPPORTED);
+                    }
+                }
+            } else {
+                if (IA32_family5) {
+                    if (sp6->num_event_reg_set) {
+                        for (i = 0, j = 0, tot = 0; (i < sp6->num_event_reg_set)
+                             && (tot < (MAX_REG_SET_ENTRIES - 1)); ++i) {
+                            switch (sp6->esr_set[i].command) {
+                            case ERS_CMD_WRITE_MSR:
+                                reg_set[j].reg_num = sp6->esr_set[i].esr_value;
+                                reg_set[j].reg_val = sp6->esr_set[i].esr_count;
+                                reg_set[j].event_ID = (char) sp6->esr_set[i].event_ID;
+                                if ((reg_set[j].reg_num == EM_MSR_CTR0)
+                                    || (reg_set[j].reg_num == EM_MSR_CTR1)) {
+                                    reg_set[j].event_inc = reg_set[j].reg_val;
+                                    if (reg_set[j].reg_val.quad_part) {
+                                        reg_set
+                                            [j].
+                                            reg_val.
+                                            quad_part
+                                            =
+                                            ((~reg_set[j].reg_val.quad_part) +
+                                             1) & IA32_FAMILY5_MAX_COUNTER;
+                                    } else {
+                                        reg_set
+                                            [j].
+                                            event_inc.
+                                            quad_part
+                                            = IA32_FAMILY5_MAX_COUNTER;
+                                    }
+                                    reg_set[j].options = REG_SET_COUNTER;
+                                }
+                                j++;
+                                tot++;
+                                break;
+                            case ERS_CMD_SET_CONFIG_AND_COUNTER_REGS:
+                                if (sp6->esr_set[i].data == 0) {
+                                    event_sel_reg = EM_MSR_CESR;
+                                    perf_ctr_reg = EM_MSR_CTR0;
+                                } else {
+                                    if (sp6->esr_set[i].data == 1) {
+                                        event_sel_reg = EM_MSR_CESR;
+                                        perf_ctr_reg = EM_MSR_CTR1;
+                                    } else {
+                                        return (STATUS_NOT_SUPPORTED);
+                                    }
+                                }
+                                reg_set[j].reg_num = event_sel_reg;
+                                reg_set[j].reg_val.low_part = sp6->esr_set[i].esr_value;
+                                j++;
+                                tot++;
+                                reg_set[j].reg_num = perf_ctr_reg;
+                                reg_set[j].reg_val = sp6->esr_set[i].esr_count;
+                                reg_set[j].event_ID = (char) sp6->esr_set[i].event_ID;
+                                reg_set[j].event_inc = reg_set[j].reg_val;
+                                if (reg_set[j].reg_val.quad_part) {
+                                    reg_set
+                                        [j].
+                                        reg_val.
+                                        quad_part
+                                        =
+                                        ((~reg_set[j].reg_val.quad_part) +
+                                         1) & IA32_FAMILY5_MAX_COUNTER;
+                                } else {
+                                    reg_set
+                                        [j].
+                                        event_inc.
+                                        quad_part = IA32_FAMILY5_MAX_COUNTER;
+                                }
+                                reg_set[j].options = REG_SET_COUNTER;
+                                j++;
+                                tot++;
+                                break;
+                            case ERS_CMD_NOP:
+                                break;
+                            default:
+                                return (STATUS_NOT_SUPPORTED);
+                            }
+                        }
+                        if ((i < sp6->num_event_reg_set)
+                            && (tot == MAX_REG_SET_ENTRIES)) {
+                            return (STATUS_NOT_SUPPORTED);
+                        }
+                    }
+                } else {
+                    return (STATUS_NOT_SUPPORTED);
+                }
+            }
+        }
+    }
+    //
+    // Build table of unique event IDs that are active for this sampling session
+    //
+    memset(event_Ids, 0, sizeof (event_Ids));
+    event_count = 0;
+    if (!set_event_ids(reg_set_init)) {
+        return (STATUS_INVALID_PARAMETER);
+    }
+    if (!set_event_ids(reg_set)) {
+        return (STATUS_INVALID_PARAMETER);
+    }
+    if (!set_event_ids(reg_set0)) {
+        return (STATUS_INVALID_PARAMETER);
+    }
+    if (!set_event_ids(reg_set1)) {
+        return (STATUS_INVALID_PARAMETER);
+    }
+
+    return (STATUS_SUCCESS);
+}
+
+/*++
+
+Routine description:
+
+    get address of next free byte in Sample buffer.
+
+Arguments:
+
+    length = # of bytes to be allocated in sample buf.
+
+return value:
+
+--*/
+void_ptr
+samp_get_buf_space(__u32 length, u32 *wake_up_thread)
+{
+    void_ptr buf_ptr;
+
+    buf_ptr = NULL;
+    *wake_up_thread = FALSE;
+
+    spin_lock(&sample_int_lock);
+    if (samp_info.sampling_active) {
+        if ((p_sample_buf >= buf_start) && (p_sample_buf < buf_end)) {
+            buf_ptr = p_sample_buf;
+            (void_ptr) p_sample_buf += length;
+            if (p_sample_buf >= buf_end) {   
+                if (p_sample_buf > buf_end) {   // room for sample?
+                    p_sample_buf = buf_ptr;     // ..no
+                    buf_ptr = NULL;
+                }
+                samp_info.flags |= SINFO_DO_WRITE;
+                samp_info.sampling_active = FALSE;
+                *wake_up_thread = TRUE;
+            }
+            if (buf_ptr) {
+                //
+                // If max samples taken, then set flag to stop sampling
+                //
+                if (sample_max_samples) {
+                    __u32 i;
+                    i = pdsa->sample_count + 1;
+                    if (i >= sample_max_samples) {
+                        samp_info.flags |= (SINFO_DO_WRITE | SINFO_DO_STOP);
+                        samp_info.sampling_active = FALSE;
+                        *wake_up_thread = TRUE;
+                        if (i > sample_max_samples) {
+                            p_sample_buf = buf_ptr;     
+                            buf_ptr = NULL;
+                        }
+                    }
+                }
+                if (buf_ptr) {
+                    pdsa->sample_count++;  
+                    samp_info.sample_count = pdsa->sample_count;
+                }
+            }
+        }
+    }
+    spin_unlock(&sample_int_lock);
+
+    return (buf_ptr);
+}
+
+
+
+
+
+/*++
+
+Routine description:
+
+    Handle cpu counter overflow interrupt. This routine is called
+    with the SamplLock held and cpu emon counters stopped.
+
+Arguments:
+
+return value:
+
+    none
+
+--*/
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          u32 samp_emon_interrupt (PINT_FRAME int_frame) 
+ * @brief       Routine that gets space and calls another to fill in sample record
+ *
+ * @param       int_frame IN  - Contins cs, eip,  and eflags
+ *
+ * @return	if buffer is full, returns true to wake up writer thread
+ *
+ * This routine finds the space for the sample record and calls another
+ * routine to actually do the work. If the buffer area is full, this
+ * rotuine will return true to tell the caller to wake the writer thread.
+ *
+ */
+u32
+samp_emon_interrupt(PINT_FRAME int_frame)
+{
+    __u32 i, events = 0, wake_up_thread = FALSE;
+    BOOLEAN sr_built;
+    P_sample_record_PC p_sample = NULL, p_sample0 = NULL;
+
+    if (IA32_family6) {
+        events = samp_check_emon_counter_overflow_IA32_family6();
+    } else {
+        if (IA32_familyF) {
+            events = samp_check_emon_counter_overflow_IA32_familyF();
+        } else {
+            if (IA32_family5) {
+                events = samp_check_emon_counter_overflow_IA32_family5();
+            }
+        }
+    }
+
+    if (check_pause_mode()) {
+        sample_skipped();
+        return(wake_up_thread);
+    }
+
+    for (i = 0, sr_built = FALSE; events && (i < MAX_ACTIVE_EVENTS); i++, events = events >> 1) {
+        if (events & 1) {
+            p_sample = samp_get_buf_space(sample_rec_length, &wake_up_thread);
+            if (p_sample) {
+                //
+                // Build sample record once for first event found
+                // and copy that record for each additional event that occured
+                //
+                if (!sr_built) {
+                    sr_built = TRUE;
+                    p_sample0 = p_sample;
+                    samp_build_csip_sample(int_frame, p_sample);
+                } else {
+                    __u32 j;
+                    char *psrc, *pdest;
+
+                    // memcpy(p_sample, &sr, sizeof(sr)); 
+                    psrc = (char *) p_sample0;
+                    pdest = (char *) p_sample;
+                    for (j = 0; j < sample_rec_length; j++) {
+                        *pdest++ = *psrc++;
+                    }
+                }
+                p_sample->event_index = event_Ids[i];
+            } else {
+                break;
+            }
+        }
+    }
+
+    return(wake_up_thread);
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void __outbyte(__u32 port, __u8 value)
+ * @brief       Write a value to the the io port
+ *
+ * @return	none
+ *
+ * Use the compiler for this, but beware, the args are reversed!
+ *
+ * <I>Special Notes:</I>
+ *
+ *	None
+ *
+ * @todo Could inline this too...
+ *
+ */
+void
+__outbyte(__u32 port, __u8 value)
+{
+    outb(value, port);
+
+    return;
+}
+
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          __u8 __inbyte(__u32 port)
+ * @brief       Read a value from the io port
+ *
+ * @return	none
+ *
+ * Use the compiler for this...
+ *
+ * <I>Special Notes:</I>
+ *
+ *	None
+ *
+ * @todo Could inline this too...
+ *
+ */
+__u8
+__inbyte(__u32 port)
+{
+    return inb(port);
+}
+
+
+#if defined(ALLOW_LBRS)
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void disable_lbr_capture (void) 
+ * @brief       Disable capturing of the lbr information
+ *
+ * @return	none
+ *
+ * Other routines in other C modules might want to disable the lbrs, but
+ * only this C file will be enabling it, so no need to have a corresponding
+ * enable_lbr_capture() routine...
+ * 
+ *
+ * <I>Special Notes:</I>
+ *
+ *	None
+ *
+ *
+ */
+void
+disable_lbr_capture(void)
+{
+    capture_lbrs = FALSE;
+    quick_freeze_msr = 0;
+
+    return;
+}
+#endif
diff -urN linux.orig/drivers/char/vtune/vtlibcommon32.h linux/drivers/char/vtune/vtlibcommon32.h
--- linux.orig/drivers/char/vtune/vtlibcommon32.h	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/vtlibcommon32.h	2004-03-29 10:06:06.000000000 -0800
@@ -0,0 +1,198 @@
+/*
+ *  vtlibcommon32.h
+ *
+ *  Copyright (C) 2002-2004 Intel Corporation 
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: vtlibcommon32.h 
+ *
+ *	Description: common IA32 performance counter programming functions
+ *
+ *	Author(s): Birju Shah, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#if !defined(_VTUNE_LIBCOMMON32_H)
+#define _VTUNE_LIBCOMMON32_H
+
+#define interlocked_exchange xchg
+
+__u32
+IA32_family_F_reg_type(__u32 reg);
+
+ULARGE_INTEGER
+samp_read_msr(__u32 reg);
+
+void
+samp_write_msr(__u32 reg, ULARGE_INTEGER val);
+
+__u32
+DTS_get_CPU_features(void);
+
+BOOLEAN
+known_reg(__u32 reg);
+
+BOOLEAN
+OS_safe_RDMSR(__u32 reg, PULARGE_INTEGER pui64_ret_value);
+
+BOOLEAN
+OS_safe_WRMSR_direct(__u32 reg, PULARGE_INTEGER pui64_new_value);
+
+__u32
+get_APICID(void);
+
+void
+samp_set_PEBS_this_thread(BOOLEAN enable);
+
+void
+samp_disable_PEB_sand_DTS(void);
+
+BOOLEAN
+CCCR_disable_counting_and_clear_ovf(__u32 cccr_reg);
+
+void
+reset_and_power_on_pub_counter(__u32 cccr_reg);
+
+void
+init_EBS_regs_for_package(void);
+
+void
+restore_emon_regs(void);
+
+void
+save_clear_init_emon_regs_for_package(BOOLEAN init_EBS_regs_for_package_bool);
+
+
+
+PREG_SET
+get_reg_set_table(void);
+
+void
+add_to_counter_total_on_overflow(PREG_SET p_reg_set, __u32 cpu);
+
+void
+add_to_counter_total_on_stop(void);
+
+void
+read_cpu_perf_counters_for_current_cpu(
+    void *info
+    );
+
+__u32
+samp_check_emon_counter_overflow_IA32_family5(void);
+
+void
+samp_stop_emon_IA32_family6(void);
+
+void
+samp_start_emon_IA32_family6(u32 do_stop);
+
+__u32
+samp_check_emon_counter_overflow_IA32_family6(void);
+
+void
+samp_stop_emon_IA32_familyF(void);
+
+void
+samp_start_emon_IA32_familyF(u32 do_stop);
+
+__u32
+samp_check_emon_counter_overflow_IA32_familyF(void);
+
+void
+samp_start_emon(void *info);
+
+void
+samp_stop_emon(void);
+
+BOOLEAN
+validate_reg_RW(__u32 reg, ULARGE_INTEGER val);
+
+void
+validate_emon_regs(void);
+
+__u32
+validate_EBS_regs(void);
+
+
+void
+driver_load(void);
+
+void
+driver_open(void);
+
+void
+driver_unload(void);
+
+void
+samp_init_emon_regs(void *info);
+
+void
+samp_start_profile_interrupt(void *info);
+
+void
+samp_stop_ints(void);
+
+void
+set_IA32_family_F_emon_defaults(void);
+
+void
+set_IA32_family6_emon_defaults(void);
+
+BOOLEAN
+set_event_ids(PREG_SET p_reg_set);
+
+int
+validate_samp_parm6(samp_parm6 * sp6, int sp6_len);
+
+int
+samp_configure6(samp_parm6 * sp6, int sp6_len);
+
+void_ptr
+samp_get_buf_space(__u32 length, u32 *wake_up_thread);
+
+u32
+samp_emon_interrupt(PINT_FRAME int_frame);
+
+void
+__outbyte(__u32 port, __u8 value);
+
+__u8
+__inbyte(__u32 port);
+
+void
+samp_build_csip_sample(PINT_FRAME int_frame, P_sample_record_PC p_sample);
+
+#if defined(ALLOW_LBRS)
+void
+disable_lbr_capture(void);
+#endif
+
+void
+samp_start_ints(void);
+
+#endif /* _VTUNE_LIBCOMMON32_H */
diff -urN linux.orig/drivers/char/vtune/vtoshooks32_64.c linux/drivers/char/vtune/vtoshooks32_64.c
--- linux.orig/drivers/char/vtune/vtoshooks32_64.c	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/vtoshooks32_64.c	2004-05-11 05:30:57.077877146 -0700
@@ -0,0 +1,736 @@
+/*
+ *  vtoshooks32_64.c
+ *
+ *  Copyright (C) 2002-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: vtoshooks32_64.c
+ *
+ *	Description: OS hooks for sampling on Pentium(R) 4 processors
+ *	             with Intel(R) Extended Memory 64 Technology
+ *
+ *	Author(s): George Artz, Intel Corp.
+ *                 Juan Villacis, Intel Corp. (sys_call_table scan support)
+ *                 Thomas M Johnson, Intel Corp. (sys_call_table support on
+ *                                                Intel(R) EM64T platforms)
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#include <linux/vmalloc.h>
+#include <linux/slab.h>		/*malloc */
+#include <linux/interrupt.h>
+#include <asm/uaccess.h>
+#include <asm/unistd.h>
+#include <asm/ia32_unistd.h>
+#include <linux/fs.h>
+#include <linux/file.h>
+#include <linux/module.h>
+#include <asm/mman.h>
+
+#ifdef USE_MM_HEADER
+#include <linux/mm.h>
+#endif
+
+#include "vtdef.h"
+#include "vtuneshared.h"
+#include "vtoshooks.h"
+
+#ifdef EXPORTED_SYS_CALL_TABLE
+extern unsigned long sys_call_table[];
+#else
+static void **sys_call_table;
+#endif  /* EXPORTED_SYS_CALL_TABLE */
+
+#ifdef EXPORTED_IA32_SYS_CALL_TABLE
+extern unsigned long ia32_sys_call_table[];
+#else
+// TODO:  need to handle special cases for searching for ia32_sys_call_table
+// currently only support symbol being directly exported
+static void **ia32_sys_call_table = NULL;
+#endif
+
+void vt_clone_prologue(void);
+void vt_clone32_prologue(void);
+void vt_fork32_prologue(void);
+void vt_vfork32_prologue(void);
+void vt_execve32_prologue(void);
+
+struct mmap_arg_struct {
+        unsigned int addr;
+        unsigned int len;
+        unsigned int prot;
+        unsigned int flags;
+        unsigned int fd;
+        unsigned int offset;
+};
+
+struct short_sys32_pt_regs {
+	ulong r11;
+	ulong r10;
+	ulong r9;
+	ulong r8;
+	ulong rax;
+	ulong rcx;
+	ulong rdx;
+	ulong rsi;
+	ulong rdi;
+	ulong orig_rax;
+};
+
+struct short_pt_regs {
+	struct short_sys32_pt_regs sys32_regs;
+	ulong rip;
+	ulong cs;
+	ulong eflags;
+	ulong rsp;
+	ulong ss;
+};
+
+/*
+ * typedefs for 64-bit entry points
+ */
+typedef asmlinkage long (*sys_fork_t) (struct short_pt_regs regs);
+typedef asmlinkage long (*sys_vfork_t) (struct short_pt_regs regs);
+typedef asmlinkage long (*sys_clone_t) (struct short_pt_regs regs);
+typedef asmlinkage long (*sys_execve_t) (char* name, char** argv,
+					 char** envp, struct short_pt_regs regs);
+typedef asmlinkage long (*sys_mmap_t) (unsigned long addr, unsigned long len,
+					unsigned long prot, unsigned long flags,
+					unsigned long fd, unsigned long pgoff);
+typedef asmlinkage long (*sys_create_module_t) (char *name, long size);
+
+/*
+ * typedefs for 32-bit entry points
+ */
+typedef asmlinkage long (*sys32_mmap2_t) (unsigned long addr, unsigned long len,
+					unsigned long prot, unsigned long flags,
+					unsigned long fd, unsigned long pgoff);
+typedef asmlinkage long (*sys32_fork_t) (struct short_pt_regs regs);
+typedef asmlinkage long (*sys32_vfork_t) (struct short_pt_regs regs);
+typedef asmlinkage long (*sys32_clone_t) (unsigned int flags, unsigned int newsp, struct short_pt_regs regs);
+typedef asmlinkage long (*sys32_mmap_t) (struct mmap_arg_struct* arg);
+typedef asmlinkage long (*sys32_execve_t) (char* name, u32 argv, u32 envp, struct short_pt_regs regs);
+
+/*
+ * function pointers for original 64-bit entry points
+ */
+sys_fork_t original_sys_fork = NULL;
+sys_vfork_t original_sys_vfork = NULL;
+sys_clone_t original_sys_clone = NULL;
+sys_mmap_t original_sys_mmap = NULL;
+sys_execve_t original_sys_execve = NULL;
+sys_create_module_t original_sys_create_module = NULL;
+
+/*
+ * function pointers for original 32-bit entry points
+ */
+sys32_mmap2_t original_ia32_sys_mmap2 = NULL;
+sys32_fork_t original_ia32_sys_fork = NULL;
+sys32_vfork_t original_ia32_sys_vfork = NULL;
+sys32_clone_t original_ia32_sys_clone = NULL;
+sys32_mmap_t original_ia32_sys_mmap = NULL;
+sys32_execve_t original_ia32_sys_execve = NULL;
+
+void
+enum_modules_for_process(struct task_struct *p, PMGID_INFO pmgid_info)
+{
+	unsigned int i, options;
+	unsigned short mode;
+	struct vm_area_struct *mmap;
+	char name[MAXNAMELEN];
+	char *pname = NULL;
+
+	//
+	//  Call driver notification routine for each module 
+	//  that is mapped into the process created by the fork
+	//
+	if (!p)
+		return;
+	if (!p->mm)
+		return;
+
+#ifdef SUPPORTS_MMAP_READ
+	mmap_down_read(p->mm);
+#else
+	down_read((struct rw_semaphore *) &p->mm->mmap_sem);
+#endif
+	i = 0;
+	for (mmap = p->mm->mmap; mmap; mmap = mmap->vm_next) {
+	    if ((mmap->vm_flags & VM_EXEC) && (!(mmap->vm_flags & VM_WRITE))
+		&& mmap->vm_file && mmap->vm_file->f_dentry) {
+		memset(name, 0, MAXNAMELEN * sizeof (char));
+		pname = d_path(mmap->vm_file->f_dentry,
+			   mmap->vm_file->f_vfsmnt, name, MAXNAMELEN-1);
+		if (pname) {
+		   VDK_PRINT_DEBUG("enum: %s, %d, %lx, %lx \n",
+				   pname, p->pid, mmap->vm_start,
+				   (mmap->vm_end - mmap->vm_start));
+		   options = 0;
+		   if (i == 0) {
+		       options |= LOPTS_1ST_MODREC;
+		       i++;
+		   }
+		   mode = get_exec_mode(p);
+		   VDK_PRINT_DEBUG("enum_modules_for_process: name=%s, mode=%d\n",pname,mode);
+		   samp_load_image_notify_routine(pname,
+						mmap->vm_start,
+						(mmap->vm_end -
+						 mmap->vm_start),
+						p->pid, options,
+						pmgid_info, mode);
+		}
+	    }
+	}
+
+#ifdef SUPPORTS_MMAP_READ
+	mmap_up_read(p->mm);
+#else
+	up_read((struct rw_semaphore *) &p->mm->mmap_sem);
+#endif
+	return;
+}
+
+asmlinkage long
+vt_sys_fork(struct short_pt_regs regs)
+{
+	struct short_pt_regs* pregs;
+	long ret = -EINVAL;
+	MGID_INFO mgid_info;
+
+	//	atomic_inc(&hook_in_progress);
+
+	if (track_module_loads) {
+		alloc_module_group_ID(&mgid_info);
+		enum_modules_for_process(current, &mgid_info);
+	}
+
+	ret = original_sys_fork(regs);
+	__asm__("movq %%rsp, %0" : "=m" (pregs));
+	regs = *pregs;
+
+	if (track_module_loads && (ret >= 0 )) {
+		update_pid_for_module_group(ret, &mgid_info);
+	}
+
+	//	atomic_dec(&hook_in_progress);
+
+	return (ret);
+}
+
+asmlinkage long
+vt_sys_vfork(struct short_pt_regs regs)
+{
+	struct short_pt_regs *pregs;
+	long ret = -EINVAL;
+	MGID_INFO mgid_info;
+
+	//	atomic_inc(&hook_in_progress);
+
+	if (track_module_loads) {
+		alloc_module_group_ID(&mgid_info);
+		enum_modules_for_process(current, &mgid_info);
+	}
+
+	ret = original_sys_vfork(regs);
+	__asm__("movq %%rsp, %0" : "=m" (pregs));
+	regs = *pregs;
+
+	if (track_module_loads && (ret >= 0)) {
+		update_pid_for_module_group(ret, &mgid_info);
+	}
+
+	//	atomic_dec(&hook_in_progress);
+
+	return (ret);
+}
+
+asmlinkage void* vt_sys_clone_enter(void)
+{
+    PMGID_INFO pmgid_info = NULL;
+    
+    //    atomic_inc(&hook_in_progress);
+    
+    if (track_module_loads) {
+        pmgid_info = kmalloc(sizeof (MGID_INFO), GFP_ATOMIC);
+        VDK_PRINT_DEBUG("clone: before... pmgid_info %p\n", pmgid_info);
+        if (pmgid_info) {
+            alloc_module_group_ID(pmgid_info);
+            enum_modules_for_process(current, pmgid_info);
+        } else {
+            VDK_PRINT("clone: unable to allocate mgid_info \n");
+        }
+    }
+
+    return (pmgid_info);
+}
+
+asmlinkage void vt_sys_clone_exit(long ret, void* ptr)
+{
+    VDK_PRINT_DEBUG("clone: after.... pid 0x%x pmgid_info 0x%p\n", ret, ptr);
+    if (ptr) {
+        if (track_module_loads && (ret > 0)) {
+            update_pid_for_module_group(ret, ptr);
+        }
+        kfree(ptr);
+    }
+
+    //    atomic_dec(&hook_in_progress);
+
+    return;
+}
+
+asmlinkage long
+vt_sys_execve(char* name, char** argv, char** envp, struct short_pt_regs regs)
+{
+	struct short_pt_regs *pregs;
+	long ret = -EINVAL;
+
+	//	atomic_inc(&hook_in_progress);
+
+	ret = original_sys_execve(name, argv, envp, regs);
+	__asm__("movq %%rsp, %0" : "=m" (pregs));
+	regs = *pregs;
+
+	if (track_module_loads && !ret) {
+		VDK_PRINT_DEBUG("exec: %d\n", current->pid);
+		//samp_create_process_notify_routine(0, current->pid, 1);
+		enum_modules_for_process(current, 0);
+	}
+
+	//	atomic_dec(&hook_in_progress);
+
+	return (ret);
+}
+
+asmlinkage long
+vt_sys_mmap(unsigned long addr, unsigned long len,
+	    unsigned long prot, unsigned long flags,
+	    unsigned long fd, unsigned long pgoff)
+{
+	long ret = -EINVAL;
+	struct file *file;
+
+	VDK_PRINT_DEBUG("vt_sys_mmap() entered pid %d \n", current->pid);
+
+	//	atomic_inc(&hook_in_progress);
+
+	ret = original_sys_mmap(addr, len, prot, flags, fd, pgoff);
+
+	if (track_module_loads && !IS_ERR((void *) ret) && (prot & PROT_EXEC)
+	    && (fd)) {
+
+		if ((file = fcheck(fd)) != NULL) {
+			char *pname;
+			char name[MAXNAMELEN];
+			memset(name, 0, MAXNAMELEN * sizeof (char));
+			pname = d_path(file->f_dentry,
+				       file->f_vfsmnt, name, MAXNAMELEN-1);
+			if (pname) {
+                                VDK_PRINT_DEBUG("vt_sys_mmap:  %s, %d, %x \n",
+					     pname, current->pid, ret);
+				samp_load_image_notify_routine(pname, ret, len,
+							   current->pid, 0, 0, get_exec_mode(current));
+			}
+		}
+	}
+
+	//	atomic_dec(&hook_in_progress);
+
+	return (ret);
+}
+
+asmlinkage long
+vt_sys_create_module(char *name, long size)
+{
+	long ret = -EINVAL;
+
+	VDK_PRINT_DEBUG("vt_sys_module() entered pid %d\n", current->pid);
+
+	//	atomic_inc(&hook_in_progress);
+
+	ret = original_sys_create_module(name, size);
+
+	if (track_module_loads && !IS_ERR((void *) ret)) {
+		struct module *mod = (struct module *) ret;
+		int msize;
+#ifdef KERNEL_26X
+		msize = mod->init_size + mod->core_size;
+#else
+		msize = mod->size;
+#endif
+                VDK_PRINT_DEBUG("create_module: %s, %d, %x, %x \n",
+			     mod->name, current->pid, ret, msize);
+		samp_load_image_notify_routine((char *) mod->name, ret, msize,
+					   0, LOPTS_GLOBAL_MODULE,
+					   (PMGID_INFO) 0, get_exec_mode(current));
+	}
+
+	//	atomic_dec(&hook_in_progress);
+
+	return (ret);
+}
+
+asmlinkage void*
+vt_sys32_fork_enter(void)
+{
+	PMGID_INFO pmgid_info = NULL;
+
+	//	atomic_inc(&hook_in_progress);
+
+	if (track_module_loads) {
+	  pmgid_info = kmalloc(sizeof (MGID_INFO), GFP_ATOMIC);
+	  VDK_PRINT_DEBUG("fork32: before... pmgid_info %p\n", pmgid_info);
+	  if (pmgid_info) {
+	      alloc_module_group_ID(pmgid_info);
+	      enum_modules_for_process(current, pmgid_info);
+	  } else {
+            VDK_PRINT("fork32: unable to allocate mgid_info \n");
+	  }
+	}
+
+	return (pmgid_info);
+}
+
+asmlinkage void
+vt_sys32_fork_exit(long ret, void* ptr)
+{
+    VDK_PRINT_DEBUG("fork32: after.... pid 0x%x pmgid_info 0x%p\n", ret, ptr);
+    if (ptr) {
+        if (track_module_loads && (ret > 0)) {
+	    VDK_PRINT_DEBUG("fork32: updating pids for module group\n");
+            update_pid_for_module_group(ret, ptr);
+        }
+        kfree(ptr);
+    }
+
+    //    atomic_dec(&hook_in_progress);
+
+    return;
+}
+
+asmlinkage void*
+vt_sys32_vfork_enter(void)
+{
+	PMGID_INFO pmgid_info = NULL;
+
+	//	atomic_inc(&hook_in_progress);
+
+	if (track_module_loads) {
+	  pmgid_info = kmalloc(sizeof (MGID_INFO), GFP_ATOMIC);
+	  VDK_PRINT_DEBUG("vfork32: before... pmgid_info %p\n", pmgid_info);
+	  if (pmgid_info) {
+	      alloc_module_group_ID(pmgid_info);
+	      enum_modules_for_process(current, pmgid_info);
+	  } else {
+            VDK_PRINT("vfork32: unable to allocate mgid_info \n");
+	  }
+	}
+
+	return (pmgid_info);
+}
+
+asmlinkage void
+vt_sys32_vfork_exit(long ret, void* ptr)
+{
+    VDK_PRINT_DEBUG("vfork32: after.... pid 0x%x pmgid_info 0x%p\n", ret, ptr);
+    if (ptr) {
+        if (track_module_loads && (ret > 0)) {
+	    VDK_PRINT_DEBUG("vfork32: updating pids for module group\n");
+            update_pid_for_module_group(ret, ptr);
+        }
+        kfree(ptr);
+    }
+
+    //    atomic_dec(&hook_in_progress);
+
+    return;
+}
+
+asmlinkage void*
+vt_sys32_clone_enter(void)
+{
+    PMGID_INFO pmgid_info = NULL;
+
+    //    atomic_inc(&hook_in_progress);
+    
+    if (track_module_loads) {
+        pmgid_info = kmalloc(sizeof (MGID_INFO), GFP_ATOMIC);
+        VDK_PRINT_DEBUG("clone: before... pmgid_info %p\n", pmgid_info);
+        if (pmgid_info) {
+            alloc_module_group_ID(pmgid_info);
+            enum_modules_for_process(current, pmgid_info);
+        } else {
+            VDK_PRINT("clone: unable to allocate mgid_info \n");
+        }
+    }
+
+    return (pmgid_info);
+}
+
+asmlinkage void
+vt_sys32_clone_exit(long ret, void* ptr)
+{
+    VDK_PRINT_DEBUG("clone: after.... pid 0x%x pmgid_info 0x%p\n", ret, ptr);
+    if (ptr) {
+        if (track_module_loads && (ret > 0)) {
+	    VDK_PRINT_DEBUG("vt_sys32_clone_exit: updating pids for module group\n");
+            update_pid_for_module_group(ret, ptr);
+        }
+        kfree(ptr);
+    }
+
+    //    atomic_dec(&hook_in_progress);
+
+    return;
+}
+
+asmlinkage void*
+vt_sys32_execve_enter(void)
+{
+    //	atomic_inc(&hook_in_progress);
+    
+    return 0;
+}
+
+asmlinkage void
+vt_sys32_execve_exit(long ret, void* ptr)
+{
+    if (track_module_loads && !ret) {
+	enum_modules_for_process(current, 0);
+    }
+    
+    //	atomic_dec(&hook_in_progress);
+
+    return;
+}
+
+asmlinkage long
+vt_sys32_mmap(struct mmap_arg_struct* map_arg)
+{
+	long ret = -EINVAL;
+	struct mmap_arg_struct tmp;
+	struct file *file;
+
+	//	atomic_inc(&hook_in_progress);
+
+	if (track_module_loads) {
+	  if (copy_from_user(&tmp, map_arg, sizeof (tmp)))
+	    //	    atomic_dec(&hook_in_progress);
+	    return (ret);
+	}
+
+	ret = original_ia32_sys_mmap(map_arg);
+
+	if (track_module_loads && !IS_ERR((void *) ret) && (tmp.prot & PROT_EXEC)
+	    && (tmp.fd)) {
+
+		if ((file = fcheck(tmp.fd)) != NULL) {
+			char *pname;
+			char name[MAXNAMELEN];
+			memset(name, 0, MAXNAMELEN * sizeof (char));
+			pname = d_path(file->f_dentry,
+				       file->f_vfsmnt, name, MAXNAMELEN-1);
+			if (pname) {
+				samp_load_image_notify_routine(pname, ret, tmp.len,
+							   current->pid, 0, 0, get_exec_mode(current));
+			}
+		}
+	}
+
+	//	atomic_dec(&hook_in_progress);
+
+	return (ret);
+}
+
+asmlinkage long
+vt_sys32_mmap2(unsigned long addr,
+	     unsigned long len,
+	     unsigned long prot,
+	     unsigned long flags, unsigned long fd, unsigned long pgoff)
+{
+	long ret = -EINVAL;
+	struct file *file;
+
+	//	atomic_inc(&hook_in_progress);
+
+	VDK_PRINT_DEBUG("vt_sys32_mmap2() entered pid %d \n", current->pid);
+
+	ret = original_ia32_sys_mmap2(addr, len, prot, flags, fd, pgoff);
+
+	if (track_module_loads && !IS_ERR((void *) ret) && (prot & PROT_EXEC)) {
+		if ((file = fcheck(fd)) != NULL) {
+			char *pname;
+			char name[MAXNAMELEN];
+			memset(name, 0, MAXNAMELEN * sizeof (char));
+			pname = d_path(file->f_dentry,
+				       file->f_vfsmnt, name, MAXNAMELEN-1);
+			if (pname) {
+                                VDK_PRINT_DEBUG("vt_sys32_mmap2: %s, %d, %x, %x \n",
+					     pname, current->pid, ret, len);
+			samp_load_image_notify_routine(pname, ret, len,
+						   current->pid, 0, 0, get_exec_mode(current));
+			}
+		}
+	}
+
+	//	atomic_dec(&hook_in_progress);
+
+	return (ret);
+}
+
+long
+enum_user_mode_modules(void)
+{
+	struct task_struct *p;
+
+	read_lock(&tasklist_lock);
+	for_each_task(p) {
+		enum_modules_for_process(p, 0);
+	}
+	read_unlock(&tasklist_lock);
+
+	return (0);
+}
+
+void
+install_OS_hooks(void)
+{
+
+#ifndef EXPORTED_SYS_CALL_TABLE
+  sys_call_table = find_sys_call_table_symbol(1);
+#endif
+
+  if (sys_call_table)
+  {
+        // handle 64-bit entry points
+	original_sys_fork =
+	    (sys_fork_t) xchg(&sys_call_table[__NR_fork], vt_sys_fork);
+	original_sys_vfork =
+	    (sys_vfork_t) xchg(&sys_call_table[__NR_vfork], vt_sys_vfork);
+	original_sys_clone =
+	    (sys_clone_t) xchg(&sys_call_table[__NR_clone], vt_clone_prologue);
+	original_sys_mmap =
+	    (sys_mmap_t) xchg(&sys_call_table[__NR_mmap], vt_sys_mmap);
+	original_sys_execve =
+	    (sys_execve_t) xchg(&sys_call_table[__NR_execve], vt_sys_execve);
+	original_sys_create_module =
+	    (sys_create_module_t) xchg(&sys_call_table[__NR_create_module],
+				       vt_sys_create_module);
+  }
+  else
+    VDK_PRINT_WARNING("64-bit process creates and image loads will not be tracked during sampling session\n");
+
+  if (FALSE && ia32_sys_call_table)
+  {
+	// handle 32-bit entry points
+	original_ia32_sys_mmap2 =
+	    (sys32_mmap2_t) xchg(&ia32_sys_call_table[__NR_ia32_mmap2], vt_sys32_mmap2);
+	original_ia32_sys_fork =
+	    (sys32_fork_t) xchg(&ia32_sys_call_table[__NR_ia32_fork], vt_fork32_prologue);
+	original_ia32_sys_vfork =
+	    (sys32_vfork_t) xchg(&ia32_sys_call_table[__NR_ia32_vfork], vt_vfork32_prologue);
+	original_ia32_sys_clone =
+	    (sys32_clone_t) xchg(&ia32_sys_call_table[__NR_ia32_clone], vt_clone32_prologue);
+	original_ia32_sys_mmap =
+	    (sys32_mmap_t) xchg(&ia32_sys_call_table[__NR_ia32_mmap], vt_sys32_mmap);
+	original_ia32_sys_execve =
+	    (sys32_execve_t) xchg(&ia32_sys_call_table[__NR_ia32_execve], vt_execve32_prologue);
+  }
+  else
+    VDK_PRINT_WARNING("32-bit process creates and image loads will not be tracked during sampling session\n");
+
+  return;
+}
+
+void
+un_install_OS_hooks(void)
+{
+  void *org_fn;
+
+  /*
+   * NOTE: should check that there are no more "in flight" hooks occuring
+   *       before uninstalling the hooks and restoring the original hooks;
+   *       this can be done using atomic_inc/dec(&hook_in_progress) within
+   *       each of the vt_sys_* functions and then block here until the
+   *       hook_in_progress is 0, e.g.,
+   *
+   *       while (atomic_read(&hook_in_progress)) ; // may want to sleep
+   *
+   */
+
+#ifndef EXPORTED_SYS_CALL_TABLE
+  sys_call_table = find_sys_call_table_symbol(0);
+#endif
+
+  if (sys_call_table)
+  {
+        // restore 64-bit entry points
+	if ((org_fn = xchg(&original_sys_fork, 0))) {
+		xchg(&sys_call_table[__NR_fork], org_fn);
+	}
+	if ((org_fn = xchg(&original_sys_vfork, 0))) {
+		xchg(&sys_call_table[__NR_vfork], org_fn);
+	}
+	if ((org_fn = xchg(&original_sys_clone, 0))) {
+		xchg(&sys_call_table[__NR_clone], org_fn);
+	}
+	if ((org_fn = xchg(&original_sys_mmap, 0))) {
+		xchg(&sys_call_table[__NR_mmap], org_fn);
+	}
+	if ((org_fn = xchg(&original_sys_execve, 0))) {
+		xchg(&sys_call_table[__NR_execve], org_fn);
+	}
+	if ((org_fn = xchg(&original_sys_create_module, 0))) {
+		xchg(&sys_call_table[__NR_create_module], org_fn);
+	}
+  }
+
+  if (FALSE && ia32_sys_call_table)
+  {
+	// restore 32-bit entry points
+	if ((org_fn = xchg(&original_ia32_sys_mmap2, 0))) {
+		xchg(&ia32_sys_call_table[__NR_ia32_mmap2], org_fn);
+	}
+	if ((org_fn = xchg(&original_ia32_sys_fork, 0))) {
+		xchg(&ia32_sys_call_table[__NR_ia32_fork], org_fn);
+	}
+	if ((org_fn = xchg(&original_ia32_sys_vfork, 0))) {
+		xchg(&ia32_sys_call_table[__NR_ia32_vfork], org_fn);
+	}
+	if ((org_fn = xchg(&original_ia32_sys_clone, 0))) {
+		xchg(&ia32_sys_call_table[__NR_ia32_clone], org_fn);
+	}
+	if ((org_fn = xchg(&original_ia32_sys_mmap, 0))) {
+		xchg(&ia32_sys_call_table[__NR_ia32_mmap], org_fn);
+	}
+	if ((org_fn = xchg(&original_ia32_sys_execve, 0))) {
+		xchg(&ia32_sys_call_table[__NR_ia32_execve], org_fn);
+	}
+  }
+
+  return;
+}
diff -urN linux.orig/drivers/char/vtune/vtoshooks32.c linux/drivers/char/vtune/vtoshooks32.c
--- linux.orig/drivers/char/vtune/vtoshooks32.c	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/vtoshooks32.c	2004-05-04 14:49:19.000000000 -0700
@@ -0,0 +1,444 @@
+/*
+ *  vtoshooks32.c
+ *
+ *  Copyright (C) 2002-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: vtoshooks32.c
+ *
+ *	Description: OS hooks for sampling on IA32 platforms
+ *
+ *	Author(s): George Artz, Intel Corp.
+ *                 Juan Villacis, Intel Corp. (sys_call_table scan support)
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#include <linux/vmalloc.h>
+#include <linux/slab.h>		/*malloc */
+#include <linux/interrupt.h>
+#include <asm/uaccess.h>
+#include <asm/unistd.h>
+#include <linux/fs.h>
+#include <linux/file.h>
+#include <linux/module.h>
+#include <asm/mman.h>
+
+#ifdef USE_MM_HEADER
+#include <linux/mm.h>
+#endif
+
+#include "vtdef.h"
+#include "vtuneshared.h"
+#include "vtoshooks.h"
+
+#ifdef EXPORTED_SYS_CALL_TABLE
+extern unsigned long sys_call_table[];
+#else
+static void **sys_call_table;
+#endif  /* EXPORTED_SYS_CALL_TABLE */
+
+// static atomic_t hook_in_progress = ATOMIC_INIT(0);   // track number of hooks being executed
+
+struct mmap_arg {
+	unsigned long addr;
+	unsigned long len;
+	unsigned long prot;
+	unsigned long flags;
+	unsigned long fd;
+	unsigned long pgoff;
+};
+
+typedef asmlinkage int (*sys_fork_t) (struct pt_regs regs);
+typedef asmlinkage int (*sys_vfork_t) (struct pt_regs regs);
+typedef asmlinkage int (*sys_clone_t) (struct pt_regs regs);
+typedef asmlinkage int (*sys_execve_t) (struct pt_regs regs);
+typedef asmlinkage long (*sys_mmap_t) (struct mmap_arg * arg);
+typedef asmlinkage long (*sys_mmap2_t) (unsigned long addr, unsigned long len,
+					unsigned long prot, unsigned long flags,
+					unsigned long fd, unsigned long pgoff);
+typedef asmlinkage long (*sys_create_module_t) (char *name, long size);
+
+sys_fork_t original_sys_fork = NULL;	// address of original routine
+sys_vfork_t original_sys_vfork = NULL;	// address of original routine
+sys_clone_t original_sys_clone = NULL;	// address of original routine
+sys_mmap_t original_sys_mmap = NULL;	// address of original routine
+sys_mmap2_t original_sys_mmap2 = NULL;	// address of original routine
+sys_execve_t original_sys_execve = NULL;	// address of original routine
+sys_create_module_t original_sys_create_module = NULL;	// address of original routine
+
+void
+enum_modules_for_process(struct task_struct *p, PMGID_INFO pmgid_info)
+{
+	unsigned int i, options;
+	struct vm_area_struct *mmap;
+	char name[MAXNAMELEN];
+	char *pname = NULL;
+
+	//
+	//  Call driver notification routine for each module 
+	//  that is mapped into the process created by the fork
+	//
+	if (!p)
+		return;
+	if (!p->mm)
+		return;
+
+#ifdef SUPPORTS_MMAP_READ
+	mmap_down_read(p->mm);
+#else
+	down_read((struct rw_semaphore *) &p->mm->mmap_sem);
+#endif
+	i = 0;
+	for (mmap = p->mm->mmap; mmap; mmap = mmap->vm_next) {
+	    if ((mmap->vm_flags & VM_EXEC) && (!(mmap->vm_flags & VM_WRITE))
+		&& mmap->vm_file && mmap->vm_file->f_dentry) {
+		memset(name, 0, MAXNAMELEN * sizeof (char));
+		pname = d_path(mmap->vm_file->f_dentry,
+			   mmap->vm_file->f_vfsmnt, name, MAXNAMELEN-1);
+		if (pname) {
+		   VDK_PRINT_DEBUG("enum: %s, %d, %lx, %lx \n",
+				   pname, p->pid, mmap->vm_start,
+				   (mmap->vm_end - mmap->vm_start));
+		   options = 0;
+		   if (i == 0) {
+		       options |= LOPTS_1ST_MODREC;
+		       i++;
+		   }
+		   samp_load_image_notify_routine(pname,
+						mmap->vm_start,
+						(mmap->vm_end -
+						 mmap->vm_start),
+						p->pid, options,
+						pmgid_info, get_exec_mode(p));
+		}
+	    }
+	}
+
+#ifdef SUPPORTS_MMAP_READ
+	mmap_up_read(p->mm);
+#else
+	up_read((struct rw_semaphore *) &p->mm->mmap_sem);
+#endif
+	return;
+}
+
+asmlinkage int
+vt_sys_fork(struct pt_regs regs)
+{
+	struct pt_regs *tmp;
+	long ret = -EINVAL;
+	MGID_INFO mgid_info;
+
+	//	atomic_inc(&hook_in_progress);
+
+	if (track_module_loads) {
+		alloc_module_group_ID(&mgid_info);
+		enum_modules_for_process(current, &mgid_info);
+	}
+
+	ret = original_sys_fork(regs);
+	__asm__("movl %%esp, %0": "=m"(tmp):);
+	regs = *tmp;
+
+	if (track_module_loads && (ret >= 0 )) {
+		update_pid_for_module_group(ret, &mgid_info);
+	}
+
+	//	atomic_dec(&hook_in_progress);
+
+	return (ret);
+}
+
+asmlinkage int
+vt_sys_vfork(struct pt_regs regs)
+{
+	struct pt_regs *tmp;
+	long ret = -EINVAL;
+	MGID_INFO mgid_info;
+
+	//	atomic_inc(&hook_in_progress);
+
+	if (track_module_loads) {
+		alloc_module_group_ID(&mgid_info);
+		enum_modules_for_process(current, &mgid_info);
+	}
+
+	ret = original_sys_vfork(regs);
+	__asm__("movl %%esp, %0": "=m"(tmp):);
+	regs = *tmp;
+
+	if (track_module_loads && (ret >= 0)) {
+		update_pid_for_module_group(ret, &mgid_info);
+	}
+
+	//	atomic_dec(&hook_in_progress);
+
+	return (ret);
+}
+
+asmlinkage int
+vt_sys_clone(struct pt_regs regs)
+{
+	struct pt_regs *tmp;
+	long ret = -EINVAL;
+	MGID_INFO mgid_info;
+
+	//	atomic_inc(&hook_in_progress);
+
+	if (track_module_loads) {
+		alloc_module_group_ID(&mgid_info);
+		enum_modules_for_process(current, &mgid_info);
+	}
+
+	ret = original_sys_clone(regs);
+	__asm__("movl %%esp, %0": "=m"(tmp):);
+	regs = *tmp;
+
+	if (track_module_loads && (ret >= 0)) {
+		update_pid_for_module_group(ret, &mgid_info);
+	}
+
+	//	atomic_dec(&hook_in_progress);
+
+	return (ret);
+}
+
+asmlinkage long
+vt_sys_execve(struct pt_regs regs)
+{
+	struct pt_regs *tmp;
+	long ret = -EINVAL;
+
+	//	atomic_inc(&hook_in_progress);
+
+	ret = original_sys_execve(regs);
+	__asm__("movl %%esp, %0": "=m"(tmp):);
+	regs = *tmp;
+
+	if (track_module_loads && !ret) {
+		enum_modules_for_process(current, 0);
+	}
+
+	//	atomic_dec(&hook_in_progress);
+
+	return (ret);
+}
+
+asmlinkage long
+vt_sys_mmap(struct mmap_arg *map_arg)
+{
+	long ret = -EINVAL;
+	struct mmap_arg tmp;
+	struct file *file;
+
+	//	atomic_inc(&hook_in_progress);
+
+	if (track_module_loads) {
+	  if (copy_from_user(&tmp, map_arg, sizeof (tmp)))
+	    //	    atomic_dec(&hook_in_progress);
+	    return (ret);
+	}
+
+	ret = original_sys_mmap(map_arg);
+
+	if (track_module_loads && !IS_ERR((void *) ret) && (tmp.prot & PROT_EXEC)
+	    && (tmp.fd)) {
+
+		if ((file = fcheck(tmp.fd)) != NULL) {
+			char *pname;
+			char name[MAXNAMELEN];
+			memset(name, 0, MAXNAMELEN * sizeof (char));
+			pname = d_path(file->f_dentry,
+				       file->f_vfsmnt, name, MAXNAMELEN-1);
+			if (pname) {
+				samp_load_image_notify_routine(pname, ret, tmp.len,
+							   current->pid, 0, 0, get_exec_mode(current));
+			}
+		}
+	}
+
+	//	atomic_dec(&hook_in_progress);
+
+	return (ret);
+}
+
+asmlinkage long
+vt_sys_mmap2(unsigned long addr,
+	     unsigned long len,
+	     unsigned long prot,
+	     unsigned long flags, unsigned long fd, unsigned long pgoff)
+{
+	long ret = -EINVAL;
+	struct file *file;
+
+	//	atomic_inc(&hook_in_progress);
+
+	ret = original_sys_mmap2(addr, len, prot, flags, fd, pgoff);
+
+	if (track_module_loads && !IS_ERR((void *) ret) && (prot & PROT_EXEC)) {
+		if ((file = fcheck(fd)) != NULL) {
+			char *pname;
+			char name[MAXNAMELEN];
+			memset(name, 0, MAXNAMELEN * sizeof (char));
+			pname = d_path(file->f_dentry,
+				       file->f_vfsmnt, name, MAXNAMELEN-1);
+			if (pname) {
+			  samp_load_image_notify_routine(pname, ret, len,
+						   current->pid, 0, 0, get_exec_mode(current));
+			}
+		}
+	}
+
+	//	atomic_dec(&hook_in_progress);
+
+	return (ret);
+}
+
+asmlinkage long
+vt_sys_create_module(char *name, long size)
+{
+	long ret = -EINVAL;
+
+	//	atomic_inc(&hook_in_progress);
+
+	ret = original_sys_create_module(name, size);
+
+	if (track_module_loads && !IS_ERR((void *) ret)) {
+		struct module *mod = (struct module *) ret;
+		int msize;
+#ifdef KERNEL_26X
+		msize = mod->init_size + mod->core_size;
+#else
+		msize = mod->size;
+#endif
+		samp_load_image_notify_routine((char *) mod->name, ret, msize,
+					   0, LOPTS_GLOBAL_MODULE,
+					   (PMGID_INFO) 0, get_exec_mode(current));
+	}
+
+	//	atomic_dec(&hook_in_progress);
+
+	return (ret);
+}
+
+long
+enum_user_mode_modules(void)
+{
+	struct task_struct *p;
+
+	read_lock(&tasklist_lock);
+	for_each_task(p) {
+		enum_modules_for_process(p, 0);
+	}
+	read_unlock(&tasklist_lock);
+
+	return (0);
+}
+
+void
+install_OS_hooks(void)
+{
+
+#if !defined(EXPORTED_SYS_CALL_TABLE) && !defined(KERNEL_26X)
+  sys_call_table = find_sys_call_table_symbol(1);
+#endif
+
+  if (sys_call_table)
+  {
+	original_sys_fork =
+	    (sys_fork_t) xchg(&sys_call_table[__NR_fork], vt_sys_fork);
+	original_sys_vfork =
+	    (sys_vfork_t) xchg(&sys_call_table[__NR_vfork], vt_sys_vfork);
+	original_sys_clone =
+	    (sys_clone_t) xchg(&sys_call_table[__NR_clone], vt_sys_clone);
+	original_sys_mmap =
+	    (sys_mmap_t) xchg(&sys_call_table[__NR_mmap], vt_sys_mmap);
+	original_sys_mmap2 =
+	    (sys_mmap2_t) xchg(&sys_call_table[__NR_mmap2], vt_sys_mmap2);
+	original_sys_execve =
+	    (sys_execve_t) xchg(&sys_call_table[__NR_execve], vt_sys_execve);
+	original_sys_create_module =
+	    (sys_create_module_t) xchg(&sys_call_table[__NR_create_module],
+				       vt_sys_create_module);
+  }
+  else
+    VDK_PRINT_WARNING("process creates and image loads will not be tracked during sampling session\n");
+
+  return;
+}
+
+void
+un_install_OS_hooks(void)
+{
+  void *org_fn;
+
+  /*
+   * NOTE: should check that there are no more "in flight" hooks occuring
+   *       before uninstalling the hooks and restoring the original hooks;
+   *       this can be done using atomic_inc/dec(&hook_in_progress) within
+   *       each of the vt_sys_* functions and then block here until the
+   *       hook_in_progress is 0, e.g.,
+   *
+   *       while (atomic_read(&hook_in_progress)) ; // may want to sleep
+   *
+   */
+
+#if !defined(EXPORTED_SYS_CALL_TABLE) && !defined(KERNEL_26X)
+  sys_call_table = find_sys_call_table_symbol(0);
+#endif
+
+  if (sys_call_table)
+  {
+	if ((org_fn = xchg(&original_sys_fork, 0))) {
+		xchg(&sys_call_table[__NR_fork], org_fn);
+	}
+
+	if ((org_fn = xchg(&original_sys_vfork, 0))) {
+		xchg(&sys_call_table[__NR_vfork], org_fn);
+	}
+
+	if ((org_fn = xchg(&original_sys_clone, 0))) {
+		xchg(&sys_call_table[__NR_clone], org_fn);
+	}
+
+	if ((org_fn = xchg(&original_sys_mmap, 0))) {
+		xchg(&sys_call_table[__NR_mmap], org_fn);
+	}
+	if ((org_fn = xchg(&original_sys_mmap2, 0))) {
+		xchg(&sys_call_table[__NR_mmap2], org_fn);
+	}
+	if ((org_fn = xchg(&original_sys_execve, 0))) {
+		xchg(&sys_call_table[__NR_execve], org_fn);
+	}
+
+	if ((org_fn = xchg(&original_sys_create_module, 0))) {
+		xchg(&sys_call_table[__NR_create_module], org_fn);
+	}
+  }
+
+  return;
+}
diff -urN linux.orig/drivers/char/vtune/vtoshooks64.c linux/drivers/char/vtune/vtoshooks64.c
--- linux.orig/drivers/char/vtune/vtoshooks64.c	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/vtoshooks64.c	2004-05-11 05:13:25.107186908 -0700
@@ -0,0 +1,561 @@
+/*
+ *  vtoshooks64.c
+ *
+ *  Copyright (C) 2002-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: vtoshooks64.c
+ *
+ *	Description: Linux* OS hooks for sampling on Itanium(R) processor
+ *                   family platforms
+ *
+ *	Author(s): George Artz, Intel Corp.
+ *                 Juan Villacis, Intel Corp. (sys_call_table scan support)
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#include <linux/module.h>
+#include <linux/config.h>
+#include <linux/init.h>
+
+#include <linux/vmalloc.h>
+#include <linux/slab.h>
+#include <linux/interrupt.h>
+#include <asm/uaccess.h>
+#include <asm/unistd.h>
+#include <linux/fs.h>
+#include <linux/file.h>
+#include <asm/mman.h>
+#ifdef USE_MM_HEADER
+#include <linux/mm.h>
+#endif
+
+#include "vtdef.h"
+#include "vtuneshared.h"
+#include "vtoshooks.h"
+
+#ifdef EXPORTED_SYS_CALL_TABLE
+extern unsigned long sys_call_table[];
+#else
+static void **sys_call_table;
+#endif  /* EXPORTED_SYS_CALL_TABLE */
+
+// static atomic_t hook_in_progress = ATOMIC_INIT(0);   // track number of hooks being executed
+
+static volatile int vtsync;
+static spinlock_t clone2_table_lock = SPIN_LOCK_UNLOCKED;
+
+void
+enum_modules_for_process(struct task_struct *p, PMGID_INFO pmgid_info)
+{
+    unsigned int i, options;
+    struct vm_area_struct *mmap;
+    char name[MAXNAMELEN];
+    char *pname = NULL;
+
+    //
+    //  Call driver notification routine for each module 
+    //  that is mapped into the process created by the fork
+    //
+    if (!p)
+        return;
+    if (!p->mm)
+        return;
+
+#ifdef SUPPORTS_MMAP_READ
+    mmap_down_read(p->mm);
+#else
+    down_read(&p->mm->mmap_sem);
+#endif
+
+    i = 0;
+    for (mmap = p->mm->mmap; mmap; mmap = mmap->vm_next) {
+        if ((mmap->vm_flags & VM_EXEC) && (!(mmap->vm_flags & VM_WRITE))
+            && mmap->vm_file && mmap->vm_file->f_dentry) {
+            memset(name, 0, MAXNAMELEN * sizeof (char));
+            pname = d_path(mmap->vm_file->f_dentry,
+                       mmap->vm_file->f_vfsmnt, name, MAXNAMELEN-1);
+            if (pname) {
+                VDK_PRINT_DEBUG("enum: %s, %d, %lx, %lx \n",
+				pname, p->pid, mmap->vm_start,
+				(mmap->vm_end - mmap->vm_start));
+                options = 0;
+                if (i == 0) {
+                    options |= LOPTS_1ST_MODREC;
+                    i++;
+                }
+                samp_load_image_notify_routine(pname,
+                               mmap->vm_start,
+                               (mmap->vm_end -
+                                mmap->vm_start),
+                               p->pid, options,
+                               pmgid_info, get_exec_mode(p));
+            }
+        }
+    }
+
+#ifdef SUPPORTS_MMAP_READ
+    mmap_up_read(p->mm);
+#else
+    up_read(&p->mm->mmap_sem);
+#endif
+
+    return;
+}
+
+static void
+vt_set_pp_for_cpu(void *arg)
+{
+    __asm__ __volatile__(";; ssm psr.pp;; srlz.d;;":::"memory");
+
+    itp_set_dcr(itp_get_dcr() | IA64_DCR_PP);
+
+    while (vtsync) ;
+
+    return;
+}
+
+//
+// Set PSR.PP and DCR.PP to enable system wide perfomance counters
+//
+// PSR.PP is set for all tasks and set DCR.PP for each cpu.
+//
+void
+set_PP_SW(void)
+{
+    struct task_struct *p;
+
+    vtsync = 1;
+
+    read_lock(&tasklist_lock);
+
+    smp_call_function(vt_set_pp_for_cpu, NULL, 1, 0);
+    __asm__ __volatile__(";; ssm psr.pp;; srlz.d;;":::"memory");
+
+    itp_set_dcr(itp_get_dcr() | IA64_DCR_PP);
+
+    for_each_task(p) {
+        struct pt_regs *p_regs;
+
+        // psr.pp should always be one
+        p_regs = ia64_task_regs(p);
+        ia64_psr(p_regs)->pp = 1;
+    }
+
+    vtsync = 0;
+
+    read_unlock(&tasklist_lock);
+
+    return;
+}
+
+#ifdef  __ia64
+typedef struct {
+    long fptr;
+    long gp;
+} plabel_t;
+long kernel_gp;
+#endif              /*  */
+
+void *original_sys_mmap = NULL; // address of original routine
+void *original_sys_mmap2 = NULL;    // address of original routine
+void *original_sys_clone = NULL;    // address of original routine
+void *original_sys_clone2 = NULL;   // address of original routine
+void *original_sys_create_module = NULL;    // address of original routine
+void *original_sys_execve = NULL;   // address of original routine
+
+asmlinkage void
+vt_sys_mmap(unsigned long ret, unsigned long len, int prot, int fd)
+{
+    struct file *file;
+
+    //    atomic_inc(&hook_in_progress);
+
+    if (track_module_loads && !IS_ERR((void *) ret) && (prot & PROT_EXEC)) {
+        if ((file = fcheck(fd)) != NULL) {
+            char *pname;
+            char name[MAXNAMELEN];
+            memset(name, 0, MAXNAMELEN * sizeof (char));
+            pname = d_path(file->f_dentry, file->f_vfsmnt, name, MAXNAMELEN-1);
+            if (pname) {
+                VDK_PRINT_DEBUG("mmap: %s, %d, %lx, %lx \n",
+				pname, current->pid, ret, len);
+                samp_load_image_notify_routine(pname, ret, len,
+                               current->pid, 0, 0, get_exec_mode(current));
+            }
+        }
+    }
+
+    //    atomic_dec(&hook_in_progress);
+
+    return;
+}
+
+asmlinkage void
+vt_sys_mmap2(unsigned long ret, unsigned long len, int prot, int fd)
+{
+    struct file *file;
+
+    //    atomic_inc(&hook_in_progress);
+
+    if (track_module_loads && !IS_ERR((void *) ret) && (prot & PROT_EXEC)) {
+        if ((file = fcheck(fd)) != NULL) {
+            char *pname;
+            char name[MAXNAMELEN];
+            memset(name, 0, MAXNAMELEN * sizeof (char));
+            pname = d_path(file->f_dentry, file->f_vfsmnt, name, MAXNAMELEN-1);
+            if (pname) {
+                VDK_PRINT_DEBUG("mmap2: %s, %d, %lx, %lx \n",
+				pname, current->pid, ret, len);
+                samp_load_image_notify_routine(pname, ret, len,
+                               current->pid, 0, 0, get_exec_mode(current));
+            }
+        }
+    }
+
+    //    atomic_dec(&hook_in_progress);
+
+    return;
+}
+
+asmlinkage void *
+vt_sys_clone_before(void)
+{
+    PMGID_INFO pmgid_info = NULL;
+
+    //    atomic_inc(&hook_in_progress);
+
+    if (track_module_loads) {
+        pmgid_info = kmalloc(sizeof (MGID_INFO), GFP_ATOMIC);
+        VDK_PRINT_DEBUG("clone: before... pmgid_info %p\n", pmgid_info);
+        if (pmgid_info) {
+            alloc_module_group_ID(pmgid_info);
+            enum_modules_for_process(current, pmgid_info);
+        } else {
+            VDK_PRINT("clone: unable to allocate mgid_info \n");
+        }
+    }
+    return (pmgid_info);
+}
+
+asmlinkage void
+vt_sys_clone_after(unsigned long ret, void *ptr)
+{
+    VDK_PRINT_DEBUG("clone: after.... pid 0x%x pmgid_info 0x%p\n", ret, ptr);
+    if (ptr) {
+        if (ret > 0) {
+            update_pid_for_module_group(ret, ptr);
+        }
+        kfree(ptr);
+    }
+
+    //    atomic_dec(&hook_in_progress);
+
+    return;
+}
+
+typedef struct {
+  //    u64 gp;
+    u64 rp;
+  //    u64 arpfs;
+} cloneinfo_t;
+
+typedef struct _CLONE_INFO {
+    union {
+        u32 flags;
+        struct {
+            u32 entry_in_use:1;
+             u32:31;
+        };
+    };
+    u32 pid;
+    cloneinfo_t clone_info;
+    MGID_INFO mgid_info;
+} CLONE_INFO, *PCLONE_INFO;
+
+#define CLONE_TBL_ENTRIES 100
+CLONE_INFO clone_tbl[CLONE_TBL_ENTRIES];
+
+//asmlinkage void
+//vt_sys_clone2_before(u64 gp, u64 rp, u64 arpfs)
+//
+// Using a cmp4 in the assembly check so don't change the u32 return
+// value without changing the assembly code.
+//
+// True means we successfully added the info to the clone_tbl, FALSE
+// means we failed. The callee can then decide where the clone call should
+// return to (ie, don't do the clone2_after() routine if the add
+// to the table failed)
+//
+asmlinkage u32
+vt_sys_clone2_before(u64 rp)
+{
+    u32 i;
+    PCLONE_INFO p;
+
+    //    atomic_inc(&hook_in_progress);
+
+    VDK_PRINT_DEBUG("clone2: before... current pid 0x%x \n", current->pid);
+
+    spin_lock(&clone2_table_lock);
+    p = &clone_tbl[0];
+    for (i = 0; i < CLONE_TBL_ENTRIES; i++, p++) {
+        if (!p->entry_in_use) {
+            p->entry_in_use = 1;
+            break;
+        }
+    }
+    spin_unlock(&clone2_table_lock);
+
+    if (i >= CLONE_TBL_ENTRIES) {
+        VDK_PRINT_ERROR("clone2: before... clone table size exceeded\n");
+        return FALSE;
+    }
+
+    //    p->clone_info.gp = gp;
+    p->clone_info.rp = rp;
+    //    p->clone_info.arpfs = arpfs;
+    p->pid = current->pid;
+    p->mgid_info.mgid = 0;
+
+    if (track_module_loads) {
+        VDK_PRINT_DEBUG("clone: before... pmgid_info %p\n", p->mgid_info);
+        alloc_module_group_ID(&p->mgid_info);
+        enum_modules_for_process(current, &p->mgid_info);
+    }
+
+    return TRUE;
+}
+
+//asmlinkage cloneinfo_t
+asmlinkage u64
+vt_sys_clone2_after(unsigned long ret)
+{
+    cloneinfo_t r;
+    PCLONE_INFO p;
+    u32 i, pid;
+
+    VDK_PRINT_DEBUG("clone2: after.... current pid 0x%x new pid 0x%x \n", current->pid, ret);
+
+    pid = current->pid;
+    p = &clone_tbl[0];
+    for (i = 0; i < CLONE_TBL_ENTRIES; i++, p++) {
+        if (p->entry_in_use && p->pid == pid) {
+            if (p->mgid_info.mgid && (ret > 0)) {
+                update_pid_for_module_group(ret, &p->mgid_info);
+            }
+            r = p->clone_info;
+            p->entry_in_use = 0;
+            break;
+        }
+    }
+
+    if (i >= CLONE_TBL_ENTRIES) {
+        VDK_PRINT_ERROR("clone2: after.... pid not found. Table full?\n");
+        //
+        // Caller will be branching to zero because of this
+        // which will kill it... Not sure there is much else we an do...
+        //
+        return (0);
+    }
+
+    //    atomic_dec(&hook_in_progress);
+
+    return (r.rp);
+}
+
+asmlinkage void
+vt_sys_create_module(unsigned long ret)
+{
+    //    atomic_inc(&hook_in_progress);
+
+    if (track_module_loads && !IS_ERR((void *) ret)) {
+        struct module *mod = (struct module *) ret;
+	int msize;
+#ifdef KERNEL_26X
+	msize = mod->init_size + mod->core_size;
+#else
+	msize = mod->size;
+#endif
+        VDK_PRINT_DEBUG("create_module: %s, %d, %lx, %lx \n",
+			mod->name, current->pid, ret, msize);
+        samp_load_image_notify_routine((char *) mod->name, ret, msize,
+                       0, LOPTS_GLOBAL_MODULE,
+                       (PMGID_INFO) 0, get_exec_mode(current));
+    }
+
+    //    atomic_dec(&hook_in_progress);
+
+    return;
+}
+
+void *vt_save_execve_ret;
+
+asmlinkage void
+vt_sys_execve(void)
+{
+    //    atomic_inc(&hook_in_progress);
+
+    VDK_PRINT_DEBUG("exec: pid=%d\n", current->pid);
+    // samp_create_process_notify_routine(0, current->pid, 1);
+    if (track_module_loads) {
+        enum_modules_for_process(current, 0);
+    }
+
+    //    atomic_dec(&hook_in_progress);
+
+    return;
+}
+
+long
+enum_user_mode_modules(void)
+{
+    struct task_struct *p;
+
+    read_lock(&tasklist_lock);
+    for_each_task(p) {
+        enum_modules_for_process(p, 0);
+    }
+    read_unlock(&tasklist_lock);
+
+    return (0);
+}
+
+void
+install_OS_hooks(void)
+{
+    plabel_t *p;
+    extern int vt_sys_mmap_stub(void);
+    extern int vt_sys_mmap2_stub(void);
+    extern int vt_sys_clone_stub(void);
+    extern int vt_sys_clone2_stub(void);
+    extern int vt_sys_create_module_stub(void);
+    extern int vt_sys_execve_stub(void);
+
+#ifdef KERNEL_26X
+// Note:  currently unable to track processes started during collection
+//        (see note below)
+    VDK_PRINT_WARNING("process creates and image loads will not be tracked during sampling session\n");
+    return;
+#endif
+
+    memset(clone_tbl, sizeof(clone_tbl), 0);
+
+#if !defined(EXPORTED_SYS_CALL_TABLE)
+    sys_call_table = find_sys_call_table_symbol(1);
+#endif
+
+    if (sys_call_table) {
+
+    p = (plabel_t *) vt_sys_mmap_stub;
+    original_sys_mmap =
+        (void *) xchg(&sys_call_table[__NR_mmap - 1024], p->fptr);
+
+    p = (plabel_t *) vt_sys_mmap2_stub;
+    original_sys_mmap2 =
+        (void *) xchg(&sys_call_table[__NR_mmap2 - 1024], p->fptr);
+#ifdef OS_RHEL3
+    VDK_PRINT_WARNING("disabling sys_clone hooking\n");
+#else
+    p = (plabel_t *) vt_sys_clone2_stub; // use sys_clone2 stub for sys_clone
+    original_sys_clone =
+        (void *) xchg(&sys_call_table[__NR_clone - 1024], p->fptr);
+#endif
+    p = (plabel_t *) vt_sys_clone2_stub;
+    original_sys_clone2 =
+        (void *) xchg(&sys_call_table[__NR_clone2 - 1024], p->fptr);
+#ifdef KERNEL_26X
+// Note: __NR_create_module is not implemented (see include/asm-ia64/unistd.h)
+    VDK_PRINT_WARNING("process creates will not be tracked during sampling session\n");
+#else
+    p = (plabel_t *) vt_sys_create_module_stub;
+    original_sys_create_module =
+        (void *) xchg(&sys_call_table[__NR_create_module - 1024], p->fptr);
+#endif
+    p = (plabel_t *) vt_sys_execve_stub;
+    original_sys_execve =
+        (void *) xchg(&sys_call_table[__NR_execve - 1024], p->fptr);
+
+    } else
+      VDK_PRINT_WARNING("process creates and image loads will not be tracked during sampling session\n");
+
+    return;
+}
+
+void
+un_install_OS_hooks(void)
+{
+    void *org_fn;
+
+#ifdef KERNEL_26X
+    return;
+#endif
+
+  /*
+   * NOTE: should check that there are no more "in flight" hooks occuring
+   *       before uninstalling the hooks and restoring the original hooks;
+   *       this can be done using atomic_inc/dec(&hook_in_progress) within
+   *       each of the vt_sys_* functions and then block here until the
+   *       hook_in_progress is 0, e.g.,
+   *
+   *       while (atomic_read(&hook_in_progress)) ; // may want to sleep
+   *
+   */
+
+#if !defined(EXPORTED_SYS_CALL_TABLE)
+    sys_call_table = find_sys_call_table_symbol(0);
+#endif
+
+    if (sys_call_table) {
+
+    if ((org_fn = xchg(&original_sys_mmap, 0))) {
+        xchg(&sys_call_table[__NR_mmap - 1024], org_fn);
+    }
+    if ((org_fn = xchg(&original_sys_mmap2, 0))) {
+        xchg(&sys_call_table[__NR_mmap2 - 1024], org_fn);
+    }
+#ifndef OS_RHEL3
+    if ((org_fn = xchg(&original_sys_clone, 0))) {
+        xchg(&sys_call_table[__NR_clone - 1024], org_fn);
+    }
+#endif
+    if ((org_fn = xchg(&original_sys_clone2, 0))) {
+        xchg(&sys_call_table[__NR_clone2 - 1024], org_fn);
+    }
+#ifndef KERNEL_26X
+    if ((org_fn = xchg(&original_sys_create_module, 0))) {
+        xchg(&sys_call_table[__NR_create_module - 1024], org_fn);
+    }
+#endif
+    if ((org_fn = xchg(&original_sys_execve, 0))) {
+        xchg(&sys_call_table[__NR_execve - 1024], org_fn);
+    }
+
+    }
+
+    return;
+}
diff -urN linux.orig/drivers/char/vtune/vtoshooks.h linux/drivers/char/vtune/vtoshooks.h
--- linux.orig/drivers/char/vtune/vtoshooks.h	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/vtoshooks.h	2004-04-08 13:28:56.000000000 -0700
@@ -0,0 +1,79 @@
+/*
+ *  vtoshooks.h
+ *
+ *  Copyright (C) 2002-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: vtoshooks.h
+ *
+ *	Description: common header file used by vtoshooks*.c
+ *
+ *	Author(s): George Artz, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#if !defined(_VTUNE_OSHOOKS_H)
+#define _VTUNE_OSHOOKS_H
+
+#include "vtypes.h"
+
+extern __u32 track_module_loads;	// track module loads
+extern __u32 track_process_creates;	// track module loads
+
+typedef struct _MGID_INFO	// Module Group ID information
+{
+	__u32 mgid;		// module group id
+	__u32 reserved;		// reserved
+	void_ptr mr_first;	// address of first module record in the module group (may be in any module record buffer)
+	void_ptr *mr_last;	// address of last module record in the module group (may be in any module record buffer)
+} MGID_INFO, *PMGID_INFO;
+
+void alloc_module_group_ID(PMGID_INFO pmgid_info);
+
+void update_pid_for_module_group(__u32 pid, PMGID_INFO pmgid_info);
+
+int samp_load_image_notify_routine(char *name, __u32_PTR base,__u32_PTR size,
+				   __u32 pid, __u32 options, PMGID_INFO pmgid_info,
+				   unsigned short  mode);
+
+unsigned short get_exec_mode(struct task_struct *p);
+
+// defines for options paramater of samp_load_image_notify_routine(...)
+#define LOPTS_1ST_MODREC    0x1
+#define LOPTS_GLOBAL_MODULE 0x2
+
+void samp_create_process_notify_routine(__u32 parent_id, __u32 process_id, __u32 create);
+
+#ifndef EXPORTED_SYS_CALL_TABLE
+void *find_sys_call_table_symbol(int verbose);
+#endif
+
+#ifdef EXPORTED_FOR_EACH_PROCESS
+#define for_each_task for_each_process
+#endif
+
+#endif /* _VTUNE_OSHOOKS_H */
diff -urN linux.orig/drivers/char/vtune/vtproto.h linux/drivers/char/vtune/vtproto.h
--- linux.orig/drivers/char/vtune/vtproto.h	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/vtproto.h	2004-05-11 05:15:04.592537252 -0700
@@ -0,0 +1,152 @@
+/*
+ *  vtproto.h
+ *
+ *  Copyright (C) 2002-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: vtproto.h
+ *
+ *	Description: function prototypes
+ *
+ *	Author(s): George Artz, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#if !defined(_VTUNE_PROTO_H)
+#define _VTUNE_PROTO_H
+
+#ifdef KERNEL_26X
+#include <linux/fs.h>
+#endif
+
+#include "vtoshooks.h"
+
+#ifdef KERNEL_26X
+#ifndef smp_num_cpus
+#define smp_num_cpus num_online_cpus()
+#endif
+#endif
+
+#ifdef DSA_SUPPORT_MMAP
+int vtune_mmap(struct file *, struct vm_area_struct *);
+#endif // DSA_SUPPORT_MMAP
+
+driver_shared_area *create_dsa(void);
+int destroy_dsa(void);
+
+#if defined(linux32) || defined(linux32_64)
+void samp_apic_set_perf_lvt_int_mask(void);
+void samp_apic_clear_perf_lvt_int_mask(void);
+__u32 SAMP_Set_apic_perf_lvt(long apic_perf_lvt);
+void SAMP_Set_Apic_Virtual_Wire_Mode(void);
+#elif defined(linux64)
+void ebs_intr(int irq, void *arg, struct pt_regs *regs);
+#else
+#error Compiling for unsupported architecture
+#endif
+
+ssize_t samp_write_module_file(struct file *, char *, size_t, loff_t *);
+void samp_write_sample_file(void);
+
+ssize_t vtune_read(struct file *, char *, size_t, loff_t *);
+ssize_t vtune_write(struct file *, const char *, size_t, loff_t *);
+int vtune_open(struct inode *, struct file *);
+int vtune_release(struct inode *inode, struct file *filp);
+int vtune_ioctl(struct inode *, struct file *, unsigned int, unsigned long);
+void vtune_sampreset(void);
+int vtune_sampuserstop(void);
+
+void vdrv_init_emon_regs(void);
+int vdrv_start_EBS(void);
+void vdrv_stop_EBS(void);
+
+u32  samp_emon_interrupt(PINT_FRAME int_frame);
+void samp_init_emon_regs(void *info);
+void samp_start_profile_interrupt(void *info);
+void samp_stop_profile_interrupt(void *info);
+void samp_start_emon(void *info);
+void samp_stop_emon(void);
+void samp_restore_cpu_vectors(void);
+
+int start_sampling6(samp_parm6 * sp6, int sp6_len);
+
+PREG_SET get_reg_set_table(void);
+
+void samp_start_delay(unsigned long);
+void samp_max_sample_time_expired(unsigned long);
+int samp_get_stats(sampinfo_t *);
+int samp_get_parm(samp_parm3 *);
+
+void free_mrlist(void);
+int vdrvgetsysinfo(void);
+
+ULARGE_INTEGER samp_read_msr(__u32 reg);
+
+void samp_write_msr(__u32 reg, ULARGE_INTEGER val);
+
+//
+// CSS: TODO: Update the linux32 to use the newer C code instead...
+//
+#if defined(linux32)
+void get_CSD(__u32, __u32 *, __u32 *);
+#elif defined(linux32_64)
+__u32 get_CSD(__u32, __u32 *, __u32 *);
+#endif
+
+/* ASM types (vtxsys*.S) */
+__u32 get_APICID(void);
+void save_clear_init_emon_regs_for_package(BOOLEAN);
+void validate_emon_regs(void);
+void driver_load(void);
+void driver_open(void);
+void driver_unload(void);
+int init_driver_OS(void);
+extern void t_ebs(void);
+extern void samp_get_set_idt_entry(unsigned long, unsigned long, __u64 *);
+extern void samp_restore_idt_entry(unsigned long, long long *);
+
+/* vtlib*.c */
+void set_IA32_family6_emon_defaults(void);
+int samp_configure6(samp_parm6 * sp6, int sp6_len);
+BOOLEAN set_event_I_ds(PREG_SET p_reg_set);
+void read_cpu_perf_counters_for_current_cpu(void *info);
+
+/* OS Services */
+void_ptr allocate_pool(__u32 pool_type, __u32 i);
+void free_pool(void_ptr i);
+
+/* vtune.c */
+void sample_skipped(void);
+BOOLEAN check_pause_mode(void);
+
+#ifdef ENABLE_TGID
+/* Thread Group ID support */
+int get_thread_group_id(struct task_struct *proc_task);
+int find_thread_id(thread_info *p_thread_id);
+#endif
+
+#endif /* _VTUNE_PROTO_H */
diff -urN linux.orig/drivers/char/vtune/vtune.c linux/drivers/char/vtune/vtune.c
--- linux.orig/drivers/char/vtune/vtune.c	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/vtune.c	2004-05-11 05:33:26.426508129 -0700
@@ -0,0 +1,3134 @@
+/*
+ *  vtune.c
+ *
+ *  Copyright (C) 2002-2004 Intel Corporation
+ *  Maintainer - Juan Villacis <juan.villacis@intel.com>
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; either version 2 of the License, or
+ *  (at your option) any later version.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program; if not, write to the Free Software
+ *  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+/*
+ * ===========================================================================
+ *
+ *	File: vtune.c
+ *
+ *	Description: sampling driver main program
+ *
+ *	Author(s): George Artz, Intel Corp.
+ *                 Juan Villacis, Intel Corp.
+ *
+ *	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+ *
+ * ===========================================================================
+ */
+
+#include <linux/vmalloc.h>
+#include <linux/slab.h>     /* malloc */
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/list.h>
+#ifdef KERNEL_26X
+#include <linux/fs.h>
+#include <linux/init.h>
+#include <linux/vermagic.h>
+#ifdef DSA_SUPPORT_MMAP
+#include <linux/mm.h>
+#endif
+#ifdef linux32_64
+#include <asm/thread_info.h>
+#endif
+#endif
+#include <asm/uaccess.h>
+#ifdef ENFORCE_ROOT_ONLY_ACCESS
+#include <linux/capability.h>
+#endif
+#ifdef linux32
+#include <asm/unistd.h>
+#endif
+#ifdef PERFMON_SUPPORTED
+#include <asm/perfmon.h>
+#endif
+
+#ifdef linux32_64
+#include "asm/apic.h"
+#include "apic.h"
+#endif
+
+#include "vtdef.h"
+#include "vtuneshared.h"
+#include "vtoshooks.h"
+#include "vtproto.h"
+#include "vtglobal.h"
+#include "familyf_msr.h"
+
+long enum_user_mode_modules(void);
+
+#ifdef linux64
+void set_PP_SW(void);
+#endif
+
+// The following is added by Fleming for the algorithm of eliminating dup
+// entries for a single process when calling do_fork and do_execve in
+// consecutive order.
+
+typedef struct tag_MR_pointer {
+    struct tag_MR_pointer *m_p_prev;
+    struct tag_MR_pointer *m_p_next;
+    module_record *m_p_mrp;
+} MRPOINTER, *PMRPOINTER;
+// g_MR_pointer_head used to be "last", g_MR_pointer_tail used to be
+// "gMRpointer"
+PMRPOINTER g_MR_pointer_tail = NULL, g_MR_pointer_head = NULL;
+
+typedef struct tag_pid_record_pointer {
+    struct tag_pid_record_pointer *m_p_next;
+    pid_record *m_p_pid_record;
+} pid_record_pointer, *P_pid_record_pointer;
+P_pid_record_pointer g_pid_record_pointer_tail = NULL, g_pid_record_pointer_head = NULL;
+
+ssize_t samp_write_pid_record_file(struct file *fp, char *ubuf, size_t cnt,
+                   loff_t * pp);
+void add_PID_create_record_to_list(pid_t pid);
+void free_PID_create_record_list(void);
+
+// For eliminating the wake_up_interruptible( ) in the ebs isr
+// Added by Fleming
+__u32 g_b_wake_up_interruptible = 0;
+__u32 b_stop_sampling = FALSE;
+__u32 restart_EBS = FALSE;
+struct irqaction *pebs_irqaction = NULL;
+
+spinlock_t vtune_modlist_lock = SPIN_LOCK_UNLOCKED;
+spinlock_t pidlist_lock = SPIN_LOCK_UNLOCKED;
+spinlock_t mgid_lock = SPIN_LOCK_UNLOCKED;
+
+__u32 total_loads;      // debug code 
+__u32 total_loads_init;     // debug code 
+__u32 total_pid_updates;    // debug code
+
+// module_record buffer
+#define MR_BUFFER_DATA_SIZE (32*1024)
+typedef struct _MR_BUF {    // 
+    LIST_ENTRY link;    //
+    __u32 buf_size;     // byte length of buffer
+    __u32 bytes_in_buf; // count of bytes in buffer
+    char buf[MR_BUFFER_DATA_SIZE];  
+} MR_BUF, *PMR_BUF;
+
+PMR_BUF p_current_mr_buf;       // current module record buffer    
+LIST_ENTRY mr_buf_free_list_head;   // head of free mr  bufs           
+LIST_ENTRY mr_buf_out_list_head;    // head of mr  bufs to be output   
+
+PMR_BUF p_mr_buf_out;       // pointer to module record buffer that has
+                // been popped off
+                // ..output list and contains modules record
+                // ready to be written to .tb5 file 
+__u32 mr_buf_out_offset;    // offset to next module record in the output
+                // buffer  
+
+// Process create Record buffer        
+#define PID_BUFFER_DATA_SIZE (16*1024)
+typedef struct _PID_BUF {   // 
+    LIST_ENTRY link;    //
+    __u32 buf_size;     // byte length of buffer
+    __u32 bytes_in_buf; // count of bytes in buffer
+    char buf[PID_BUFFER_DATA_SIZE]; 
+} PID_BUF, *PPID_BUF;
+
+PPID_BUF p_current_pid_buf;     // current pid record buffer
+LIST_ENTRY pid_buf_free_list_head;  // head of free pid  bufs
+LIST_ENTRY pid_buf_out_list_head;   // head of pr  bufs to be output
+
+PPID_BUF p_pid_buf_out;         // pointer to process create record
+                    // buffer that has been popped off
+// ..output list and contains pid
+// record ready to be written to .tb5 file 
+__u32 pid_buf_out_offset;       // offset to next pid record in the
+                    // output buffer  
+
+__u32 track_module_loads = FALSE;   // track module loads
+__u32 track_process_creates = FALSE;    // track process creates
+static atomic_t samp_isr_active = ATOMIC_INIT(0);
+
+__u32 mgid;             // module group ID
+
+void install_OS_hooks(void);
+void un_install_OS_hooks(void);
+int vdrv_resume_EBS(void);
+
+#ifdef linux32
+__u32 validate_EBS_regs(void);
+void SAMP_Set_Apic_Virtual_Wire_Mode(void);
+void disable_lbr_capture(void);
+#elif defined(linux32_64)
+__u32 validate_EBS_regs(void);
+#endif
+
+void add_to_mr_buf_out_offset(__u32 rec_length);
+
+module_record *get_next_mr_rec_from_out_list(void);
+
+PMR_BUF pop_mr_buf_from_head_of_free_list(void);
+
+PMR_BUF pop_mr_buf_from_head_of_out_list(void);
+
+PMR_BUF alloc_new_mr_buf(void);
+
+void add_mr_buf_to_free_list(PMR_BUF pmr_buf);
+
+void free_mr_bufs(void);
+
+void move_current_mr_buf_to_out_list(void);
+
+void add_to_pid_buf_out_offset(__u32 rec_length);
+
+pid_record *get_next_pid_rec_from_out_list(void);
+
+PPID_BUF alloc_new_pid_buf(void);
+
+void add_pid_buf_to_free_list(PPID_BUF p_pid_buf);
+
+void free_pid_bufs(void);
+
+void move_current_pid_buf_to_out_list(void);
+
+#ifdef DSA_SUPPORT_MMAP
+#ifdef KERNEL_26X
+#define mem_map_reserve(p)    set_bit(PG_reserved, &((p)->flags))
+#define mem_map_unreserve(p)  clear_bit(PG_reserved, &((p)->flags))
+#else
+#include <linux/wrapper.h>   /* for call to mem_map_unreserve and mem_map_reserve */
+#endif
+__u32 *dsa_kmalloc_ptr = 0;  /* pointer to unaligned dsa area */
+__u32 g_dsa_size = sizeof(driver_shared_area);
+__u32 g_dsa_kmalloc_size = 0;
+#endif
+
+PER_CPU eachCPU[MAX_PROCESSORS];
+
+#if defined(linux32) || defined(linux32_64)
+__u32 package_status[MAX_PROCESSORS + 1];
+#endif
+
+#ifdef USE_NMI
+
+#include <asm/nmi.h>
+
+int nmi_interrupts;
+
+static nmi_callback_t
+ebs_nmi_callback(struct pt_regs * regs, int cpu)
+{
+  ebs_intr(regs);
+
+  nmi_interrupts++;
+
+  return (1);
+}
+
+static int
+register_nmi_callback(void)
+{
+    nmi_interrupts = 0;
+
+    set_nmi_callback((nmi_callback_t)ebs_nmi_callback);
+
+    return (0);
+}
+
+static int
+unregister_nmi_callback(void)
+{
+    unset_nmi_callback();
+
+    VDK_PRINT_DEBUG("number of NMI interrupts generated: %d\n",nmi_interrupts);
+
+    return (0);
+}
+
+#endif // USE_NMI
+
+#ifdef linux32
+void
+get_cpu_info_for_current_cpu(void *p)
+{
+    __u32 i, j, eax, ebx, ecx, edx;
+    unsigned long ul_logical_processors_per_package, ul_logical_processors_shift;
+    cpu_information *p_cpu_infor0;
+    cpuid_output *pout;
+    cpu_map *pmap;
+
+    p_cpu_infor0 = (cpu_information *) p;
+
+    //
+    // Skip cpu if it's out of requested range 
+    //
+    if (smp_processor_id() >= p_cpu_infor0->ul_num_cpus_available) {
+        return;
+    }
+
+    pmap =
+        (cpu_map *) ((__u32) p_cpu_infor0 + p_cpu_infor0->ul_offset_to_cpu_map_array +
+            (smp_processor_id() * sizeof (cpu_map)));
+    pout =
+        (cpuid_output *) ((__u32) p_cpu_infor0 +
+                 p_cpu_infor0->ul_offset_to_cpuid_output_array +
+                 (smp_processor_id() *
+                  (sizeof (cpuid_output) *
+                   (p_cpu_infor0->ul_num_EAX_inputs +
+                p_cpu_infor0->ul_num_EAX_extended_inputs))));
+    VDK_PRINT_DEBUG("get_cpu_infoForCurrentCpu: p_cpu_infor0 0x%x  pmap 0x%x pout 0x%x \n",
+		    p_cpu_infor0, pmap, pout);
+
+    //
+    // Fill in cpu_map for current cpu
+    //
+    cpuid(1, &eax, &ebx, &ecx, &edx);
+    pmap->ul_cpu_num = smp_processor_id();
+    if (edx & CPUID_HT_MASK) {
+        ul_logical_processors_per_package = (ebx & 0xff0000) >> 16;
+        if (ul_logical_processors_per_package == 0) {
+            ul_logical_processors_per_package = 1;
+        }
+        ul_logical_processors_shift = 0;
+        ul_logical_processors_per_package--;
+        while (1) {
+            ul_logical_processors_shift++;
+            ul_logical_processors_per_package >>= 1;
+            if (!ul_logical_processors_per_package) {
+                break;
+            }
+        }
+        pmap->ul_hardware_thread_num =
+            (ebx >> 24) & ul_logical_processors_shift;
+        pmap->ul_package_num = ((ebx >> 24) >> ul_logical_processors_shift);
+        VDK_PRINT_DEBUG("cpu %d HT enabled HW threads per package %d shift %d \n",
+			smp_processor_id(), (ebx & 0xff0000) >> 16,
+			ul_logical_processors_shift);
+    } else {
+        //
+        // If one HW thread per core (HT disabled), then
+        // set package# = OS cpu number, and HT thread =0.
+        pmap->ul_hardware_thread_num = 0;
+        pmap->ul_package_num = smp_processor_id();
+        VDK_PRINT_DEBUG("cpu %d HT disabled \n", smp_processor_id());
+    }
+
+    //
+    // get cpuid data for standard inputs for current cpu
+    //
+    for (i = 1; i <= p_cpu_infor0->ul_num_EAX_inputs; i++, pout++) {
+        pout->ul_cpu_num = smp_processor_id();
+        pout->ul_EAX_input = i;
+        cpuid(i, &pout->ul_EAX_output, &pout->ul_EBX_output,
+              &pout->ul_ECX_output, &pout->ul_EDX_output);
+    }
+
+    //                                                 
+    // get cpuid data for extended inputs for current cpu
+    //
+    VDK_PRINT_DEBUG("ext outputs pCOUinfoR0 0x%x pout 0x%x std inputs %d  ext inputs %d pout->ul_EDX_output 0x%x \n",
+		    p_cpu_infor0, pout, p_cpu_infor0->ul_num_EAX_inputs,
+		    p_cpu_infor0->ul_num_EAX_extended_inputs, pout->ul_EDX_output);
+    for (i = 0x80000001, j = 0; j < p_cpu_infor0->ul_num_EAX_extended_inputs;
+         i++, j++, pout++) {
+        pout->ul_cpu_num = smp_processor_id();
+        pout->ul_EAX_input = i;
+        cpuid(i, &pout->ul_EAX_output, &pout->ul_EBX_output,
+              &pout->ul_ECX_output, &pout->ul_EDX_output);
+    }
+
+    return;
+}
+
+int
+get_cpu_info(cpu_information * p_cpu_info)
+{
+    __u32 buf_size, num_cpus;
+    cpu_information cpu_info;
+    cpu_information *p_cpu_infor0;
+    __u32 map_offset, map_size, out_offset, out_size;
+
+    //
+    // Validate input
+    //
+
+    if (!p_cpu_info) {
+        return (-EFAULT);
+    }
+
+    if (!access_ok(VERIFY_WRITE, p_cpu_info, sizeof(cpu_information))) {
+        return (-EFAULT);
+    }
+
+    copy_from_user(&cpu_info, p_cpu_info, sizeof(cpu_information));
+    
+    num_cpus = cpu_info.ul_num_cpus_available;
+
+    if (num_cpus > MAX_PROCESSORS) {
+        return (-EINVAL);
+    }
+
+    map_offset = cpu_info.ul_offset_to_cpu_map_array;
+    out_offset = cpu_info.ul_offset_to_cpuid_output_array;
+
+    if (!map_offset || (map_offset < sizeof(cpu_information))) {
+        return (-EINVAL);
+    }
+
+    if (!out_offset || (out_offset < sizeof(cpu_information))) {
+        return (-EINVAL);
+    }
+
+    map_size = num_cpus * sizeof(cpu_map);
+    out_size = num_cpus * (sizeof(cpuid_output) * (cpu_info.ul_num_EAX_inputs + cpu_info.ul_num_EAX_extended_inputs));
+
+    if (map_offset > out_offset) {
+        if ((out_offset + out_size) > map_offset) {
+            return (-EINVAL);
+        }
+        buf_size = map_offset + map_size;
+    }
+    else {
+        if (out_offset > map_offset) {
+            if ((map_offset + map_size) > out_offset) {
+                return (-EINVAL);
+            }
+            buf_size = out_offset + out_size;
+        }
+        else {
+            return (-EINVAL);
+        }
+    }
+
+    VDK_PRINT_DEBUG("get_cpu_info: vmalloc buf_size = %d NumCpus %d mapOffset 0x%x outOffset 0x%x \n",
+		    buf_size, p_cpu_info->ul_num_cpus_available,
+		    p_cpu_info->ul_offset_to_cpu_map_array,
+		    p_cpu_info->ul_offset_to_cpuid_output_array);
+
+
+    if (!access_ok(VERIFY_WRITE, p_cpu_info, buf_size)) {
+        return (-EFAULT);
+    }
+
+    //
+    // Allocate buffer for cpu info
+    //
+
+    p_cpu_infor0 = vmalloc(buf_size);
+
+    if (!p_cpu_infor0) {
+        return (-ENOMEM);
+    }
+    copy_from_user(p_cpu_infor0, p_cpu_info, buf_size);
+
+#ifdef CONFIG_SMP
+    smp_call_function(get_cpu_info_for_current_cpu, (void *) p_cpu_infor0, 1, 1);
+#endif
+    get_cpu_info_for_current_cpu((void *) p_cpu_infor0);
+
+    copy_to_user((void *) p_cpu_info, (void *) p_cpu_infor0, buf_size);
+
+    vfree(p_cpu_infor0);
+
+    return (0);
+}
+#endif              // linux32
+
+#ifdef ENABLE_TGID
+
+/*++
+
+Routine description:
+
+   Determine the process id of thread group or parent process 
+   that spawned the threads
+
+Arguments:
+
+return value:
+
+--*/   
+int get_thread_group_id(struct task_struct *proc_task)
+{
+  int pid;
+  void *proc_vm, *p_proc_vm;
+  struct task_struct *orig_proc_task;
+
+  read_lock(&tasklist_lock);
+  if (proc_task->pid != proc_task->tgid)
+  {
+    pid = proc_task->tgid;
+  }
+  else
+  {
+	  proc_vm = proc_task->mm;
+	  p_proc_vm = VT_GET_PARENT(proc_task)->mm;
+	  while(proc_vm == p_proc_vm)
+	  {
+		  orig_proc_task = proc_task;
+		  proc_task = VT_GET_PARENT(proc_task);
+		  if(orig_proc_task == proc_task)
+			  break;
+		  proc_vm = proc_task->mm;
+		  p_proc_vm = VT_GET_PARENT(proc_task)->mm;
+	  }
+	  pid = proc_task->pid;
+  }
+  read_unlock(&tasklist_lock);
+
+  return(pid);
+}
+
+int find_thread_id(thread_info *p_thread_id)
+{
+  thread_info user_thread;
+  struct task_struct *process_task;
+
+  if (!p_thread_id) {
+    return (-EFAULT);
+  }
+
+  if (!access_ok(VERIFY_WRITE, p_thread_id, sizeof(thread_info))) {
+    return (-EFAULT);
+  }
+
+  copy_from_user(&user_thread, p_thread_id, sizeof(thread_info));
+  
+  if (user_thread.tgrp_id <=0)
+  {
+    return(-EINVAL);
+  }
+    
+  read_lock(&tasklist_lock);
+  process_task = find_task_by_pid(user_thread.tgrp_id);
+  read_unlock(&tasklist_lock);
+
+  user_thread.tgrp_id = get_thread_group_id(process_task);
+    
+  copy_to_user((void *) p_thread_id, (void *) &user_thread, 
+	       sizeof (thread_info));
+  return(0);
+}
+
+#endif // ENABLE_TGID
+
+/*
+ *
+ *  Function: samp_read_cpu_perf_counters
+ *
+ *  Description: 
+ *  Read and Construct Event Totals
+ *
+ *  Parms:
+ *      Entry:      prBuf
+ *  
+ *      Return:     status
+ *
+ */
+int samp_read_cpu_perf_counters(RDPMC_BUF *pr_buf)
+{
+    RDPMC_BUF   *pr_buf_r0;
+
+    pr_buf_r0 = vmalloc(sizeof(RDPMC_BUF));
+
+    if (!pr_buf_r0) {
+        return(-ENOMEM);
+    }
+
+    if (copy_from_user(pr_buf_r0, pr_buf, sizeof(RDPMC_BUF))) {
+        VDK_PRINT_DEBUG("samp_read_cpu_perf_counters: copy_from_user failed \n");
+        return(-EINVAL);
+    }
+
+    //
+    // The caller should set cpuMaskIn and pmcMask.
+    // Set defaults if the fields were not set by caller.
+    //
+    if (pr_buf_r0->cpu_mask_in == 0) {
+        pr_buf_r0->cpu_mask_in = -1;
+    }
+    if (pr_buf_r0->pmc_mask.quad_part == 0) {
+        pr_buf_r0->pmc_mask.low_part = -1;
+    }
+    pr_buf_r0->cpu_mask_out = 0;
+    pr_buf_r0->duration = pdsa->duration;
+
+#ifdef SMP_ON
+    smp_call_function(read_cpu_perf_counters_for_current_cpu, (void *) pr_buf_r0, 1, 1);  
+#endif
+    read_cpu_perf_counters_for_current_cpu((void *) pr_buf_r0);  
+
+#ifdef USE_NMI
+    copy_to_user(pr_buf,pr_buf_r0,sizeof(RDPMC_BUF));
+#else
+    memcpy((void *) pr_buf, (void*) pr_buf_r0, sizeof(RDPMC_BUF));
+#endif
+
+    vfree(pr_buf_r0);
+
+    return(0);
+}
+
+#ifdef linux64
+
+#ifdef PERFMON_SUPPORTED
+static pfm_intr_handler_desc_t desc;
+#endif
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          int install_perf_isr(void) 
+ * @brief       Assign the PMU interrupt to the driver
+ *
+ * @return	zero if successful, non-zero error value if something failed
+ *
+ * Install the driver ebs handler onto the PMU interrupt. If perfmon is
+ * compiled in then we ask perfmon for the interrupt, otherwise we ask the
+ * kernel...
+ *
+ * <I>Special Notes:</I>
+ *
+ * @Note This routine is for Itanium(R)-based systems only!
+ *
+ *	For IA32, the LBRs are not frozen when a PMU interrupt is taken.
+ * Since the LBRs capture information on every branch, for the LBR
+ * registers to be useful, we need to freeze them as quickly as
+ * possible after the interrupt. This means hooking the IDT directly
+ * to call a driver specific interrupt handler. That happens in the
+ * vtxsys.S file via samp_get_set_idt_entry. The real routine being
+ * called first upon PMU interrupt is t_ebs (in vtxsys.S) and that
+ * routine calls ebs_intr()...
+ *
+ */
+int
+install_perf_isr(void)
+{
+    int status = -EPERM;
+
+    VDK_PRINT_DEBUG("install_perf_isr: entered... pmv 0x%p \n", itp_get_pmv());
+
+#ifdef PERFMON_SUPPORTED
+    /*
+     * if PERFMON_SUPPORTED is set, we can use the perfmon.c interface 
+     * to steal perfmon.c's interrupt handler for our use
+     * perfmon.c has already done register_percpu_irq()
+     */
+
+     ebs_irq = VTUNE_PERFMON_IRQ;
+     desc.handler = &ebs_intr;
+     status = pfm_install_alternate_syswide_subsystem(&desc);
+     if (status) {
+         VDK_PRINT_ERROR("install_perf_isr: pfm_install_alternate_syswide_subsystem returned %d\n",status);
+     }
+#else // PERFMON_SUPPORTED
+
+    if (pebs_irqaction) {
+        return (status);
+    }
+
+#ifdef SA_PERCPU_IRQ_SUPPORTED
+
+    ebs_irq = VTUNE_PERFMON_IRQ;
+    pebs_irqaction = (struct irqaction *) 1;
+    status = request_irq(VTUNE_PERFMON_IRQ, ebs_intr,
+			 SA_INTERRUPT | SA_PERCPU_IRQ, "VTune Sampling", NULL);
+
+#else // SA_PERCPU_IRQ_SUPPORTED
+
+    {
+        static char name[] = "VTune";
+
+        pebs_irqaction = kmalloc(sizeof (struct irqaction), GFP_ATOMIC);
+        if (pebs_irqaction) {
+            memset(pebs_irqaction, 0, sizeof (struct irqaction));
+            ebs_irq = VTUNE_PERFMON_IRQ;
+            pebs_irqaction->handler = ebs_intr;
+            pebs_irqaction->flags = SA_INTERRUPT;
+            pebs_irqaction->name = name;
+            pebs_irqaction->dev_id = NULL;
+
+            register_percpu_irq(ebs_irq, pebs_irqaction);
+            status = 0;
+        }
+    }
+#endif // SA_PERCPU_IRQ_SUPPORTED
+
+#endif // PERFMON_SUPPORTED
+
+    VDK_PRINT_DEBUG("install_perf_isr: exit...... rc=0x%x pmv=0x%p \n", status, itp_get_pmv());
+
+    return (status);
+}
+
+void
+uninstall_perf_isr(void)
+{
+#ifdef PERFMON_SUPPORTED
+    int status;
+#endif
+
+    VDK_PRINT_DEBUG("uninstall_perf_isr: entered... pmv=0x%p \n", itp_get_pmv());
+
+#ifdef PERFMON_SUPPORTED
+    /*
+     * if PERFMON_SUPPORTED is set, we used the perfmon.c interface 
+     * to steal perfmon.c's interrupt handler for our use
+     * Now we must release it back.
+     * Don't free_irq() because perfmon.c still wants to use it
+     */
+     status = pfm_remove_alternate_syswide_subsystem(&desc);
+     VDK_PRINT_DEBUG("pfm_remove_alternate_syswide_subsystem returned %d\n", status); 
+     // grumble, no way to return this error to caller, log it and hope
+     if (status != 0) 
+         VDK_PRINT_WARNING("uninstall_perf_isr: pfm_remove_alternate_syswide_subsystem returned: %d\n",status);
+
+#else // PERFMON_SUPPORTED
+
+    if (xchg(&pebs_irqaction, 0)) {
+        free_irq(ebs_irq, NULL);
+    }
+
+#endif // PERFMON_SUPPORTED
+
+    VDK_PRINT_DEBUG("uninstall_perf_isr: exit... pmv=0x%p \n", itp_get_pmv());
+
+    return;
+}
+
+#endif // linux64
+
+/*
+ *
+ *
+ *   Function:  init_driver_OS
+ *
+ *   description:
+ *   Initialize OS specific portions of driver
+ *
+ *   Parms:
+ *       entry:  None
+ *   
+ *       return: status
+ *
+ *
+ */
+int
+init_driver_OS(void)
+{
+    /* Determine CPU support */
+    vdrvgetsysinfo();
+
+    /* Initialize Global Driver Locks */
+    spin_lock_init(&sample_exec_lock);
+    spin_lock_init(&sample_int_lock);
+    spin_lock_init(&reg3f1_write_lock);
+
+    p_current_mr_buf = 0;
+    INIT_LIST_HEAD(&mr_buf_free_list_head);
+    INIT_LIST_HEAD(&mr_buf_out_list_head);
+
+    p_current_pid_buf = 0;
+    INIT_LIST_HEAD(&pid_buf_free_list_head);
+    INIT_LIST_HEAD(&pid_buf_out_list_head);
+
+#if defined(linux32) || defined(linux32_64)
+    app_base_low = LINUX32_APP_BASE_LOW;
+    app_base_high = LINUX32_APP_BASE_HIGH;
+#elif defined(linux64)
+    app_base_low = LINUX64_APP_BASE_LOW;
+    app_base_high = LINUX64_APP_BASE_HIGH;
+#else
+#error Unknown architecture
+#endif
+
+    install_OS_hooks();
+
+    return (0);
+}
+
+/*
+ *
+ *
+ *   Function:  init_module
+ *
+ *   description:
+ *   initialization of driver resources and system registration.
+ *
+ *   Parms:
+ *       entry:  None
+ *   
+ *       return: status
+ *
+ *
+ */
+int
+init_module(void)
+{
+    int result;
+
+    /* Obtain an available device number and obtain a lock on the device */
+
+    result = register_chrdev(0, "vtune", &vtune_fops);
+
+    if (result < 0) {
+        VDK_PRINT_ERROR("Module register Failed!\n");
+        return (result);
+    }
+
+    /* set major node number (created when first arg to register_chrdev is 0) */
+    vtune_major = result;
+
+    /* Send load message to system log file */
+    VDK_PRINT_BANNER("loaded");
+
+#ifdef USE_NMI
+    VDK_PRINT_WARNING("Event calibration is disabled for this kernel.\n");
+    VDK_PRINT_WARNING("Sampling with precise events is disabled for this kernel.\n");
+#endif
+
+    memset(eachCPU, 0, sizeof(eachCPU)); // see also init_driver() in vtlib*.c
+
+    return (0);
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void stop_sampling(BOOLEAN do_wakeup) 
+ * @brief       Is what it says it is...
+ *
+ * @param       do_wakeup IN  - wakeup readers
+ *
+ * @return	none
+ *
+ * Stop any sampling that is going on and does some cleanup
+ *
+ * <I>Special Notes:</I>
+ *
+ *	None
+ */
+void
+stop_sampling(BOOLEAN do_wakeup)
+{
+    VDK_PRINT_DEBUG("stop_sampling: entered current pid %d \n", current->pid);
+
+    /* START - Critical Section */
+    spin_lock(&sample_exec_lock);
+
+    samp_info.sampling_active = FALSE;
+    track_module_loads = FALSE;
+    track_process_creates = FALSE;
+    //
+    // Wait for any in progress interrupt handlers to complete
+    //
+    while (atomic_read(&samp_isr_active)) ;
+
+    if (samp_info.flags & SINFO_STARTED) {
+
+        samp_info.flags &= ~(SINFO_STARTED | SINFO_DO_STOP);
+        samp_info.flags |= (SINFO_DO_WRITE | SINFO_STOPPING);
+        signal_thread_event = FALSE;
+
+        //
+        // Save current time as stop time, cancel max sample time timer,
+        // and signal the thread to write data and free resources.
+        //
+
+        pdsa->duration = (unsigned long) ((jiffies - start_time) * 10);
+        samp_info.sample_time = pdsa->duration; // 12-15-97
+        pdsa->tot_samples = pdsa->sample_count;
+
+        //
+        // Clear current sample count
+        // to avoid confusing future
+        // sampling sessions.
+        //
+        samp_info.sample_count = 0;
+        pdsa->sample_count = 0; // 04-01-96
+        pdsa->pause_count = 0;
+        pdsa->running = 0;
+
+        spin_unlock(&sample_exec_lock);
+
+        VDK_PRINT_DEBUG("sampling stopped. total_loads %d total_loads_init %d  total_pid_updates %d buf size %d \n",
+			total_loads, total_loads_init, total_pid_updates,
+			MR_BUFFER_DATA_SIZE);
+
+        if (sample_method & METHOD_EBS) {
+            vdrv_stop_EBS();
+        }
+
+        if (xchg(&g_start_delay_timer_ON, 0)) {
+            del_timer(&delay_tmr);
+        }
+        if (xchg(&g_max_samp_timer_ON, 0)) {
+            del_timer(&time_out_tmr);
+        }
+
+        if (do_wakeup) {
+            wake_up_interruptible(&pc_write);
+        }
+    } else {
+        spin_unlock(&sample_exec_lock);
+    }
+
+#if defined(ALLOW_LBRS) && defined(linux32)
+    disable_lbr_capture();
+#endif
+
+    return;
+}
+
+void
+samp_cleanup(void)
+{
+    void *pbuf;
+
+    samp_info.flags = SINFO_STOP_COMPLETE;
+
+    /* Free sampling buffer back to OS */
+    if ((pbuf = xchg(&buf_start, 0))) {
+        vfree(pbuf);
+    }
+
+    if (pdsa) {
+        pdsa->pause_count = 0;
+        pdsa->running = 0;
+    }
+
+#ifdef USE_NMI
+      unregister_nmi_callback();
+      return;
+#endif
+
+#ifdef linux64
+    uninstall_perf_isr();
+#endif
+
+    return;
+}
+
+void
+abort_sampling(void)
+{
+    stop_sampling(TRUE);
+    samp_cleanup();
+
+    return;
+}
+
+/*
+ *
+ *
+ *   Function:  cleanup_module
+ *
+ *   description:
+ *
+ *   Parms:
+ *       entry:  None
+ *   
+ *       return: None
+ *
+ *
+ */
+void
+cleanup_module(void)
+{
+    stop_sampling(TRUE);
+    samp_cleanup();
+
+    driver_unload();
+    un_install_OS_hooks();
+    unregister_chrdev(vtune_major, "vtune");
+
+    /* Send unload message to system log file */
+    VDK_PRINT_BANNER("unloaded");
+
+    return;
+}
+
+/*
+ *
+ *
+ *   Function:  vtune_open
+ *
+ *   description:
+ *
+ *   Parms:
+ *       entry:  *inode
+ *               *filp
+ *
+ *       return: status
+ *
+ *
+ */
+int
+vtune_open(struct inode *inode, struct file *filp)
+{  
+#ifdef ENFORCE_ROOT_ONLY_ACCESS
+    if (!capable(CAP_SYS_PTRACE)) {
+        VDK_PRINT_ERROR("module open failed, root access required\n");
+        return (-EPERM);
+    }
+#endif
+  
+    if (MINOR(inode->i_rdev) == 1) {
+        if (file_count) {
+            return (-EBUSY);
+        } else {
+            file_count++;
+        }
+    }
+    driver_open();
+
+    return (0);
+}
+
+/*
+ *
+ *
+ *   Function:  vtune_write
+ *
+ *   description:
+ *
+ *   Parms:
+ *       entry:  *filp
+ *               *buf
+ *               count
+ *               *ppos
+ *   
+ *       return: status
+ *
+ *
+ */
+ssize_t
+vtune_write(struct file * filp, const char *buf, size_t count, loff_t * ppos)
+{
+    return (-ENOSYS);
+}
+
+
+
+/*
+ *
+ *
+ *   Function:  vtune_ioctl
+ *
+ *   description:
+ *
+ *   Parms:
+ *       entry:  *inode
+ *               *filp
+ *               cmd
+ *               *arg
+ *
+ *       return: status
+ *
+ *
+ */
+int
+vtune_ioctl(struct inode *inode, struct file *filp,
+	    unsigned int cmd, unsigned long arg)
+{
+#ifdef USE_NMI
+    samp_parm_ex spx;
+#else
+    samp_parm_ex *spx;
+#endif
+    samp_parm6   *sp6 = NULL;
+    int sp_offset = 0;
+    int sp_length = 0;
+
+    int status = 0;
+
+#ifdef ENFORCE_ROOT_ONLY_ACCESS
+    if (!capable(CAP_SYS_PTRACE))
+      return (-EPERM);
+#endif
+
+    switch (cmd) {
+    case VTUNE_CONFIG_EX:
+    case VTUNE_START_EX:
+
+        //
+        // Verify access to samp parm header
+        //
+        if (!access_ok(VERIFY_READ, arg, sizeof(samp_parm_header))) {
+            status = -EINVAL;
+            break;
+        }
+
+#ifdef USE_NMI
+	memset(&spx,0,sizeof(samp_parm_ex));
+	copy_from_user(&spx, (samp_parm_ex *)arg, sizeof(samp_parm_ex));
+        sp_offset = spx.hdr.sp_offset;
+        sp_length = spx.hdr.sp_length;
+#else
+        spx = (samp_parm_ex *) arg;
+        sp_offset = spx->hdr.sp_offset;
+        sp_length = spx->hdr.sp_length;
+#endif
+	VDK_PRINT_DEBUG("vtune_ioctl(): sp_offset=0x%x, sp_length=0x%x\n",sp_offset,sp_length);
+
+        if (sp_offset < sizeof(samp_parm_header)) {
+            status = -EINVAL;
+            break;
+        }
+
+        //
+        // Verify read access to entire samp parm structure
+        // 
+        if (!access_ok(VERIFY_READ, arg, sp_offset + sp_length)) {
+            status = -EINVAL;
+            break;
+        }
+
+#ifdef USE_NMI
+        switch (spx.hdr.sp_version) {
+#else
+        switch (spx->hdr.sp_version) {
+#endif
+        case 6:
+            //
+            // Configure or start sampling
+            //
+#ifdef USE_NMI
+            sp6 = kmalloc(sp_length, GFP_ATOMIC);
+	    if (sp6==NULL) {
+	      status = -ENOMEM;
+	      break;
+	    }
+	    copy_from_user(sp6, (samp_parm6 *)(arg + sp_offset), sp_length);
+#else
+            sp6 = (void *) spx + sp_offset;
+#endif
+            if (cmd == VTUNE_CONFIG_EX) {
+                status = samp_configure6(sp6, sp_length);
+            }
+            else {
+#ifdef USE_NMI
+	      if (! sp6->calibration) // temporarily disable calibration for hugemem
+#endif
+                status = start_sampling6(sp6, sp_length);
+            }
+#ifdef USE_NMI
+	    kfree(sp6);
+#endif
+            break;
+        default:
+            status = -EINVAL;
+            break;
+        }
+
+        break;
+    case VTUNE_READPERF:
+
+        if (!access_ok(VERIFY_WRITE, arg, sizeof(RDPMC_BUF))) {
+            status = -EINVAL;
+            break;
+        }
+        status = samp_read_cpu_perf_counters((RDPMC_BUF *) arg);
+        break;
+    case VTUNE_STOP:
+
+        status = vtune_sampuserstop();
+        break;
+    case VTUNE_STAT:
+
+        if (!access_ok(VERIFY_WRITE, arg, sizeof(sampinfo_t))) {
+            status = -EINVAL;
+            break;
+        }
+        samp_info.sample_rec_length = pdsa->sample_rec_length;
+        copy_to_user((void *) arg, (void *) &samp_info, sizeof (sampinfo_t));
+        status = 0;
+        break;
+    case VTUNE_SYSINFO:
+
+        if (!access_ok(VERIFY_WRITE, arg, sizeof(vtune_sys_info))) {
+            status = -EINVAL;
+            break;
+        }
+        copy_to_user((void *) arg, (void *) &vtune_sys_info, sizeof (vtune_sys_info));
+        status = 0;
+        break;
+#ifdef linux32
+    case VTUNE_GETCPUINFO:
+
+        status = get_cpu_info((cpu_information *) arg);
+        break;
+#endif
+    case VTUNE_ABORT:
+
+        abort_sampling();
+        status = 0;
+        break;
+#ifdef ENABLE_TGID
+    case VTUNE_GETTGRP:
+        status = find_thread_id((thread_info *) arg);
+        break;
+#endif
+    default:
+        status = -EINVAL;
+        break;
+    }
+
+    return (status);
+}
+
+/*
+ *
+ *
+ *   Function:  samp_write_pc_file
+ *
+ *   description:
+ *
+ *   Parms:
+ *       entry:  *filp
+ *               *buf
+ *               count
+ *               *ppos
+ *
+ *       return: Size
+ *
+ *
+ */
+size_t
+samp_write_pc_file(struct file * filp, char *buf, size_t count, loff_t * ppos)
+{
+    size_t buf_len, bytes_not_copied;
+    BOOLEAN do_copy, do_stop;
+
+    VDK_PRINT_DEBUG("samp_write_pc_file: entered current pid %d \n", current->pid);
+
+    for (;;) {
+        if (xchg(&b_stop_sampling, 0)) {
+            stop_sampling(FALSE);
+        }
+
+        spin_lock(&sample_exec_lock);
+        do_copy = do_stop = FALSE;
+
+        buf_len = (size_t) p_sample_buf - (size_t) buf_start;
+        if ((samp_info.flags & SINFO_DO_WRITE) && buf_len) {
+            do_copy = TRUE;
+        }
+        if (samp_info.flags & SINFO_STOPPING) {
+            do_stop = TRUE;
+        }
+        samp_info.flags &= ~SINFO_DO_WRITE;
+        spin_unlock(&sample_exec_lock);
+
+        bytes_not_copied = 0;
+        if (do_copy) {
+            if (samp_parms.calibration) {
+                do_copy = FALSE;
+            } else {
+                bytes_not_copied =
+                    copy_to_user(buf, buf_start, buf_len);
+                VDK_PRINT_DEBUG("samp_write_pc_file: pid 0x%x copy_to_user size %d bytes_not_copied %d \n",
+				current->pid, buf_len,
+				bytes_not_copied);
+            }
+            p_sample_buf = buf_start;
+            current_buffer_count = 0;
+            memset((char *) buf_start, 0, buf_length);
+        }
+        if (do_copy) {
+            if (bytes_not_copied == 0) {
+                if (sample_method & METHOD_EBS) {
+                    restart_EBS = TRUE;
+                }
+                break;
+            } else {
+                stop_sampling(FALSE);
+                do_stop = TRUE;
+            }
+        }
+        if (do_stop) {
+            VDK_PRINT_DEBUG("samp_write_pc_file: pid 0x%x stopping \n", current->pid);
+            stop_sampling(FALSE);
+            samp_cleanup();
+            buf_len = 0;
+            break;
+        }
+        spin_lock(&sample_exec_lock);
+        if (samp_info.flags & SINFO_STARTED) {
+            if (xchg(&restart_EBS, 0)) {
+                samp_info.sampling_active = TRUE;
+                spin_unlock(&sample_exec_lock);
+                VDK_PRINT_DEBUG("samp_write_pc_file: pid 0x%x resume EBS \n", current->pid);
+                vdrv_resume_EBS();
+            } else {
+                spin_unlock(&sample_exec_lock);
+            }
+        } else {
+            spin_unlock(&sample_exec_lock);
+            buf_len = 0;
+            break;
+        }
+        VDK_PRINT_DEBUG("samp_write_pc_file: pid 0x%x entering sleep \n", current->pid);
+	//        wait_event_interruptible(pc_write, (!samp_info.sampling_active));
+	interruptible_sleep_on(&pc_write);  // fixes Ctrl-C problem during collection
+    }
+
+    return (buf_len);
+}
+
+/*
+ *
+ *
+ *   Function:  samp_write_module_file
+ *
+ *   description:
+ *
+ *   Parms:
+ *       entry:  *fp
+ *               *ubuf
+ *               cnt
+ *               *pp
+ *
+ *   return: status
+ *
+ *
+ */
+ssize_t
+samp_write_module_file(struct file * fp, char *ubuf, size_t cnt, loff_t * pp)
+{
+    unsigned int rec_len, bytes_written, bytes_in_ubuf;
+    char *pubuf;
+    module_record *mra;
+
+    bytes_in_ubuf = cnt;
+    pubuf = ubuf;
+    bytes_written = 0;
+    while ((mra = get_next_mr_rec_from_out_list())) {
+        rec_len = mra->rec_length;
+
+        if (!mra->pid_rec_index_raw) {  // skip modules records that were created before a failed sys_call (eg fork, clone etc.)
+            add_to_mr_buf_out_offset(rec_len);
+            continue;
+        }
+        if (bytes_in_ubuf < rec_len) {
+            break;
+        }
+        if (copy_to_user(pubuf, mra, rec_len)) {
+            VDK_PRINT_DEBUG("samp_write_module_file: copy_to_user failed \n");
+            return (-EFAULT);
+        }
+        add_to_mr_buf_out_offset(rec_len);
+
+        bytes_written += rec_len;
+        pubuf += rec_len;
+        bytes_in_ubuf -= rec_len;
+    }
+    fp->f_pos += bytes_written;
+    VDK_PRINT_DEBUG("samp_write_module_file: cnt %d bytes_written %d \n", cnt, bytes_written);
+
+    return (bytes_written);
+}
+
+/*
+ *
+ *
+ *   Function:  samp_write_pid_file
+ *
+ *   description:
+ *
+ *   Parms:
+ *       entry:  *fp
+ *               *ubuf
+ *               cnt
+ *               *pp
+ *
+ *   return: status
+ *
+ *
+ */
+ssize_t
+samp_write_pid_file(struct file * fp, char *ubuf, size_t cnt, loff_t * pp)
+{
+    unsigned int rec_len, bytes_written, bytes_in_ubuf;
+    char *pubuf;
+    pid_record *pra;
+
+    bytes_in_ubuf = cnt;
+    pubuf = ubuf;
+    bytes_written = 0;
+    while ((pra = get_next_pid_rec_from_out_list())) {
+        rec_len = sizeof (pid_record);
+
+        if (bytes_in_ubuf < rec_len) {
+            break;
+        }
+        if (copy_to_user(pubuf, pra, rec_len)) {
+            VDK_PRINT_DEBUG("samp_write_pid_file: copy_to_user failed \n");
+            return (-EFAULT);
+        }
+        add_to_pid_buf_out_offset(rec_len);
+
+        bytes_written += rec_len;
+        pubuf += rec_len;
+        bytes_in_ubuf -= rec_len;
+    }
+    fp->f_pos += bytes_written;
+    VDK_PRINT_DEBUG("samp_write_pid_file: cnt %d bytes_written %d \n", cnt, bytes_written);
+
+    return (bytes_written);
+}
+
+/*
+ *
+ *
+ *   Function:  vtune_read
+ *
+ *   description:
+ *
+ *   Parms:
+ *       entry:  *filp
+ *               *buf
+ *               count
+ *               *ppos
+ *
+ *       return: Size
+ *
+ *
+ */
+ssize_t
+vtune_read(struct file * filp, char *buf, size_t count, loff_t * ppos)
+{
+    ssize_t n;
+    struct inode *inode = filp->f_dentry->d_inode;
+
+    VDK_PRINT_DEBUG("vtune_read: entered  current pid %d \n", current->pid);
+
+#ifdef ENFORCE_ROOT_ONLY_ACCESS
+    if (!capable(CAP_SYS_PTRACE))
+      return (-EPERM);
+#endif
+
+    //
+    // Add semaphore here for one thread at a time
+    //
+
+    if (xchg(&b_stop_sampling, 0)) {
+        stop_sampling(FALSE);
+    }
+
+    n = 0;
+    switch (MINOR(inode->i_rdev)) {
+    case PCMINOR:
+        n = samp_write_pc_file(filp, buf, count, ppos);
+        break;
+    case MDMINOR:
+        n = samp_write_module_file(filp, buf, count, ppos);
+        if (!n && !(samp_info.flags & SINFO_STARTED)) {
+            move_current_mr_buf_to_out_list();
+            n = samp_write_module_file(filp, buf, count, ppos);
+            if (!n) {
+                free_mr_bufs();
+            }
+        }
+        break;
+    case PIDMINOR:
+        n = samp_write_pid_file(filp, buf, count, ppos);
+        if (!n && !(samp_info.flags & SINFO_STARTED)) {
+            move_current_pid_buf_to_out_list();
+            n = samp_write_pid_file(filp, buf, count, ppos);
+            if (!n) {
+                free_pid_bufs();
+            }
+        }
+        break;
+    default:
+        break;
+    }
+    return (n);
+}
+
+/*
+ *
+ *
+ *   Function:  vtune_release
+ *
+ *   description:
+ *
+ *   Parms:
+ *       entry:  *inode
+ *               *filp
+ *
+ *       return: status
+ *
+ *
+ *
+ */
+int
+vtune_release(struct inode *inode, struct file *filp)
+{
+    if (MINOR(inode->i_rdev) == 1)
+        file_count--;
+
+    return (0);
+}
+
+/*
+ *
+ *
+ *   Function: vdrvgetsysinfo
+ *
+ *   description:
+ *   This routines provides CPU info for local storage and
+ *   export that will be placed into the TB3 file header. For
+ *   local lookup we reference the kernel's exported
+ *   "boot_cpu_data" struct. For export we execute the
+ *   CPUID data and store it accordingly
+ *
+ *
+ *   Parms:
+ *       entry:  *callerinfo
+ *               *system_cpu_data
+ *
+ *       return: status
+ *
+ *
+ */
+
+int
+vdrvgetsysinfo(void)
+{
+    int iloop;
+
+    memset(&vtune_sys_info, 0, sizeof (sys_samp_info));
+    /* Determine CPUID and features */
+#if defined(linux32) || defined(linux32_64)
+    g_this_CPUID = cpuid_eax(1);
+    g_this_CPU_features = cpuid_edx(1);
+    g_CPU_family = (char) ((g_this_CPUID & CPUFAMILY) >> 8);
+    g_CPU_model = (char) ((g_this_CPUID & CPUMODEL) >> 4);
+#elif defined(linux64)
+    g_this_CPUID = itp_get_cpuid(3);   // Itanium(R) processor cpuid reg 3 - version information 39:32 arch rev, 31:24 family, 23:16 model, 15:8 rev, 7:0 largest cpuid index
+    g_this_CPU_features = itp_get_cpuid(4);    //
+    g_CPU_family = (__u32) ((g_this_CPUID & ITP_CPUID_REG3_FAMILY) >> 24);
+    g_CPU_model = (__u32) ((g_this_CPUID & ITP_CPUID_REG3_MODEL) >> 16);
+#else
+#error Unknown processor
+#endif
+
+#ifdef CONFIG_SMP
+    /* Total CPUs detected */
+    vtune_sys_info.num_processors = smp_num_cpus;
+#else
+    vtune_sys_info.num_processors = 1;
+#endif
+
+    /* Zero Out local storage */
+    for (iloop = 0; iloop < MAX_PROCESSORS; iloop++)
+        vtune_sys_info.cpu_I_dmap[iloop] = 0;
+
+#ifdef CONFIG_SMP
+    /* Fill in cpu_ID info for all processors */
+    for (iloop = 0; (iloop < smp_num_cpus) && (iloop < MAX_PROCESSORS); iloop++)
+        vtune_sys_info.cpu_I_dmap[iloop] = g_this_CPUID;
+#else
+    vtune_sys_info.cpu_I_dmap[0] = g_this_CPUID;
+#endif
+
+    /* Store additional CPU info */
+    vtune_sys_info.cpu_feature_bits = g_this_CPU_features;
+
+    vtune_sys_info.feature_set |=
+        FSET_DRV_PROCESS_CREATES_TRACKED_IN_MODULE_RECS;
+
+    /* Driver Version info */
+    vtune_sys_info.sysdrv_version_major = SYS_VERSION_MAJOR;
+    vtune_sys_info.sysdrv_version_minor = SYS_VERSION_MINOR;
+
+#if defined(linux32) || defined(linux32_64)
+    /* Kernel selector info (needed for drilling down to x86-based kernel modules) */
+    vtune_sys_info.kernel_cs = __KERNEL_CS; // see asm/segment.h
+#endif
+
+    VDK_PRINT_DEBUG("vdrvgetsysinfo: CPUID=%08lx  features=%08lx  Family=%X  numcpus=%d \n",
+		    g_this_CPUID, g_this_CPU_features, g_CPU_family,
+		    vtune_sys_info.num_processors);
+
+    return (0);
+}
+
+/*
+ *
+ *
+ *   Function:  vtune_sampuserstop
+ *
+ *   description:
+ *   This routine stops sampling, writes frees resources and 
+ *   optionally copies the sampling information structure to the 
+ *   callers buffer. If sampling mode is EBS we need to disable
+ *   the APIC delivery INTR and signal the task wake up
+ *   manually from here.
+ *   
+ *   Parms:
+ *       entry:  out_buf  - pointer to caller's optional 
+ *                         samp_info buffer
+ *   
+ *       return: status
+ *
+ *
+ */
+int
+vtune_sampuserstop(void)
+{
+    VDK_PRINT_DEBUG("vtune_sampuserstop: pid 0x%x entered \n", current->pid);
+    stop_sampling(TRUE);
+
+    return (0);
+}
+
+/*
+ *
+ *
+ *   Function:   start_sampling6
+ *
+ *   description:
+ *   Sets up sampling session. For TBS sampling  we arm the timer
+ *   handler in the kernel patch. For EBS we call the specific
+ *   setup routine to enable the performance counter overflow
+ *   detect. Startup delay and interval timers are also initialized
+ *   here.
+ *
+ *   Note:   This routine is now a merged source of the original
+ *           driver Config and Start Sampling functions.
+ *
+ *   Parms:
+ *       entry:  samp_parm6
+ *   
+ *       return: status
+ *
+ *
+ */
+int
+start_sampling6(samp_parm6 * sp6, int sp6_len)
+{
+    int i, errCode;
+    unsigned long max_sample_time = 0;
+
+    /* Check Sampling State */
+    if ((samp_info.sampling_active == TRUE)
+        || (samp_info.flags & (SINFO_STARTED | SINFO_STOPPING | SINFO_WRITE)))
+        return (-EBUSY);
+
+    /* Configure for SP6 Parms */
+    errCode = samp_configure6(sp6, sp6_len);
+    if (errCode)
+        return (errCode);
+
+    /* Xfer current SP6 to global store */
+    memcpy(&samp_parms, sp6, sizeof (samp_parm6));
+
+    /* Save sample rate in microseconds and milliseconds. Set default if necessary. */
+    if (!sample_rate_us)
+        sample_rate_us = 1000;
+
+
+    if (sample_method & METHOD_EBS) {
+        for (i=0; i<MAX_PROCESSORS; i++)
+	{
+	  eachCPU[i].processor_status = 0;
+	  eachCPU[i].processor_EBS_status = 0;
+	}
+
+#if defined(linux32) || defined(linux32_64)
+        memset(package_status, 0, sizeof (package_status));
+	/*
+        memset(processor_status, 0, sizeof (processor_status));
+        memset(processor_EBS_status, 0, sizeof (processor_EBS_status));
+	*/
+
+        //
+        // Validate EBS regs
+        //
+        if (validate_EBS_regs()) {
+            // return (STATUS_INVALID_PARAMETER);
+        }
+        //
+        // Make sure apic is enabled. This fixes "no samples" problem after 
+        // going into and out of "standby" mode on laptop 
+        //
+        // CSS: TODO: When the C version has been tested, we can just
+        // remove the x86 assembly version...
+        // Is this really needed? Seems like the answer is no and we can just check
+        // for the apic being re-initialized by the os and then we re-do if needed
+        // (i.e. keep track of what we did in apic init, or maybe even call apic init
+        // again...)
+        //
+#if defined(linux32)
+        SAMP_Set_Apic_Virtual_Wire_Mode();
+#elif defined(linux32_64) 
+        if (!IsApicEnabled()) {
+            //
+            // CSS: TODO: 
+            // Should we just do a full apic init?
+            //
+            VDK_PRINT_WARNING("APIC not enabled. Putting APIC in virtual wire mode.\n");
+            SetVirtualWireMode();
+        }
+#else
+#error Unknown architecture
+#endif
+#endif
+    }
+
+    /* Sanity check to we cannot go lower than 1ms */
+    sample_rate_ms = sample_rate_us / 1000;
+    sample_rate_ms = (sample_rate_ms >= 1) ? sample_rate_ms : 1;
+
+    samp_info.sample_time = 0;
+    samp_info.profile_ints_idt = 0;
+    samp_info.profile_ints = 0;
+    samp_info.sample_count = 0;
+    samp_info.test_IO_errs = 0;
+    current_buffer_count = 0;
+    sample_max_samples = samp_parms.maximum_samples;
+
+    pdsa->sample_session_count++;   //                     06-25-97
+    pdsa->sample_count = 0;
+    pdsa->sample_skip_count = 0;    //                     01-15-99
+    pdsa->suspend_start_count = 0;  //                     01-15-99
+    pdsa->suspend_end_count = 0;    //                     01-15-99
+
+    pdsa->duration = 0;
+    pdsa->tot_samples = 0;
+    pdsa->tot_idt_ints = 0;
+    pdsa->tot_profile_ints = 0;
+    pdsa->tot_skip_samp = 0;
+
+
+    pebs_err = 0;       // clear count for Pentium(R) 4 processor  05-31-00
+
+    /* Remove any previous allocations before initializing sample buffer. */
+    if (buf_start != NULL)
+        vfree(buf_start);
+
+    /* Make sure to check/set default buffer size */
+    if (!samp_parms.samps_per_buffer)
+        samp_parms.samps_per_buffer = 1000;
+
+    /* Allocate sample buffer per user requested size and initialize. 
+     * For safeguard of memory boundary issues with vmalloc() we alloc
+     * extra memory for sampling headroom. buffer size should match
+     * copy_to_user() recipient in user mode. */
+    buf_length = samp_parms.samps_per_buffer * sizeof (sample_record_PC);
+    VDK_PRINT_DEBUG("start_sampling6: samps_per_buffer %d  buf_length %d \n",
+		    samp_parms.samps_per_buffer, buf_length);
+    buf_start = (void *) vmalloc(buf_length);
+
+    /* Check for allocation success */
+    if (!buf_start) {
+        VDK_PRINT_ERROR("Unable to allocate buffer space for sampling!\n");
+        return (-ENOMEM);
+    }
+
+    /* Initialize buffer Contents */
+    memset((char *) buf_start, 0, buf_length);
+
+
+    /* Init Sampling Stats - Sanity Check */
+    samp_info.flags = SINFO_STOP_COMPLETE;
+    samp_info.sampling_active = FALSE;
+    signal_thread_event = FALSE;
+    samp_info.sample_count = 0;
+
+    /* Initialize Sampling buffer */
+    buf_end = (void *) ((unsigned long) buf_start + buf_length);
+    p_sample_buf = buf_start;
+    memset((char *) buf_start, 0, buf_length);
+    current_buffer_count = 0;
+
+    /* Initialize Module load info */
+    num_mod_rec = 0;
+    num_pid_create_rec = 0;
+    // The following pointer should also be initialized!
+    g_MR_pointer_head = NULL;
+
+    /* Reset Timer status */
+    if (g_start_delay_timer_ON) {
+        g_start_delay_timer_ON = FALSE;
+        del_timer(&delay_tmr);
+    }
+    if (g_max_samp_timer_ON) {
+        g_max_samp_timer_ON = FALSE;
+        del_timer(&time_out_tmr);
+    }
+
+    /* Start delay Timer */
+    if (samp_parms.start_delay) {
+        g_start_delay_timer_ON = TRUE;
+        init_timer(&delay_tmr);
+        delay_tmr.expires = jiffies + samp_parms.start_delay * HZ;
+        delay_tmr.function = (void (*)(unsigned long)) samp_start_delay;
+        add_timer(&delay_tmr);
+        interruptible_sleep_on(&samp_delay);
+    }
+
+    /* Set max Sampling Time */
+    b_stop_sampling = FALSE;
+    restart_EBS = FALSE;
+    if (samp_parms.max_interval) {
+        g_max_samp_timer_ON = TRUE;
+        max_sample_time = samp_parms.max_interval * HZ;
+        init_timer(&time_out_tmr);
+        time_out_tmr.function =
+            (void (*)(unsigned long)) samp_max_sample_time_expired;
+        time_out_tmr.expires = jiffies + max_sample_time;
+        add_timer(&time_out_tmr);
+    }
+
+    VDK_PRINT_DEBUG("sampling started pid 0x%x \n", current->pid);
+
+    //
+    // Preallocate buffers to track system-wide module loads           
+    //
+    mgid = 0;
+    p_current_mr_buf = 0;
+    p_mr_buf_out = 0;
+    mr_buf_out_offset = 0;
+    free_mr_bufs();
+    free_pid_bufs();
+
+    samp_info.flags = SINFO_STARTED;
+    samp_info.sampling_active = TRUE;
+
+    /* Kick off all counters on each CPU */
+    for (i = 0; i < MAX_PROCESSORS; i++) {
+        eachCPU[i].start_all = TRUE;
+    }
+
+    /* Track start time for elapsed measurement */
+    start_time = jiffies;
+
+    total_loads = total_loads_init = total_pid_updates = 0; // Debug Code
+
+    track_module_loads = track_process_creates = FALSE;
+    if (!samp_parms.calibration) {
+        for (i = 0; i < 4; i++) {
+            add_mr_buf_to_free_list(alloc_new_mr_buf());
+        }
+        for (i = 0; i < 4; i++) {
+            add_pid_buf_to_free_list(alloc_new_pid_buf());
+        }
+        enum_user_mode_modules();
+        total_loads_init = total_loads; // Debug Code
+        track_module_loads = track_process_creates = TRUE;
+    }
+
+    if (sample_method & METHOD_EBS) {
+#ifdef linux64
+        set_PP_SW();
+        errCode = install_perf_isr();
+	if (errCode != 0) {
+	  VDK_PRINT_ERROR("Unable to install interrupt handler (error=%d)!\n",errCode);
+	  stop_sampling(TRUE);
+	  samp_cleanup();
+	  return (errCode);
+	}
+#elif defined(linux32) || defined(linux32_64)
+	vdrv_init_emon_regs();
+#ifdef USE_NMI
+	register_nmi_callback();
+#endif
+#else
+#error Unkown Architecture
+#endif
+	vdrv_start_EBS(); // causes unexpected IRQ trap at vector 20 when NMI is enabled
+    }
+
+    return (0);
+}
+
+/*
+ *
+ *
+ *   Function:  samp_start_delay
+ *
+ *   description:
+ *   Wake up timer delay handler to start sampling.
+ *
+ *   Parms:
+ *       entry:  ptr
+ *   
+ *       return: None
+ *
+ *
+ */
+void
+samp_start_delay(unsigned long ptr)
+{
+    /* Wake up - Start Sampling! */
+    g_start_delay_timer_ON = FALSE;
+    wake_up_interruptible(&samp_delay);
+
+    return;
+}
+
+/*
+ *
+ *
+ *   Function:  samp_max_sample_time_expired
+ *
+ *   description: 
+ *   Terminates sampling from user timeout spec,
+ *
+ *   Parms:
+ *       entry:  ptr         - pointer to caller's option
+ *                             samp_info buffer
+ *   
+ *       return: None
+ *
+ *
+ */
+void
+samp_max_sample_time_expired(unsigned long ptr)
+{
+    g_max_samp_timer_ON = FALSE;
+    xchg(&b_stop_sampling, 1);
+    wake_up_interruptible(&pc_write);
+
+    return;
+}
+
+/*
+ *
+ *
+ *   Function:  samp_get_stats
+ *
+ *   description:
+ *   This routine copies the sampling information structure
+ *   to the callers buffer.
+ *
+ *   Parms:
+ *       entry:  out_buf       - pointer to caller's optional 
+ *                              samp_info buffer
+ *   
+ *       return: status
+ *
+ *
+ */
+int
+samp_get_stats(sampinfo_t * out_buf)
+{
+    if (out_buf)
+        copy_to_user(out_buf, &samp_info, sizeof (sampinfo_t));
+
+    return (0);
+}
+
+/*
+ *
+ *
+ *   Function:  samp_get_parm
+ *
+ *   description:
+ *   Retrieves the current sampling parameters/configuration 
+ *   contained in samp_parms.
+ *
+ *   Parms:
+ *       entry:  out_buf      - pointer to caller's optional 
+ *                             samp_info buffer
+ *
+ *       return: status  -  OK
+ *       
+ *
+ */
+int
+samp_get_parm(samp_parm3 * out_buf)
+{
+    if (out_buf)
+        copy_to_user(out_buf, &samp_parms, sizeof (samp_parm3));
+
+    return (0);
+}
+
+/*
+******************************************************************************
+                              E B S   S A M P L I N G
+******************************************************************************
+*/
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @brief       Handle the PMU interrupt
+ *
+ * @return	none
+ *
+ * Routine that handles the PMU interrupt and does the actual sampling.
+ *
+ * <I>Special Notes:</I>
+ *
+ *	The parameters are different depending on if you are compiling IA32 or
+ * Itanium(R)-based systems. In the end, what is really significant is that
+ * for IA32, this routine is being called by assembly code, not the usual 
+ * Linux* OS interrupt handler. For IA32, we actually hijack the IDT directly 
+ * which lets us capture LBR information which would otherwise be lost...
+ *
+ * @todo Is there anyway to make the func declaration the same? Is it worth it?
+ *
+ */
+#if defined(linux32) || defined(linux32_64)
+asmlinkage void
+ebs_intr(struct pt_regs *regs)
+#elif defined(linux64)
+void
+ebs_intr(int irq, void *arg, struct pt_regs *regs)
+#else
+#error Unknown architecture
+#endif
+{
+    INT_FRAME int_frame;
+    u32 wake_up_thread;
+
+#ifdef linux32
+
+#if defined(DEBUG) && defined(ALLOW_LBRS)
+    //
+    // If no lbrs, regs == &regs + 8
+    //   (4 for save of quick_freeze_msr, 4 for 1st argument to this routine)
+    // If lbrs are being captured, regs == &regs + 16) 
+    //   (above + old msr/lbr enable values saved on the stack)
+    //
+    if ((((char *) regs) != (((char *) &regs) + 8)) &&
+    	(((char *) regs) != (((char *) &regs) + 16))) {
+#ifndef USE_NMI
+	// using printk's during NMI interrupt handling can cause problems
+	VDK_PRINT_ERROR("interrupt stack appears wrong. regs is 0x%p, &regs is 0x%p\n", regs, &regs);
+#endif
+    }
+#endif
+
+    samp_stop_emon();
+    int_frame.seg_cs = regs->xcs;
+    int_frame.eip = regs->eip;
+    int_frame.E_flags = regs->eflags;
+
+#elif linux64
+    int_frame.iip.quad_part = regs->cr_iip;
+    int_frame.ipsr.quad_part = regs->cr_ipsr;
+    if (int_frame.ipsr.quad_part & IA64_PSR_IS) {   // check addressing mode at time of profile int (Itanium(R) instructions or IA32)
+        unsigned long eflag, csd;
+
+        asm("mov %0=ar.eflag;"  // get IA32 eflags
+            "mov %1=ar.csd;"    // get IA32 unscrambled code segment descriptor
+          :    "=r"(eflag), "=r"(csd));
+        int_frame.E_flags = (__u32) eflag;
+        int_frame.csd = csd;
+        int_frame.seg_cs = (__u32) regs->r17;
+    }
+#elif linux32_64
+    samp_stop_emon();
+    int_frame.iip.quad_part = regs->rip;
+    int_frame.ipsr.quad_part = regs->eflags;
+    int_frame.seg_cs = regs->cs;
+    int_frame.csd = 0;
+#else
+#error unsupported architecture
+#endif
+
+    atomic_inc(&samp_isr_active);
+    wake_up_thread = samp_emon_interrupt(&int_frame);
+
+    //
+    // If time to signal thread then call wake_up_interruptible() to
+    // handle the "wake_up_thread" condition.
+    //
+    if (wake_up_thread) {
+        if (samp_info.flags & SINFO_DO_STOP) {
+            xchg(&b_stop_sampling, 1);
+        }
+        wake_up_interruptible(&pc_write);
+    }
+
+    if (samp_info.sampling_active) {
+        samp_start_emon(FALSE);
+    }
+
+    atomic_dec(&samp_isr_active);
+
+#if defined(linux32_64)
+    ack_APIC_irq();     // C code responsible for ACK'ing the APIC
+#endif
+
+    return;
+}
+
+/*
+ *
+ *
+ *   Function:  vdrv_start_EBS
+ *
+ *   description:
+ *   Dispatches control to activate sampling on all CPUs.
+ *
+ *   Parms:
+ *       entry:  Dispatch Function
+ *               NULL
+ *               1
+ *               0
+ *
+ *       return: status
+ *
+ *
+ */
+void
+vdrv_init_emon_regs(void)
+{
+#if defined(linux32) || defined(linux32_64)
+#ifdef CONFIG_SMP
+    smp_call_function(samp_init_emon_regs, NULL, 1, 1); // wait=TRUE to wait for function to complete on other cpu's
+#endif
+
+    samp_init_emon_regs(NULL);  //
+#endif
+
+    return;
+}
+
+/*
+ *
+ *
+ *   Function:  vdrv_start_EBS
+ *
+ *   description:
+ *   Dispatches control to activate sampling on all CPUs.
+ *
+ *   Parms:
+ *       entry:  Dispatch Function
+ *               NULL
+ *               1
+ *               0
+ *
+ *       return: status
+ *
+ *
+ */
+int
+vdrv_start_EBS(void)
+{
+#ifdef CONFIG_SMP
+    smp_call_function(samp_start_profile_interrupt, NULL, 1, 0);
+#endif
+
+    samp_start_profile_interrupt(NULL);
+
+    return (0);
+}
+
+/*
+ *
+ *   Function:  vdrv_resume_EBS
+ *
+ *   description:
+ *   Dispatches control to resume sampling on all CPUs.
+ *
+ *   Parms:
+ *       entry:  Dispatch Function
+ *               NULL
+ *               1
+ *               0
+ *
+ *       return: status
+ *
+ */
+int
+vdrv_resume_EBS(void)
+{
+#ifdef CONFIG_SMP
+    smp_call_function(samp_start_emon, (void *) 1, 1, 0);    // call samp_start_emon() with do_start=TRUE
+#endif
+
+    samp_start_emon((void *) 1);
+
+    return (0);
+}
+
+/*
+ *
+ *   Function:  vdrv_stop_EBS
+ *
+ *   description:
+ *   Dispatches control to deactivate sampling on all CPUs.
+ *
+ *   Parms:
+ *       entry:  Dispatch Function
+ *               NULL
+ *               1
+ *               0
+ *
+ *       return: None
+ *
+ */
+void
+vdrv_stop_EBS(void)
+{
+#ifdef CONFIG_SMP
+    smp_call_function(samp_stop_profile_interrupt, NULL, 1, 1);     // wait=TRUE
+#endif
+
+    samp_stop_profile_interrupt(NULL);
+
+    return;
+}
+
+/*
+******************************************************************************
+                    M O D U L E / P R O C E S S    T R A C K I N G
+******************************************************************************
+*/
+
+void
+alloc_module_group_ID(PMGID_INFO pmgid_info)
+{
+    if (!pmgid_info) {
+        return;
+    }
+
+    memset(pmgid_info, 0, sizeof (MGID_INFO));
+    spin_lock(&mgid_lock);
+    mgid++;
+    pmgid_info->mgid = mgid;
+    spin_unlock(&mgid_lock);
+
+    return;
+}
+
+PMR_BUF
+get_next_mr_buf_on_out_list(PMR_BUF pmr_buf)
+{
+    __u32 i;
+    LIST_ENTRY *p_list;
+
+    if (!pmr_buf) {
+        if (p_mr_buf_out) {
+            return (p_mr_buf_out);
+        }
+    }
+
+    i = (pmr_buf) ? 0 : 1;
+    list_for_each(p_list, &mr_buf_out_list_head) {
+        if (i) {
+            return ((PMR_BUF) p_list);
+        }
+        if (p_list == &pmr_buf->link) {
+            i++;
+        }
+    }
+
+    return (0);
+}
+
+void
+update_pid_for_module_group(__u32 pid, PMGID_INFO pmgid_info)
+{
+    __u32 mr_mgid, first_mr_found, last_mr_found;
+    void_ptr mr_first, mr_last, pbuf, pbuf_end;
+    PMR_BUF pmr_buf;
+    module_record *mra;
+
+    if (!(samp_info.flags & SINFO_STARTED) || !pmgid_info) {
+        return;
+    }
+
+    total_pid_updates++;    // Debug Code
+
+    VDK_PRINT_DEBUG("update_pid_for_module_group: pid 0x%x mgid %d mrfirst 0x%x mr_last 0x%x \n",
+		    pid, pmgid_info->mgid, pmgid_info->mr_first,
+		    pmgid_info->mr_last);
+
+    spin_lock(&vtune_modlist_lock);
+    mr_first = pmgid_info->mr_first;
+    mr_last = pmgid_info->mr_last;
+    mr_mgid = pmgid_info->mgid;
+
+    for (;;) {
+        if ((mr_first == 0) || (mr_last == 0)) {
+            mr_first = mr_last = 0;
+        }
+        //
+        // Look in current module buffer for module records in module group.
+        //
+        first_mr_found = last_mr_found = 0;
+        pmr_buf = p_current_mr_buf;
+        VDK_PRINT_DEBUG("update_pid_for_module_group: p_current_mr_buf 0x%x \n", p_current_mr_buf);
+        if (pmr_buf) {
+            pbuf = &pmr_buf->buf;
+            pbuf_end = pbuf + pmr_buf->bytes_in_buf;
+            if ((mr_first >= pbuf) && (mr_first <= pbuf_end)) {
+                first_mr_found = TRUE;
+                pbuf = mr_first;
+            }
+            if ((mr_last >= pbuf) && (mr_last < pbuf_end)) {
+                pbuf_end = mr_last + 1;
+            }
+            for (; pbuf < pbuf_end; pbuf += mra->rec_length) {
+                mra = (module_record *) pbuf;
+                VDK_PRINT_DEBUG("update_pid_for_module_group: checking record mra 0x%x pid_rec_index %d \n",
+				mra, mra->pid_rec_index);
+                if (!mra->pid_rec_index_raw
+                    && (mra->pid_rec_index == mr_mgid)) {
+                    mra->pid_rec_index = pid;
+                    mra->pid_rec_index_raw = 1;
+                    VDK_PRINT_DEBUG("update_pid_for_module_group  module record updated pid 0x%x mgid %d mra 0x%x \n",
+				    pid, pmgid_info->mgid, mra);
+                }
+            }
+            if (first_mr_found) {
+                break;
+            }
+        }
+        //
+        // Scan module record buffers on output list from oldest to newest.
+        // Update pid for each module record in the module group.
+        //
+        pmr_buf = 0;
+        first_mr_found = last_mr_found = FALSE;
+        while ((pmr_buf = get_next_mr_buf_on_out_list(pmr_buf))) {
+            VDK_PRINT_DEBUG("Update... 2 pmr_buf 0x%x \n", pmr_buf);
+            pbuf = &pmr_buf->buf;
+            pbuf_end = pbuf + pmr_buf->bytes_in_buf;
+            if (!first_mr_found) {
+                if ((mr_first >= pbuf) && (mr_first <= pbuf_end)) {
+                    first_mr_found = TRUE;
+                    pbuf = mr_first;
+                } else {
+                    if (mr_first) {
+                        continue;   // if mr_first specified, then skip to next (newer) mr buffer
+                    }
+                }
+            }
+            if ((mr_last >= pbuf) && (mr_last < pbuf_end)) {
+                pbuf_end = mr_last + 1;
+                last_mr_found = TRUE;
+            }
+            for (; pbuf < pbuf_end; pbuf += mra->rec_length) {
+                mra = (module_record *) pbuf;
+                VDK_PRINT_DEBUG("update_pid_for_module_group 2 checking record mra 0x%x pid_rec_index %d \n",
+				mra, mra->pid_rec_index);
+                if (!mra->pid_rec_index_raw
+                    && mra->pid_rec_index == mr_mgid) {
+                    mra->pid_rec_index = pid;
+                    mra->pid_rec_index_raw = 1;
+                    VDK_PRINT_DEBUG("update_pid_for_module_group 2  module record updated pid 0x%x mgid %d mra 0x%x \n",
+				    pid, pmgid_info->mgid, mra);
+                }
+            }
+            if (last_mr_found) {
+                break;
+            }
+        }
+        break;
+    }
+
+    spin_unlock(&vtune_modlist_lock);
+
+    return;
+}
+
+#define insert_tail_list(a,b) list_add_tail(b, a)
+
+PLIST_ENTRY
+remove_head_list(PLIST_ENTRY entry)
+{
+    PLIST_ENTRY poped_entry = NULL;
+
+    if (entry->next && (entry->next != entry)) {
+        poped_entry = entry->next;
+        list_del(poped_entry);
+    }
+
+    return (poped_entry);
+}
+
+PMR_BUF
+pop_mr_buf_from_head_of_free_list(void)
+{
+    PLIST_ENTRY entry;
+
+    entry = remove_head_list(&mr_buf_free_list_head);
+    if (entry == &mr_buf_free_list_head) {
+        entry = NULL;
+    }
+
+    return ((PMR_BUF) entry);
+}
+
+PMR_BUF
+pop_mr_buf_from_head_of_out_list(void)
+{
+    PLIST_ENTRY entry;
+
+    entry = remove_head_list(&mr_buf_out_list_head);
+    if (entry == &mr_buf_out_list_head) {
+        entry = NULL;
+    }
+
+    return ((PMR_BUF) entry);
+}
+
+void
+add_mr_buf_to_out_list(PMR_BUF pmr_buf)
+{
+    insert_tail_list(&mr_buf_out_list_head, &pmr_buf->link);
+
+    return;
+}
+
+void
+init_mr_buf(PMR_BUF pmr_buf)
+{
+    if (pmr_buf) {
+        memset(pmr_buf, 0, sizeof (MR_BUF));
+        pmr_buf->buf_size = MR_BUFFER_DATA_SIZE;
+    }
+
+    return;
+}
+
+PMR_BUF
+alloc_new_mr_buf(void)
+{
+    PMR_BUF pmr_buf;
+
+    pmr_buf = (PMR_BUF) allocate_pool(non_paged_pool, sizeof (MR_BUF));
+    if (pmr_buf) {
+        init_mr_buf(pmr_buf);
+        VDK_PRINT_DEBUG("segs buffer allocated. addr 0x%x \n", pmr_buf);  // 07-11-97
+    } else {
+        VDK_PRINT_DEBUG("segs buffer alloc failed.\n");  // 07-11-97
+    }
+
+    return (pmr_buf);
+}
+
+void
+free_mr_bufs(void)
+{
+    PMR_BUF pmr_buf;
+
+    while ((pmr_buf = pop_mr_buf_from_head_of_free_list())) {
+        free_pool(pmr_buf);
+        VDK_PRINT_DEBUG("segs buffer freed from free list 0x%x \n", pmr_buf);
+    }
+
+    while ((pmr_buf = pop_mr_buf_from_head_of_out_list())) {
+        free_pool(pmr_buf);
+        VDK_PRINT_DEBUG("segs buffer freed from out list  0x%x \n", pmr_buf);
+    }
+
+    return;
+}
+
+void
+add_mr_buf_to_free_list(PMR_BUF pmr_buf)
+{
+    if (pmr_buf) {
+        init_mr_buf(pmr_buf);
+        insert_tail_list(&mr_buf_free_list_head, &pmr_buf->link);
+    }
+
+    return;
+}
+
+PMR_BUF
+get_current_mr_buf(void)
+{
+    if (!p_current_mr_buf) {
+        if (!(p_current_mr_buf = pop_mr_buf_from_head_of_free_list())) {
+            p_current_mr_buf = alloc_new_mr_buf();
+        }
+    }
+
+    return (p_current_mr_buf);
+}
+
+void
+move_current_mr_buf_to_out_list(void)
+{
+    PMR_BUF pmr_buf;
+
+    pmr_buf = p_current_mr_buf;
+    p_current_mr_buf = 0;
+    if (pmr_buf)
+        add_mr_buf_to_out_list(pmr_buf);
+    
+    return;
+}
+
+void_ptr
+copy_to_mr_buf(void_ptr pdata, __u32 size)
+{
+    void_ptr pmr;
+    PMR_BUF pmr_buf;
+
+    if (!(pmr_buf = get_current_mr_buf())) {
+        return (0);
+    }
+
+    if ((pmr_buf->bytes_in_buf + size) > pmr_buf->buf_size) {   // will data fit in buffer?
+        move_current_mr_buf_to_out_list();  // ..no
+        return (copy_to_mr_buf(pdata, size));   // try again
+    }
+
+    pmr = &pmr_buf->buf;
+    pmr += pmr_buf->bytes_in_buf;
+    memcpy(pmr, pdata, size);
+    pmr_buf->bytes_in_buf += size;
+
+    if (pmr_buf->bytes_in_buf == pmr_buf->buf_size) {
+        move_current_mr_buf_to_out_list();
+    }
+
+    return (pmr);
+}
+
+void
+add_to_mr_buf_out_offset(__u32 rec_length)
+{
+    mr_buf_out_offset += rec_length;
+
+    return;
+}
+
+module_record *
+get_next_mr_rec_from_out_list(void)
+{
+    PMR_BUF pmr_buf;
+    module_record *pmr = 0;
+
+    //
+    // return address of next module record in 
+    //
+    if (p_mr_buf_out) {
+        if (p_mr_buf_out->bytes_in_buf) {
+            pmr = (module_record *) & p_mr_buf_out->buf[mr_buf_out_offset];
+            // mr_buf_out_offset += pmr->rec_length;
+            if (mr_buf_out_offset < p_mr_buf_out->bytes_in_buf) {
+                return ((module_record *) & p_mr_buf_out->
+                    buf[mr_buf_out_offset]);
+            }
+        }
+        pmr = 0;
+        pmr_buf = xchg(&p_mr_buf_out, 0);
+        mr_buf_out_offset = 0;
+        add_mr_buf_to_free_list(pmr_buf);
+    }
+
+    pmr_buf = pop_mr_buf_from_head_of_out_list();
+    if (pmr_buf) {
+        p_mr_buf_out = pmr_buf;
+        mr_buf_out_offset = 0;
+        if (p_mr_buf_out->bytes_in_buf) {
+            return ((module_record *) & p_mr_buf_out->buf);
+        }
+    }
+
+    return (0);
+}
+
+//
+// Routines to manage pid record buffers
+//
+
+PPID_BUF
+pop_pid_buf_from_head_of_free_list(void)
+{
+    PLIST_ENTRY entry;
+
+    entry = remove_head_list(&pid_buf_free_list_head);
+    if (entry == &pid_buf_free_list_head) {
+        entry = NULL;
+    }
+
+    return ((PPID_BUF) entry);
+}
+
+PPID_BUF
+pop_pid_buf_from_head_of_out_list(void)
+{
+    PLIST_ENTRY entry;
+
+    entry = remove_head_list(&pid_buf_out_list_head);
+    if (entry == &pid_buf_out_list_head) {
+        entry = NULL;
+    }
+
+    return ((PPID_BUF) entry);
+}
+
+void
+add_pid_buf_to_out_list(PPID_BUF p_pid_buf)
+{
+    insert_tail_list(&pid_buf_out_list_head, &p_pid_buf->link);
+
+    return;
+}
+
+void
+init_pid_buf(PPID_BUF p_pid_buf)
+{
+    if (p_pid_buf) {
+        memset(p_pid_buf, 0, sizeof (MR_BUF));
+        p_pid_buf->buf_size = MR_BUFFER_DATA_SIZE;
+    }
+
+    return;
+}
+
+PPID_BUF
+alloc_new_pid_buf(void)
+{
+    PPID_BUF p_pid_buf;
+
+    p_pid_buf = (PPID_BUF) allocate_pool(non_paged_pool, sizeof (MR_BUF));
+    if (p_pid_buf) {
+        init_pid_buf(p_pid_buf);
+        VDK_PRINT_DEBUG("pid buffer allocated. addr 0x%x \n", p_pid_buf); // 07-11-97
+    } else {
+        VDK_PRINT_DEBUG("pid buffer alloc failed.\n"); // 07-11-97
+    }
+
+    return (p_pid_buf);
+}
+
+void
+free_pid_bufs(void)
+{
+    PPID_BUF p_pid_buf;
+
+    while ((p_pid_buf = pop_pid_buf_from_head_of_free_list())) {
+        free_pool(p_pid_buf);
+        VDK_PRINT_DEBUG("pid buffer freed from free list 0x%x \n", p_pid_buf);
+    }
+
+    while ((p_pid_buf = pop_pid_buf_from_head_of_out_list())) {
+        free_pool(p_pid_buf);
+        VDK_PRINT_DEBUG("pid buffer freed from free out  0x%x \n", p_pid_buf);
+    }
+
+    return;
+}
+
+void
+add_pid_buf_to_free_list(PPID_BUF p_pid_buf)
+{
+    if (p_pid_buf) {
+        init_pid_buf(p_pid_buf);
+        insert_tail_list(&pid_buf_free_list_head, &p_pid_buf->link);
+    }
+
+    return;
+}
+
+PPID_BUF
+get_current_pid_buf(void)
+{
+    if (!p_current_pid_buf) {
+        if (!(p_current_pid_buf = pop_pid_buf_from_head_of_free_list())) {
+            p_current_pid_buf = alloc_new_pid_buf();
+        }
+    }
+
+    return (p_current_pid_buf);
+}
+
+void
+move_current_pid_buf_to_out_list(void)
+{
+    PPID_BUF p_pid_buf;
+
+    p_pid_buf = p_current_pid_buf;
+    p_current_pid_buf = 0;
+    if (p_pid_buf) {
+        add_pid_buf_to_out_list(p_pid_buf);
+        /*  Not currently used on Linux*
+           KeSetEvent(&samp_threadEvent,     // signal sampler thread
+           (KPRIORITY) 0,
+           FALSE);
+         */
+    }
+
+    return;
+}
+
+BOOLEAN
+copy_to_pid_buf(void_ptr pdata, __u32 size)
+{
+    void_ptr i;
+    PPID_BUF p_pid_buf;
+
+    if (!(p_pid_buf = get_current_pid_buf())) {
+        return (FALSE);
+    }
+
+    if ((p_pid_buf->bytes_in_buf + size) > p_pid_buf->buf_size) {   // will data fit in buffer?
+        move_current_pid_buf_to_out_list(); // ..no
+        return (copy_to_pid_buf(pdata, size));  // try again
+    }
+
+    i = &p_pid_buf->buf;
+    i += p_pid_buf->bytes_in_buf;
+    memcpy(i, pdata, size);
+    p_pid_buf->bytes_in_buf += size;
+
+    if (p_pid_buf->bytes_in_buf == p_pid_buf->buf_size) {
+        move_current_pid_buf_to_out_list();
+    }
+
+    return (TRUE);
+}
+
+void
+add_to_pid_buf_out_offset(__u32 rec_length)
+{
+    pid_buf_out_offset += rec_length;
+
+    return;
+}
+
+pid_record *
+get_next_pid_rec_from_out_list(void)
+{
+    PPID_BUF p_pid_buf;
+    pid_record *ppr = 0;
+
+    //
+    // return address of next pid record in 
+    //
+    if (p_pid_buf_out) {
+        if (p_pid_buf_out->bytes_in_buf) {
+            ppr = (pid_record *) & p_pid_buf_out->buf[pid_buf_out_offset];
+            // mr_buf_out_offset += pmr->rec_length;
+            if (pid_buf_out_offset < p_pid_buf_out->bytes_in_buf) {
+                return ((pid_record *) & p_pid_buf_out->
+                    buf[pid_buf_out_offset]);
+            }
+        }
+        ppr = 0;
+        p_pid_buf = xchg(&p_pid_buf_out, 0);
+        pid_buf_out_offset = 0;
+        add_pid_buf_to_free_list(p_pid_buf);
+    }
+
+    p_pid_buf = pop_pid_buf_from_head_of_out_list();
+    if (p_pid_buf) {
+        p_pid_buf_out = p_pid_buf;
+        pid_buf_out_offset = 0;
+        if (p_pid_buf_out->bytes_in_buf) {
+            return ((pid_record *) & p_pid_buf_out->buf);
+        }
+    }
+
+    return (0);
+}
+
+unsigned short
+get_exec_mode(struct task_struct *p)
+{
+#if defined(linux32)
+	return ((unsigned short) MODE_32BIT);
+#elif defined(linux32_64)
+	if (!p)
+		return (MODE_UNKNOWN);
+#ifdef KERNEL_26X
+	if (p->thread_info->flags & TIF_IA32)
+#else
+	if (p->thread.flags & THREAD_IA32)
+#endif
+		return ((unsigned short) MODE_32BIT);
+	else
+		return ((unsigned short) MODE_64BIT);
+#elif defined(linux64)
+	return ((unsigned short) MODE_64BIT);
+#endif
+}
+
+int
+samp_load_image_notify_routine(char *name, __u32_PTR base,__u32_PTR size,
+__u32 pid, __u32 options, PMGID_INFO pmgid_info, unsigned short  mode)
+{
+    void_ptr p_mr_rec;
+    char *raw_path;
+    module_record *mra;
+    char buf[sizeof (module_record) + MAXNAMELEN + 32];
+
+    if ((samp_info.flags & SINFO_STARTED)
+        && !(samp_info.flags & SINFO_STOPPING)) {
+
+        total_loads++;  // Debug code
+
+        mra = (module_record *) buf;
+        raw_path = (char *) ((__u32_PTR) mra + sizeof (module_record));
+
+        memset(mra, 0, sizeof (module_record));
+
+        mra->segment_type = mode;
+        mra->load_addr64.quad_part = (__u32_PTR) base;
+        mra->length64.quad_part = size;
+#ifdef KERNEL_26X
+        mra->segment_number = 1; // for user modules
+#endif
+        mra->global_module_Tb5 = options & LOPTS_GLOBAL_MODULE;
+        mra->first_module_rec_in_process = options & LOPTS_1ST_MODREC;
+        mra->unload_sample_count = 0xFFFFFFFF;
+        if (pmgid_info && pmgid_info->mgid) {
+            mra->pid_rec_index = pmgid_info->mgid;
+        } else {
+            mra->pid_rec_index = pid;
+            mra->pid_rec_index_raw = 1; // raw pid
+#if defined(DEBUG)
+            if (total_loads_init) {
+                VDK_PRINT_DEBUG("samp_load_image_notify: setting pid_rec_index_raw pid 0x%x %s \n", pid, name);
+            }
+#endif
+        }
+        strncpy(raw_path, name, MAXNAMELEN);
+        raw_path[MAXNAMELEN] = 0;
+        mra->path_length = (__u16) strlen(raw_path) + 1;
+        mra->rec_length =
+            (__u16) ((sizeof (module_record) + mra->path_length + 7) & ~7);
+
+#if defined(linux32)
+	mra->selector = (pid==0) ? __KERNEL_CS : __USER_CS;
+#endif
+#if defined(linux32_64)
+	if (mode==MODE_64BIT)
+	  mra->selector = (pid==0) ? __KERNEL_CS : __USER_CS;
+	else if (mode==MODE_32BIT)
+	  mra->selector = (pid==0) ? __KERNEL32_CS : __USER32_CS;
+	// 0 otherwise ...
+#endif
+        //
+        // See if this module is the Exe for the process
+        // 10/21/97 
+        //
+#if defined(linux64) || defined(linux32)
+        if ((mra->load_addr64.quad_part >= app_base_low) &&
+            (mra->load_addr64.quad_part < app_base_high))
+            mra->exe = 1;
+	else
+	  mra->exe = 0;
+#endif
+#if defined(linux32_64)
+	// 0x0000000040000000 = 1GB  SLES 8
+	//         0x40000000  TASK_UNMAPPED_32 in include/asm/processor.h SLES 8
+	// 0x0000000000400000 = 4MB  RH EL 3 64-bit
+	// 0x08048000 = ?  RH EL 3 32-bit
+	if (mode==MODE_64BIT)
+	  if ( ((mra->load_addr64.quad_part >= 0x0000000000400000) &&
+		(mra->load_addr64.quad_part <  0x0000000000400000 + 0x00000000002FFFFF))
+	       ||
+	       ((mra->load_addr64.quad_part >= 0x0000000040000000) &&
+		(mra->load_addr64.quad_part <  0x0000000040000000 + 0x00000000002FFFFF)) )
+	    mra->exe = 1;
+	  else
+	    mra->exe = 0;
+	else if (mode==MODE_32BIT)
+	  if ((mra->load_addr64.quad_part >= 0x08048000) &&
+	      (mra->load_addr64.quad_part <  0x08048000 + 0x00FFFFFF))
+	    mra->exe = 1;
+	  else
+	    mra->exe = 0;
+	else
+	  mra->exe = 0;  // unknown
+#endif // linux32_64
+
+        spin_lock(&vtune_modlist_lock);
+        mra->load_sample_count = pdsa->sample_count;    // get sample count while lock is held
+        // ..to ensure modules records are in sample count
+        // ..oreder in the sample file (.tb5 file)
+        p_mr_rec = copy_to_mr_buf(mra, mra->rec_length);
+
+        if (p_mr_rec && pmgid_info && pmgid_info->mgid) {
+            if (!pmgid_info->mr_first) {
+                pmgid_info->mr_first = pmgid_info->mr_last = p_mr_rec;  // save address of module record in module group
+            } else {
+                pmgid_info->mr_last = p_mr_rec;
+            }
+        }
+        spin_unlock(&vtune_modlist_lock);
+    }
+
+    return (STATUS_SUCCESS);
+}
+
+//
+// create Process Notify Routine
+//
+void
+samp_create_process_notify_routine(__u32 parent_id, __u32 process_id, __u32 create)
+{
+    char buf[sizeof (pid_record) + 16]; // pid record + some for roundup
+    pid_record *pr;
+
+    if (create) {
+        VDK_PRINT_DEBUG("create process..... pid 0x%x \n", process_id);
+    } else {
+        VDK_PRINT_DEBUG("terminate process.. pid 0x%x \n", process_id);
+        // FreeProcessinfo(process_id);
+        // don't need to track terminates since we deduce terminates
+        // from creates 
+        return;
+
+    }
+
+    if ((samp_info.flags & SINFO_STARTED)
+        && !(samp_info.flags & SINFO_STOPPING)) {
+
+        pr = (pid_record *) buf;
+        memset(pr, 0, sizeof (pid_record));
+        pr->rec_length = (sizeof (pid_record) + 7) & ~7;    // mult of 8
+        pr->pid_event = (create) ? 0 : 1;   // pid_event = 0 = process create, 1 = process terminate
+        pr->os_pid = (__u32) process_id;
+        pr->sample_count = pdsa->sample_count;
+        pr->sample_count_term = 0xFFFFFFFF;
+        pr->path_length = 0;
+        pr->filename_offset = 0;
+
+        spin_lock(&pidlist_lock);
+
+        copy_to_pid_buf(pr, pr->rec_length);
+
+        spin_unlock(&pidlist_lock);
+    }
+
+    return;
+}
+
+#ifdef DSA_SUPPORT_MMAP
+int 
+vtune_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+	__u32 u32_size;
+	__u32 u32_offset;
+
+	VDK_PRINT_DEBUG("vtune_mmap:entering vtune_mmap\n");
+
+	vma->vm_flags |= VM_LOCKED;
+/*	vma->vm_private_data =  */
+	
+	u32_offset = vma->vm_pgoff << PAGE_SHIFT;
+	u32_size = vma->vm_end - vma->vm_start;
+
+	if(u32_offset & ~PAGE_MASK)
+	{
+		VDK_PRINT_ERROR("vtune_mmap: Aborting mmap because offset not page aligned. Offset: %d\n", u32_offset);
+		return (-ENXIO);
+	}
+
+	if(!g_dsa_kmalloc_size || (u32_size > g_dsa_kmalloc_size))
+	{
+		VDK_PRINT_ERROR("vtune_mmap: mmap trying to map something bigger than driver shared area. Size to map: %d\n", u32_size);
+/*		return (-ENXIO); */
+	}
+
+#ifdef REMAP_PAGE_RANGE_REQUIRES_EXTRA_ARGS
+	if(remap_page_range(vma, vma->vm_start,__pa(pdsa),pdsa->length,vma->vm_page_prot))
+#else
+	if(remap_page_range(vma->vm_start,__pa(pdsa),pdsa->length,vma->vm_page_prot))
+#endif
+	{
+		VDK_PRINT_ERROR("vtune_mmap: Aborting mmap because remap_page_range failed\n");
+		return (-EAGAIN);
+	}
+
+	VDK_PRINT_DEBUG("vtune_mmap: pdsa->length=%d\n", pdsa->length);
+	VDK_PRINT_DEBUG("vtune_mmap: pdsa->driver_version=%d\n", pdsa->driver_version);
+	VDK_PRINT_DEBUG("vtune_mmap: pdsa->pause_count=%d\n", pdsa->pause_count);
+	VDK_PRINT_DEBUG("vtune_mmap: pdsa->area_version=%d\n", pdsa->area_version);
+	VDK_PRINT_DEBUG("vtune_mmap: pdsa->mega_hertz=%d\n", pdsa->mega_hertz);
+	VDK_PRINT_DEBUG("vtune_mmap: pdsa->tot_samples=%d\n", pdsa->tot_samples);
+
+	VDK_PRINT_DEBUG("vtune_mmap:exiting vtune_mmap\n");
+
+	return (0);
+}
+
+/*
+  Routine description:
+	Allocates memory for the DSA. If memory was not allocated on a page boundary, then the pointer to
+	the DSA is set on a page boundary in the memory block. The pages of the DSA are then reserved so
+	the call to remap_page_range in vtune_mmap can access them.
+
+   Arguments:
+	none
+   Return value:
+	non-NULL pointer if successful
+*/
+driver_shared_area *
+create_dsa(void)
+{
+	unsigned long virt_addr;
+	/* pointer to page aligned area */
+	__u32 *kmalloc_area = NULL;
+	
+	g_dsa_kmalloc_size = g_dsa_size + 2*PAGE_SIZE; /* size of memory block allocated for DSA */
+	VDK_PRINT_DEBUG("create_dsa: dsa size: %d\n",g_dsa_size);
+	/* 
+	 * allocate space for the driver shared area plus 2 pages. The extra 2 pages could be
+	 * be necessary when the pointer to the DSA is moved onto a page boundary. 
+	 * We want the DSA pointer on a page boundry because ????????????????
+	 * If the DSA pointer is moved to a page boundry we want an extra page on each end of the DSA,
+	 * so that we don't expose ??????
+	 */
+	if(dsa_kmalloc_ptr != NULL)
+	{
+		VDK_PRINT_ERROR("create_dsa: DSA already allocated\n");
+		return (NULL);
+	}
+	VDK_PRINT_DEBUG("create_dsa: kmalloc bytes: %d\n",(g_dsa_size + 2*PAGE_SIZE));
+	dsa_kmalloc_ptr = kmalloc( g_dsa_kmalloc_size, GFP_ATOMIC);
+	if(dsa_kmalloc_ptr == NULL)
+	{
+		VDK_PRINT_ERROR("create_dsa: couldn't kmalloc DSA\n");
+		return (NULL);
+	}
+	VDK_PRINT_DEBUG("create_dsa: kmalloc_ptr: %p\n",dsa_kmalloc_ptr);
+    
+	/* 
+	 * Just zero out the driver_shared_area (not all of the allocated pages). 
+	 * There is no reason why we couldn't zero out everything if we wanted to. 
+	 */
+	memset(dsa_kmalloc_ptr, 0, g_dsa_kmalloc_size);
+	
+	/* move pointer onto a page boundry */
+	kmalloc_area = (__u32 *)(((unsigned long)dsa_kmalloc_ptr + PAGE_SIZE -1) & PAGE_MASK);
+	VDK_PRINT_DEBUG("create_dsa: kmalloc_area: %p\n",kmalloc_area);
+
+	/* reserve the page so remap_page_range can access it */
+	for (virt_addr=(unsigned long)kmalloc_area; virt_addr < (unsigned long)kmalloc_area + g_dsa_size; virt_addr += PAGE_SIZE)
+	{
+	  VDK_PRINT_DEBUG("create_dsa: reserving dsa address: %p which is page address:%p\n",(void*)virt_addr, virt_to_page(virt_addr));
+	  mem_map_reserve(virt_to_page(virt_addr));
+	}
+
+	/* 
+	 * set the length of the driver_shared_area; we will use this when the DSA is destroyed
+	 * to un-reserve the pages
+	 */
+	((driver_shared_area*)kmalloc_area)->length = g_dsa_size;
+	
+	VDK_PRINT_DEBUG("create_dsa: created DSA size:%d\n",g_dsa_size);
+	return ((driver_shared_area*)kmalloc_area);
+}
+
+
+/*
+  Routine description:
+       Checks to see if the pointer to the memory allocated for the DSA is not null and if not sets
+       it to zero. Then calculates a pointer to the first reserved page and un-reserves the reserved 
+       pages of the DSA. After that the allocated memory for the DSA is freed.
+
+  Arguments:
+       none
+
+  Return value:
+       0 always
+*/
+int 
+destroy_dsa(void)
+{
+	unsigned long virt_addr;
+	void_ptr p;
+	/* pointer to page aligned area */
+	__u32 *kmalloc_area = NULL;
+
+	/* 
+	 * Free all of the pages we allocated; remember dsa_ptr could point to a page boundary inside
+	 * the total allocated area, so we need to use the pointer we got back from the 
+	 * kmalloc to free all of the pages.
+	 */
+	if ((p = xchg(&dsa_kmalloc_ptr, 0))) 
+	{
+		/* move pointer onto a page boundry to get the first page we reserved */
+		kmalloc_area = (__u32 *)(((unsigned long)p + PAGE_SIZE -1) & PAGE_MASK);
+		VDK_PRINT_DEBUG("create_dsa: kmalloc_area: %p\n",kmalloc_area);
+
+		/* reserve the page so remap_page_range can access it */
+		for(virt_addr=(unsigned long)kmalloc_area; virt_addr < (unsigned long)kmalloc_area + g_dsa_size; virt_addr += PAGE_SIZE)
+		{
+		  VDK_PRINT_DEBUG("create_dsa: reserving dsa address: %p which is page address:%p\n",
+				  (void*)virt_addr, virt_to_page(virt_addr));
+		  mem_map_unreserve(virt_to_page(virt_addr));
+		}
+
+		free_pool(p);
+		VDK_PRINT_DEBUG("destroy_dsa: DSA freed\n");
+	}
+
+	return (0);
+}
+
+#endif // DSA_SUPPORT_MMAP
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          void sample_skipped(void)
+ * @brief       Bookkeeping when a sample is skipped (i.e driver is "paused")
+ *
+ * @return	none
+ *
+ * Currently skipping a smaple just means updating a count in the
+ * driver shared area.
+ *
+ * <I>Special Notes:</I>
+ *
+ *	None
+ *
+ */
+void
+sample_skipped(void)
+{
+    pdsa->sample_skip_count++;  // profile interrupt occured but PC sample
+                                // ..is being skipped
+    return;
+}
+
+/* ------------------------------------------------------------------------- */
+/*!
+ * @fn          BOOLEAN check_pause_mode(void) 
+ * @brief       See if the driver is skipping samples (ie. is "paused")
+ *
+ * @return	true if skipping samples, false otherwise
+ *
+ * <I>Special Notes:</I>
+ *
+ *	None
+ *
+ */
+BOOLEAN
+check_pause_mode(void)
+{
+    return ((pdsa->pause_count > 0) ? TRUE : FALSE);
+}
+
+#ifndef OLDER_KERNEL_SUPPORT
+MODULE_LICENSE("GPL");
+#endif
+
+#ifdef KERNEL_26X
+MODULE_INFO(vermagic, VERMAGIC_STRING);
+#else
+EXPORT_NO_SYMBOLS;
+#endif
diff -urN linux.orig/drivers/char/vtune/vtxsys32_64.S linux/drivers/char/vtune/vtxsys32_64.S
--- linux.orig/drivers/char/vtune/vtxsys32_64.S	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/vtxsys32_64.S	2004-04-01 06:09:47.000000000 -0800
@@ -0,0 +1,385 @@
+#
+#  vtxsys32_64.S
+#
+#  Copyright (C) 2002-2004 Intel Corporation
+#  Maintainer - Juan Villacis <juan.villacis@intel.com>
+#
+# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+#
+#  This program is free software; you can redistribute it and/or modify
+#  it under the terms of the GNU General Public License as published by
+#  the Free Software Foundation; either version 2 of the License, or
+#  (at your option) any later version.
+#
+#  This program is distributed in the hope that it will be useful,
+#  but WITHOUT ANY WARRANTY; without even the implied warranty of
+#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+#  GNU General Public License for more details.
+#
+#  You should have received a copy of the GNU General Public License
+#  along with this program; if not, write to the Free Software
+#  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+#
+# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+#
+# ===========================================================================
+#
+#	File: vtxsys32_64.S
+#
+#	Description: routines for sampling on Pentium(R) 4 processors
+#                    with Intel(R) Extended Memory 64 Technology
+#
+#	Author(s): Charles Spirakis, Intel Corp.
+#	           Thomas M Johnson, Intel Corp.
+#
+#	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+#
+# ===========================================================================
+#
+
+#define ASSEMBLY    1
+
+#include <asm/calling.h>
+#include <asm/msr.h>
+#include <asm/errno.h>
+#include <asm/segment.h>
+#include <asm/unistd.h>
+#include <asm/ia32_unistd.h>
+#include <asm/current.h>
+		
+#define ORIG_RCX_OFFSET    0xD8
+#define VT_ARGS_SKIP       56
+#define RET_VAL_PTR      0xA8
+#define FINAL_RET_VAL_IA32 RAX+VT_ARGS_SKIP+8*9+8
+#define ORIG_STACK_RIP     0x108
+#define ORIG_STACK_CS      0x110
+#define ORIG_STACK_EFLAGS  0x118
+#define ORIG_STACK_RSP     0x120
+#define ORIG_STACK_SS      0x128
+	
+.text
+
+#define PDAREF(field) %gs:field	 		
+	
+.macro FIXUP_TOP_OF_STACK tmp
+	movq	PDAREF(pda_oldrsp),\tmp
+	movq  	\tmp,RSP(%rsp)
+	movq    $__USER_DS,SS(%rsp)
+	movq    $__USER_CS,CS(%rsp)
+	movq	$-1,RCX(%rsp)	/* contains return address, already in RIP */
+	movq	R11(%rsp),\tmp  /* get eflags */
+	movq	\tmp,EFLAGS(%rsp)
+.endm
+
+.macro RESTORE_TOP_OF_STACK tmp,offset=0
+	movq   RSP-\offset(%rsp),\tmp
+	movq   \tmp,PDAREF(pda_oldrsp)
+	movq   EFLAGS-\offset(%rsp),\tmp
+	movq   \tmp,R11-\offset(%rsp)
+.endm
+
+.macro IA32_ARG_FIXUP
+	movl %edi,%r8d
+	movl %ebp,%r9d
+	xchg %ecx,%esi
+	movl %ebx,%edi
+	movl %edx,%edx
+.endm
+
+.macro UNDO_IA32_ARG_FIXUP
+	movq R8-ARGOFFSET+8(%rsp),%r8
+	movq R9-ARGOFFSET+8(%rsp),%r9
+	movq RCX-ARGOFFSET+8(%rsp),%rcx
+	movq RSI-ARGOFFSET+8(%rsp),%rsi
+	movq RDI-ARGOFFSET+8(%rsp),%rdi
+	movq RDX-ARGOFFSET+8(%rsp),%rdx
+.endm
+
+.macro COMPLETE_STACK_FRAME
+	movq %rax, ORIG_RAX(%rsp)
+	movq ORIG_STACK_RIP(%rsp), %rax
+	movq %rax, RIP(%rsp)
+	movq ORIG_STACK_CS(%rsp), %rax
+	movq %rax, CS(%rsp)
+	movq ORIG_STACK_EFLAGS(%rsp), %rax
+	movq %rax, EFLAGS(%rsp)
+	movq ORIG_STACK_RSP(%rsp), %rax
+	movq %rax, RSP(%rsp)
+	movq ORIG_STACK_SS(%rsp), %rax
+	movq %rax, SS(%rsp)
+	movq ORIG_RAX(%rsp), %rax
+.endm
+		
+#***********************************************************************
+#
+#    GetIDTDesc
+#            Get the IDT Desc address
+#
+#    Entry:  pointer to location to store idt Desc
+#
+#    Exit:  none 
+#
+# void GetIDTDesc(U64 *pIdtDesc);
+#
+#***********************************************************************
+        .global GetIDTDesc
+GetIDTDesc:
+        SIDT (%rdi)
+        ret
+
+#***********************************************************************
+#
+#    GetGDTDesc
+#            Get the GDT Desc address
+#
+#    Entry:  pointer to location to store gdt Desc
+#
+#    Exit:  none 
+#
+# void GetGDTDesc(U64 *pGdtDesc);
+#
+#***********************************************************************
+        .global GetIDTDesc
+GetGDTDesc:
+        SGDT (%rdi)
+        ret
+
+# ----------------------------------------------------------------------------
+# name:         t_ebs
+#
+# description:  ISR entry for local APIC PERF interrupt vector
+#
+# Input:        n/a
+#
+# Output:       n/a 
+# ----------------------------------------------------------------------------
+
+        .global t_ebs
+t_ebs:
+        pushq %rax          // fake an error code...
+        cld                 // cause the kernel likes it this way...
+
+        SAVE_ALL            // Save the world!
+
+        movl  $MSR_GS_BASE,%ecx     // for the moment, do the safe swapgs check
+        rdmsr
+        xorl  %ebx,%ebx             // assume no swapgs (ebx == 0)
+        testl %edx,%edx
+        js    1f
+        swapgs
+        movl  $1,%ebx               // ebx == 1 means we did a swapgs
+1:      movq %rsp,%rdi              // pt_regs is the first argument
+
+        //
+        // ebx is zero if no swap, one if swap
+        // ebx is preserved in C calling convention...
+        //
+        // NOTE: the C code is responsible for ACK'ing the APIC!!!
+        //
+        call ebs_intr
+
+        //
+        // Don't want an interrupt while we are doing the swapgs stuff
+        //
+        cli
+        testl %ebx,%ebx
+        jz 2f
+        swapgs
+2:      RESTORE_ALL
+        popq    %rax
+        iretq
+
+# ----------------------------------------------------------------------------
+# name:         vt_clone_prologue
+#
+# description:  Entry point for intercepting 64-bit clone system call.
+#
+# Input:        partial stack frame expected by clone system call
+#
+# Output:       clone result value
+# ----------------------------------------------------------------------------
+
+	.global vt_clone_prologue
+vt_clone_prologue:
+	SAVE_ARGS VT_ARGS_SKIP,0
+	movq ORIG_RCX_OFFSET(%rsp),%rcx
+	movq %rax,ORIG_RAX-ARGOFFSET(%rsp)
+	movq %rcx,RIP-ARGOFFSET(%rsp)
+	movq %r10,%rcx
+	SAVE_REST
+	movq $-ENOSYS,RAX(%rsp)
+	FIXUP_TOP_OF_STACK %rdi
+	call vt_sys_clone_enter
+vt_clone_prologue_done:
+	movq %rax,RET_VAL_PTR(%rsp)
+	LOAD_ARGS ARGOFFSET
+	RESTORE_REST
+	movq %r10,%rcx
+	movq original_sys_clone,%rax
+	call *%rax
+vt_clone_epilogue:
+	movq %rax,RAX-ARGOFFSET(%rsp)
+	SAVE_REST
+	movq %rsp,%rdi
+	movq ORIG_RAX(%rsp),%rax
+	movq RET_VAL_PTR(%rsp),%rsi
+	movq RAX(%rsp),%rdi
+	call vt_sys_clone_exit
+vt_clone_cleanup:	
+	RESTORE_TOP_OF_STACK %rbx
+	RESTORE_REST
+	RESTORE_ARGS 0,VT_ARGS_SKIP,0
+	retq
+
+# ----------------------------------------------------------------------------
+# name:         vt_fork32_prologue
+#
+# description:  Entry point for intercepting 32-bit fork system call.
+#
+# Input:        partial stack frame expected by fork system call
+#
+# Output:       fork result value
+# ----------------------------------------------------------------------------
+
+	.global vt_fork32_prologue
+vt_fork32_prologue:
+	UNDO_IA32_ARG_FIXUP
+	SAVE_ARGS VT_ARGS_SKIP
+	SAVE_REST
+	COMPLETE_STACK_FRAME
+	movq $-ENOSYS,RAX(%rsp)
+	call vt_sys32_fork_enter
+vt_fork32_prologue_done:
+	movq %rax,RET_VAL_PTR(%rsp)
+	LOAD_ARGS ARGOFFSET
+	addq $ARGOFFSET,%rsp
+	IA32_ARG_FIXUP
+	movq original_ia32_sys_fork,%rax
+	call *%rax
+vt_fork32_epilogue:
+	movq %rax,RAX-ARGOFFSET(%rsp)
+	SAVE_REST
+	movq RET_VAL_PTR(%rsp),%rsi
+	movq RAX(%rsp),%rdi
+	call vt_sys32_fork_exit
+vt_fork32_epilogue_done:
+	movq RAX(%rsp),%rax
+	movq %rax,FINAL_RET_VAL_IA32(%rsp)
+	addq $ARGOFFSET,%rsp
+	addq $VT_ARGS_SKIP+8*9,%rsp
+	retq
+
+# ----------------------------------------------------------------------------
+# name:         vt_vfork32_prologue
+#
+# description:  Entry point for intercepting 32-bit vfork system call.
+#
+# Input:        partial stack frame expected by vfork system call
+#
+# Output:       vfork result value
+# ----------------------------------------------------------------------------
+
+	.global vt_vfork32_prologue
+vt_vfork32_prologue:
+	UNDO_IA32_ARG_FIXUP
+	SAVE_ARGS VT_ARGS_SKIP
+	SAVE_REST
+	COMPLETE_STACK_FRAME
+	movq $-ENOSYS,RAX(%rsp)
+	call vt_sys32_vfork_enter
+vt_vfork32_prologue_done:
+	movq %rax,RET_VAL_PTR(%rsp)
+	LOAD_ARGS ARGOFFSET
+	addq $ARGOFFSET,%rsp
+	IA32_ARG_FIXUP
+	movq original_ia32_sys_vfork,%rax
+	call *%rax
+vt_vfork32_epilogue:
+	movq %rax,RAX-ARGOFFSET(%rsp)
+	SAVE_REST
+	movq RET_VAL_PTR(%rsp),%rsi
+	movq RAX(%rsp),%rdi
+	call vt_sys32_vfork_exit
+vt_vfork32_epilogue_done:
+	movq RAX(%rsp),%rax
+	movq %rax,FINAL_RET_VAL_IA32(%rsp)
+	addq $ARGOFFSET,%rsp
+	addq $VT_ARGS_SKIP+8*9,%rsp
+	retq
+
+# ----------------------------------------------------------------------------
+# name:         vt_clone32_prologue
+#
+# description:  Entry point for intercepting 32-bit clone system call.
+#
+# Input:        partial stack frame expected by clone system call
+#
+# Output:       clone result value
+# ----------------------------------------------------------------------------
+
+	.global vt_clone32_prologue
+vt_clone32_prologue:
+	UNDO_IA32_ARG_FIXUP
+	SAVE_ARGS VT_ARGS_SKIP
+	SAVE_REST
+	COMPLETE_STACK_FRAME
+	movq $-ENOSYS,RAX(%rsp)
+	call vt_sys32_clone_enter
+vt_clone32_prologue_done:
+	movq %rax,RET_VAL_PTR(%rsp)
+	LOAD_ARGS ARGOFFSET
+	addq $ARGOFFSET,%rsp
+	IA32_ARG_FIXUP
+	movq original_ia32_sys_clone,%rax
+	call *%rax
+vt_clone32_epilogue:
+	movq %rax,RAX-ARGOFFSET(%rsp)
+	SAVE_REST
+	movq RET_VAL_PTR(%rsp),%rsi
+	movq RAX(%rsp),%rdi
+	call vt_sys32_clone_exit
+vt_clone32_epilogue_done:
+	movq RAX(%rsp),%rax
+	movq %rax,FINAL_RET_VAL_IA32(%rsp)
+	addq $ARGOFFSET,%rsp
+	addq $VT_ARGS_SKIP+8*9,%rsp
+	retq
+
+# ----------------------------------------------------------------------------
+# name:         vt_execve32_prologue
+#
+# description:  Entry point for intercepting 32-bit execve system call.
+#
+# Input:        partial stack frame expected by execve system call
+#
+# Output:       execve result value
+# ----------------------------------------------------------------------------
+
+	.global vt_execve32_prologue
+vt_execve32_prologue:
+	UNDO_IA32_ARG_FIXUP
+	SAVE_ARGS VT_ARGS_SKIP
+	SAVE_REST
+	COMPLETE_STACK_FRAME
+	movq $-ENOSYS,RAX(%rsp)
+	call vt_sys32_execve_enter
+vt_execve32_prologue_done:
+	movq %rax,RET_VAL_PTR(%rsp)
+	LOAD_ARGS ARGOFFSET
+	addq $ARGOFFSET,%rsp
+	IA32_ARG_FIXUP
+	movq original_ia32_sys_execve,%rax
+	call *%rax
+vt_execve32_epilogue:
+	movq %rax,RAX-ARGOFFSET(%rsp)
+	SAVE_REST
+	movq RET_VAL_PTR(%rsp),%rsi
+	movq RAX(%rsp),%rdi
+	call vt_sys32_execve_exit
+vt_execve32_epilogue_done:
+	movq RAX(%rsp),%rax
+	movq %rax,FINAL_RET_VAL_IA32(%rsp)
+	addq $ARGOFFSET,%rsp
+	addq $VT_ARGS_SKIP+8*9,%rsp
+	retq
+
diff -urN linux.orig/drivers/char/vtune/vtxsys32.S linux/drivers/char/vtune/vtxsys32.S
--- linux.orig/drivers/char/vtune/vtxsys32.S	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/vtxsys32.S	2004-03-29 10:06:06.000000000 -0800
@@ -0,0 +1,603 @@
+#
+#  vtxsys32.S
+#
+#  Copyright (C) 2002-2004 Intel Corporation
+#  Maintainer - Juan Villacis <juan.villacis@intel.com>
+#
+# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+#
+#  This program is free software; you can redistribute it and/or modify
+#  it under the terms of the GNU General Public License as published by
+#  the Free Software Foundation; either version 2 of the License, or
+#  (at your option) any later version.
+#
+#  This program is distributed in the hope that it will be useful,
+#  but WITHOUT ANY WARRANTY; without even the implied warranty of
+#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+#  GNU General Public License for more details.
+#
+#  You should have received a copy of the GNU General Public License
+#  along with this program; if not, write to the Free Software
+#  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+#
+# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+#
+# ===========================================================================
+#
+#	File: vtxsys32.S
+#
+#	Description: routines for sampling on IA32 platforms
+#
+#	Author(s): George Artz, Intel Corp.
+#
+#	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+#
+# ===========================================================================
+#
+
+#include <linux/linkage.h>
+#include <asm/segment.h>
+#include <asm/page.h>
+#include <asm/pgtable.h>
+#include <asm/desc.h>
+
+#define RDMSR   .byte   0x0f,0x32
+#define WRMSR   .byte   0x0f,0x30
+
+MASK_APIC_BASE          = 0xfffff000
+
+APIC_BASE_MSR           = 0x1b
+
+
+# ------ local APIC equates
+                                        # local apic registers
+APIC_LCL_ID             = 0x20          # ..ID reg
+APIPC_LCL_VER           = 0x30          # ..version
+APIC_LCL_TASKPRI        = 0x80          # ..version
+APIC_LCL_LDEST          = 0xD0          # ..local destination reg
+APIC_LCL_EOI            = 0xB0          # ..eoi register
+APIC_LCL_DESTFMT        = 0xE0          # ..destination format reg
+APIC_LVT_SPURIOUS       = 0xF0          # ..spurious vector reg
+APIC_LVT_TIMER          = 0x320         # ..local timer
+APIC_LVT_PERF           = 0x340         # ..performance monitor lvt
+APIC_LVT_LINT0          = 0x350         # ..local int 0 lvt
+APIC_LVT_LINT1          = 0x360         # ..local int 0 lvt
+
+# ------ various values for apic registers
+
+AV_MASK                 = 0x10000       #
+AV_TOALL                = 0x7fffffff    #
+AV_IM_OFF               = 0x80000000    # logical destination if OFF *
+AV_FIXED                = 0x0           #
+AV_LOPRI                = 0x100         #
+AV_NMI                  = 0x400         #
+AV_RESET                = 0x500         #
+AV_STARTUP              = 0x600         #
+AV_EXTINT               = 0x700         #
+AV_PENDING              = 0x1000        #
+AV_PDEST                = 0x0           #
+AV_LDEST                = 0x800         #
+AV_POLOW                = 0x2000        #
+AV_POHIGH               = 0x0           #
+AV_ASSERT               = 0x4000        #
+AV_DEASSERT             = 0x0           #
+AV_EDGE                 = 0x0           #
+AV_LEVEL                = 0x8000        #
+AV_POLARITY             = 0x2000        #
+AV_XTOSELF              = 0x40000       #
+AV_XTOALL               = 0x80000       #
+
+AV_EXTINT_EDGE          = 0x00000700    #
+AV_NMI_LEVEL            = 0x00008400    #
+
+
+# ------ APIC Task priority values
+TPRIHI                  = 0xf0          #
+TPRILO                  = 0x0           #
+
+P6_APIC_ENABLE          = 0x800         # Enable bit in P6 APIC_BASE_MSR
+
+
+# ----------------------------------------------------------------------------
+# name:         get_CSD
+# description:  get the CS descriptor
+# Input:        code segment selector
+# Output:       code segment descriptor
+# ----------------------------------------------------------------------------
+        .text
+        .align  4
+        .globl  get_CSD
+
+get_CSD:
+        pushl   %ebp
+        movl    %esp, %ebp
+        pushal                                  # save regs
+
+        subl    $8,%esp
+        xorl    %eax, %eax
+        movw    8(%ebp), %ax                    # eax.lo = cs
+        sgdt    (%esp)                          # store gdt reg
+        leal    (%esp), %ebx                    # ebx = gdt reg ptr
+        movl    2(%ebx), %ecx                   # ecx = gdt base
+        xorl    %edx, %edx
+        movw    %ax, %dx
+        andl    $4, %edx
+        cmpl    $0, %edx                        # test ti. GDT?
+        jz      .bsr_10                         # ..yes
+        xorl    %edx, %edx
+        sldt    %dx                             # ..no dx=ldtsel
+        andb    $0xf8, %dl                      # clear ti,rpl
+        addl    2(%ebx), %edx                   # add gdt base
+        movb    7(%edx), %cl                    # ecx = ldt base
+        shll    $8, %ecx                        # ..
+        movb    4(%edx), %cl                    # ..
+        shll    $16, %ecx                       # ..
+        movw    2(%edx), %cx                    # ..
+.bsr_10:
+        andb    $0xf8, %al                      # clear ti & rpl
+        addl    %eax, %ecx                      # add to gdt/ldt
+        movl    (%ecx), %eax                    # copy code seg
+        movl    12(%ebp), %edx                  # ..descriptor (csdlo)
+        movl    %eax, (%edx)                    # ..descriptor (csdlo)
+        movl    4(%ecx), %eax                   # ..from gdt or
+        movl    16(%ebp), %edx                  # ..ldt to sample (csdhi)
+        movl    %eax, (%edx)                    # ..ldt to sample (csdhi)
+        addl    $8,%esp
+        popal                                   # restore regs
+        leave
+        ret
+
+
+
+# ----------------------------------------------------------------------------
+# name:         samp_get_set_idt_entry
+# description:  get/Set the IDT entry for the interrupt vector 
+# Input:        vector number, handler, address of the original entry,
+#               address of original handler
+# Output:       original entry and handler
+# ----------------------------------------------------------------------------
+
+        .text
+        .align  4
+        .globl  samp_get_set_idt_entry
+
+samp_get_set_idt_entry:
+
+        pushl   %ebp
+        movl    %esp, %ebp
+
+        pushal                          # save regs
+        cli                             # disable on this cpu
+
+        # get address of idt entry for caller's interrupt #
+        subl    $8, %esp
+        sidt    2(%esp)                         # get idt base address
+        movl    4(%esp),%esi            # esi = addr of idt
+        movl    8(%ebp),%eax            # eax = idt index
+        leal    (%esi,%eax,8),%esi      # esi = addr of idt entry
+
+        # Copy current idt entry in caller's save area
+
+        movl    16(%ebp),%edi           # edi = addr to save idt entry
+        or      %edi,%edi               # save addr set?
+        jz      set_idt_10              # ..no
+        movl    (%esi),%eax             # ..yes  copy current
+        movl    %eax,(%edi)             # ..idt
+        movl    4(%esi),%eax            # ..entry
+        movl    %eax,4(%edi)            # ..to caller's save area
+
+set_idt_10:
+        #
+        # Set new idt entry. int32 gate with caller's handler
+        #
+
+        movl    12(%ebp),%ebx           # ebx = new int handler addr
+        or      %ebx,%ebx               # handler supplied?
+        jz      set_idt_20              # ..no
+        movw    %cs,%ax                 # set cs in IDT gate
+        movw    %ax,2(%esi)             # ..
+        movw    %bx,(%esi)              # set offset.lo in IDT gate
+        movw    $0x8e00,%bx             # bx = P,DPL=0,TYPE=int_gate32
+        mov     %ebx,4(%esi)            # ebx.hi = hi half of offset
+
+set_idt_20:
+        sti
+        addl    $8,%esp
+        popal                           # restore regs
+        leave
+        ret
+
+# ----------------------------------------------------------------------------
+# name:         samp_restore_idt_entry
+# description:  restore the IDT entry for the interrupt vector 
+# Input:        vector number, original handler
+# Output:       none
+# ----------------------------------------------------------------------------
+        .text
+        .align  4
+        .globl  samp_restore_idt_entry
+
+ samp_restore_idt_entry:
+
+        pushl   %ebp
+        movl    %esp, %ebp
+
+        pushal                          # save regs
+        cli                             # disable on this cpu
+
+        # get address of idt entry for caller's interrupt #
+        subl    $8, %esp
+        sidt    2(%esp)                 # get idt base address
+        movl    4(%esp),%edi            # edi = addr of idt
+        movl    8(%ebp),%eax            # eax = idt index
+        leal    (%edi,%eax,8),%edi      # edi = addr of idt entry
+
+        # restore idt entry
+
+        movl    12(%ebp),%esi           # esi = addr to save idt entry
+        or      %esi,%esi               # save addr set?
+        jz      restore_idt_10          # ..no
+        movl    (%esi),%eax             # ..yes
+        movl    4(%esi),%ebx            # ..is original entry set?
+        mov     %eax,(%edi)             # ..yes restore original
+        mov     %ebx,4(%edi)            # ..idt entry
+
+restore_idt_10:
+        sti
+        addl    $8,%esp
+        popal                           # restore regs
+        leave
+        ret
+
+        .globl  apic_local_addr
+
+        .text
+        .align  4
+        .globl  samp_apic_set_perf_lvt_int_mask
+#***********************************************************************
+#    Set interrupt mask bit in APIC perf LVT
+#
+#
+#    On entry: 
+#
+#***********************************************************************
+samp_apic_set_perf_lvt_int_mask:
+        pushl   %eax
+        pushl   %edx
+
+        movl    apic_local_addr,%edx            # edx = mapped local apic addr
+        orl     %edx,%edx     
+        jz      apic_set_perf_LVT_exit    
+        movl    APIC_LVT_PERF(%edx),%eax        # set int mask in perf lvt
+        orl     $0x10000,%eax                   # ..
+        movl    %eax,APIC_LVT_PERF(%edx)        # ..
+apic_set_perf_LVT_exit:
+        popl    %edx
+        popl    %eax
+        ret
+
+
+        .text
+        .align  4
+        .globl  samp_apic_clear_perf_lvt_int_mask
+#***********************************************************************
+#    Clear interrupt mask bit in APIC perf LVT
+#
+#
+#    On entry: 
+#
+#***********************************************************************
+samp_apic_clear_perf_lvt_int_mask:
+        pushl   %eax
+        pushl   %edx
+
+        movl    apic_local_addr,%edx            # edx = mapped local apic addr
+        orl     %edx,%edx     
+        jz      apic_set_perf_LVT_exit    
+        movl    APIC_LVT_PERF(%edx),%eax        # clear int mask in perf lvt
+        andl    $0xFFFEFFFF,%eax                # ..
+        movl    %eax,APIC_LVT_PERF(%edx)        # ..
+apic_clear_perf_LVT_exit:
+        popl    %edx
+        popl    %eax
+        ret
+
+#***********************************************************************
+#    EOI the local APIC
+#
+#
+#    On entry: 
+#
+#;***********************************************************************
+samp_apic_EOI:
+        pushl   %edx
+
+        movl    apic_local_addr,%edx            # edx = mapped local apic addr
+        orl     %edx,%edx                       #
+        jz      apic_eoi_10                     #
+	xor	%eax, %eax                      # Pentium(R) 4 processor 
+						# likes to see a zero
+        movl    %eax,APIC_LCL_EOI(%edx)         # eoi local apic
+apic_eoi_10:
+
+        popl    %edx                            # restore regs
+        ret
+
+
+
+# ----------------------------------------------------------------------------
+# name:         SAMP_Set_Apic_Virtual_Wire_Mode
+# description:  get performance vector in local APIC
+# Input:        apic_local_addr = mapped virt addr of local apic
+# Output:       local apic placed in virtual wire mode
+#
+# ----------------------------------------------------------------------------
+
+
+       
+        .text
+        .align  4
+        .globl  SAMP_Set_Apic_Virtual_Wire_Mode
+
+SAMP_Set_Apic_Virtual_Wire_Mode:
+
+        pushal                                  #save regs
+        pushfl                                  #save flags (if state)
+        
+# ------ If apic is disabled, switch apic to virtual wire mode
+
+
+        movl    $(APIC_BASE_MSR),%ecx           # APIC base MSR
+        xorl    %eax, %eax                      # ..
+        xorl    %edx, %edx                      # ..
+        RDMSR                                   # eax = apic physical base
+
+        test    $(P6_APIC_ENABLE),%eax          # is apic enabled?
+        jnz     set_vwm_exit                    # ..yes all done
+
+        movl    apic_local_addr,%edx            # edx = mapped local apic addr
+        orl     %edx,%edx     
+        jz      set_vwm_exit    
+
+        
+# ------ mask both 8259's to prevent interrupts from
+# ------ being trapped while switching to virtual wire mode
+
+        xorl    %eax,%eax                       #mask both 8259's
+        xorl    %ebx,%ebx                       #ebx = 0
+        cli                                     #..
+        inb     $0xa1,%al                       #..
+        
+        nop                                     # IO delay..
+        nop                                     # ..
+        
+        movb    %al,%bh                         #..bh = original pic2 mask
+        movl    $0x0ff,%eax                     #..
+        outb    %al,$0xa1                       #..
+        inb     $0x21,%al                       #..
+
+        nop                                     # IO delay..
+        nop                                     # ..
+
+        movb    %al,%bl                         #..bl = original pic1 mask
+        movl    $0xff,%eax                      #..
+        outb    %al,$0x21                       #..
+
+        sti                                     #enable ints to make sure ints
+        nop                                     #..are not trapped in 8259
+        nop                                     #..
+        cli                                     #..
+
+
+
+# ------ write P6 APIC_BASE_MSR to enable local apic memory mapped io
+
+        movl    $(APIC_BASE_MSR),%ecx           # APIC base MSR
+        xorl    %eax, %eax                      #
+        xorl    %edx, %edx                      #
+        RDMSR                                   # RDMSR
+        
+        orl             $(P6_APIC_ENABLE),%eax  # eax = set apic enable bit
+        WRMSR                                   # WRMSR write P6 APIC_BASE_MSR
+
+# ------ Set apic registers for virtual wire mode.
+
+        movl    apic_local_addr,%edx            # edx = mapped local apic addr
+
+        movl    $0x000001ff,APIC_LVT_SPURIOUS(%edx)     # enable virtual wire mode
+        movl    $(TPRIHI),APIC_LCL_TASKPRI(%edx)        # set task priority hi
+        movl    $0,APIC_LCL_ID(%edx)                    # set local unit id = 0
+        movl    $0,APIC_LCL_LDEST(%edx)                 # set logical unit id = 0
+        movl    $-1,APIC_LCL_DESTFMT(%edx)              # set destination format
+        movl    $(AV_MASK),APIC_LVT_TIMER(%edx)         # mask local timer
+        movl    $(AV_EXTINT_EDGE),APIC_LVT_LINT0(%edx)  # 8259->lint0
+        movl    $(AV_NMI_LEVEL),APIC_LVT_LINT1(%edx)    # nmi ->lint1
+        movl    $(TPRILO),APIC_LCL_TASKPRI(%edx)        # set task priority lo
+
+# ------ restore mask state for pic1 & pic2
+
+        movb    %al,%bh                         # restore pic2 mask
+        outb    %al,$0xa1                       # ..
+
+        nop                                     # IO delay..
+        nop                                     # ..
+
+        movb    %bl,%al                         # restore pic1 mask
+        outb    %al,$0x21                       # ..
+
+
+set_vwm_exit:
+        popfl                                   # restore flags
+        popal                                   #
+        ret                                     #
+
+# ----------------------------------------------------------------------------
+# name:         vtune_sys_init
+# description:  low level initialization. determines local APIC phys addr.
+# Input:        none
+# Output:       none
+# ----------------------------------------------------------------------------
+        .text
+        .align  4
+        .globl  vtune_sys_init
+
+vtune_sys_init: 
+
+        pushl   %eax                            # save regs
+        pushl   %ecx
+        pushl   %edx
+
+        movl    $(APIC_BASE_MSR),%ecx           # APIC base MSR
+        xorl    %eax, %eax                      #
+        xorl    %edx, %edx                      #
+        RDMSR                                   # RDMSR
+        
+        andl    $(MASK_APIC_BASE),%eax          # eax = local apic physical base
+        movl    %eax, apic_paddr                # Save apic phys addr as global
+
+        popl    %edx                            # restore regs
+        popl    %ecx
+        popl    %eax
+        ret
+
+
+# ----------------------------------------------------------------------------
+# name: SAMP_Set_apic_perf_lvt
+# description:          Set performance vector in local APIC
+# Input:                        new apic perf lvt value
+# Output:                       0 if local apic not present
+#                                       previous value of local apic perf lvt
+# ----------------------------------------------------------------------------
+        .text
+        .align  4
+        .globl  SAMP_Set_apic_perf_lvt
+
+SAMP_Set_apic_perf_lvt:
+
+        pushl   %ebx
+        xorl    %eax,%eax                       # eax = 0 (if no local apic)
+        cmpl    $0,apic_local_addr              # is local apic mapped?
+                                                # pcmp platform-specific
+        je      set_perf_exit                   #...no
+        movl    apic_local_addr,%ebx            #..yes ebx = local apic addr
+        movl    8(%esp),%eax                    # ..eax = new perf lvt value
+        xchgl    %eax,APIC_LVT_PERF(%ebx)       #...set new perf lvt.
+                                                #..eax = original perf lvt
+set_perf_exit:
+        popl    %ebx                            # restore regs
+        ret
+
+
+# ----------------------------------------------------------------------------
+# name:         SAMP_get_apic_perf_lvt
+# description:  get performance vector in local APIC
+# Input:        none
+# Output:       0 if local apic not present
+#                       value of local apic perf lvt
+# ----------------------------------------------------------------------------
+        .text
+        .align  4
+        .globl  SAMP_get_apic_perf_lvt
+
+SAMP_get_apic_perf_lvt:
+
+        xorl    %eax,%eax                       # eax = 0 (if no local apic)
+        cmpl    $0,apic_local_addr              # is local apic mapped?
+        je              get_perf_exit           #..no
+        leal    apic_local_addr,%eax            #..yes eax = local apic addr
+        movl    APIC_LVT_PERF(%eax),%eax        #..eax = local apic perf lvt
+
+get_perf_exit:
+        ret
+
+
+# ----------------------------------------------------------------------------
+# name:         t_ebs
+#
+# description:  ISR entry for local APIC PERF interrupt vector 
+#
+# Input:        n/a
+#
+# Output:       n/a original entry and handler
+# ----------------------------------------------------------------------------
+
+        .text
+        .align  8
+        .globl  t_ebs
+
+t_ebs: 
+                                        # This is the same as KERNEL's
+        pushl   %eax                    # Filler for Error Code
+
+        cld
+        pushl   %es                     # SAVE_ALL macro to access pt_regs
+        pushl   %ds                     # inside our ISR.
+        pushl   %eax
+        pushl   %ebp
+        pushl   %edi
+        pushl   %esi
+        pushl   %edx
+        pushl   %ecx
+        pushl   %ebx
+
+        xor     %edx, %edx              # Clear DX storage (css why? overwritten below?)
+        
+        movl    $(__KERNEL_DS), %edx    # Use KERNEL DS selector
+        movl    %edx,%ds                # Make sure we set Kernel
+        movl    %edx,%es                # DS into local DS and ES
+
+	movl	%esp, %ebx		# get ready to put *pt_regs on stack
+        
+#if defined(ALLOW_LBRS)
+
+	movl	quick_freeze_msr, %ecx	# see if we need to freeze the LBRs
+	cmpl	$0, %ecx		# zero means no freeze
+	je	samp_no_lbrs
+
+	RDMSR				# get the old freeze state
+	pushl	%eax
+	pushl	%edx
+
+	xor 	%eax, %eax
+	xor 	%edx, %edx
+	WRMSR				# write zeros to cause an LBR freeze
+
+samp_no_lbrs:
+	pushl	%ecx			# always save whether we did extra work
+
+#endif
+
+        pushl   %ebx			# put *pt_regs on the stack
+        call ebs_intr
+        addl $0x4, %esp			# pop to nowhere...
+
+#if defined(ALLOW_LBRS)
+	popl	%ecx			# did we do extra work on the way in?
+	cmpl	$0, %ecx
+	je	samp_skip_msr_restore	# nope, so we done with lbr stuff
+
+	popl	%edx
+	popl	%eax
+	WRMSR				# restore the freeze register
+
+samp_skip_msr_restore:
+
+#endif
+
+        call    samp_apic_EOI           # send EOI to APIC
+
+        pop     %ebx                    # restore register set
+        pop     %ecx
+        pop     %edx
+        pop     %esi
+        pop     %edi
+        pop     %ebp
+        pop     %eax
+        pop     %ds
+        pop     %es
+        pop     %eax
+        
+        iret
+
+
+
diff -urN linux.orig/drivers/char/vtune/vtxsys64.S linux/drivers/char/vtune/vtxsys64.S
--- linux.orig/drivers/char/vtune/vtxsys64.S	1969-12-31 16:00:00.000000000 -0800
+++ linux/drivers/char/vtune/vtxsys64.S	2004-03-29 10:06:06.000000000 -0800
@@ -0,0 +1,502 @@
+#
+#  vtxsys64.S
+#
+#  Copyright (C) 2002-2004 Intel Corporation
+#  Maintainer - Juan Villacis <juan.villacis@intel.com>
+#
+# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+#
+#  This program is free software; you can redistribute it and/or modify
+#  it under the terms of the GNU General Public License as published by
+#  the Free Software Foundation; either version 2 of the License, or
+#  (at your option) any later version.
+#
+#  This program is distributed in the hope that it will be useful,
+#  but WITHOUT ANY WARRANTY; without even the implied warranty of
+#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+#  GNU General Public License for more details.
+#
+#  You should have received a copy of the GNU General Public License
+#  along with this program; if not, write to the Free Software
+#  Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA 02111-1307 USA
+#
+# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+#
+# ===========================================================================
+#
+#	File: vtxsys64.S
+#
+#	Description: routines for sampling on Itanium(R)-based systems
+#
+#	Author(s): George Artz, Intel Corp.
+#                  Tony Luck, Intel Corp.
+#
+#	System: VTune(TM) Performance Analyzer Driver Kit for Linux*
+#
+# ===========================================================================
+#
+
+#include <asm/asmmacro.h>
+
+        .file "vtxsys64.s"
+        
+
+        .text
+        .align 16
+        .global vt_sys_mmap_stub
+vt_sys_mmap_stub:
+        alloc   r2=ar.pfs,8,4,8,0
+        ;;
+        movl    r8=original_sys_mmap
+        mov     loc0=r2
+        mov     loc1=rp
+        ;;
+        ld8     r8=[r8]
+        mov     loc3=gp
+
+        mov     out0=in0
+        mov     out1=in1
+        mov     out2=in2
+        mov     out3=in3
+        mov     out4=in4
+        mov     out5=in5
+        mov     out6=in6
+        mov     out7=in7
+        ;;
+        mov     b6=r8
+
+        br.call.sptk.many rp=b6
+
+        mov     loc2=r8
+
+        movl    r9=@fptr(vt_sys_mmap)
+
+        mov     out0=r8
+        mov     out1=in1        // length
+        mov     out2=in2        // prot
+        mov     out3=in4        // fd
+        ;;
+        add     r8=8,r9
+        ;;
+        ld8     gp=[r8]
+        br.call.sptk.many rp=vt_sys_mmap
+
+        mov     rp=loc1
+        mov     r8=loc2
+        mov     gp=loc3
+        mov     ar.pfs=loc0
+        ;;
+        br.ret.sptk.many rp
+
+
+
+        .text
+        .align 16
+        .global vt_sys_mmap2_stub
+vt_sys_mmap2_stub:
+        alloc   r2=ar.pfs,8,4,8,0
+        ;;
+        movl    r8=original_sys_mmap2
+        mov     loc0=r2
+        mov     loc1=rp
+        ;;
+        ld8     r8=[r8]
+        mov     loc3=gp
+
+        mov     out0=in0
+        mov     out1=in1
+        mov     out2=in2
+        mov     out3=in3
+        mov     out4=in4
+        mov     out5=in5
+        mov     out6=in6
+        mov     out7=in7
+        ;;
+        mov     b6=r8
+
+        br.call.sptk.many rp=b6
+
+        mov     loc2=r8
+
+        movl    r9=@fptr(vt_sys_mmap2)
+
+        mov     out0=r8
+        mov     out1=in1        // length
+        mov     out2=in2        // prot
+        mov     out3=in4        // fd
+        ;;
+        add     r8=8,r9
+        ;;
+        ld8     gp=[r8]
+        br.call.sptk.many rp=vt_sys_mmap2
+
+        mov     rp=loc1
+        mov     r8=loc2
+        mov     gp=loc3
+        mov     ar.pfs=loc0
+        ;;
+        br.ret.sptk.many rp
+
+
+        .text
+        .align 16
+        .global vt_sys_clone2_stub
+vt_sys_clone2_stub:
+        alloc   r2=ar.pfs,95,0,1,0
+        ;;
+        mov     r40 = r0            // force rse if it hasn't already happened
+        mov     r41 = r0
+        mov     r42 = r0
+        mov     r43 = r0
+        mov     r44 = r0
+        mov     r45 = r0
+        mov     r46 = r0
+        mov     r47 = r0
+        mov     r48 = r0
+        mov     r49 = r0
+        mov     r50 = r0
+        mov     r51 = r0
+        mov     r52 = r0
+        mov     r53 = r0
+        mov     r54 = r0
+        mov     r55 = r0
+        mov     r56 = r0
+        mov     r57 = r0
+        mov     r58 = r0
+        mov     r59 = r0
+        mov     r60 = r0
+        mov     r61 = r0
+        mov     r62 = r0
+        mov     r63 = r0
+        mov     r64 = r0
+        mov     r65 = r0
+        mov     r66 = r0
+        mov     r67 = r0
+        mov     r68 = r0
+        mov     r69 = r0
+        mov     r70 = r0
+        mov     r71 = r0
+        mov     r72 = r0
+        mov     r73 = r0
+        mov     r74 = r0
+        mov     r75 = r0
+        mov     r76 = r0
+        mov     r77 = r0
+        mov     r78 = r0
+        mov     r79 = r0
+        mov     r80 = r0
+        mov     r81 = r0
+        mov     r82 = r0
+        mov     r83 = r0
+        mov     r84 = r0
+        mov     r85 = r0
+        mov     r86 = r0
+        mov     r87 = r0
+        mov     r88 = r0
+        mov     r89 = r0
+        mov     r90 = r0
+        mov     r91 = r0
+        mov     r92 = r0
+        mov     r93 = r0
+        mov     r94 = r0
+        mov     r95 = r0
+        mov     r96 = r0
+        mov     r97 = r0
+        mov     r98 = r0
+        mov     r99 = r0
+        mov     r100 = r0
+        mov     r101 = r0
+        mov     r102 = r0
+        mov     r103 = r0
+        mov     r104 = r0
+        mov     r105 = r0
+        mov     r106 = r0
+        mov     r107 = r0
+        mov     r108 = r0
+        mov     r109 = r0
+        mov     r110 = r0
+        mov     r111 = r0
+        mov     r112 = r0
+        mov     r113 = r0
+        mov     r114 = r0
+        mov     r115 = r0
+        mov     r116 = r0
+        mov     r117 = r0
+        mov     r118 = r0
+        mov     r119 = r0
+        mov     r120 = r0
+        mov     r121 = r0
+        mov     r122 = r0
+        mov     r123 = r0
+        mov     r124 = r0
+        mov     r125 = r0
+        mov     r126 = r0
+        mov     r127 = r0
+        ;;
+        mov     r40 = gp                // loc 0
+        mov     r41 = rp                // loc 1
+        mov     r42 = r2                // loc 2
+
+        mov     r127 = rp               // arg 0
+
+        movl    r60 = @fptr(vt_sys_clone2_before)       // need gp to make call
+        ;;
+        add     r61 = 8, r60                            // gp is second entry
+        ;;
+        ld8     gp = [r61]                              // load gp
+        br.call.sptk.many   rp=vt_sys_clone2_before     // make call
+        ;;
+        // r8 indicates if the the rp was added to the clone table.
+        // if it was (r8 non-zero) then call back to our post code
+        // if it wasn't (r8 zero) then use the original rp
+        // so that we don't do a lookup of something that won't be
+        // there. this lets the system continue running even if the
+        // clone table is full for some reason.
+        //
+        cmp4.eq.unc p6,p7=r0,r8         // p6 true means r8 == 0
+        mov     gp = r40                // restore gp
+        movl    r62=original_sys_clone2 // setup to make original call
+        ;;
+        mov     r127 = r0
+(p7)    movl    r70  = vt_sys_clone2_stub_ret   // added, want to come back
+        ;;
+(p7)    mov     rp = r70                // we added so come back to us
+(p6)    mov     rp = r41                // we didn't add, so don't bother
+        ld8     r62=[r62]
+        ;;
+        mov     b6=r62
+        mov ar.pfs = r42
+        ;;
+        br.sptk.many b6
+
+//
+// Taken fron entry.S - a copy of invoke_syscall_trace
+//
+// According to notes there, we can't allow ar.pfs to change in the
+// return path of a syscall, so we do a short call to this routine
+// which takes care of the call to the bigger routine and as
+// long as we pass values in a non-standard way (aka scratch registers
+// then life can be good... At least in theory...
+//
+
+GLOBAL_ENTRY(vt_post_helper)
+    .prologue ASM_UNW_PRLG_RP|ASM_UNW_PRLG_PFS, ASM_UNW_PRLG_GRSAVE(8)
+    alloc loc1=ar.pfs,8,5,1,0
+    mov loc0=rp
+    .body
+    mov loc2=b6
+    ;;
+    //
+    // Now that the precursor is out of the way... We can do what we want...
+    //
+    mov loc3 = r8           // save the return value from the syscall
+    mov loc4 = gp           // save the gp
+
+    mov out0 = r8           // and prepare to pass it along
+    ;;
+
+    movl r8 = @fptr(vt_sys_clone2_after)
+    ;;
+
+    adds    r8 = 8, r8      // don't care about func addr, just func gp
+    ;;
+
+    ld8     gp = [r8]
+    br.call.sptk.many rp=vt_sys_clone2_after
+    ;;
+
+    mov r14 = r8            // private spot of old return address
+    ;;
+
+    mov r8 = loc3           // so the rest of the syscall people can see it
+    mov gp = loc4
+    
+    //
+    // Carefule here, this is for unwinding too...
+    //
+.ret3:  mov rp=loc0
+    mov ar.pfs=loc1
+    mov b6=loc2
+    br.ret.sptk.many rp
+END(vt_post_helper)
+
+
+//
+// At this point we are on the original break alloc/stack. Need to not
+// interfere with any locals they were using.
+//
+vt_sys_clone2_stub_ret:
+        nop.m 0
+        nop.i 0
+        //
+        // looks like the people in entry.S knew what they were doing...
+        //
+        br.call.sptk.many rp=vt_post_helper
+        ;;
+
+        //
+        // Put in by the vt_post_helper routine...
+        //
+        mov     rp = r14
+        ;;
+        br.sptk.many rp
+
+
+        .text
+        .align 16
+        .global vt_sys_clone_stub
+vt_sys_clone_stub:
+        alloc   r2=ar.pfs,2,5,2,0
+        ;;
+        mov     loc3=gp
+        mov     loc0=r2
+        mov     loc1=rp
+
+        movl    r9=@fptr(vt_sys_clone_before)
+        ;;
+        add     r8=8,r9
+        ;;
+        ld8     gp=[r8]
+        br.call.sptk.many rp=vt_sys_clone_before
+        mov     loc4=r8
+        ;;
+        movl    r8=original_sys_clone
+        ;;
+        ld8     r8=[r8]
+
+        mov     out0=in0
+        mov     out1=in1
+        ;;
+        mov     b6=r8
+
+        mov     gp=loc3
+        br.call.sptk.many rp=b6
+
+        mov     loc2=r8
+
+        movl    r9=@fptr(vt_sys_clone_after)
+
+        mov     out0=r8
+        ;;
+        add     r8=8,r9
+        ;;
+        ld8     gp=[r8]
+        mov     out1=loc4
+        br.call.sptk.many rp=vt_sys_clone_after
+
+        mov     rp=loc1
+        mov     r8=loc2
+        mov     gp=loc3
+        mov     ar.pfs=loc0
+        ;;
+        br.ret.sptk.many rp
+
+        .text
+        .align 16
+        .global vt_sys_create_module_stub
+vt_sys_create_module_stub:
+        alloc   r2=ar.pfs,8,4,8,0
+        ;;
+        movl    r8=original_sys_create_module
+        mov     loc0=r2
+        mov     loc1=rp
+        ;;
+        ld8     r8=[r8]
+        mov     loc3=gp
+
+        mov     out0=in0
+        mov     out1=in1
+        mov     out2=in2
+        mov     out3=in3
+        mov     out4=in4
+        mov     out5=in5
+        mov     out6=in6
+        mov     out7=in7
+        ;;
+        mov     b6=r8
+
+        br.call.sptk.many rp=b6
+
+        mov     loc2=r8
+
+        movl    r9=@fptr(vt_sys_create_module)
+
+        mov     out0=r8
+        ;;
+        add     r8=8,r9
+        ;;
+        ld8     gp=[r8]
+        br.call.sptk.many rp=vt_sys_create_module
+
+        mov     rp=loc1
+        mov     r8=loc2
+        mov     gp=loc3
+        mov     ar.pfs=loc0
+        ;;
+        br.ret.sptk.many rp
+
+        .text
+        .align 16
+        .global vt_sys_execve_stub
+vt_sys_execve_stub:
+        alloc   r2=ar.pfs,3,2,3,0
+        ;;
+        movl    r8=original_sys_execve
+        movl    r9=vt_save_execve_ret
+        mov     loc0=r2
+        mov     loc1=rp
+        ;;
+        ld8     r8=[r8]
+        st8     [r9]=loc1
+
+        mov     out0=in0
+        mov     out1=in1
+        mov     out2=in2
+        ;;
+        mov     b6=r8
+
+        /* call original exec function in kernel */
+        br.call.sptk.many rp=b6
+
+        cmp4.ge p6,p7=r8,r0
+        ;;
+
+        /*
+         * if exec failed, we still have a frame, so we can restore
+         * registers and return.
+         */
+(p7)    mov     rp=loc1
+(p7)    mov     ar.pfs=loc0
+        ;;
+(p7)    br.ret.sptk.many rp
+
+        /*
+         * exec succeeded, we are on a new register window frame
+         * set gp for module and call our stub
+         */
+        alloc   r4=ar.pfs,0,3,0,0
+        ;;
+        mov     loc1=r8
+        mov     loc2=gp
+
+        mov     loc0=r4
+        movl    r9=@fptr(vt_sys_execve)
+        ;;
+        add     r8=8,r9
+        ;;
+        ld8     gp=[r8]
+        br.call.sptk.many rp=vt_sys_execve
+
+        movl    r9=vt_save_execve_ret
+        ;;
+        ld8     r4=[r9]
+        mov     gp=loc2
+        mov     r8=loc1
+        mov     ar.pfs=loc0
+        ;;
+        mov     rp=r4
+        ;;
+        mov     r4=r0
+        
+
+        br.ret.sptk.many rp
