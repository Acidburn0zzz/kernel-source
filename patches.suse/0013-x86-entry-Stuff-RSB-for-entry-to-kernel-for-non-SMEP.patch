From: Tim Chen <tim.c.chen@linux.intel.com>
Date: Tue, 14 Nov 2017 17:16:30 -0800
Subject: x86/entry: Stuff RSB for entry to kernel for non-SMEP platform
Patch-mainline: submitted on 2018/1/9
References: bnc#1068032 CVE-2017-5715

Stuff RSB to prevent RSB underflow on non-SMEP platform.

Signed-off-by: Jiri Slaby <jslaby@suse.cz>
---
 arch/x86/entry/entry_64.S        |   13 +++++++
 arch/x86/entry/entry_64_compat.S |    5 ++
 arch/x86/include/asm/spec_ctrl.h |   71 +++++++++++++++++++++++++++++++++++++++
 3 files changed, 89 insertions(+)

--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@ -238,6 +238,7 @@ GLOBAL(entry_SYSCALL_64_after_hwframe)
 
 	TRACE_IRQS_OFF
 
+	STUFF_RSB
 	/*
 	 * If we need to do entry work or if we guess we'll need to do
 	 * exit work, go straight to the slow path.
@@ -656,6 +657,13 @@ END(irq_entries_start)
 	ALLOC_PT_GPREGS_ON_STACK
 	SAVE_C_REGS
 	SAVE_EXTRA_REGS
+
+	/*
+	 * Have to do stuffing before encoding frame pointer.
+	 * Could add some unnecessary RSB clearing if coming
+	 * from kernel for non-SMEP platform.
+	 */
+	STUFF_RSB
 	ENCODE_FRAME_POINTER
 
 	testb	$3, CS(%rsp)
@@ -1267,6 +1275,10 @@ ENTRY(paranoid_entry)
 	cld
 	SAVE_C_REGS 8
 	SAVE_EXTRA_REGS 8
+	/*
+	 * Do the stuffing unconditionally from user/kernel to be safe
+	 */
+	STUFF_RSB
 	ENCODE_FRAME_POINTER 8
 	movl	$1, %ebx
 	movl	$MSR_GS_BASE, %ecx
@@ -1321,6 +1333,7 @@ ENTRY(error_entry)
 	cld
 	SAVE_C_REGS 8
 	SAVE_EXTRA_REGS 8
+	STUFF_RSB
 	ENCODE_FRAME_POINTER 8
 	xorl	%ebx, %ebx
 	testb	$3, CS+8(%rsp)
--- a/arch/x86/entry/entry_64_compat.S
+++ b/arch/x86/entry/entry_64_compat.S
@@ -98,6 +98,8 @@ ENTRY(entry_SYSENTER_compat)
 	pushq   $0			/* pt_regs->r15 = 0 */
 	cld
 
+	STUFF_RSB
+
 	/*
 	 * SYSENTER doesn't filter flags, so we need to clear NT and AC
 	 * ourselves.  To save a few cycles, we can check whether
@@ -198,6 +200,7 @@ ENTRY(entry_SYSCALL_compat)
 	/* Use %rsp as scratch reg. User ESP is stashed in r8 */
 	SWITCH_TO_KERNEL_CR3 scratch_reg=%rsp
 	ENABLE_IBRS_CLOBBER
+	STUFF_RSB
 
 	/* Switch to the kernel stack */
 	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
@@ -353,6 +356,8 @@ ENTRY(entry_INT80_compat)
 	pushq   %r15                    /* pt_regs->r15 */
 	cld
 
+	STUFF_RSB
+
 	/*
 	 * User mode is traced as though IRQs are on, and the interrupt
 	 * gate turned them off.
--- a/arch/x86/include/asm/spec_ctrl.h
+++ b/arch/x86/include/asm/spec_ctrl.h
@@ -40,6 +40,73 @@
 	movl $0, %edx;				\
 	movl $0, %eax;				\
 	wrmsr;
+#define __ASM_STUFF_RSB				\
+	call	1f;				\
+	pause;					\
+1:	call	2f;				\
+	pause;					\
+2:	call	3f;				\
+	pause;					\
+3:	call	4f;				\
+	pause;					\
+4:	call	5f;				\
+	pause;					\
+5:	call	6f;				\
+	pause;					\
+6:	call	7f;				\
+	pause;					\
+7:	call	8f;				\
+	pause;					\
+8:	call	9f;				\
+	pause;					\
+9:	call	10f;				\
+	pause;					\
+10:	call	11f;				\
+	pause;					\
+11:	call	12f;				\
+	pause;					\
+12:	call	13f;				\
+	pause;					\
+13:	call	14f;				\
+	pause;					\
+14:	call	15f;				\
+	pause;					\
+15:	call	16f;				\
+	pause;					\
+16:	call	17f;				\
+	pause;					\
+17:	call	18f;				\
+	pause;					\
+18:	call	19f;				\
+	pause;					\
+19:	call	20f;				\
+	pause;					\
+20:	call	21f;				\
+	pause;					\
+21:	call	22f;				\
+	pause;					\
+22:	call	23f;				\
+	pause;					\
+23:	call	24f;				\
+	pause;					\
+24:	call	25f;				\
+	pause;					\
+25:	call	26f;				\
+	pause;					\
+26:	call	27f;				\
+	pause;					\
+27:	call	28f;				\
+	pause;					\
+28:	call	29f;				\
+	pause;					\
+29:	call	30f;				\
+	pause;					\
+30:	call	31f;				\
+	pause;					\
+31:	call	32f;				\
+	pause;					\
+32:						\
+	add $(32*8), %rsp;
 
 .macro ENABLE_IBRS
 ALTERNATIVE "", __stringify(__ASM_ENABLE_IBRS), X86_FEATURE_SPEC_CTRL
@@ -91,5 +158,9 @@ ALTERNATIVE "", __stringify(__ASM_DISABL
 ALTERNATIVE "", __stringify(__ASM_DISABLE_IBRS_CLOBBER), X86_FEATURE_SPEC_CTRL
 .endm
 
+.macro STUFF_RSB
+ALTERNATIVE __stringify(__ASM_STUFF_RSB), "", X86_FEATURE_SMEP
+.endm
+
 #endif /* __ASSEMBLY__ */
 #endif /* _ASM_X86_SPEC_CTRL_H */
