# This is a BitKeeper generated diff -Nru style patch.
#
# ChangeSet
#   2004/03/31 15:49:53-05:00 nagar@elinux03.watson.ibm.com 
#   ckrm-E5.core 
# 
# kernel/ckrm/ckrmutils.c
#   2004/03/31 15:49:52-05:00 nagar@elinux03.watson.ibm.com +65 -0
#   ckrm-E5.core 
# 
# kernel/ckrm/ckrm.c
#   2004/03/31 15:49:52-05:00 nagar@elinux03.watson.ibm.com +905 -307
#   ckrm-E5.core 
# 
# kernel/ckrm/Makefile
#   2004/03/31 15:49:52-05:00 nagar@elinux03.watson.ibm.com +2 -1
#   ckrm-E5.core 
# 
# include/linux/ckrm_rc.h
#   2004/03/31 15:49:51-05:00 nagar@elinux03.watson.ibm.com +36 -22
#   ckrm-E5.core 
# 
# include/linux/ckrm_ce.h
#   2004/03/31 15:49:51-05:00 nagar@elinux03.watson.ibm.com +41 -18
#   ckrm-E5.core 
# 
diff -Nru a/include/linux/ckrm_ce.h b/include/linux/ckrm_ce.h
--- a/include/linux/ckrm_ce.h	Wed Mar 31 16:52:17 2004
+++ b/include/linux/ckrm_ce.h	Wed Mar 31 16:52:17 2004
@@ -25,37 +25,57 @@
 #ifndef _LINUX_CKRM_CE_H
 #define _LINUX_CKRM_CE_H
 
-#ifdef __KERNEL__
-
 #ifdef CONFIG_CKRM
 
-//#include <linux/ckrm.h>
-
 // Max engine name length
 #define CKRM_MAX_ENG_NAME 128
 
+/* Action parameters identifying the cause of a task<->class notify callback 
+ * these can perculate up to user daemon consuming records send by the classification
+ * engine
+ */
+
+enum {
+	CKRM_ACTION_RECLASSIFY,
+	CKRM_ACTION_MANUAL,
+	CKRM_ACTION_FORK,
+	CKRM_ACTION_EXEC,
+	CKRM_ACTION_GID,
+	CKRM_ACTION_UID,
+	CKRM_ACTION_LISTEN,
+
+	CKRM_ACTION_LAST  /* always the last entry */
+};
+
+#ifdef __KERNEL__
+
 typedef struct ckrm_eng_callback {
+	/* general state information */
 	char ckrm_eng_name[CKRM_MAX_ENG_NAME];
-	void * (*fork)(struct task_struct*); // on fork
-	void * (*exec)(struct task_struct*,const char *filename); // on exec
+	int  always_callback;  /* set if CE should always be called back regardless of numclasses */
+
+	/* callbacks which are called without holding locks */
+
+	void * (*fork)      (struct task_struct*); // on fork
+	void * (*exec)      (struct task_struct*,const char *filename); // on exec
 
 	void * (*reclassify)(struct task_struct *); // on need
-	void * (*uid)(struct task_struct*);  // on uid change
-	void * (*gid)(struct task_struct*);  // on gid change
+	void * (*uid)       (struct task_struct*);  // on uid change
+	void * (*gid)       (struct task_struct*);  // on gid change
+	void * (*listen)    (void *n); // listen callback
 
-	void (*manual)(struct task_struct *);  /* manual reclassification */
-	void (*exit)(struct task_struct *);  /* on exit - just notification */
+	void   (*manual)    (struct task_struct *);  /* mark manual */
 
-	void (*class_add)(const char *name, void *core);   /* class added */
-	void (*class_delete)(const char *name, void *core);      /* class deleted */
+	void   (*class_add) (const char *name, void *core);   /* class added */
+	void   (*class_delete)(const char *name, void *core); /* class deleted */
+
+
+	/* callba which are called while holding task_lock(tsk) */
+	void (*notify)(struct task_struct *tsk, void *core, int action); /* notify on class switch */
+	void (*exit)  (struct task_struct *tsk);                         /* on exit */
 
-	void * (*listen_cb)(void *n); // listen callback
-	int always_callback;
 	/* and more to come */
 
-	/* Hubertus.. this should be removed in the final version when API 
-	   transition was done */
-	int  (*engine_ctl)(unsigned int op, void *data); /* user level ctl api */
 } ckrm_eng_callback_t;
 
 typedef struct rbce_eng_callback {
@@ -66,12 +86,15 @@
 extern int ckrm_register_engine(ckrm_eng_callback_t *);
 extern int ckrm_unregister_engine(ckrm_eng_callback_t *);
 extern void *ckrm_classobj(char *);
-extern void ckrm_reclassify(int);
 extern int get_exe_path_name(struct task_struct *t, char *filename, int max_size);
 
 extern int rcfs_register_engine(rbce_eng_callback_t *);
 extern int rcfs_unregister_engine(rbce_eng_callback_t *);
 
+extern int ckrm_reclassify(int pid);
+
+extern void ckrm_core_grab(void *);
+extern void ckrm_core_drop(void *);
 
 #endif // CONFIG_CKRM
 
diff -Nru a/include/linux/ckrm_rc.h b/include/linux/ckrm_rc.h
--- a/include/linux/ckrm_rc.h	Wed Mar 31 16:52:17 2004
+++ b/include/linux/ckrm_rc.h	Wed Mar 31 16:52:17 2004
@@ -40,12 +40,6 @@
 	CKRM_NET_CLASS,
 };
 
-// Modes of CKRM   Hubertus... Mode can disappear ? no implicitely through DONTCARE'S
-#define CKRM_MONITOR_MODE		1
-#define CKRM_MANAGE_MODE		2
-
-extern int ckrm_mode;   /* are we in monitor or in managed mode */
-
 // predefined constants
 #define CKRM_MAX_RES_CTLRS 32
 #define CKRM_MAX_RES_NAME		128
@@ -78,13 +72,28 @@
 #define CKRM_SHARE_DFLT_TOTAL_GUARANTEE (100) // Start off with these values
 #define CKRM_SHARE_DFLT_TOTAL_LIMIT     (100) // to simplify set_res_shares logic
 
+/* CKRM net struct used to keep track of the network members.
+ */
+struct ckrm_net_struct {
+	int family;			// IPPROTO_IPV4 || IPPROTO_IPV6
+					// Currently only IPV4 is supported
+	__u32 daddr4;			// V4 listener's address
+	__u16 dport;			// listener's port
+	struct ckrm_core_class	*core;
+	struct list_head ckrm_link;
+};
+
 /* basic definition of a hierarchy that is to be used by the the CORE classes
  * and can be used by the resource class objects
  */
 
 struct ckrm_hnode {
+#ifndef NEW_HNODE_IMPLMN
 	struct ckrm_hnode *parent;
-	struct list_head   siblings; /* anchor for sibling list */
+#else
+	struct ckrm_core_class *parent;
+#endif
+	struct list_head   siblings; /* linked list of siblings */
 	struct list_head   children; /* anchor for children     */
 };
 
@@ -99,6 +108,9 @@
 	int magic;
 	void *res_class[CKRM_MAX_RES_CTLRS]; // per registered resource
 	struct ckrm_hnode  hnode;    // hierarchy
+	rwlock_t hnode_rwlock; // rw_clock protecting the hnode above.
+	atomic_t refcnt;
+	char name[100];// debug only
 } ckrm_core_class_t;
 
 #define ckrm_get_res_class(rescls,resid,type)   ((type*)((rescls)->res_class[resid]))
@@ -112,8 +124,7 @@
 	int  resid;		// (for now) same as the enum resid
 
 	/* allocate/free new resource class object for resource controller */
-	void * (*res_alloc)  (struct ckrm_core_class *core, 
-					struct ckrm_core_class *parent);
+	void * (*res_alloc)  (struct ckrm_core_class *, struct ckrm_core_class *);
 	void (*res_free)     (void *);
 	/* reinitialize existing resource class object */
 	void (*res_initcls)  (void *);
@@ -123,17 +134,17 @@
 	int  (*get_share_values)    (void *, struct ckrm_shares *);
 
 	/* statistics access */
-	int  (*get_stats)    (void *, struct seq_file *s);
+	int  (*get_stats)    (void *, struct seq_file *);
 
-	void (*change_resclass)(struct task_struct *, void *, void *);
+	void (*change_resclass)(void *, void *, void *);
 } ckrm_res_callback_t;
 
 extern int ckrm_register_res_ctlr(ckrm_res_callback_t *);
 extern int ckrm_unregister_res_ctlr(int);
 
-extern inline unsigned int is_core_valid(ckrm_core_class_t *core);
-extern inline unsigned int is_res_regd(int resid);
-extern inline int ckrm_resid_lookup (char *resname);
+extern inline unsigned int is_core_valid(ckrm_core_class_t *);
+extern inline unsigned int is_res_regd(int);
+extern inline int ckrm_resid_lookup (char *);
 
 #define for_each_resid(rid) \
 	for (rid=0; rid < CKRM_MAX_RES_CTLRS; rid++) 
@@ -144,17 +155,20 @@
 extern struct ckrm_core_class ckrm_dflt_class;
 extern struct ckrm_core_class ckrm_net_root;
 
-extern struct ckrm_core_class *ckrm_alloc_core_class(struct ckrm_core_class *parent, struct dentry *dentry);
-extern int ckrm_free_core_class(struct ckrm_core_class *cls);
+extern struct ckrm_core_class *ckrm_alloc_core_class(struct ckrm_core_class *,
+				struct dentry *);
+extern int ckrm_free_core_class(struct ckrm_core_class *);
+
+// Reclassify the given pid to the given core class by force
+extern void ckrm_forced_reclassify_pid(int, struct ckrm_core_class *);
 
-// Reclassify the given task to the given core class.
-extern void ckrm_reclassify_task(struct task_struct *, struct ckrm_core_class *);
+// Reclassify the given net_struct  to the given core class by force
+extern void ckrm_forced_reclassify_net(struct ckrm_net_struct *, 
+		struct ckrm_core_class *);
 
-// Reclassify the given task to the given core class by force
-extern void ckrm_forced_reclassify_task(struct task_struct *, struct ckrm_core_class *);
 
-extern void ckrm_hnode_add(struct ckrm_hnode *node,struct ckrm_hnode *parent );
-extern int  ckrm_hnode_remove(struct ckrm_hnode *node);
+extern void ckrm_hnode_add(struct ckrm_hnode *,struct ckrm_hnode *);
+extern int  ckrm_hnode_remove(struct ckrm_hnode *);
 
 
 #endif // CONFIG_CKRM
diff -Nru a/kernel/ckrm/Makefile b/kernel/ckrm/Makefile
--- a/kernel/ckrm/Makefile	Wed Mar 31 16:52:17 2004
+++ b/kernel/ckrm/Makefile	Wed Mar 31 16:52:17 2004
@@ -8,5 +8,6 @@
 	obj-y = ckrmstub.o
 endif
 
-obj-$(CONFIG_CKRM_RES_DUMMY) += ckrm_dummy.o ckrm_socketaq.o
+obj-$(CONFIG_CKRM_RES_DUMMY) += ckrm_dummy.o 
+obj-$(CONFIG_CKRM_RES_SOCKETAQ) += ckrm_socketaq.o
 
diff -Nru a/kernel/ckrm/ckrm.c b/kernel/ckrm/ckrm.c
--- a/kernel/ckrm/ckrm.c	Wed Mar 31 16:52:17 2004
+++ b/kernel/ckrm/ckrm.c	Wed Mar 31 16:52:17 2004
@@ -28,6 +28,8 @@
  * 10 Nov 2003
  *        Fixed a bug in fork and exit callbacks. Added callbacks_active and
  *        surrounding logic. Added task paramter for all CE callbacks.
+ * 23 Mar 2004
+ *        moved to referenced counted class objects and correct locking
  */
 
 #include <linux/config.h>
@@ -44,6 +46,8 @@
 #include <linux/module.h>
 #include <linux/ckrm_ce.h>
 #include <linux/ckrm_rc.h>
+#include <net/sock.h>
+#include <linux/ip.h>
 
 /*
  * callback structures for each registered resource controller.
@@ -51,10 +55,12 @@
 ckrm_res_callback_t ckrm_res_ctlrs[CKRM_MAX_RES_CTLRS];
 static spinlock_t ckrm_res_ctlrs_lock = SPIN_LOCK_UNLOCKED; // protects the array
 long bit_res_ctlrs = 0; // bit set for registered resource controllers
-static atomic_t nr_resusers[CKRM_MAX_RES_CTLRS]; // no of users for each res ctlr
+atomic_t nr_resusers[CKRM_MAX_RES_CTLRS]; // no of users for each res ctlr
 static int max_resid = 0; // highest resource id currently used.
 
+
 EXPORT_SYMBOL(ckrm_res_ctlrs);
+EXPORT_SYMBOL(nr_resusers);
 
 /*
  * Callback registered by the classification engine.
@@ -64,22 +70,20 @@
  */
 static ckrm_eng_callback_t ckrm_eng_callbacks;
 static atomic_t nr_engusers; // Number of users using the data structure above.
-static int callbacks_active = 0;
-
-/* 
- * Default Class and global variables
- */
-
-static int ckrm_num_classes = 1;
-static LIST_HEAD(ckrm_classes);
+// global variables
+static int ckrm_num_classes = 2; // no need to hold lock for reading
+static LIST_HEAD(ckrm_classes); // hold read lock for read, write lock for write
+rwlock_t ckrm_class_lock = RW_LOCK_UNLOCKED;  // protect above 3 variables
 
+// Default classes
 struct ckrm_core_class ckrm_dflt_class = { .class_type = CKRM_TASK_CLASS };
 struct ckrm_core_class ckrm_net_root   = { .class_type = CKRM_NET_CLASS };
 
 EXPORT_SYMBOL(ckrm_dflt_class);
 EXPORT_SYMBOL(ckrm_net_root);
 
-rwlock_t               ckrm_class_lock = RW_LOCK_UNLOCKED;  // protect the class and rc hierarchy structure add/del
+// is classification engine callback active ?
+static int callbacks_active = 0;
 
 
 /**************************************************************************
@@ -92,9 +96,9 @@
 inline unsigned int
 is_core_valid(ckrm_core_class_t *core)
 {
-	if (core && (core->magic == CKRM_CORE_MAGIC))
-		return 1;
-	return 0;
+	return (core && (core->magic == CKRM_CORE_MAGIC) &&
+		((core->class_type == CKRM_TASK_CLASS) || 
+		 (core->class_type == CKRM_NET_CLASS)));
 }
 
 /*
@@ -103,18 +107,22 @@
 inline unsigned int
 is_res_regd(int resid)
 {
-	return (test_bit(resid, &bit_res_ctlrs));
+	if (resid < 0 || resid > CKRM_MAX_RES_CTLRS)
+		return 0;
+	else
+		return (test_bit(resid, &bit_res_ctlrs));
 }
 
 int 
-ckrm_resid_lookup (char *resname)
+ckrm_resid_lookup(char *resname)
 {
 	int resid = -1;
 	
 	//for_each_resid(resid) {
 	for (resid=0; resid < CKRM_MAX_RES_CTLRS; resid++) { 
 		if (is_res_regd(resid)) {
-			if (!strncmp(resname, ckrm_res_ctlrs[resid].res_name,CKRM_MAX_RES_NAME))
+			if (!strncmp(resname, ckrm_res_ctlrs[resid].res_name,
+							CKRM_MAX_RES_NAME))
 				return resid;
 		}
 	}
@@ -143,135 +151,638 @@
 }
 
 
-
-
 EXPORT_SYMBOL(is_core_valid);
 EXPORT_SYMBOL(is_res_regd);
 EXPORT_SYMBOL(ckrm_resid_lookup);
 EXPORT_SYMBOL(ckrm_classobj);
 
 
-
 /**************************************************************************
- *                   Internal Functions                                   *
+ *                   Internal Functions/macros                            *
  **************************************************************************/
 
-#define hnode_2_core(ptr)  ((ptr) ? container_of(ptr,struct ckrm_core_class,hnode) : NULL)
+#define hnode_2_core(ptr) \
+		((ptr) ? container_of(ptr, struct ckrm_core_class, hnode) : NULL)
+
+#define ce_protect()      (atomic_inc(&nr_engusers))
+#define ce_release()      (atomic_dec(&nr_engusers))
+#define CE_PROTECTED(cmd) do { ce_protect();  cmd ; ce_release(); } while (0)  // wrapper for code
+
+
+// CE callback that takes one parameter and returns nothing
+#define CECB_1ARG_NORET(fn, arg)				\
+do {								\
+	if (callbacks_active && ckrm_eng_callbacks.fn) {	\
+		(*ckrm_eng_callbacks.fn)(arg);			\
+	}							\
+} while (0) 
+
+// CE callback that takes one parameter and returns, whatever the function
+// returns
+#define CECB_1ARG_RET(fn, arg, ret)				\
+do {								\
+	if (callbacks_active && ckrm_eng_callbacks.fn) {	\
+		ret = (*ckrm_eng_callbacks.fn)(arg);		\
+	}							\
+} while (0)
+
+// CE callback that takes two parameteasr and returns nothing
+#define CECB_2ARGS_NORET(fn, arg1, arg2)			\
+do {								\
+	if (callbacks_active && ckrm_eng_callbacks.fn) {	\
+		(*ckrm_eng_callbacks.fn)(arg1, arg2);		\
+	}							\
+} while (0)
+
+// CE callback that takes two parameters and returns whatever the function
+// returns
+#define CECB_2ARGS_RET(fn, arg1, arg2, ret)			\
+{								\
+	if (callbacks_active && ckrm_eng_callbacks.fn) {	\
+		ret = (*ckrm_eng_callbacks.fn)(arg1, arg2);	\
+	}							\
+}
+
+// CE callback that takes three parameters and returns nothing
+#define CECB_3ARGS_NORET(fn, arg1, arg2, arg3)			\
+do {								\
+	if (callbacks_active && ckrm_eng_callbacks.fn) {	\
+		(*ckrm_eng_callbacks.fn)(arg1, arg2, arg3);	\
+	}							\
+} while (0)
+
+
 
-static inline
-void set_callbacks_active(void)
+static inline void 
+set_callbacks_active(void)
 {
 	callbacks_active = ((atomic_read(&nr_engusers) > 0) &&
 			    (ckrm_eng_callbacks.always_callback || (ckrm_num_classes > 1)));
 }
 
-/*
- * Change the core class of the given task.
- */
+inline void
+ckrm_core_grab(void *core)
+{
+	ckrm_core_class_t *lcore = core;
+	if (lcore) {
+		atomic_inc(&lcore->refcnt);
+ 	}
+}
+
+inline void
+ckrm_core_drop(void *core)
+{
+	ckrm_core_class_t *lcore = core;
+	if (lcore) {
+		atomic_dec(&lcore->refcnt);
+ 	}
+}
+
 void
-ckrm_reclassify_task(struct task_struct *tsk, ckrm_core_class_t *core)
+ckrm_reclassify_net(struct ckrm_net_struct *ns, ckrm_core_class_t *core, int action)
 {
 	int i;
 	ckrm_res_callback_t *rcbs;
 	ckrm_core_class_t *old_core;
-	void *old_res_class, *new_res_class;
+	void  *old_res_class, *new_res_class;
 
-	// Remove the task from the current core class
-	if ((old_core = tsk->ckrm_core) != NULL) {
+	// remove the net_struct from the current class
+	if ((old_core = ns->core) != NULL) {
 		spin_lock(&old_core->ckrm_lock);
-		list_del(&tsk->ckrm_link);
-		tsk->ckrm_core = NULL;
+		list_del(&ns->ckrm_link);
+		ns->core = NULL;
 		spin_unlock(&old_core->ckrm_lock);
-	}	
-
+	}
+	
 	if (core != NULL) {
 		spin_lock(&core->ckrm_lock);
-		tsk->ckrm_core = core;
-		list_add(&tsk->ckrm_link, &core->tasklist);
+		ns->core = core;
+		list_add(&ns->ckrm_link, &core->tasklist);
 		spin_unlock(&core->ckrm_lock);
 	}
 
 	if (core != old_core) {
 		for (i = 0; i < max_resid; i++) {
 			rcbs = &ckrm_res_ctlrs[i];
-			old_res_class = old_core ? old_core->res_class[i] : NULL;
+			old_res_class = old_core ? 
+				old_core->res_class[i] : NULL;
 			new_res_class = core ? core->res_class[i] : NULL;
-			if (rcbs->change_resclass && (old_res_class != new_res_class)) 
-				(*rcbs->change_resclass)(tsk,old_res_class, new_res_class);
+			if (rcbs->change_resclass && 
+				(old_res_class != new_res_class)) 
+					(*rcbs->change_resclass)
+						((void *)ns,old_res_class, 
+						 	new_res_class);
 		}
 	}
 	return;
 }
 
+
 /*
- * Reclassify all tasks in the given core class.
+ * Change the core class of the given task.
+ *
+ * Change the task's core class  to "core" if the task's current 
+ * core(task->ckrm_core) is same as given "oldcore", if it is non-NULL.
+ *
+ * Caller is responsible to make sure the task structure stays put through
+ * this function.
+ *
+ * This function should be called with the following locks not held
+ * 	- task_lock
+ * 	- core->ckrm_lock, if core is NULL then ckrm_dflt_class.ckrm_lock
+ * 	- tsk->ckrm_core->ckrm_lock
+ * 
+ * Function is also called with a ckrm_core_grab on the new core, hence
+ * it needs to be dropped if no assignment takes place.
  */
-int
-ckrm_reclassify_class_tasks(struct ckrm_core_class *core)
+
+static void
+ckrm_set_taskclass(struct task_struct *tsk, ckrm_core_class_t *core, 
+		   ckrm_core_class_t *oldcore, int action)
 {
-	int ret = 0;
-  
-	// Reclassify all tasks in the given core class.
-	if (!list_empty(&core->tasklist)) {
-		struct list_head *lh1, *lh2;
-		ckrm_core_class_t *temp;
-		struct task_struct *tsk;
+	int i;
+	ckrm_res_callback_t *rcbs;
+	ckrm_core_class_t *curr_core;
+	void *old_res_class, *new_res_class;
+	int drop_old_core;
 
-		read_lock(&ckrm_class_lock);
-		spin_lock(&core->ckrm_lock);
 
-		list_for_each_safe(lh1, lh2, &core->tasklist) {
-			list_del(lh1);
-			tsk = container_of(lh1, struct task_struct, ckrm_link);
-
-			if (ckrm_num_classes > 0) {
-				temp = (*ckrm_eng_callbacks.reclassify)(tsk);
-			} else {
-				temp = &ckrm_dflt_class;
-			}
-			if (unlikely(temp == core)) {
-				// Classification engine still using this core class,
-				temp = NULL;
-				ret = -EBUSY;
-			}
-			tsk->ckrm_core = NULL;
-			ckrm_reclassify_task(tsk, temp);
+	task_lock(tsk);
+	curr_core = tsk->ckrm_core;
+
+	// check whether compare_and_exchange should
+	if (oldcore && (oldcore != curr_core)) {
+		task_unlock(tsk);
+		if (core) {
+			/* compensate for previous grab */
+			// printk("ckrm_set_taskclass(%s:%d): Race-condition caught <%s>\n",
+			//	tsk->comm,tsk->pid,core->name);
+			ckrm_core_drop(core);
 		}
-		
+		return;
+	}
+
+	// make sure we have a real destination core
+	if (!core) {
+		core = &ckrm_dflt_class;
+		ckrm_core_grab(core);
+	}
+
+	// take out of old class 
+	// remember that we need to drop the oldcore
+	if ((drop_old_core = (curr_core != NULL))) {
+		spin_lock(&curr_core->ckrm_lock);
+		if (core == curr_core) {
+			// we are already in the destination class.
+			// we still need to drop oldcore
+			spin_unlock(&curr_core->ckrm_lock);
+			task_unlock(tsk);
+			goto out;
+		}
+		list_del(&tsk->ckrm_link);
+		INIT_LIST_HEAD(&tsk->ckrm_link);
+		tsk->ckrm_core = NULL;
+		spin_unlock(&curr_core->ckrm_lock);
+	}	
+
+	// put into new class 
+	spin_lock(&core->ckrm_lock);
+	tsk->ckrm_core = core;
+	list_add(&tsk->ckrm_link, &core->tasklist);
+	spin_unlock(&core->ckrm_lock);
+
+	if (core == curr_core) {
+		task_unlock(tsk);
+		goto out;
+	}
+
+	CECB_3ARGS_NORET(notify, tsk, core, action);
+
+	task_unlock(tsk);
+
+	for (i = 0; i < max_resid; i++) {
+		atomic_inc(&nr_resusers[i]);
+		rcbs = &ckrm_res_ctlrs[i];
+		old_res_class = curr_core ? curr_core->res_class[i] : NULL;
+		new_res_class = core ? core->res_class[i] : NULL;
+		if (rcbs && rcbs->change_resclass && (old_res_class != new_res_class)) 
+			(*rcbs->change_resclass)(tsk, old_res_class, new_res_class);
+		atomic_dec(&nr_resusers[i]);
+	}
+
+ out:
+	if (drop_old_core) 
+		ckrm_core_drop(curr_core);
+	return;
+}
+
+/**************************************************************************
+ *                   Functions called from classification points          *
+ **************************************************************************/
+
+#define TASK_MAGIC 0xDEADB0DE
+inline void
+ckrm_new_task(struct task_struct *tsk)
+{
+	tsk->ckrm_core = NULL;
+	tsk->ce_data   = NULL;
+	INIT_LIST_HEAD(&tsk->ckrm_link);
+	return;
+}
+
+
+#define CKRM_CB_TASK(function, tsk, action)					\
+do {										\
+	struct ckrm_core_class *core = NULL, *old_core = tsk->ckrm_core;	\
+										\
+	CECB_1ARG_RET(function, tsk, core);					\
+	if (core) {								\
+		/* called synchrously. no need to get task struct */		\
+		ckrm_set_taskclass(tsk, core, old_core, action);		\
+	}									\
+} while (0)
+
+
+#define CKRM_CB_NET(function, tsk, action)	\
+do {						\
+	struct ckrm_core_class *core = NULL;	\
+						\
+	CECB_1ARG_RET(function, tsk, core);	\
+	if (core && (core != tsk->core)) {	\
+		ckrm_reclassify_net(tsk, core, action);	\
+	}					\
+} while (0)
+
+void
+ckrm_cb_exec(const char *filename)
+{
+	struct ckrm_core_class *core = NULL, *old_core = current->ckrm_core;
+
+	ce_protect();
+	CECB_2ARGS_RET(exec, current, filename, core);
+
+	// called synchrously. no need to get task struct
+	ckrm_set_taskclass(current, core, old_core, CKRM_ACTION_EXEC);
+	ce_release();
+	return;
+}
+
+void
+ckrm_cb_fork(struct task_struct *tsk)
+{
+	struct ckrm_core_class *core = NULL;
+
+	ce_protect();
+	CECB_1ARG_RET(fork, tsk, core);
+	if (core == NULL) {
+		task_lock(tsk->parent);
+		core = tsk->parent->ckrm_core;
+		ckrm_core_grab(core);
+		task_unlock(tsk->parent);
+	}
+	if (!list_empty(&tsk->ckrm_link))
+		printk("BUG in cb_fork.. tsk (%s:%d> already linked\n",
+			tsk->comm,tsk->pid);
+
+	// called synchrously. no need to get task struct
+	ckrm_set_taskclass(tsk, core, NULL, CKRM_ACTION_FORK);
+	ce_release();
+	return;
+}
+
+void
+ckrm_cb_listen(struct sock *sk)
+{
+	struct ckrm_net_struct *ns;
+	struct ckrm_core_class *core;
+
+	ns = (struct ckrm_net_struct *)
+		kmalloc(sizeof(struct ckrm_net_struct), GFP_KERNEL);
+	if (!ns)
+		return;
+
+	ns->family = sk->sk_family;
+	ns->daddr4 = inet_sk(sk)->daddr;
+	ns->dport = inet_sk(sk)->dport;
+
+	CE_PROTECTED ( CKRM_CB_NET(listen, ns, CKRM_ACTION_LISTEN) );
+
+	return ;        /* Hubertus (3/26):  <core> is never set */
+
+	// FIXME: where is this core coming from ?
+	// core in _CB4 is in different context
+	// what if core is NULL ?
+	spin_lock(&core->ckrm_lock);
+	read_lock(&tasklist_lock);
+	ns->core = core;
+	printk("ckrm_cb_listen: adding %p to tasklist of %s\n", ns, core->name);
+	list_add(&ns->ckrm_link, &core->tasklist);
+	read_unlock(&tasklist_lock);
+	spin_unlock(&core->ckrm_lock);	
+}
+
+void
+ckrm_cb_exit(struct task_struct *tsk)
+{
+	ckrm_core_class_t *core;
+
+	// Remove the task from the current core class
+	
+
+	task_lock(tsk);
+
+	CE_PROTECTED ( CECB_1ARG_NORET(exit, tsk) );
+
+	if ((core = tsk->ckrm_core) != NULL) {
+		spin_lock(&core->ckrm_lock);
+		tsk->ckrm_core = NULL;
+		tsk->ce_data = NULL;
+		list_del(&tsk->ckrm_link);
+		ckrm_core_drop(core);
 		spin_unlock(&core->ckrm_lock);
-		read_unlock(&ckrm_class_lock);
+	} else {
+		tsk->ce_data = NULL;
+		INIT_LIST_HEAD(&tsk->ckrm_link);
 	}
-	return ret;
+	task_unlock(tsk);
+
+	return;
+}
+
+void
+ckrm_cb_uid(void)
+{
+	CE_PROTECTED ( CKRM_CB_TASK(uid, current, CKRM_ACTION_UID) );
+}
+
+void
+ckrm_cb_gid(void)
+{
+	CE_PROTECTED ( CKRM_CB_TASK(gid, current, CKRM_ACTION_GID) );
+}
+
+
+/***********************************************************************
+ *
+ * Asynchronous callback functions   (driven by RCFS)
+ * 
+ *    Async functions force a setting of the task structure
+ *    synchronous callbacks are protected against race conditions 
+ *    by using a cmpxchg on the core before setting it.
+ *    Async calls need to be serialized to ensure they can't 
+ *    race against each other 
+ *
+ ***********************************************************************/
+
+DECLARE_MUTEX(async_serializer);    // serialize all async functions
+
+
+static inline int
+validate_and_grab_core(struct ckrm_core_class *core)
+{
+	int rc = 0;
+	read_lock(&ckrm_class_lock);
+	if (likely(is_core_valid(core))) {
+		ckrm_core_grab(core);
+		rc = 1;
+	}
+	read_unlock(&ckrm_class_lock);
+	return rc;
 }
 
 /*
  * Go through the task list and reclassify all tasks according to the current
  * classification rules.
  *
+ * We have the problem that we can not hold any lock (including the 
+ * tasklist_lock) while classifying. Two methods possible
+ *
+ * (a) go through entire pidrange (0..pidmax) and if a task exists at 
+ *     that pid then reclassify it
+ * (b) go several time through task list and build a bitmap for a particular 
+ *     subrange of pid otherwise the memory requirements ight be too much.
+ * 
+ * We use a hybrid by comparing ratio nr_threads/pidmax
  */
+
 static void
 ckrm_reclassify_all_tasks(void)
 {
-	ckrm_core_class_t *temp;
+	extern int pid_max;
+
 	struct task_struct *proc, *thread;
+	int i;
+	int curpidmax = pid_max;
+	int ratio;
+	int use_bitmap;
+
+
+	ratio = curpidmax / nr_threads;
+	if (curpidmax <= PID_MAX_DEFAULT) {
+	     use_bitmap = 1;
+	} else {
+	     use_bitmap = (ratio >= 2);
+	}
+
+	ce_protect();
+
+ retry:		
+	if (use_bitmap == 0) {
+		// go through it in one walk
+		read_lock(&tasklist_lock);
+		for ( i=0 ; i<curpidmax ; i++ ) {
+			if ((thread = find_task_by_pid(i)) == NULL) 
+				continue;
+			get_task_struct(thread);
+			read_unlock(&tasklist_lock);
+			CKRM_CB_TASK(reclassify, thread, CKRM_ACTION_RECLASSIFY);
+			put_task_struct(thread);
+			read_lock(&tasklist_lock);
+		}
+		read_unlock(&tasklist_lock);
+	} else {
+		unsigned long *bitmap;
+		int bitmapsize;
+		int order = 0;
+		int num_loops;
+		int pid;
 
-	read_lock(&ckrm_class_lock);
-	read_lock(&tasklist_lock);
-	do_each_thread(proc, thread) {
-		if (ckrm_num_classes > 0) {
-			temp = (*ckrm_eng_callbacks.reclassify)(thread);
+
+		bitmap = (unsigned long*) __get_free_pages(GFP_KERNEL,order);
+		if (bitmap == NULL) {
+			ratio = 0;
+			goto retry;
+		}
+
+		bitmapsize = 8 * (1 << (order + PAGE_SHIFT));
+		num_loops  = (curpidmax + bitmapsize - 1) >> order;
+
+		for ( i=0 ; i < num_loops; i++) {
+			int pid_start = i*bitmapsize; 
+			int pid_end   = pid_start + bitmapsize;
+			int num_found = 0;
+			int pos;
+
+			memset(bitmap, 0, bitmapsize/8); // start afresh
+
+			read_lock(&tasklist_lock);
+			do_each_thread(proc, thread) {
+				pid = thread->pid;
+				if ((pid < pid_start) || (pid >= pid_end))
+					continue;
+				pid -= pid_start;
+				set_bit(pid, bitmap);
+				num_found++;
+			} while_each_thread(proc, thread);
+			read_unlock(&tasklist_lock);
+		
+			if (num_found == 0) 
+				continue;
+
+			pos = 0;
+			for ( ; num_found-- ; ) {
+				pos = find_next_bit(bitmap, bitmapsize, pos);
+				pid = pos + pid_start;
+
+				read_lock(&tasklist_lock);
+				if ((thread = find_task_by_pid(pid)) != NULL) {
+					get_task_struct(thread);
+					read_unlock(&tasklist_lock);
+					CKRM_CB_TASK(reclassify, thread, CKRM_ACTION_RECLASSIFY);
+					put_task_struct(thread);
+				} else {
+					read_unlock(&tasklist_lock);
+				}
+			}
+		}
+
+	}
+	ce_release();
+}
+
+int
+ckrm_reclassify(int pid)
+{
+	struct task_struct *tsk;
+	int rc = 0;
+
+	down(&async_serializer);   // protect again race condition
+	if (pid < 0) {
+		// do we want to treat this as process group .. should YES ToDo
+		 rc = -EINVAL;
+	} else if (pid == 0) {
+		// reclassify all tasks in the system
+		ckrm_reclassify_all_tasks();
+	} else {
+		// reclassify particular pid
+		read_lock(&tasklist_lock);
+		if ((tsk = find_task_by_pid(pid)) != NULL) {
+			get_task_struct(tsk);
+			read_unlock(&tasklist_lock);
+			CE_PROTECTED ( CKRM_CB_TASK(reclassify, tsk, CKRM_ACTION_RECLASSIFY) );
+			put_task_struct(tsk);
 		} else {
-			temp = &ckrm_dflt_class;
+			read_unlock(&tasklist_lock);
+			rc = -EINVAL;
 		}
-		if (temp != thread->ckrm_core) {
-			ckrm_reclassify_task(thread, temp);
+	}
+	up(&async_serializer);
+	return rc;
+}
+
+/*
+ * Reclassify all tasks in the given core class.
+ */
+
+static void
+ckrm_reclassify_class_tasks(struct ckrm_core_class *core)
+{
+
+	if (!validate_and_grab_core(core))
+		return;
+
+	down(&async_serializer);   // protect again race condition
+
+next_task:
+	spin_lock(&core->ckrm_lock);
+	// Reclassify all tasks in the given core class.
+	if (!list_empty(&core->tasklist)) {
+		ckrm_core_class_t *new_core = NULL;
+		struct task_struct *tsk = 
+				list_entry(core->tasklist.next ,struct task_struct, ckrm_link);
+		
+		get_task_struct(tsk);
+		spin_unlock(&core->ckrm_lock);
+
+		CECB_1ARG_RET(reclassify, tsk, new_core);
+		if (core == new_core) {
+			// don't allow reclassifying to the same class
+			// as we are in the process of cleaning up this class
+			ckrm_core_drop(new_core); // to compensate CE's grab
+			new_core = NULL;
 		}
-	} while_each_thread(proc, thread);
-	read_unlock(&tasklist_lock);
-	read_unlock(&ckrm_class_lock);
+		ckrm_set_taskclass(tsk, new_core, core, CKRM_ACTION_RECLASSIFY);
+		put_task_struct(tsk);
+		goto next_task;
+	}
+	ckrm_core_drop(core);
+	spin_unlock(&core->ckrm_lock);
+
+	up(&async_serializer);
+
+	return ;
 }
 
-	
+/*
+ * Change the core class of the given task.
+ */
+
+void
+ckrm_forced_reclassify_pid(int pid, ckrm_core_class_t *core)
+{
+	struct task_struct *tsk;
+
+	if (!validate_and_grab_core(core))
+		return;
+
+	if (core->class_type == CKRM_TASK_CLASS) {
+		read_lock(&tasklist_lock);
+		if ((tsk = find_task_by_pid(pid)) == NULL) {
+			read_unlock(&tasklist_lock);
+			return;
+		}
+		get_task_struct(tsk);
+		read_unlock(&tasklist_lock);
+
+		down(&async_serializer);   // protect again race condition
+
+		ce_protect();
+		CECB_1ARG_NORET(manual, tsk);
+
+		ckrm_set_taskclass(tsk, core, NULL, CKRM_ACTION_MANUAL);
+		put_task_struct(tsk);
+		ce_release();
+
+		up(&async_serializer);
+	}
+	return;
+}
+EXPORT_SYMBOL(ckrm_forced_reclassify_pid);
+
+/*
+ * Change the core class of the given net struct
+ */
+void
+ckrm_forced_reclassify_net(struct ckrm_net_struct *ns, ckrm_core_class_t *core)
+{
+	if (!is_core_valid(core))	
+		return;
+
+	if (core->class_type == CKRM_NET_CLASS) {
+		ckrm_reclassify_net(ns, core, CKRM_ACTION_MANUAL);
+	}
+	return;
+}
 
 /****************************************************************************
  *           Interfaces for classification engine                           *
@@ -285,19 +796,20 @@
 int
 ckrm_register_engine(ckrm_eng_callback_t *ecbs)
 {
-
-	if (atomic_read(&nr_engusers) > 0) {
+	ce_protect();
+	if (atomic_read(&nr_engusers) != 1) {
 		// Some engine is acive, deregister it first.
+		ce_release();
 		return (-EBUSY);
 	}
 	
 	if (strnlen(ecbs->ckrm_eng_name, CKRM_MAX_ENG_NAME) == 0 ||
-				ecbs->reclassify == NULL) {
+	    ecbs->reclassify == NULL) {
+		ce_release();
 		return (-EINVAL);
 	}
 
 	ckrm_eng_callbacks = *ecbs;
-	atomic_set(&nr_engusers, 1); // existence counted as 1
 	set_callbacks_active();
 	return 0;
 }
@@ -310,8 +822,11 @@
 int
 ckrm_unregister_engine(ckrm_eng_callback_t *ecbs)
 {
+	callbacks_active = 0; 
+
 	if (atomic_dec_and_test(&nr_engusers) != 1) {
 		// Somebody is currently using the engine, cannot deregister.
+		atomic_inc(&nr_engusers);
 		return (-EBUSY);
 	}
 
@@ -321,27 +836,28 @@
 		// Somebody other than the owner is trying to unregister.
 		return (-EINVAL);
 	}
-
 	memset(&ckrm_eng_callbacks, 0, sizeof(ckrm_eng_callbacks));
 	set_callbacks_active();
 	return 0;
 }
 
-/* functions to manipulate class (core or resource) hierarchies 
+/*
+ * functions to manipulate class (core or resource) hierarchies 
  */
 
+#ifndef NEW_HNODE_IMPLMN
+/*
+ * functions to manipulate class (core or resource) hierarchies 
+ */
 
 /* Caller must ensure ckrm_class_lock held */
-void ckrm_hnode_add(struct ckrm_hnode *node,
-	  	    struct ckrm_hnode *parent )
+void
+ckrm_hnode_add(struct ckrm_hnode *node, struct ckrm_hnode *parent)
 {
 	node->parent = parent;
 	INIT_LIST_HEAD(&node->children);
 	INIT_LIST_HEAD(&node->siblings);
 
-//	if (parent)
-//		printk(KERN_ERR "hnode_add: %p\n",&parent->children);
-
  	if (parent) {
 		if (!is_core_valid(hnode_2_core(parent)))
 			printk(KERN_ERR "hnode_add: non-NULL invalid parent\n");
@@ -356,7 +872,8 @@
 }
 
 /* Caller must ensure ckrm_class_lock held */
-int ckrm_hnode_remove(struct ckrm_hnode *node)
+int
+ckrm_hnode_remove(struct ckrm_hnode *node)
 {
 	/* ensure that the node does not have children */
 	if (!list_empty(&node->children))
@@ -366,6 +883,144 @@
 	return 1;
 }
 
+#else
+/* 
+ */
+static void
+ckrm_add_child(struct ckrm_core_class *parent, struct ckrm_core_class *child)
+{
+	struct ckrm_hnode *cnode = &child->hnode;
+
+	if (!is_core_valid(child)) {
+		printk(KERN_ERR "Invalid child %p given in ckrm_add_child\n", child);
+		return;
+	}
+	
+	spin_lock(&child->ckrm_lock);
+	INIT_LIST_HEAD(&cnode->children);
+	INIT_LIST_HEAD(&cnode->siblings);
+
+ 	if (parent) {
+		struct ckrm_hnode *pnode;
+
+		if (!is_core_valid(parent)) {
+			printk(KERN_ERR "Invalid parent %p given in ckrm_add_child\n",
+					parent);
+			parent = NULL;
+		} else {
+			pnode = &parent->hnode;
+			write_lock(&parent->hnode_rwlock);
+			list_add(&cnode->siblings, &pnode->children);
+			write_unlock(&parent->hnode_rwlock);
+		}
+	}
+	cnode->parent = parent;
+	spin_unlock(&child->ckrm_lock);
+	return;
+}
+
+/* 
+ */
+static int
+ckrm_remove_child(struct ckrm_core_class *child)
+{
+	struct ckrm_hnode *cnode, *pnode;
+	struct ckrm_core_class *parent;
+
+	if (!is_core_valid(child)) {
+		printk(KERN_ERR "Invalid child %p given in ckrm_remove_child\n", child);
+		return 0;
+	}
+
+	cnode = &child->hnode;
+	parent = cnode->parent;
+	if (!is_core_valid(parent)) {
+		printk(KERN_ERR "Invalid parent %p in ckrm_remove_child\n", parent);
+		return 0;
+	}
+
+	pnode = &parent->hnode;
+
+	if (cnode->parent != parent) {
+		printk(KERN_ERR "Invalid parent %p in child %p in ckrm_remove_child\n",
+				parent, child);
+		return 0;
+	}
+
+	spin_lock(&child->ckrm_lock);
+	/* ensure that the node does not have children */
+	if (!list_empty(&cnode->children)) {
+		spin_unlock(&child->ckrm_lock);
+		return 0;
+	}
+	write_lock(&parent->hnode_rwlock);
+	list_del(&cnode->siblings);
+	write_unlock(&parent->hnode_rwlock);
+	cnode->parent = NULL;
+	spin_unlock(&child->ckrm_lock);
+	return 1;
+}
+
+void
+ckrm_lock_hier(struct ckrm_core_class *parent)
+{
+	if (is_core_valid(parent)) {
+		read_lock(&parent->hnode_rwlock);
+	}
+}
+
+void 
+ckrm_unlock_hier(struct ckrm_core_class *parent)
+{
+	if (is_core_valid(parent)) {
+		read_unlock(&parent->hnode_rwlock);
+	}
+}
+
+/*
+ * hnode_rwlock of the parent core class must held in read mode.
+ * external callers should 've called ckrm_lock_hier before calling this
+ * function.
+ */
+struct ckrm_core_class *
+ckrm_get_next_child(struct ckrm_core_class *parent,
+			struct ckrm_core_class *child)
+{
+	struct ckrm_hnode *next_cnode;
+	struct ckrm_core_class *next_childcore;
+
+	if (!is_core_valid(parent)) {
+		printk(KERN_ERR "Invalid parent %p in ckrm_get_next_child\n", parent);
+		return NULL;
+	}
+	if (list_empty(&parent->hnode.children)) {
+		return NULL;
+	}
+
+	if (child) {
+		if (!is_core_valid(child)) {
+			printk(KERN_ERR "Invalid child %p in ckrm_get_next_child\n", child);
+			return NULL;
+		}
+		next_cnode = (struct ckrm_hnode *) child->hnode.siblings.next;
+	} else {
+		next_cnode = (struct ckrm_hnode *) parent->hnode.children.next;
+	}
+	next_childcore = hnode_2_core(next_cnode);
+
+	if (next_childcore == parent) { // back at the anchor
+		return NULL;
+	}
+
+	if (!is_core_valid(next_childcore)) {
+		printk(KERN_ERR "Invalid next child %p in ckrm_get_next_child\n",
+				next_childcore);
+		return NULL;
+	}
+	return next_childcore;
+}
+
+#endif // NEW_HNODE_IMPLMN
 
 static void 
 ckrm_alloc_res_class(struct ckrm_core_class *core,
@@ -384,14 +1039,19 @@
 	core->res_class[resid] = NULL;
 
 	if (test_bit(resid, &bit_res_ctlrs)) {
-		ckrm_res_callback_t *rcbs = &ckrm_res_ctlrs[resid];
+		ckrm_res_callback_t *rcbs;
+
+		atomic_inc(&nr_resusers[resid]);
+		rcbs = &ckrm_res_ctlrs[resid];
 		
-		if (rcbs->res_alloc) {
+		if (rcbs && rcbs->res_alloc) {
 			core->res_class[resid] =(*rcbs->res_alloc)(core,parent);
-			if (core->res_class[resid]) 
-				atomic_inc(&nr_resusers[resid]);
-			else
+			if (!core->res_class[resid]) {
 				printk(KERN_ERR "Error creating res class\n");
+				atomic_dec(&nr_resusers[resid]);
+			}
+		} else {
+			atomic_dec(&nr_resusers[resid]);
 		}
 	}
 }
@@ -403,12 +1063,12 @@
  * Return the handle to the core class on success, NULL on failure.
  */
 
-ckrm_core_class_t *dcore;
 
 struct ckrm_core_class *
 ckrm_alloc_core_class(struct ckrm_core_class *parent, struct dentry *dentry)
 {
 	int i;
+	ckrm_core_class_t *dcore;
 
 	if (!is_core_valid(parent))
 		return NULL; 
@@ -422,12 +1082,26 @@
 	INIT_LIST_HEAD(&dcore->tasklist);
 	dcore->ckrm_lock = SPIN_LOCK_UNLOCKED;
 
+	atomic_set(&dcore->refcnt, 0);
+	memset(dcore->name, 0, 16);
+	strcpy(dcore->name, dentry->d_name.name);
 	write_lock(&ckrm_class_lock);
 
 	list_add(&dcore->clslist,&ckrm_classes);
+#ifndef NEW_HNODE_IMPLMN
 	ckrm_hnode_add(&dcore->hnode, parent ? &parent->hnode : NULL); 
 	ckrm_num_classes++;
 	set_callbacks_active();
+	write_unlock(&ckrm_class_lock);
+
+#else
+	ckrm_num_classes++;
+	set_callbacks_active();
+	write_unlock(&ckrm_class_lock);
+
+	ckrm_add_child(parent, dcore); 
+#endif
+
 
 	dcore->class_type = parent->class_type;
 	for (i = 0; i < CKRM_MAX_RES_CTLRS; i++) {
@@ -437,14 +1111,9 @@
 			dcore->res_class[i] = NULL;
 	}
 
-	write_unlock(&ckrm_class_lock);
 
 	/* Inform CE at last, once core is ready for use */
-	if (callbacks_active && *ckrm_eng_callbacks.class_add) {
-		(*ckrm_eng_callbacks.class_add)(dcore->dentry->d_name.name, (void *)dcore);
-	}
-
-
+	CE_PROTECTED ( CECB_2ARGS_NORET(class_add, dcore->dentry->d_name.name, (void *)dcore) );
 	return dcore;
 }
 
@@ -458,13 +1127,17 @@
 	 */
 
 	if (core->res_class[resid]) {
-		ckrm_res_callback_t *rcbs = &ckrm_res_ctlrs[resid];
+		ckrm_res_callback_t *rcbs;
+
+		atomic_inc(&nr_resusers[resid]);
+		rcbs = &ckrm_res_ctlrs[resid];
 
 		if (rcbs->res_free) {
 			(*rcbs->res_free)(core->res_class[resid]);
-			atomic_dec(&nr_resusers[resid]);
+			atomic_dec(&nr_resusers[resid]); // for inc in alloc
 			core->res_class[resid] = NULL;	
-		}	
+		}
+		atomic_dec(&nr_resusers[resid]);
 	}
 }
 
@@ -479,6 +1152,12 @@
 ckrm_free_core_class(struct ckrm_core_class *core)
 {
 	int i;
+	int retry_count = 10;
+
+	if (core == &ckrm_dflt_class) {
+		// cannot remove the default class
+		return -EINVAL;
+	}
 
 	if (!is_core_valid(core)) {
 		// Invalid core
@@ -486,37 +1165,60 @@
 	}
 
 	/* Inform CE first, in case it needs any core class data */
-	if (callbacks_active && *ckrm_eng_callbacks.class_delete) {
-		(*ckrm_eng_callbacks.class_delete)(core->dentry->d_name.name, 
-								(void *)core);
+	CE_PROTECTED ( CECB_2ARGS_NORET(class_delete, core->dentry->d_name.name, (void *)core) );
+
+retry_class:
+	ckrm_reclassify_class_tasks(core);
+
+	if (atomic_read(&core->refcnt) > 0) {
+		// at least try one more time
+		printk("Core class <%s> removal failed. refcount non-zero %d  retries=%d\n",
+		       core->name,atomic_read(&core->refcnt),retry_count-1);
+		// ckrm_debug_free_core_class(core);
+		if (--retry_count) { 
+			schedule_timeout(100);
+			goto retry_class;
+		}
+		
+		return -EBUSY;
 	}
+
+#ifndef NEW_HNODE_IMPLMN
+	if (ckrm_hnode_remove(&core->hnode) == 0) {
+		printk("Core class removal failed. Chilren present\n");
+		return -EBUSY;
+	}
+#else
+	if (ckrm_remove_child(core) == 0) {
+		printk("Core class removal failed. Chilren present\n");
+		return -EBUSY;
+	}
+#endif
+
+	for (i = 0; i < max_resid; i++) {
+		ckrm_free_res_class(core,i);
+	}
+
+	write_lock(&ckrm_class_lock);
+
 	// Clear the magic, so we would know if this core is reused.
 	core->magic = 0;
 
 	// Remove this core class from its linked list.
-	write_lock(&ckrm_class_lock);
-
-	ckrm_hnode_remove(&core->hnode);   /* Hubertus ; locking */
 	list_del(&core->clslist);
 	ckrm_num_classes--;
-
 	set_callbacks_active();
 
-	for (i = 0; i < max_resid; i++) {
-//		if (core->res_class[i]) {
-//		(*ckrm_res_ctlrs[i].res_free)(core->res_class[i]);
-//			atomic_dec(&nr_resusers[i]);
-//		}
-		ckrm_free_res_class(core,i);
-	}
 	write_unlock(&ckrm_class_lock);
 
 	kfree(core);
 	return 0;
 }
 
+#ifndef NEW_HNODE_IMPLMN
 EXPORT_SYMBOL(ckrm_hnode_add);
 EXPORT_SYMBOL(ckrm_hnode_remove);
+#endif
 
 /****************************************************************************
  *           Interfaces for the resource controller                         *
@@ -529,12 +1231,13 @@
 static int
 ckrm_register_res_ctlr_intern(ckrm_res_callback_t *rcbs)
 {
-	int resid;
+	int resid, ret, i;
 	
 	if (!rcbs)
 		return -EINVAL;
 
 	resid = rcbs->resid;
+	/*
 	if (strnlen(rcbs->res_name, CKRM_MAX_RES_NAME) == 0 ||
 	    rcbs->res_alloc        == NULL ||
 	    rcbs->res_free         == NULL ||
@@ -547,15 +1250,16 @@
 		return (-EINVAL);
 	}
 
+	*/
 	spin_lock(&ckrm_res_ctlrs_lock);
 
-	ckrm_res_ctlrs[resid] = *rcbs;
-	atomic_set(&nr_resusers[resid], 0);
-	set_bit(resid, &bit_res_ctlrs);	
-	spin_unlock(&ckrm_res_ctlrs_lock);
-	return resid;
+//	ckrm_res_ctlrs[resid] = *rcbs;
+	printk(KERN_WARNING "resid is %d name is %s %s\n", resid, rcbs->res_name,ckrm_res_ctlrs[resid].res_name);
+//	atomic_set(&nr_resusers[resid], 0);
+//	set_bit(resid, &bit_res_ctlrs);	
+//	spin_unlock(&ckrm_res_ctlrs_lock);
+//	return resid;
 
-#if 0
 	if (resid >= 0) {
 		if (strnlen(ckrm_res_ctlrs[resid].res_name,CKRM_MAX_RES_NAME) == 0) {
 			ckrm_res_ctlrs[resid] = *rcbs;
@@ -586,7 +1290,7 @@
 	}
 	spin_unlock(&ckrm_res_ctlrs_lock);
 	return (-ENOMEM);
-#endif
+
 }
 
 int
@@ -599,11 +1303,17 @@
 	
 	if (resid >= 0) {
 		/* run through all classes and create the resource class object */
-
+		
+		read_lock(&ckrm_class_lock);
 		list_for_each_entry(core, &ckrm_classes, clslist) {
 			printk(KERN_ERR "CKRM .. create res clsobj for <%s>\n",rcbs->res_name);
-			ckrm_alloc_res_class(core,hnode_2_core(core->hnode.parent),resid);
+#ifndef NEW_HNODE_IMPLMN
+			ckrm_alloc_res_class(core, hnode_2_core(core->hnode.parent), resid);
+#else
+			ckrm_alloc_res_class(core, core->hnode.parent, resid);
+#endif
 		}
+		read_unlock(&ckrm_class_lock);
 	}
 	return resid;
 }
@@ -617,7 +1327,7 @@
 ckrm_unregister_res_ctlr(int resid)
 {
 	ckrm_res_callback_t *rcbs;
-	struct ckrm_core_class *core;
+//	struct ckrm_core_class *core;
 
 	if (!is_res_regd(resid))
 		return -EINVAL;
@@ -634,195 +1344,83 @@
 	max_resid = fls(bit_res_ctlrs);
 	spin_unlock(&ckrm_res_ctlrs_lock);
 
+#if 0
+	// FIXME: remove this part of code - chandra
+	// NOT needed, as the original design was to not allow removal of
+	// resource classes if any core class has a reference to the resource
+	// class, made sure by the atomic variable nr_resusers.
+	// Also if we want to change the design to work in this model, move this
+	// code to the top, before we clear the resource controller data
+	// structure with the memset above. - chandra
+	//
 	/* run through all classes and delete the resource class object */
 
 	list_for_each_entry(core, &ckrm_classes, clslist) {
 		printk(KERN_ERR "CKRM .. delete res clsobj for <%s>\n",rcbs->res_name);
 		ckrm_free_res_class(core,resid);
 	}
-
+#endif
 	return 0;
 }
 
-/**************************************************************************
- *                   Functions called from classification points          *
- **************************************************************************/
+/*******************************************************************
+ *   Initialization 
+ *******************************************************************/
 
-inline void
-ckrm_new_task(struct task_struct *tsk)
-{
-	/* nothing has to be done at this point 
-	 * but we keep it as a place holder for now
-	 */
-}
-
-
-#define CKRM_CB1(function, tsk) \
-{ \
-	struct ckrm_core_class *core = NULL; \
-\
-	if (callbacks_active && ckrm_eng_callbacks.function) { \
-		        read_lock(&ckrm_class_lock); \
-                        core = (*ckrm_eng_callbacks.function)(tsk); \
-	                if (core && (core != tsk->ckrm_core)) { \
-		               ckrm_reclassify_task(tsk, core); \
-	                } \
-	                read_unlock(&ckrm_class_lock); \
-	} \
-	return; \
-}
-
-#define CKRM_CB1_ARGS(function, tsk, args) \
-{ \
-	struct ckrm_core_class *core = NULL; \
-\
-	if (callbacks_active && ckrm_eng_callbacks.function) { \
-                read_lock(&ckrm_class_lock); \
-                core = (*ckrm_eng_callbacks.function)(tsk, args); \
-	        if (core && (core != tsk->ckrm_core)) { \
-		        ckrm_reclassify_task(tsk, core); \
-	        } \
-		read_unlock(&ckrm_class_lock); \
-	} \
-	return; \
-}
-
-#define CKRM_CB2(function) \
-{ \
-	if (callbacks_active && ckrm_eng_callbacks.function) { \
-                read_lock(&ckrm_class_lock); \
-		(*ckrm_eng_callbacks.function)();\
-		read_unlock(&ckrm_class_lock); \
-	} \
-	return;\
-}
-
-#define CKRM_CB3(function, arg) \
-{\
-	if (callbacks_active && ckrm_eng_callbacks.function) { \
-                read_lock(&ckrm_class_lock); \
-		(*ckrm_eng_callbacks.function)(arg); \
-		read_unlock(&ckrm_class_lock); \
-	} \
-	return;\
-}
-
-void
-ckrm_cb_exec(const char *filename)
-{
-	CKRM_CB1_ARGS(exec, current, filename);
-}
-
-void
-ckrm_cb_fork(struct task_struct *tsk)
-{
-	CKRM_CB1(fork, tsk);
-}
-
-void
-ckrm_reclassify(int pid)
-{
-	struct task_struct *tsk;
-
-	if (pid < 0) {
-		// do we want to treat this as process group ?
-		return;
-	}
-	if (pid) {
-		if ((tsk = find_task_by_pid(pid)) != NULL) {
-			CKRM_CB1(reclassify, tsk);
-		}
-	} else { // reclassify all tasks in the system
-		ckrm_reclassify_all_tasks();
-	}
-	return;
-}
-
-void
-ckrm_cb_exit(struct task_struct *tsk)
-{
-	ckrm_core_class_t *core;
-
-	// Remove the task from the current core class
-	if ((core = tsk->ckrm_core) != NULL) {
-		spin_lock(&core->ckrm_lock);
-		list_del(&tsk->ckrm_link);
-		tsk->ckrm_core = NULL;
-		spin_unlock(&core->ckrm_lock);
-	}	
-
-	CKRM_CB3(exit, tsk);
-}
-
-void
-ckrm_cb_uid(void)
-{
-	CKRM_CB1(uid, current);
-}
-
-void
-ckrm_cb_gid(void)
-{
-	CKRM_CB1(gid, current);
-}
-
-void
-ckrm_cb_manual(struct task_struct *tsk)
-{
-	if (callbacks_active) { 
-		if (ckrm_eng_callbacks.manual) { 
-			read_lock(&ckrm_class_lock); 
-			(*ckrm_eng_callbacks.manual)(tsk); 
-			read_unlock(&ckrm_class_lock); 
-		} 
-	} 
-	return; 
-}
-
-
-void __init ckrm_init(void) 
+void __init
+ckrm_init(void) 
 {
 	struct ckrm_core_class *core = &ckrm_dflt_class;
 	struct task_struct *tsk;
 	int i;
 
+	while(core) {
+		core->magic = CKRM_CORE_MAGIC;
+		core->dentry = NULL;
+		INIT_LIST_HEAD(&core->tasklist);
+		core->ckrm_lock = SPIN_LOCK_UNLOCKED;
+		for (i = 0; i < CKRM_MAX_RES_CTLRS; i++)
+			core->res_class[i] = NULL;
+
+#ifndef NEW_HNODE_IMPLMN
+		ckrm_hnode_add(&core->hnode, NULL);
+#else
+		ckrm_add_child(NULL, core);
+#endif
 
-	if (!core)
-		return;
+		if (core == &ckrm_dflt_class) {
+			
+			spin_lock(&core->ckrm_lock);
+			memset(core->name, 0, 16);
+			strcpy(core->name, "/rcfs");
+
+			read_lock(&tasklist_lock);
+			for_each_process(tsk) {
+				task_lock(tsk);
+				tsk->ckrm_core = core;
+				INIT_LIST_HEAD(&tsk->ckrm_link);
+				list_add(&tsk->ckrm_link, &core->tasklist);
+				task_unlock(tsk);
+				ckrm_core_grab(core);
+				//printk("ckrm_init: Added %ld to %p\n",(long)tsk->pid,core);
+			}
+			read_unlock(&tasklist_lock);
+			
+			spin_unlock(&core->ckrm_lock);	
+		} else {
+			memset(core->name, 0, 16);
+			strcpy(core->name, "/net");
+		}
 
+		// Add the default class to the global classes list.
+		// ckrm_num_classes already incremented
+		write_lock(&ckrm_class_lock);
+		list_add(&core->clslist, &ckrm_classes);
+		write_unlock(&ckrm_class_lock);
 
-	/* Initialize default core class */
-	core->magic = CKRM_CORE_MAGIC;
-	core->dentry = NULL;
-	INIT_LIST_HEAD(&core->tasklist);
-	core->ckrm_lock = SPIN_LOCK_UNLOCKED;
-	for (i = 0; i < CKRM_MAX_RES_CTLRS; i++)
-		core->res_class[i] = NULL;
-	
-	/* Add the default class to the global classes list 
-	 * ckrm_num_classes initialized to 1, don't increment
-	 */
-	
-	write_lock(&ckrm_class_lock);
-	list_add(&core->clslist,&ckrm_classes);
-	ckrm_hnode_add(&core->hnode,NULL);
-	write_unlock(&ckrm_class_lock);
-	
-	spin_lock(&core->ckrm_lock);
-	
-
-	read_lock(&tasklist_lock);
-	for_each_process(tsk) {
-		task_lock(tsk);
-		tsk->ckrm_core = core;
-		INIT_LIST_HEAD(&tsk->ckrm_link);
-		list_add(&tsk->ckrm_link, &core->tasklist);
-			task_unlock(tsk);
-			//printk("ckrm_init: Added %ld to %p\n",(long)tsk->pid,core);
+		core = (core==&ckrm_dflt_class)?&ckrm_net_root:NULL;
 	}
-	read_unlock(&tasklist_lock);
-	
-	spin_unlock(&core->ckrm_lock);	
+
 	printk("CKRM Initialized\n");
 }
 
@@ -830,8 +1428,6 @@
 EXPORT_SYMBOL(ckrm_register_engine);
 EXPORT_SYMBOL(ckrm_unregister_engine);
 
-EXPORT_SYMBOL(ckrm_reclassify_task);
-
 EXPORT_SYMBOL(ckrm_register_res_ctlr);
 EXPORT_SYMBOL(ckrm_unregister_res_ctlr);
 
@@ -839,3 +1435,5 @@
 EXPORT_SYMBOL(ckrm_free_core_class);
 
 EXPORT_SYMBOL(ckrm_reclassify);
+EXPORT_SYMBOL(ckrm_core_grab);
+EXPORT_SYMBOL(ckrm_core_drop);
diff -Nru a/kernel/ckrm/ckrmutils.c b/kernel/ckrm/ckrmutils.c
--- a/kernel/ckrm/ckrmutils.c	Wed Mar 31 16:52:17 2004
+++ b/kernel/ckrm/ckrmutils.c	Wed Mar 31 16:52:17 2004
@@ -1,6 +1,7 @@
 /* ckrmutils.c - Utility functions for CKRM
  *
  * Copyright (C) Chandra Seetharaman,  IBM Corp. 2003
+ *           (C) Hubertus Franke    ,  IBM Corp. 2004
  * 
  * Provides simple utility functions for the core module, CE and resource
  * controllers.
@@ -66,3 +67,67 @@
 }
 
 EXPORT_SYMBOL(get_exe_path_name);
+
+
+
+/*
+ *  functions that come in handy for debugging 
+ */
+
+#if 0
+
+#include <linux/ckrm_rc.h>
+
+void
+check_tasklist_sanity(struct ckrm_core_class *core)
+{
+	struct list_head *lh1, *lh2;
+	int count = 0;
+
+	if (core) {
+		spin_lock(&core->ckrm_lock);
+		if (list_empty(&core->tasklist)) {
+			spin_unlock(&core->ckrm_lock);
+			printk("check_tasklist_sanity: class %s empty list\n",
+					core->name);
+			return;
+		}
+		list_for_each_safe(lh1, lh2, &core->tasklist) {
+			struct task_struct *tsk = container_of(lh1, struct task_struct, ckrm_link);
+			if (count++ > 20000) {
+				printk("list is CORRUPTED\n");
+				break;
+			}
+			if (tsk->ckrm_core != core) {
+				ckrm_core_class_t *tcore = tsk->ckrm_core;
+				printk("sanity: task %s:%d has ckrm_core |%s| but in list |%s|\n",
+						tsk->comm,tsk->pid,
+						tcore ? tcore->name: "NULL",
+						core->name);
+			}
+		}
+		spin_unlock(&core->ckrm_lock);
+	}
+}
+
+void 
+ckrm_debug_free_core_class(struct ckrm_core_class *core)
+{
+	struct task_struct *proc, *thread;
+	int count = 0;
+
+	printk("Analyze Error <%s> %d\n",core->name,atomic_read(&core->refcnt));
+	read_lock(&tasklist_lock);
+	spin_lock(&core->ckrm_lock);
+	do_each_thread(proc, thread) {
+		struct ckrm_core_class *tcore = (struct ckrm_core_class*) (thread->ckrm_core);
+		count += (core == thread->ckrm_core);
+		printk("%d thread=<%s:%d>  -> <%s> <%lx>\n",
+			count,thread->comm,thread->pid,tcore ? tcore->name : "NULL", thread->flags & PF_EXITING);
+	} while_each_thread(proc, thread);
+	spin_unlock(&core->ckrm_lock);
+	read_unlock(&tasklist_lock);
+	printk("End Analyze Error <%s> %d\n",core->name,atomic_read(&core->refcnt));
+} 
+
+#endif
