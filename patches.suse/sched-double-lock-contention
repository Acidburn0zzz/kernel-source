Nick Piggin <nickpiggin@yahoo.com.au>
Analysis and basic idea from Suresh Siddha <suresh.b.siddha@intel.com>

Performance

"This small change in load_balance() brings the performance back upto base
scheduler(infact I see a ~1.5% performance improvement now).  Basically
this fix removes the unnecessary double_lock.."

Workload is SpecJBB on 16-way Altix.

--- 1.306/kernel/sched.c	Tue May 18 06:37:04 2004
+++ 1.307/kernel/sched.c	Tue May 18 06:37:05 2004
@@ -1685,12 +1685,20 @@
 		goto out_balanced;
 	}
 
-	/* Attempt to move tasks */
-	double_lock_balance(this_rq, busiest);
-
-	nr_moved = move_tasks(this_rq, this_cpu, busiest, imbalance, sd, idle);
+	nr_moved = 0;
+	if (busiest->nr_running > 1) {
+		/*
+		 * Attempt to move tasks. If find_busiest_group has found
+		 * an imbalance but busiest->nr_running <= 1, the group is
+		 * still unbalanced. nr_moved simply stays zero, so it is
+		 * correctly treated as an imbalance.
+		 */
+		double_lock_balance(this_rq, busiest);
+		nr_moved = move_tasks(this_rq, this_cpu, busiest,
+						imbalance, sd, idle);
+		spin_unlock(&busiest->lock);
+	}
 	spin_unlock(&this_rq->lock);
-	spin_unlock(&busiest->lock);
 
 	if (!nr_moved) {
 		sd->nr_balance_failed++;

