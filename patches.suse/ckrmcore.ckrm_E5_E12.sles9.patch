diff -Nru --exclude='*SCCS*' a/kernel/ckrm/ckrm.c b/kernel/ckrm/ckrm.c
--- a/kernel/ckrm/ckrm.c	2004-04-26 12:35:37.000000000 -0400
+++ b/kernel/ckrm/ckrm.c	2004-04-28 22:57:42.000000000 -0400
@@ -1,7 +1,7 @@
 /* ckrm.c - Class-based Kernel Resource Management (CKRM)
  *
- * Copyright (C) Hubertus Franke, IBM Corp. 2003
- *           (C) Shailabh Nagar,  IBM Corp. 2003
+ * Copyright (C) Hubertus Franke, IBM Corp. 2003, 2004
+ *           (C) Shailabh Nagar,  IBM Corp. 2003, 2004
  *           (C) Chandra Seetharaman,  IBM Corp. 2003
  *	     (C) Vivek Kashyap,	IBM Corp. 2004
  * 
@@ -30,6 +30,9 @@
  *        surrounding logic. Added task paramter for all CE callbacks.
  * 23 Mar 2004
  *        moved to referenced counted class objects and correct locking
+ * 19 Apr 2004
+ *        Integrated ckrm hooks, classtypes, ...
+ *  
  */
 
 #include <linux/config.h>
@@ -44,47 +47,16 @@
 #include <linux/list.h>
 #include <linux/spinlock.h>
 #include <linux/module.h>
-#include <linux/ckrm_ce.h>
 #include <linux/ckrm_rc.h>
+#include <linux/rcfs.h>
 #include <net/sock.h>
 #include <linux/ip.h>
 
-/*
- * callback structures for each registered resource controller.
- */
-ckrm_res_callback_t ckrm_res_ctlrs[CKRM_MAX_RES_CTLRS];
-static spinlock_t ckrm_res_ctlrs_lock = SPIN_LOCK_UNLOCKED; // protects the array
-long bit_res_ctlrs = 0; // bit set for registered resource controllers
-atomic_t nr_resusers[CKRM_MAX_RES_CTLRS]; // no of users for each res ctlr
-static int max_resid = 0; // highest resource id currently used.
-
-
-EXPORT_SYMBOL(ckrm_res_ctlrs);
-EXPORT_SYMBOL(nr_resusers);
-
-/*
- * Callback registered by the classification engine.
- * The chances of two engines trying to register/unregister at the same
- * moment is very less. So, it is ok to not have any locking mechanism
- * protecting ckrm_eng_callbacks data structure.
- */
-static ckrm_eng_callback_t ckrm_eng_callbacks;
-static atomic_t nr_engusers; // Number of users using the data structure above.
-// global variables
-static int ckrm_num_classes = 2; // no need to hold lock for reading
-static LIST_HEAD(ckrm_classes); // hold read lock for read, write lock for write
-rwlock_t ckrm_class_lock = RW_LOCK_UNLOCKED;  // protect above 3 variables
-
-// Default classes
-struct ckrm_core_class ckrm_dflt_class = { .class_type = CKRM_TASK_CLASS };
-struct ckrm_core_class ckrm_net_root   = { .class_type = CKRM_NET_CLASS };
-
-EXPORT_SYMBOL(ckrm_dflt_class);
-EXPORT_SYMBOL(ckrm_net_root);
 
-// is classification engine callback active ?
-static int callbacks_active = 0;
+rwlock_t ckrm_class_lock = RW_LOCK_UNLOCKED;  // protect classlists 
 
+struct rcfs_functions rcfs_fn ;
+EXPORT_SYMBOL(rcfs_fn);
 
 /**************************************************************************
  *                   Helper Functions                                     *
@@ -93,456 +65,85 @@
 /*
  * Return TRUE if the given core class pointer is valid.
  */
-inline unsigned int
-is_core_valid(ckrm_core_class_t *core)
-{
-	return (core && (core->magic == CKRM_CORE_MAGIC) &&
-		((core->class_type == CKRM_TASK_CLASS) || 
-		 (core->class_type == CKRM_NET_CLASS)));
-}
 
 /*
  * Return TRUE if the given resource is registered.
  */
 inline unsigned int
-is_res_regd(int resid)
+is_res_regd(struct ckrm_classtype *clstype, int resid)
 {
-	if (resid < 0 || resid > CKRM_MAX_RES_CTLRS)
-		return 0;
-	else
-		return (test_bit(resid, &bit_res_ctlrs));
+	return ( (resid>=0) && (resid < clstype->max_resid) &&
+		 test_bit(resid, &clstype->bit_res_ctlrs)
+		);
 }
 
-int 
-ckrm_resid_lookup(char *resname)
+struct ckrm_res_ctlr*
+ckrm_resctlr_lookup(struct ckrm_classtype *clstype, const char *resname)
 {
 	int resid = -1;
 	
-	//for_each_resid(resid) {
-	for (resid=0; resid < CKRM_MAX_RES_CTLRS; resid++) { 
-		if (is_res_regd(resid)) {
-			if (!strncmp(resname, ckrm_res_ctlrs[resid].res_name,
-							CKRM_MAX_RES_NAME))
-				return resid;
+	for (resid=0; resid < clstype->max_resid; resid++) { 
+		if (test_bit(resid, &clstype->bit_res_ctlrs)) {
+			struct ckrm_res_ctlr *rctrl = clstype->res_ctlrs[resid];
+			if (!strncmp(resname, rctrl->res_name,CKRM_MAX_RES_NAME))
+				return rctrl;
 		}
 	}
-	return resid ;
+	return NULL;
 }
+EXPORT_SYMBOL(ckrm_resctlr_lookup);
 
-
-/* given a classname return the class handle */
+/* given a classname return the class handle and its classtype*/
 void *
-ckrm_classobj(char *classname)
+ckrm_classobj(char *classname, int *classTypeID)
 {
-	struct ckrm_core_class *tmp, *core = NULL;
+	int i;
 
+	*classTypeID = -1;
 	if (!classname || !*classname) {
 		return NULL;
 	}
+
 	read_lock(&ckrm_class_lock);
-	list_for_each_entry(tmp, &ckrm_classes, clslist) {
-		if (!strcmp(tmp->dentry->d_name.name, classname)) {
-			core = tmp;
-			break;
+	for ( i=0 ; i<CKRM_MAX_CLASSTYPES; i++) {
+		struct ckrm_classtype *ctype = ckrm_classtypes[i];
+		struct ckrm_core_class *core;
+
+		if (ctype == NULL) 
+			continue;
+		list_for_each_entry(core, &ctype->classes, clslist) {
+			if (core->name && !strcmp(core->name, classname)) {
+				// FIXME:   should grep reference..
+				read_unlock(&ckrm_class_lock);
+				*classTypeID = ctype->typeID;
+				return core;
+			}
 		}
 	}
 	read_unlock(&ckrm_class_lock);
-	return core;
+	return NULL;
 }
 
-
-EXPORT_SYMBOL(is_core_valid);
 EXPORT_SYMBOL(is_res_regd);
-EXPORT_SYMBOL(ckrm_resid_lookup);
 EXPORT_SYMBOL(ckrm_classobj);
 
-
 /**************************************************************************
  *                   Internal Functions/macros                            *
  **************************************************************************/
 
-#define hnode_2_core(ptr) \
-		((ptr) ? container_of(ptr, struct ckrm_core_class, hnode) : NULL)
-
-#define ce_protect()      (atomic_inc(&nr_engusers))
-#define ce_release()      (atomic_dec(&nr_engusers))
-#define CE_PROTECTED(cmd) do { ce_protect();  cmd ; ce_release(); } while (0)  // wrapper for code
-
-
-// CE callback that takes one parameter and returns nothing
-#define CECB_1ARG_NORET(fn, arg)				\
-do {								\
-	if (callbacks_active && ckrm_eng_callbacks.fn) {	\
-		(*ckrm_eng_callbacks.fn)(arg);			\
-	}							\
-} while (0) 
-
-// CE callback that takes one parameter and returns, whatever the function
-// returns
-#define CECB_1ARG_RET(fn, arg, ret)				\
-do {								\
-	if (callbacks_active && ckrm_eng_callbacks.fn) {	\
-		ret = (*ckrm_eng_callbacks.fn)(arg);		\
-	}							\
-} while (0)
-
-// CE callback that takes two parameteasr and returns nothing
-#define CECB_2ARGS_NORET(fn, arg1, arg2)			\
-do {								\
-	if (callbacks_active && ckrm_eng_callbacks.fn) {	\
-		(*ckrm_eng_callbacks.fn)(arg1, arg2);		\
-	}							\
-} while (0)
-
-// CE callback that takes two parameters and returns whatever the function
-// returns
-#define CECB_2ARGS_RET(fn, arg1, arg2, ret)			\
-{								\
-	if (callbacks_active && ckrm_eng_callbacks.fn) {	\
-		ret = (*ckrm_eng_callbacks.fn)(arg1, arg2);	\
-	}							\
-}
-
-// CE callback that takes three parameters and returns nothing
-#define CECB_3ARGS_NORET(fn, arg1, arg2, arg3)			\
-do {								\
-	if (callbacks_active && ckrm_eng_callbacks.fn) {	\
-		(*ckrm_eng_callbacks.fn)(arg1, arg2, arg3);	\
-	}							\
-} while (0)
-
-
-
 static inline void 
-set_callbacks_active(void)
-{
-	callbacks_active = ((atomic_read(&nr_engusers) > 0) &&
-			    (ckrm_eng_callbacks.always_callback || (ckrm_num_classes > 1)));
-}
-
-inline void
-ckrm_core_grab(void *core)
-{
-	ckrm_core_class_t *lcore = core;
-	if (lcore) {
-		atomic_inc(&lcore->refcnt);
- 	}
-}
-
-inline void
-ckrm_core_drop(void *core)
-{
-	ckrm_core_class_t *lcore = core;
-	if (lcore) {
-		atomic_dec(&lcore->refcnt);
- 	}
-}
-
-void
-ckrm_reclassify_net(struct ckrm_net_struct *ns, ckrm_core_class_t *core, int action)
-{
-	int i;
-	ckrm_res_callback_t *rcbs;
-	ckrm_core_class_t *old_core;
-	void  *old_res_class, *new_res_class;
-
-	// remove the net_struct from the current class
-	if ((old_core = ns->core) != NULL) {
-		spin_lock(&old_core->ckrm_lock);
-		list_del(&ns->ckrm_link);
-		ns->core = NULL;
-		spin_unlock(&old_core->ckrm_lock);
-	}
-	
-	if (core != NULL) {
-		spin_lock(&core->ckrm_lock);
-		ns->core = core;
-		list_add(&ns->ckrm_link, &core->tasklist);
-		spin_unlock(&core->ckrm_lock);
-	}
-
-	if (core != old_core) {
-		for (i = 0; i < max_resid; i++) {
-			rcbs = &ckrm_res_ctlrs[i];
-			old_res_class = old_core ? 
-				old_core->res_class[i] : NULL;
-			new_res_class = core ? core->res_class[i] : NULL;
-			if (rcbs->change_resclass && 
-				(old_res_class != new_res_class)) 
-					(*rcbs->change_resclass)
-						((void *)ns,old_res_class, 
-						 	new_res_class);
-		}
-	}
-	return;
-}
-
-
-/*
- * Change the core class of the given task.
- *
- * Change the task's core class  to "core" if the task's current 
- * core(task->ckrm_core) is same as given "oldcore", if it is non-NULL.
- *
- * Caller is responsible to make sure the task structure stays put through
- * this function.
- *
- * This function should be called with the following locks not held
- * 	- task_lock
- * 	- core->ckrm_lock, if core is NULL then ckrm_dflt_class.ckrm_lock
- * 	- tsk->ckrm_core->ckrm_lock
- * 
- * Function is also called with a ckrm_core_grab on the new core, hence
- * it needs to be dropped if no assignment takes place.
- */
-
-static void
-ckrm_set_taskclass(struct task_struct *tsk, ckrm_core_class_t *core, 
-		   ckrm_core_class_t *oldcore, int action)
-{
-	int i;
-	ckrm_res_callback_t *rcbs;
-	ckrm_core_class_t *curr_core;
-	void *old_res_class, *new_res_class;
-	int drop_old_core;
-
-
-	task_lock(tsk);
-	curr_core = tsk->ckrm_core;
-
-	// check whether compare_and_exchange should
-	if (oldcore && (oldcore != curr_core)) {
-		task_unlock(tsk);
-		if (core) {
-			/* compensate for previous grab */
-			// printk("ckrm_set_taskclass(%s:%d): Race-condition caught <%s>\n",
-			//	tsk->comm,tsk->pid,core->name);
-			ckrm_core_drop(core);
-		}
-		return;
-	}
-
-	// make sure we have a real destination core
-	if (!core) {
-		core = &ckrm_dflt_class;
-		ckrm_core_grab(core);
-	}
-
-	// take out of old class 
-	// remember that we need to drop the oldcore
-	if ((drop_old_core = (curr_core != NULL))) {
-		spin_lock(&curr_core->ckrm_lock);
-		if (core == curr_core) {
-			// we are already in the destination class.
-			// we still need to drop oldcore
-			spin_unlock(&curr_core->ckrm_lock);
-			task_unlock(tsk);
-			goto out;
-		}
-		list_del(&tsk->ckrm_link);
-		INIT_LIST_HEAD(&tsk->ckrm_link);
-		tsk->ckrm_core = NULL;
-		spin_unlock(&curr_core->ckrm_lock);
-	}	
-
-	// put into new class 
-	spin_lock(&core->ckrm_lock);
-	tsk->ckrm_core = core;
-	list_add(&tsk->ckrm_link, &core->tasklist);
-	spin_unlock(&core->ckrm_lock);
-
-	if (core == curr_core) {
-		task_unlock(tsk);
-		goto out;
-	}
-
-	CECB_3ARGS_NORET(notify, tsk, core, action);
-
-	task_unlock(tsk);
-
-	for (i = 0; i < max_resid; i++) {
-		atomic_inc(&nr_resusers[i]);
-		rcbs = &ckrm_res_ctlrs[i];
-		old_res_class = curr_core ? curr_core->res_class[i] : NULL;
-		new_res_class = core ? core->res_class[i] : NULL;
-		if (rcbs && rcbs->change_resclass && (old_res_class != new_res_class)) 
-			(*rcbs->change_resclass)(tsk, old_res_class, new_res_class);
-		atomic_dec(&nr_resusers[i]);
-	}
-
- out:
-	if (drop_old_core) 
-		ckrm_core_drop(curr_core);
-	return;
-}
-
-/**************************************************************************
- *                   Functions called from classification points          *
- **************************************************************************/
-
-#define TASK_MAGIC 0xDEADB0DE
-inline void
-ckrm_new_task(struct task_struct *tsk)
-{
-	tsk->ckrm_core = NULL;
-	tsk->ce_data   = NULL;
-	INIT_LIST_HEAD(&tsk->ckrm_link);
-	return;
-}
-
-
-#define CKRM_CB_TASK(function, tsk, action)					\
-do {										\
-	struct ckrm_core_class *core = NULL, *old_core = tsk->ckrm_core;	\
-										\
-	CECB_1ARG_RET(function, tsk, core);					\
-	if (core) {								\
-		/* called synchrously. no need to get task struct */		\
-		ckrm_set_taskclass(tsk, core, old_core, action);		\
-	}									\
-} while (0)
-
-
-#define CKRM_CB_NET(function, tsk, action)	\
-do {						\
-	struct ckrm_core_class *core = NULL;	\
-						\
-	CECB_1ARG_RET(function, tsk, core);	\
-	if (core && (core != tsk->core)) {	\
-		ckrm_reclassify_net(tsk, core, action);	\
-	}					\
-} while (0)
-
-void
-ckrm_cb_exec(const char *filename)
-{
-	struct ckrm_core_class *core = NULL, *old_core = current->ckrm_core;
-
-	ce_protect();
-	CECB_2ARGS_RET(exec, current, filename, core);
-
-	// called synchrously. no need to get task struct
-	ckrm_set_taskclass(current, core, old_core, CKRM_ACTION_EXEC);
-	ce_release();
-	return;
-}
-
-void
-ckrm_cb_fork(struct task_struct *tsk)
-{
-	struct ckrm_core_class *core = NULL;
-
-	ce_protect();
-	CECB_1ARG_RET(fork, tsk, core);
-	if (core == NULL) {
-		task_lock(tsk->parent);
-		core = tsk->parent->ckrm_core;
-		ckrm_core_grab(core);
-		task_unlock(tsk->parent);
-	}
-	if (!list_empty(&tsk->ckrm_link))
-		printk("BUG in cb_fork.. tsk (%s:%d> already linked\n",
-			tsk->comm,tsk->pid);
-
-	// called synchrously. no need to get task struct
-	ckrm_set_taskclass(tsk, core, NULL, CKRM_ACTION_FORK);
-	ce_release();
-	return;
-}
-
-void
-ckrm_cb_listen(struct sock *sk)
-{
-	struct ckrm_net_struct *ns;
-	struct ckrm_core_class *core;
-
-	ns = (struct ckrm_net_struct *)
-		kmalloc(sizeof(struct ckrm_net_struct), GFP_KERNEL);
-	if (!ns)
-		return;
-
-	ns->family = sk->sk_family;
-	ns->daddr4 = inet_sk(sk)->daddr;
-	ns->dport = inet_sk(sk)->dport;
-
-	CE_PROTECTED ( CKRM_CB_NET(listen, ns, CKRM_ACTION_LISTEN) );
-
-	return ;        /* Hubertus (3/26):  <core> is never set */
-
-	// FIXME: where is this core coming from ?
-	// core in _CB4 is in different context
-	// what if core is NULL ?
-	spin_lock(&core->ckrm_lock);
-	read_lock(&tasklist_lock);
-	ns->core = core;
-	printk("ckrm_cb_listen: adding %p to tasklist of %s\n", ns, core->name);
-	list_add(&ns->ckrm_link, &core->tasklist);
-	read_unlock(&tasklist_lock);
-	spin_unlock(&core->ckrm_lock);	
-}
-
-void
-ckrm_cb_exit(struct task_struct *tsk)
-{
-	ckrm_core_class_t *core;
-
-	// Remove the task from the current core class
-	
-
-	task_lock(tsk);
-
-	CE_PROTECTED ( CECB_1ARG_NORET(exit, tsk) );
-
-	if ((core = tsk->ckrm_core) != NULL) {
-		spin_lock(&core->ckrm_lock);
-		tsk->ckrm_core = NULL;
-		tsk->ce_data = NULL;
-		list_del(&tsk->ckrm_link);
-		ckrm_core_drop(core);
-		spin_unlock(&core->ckrm_lock);
-	} else {
-		tsk->ce_data = NULL;
-		INIT_LIST_HEAD(&tsk->ckrm_link);
-	}
-	task_unlock(tsk);
-
-	return;
-}
-
-void
-ckrm_cb_uid(void)
-{
-	CE_PROTECTED ( CKRM_CB_TASK(uid, current, CKRM_ACTION_UID) );
-}
-
-void
-ckrm_cb_gid(void)
+set_callbacks_active(struct ckrm_classtype *ctype)
 {
-	CE_PROTECTED ( CKRM_CB_TASK(gid, current, CKRM_ACTION_GID) );
+	ctype->ce_cb_active = ((atomic_read(&ctype->ce_nr_users) > 0) &&
+			       (ctype->ce_callbacks.always_callback || (ctype->num_classes > 1)));
 }
 
-
-/***********************************************************************
- *
- * Asynchronous callback functions   (driven by RCFS)
- * 
- *    Async functions force a setting of the task structure
- *    synchronous callbacks are protected against race conditions 
- *    by using a cmpxchg on the core before setting it.
- *    Async calls need to be serialized to ensure they can't 
- *    race against each other 
- *
- ***********************************************************************/
-
-DECLARE_MUTEX(async_serializer);    // serialize all async functions
-
-
-static inline int
-validate_and_grab_core(struct ckrm_core_class *core)
+int
+ckrm_validate_and_grab_core(struct ckrm_core_class *core)
 {
 	int rc = 0;
 	read_lock(&ckrm_class_lock);
-	if (likely(is_core_valid(core))) {
+	if (likely(ckrm_is_core_valid(core))) {
 		ckrm_core_grab(core);
 		rc = 1;
 	}
@@ -550,240 +151,6 @@
 	return rc;
 }
 
-/*
- * Go through the task list and reclassify all tasks according to the current
- * classification rules.
- *
- * We have the problem that we can not hold any lock (including the 
- * tasklist_lock) while classifying. Two methods possible
- *
- * (a) go through entire pidrange (0..pidmax) and if a task exists at 
- *     that pid then reclassify it
- * (b) go several time through task list and build a bitmap for a particular 
- *     subrange of pid otherwise the memory requirements ight be too much.
- * 
- * We use a hybrid by comparing ratio nr_threads/pidmax
- */
-
-static void
-ckrm_reclassify_all_tasks(void)
-{
-	extern int pid_max;
-
-	struct task_struct *proc, *thread;
-	int i;
-	int curpidmax = pid_max;
-	int ratio;
-	int use_bitmap;
-
-
-	ratio = curpidmax / nr_threads;
-	if (curpidmax <= PID_MAX_DEFAULT) {
-	     use_bitmap = 1;
-	} else {
-	     use_bitmap = (ratio >= 2);
-	}
-
-	ce_protect();
-
- retry:		
-	if (use_bitmap == 0) {
-		// go through it in one walk
-		read_lock(&tasklist_lock);
-		for ( i=0 ; i<curpidmax ; i++ ) {
-			if ((thread = find_task_by_pid(i)) == NULL) 
-				continue;
-			get_task_struct(thread);
-			read_unlock(&tasklist_lock);
-			CKRM_CB_TASK(reclassify, thread, CKRM_ACTION_RECLASSIFY);
-			put_task_struct(thread);
-			read_lock(&tasklist_lock);
-		}
-		read_unlock(&tasklist_lock);
-	} else {
-		unsigned long *bitmap;
-		int bitmapsize;
-		int order = 0;
-		int num_loops;
-		int pid;
-
-
-		bitmap = (unsigned long*) __get_free_pages(GFP_KERNEL,order);
-		if (bitmap == NULL) {
-			ratio = 0;
-			goto retry;
-		}
-
-		bitmapsize = 8 * (1 << (order + PAGE_SHIFT));
-		num_loops  = (curpidmax + bitmapsize - 1) >> order;
-
-		for ( i=0 ; i < num_loops; i++) {
-			int pid_start = i*bitmapsize; 
-			int pid_end   = pid_start + bitmapsize;
-			int num_found = 0;
-			int pos;
-
-			memset(bitmap, 0, bitmapsize/8); // start afresh
-
-			read_lock(&tasklist_lock);
-			do_each_thread(proc, thread) {
-				pid = thread->pid;
-				if ((pid < pid_start) || (pid >= pid_end))
-					continue;
-				pid -= pid_start;
-				set_bit(pid, bitmap);
-				num_found++;
-			} while_each_thread(proc, thread);
-			read_unlock(&tasklist_lock);
-		
-			if (num_found == 0) 
-				continue;
-
-			pos = 0;
-			for ( ; num_found-- ; ) {
-				pos = find_next_bit(bitmap, bitmapsize, pos);
-				pid = pos + pid_start;
-
-				read_lock(&tasklist_lock);
-				if ((thread = find_task_by_pid(pid)) != NULL) {
-					get_task_struct(thread);
-					read_unlock(&tasklist_lock);
-					CKRM_CB_TASK(reclassify, thread, CKRM_ACTION_RECLASSIFY);
-					put_task_struct(thread);
-				} else {
-					read_unlock(&tasklist_lock);
-				}
-			}
-		}
-
-	}
-	ce_release();
-}
-
-int
-ckrm_reclassify(int pid)
-{
-	struct task_struct *tsk;
-	int rc = 0;
-
-	down(&async_serializer);   // protect again race condition
-	if (pid < 0) {
-		// do we want to treat this as process group .. should YES ToDo
-		 rc = -EINVAL;
-	} else if (pid == 0) {
-		// reclassify all tasks in the system
-		ckrm_reclassify_all_tasks();
-	} else {
-		// reclassify particular pid
-		read_lock(&tasklist_lock);
-		if ((tsk = find_task_by_pid(pid)) != NULL) {
-			get_task_struct(tsk);
-			read_unlock(&tasklist_lock);
-			CE_PROTECTED ( CKRM_CB_TASK(reclassify, tsk, CKRM_ACTION_RECLASSIFY) );
-			put_task_struct(tsk);
-		} else {
-			read_unlock(&tasklist_lock);
-			rc = -EINVAL;
-		}
-	}
-	up(&async_serializer);
-	return rc;
-}
-
-/*
- * Reclassify all tasks in the given core class.
- */
-
-static void
-ckrm_reclassify_class_tasks(struct ckrm_core_class *core)
-{
-
-	if (!validate_and_grab_core(core))
-		return;
-
-	down(&async_serializer);   // protect again race condition
-
-next_task:
-	spin_lock(&core->ckrm_lock);
-	// Reclassify all tasks in the given core class.
-	if (!list_empty(&core->tasklist)) {
-		ckrm_core_class_t *new_core = NULL;
-		struct task_struct *tsk = 
-				list_entry(core->tasklist.next ,struct task_struct, ckrm_link);
-		
-		get_task_struct(tsk);
-		spin_unlock(&core->ckrm_lock);
-
-		CECB_1ARG_RET(reclassify, tsk, new_core);
-		if (core == new_core) {
-			// don't allow reclassifying to the same class
-			// as we are in the process of cleaning up this class
-			ckrm_core_drop(new_core); // to compensate CE's grab
-			new_core = NULL;
-		}
-		ckrm_set_taskclass(tsk, new_core, core, CKRM_ACTION_RECLASSIFY);
-		put_task_struct(tsk);
-		goto next_task;
-	}
-	ckrm_core_drop(core);
-	spin_unlock(&core->ckrm_lock);
-
-	up(&async_serializer);
-
-	return ;
-}
-
-/*
- * Change the core class of the given task.
- */
-
-void
-ckrm_forced_reclassify_pid(int pid, ckrm_core_class_t *core)
-{
-	struct task_struct *tsk;
-
-	if (!validate_and_grab_core(core))
-		return;
-
-	if (core->class_type == CKRM_TASK_CLASS) {
-		read_lock(&tasklist_lock);
-		if ((tsk = find_task_by_pid(pid)) == NULL) {
-			read_unlock(&tasklist_lock);
-			return;
-		}
-		get_task_struct(tsk);
-		read_unlock(&tasklist_lock);
-
-		down(&async_serializer);   // protect again race condition
-
-		ce_protect();
-		CECB_1ARG_NORET(manual, tsk);
-
-		ckrm_set_taskclass(tsk, core, NULL, CKRM_ACTION_MANUAL);
-		put_task_struct(tsk);
-		ce_release();
-
-		up(&async_serializer);
-	}
-	return;
-}
-EXPORT_SYMBOL(ckrm_forced_reclassify_pid);
-
-/*
- * Change the core class of the given net struct
- */
-void
-ckrm_forced_reclassify_net(struct ckrm_net_struct *ns, ckrm_core_class_t *core)
-{
-	if (!is_core_valid(core))	
-		return;
-
-	if (core->class_type == CKRM_NET_CLASS) {
-		ckrm_reclassify_net(ns, core, CKRM_ACTION_MANUAL);
-	}
-	return;
-}
-
 /****************************************************************************
  *           Interfaces for classification engine                           *
  ****************************************************************************/
@@ -791,27 +158,49 @@
 /*
  * Registering a callback structure by the classification engine.
  *
- * Returns 0 on success -errno for failure.
+ * Returns typeId of class on success -errno for failure.
  */
 int
-ckrm_register_engine(ckrm_eng_callback_t *ecbs)
+ckrm_register_engine(const char *typename, ckrm_eng_callback_t *ecbs)
 {
-	ce_protect();
-	if (atomic_read(&nr_engusers) != 1) {
+	struct ckrm_classtype *ctype;
+
+	ctype = ckrm_find_classtype_by_name(typename);
+	if (ctype == NULL) 
+		return (-ENOENT);
+
+	ce_protect(ctype);
+	if (atomic_read(&ctype->ce_nr_users) != 1) {
 		// Some engine is acive, deregister it first.
-		ce_release();
+		ce_release(ctype);
 		return (-EBUSY);
 	}
 	
-	if (strnlen(ecbs->ckrm_eng_name, CKRM_MAX_ENG_NAME) == 0 ||
-	    ecbs->reclassify == NULL) {
-		ce_release();
+	/* we require that either classify and class_delete are set (due to object reference)
+	 * or that notify is set (in case no real classification is supported only notification
+	 * also require that the function pointer be set the momement the mask is non-null
+	 */
+	if ( ! (((ecbs->classify) && (ecbs->class_delete)) || (ecbs->notify)) ||
+	     (ecbs->c_interest && ecbs->classify == NULL) ||
+	     (ecbs->n_interest && ecbs->notify == NULL) )
+	{
+		ce_release(ctype);
 		return (-EINVAL);
 	}
+	
 
-	ckrm_eng_callbacks = *ecbs;
-	set_callbacks_active();
-	return 0;
+	/* Is any other engine registered for this classtype ? */
+	if (ctype->ce_regd) {
+		ce_release(ctype);
+		return (-EINVAL);
+	}
+	
+	ctype->ce_regd = 1;
+	ctype->ce_callbacks = *ecbs;
+	set_callbacks_active(ctype);
+	if (ctype->ce_callbacks.class_add) 
+		(*ctype->ce_callbacks.class_add)(ctype->default_class->name,ctype->default_class);
+	return ctype->typeID;
 }
 
 /*
@@ -820,70 +209,31 @@
  * Returns 0 on success -errno for failure.
  */
 int
-ckrm_unregister_engine(ckrm_eng_callback_t *ecbs)
+ckrm_unregister_engine(const char *typename)
 {
-	callbacks_active = 0; 
+	struct ckrm_classtype *ctype;
+
+	ctype = ckrm_find_classtype_by_name(typename);
+	if (ctype == NULL) 
+		return (-ENOENT);
+
+	ctype->ce_cb_active = 0; 
 
-	if (atomic_dec_and_test(&nr_engusers) != 1) {
+	if (atomic_dec_and_test(&ctype->ce_nr_users) != 1) {
 		// Somebody is currently using the engine, cannot deregister.
-		atomic_inc(&nr_engusers);
+		atomic_inc(&ctype->ce_nr_users);
 		return (-EBUSY);
 	}
 
-	if (strncmp(ckrm_eng_callbacks.ckrm_eng_name, ecbs->ckrm_eng_name,
-				CKRM_MAX_ENG_NAME) != 0) {
-		atomic_inc(&nr_engusers);
-		// Somebody other than the owner is trying to unregister.
-		return (-EINVAL);
-	}
-	memset(&ckrm_eng_callbacks, 0, sizeof(ckrm_eng_callbacks));
-	set_callbacks_active();
+	ctype->ce_regd = 0;
+	memset(&ctype->ce_callbacks, 0, sizeof(ckrm_eng_callback_t));
 	return 0;
 }
 
-/*
- * functions to manipulate class (core or resource) hierarchies 
- */
-
-#ifndef NEW_HNODE_IMPLMN
-/*
- * functions to manipulate class (core or resource) hierarchies 
- */
-
-/* Caller must ensure ckrm_class_lock held */
-void
-ckrm_hnode_add(struct ckrm_hnode *node, struct ckrm_hnode *parent)
-{
-	node->parent = parent;
-	INIT_LIST_HEAD(&node->children);
-	INIT_LIST_HEAD(&node->siblings);
-
- 	if (parent) {
-		if (!is_core_valid(hnode_2_core(parent)))
-			printk(KERN_ERR "hnode_add: non-NULL invalid parent\n");
-		else {
-			if (&parent->children)
-				list_add(&node->siblings, &parent->children);
-			else
-				printk(KERN_ERR "hnode_add: parent->children not initialized\n");
-		}
-	}
-	
-}
-
-/* Caller must ensure ckrm_class_lock held */
-int
-ckrm_hnode_remove(struct ckrm_hnode *node)
-{
-	/* ensure that the node does not have children */
-	if (!list_empty(&node->children))
-		return 0;
-	list_del(&node->siblings);
-	node->parent = NULL;
-	return 1;
-}
+/****************************************************************************
+ *           Interfaces to manipulate class (core or resource) hierarchies 
+ ****************************************************************************/
 
-#else
 /* 
  */
 static void
@@ -891,7 +241,7 @@
 {
 	struct ckrm_hnode *cnode = &child->hnode;
 
-	if (!is_core_valid(child)) {
+	if (!ckrm_is_core_valid(child)) {
 		printk(KERN_ERR "Invalid child %p given in ckrm_add_child\n", child);
 		return;
 	}
@@ -903,7 +253,7 @@
  	if (parent) {
 		struct ckrm_hnode *pnode;
 
-		if (!is_core_valid(parent)) {
+		if (!ckrm_is_core_valid(parent)) {
 			printk(KERN_ERR "Invalid parent %p given in ckrm_add_child\n",
 					parent);
 			parent = NULL;
@@ -927,26 +277,20 @@
 	struct ckrm_hnode *cnode, *pnode;
 	struct ckrm_core_class *parent;
 
-	if (!is_core_valid(child)) {
+	if (!ckrm_is_core_valid(child)) {
 		printk(KERN_ERR "Invalid child %p given in ckrm_remove_child\n", child);
 		return 0;
 	}
 
 	cnode = &child->hnode;
 	parent = cnode->parent;
-	if (!is_core_valid(parent)) {
+	if (!ckrm_is_core_valid(parent)) {
 		printk(KERN_ERR "Invalid parent %p in ckrm_remove_child\n", parent);
 		return 0;
 	}
 
 	pnode = &parent->hnode;
 
-	if (cnode->parent != parent) {
-		printk(KERN_ERR "Invalid parent %p in child %p in ckrm_remove_child\n",
-				parent, child);
-		return 0;
-	}
-
 	spin_lock(&child->ckrm_lock);
 	/* ensure that the node does not have children */
 	if (!list_empty(&cnode->children)) {
@@ -964,7 +308,7 @@
 void
 ckrm_lock_hier(struct ckrm_core_class *parent)
 {
-	if (is_core_valid(parent)) {
+	if (ckrm_is_core_valid(parent)) {
 		read_lock(&parent->hnode_rwlock);
 	}
 }
@@ -972,7 +316,7 @@
 void 
 ckrm_unlock_hier(struct ckrm_core_class *parent)
 {
-	if (is_core_valid(parent)) {
+	if (ckrm_is_core_valid(parent)) {
 		read_unlock(&parent->hnode_rwlock);
 	}
 }
@@ -982,14 +326,17 @@
  * external callers should 've called ckrm_lock_hier before calling this
  * function.
  */
+#define hnode_2_core(ptr) ((ptr) ? container_of(ptr, struct ckrm_core_class, hnode) : NULL)
+
 struct ckrm_core_class *
 ckrm_get_next_child(struct ckrm_core_class *parent,
 			struct ckrm_core_class *child)
 {
+	struct list_head *cnode;
 	struct ckrm_hnode *next_cnode;
 	struct ckrm_core_class *next_childcore;
 
-	if (!is_core_valid(parent)) {
+	if (!ckrm_is_core_valid(parent)) {
 		printk(KERN_ERR "Invalid parent %p in ckrm_get_next_child\n", parent);
 		return NULL;
 	}
@@ -998,21 +345,23 @@
 	}
 
 	if (child) {
-		if (!is_core_valid(child)) {
+		if (!ckrm_is_core_valid(child)) {
 			printk(KERN_ERR "Invalid child %p in ckrm_get_next_child\n", child);
 			return NULL;
 		}
-		next_cnode = (struct ckrm_hnode *) child->hnode.siblings.next;
+		cnode = child->hnode.siblings.next;
 	} else {
-		next_cnode = (struct ckrm_hnode *) parent->hnode.children.next;
+		cnode = parent->hnode.children.next;
 	}
-	next_childcore = hnode_2_core(next_cnode);
 
-	if (next_childcore == parent) { // back at the anchor
+	if (cnode == &parent->hnode.children) { // back at the anchor
 		return NULL;
 	}
 
-	if (!is_core_valid(next_childcore)) {
+	next_cnode = container_of(cnode, struct ckrm_hnode, siblings);
+	next_childcore = hnode_2_core(next_cnode);
+
+	if (!ckrm_is_core_valid(next_childcore)) {
 		printk(KERN_ERR "Invalid next child %p in ckrm_get_next_child\n",
 				next_childcore);
 		return NULL;
@@ -1020,101 +369,105 @@
 	return next_childcore;
 }
 
-#endif // NEW_HNODE_IMPLMN
+EXPORT_SYMBOL(ckrm_lock_hier);
+EXPORT_SYMBOL(ckrm_unlock_hier);
+EXPORT_SYMBOL(ckrm_get_next_child);
 
 static void 
 ckrm_alloc_res_class(struct ckrm_core_class *core,
-				 struct ckrm_core_class *parent,
-				 int resid)
+		     struct ckrm_core_class *parent,
+		     int resid)
 {
+
+	struct ckrm_classtype *clstype;
+
 	/* 
 	 * Allocate a resource class only if the resource controller has
 	 * registered with core and the engine requests for the class.
 	 */
 
-	if (!is_core_valid(core))
+	if (!ckrm_is_core_valid(core))
 		return ; 
 
-	
+	clstype = core->classtype;
 	core->res_class[resid] = NULL;
 
-	if (test_bit(resid, &bit_res_ctlrs)) {
-		ckrm_res_callback_t *rcbs;
+	if (test_bit(resid, &clstype->bit_res_ctlrs)) {
+		ckrm_res_ctlr_t *rcbs;
 
-		atomic_inc(&nr_resusers[resid]);
-		rcbs = &ckrm_res_ctlrs[resid];
+		atomic_inc(&clstype->nr_resusers[resid]);
+		rcbs = clstype->res_ctlrs[resid];
 		
 		if (rcbs && rcbs->res_alloc) {
 			core->res_class[resid] =(*rcbs->res_alloc)(core,parent);
-			if (!core->res_class[resid]) {
-				printk(KERN_ERR "Error creating res class\n");
-				atomic_dec(&nr_resusers[resid]);
-			}
-		} else {
-			atomic_dec(&nr_resusers[resid]);
+			if (core->res_class[resid])
+				return;
+			printk(KERN_ERR "Error creating res class\n");
 		}
+		atomic_dec(&clstype->nr_resusers[resid]);
 	}
 }
 
 /*
- * Allocate a core class, which in turn allocates resource classes as
- * specified by the res_mask parameter.
+ * Initialize a core class
  *
- * Return the handle to the core class on success, NULL on failure.
  */
 
-
-struct ckrm_core_class *
-ckrm_alloc_core_class(struct ckrm_core_class *parent, struct dentry *dentry)
+int
+ckrm_init_core_class(struct ckrm_classtype  *clstype,
+		     struct ckrm_core_class *dcore,
+		     struct ckrm_core_class *parent,
+		     const char *name)
 {
+	// Hubertus   ... should replace name with dentry or add dentry ?
 	int i;
-	ckrm_core_class_t *dcore;
 
-	if (!is_core_valid(parent))
-		return NULL; 
+	// Hubertus .. how is this used in initialization 
+
+	printk("ckrm_init_core_class: name %s => %p\n", name?name:"default",dcore);
 	
-	dcore = kmalloc(sizeof(ckrm_core_class_t), GFP_KERNEL);
-	if (dcore == NULL) 
-		return NULL;
+	if ((dcore != clstype->default_class) && ( !ckrm_is_core_valid(parent))) {
+		printk("error not a valid parent %p\n", parent);
+		return -EINVAL;
+	}
+#if 0  // Hubertus .. dynamic allocation still breaks when RCs registers. See def in ckrm_rc.h
+	dcore->res_class = NULL;
+	if (clstype->max_resid > 0) {
+		dcore->res_class = (void**)kmalloc(clstype->max_resid * sizeof(void*) , GFP_KERNEL);
+		if (dcore->res_class == NULL) {
+			printk("error no mem\n");
+			return -ENOMEM;
+		}
+	}
+#endif
 
-	dcore->magic = CKRM_CORE_MAGIC;
-	dcore->dentry = dentry;
-	INIT_LIST_HEAD(&dcore->tasklist);
-	dcore->ckrm_lock = SPIN_LOCK_UNLOCKED;
+	dcore->classtype    = clstype;
+	dcore->magic        = CKRM_CORE_MAGIC;
+	dcore->name         = name;
+	dcore->ckrm_lock    = SPIN_LOCK_UNLOCKED;
+	dcore->hnode_rwlock = RW_LOCK_UNLOCKED;
 
 	atomic_set(&dcore->refcnt, 0);
-	memset(dcore->name, 0, 16);
-	strcpy(dcore->name, dentry->d_name.name);
 	write_lock(&ckrm_class_lock);
 
-	list_add(&dcore->clslist,&ckrm_classes);
-#ifndef NEW_HNODE_IMPLMN
-	ckrm_hnode_add(&dcore->hnode, parent ? &parent->hnode : NULL); 
-	ckrm_num_classes++;
-	set_callbacks_active();
-	write_unlock(&ckrm_class_lock);
+	INIT_LIST_HEAD(&dcore->objlist);
+	list_add(&dcore->clslist,&clstype->classes);
 
-#else
-	ckrm_num_classes++;
-	set_callbacks_active();
-	write_unlock(&ckrm_class_lock);
+	clstype->num_classes++;
+	set_callbacks_active(clstype);
 
+	write_unlock(&ckrm_class_lock);
 	ckrm_add_child(parent, dcore); 
-#endif
 
+	for (i = 0; i < clstype->max_resid; i++) 
+		ckrm_alloc_res_class(dcore,parent,i);
 
-	dcore->class_type = parent->class_type;
-	for (i = 0; i < CKRM_MAX_RES_CTLRS; i++) {
-		if (dcore->class_type == ckrm_res_ctlrs[i].res_type)
-			ckrm_alloc_res_class(dcore,parent,i);
-		else
-			dcore->res_class[i] = NULL;
-	}
-
+	// fix for race condition seen in stress with numtasks
+	if (parent) 
+		ckrm_core_grab(parent);
 
-	/* Inform CE at last, once core is ready for use */
-	CE_PROTECTED ( CECB_2ARGS_NORET(class_add, dcore->dentry->d_name.name, (void *)dcore) );
-	return dcore;
+	ckrm_core_grab( dcore );
+	return 0;
 }
 
 
@@ -1127,17 +480,18 @@
 	 */
 
 	if (core->res_class[resid]) {
-		ckrm_res_callback_t *rcbs;
+		ckrm_res_ctlr_t *rcbs;
+		struct ckrm_classtype *clstype = core->classtype;
 
-		atomic_inc(&nr_resusers[resid]);
-		rcbs = &ckrm_res_ctlrs[resid];
+		atomic_inc(&clstype->nr_resusers[resid]);
+		rcbs = clstype->res_ctlrs[resid];
 
 		if (rcbs->res_free) {
 			(*rcbs->res_free)(core->res_class[resid]);
-			atomic_dec(&nr_resusers[resid]); // for inc in alloc
+			atomic_dec(&clstype->nr_resusers[resid]); // for inc in alloc
 			core->res_class[resid] = NULL;	
 		}
-		atomic_dec(&nr_resusers[resid]);
+		atomic_dec(&clstype->nr_resusers[resid]);
 	}
 }
 
@@ -1148,54 +502,24 @@
  *
  * Returns 0 on success -errno on failure.
  */
-int
+
+void
 ckrm_free_core_class(struct ckrm_core_class *core)
 {
 	int i;
-	int retry_count = 10;
-
-	if (core == &ckrm_dflt_class) {
-		// cannot remove the default class
-		return -EINVAL;
-	}
-
-	if (!is_core_valid(core)) {
-		// Invalid core
-		return (-EINVAL);
-	}
-
-	/* Inform CE first, in case it needs any core class data */
-	CE_PROTECTED ( CECB_2ARGS_NORET(class_delete, core->dentry->d_name.name, (void *)core) );
-
-retry_class:
-	ckrm_reclassify_class_tasks(core);
-
-	if (atomic_read(&core->refcnt) > 0) {
-		// at least try one more time
-		printk("Core class <%s> removal failed. refcount non-zero %d  retries=%d\n",
-		       core->name,atomic_read(&core->refcnt),retry_count-1);
-		// ckrm_debug_free_core_class(core);
-		if (--retry_count) { 
-			schedule_timeout(100);
-			goto retry_class;
-		}
-		
-		return -EBUSY;
-	}
-
-#ifndef NEW_HNODE_IMPLMN
-	if (ckrm_hnode_remove(&core->hnode) == 0) {
-		printk("Core class removal failed. Chilren present\n");
-		return -EBUSY;
+	struct ckrm_classtype *clstype = core->classtype;
+	struct ckrm_core_class *parent = core->hnode.parent;
+	
+	printk("%s: core=%p:%s parent=%p:%s\n",__FUNCTION__,core,core->name,parent,parent->name);
+	if (core->magic == 0) {
+		/* this core was marked as late */
+		printk("class <%s> finally deleted %lu\n",core->name,jiffies);
 	}
-#else
 	if (ckrm_remove_child(core) == 0) {
 		printk("Core class removal failed. Chilren present\n");
-		return -EBUSY;
 	}
-#endif
 
-	for (i = 0; i < max_resid; i++) {
+	for (i = 0; i < clstype->max_resid; i++) {
 		ckrm_free_res_class(core,i);
 	}
 
@@ -1203,22 +527,42 @@
 
 	// Clear the magic, so we would know if this core is reused.
 	core->magic = 0;
-
+#if 0 // Dynamic not yet enabled
+	core->res_class = NULL;
+#endif
 	// Remove this core class from its linked list.
 	list_del(&core->clslist);
-	ckrm_num_classes--;
-	set_callbacks_active();
-
+	clstype->num_classes--;
+	set_callbacks_active(clstype);
 	write_unlock(&ckrm_class_lock);
 
+	// fix for race condition seen in stress with numtasks
+	if (parent) 
+		ckrm_core_drop(parent);
+ 
 	kfree(core);
-	return 0;
 }
 
-#ifndef NEW_HNODE_IMPLMN
-EXPORT_SYMBOL(ckrm_hnode_add);
-EXPORT_SYMBOL(ckrm_hnode_remove);
-#endif
+int
+ckrm_release_core_class(struct ckrm_core_class *core)
+{
+	if (!ckrm_is_core_valid(core)) {
+		// Invalid core
+		return (-EINVAL);
+	}
+
+	if (core == core->classtype->default_class)
+ 		return 0;
+
+	/* need to make sure that the classgot really dropped */
+	if (atomic_read(&core->refcnt) != 1) {
+		printk("class <%s> deletion delayed refcnt=%d jif=%ld\n",
+		       core->name,core->refcnt,jiffies);
+		core->magic = 0;  /* just so we have a ref point */
+	}
+	ckrm_core_drop(core);
+	return 0;
+}
 
 /****************************************************************************
  *           Interfaces for the resource controller                         *
@@ -1229,89 +573,73 @@
  * Returns the resource id(0 or +ve) on success, -errno for failure.
  */
 static int
-ckrm_register_res_ctlr_intern(ckrm_res_callback_t *rcbs)
+ckrm_register_res_ctlr_intern(struct ckrm_classtype *clstype, ckrm_res_ctlr_t *rcbs)
 {
-	int resid, ret, i;
+	int  resid, ret,i;
 	
 	if (!rcbs)
 		return -EINVAL;
 
 	resid = rcbs->resid;
-	/*
-	if (strnlen(rcbs->res_name, CKRM_MAX_RES_NAME) == 0 ||
-	    rcbs->res_alloc        == NULL ||
-	    rcbs->res_free         == NULL ||
-	    rcbs->set_share_values == NULL ||
-	    rcbs->get_share_values == NULL ||
-	    rcbs->get_stats        == NULL ||
-	    resid >= CKRM_MAX_RES_CTLRS ) 
-	{
-		// Name and the above functions are mandatory.
-		return (-EINVAL);
-	}
-
-	*/
-	spin_lock(&ckrm_res_ctlrs_lock);
-
-//	ckrm_res_ctlrs[resid] = *rcbs;
-	printk(KERN_WARNING "resid is %d name is %s %s\n", resid, rcbs->res_name,ckrm_res_ctlrs[resid].res_name);
-//	atomic_set(&nr_resusers[resid], 0);
-//	set_bit(resid, &bit_res_ctlrs);	
-//	spin_unlock(&ckrm_res_ctlrs_lock);
-//	return resid;
+	
+	spin_lock(&clstype->res_ctlrs_lock);
+	
+	printk(KERN_WARNING "resid is %d name is %s %s\n", 
+	       resid, rcbs->res_name,clstype->res_ctlrs[resid]->res_name);
 
 	if (resid >= 0) {
-		if (strnlen(ckrm_res_ctlrs[resid].res_name,CKRM_MAX_RES_NAME) == 0) {
-			ckrm_res_ctlrs[resid] = *rcbs;
-			atomic_set(&nr_resusers[resid], 0);
-			set_bit(resid, &bit_res_ctlrs);	
+		if ((resid < CKRM_MAX_RES_CTLRS) && (clstype->res_ctlrs[resid] == NULL)) {
+			clstype->res_ctlrs[resid] = rcbs;
+			atomic_set(&clstype->nr_resusers[resid], 0);
+			set_bit(resid, &clstype->bit_res_ctlrs);	
 			ret = resid;
-			if (resid >= max_resid) {
-				max_resid = resid + 1;
+			if (resid >= clstype->max_resid) {
+				clstype->max_resid = resid + 1;
 			}
 		} else {
 			ret = -EBUSY;
 		}
-		spin_unlock(&ckrm_res_ctlrs_lock);
+		spin_unlock(&clstype->res_ctlrs_lock);
 		return ret;
 	}
 
-	for (i = CKRM_RES_MAX_RSVD; i < CKRM_MAX_RES_CTLRS; i++) {
-		if (strnlen(ckrm_res_ctlrs[i].res_name, CKRM_MAX_RES_NAME) == 0) {
-			ckrm_res_ctlrs[i] = *rcbs;
-			atomic_set(&nr_resusers[i], 0);
-			set_bit(i, &bit_res_ctlrs);	
-			if (i >= max_resid) {
-				max_resid = i + 1;
+	for (i = clstype->resid_reserved; i < clstype->max_res_ctlrs; i++) {
+		if (clstype->res_ctlrs[i] == NULL) {
+			clstype->res_ctlrs[i] = rcbs;
+			rcbs->resid = i;
+			atomic_set(&clstype->nr_resusers[i], 0);
+			set_bit(i, &clstype->bit_res_ctlrs);	
+			if (i >= clstype->max_resid) {
+				clstype->max_resid = i + 1;
 			}
-			spin_unlock(&ckrm_res_ctlrs_lock);
+			spin_unlock(&clstype->res_ctlrs_lock);
 			return i;
 		}
 	}
-	spin_unlock(&ckrm_res_ctlrs_lock);
+	
+	spin_unlock(&clstype->res_ctlrs_lock);
 	return (-ENOMEM);
-
 }
 
 int
-ckrm_register_res_ctlr(ckrm_res_callback_t *rcbs)
+ckrm_register_res_ctlr(struct ckrm_classtype *clstype, ckrm_res_ctlr_t *rcbs)
 {
 	struct ckrm_core_class *core;
 	int resid;
-
-	resid = ckrm_register_res_ctlr_intern(rcbs);
+	
+	resid = ckrm_register_res_ctlr_intern(clstype,rcbs);
 	
 	if (resid >= 0) {
-		/* run through all classes and create the resource class object */
-		
+		/* run through all classes and create the resource class object and
+		 * if necessary "initialize" class in context of this resource 
+		 */
 		read_lock(&ckrm_class_lock);
-		list_for_each_entry(core, &ckrm_classes, clslist) {
-			printk(KERN_ERR "CKRM .. create res clsobj for <%s>\n",rcbs->res_name);
-#ifndef NEW_HNODE_IMPLMN
-			ckrm_alloc_res_class(core, hnode_2_core(core->hnode.parent), resid);
-#else
+		list_for_each_entry(core, &clstype->classes, clslist) {
+			printk("CKRM .. create res clsobj for resouce <%s> class <%s> par=%p\n", 
+			       rcbs->res_name, core->name, core->hnode.parent);
 			ckrm_alloc_res_class(core, core->hnode.parent, resid);
-#endif
+			if (clstype->add_resctrl)  // FIXME: this should be mandatory
+				(*clstype->add_resctrl)(core,resid);
 		}
 		read_unlock(&ckrm_class_lock);
 	}
@@ -1324,104 +652,320 @@
  * Returns 0 on success -errno for failure.
  */
 int
-ckrm_unregister_res_ctlr(int resid)
-{
-	ckrm_res_callback_t *rcbs;
-//	struct ckrm_core_class *core;
+ckrm_unregister_res_ctlr(struct ckrm_res_ctlr *rcbs)
+{	
+	struct ckrm_classtype *clstype = rcbs->classtype;
+	int resid = rcbs->resid;
 
-	if (!is_res_regd(resid))
+	if ((clstype == NULL) || (resid < 0))
 		return -EINVAL;
+	
+	if (atomic_read(&clstype->nr_resusers[resid]))
+		return -EBUSY;
+	
+	// FIXME: probably need to also call deregistration function
+
+	spin_lock(&clstype->res_ctlrs_lock);
+	clstype->res_ctlrs[resid] = NULL;
+	clear_bit(resid, &clstype->bit_res_ctlrs);	
+	clstype->max_resid = fls(clstype->bit_res_ctlrs);
+	rcbs->resid = -1;
+	spin_unlock(&clstype->res_ctlrs_lock);
+	
+	return 0;
+}
+
+/*******************************************************************
+ *   Class Type Registration
+ *******************************************************************/
 
-	rcbs = &ckrm_res_ctlrs[resid];
+/* Hubertus ... we got to do some locking here */
 
-	if (atomic_read(&nr_resusers[resid])) {
+struct ckrm_classtype* ckrm_classtypes[CKRM_MAX_CLASSTYPES];
+EXPORT_SYMBOL(ckrm_classtypes);     // really should build a better interface for this
+
+int
+ckrm_register_classtype(struct ckrm_classtype *clstype)
+{
+	int tid = clstype->typeID;
+
+	if (tid != -1) {
+		if ((tid < 0) || (tid > CKRM_MAX_CLASSTYPES) || (ckrm_classtypes[tid]))
+			return -EINVAL;
+	} else {
+		int i;
+		for ( i=CKRM_RESV_CLASSTYPES ; i<CKRM_MAX_CLASSTYPES; i++) {
+			if (ckrm_classtypes[i] == NULL) {
+				tid = i;
+				break;
+			}
+		}
+	}
+	if (tid == -1) 
 		return -EBUSY;
+	clstype->typeID = tid;
+	ckrm_classtypes[tid] = clstype;
+	
+	/* Hubertus .. we need to call the callbacks of the RCFS client */
+	if (rcfs_fn.register_classtype) {
+		(* rcfs_fn.register_classtype)(clstype);
+		// No error return for now ;
 	}
 
-	spin_lock(&ckrm_res_ctlrs_lock);
-	memset(&ckrm_res_ctlrs[resid], 0, sizeof(ckrm_res_callback_t));
-	clear_bit(resid, &bit_res_ctlrs);	
-	max_resid = fls(bit_res_ctlrs);
-	spin_unlock(&ckrm_res_ctlrs_lock);
-
-#if 0
-	// FIXME: remove this part of code - chandra
-	// NOT needed, as the original design was to not allow removal of
-	// resource classes if any core class has a reference to the resource
-	// class, made sure by the atomic variable nr_resusers.
-	// Also if we want to change the design to work in this model, move this
-	// code to the top, before we clear the resource controller data
-	// structure with the memset above. - chandra
-	//
-	/* run through all classes and delete the resource class object */
-
-	list_for_each_entry(core, &ckrm_classes, clslist) {
-		printk(KERN_ERR "CKRM .. delete res clsobj for <%s>\n",rcbs->res_name);
-		ckrm_free_res_class(core,resid);
+	return tid;
+}
+
+int
+ckrm_unregister_classtype(struct ckrm_classtype *clstype)
+{
+	int tid = clstype->typeID;
+
+	if ((tid < 0) || (tid > CKRM_MAX_CLASSTYPES) || (ckrm_classtypes[tid] != clstype))
+		return -EINVAL;
+
+	if (rcfs_fn.deregister_classtype) {
+		(* rcfs_fn.deregister_classtype)(clstype);
+		// No error return for now
 	}
-#endif
+
+	ckrm_classtypes[tid] = NULL;
+	clstype->typeID = -1;
 	return 0;
 }
 
+struct ckrm_classtype*
+ckrm_find_classtype_by_name(const char *name)
+{
+	int i;
+	for ( i=0 ; i<CKRM_MAX_CLASSTYPES; i++) {
+		struct ckrm_classtype *ctype = ckrm_classtypes[i];
+		if (ctype && !strncmp(ctype->name,name,CKRM_MAX_TYPENAME_LEN)) 
+			return ctype;
+	}
+	return NULL;
+}
+
+
 /*******************************************************************
- *   Initialization 
+ *   Event callback invocation
  *******************************************************************/
 
-void __init
-ckrm_init(void) 
+struct ckrm_hook_cb* ckrm_event_callbacks[CKRM_NONLATCHABLE_EVENTS];
+
+/* Registration / Deregistration / Invocation functions */
+
+int
+ckrm_register_event_cb(enum ckrm_event ev, struct ckrm_hook_cb *cb)
 {
-	struct ckrm_core_class *core = &ckrm_dflt_class;
-	struct task_struct *tsk;
-	int i;
+	struct ckrm_hook_cb **cbptr;
 
-	while(core) {
-		core->magic = CKRM_CORE_MAGIC;
-		core->dentry = NULL;
-		INIT_LIST_HEAD(&core->tasklist);
-		core->ckrm_lock = SPIN_LOCK_UNLOCKED;
-		for (i = 0; i < CKRM_MAX_RES_CTLRS; i++)
-			core->res_class[i] = NULL;
-
-#ifndef NEW_HNODE_IMPLMN
-		ckrm_hnode_add(&core->hnode, NULL);
-#else
-		ckrm_add_child(NULL, core);
-#endif
+	if ((ev < CKRM_LATCHABLE_EVENTS) || (ev >= CKRM_NONLATCHABLE_EVENTS))
+		return 1;
+	cbptr = &ckrm_event_callbacks[ev];
+	while (*cbptr != NULL) 
+		cbptr = &((*cbptr)->next);
+	*cbptr = cb;
+	return 0;
+}
 
-		if (core == &ckrm_dflt_class) {
-			
-			spin_lock(&core->ckrm_lock);
-			memset(core->name, 0, 16);
-			strcpy(core->name, "/rcfs");
-
-			read_lock(&tasklist_lock);
-			for_each_process(tsk) {
-				task_lock(tsk);
-				tsk->ckrm_core = core;
-				INIT_LIST_HEAD(&tsk->ckrm_link);
-				list_add(&tsk->ckrm_link, &core->tasklist);
-				task_unlock(tsk);
-				ckrm_core_grab(core);
-				//printk("ckrm_init: Added %ld to %p\n",(long)tsk->pid,core);
-			}
-			read_unlock(&tasklist_lock);
-			
-			spin_unlock(&core->ckrm_lock);	
-		} else {
-			memset(core->name, 0, 16);
-			strcpy(core->name, "/net");
+int
+ckrm_unregister_event_cb(enum ckrm_event ev, struct ckrm_hook_cb *cb)
+{
+	struct ckrm_hook_cb **cbptr;
+
+	if ((ev < CKRM_LATCHABLE_EVENTS) || (ev >= CKRM_NONLATCHABLE_EVENTS))
+		return -1;
+	cbptr = &ckrm_event_callbacks[ev];
+	while ((*cbptr != NULL) && (*cbptr != cb))
+		cbptr = &((*cbptr)->next);
+	if (*cbptr)
+		(*cbptr)->next = cb->next;
+	return (*cbptr == NULL);
+}
+
+int
+ckrm_register_event_set(struct ckrm_event_spec especs[])
+{
+	struct ckrm_event_spec *espec = especs;
+
+	for ( espec = especs ; espec->ev != -1 ; espec++ )
+		ckrm_register_event_cb(espec->ev,&espec->cb);
+	return 0;
+}
+
+int
+ckrm_unregister_event_set(struct ckrm_event_spec especs[])
+{
+	struct ckrm_event_spec *espec = especs;
+
+	for ( espec = especs ; espec->ev != -1 ; espec++ )
+		ckrm_unregister_event_cb(espec->ev,&espec->cb);
+	return 0;
+}
+
+#define ECC_PRINTK(fmt, args...) // printk("%s: " fmt, __FUNCTION__ , ## args)
+
+void
+ckrm_invoke_event_cb_chain(enum ckrm_event ev, void *arg)
+{
+	struct ckrm_hook_cb *cb, *anchor;
+
+	ECC_PRINTK("%d %x\n",current,ev,arg);
+	if ((anchor = ckrm_event_callbacks[ev]) != NULL) {
+		for ( cb = anchor ; cb ; cb = cb->next ) 
+			(*cb->fct)(arg);
+	}
+}
+
+/*******************************************************************
+ *   Generic Functions that can be used as default functions 
+ *   in almost all classtypes
+ *     (a) function iterator over all resource classes of a class
+ *     (b) function invoker on a named resource
+ *******************************************************************/
+
+int                      
+ckrm_class_show_shares(struct ckrm_core_class *core, struct seq_file *seq)
+{
+	int i;
+	struct ckrm_res_ctlr *rcbs;
+	struct ckrm_classtype *clstype = core->classtype;
+	struct ckrm_shares shares;
+
+	for (i = 0; i < clstype->max_resid; i++) {
+		atomic_inc(&clstype->nr_resusers[i]);
+		rcbs = clstype->res_ctlrs[i];
+		if (rcbs && rcbs->get_share_values) {
+			(*rcbs->get_share_values)(core->res_class[i], &shares);
+			seq_printf(seq,"res=%s,guarantee=%d,limit=%d,total_guarantee=%d,max_limit=%d\n",
+				   rcbs->res_name,
+				   shares.my_guarantee,
+				   shares.my_limit,
+				   shares.total_guarantee,
+				   shares.max_limit);
 		}
+		atomic_dec(&clstype->nr_resusers[i]);
+	}
+	return 0;
+}
 
-		// Add the default class to the global classes list.
-		// ckrm_num_classes already incremented
-		write_lock(&ckrm_class_lock);
-		list_add(&core->clslist, &ckrm_classes);
-		write_unlock(&ckrm_class_lock);
+int                      
+ckrm_class_show_stats(struct ckrm_core_class *core, struct seq_file *seq)
+{
+	int i;
+	struct ckrm_res_ctlr *rcbs;
+	struct ckrm_classtype *clstype = core->classtype;
 
-		core = (core==&ckrm_dflt_class)?&ckrm_net_root:NULL;
+	for (i = 0; i < clstype->max_resid; i++) {
+		atomic_inc(&clstype->nr_resusers[i]);
+		rcbs = clstype->res_ctlrs[i];
+		if (rcbs && rcbs->get_stats) 
+			(*rcbs->get_stats)(core->res_class[i], seq);
+		atomic_dec(&clstype->nr_resusers[i]);
 	}
+	return 0;
+}
+
+int                      
+ckrm_class_show_config(struct ckrm_core_class *core, struct seq_file *seq)
+{
+	int i;
+	struct ckrm_res_ctlr *rcbs;
+	struct ckrm_classtype *clstype = core->classtype;
 
-	printk("CKRM Initialized\n");
+	for (i = 0; i < clstype->max_resid; i++) {
+		atomic_inc(&clstype->nr_resusers[i]);
+		rcbs = clstype->res_ctlrs[i];
+		if (rcbs && rcbs->show_config) 
+			(*rcbs->show_config)(core->res_class[i], seq);
+		atomic_dec(&clstype->nr_resusers[i]);
+	}
+	return 0;
+}
+
+int
+ckrm_class_set_config(struct ckrm_core_class *core, const char *resname, const char *cfgstr)
+{
+	struct ckrm_classtype *clstype = core->classtype;
+	struct ckrm_res_ctlr *rcbs = ckrm_resctlr_lookup(clstype,resname);
+	int rc;
+
+	if (rcbs == NULL || rcbs->set_config == NULL)
+		return -EINVAL; 
+	rc = (*rcbs->set_config)(core->res_class[rcbs->resid],cfgstr);
+	return rc;
+}
+
+int
+ckrm_class_set_shares(struct ckrm_core_class *core, const char *resname,
+		      struct ckrm_shares *shares)
+{
+	struct ckrm_classtype *clstype = core->classtype;
+	struct ckrm_res_ctlr *rcbs;
+	int rc;
+
+	printk("ckrm_class_set_shares(%s,%s)\n",core->name,resname);
+	rcbs = ckrm_resctlr_lookup(clstype,resname);
+	if (rcbs == NULL || rcbs->set_share_values == NULL)
+		return -EINVAL; 
+	rc = (*rcbs->set_share_values)(core->res_class[rcbs->resid],shares);
+	return rc;
+}
+
+int 
+ckrm_class_reset_stats(struct ckrm_core_class *core, const char *resname, const char *unused)
+{
+	struct ckrm_classtype *clstype = core->classtype;
+	struct ckrm_res_ctlr *rcbs = ckrm_resctlr_lookup(clstype,resname);
+	int rc;
+
+	if (rcbs == NULL || rcbs->reset_stats == NULL)
+		return -EINVAL; 
+	rc = (*rcbs->reset_stats)(core->res_class[rcbs->resid]);
+	return rc;
+}	
+
+/*******************************************************************
+ *   Initialization 
+ *******************************************************************/
+
+void
+ckrm_cb_newtask(struct task_struct *tsk)
+{
+	tsk->ce_data   = NULL;
+	spin_lock_init(&tsk->ckrm_tsklock);
+	ckrm_invoke_event_cb_chain(CKRM_EVENT_NEWTASK,tsk);
+}
+
+void 
+ckrm_cb_exit(struct task_struct *tsk)
+{
+	ckrm_invoke_event_cb_chain(CKRM_EVENT_EXIT,tsk);
+	tsk->ce_data = NULL;
+}
+
+void __init
+ckrm_init(void) 
+{
+	printk("CKRM Initialization\n");
+	
+	// register/initialize the Metatypes
+	
+#ifdef CONFIG_CKRM_TYPE_TASKCLASS
+	{ 
+		extern void ckrm_meta_init_taskclass(void);
+		ckrm_meta_init_taskclass();
+	}
+#endif
+#ifdef CONFIG_CKRM_TYPE_SOCKETCLASS
+	{ 
+		extern void ckrm_meta_init_sockclass(void);
+		ckrm_meta_init_sockclass();
+	}
+#endif
+	// prepare init_task and then rely on inheritance of properties
+	ckrm_cb_newtask(&init_task);
+	printk("CKRM Initialization done\n");
 }
 
 
@@ -1431,9 +975,29 @@
 EXPORT_SYMBOL(ckrm_register_res_ctlr);
 EXPORT_SYMBOL(ckrm_unregister_res_ctlr);
 
-EXPORT_SYMBOL(ckrm_alloc_core_class);
+EXPORT_SYMBOL(ckrm_init_core_class);
 EXPORT_SYMBOL(ckrm_free_core_class);
+EXPORT_SYMBOL(ckrm_release_core_class);
+
+EXPORT_SYMBOL(ckrm_register_classtype);
+EXPORT_SYMBOL(ckrm_unregister_classtype);
+EXPORT_SYMBOL(ckrm_find_classtype_by_name);
 
-EXPORT_SYMBOL(ckrm_reclassify);
 EXPORT_SYMBOL(ckrm_core_grab);
 EXPORT_SYMBOL(ckrm_core_drop);
+EXPORT_SYMBOL(ckrm_is_core_valid);
+EXPORT_SYMBOL(ckrm_validate_and_grab_core);
+
+EXPORT_SYMBOL(ckrm_register_event_set);
+EXPORT_SYMBOL(ckrm_unregister_event_set);
+EXPORT_SYMBOL(ckrm_register_event_cb);
+EXPORT_SYMBOL(ckrm_unregister_event_cb);
+
+EXPORT_SYMBOL(ckrm_class_show_stats);
+EXPORT_SYMBOL(ckrm_class_show_config);
+EXPORT_SYMBOL(ckrm_class_show_shares);
+
+EXPORT_SYMBOL(ckrm_class_set_config);
+EXPORT_SYMBOL(ckrm_class_set_shares);
+
+EXPORT_SYMBOL(ckrm_class_reset_stats);
diff -Nru --exclude='*SCCS*' a/kernel/ckrm/ckrm_dummy.c b/kernel/ckrm/ckrm_dummy.c
--- a/kernel/ckrm/ckrm_dummy.c	2004-04-26 12:35:37.000000000 -0400
+++ b/kernel/ckrm/ckrm_dummy.c	1969-12-31 19:00:00.000000000 -0500
@@ -1,313 +0,0 @@
-/* ckrm_dummy.c - Dummy Resource Manager for CKRM
- *
- * Copyright (C) Chandra Seetharaman,  IBM Corp. 2003
- * Copyright (C) Hubertus Franke,      IBM Corp. 2004
- * 
- * Provides a Dummy Resource controller for CKRM
- *
- * Latest version, more details at http://ckrm.sf.net
- * 
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- *
- */
-
-/* Changes
- * 
- * 06 Nov 2003: Created
- * 03 Mar 2004: Adopted to new Interface
- * 
- */
-
-/* Code Description: TBD
- *
- */
-
-#include <linux/module.h>
-#include <linux/init.h>
-#include <linux/slab.h>
-#include <asm/errno.h>
-#include <linux/list.h>
-#include <linux/spinlock.h>
-#include <linux/ckrm.h>
-#include <linux/ckrm_rc.h>
-
-#define DEBUG_CKRM_DUMMY 1
-
-typedef struct ckrm_dummy_res {
-	struct ckrm_core_class *core; // the core i am part of...
-	struct ckrm_core_class *parent; // parent of the core i am part of....
-	struct ckrm_shares shares;
-} ckrm_dummy_res_t;
-
-static int my_resid = -1;
-
-/* Initialize rescls values
- * May be called on each rcfs unmount or as part of error recovery
- * to make share values sane.
- * Does not traverse hierarchy reinitializing children.
- */
-
-static void
-dummy_res_initcls_one(void *my_res)
-{
-	ckrm_dummy_res_t *res = my_res;
-
-	res->shares.my_guarantee     = CKRM_SHARE_DONTCARE;
-	res->shares.my_limit         = CKRM_SHARE_DONTCARE;
-	res->shares.total_guarantee  = CKRM_SHARE_DFLT_TOTAL_GUARANTEE;
-	res->shares.total_limit      = CKRM_SHARE_DFLT_TOTAL_LIMIT;
-	res->shares.unused_guarantee = CKRM_SHARE_DFLT_TOTAL_GUARANTEE;
-	res->shares.unused_limit     = CKRM_SHARE_DFLT_TOTAL_LIMIT;
-	
-	/* Don't initiate propagation to children here, caller will do it if needed */
-}
-
-#if 0	
-static void
-dummy_res_initcls(void *my_res)
-{
-	ckrm_dummy_res_t *res = my_res;
-
-	/* Write a version which propagates values all the way down 
-	   and replace rcbs callback with that version */
-	
-}
-#endif
-
-ckrm_dummy_res_t *dres;
-struct ckrm_hnode  *dparhnode;
-ckrm_dummy_res_t *dparres;
-
-static void *
-dummy_res_alloc(struct ckrm_core_class *core, struct ckrm_core_class *parent)
-{
-	//ckrm_dummy_res_t *res;
-	
-
-	dres = kmalloc(sizeof(ckrm_dummy_res_t), GFP_ATOMIC);
-	
-	if (dres) {
-#if 0
-		//struct ckrm_hnode *parhnode = NULL;
-		dparhnode = NULL ;
-
-		if (is_core_valid(parent)) {
-			// ckrm_dummy_res_t  *parres;
-			dparres = ckrm_get_res_class(parent,my_resid,ckrm_dummy_res_t);
-			dparhnode = &dparres->hnode;
-		}
-		ckrm_hnode_add(&dres->hnode, dparhnode);
-		printk(KERN_ERR "dummy_res_alloc: Adding dummy res class %p to %p\n",dres,parent);
-		
-		/* rescls in place, now initialize contents other than hierarchy pointers */
-#endif
-		dres->core = core;
-		dres->parent = parent;
-		dummy_res_initcls_one(dres);
-		
-		/*
-		  else {
-			kfree(dres);
-			printk(KERN_ERR "dummy_res_alloc: Invalid core \n");
-			return NULL;
-		}
-		*/
-	}
-	else
-		printk(KERN_ERR "dummy_res_alloc: failed GFP_ATOMIC alloc\n");
-	return dres;
-}
-
-ckrm_dummy_res_t *d2res, *d2parres;
-
-
-static void
-dummy_res_free(void *my_res)
-{
-	//ckrm_dummy_res_t *res = my_res;
-	//ckrm_dummy_res_t *parres;
-
-	d2res = my_res ;
-
-	if (!d2res) 
-		return;
-
-	d2parres = ckrm_get_res_class(d2res->parent, my_resid, ckrm_dummy_res_t);
-
-	// return child's limit/guarantee to parent node
-	if (d2parres) {
-		if (d2res->shares.my_guarantee >= 0)
-			d2parres->shares.unused_guarantee += d2res->shares.my_guarantee;
-		if (d2res->shares.my_limit >= 0)
-			d2parres->shares.unused_limit += d2res->shares.my_limit;
-	}
-	kfree(d2res);
-	return;
-}
-
-static int
-dummy_set_share_values(void *my_res, struct ckrm_shares *shares)
-{
-	ckrm_dummy_res_t *res = my_res;
-	ckrm_dummy_res_t *parent;
-	int reduce_by;
-	int rc = EINVAL;
-
-	if (!res) 
-		return -EINVAL;
-
-	parent = ckrm_get_res_class(res->parent, my_resid, ckrm_dummy_res_t);
-
-	// we have to ensure that the set of parameters is OK
-
-	// ensure that lim/guarantees are ok wrt to parent total values 
-	// don't have to consider negative special values
-
-	/* FIXME following doesn't appear to be working */
-	if (parent) {
-		if ((shares->my_guarantee > parent->shares.unused_guarantee) ||
-		    (shares->my_limit > parent->shares.unused_limit))
-			goto set_share_err;
-	}
-
-	// translate UNCHANGED to existing values
-	if (shares->total_guarantee == CKRM_SHARE_UNCHANGED)
-		shares->total_guarantee = res->shares.total_guarantee;
-	if (shares->total_limit == CKRM_SHARE_UNCHANGED)
-		shares->total_limit = res->shares.total_limit;
-
-	// we don't allow DONTCARE for totals
-	if ((shares->total_guarantee <= CKRM_SHARE_DONTCARE) || (shares->total_limit <= CKRM_SHARE_DONTCARE))
-		goto set_share_err;
-
-	// check whether total shares still exceeds sum of children (total - unused)
-	if (((reduce_by = shares->total_guarantee - res->shares.total_guarantee) > 0) &&
-	    (reduce_by > res->shares.unused_guarantee))
-		goto set_share_err;
-	if (((reduce_by = shares->total_limit - res->shares.total_limit) > 0) &&
-	    (reduce_by > res->shares.unused_limit))
-		goto set_share_err;
-
-
-	/* Need more sanity checks (first two not being enforced currently):
-	 *  1. guarantee < limit
-         *  2. my_* < tot_*
-	 *  etc. Do later.
-	 */
-	// values are OK now enforce them 
-
-	if (shares->my_guarantee > 0) {
-		if (parent) { 
-			parent->shares.unused_guarantee -= shares->my_guarantee;
-			if (res->shares.my_guarantee >= 0)
-				parent->shares.unused_guarantee += res->shares.my_guarantee;
-		}
-		res->shares.my_guarantee = shares->my_guarantee;
-	} else if (shares->my_guarantee == CKRM_SHARE_DONTCARE) {
-		if (parent) 
-			parent->shares.unused_guarantee += res->shares.my_guarantee;
-		res->shares.my_guarantee = CKRM_SHARE_DONTCARE;
-	}
-
-	if (shares->my_limit > 0) {
-		if (parent) { 
-			parent->shares.unused_limit -= shares->my_limit;
-			if (res->shares.my_limit >= 0)
-				parent->shares.unused_limit += res->shares.my_limit;
-		}
-		res->shares.my_limit = shares->my_limit;
-	} else if (shares->my_limit == CKRM_SHARE_DONTCARE) {
-		if (parent) 
-			parent->shares.unused_limit += res->shares.my_limit;
-		res->shares.my_limit = CKRM_SHARE_DONTCARE;
-	}
-
-
-	res->shares.unused_guarantee += (shares->total_guarantee - res->shares.total_guarantee);
-	res->shares.unused_limit     += (shares->total_limit     - res->shares.total_limit);
-
-	res->shares.total_guarantee  = shares->total_guarantee;
-	res->shares.total_limit      = shares->total_limit;
-
-	/* Here we should force the propagation of share values */
-	
-	rc = 0;
-	goto out;
-
-set_share_err:
-#ifdef DEBUG_CKRM_DUMMY
-	dummy_res_initcls_one(res);
-	rc = 0;
-#endif
-out:
-	
-	return rc;
-}
-
-static int
-dummy_get_share_values(void *my_res, struct ckrm_shares *shares)
-{
-	ckrm_dummy_res_t *res = my_res;
-
-	if (!res) 
-		return -EINVAL;
-	*shares = res->shares;
-	return 0;
-}
-
-static int  
-dummy_get_stats(void *my_res, struct seq_file *sfile)
-{
-	ckrm_dummy_res_t *res = my_res;
-
-	if (!res) 
-		return -EINVAL;
-
-	seq_printf(sfile, "res=dummy: these are my stats <none>\n");
-
-	return 0;
-}
-
-static void
-dummy_change_resclass(void *tsk, void *old, void *new)
-{
-	// does nothing
-	return;
-}
-
-ckrm_res_callback_t dummy_rcbs = {
-	.res_name          = "dummy",
-	.res_hdepth	   = 1,
-	.res_type	   = CKRM_TASK_CLASS,
-	.resid		   = CKRM_RES_DUMMY,
-	.res_alloc         = dummy_res_alloc,
-	.res_free          = dummy_res_free,
-	.set_share_values  = dummy_set_share_values,
-	.get_share_values  = dummy_get_share_values,
-	.get_stats         = dummy_get_stats,
-	.change_resclass   = dummy_change_resclass,
-	.res_initcls       = dummy_res_initcls_one,
-};
-
-int __init
-init_ckrm_dummy_res(void)
-{
-	my_resid = ckrm_register_res_ctlr(&dummy_rcbs);
-	return 0;
-}	
-
-void __exit
-exit_ckrm_dummy_res(void)
-{
-	ckrm_unregister_res_ctlr(my_resid);
-}
-
-
-module_init(init_ckrm_dummy_res)
-module_exit(exit_ckrm_dummy_res)
-
-MODULE_LICENSE("GPL");
-
diff -Nru --exclude='*SCCS*' a/kernel/ckrm/ckrm_listenaq.c b/kernel/ckrm/ckrm_listenaq.c
--- a/kernel/ckrm/ckrm_listenaq.c	1969-12-31 19:00:00.000000000 -0500
+++ b/kernel/ckrm/ckrm_listenaq.c	2004-04-28 22:57:42.000000000 -0400
@@ -0,0 +1,503 @@
+/* ckrm_socketaq.c - accept queue resource controller
+ *
+ * Copyright (C) Vivek Kashyap,      IBM Corp. 2004
+ * 
+ * Latest version, more details at http://ckrm.sf.net
+ * 
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ */
+
+/* Changes
+ * Initial version
+ */
+
+/* Code Description: TBD
+ *
+ */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/slab.h>
+#include <asm/errno.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/ckrm.h>
+#include <linux/ckrm_rc.h>
+#include <net/tcp.h>
+
+#include <linux/ckrm_net.h>
+
+#define hnode_2_core(ptr) \
+                ((ptr) ? container_of(ptr, struct ckrm_core_class, hnode) : NULL)
+
+
+#define CKRM_SAQ_MAX_DEPTH	3 // 0 => /rcfs
+				  // 1 => socket_aq
+				  // 2 => socket_aq/listen_class
+				  // 3 => socket_aq/listen_class/accept_queues
+				  // 4 => Not allowed
+
+typedef struct ckrm_laq_res {
+	spinlock_t		reslock;
+	atomic_t		refcnt;
+	struct ckrm_shares 	shares;
+	struct ckrm_core_class *core;
+	struct ckrm_core_class *pcore;
+	int 			my_depth;
+	int 			my_id;
+} ckrm_laq_res_t;
+
+static int my_resid = -1;
+
+extern	struct ckrm_core_class *rcfs_create_under_netroot(char *, int, int);
+extern struct ckrm_core_class *rcfs_make_core(struct dentry *, 
+						struct ckrm_core_class * ) ;
+
+void
+laq_res_hold(struct ckrm_laq_res *res)
+{
+        atomic_inc(&res->refcnt);
+	return;
+}
+
+void
+laq_res_put(struct ckrm_laq_res *res)
+{
+	if (atomic_dec_and_test(&res->refcnt))
+		kfree(res);
+	return;
+}
+
+/* Initialize rescls values
+ */
+static void
+laq_res_initcls(void *my_res)
+{
+	ckrm_laq_res_t *res = my_res;
+
+	res->shares.my_guarantee     = CKRM_SHARE_DONTCARE;
+	res->shares.my_limit         = CKRM_SHARE_DONTCARE;
+	res->shares.total_guarantee  = CKRM_SHARE_DFLT_TOTAL_GUARANTEE;
+	res->shares.max_limit        = CKRM_SHARE_DFLT_MAX_LIMIT;
+	res->shares.unused_guarantee = CKRM_SHARE_DFLT_TOTAL_GUARANTEE;
+	res->shares.cur_max_limit    = 0;
+}
+
+static int 
+atoi(char *s)
+{
+	int k = 0;
+	while(*s) 
+		k = *s++ - '0' + (k * 10);
+	return k;
+}
+
+static char *
+laq_get_name(struct ckrm_core_class *c)
+{
+        char *p = (char *)c->name;
+
+        while(*p)
+                p++;
+        while( *p != '/' && p != c->name)
+                p--;
+
+        return ++p;
+}
+
+static void *
+laq_res_alloc(struct ckrm_core_class *core, struct ckrm_core_class *parent)
+{
+	ckrm_laq_res_t *res, *pres;
+	int pdepth;
+
+	if (parent)
+		pres = ckrm_get_res_class(parent, my_resid, ckrm_laq_res_t);
+	else
+		pres = NULL;
+
+	if (core == core->classtype->default_class)    
+		pdepth = 1;
+	else {
+		if (!parent)
+			return NULL;
+		pdepth = 1 + pres->my_depth;
+	}
+
+	res = kmalloc(sizeof(ckrm_laq_res_t), GFP_ATOMIC);
+	if (res) {
+		memset(res, 0, sizeof(res));
+		spin_lock_init(&res->reslock);
+		laq_res_hold(res);
+		res->my_depth  = pdepth;
+		if (pdepth == 2)	// listen class
+			res->my_id = 0;
+		else if (pdepth == 3)
+			res->my_id = atoi(laq_get_name(core));
+		res->core = core;
+		res->pcore = parent;
+
+		// rescls in place, now initialize contents other than 
+		// hierarchy pointers
+		laq_res_initcls(res); // acts as initialising value
+	}
+
+	return res;
+}
+
+static void
+laq_res_free(void *my_res)
+{
+	ckrm_laq_res_t *res = (ckrm_laq_res_t *)my_res;
+	ckrm_laq_res_t *parent;
+
+	if (!res) 
+		return;
+
+	if (res->my_depth != 3) {
+		kfree(res);
+		return;
+	}
+
+	parent = ckrm_get_res_class(res->pcore, my_resid, ckrm_laq_res_t);
+	if (!parent)	// Should never happen
+		return;
+
+	spin_lock(&parent->reslock);
+	spin_lock(&res->reslock);
+
+	// return child's guarantee to parent node
+	// Limits have no meaning for accept queue control
+	child_guarantee_changed(&parent->shares, res->shares.my_guarantee, 0);
+
+	spin_unlock(&res->reslock);
+	laq_res_put(res);	
+	spin_unlock(&parent->reslock);
+	return;
+}
+
+/**************************************************************************
+ * 			SHARES					        ***
+ **************************************************************************/
+
+void
+laq_set_aq_values(ckrm_laq_res_t *my_res, ckrm_laq_res_t *parent, int updatep)
+{
+
+	struct ckrm_net_struct *ns;
+	struct ckrm_core_class *core = parent->core;
+	struct tcp_opt *tp;
+	
+	if (my_res->my_depth < 2) 
+		return;
+	
+	// XXX Instead of holding a  class_lock introduce a rw
+	// lock to be write locked by listen callbacks and read locked here.
+	// - VK
+	class_lock(core);
+	list_for_each_entry(ns, &core->objlist,ckrm_link) { 
+		tp = tcp_sk(ns->ns_sk);
+		if (updatep)
+			tp->acceptq[0].aq_ratio =
+			       parent->shares.total_guarantee/
+				parent->shares.unused_guarantee;	       
+
+		tp->acceptq[my_res->my_id].aq_ratio =
+		       my_res->shares.total_guarantee/
+			parent->shares.my_guarantee;	       
+	}
+	class_unlock(core);
+	return;
+}
+
+static int
+laq_set_share_values(void *my_res, struct ckrm_shares *shares)
+{
+	ckrm_laq_res_t *res = my_res;
+	ckrm_laq_res_t *parent, *child;
+	struct ckrm_hnode *chnode; 
+	int rc = 0;
+
+	if (!res) 
+		return -EINVAL;
+
+	if (!res->pcore) { 
+		// something is badly wrong
+		printk(KERN_ERR "socketaq internal inconsistency\n");
+		return -EBADF;
+	}
+
+	parent = ckrm_get_res_class(res->pcore, my_resid, ckrm_laq_res_t);
+	if (!parent)	// socket_class does not have a share interface
+		return -EINVAL;
+
+	// Ensure that we ignore limit values
+	shares->my_limit = shares->max_limit = CKRM_SHARE_UNCHANGED;
+
+	switch (res->my_depth) {
+
+	case 0: printk(KERN_ERR "socketaq bad entry\n");
+		rc = -EBADF;
+		break;
+
+	case 1: // can't be written to. this is internal default.
+		// return -EINVAL
+		rc = -EINVAL;
+		break;
+
+	case 2: // nothing to inherit
+		if (!shares->total_guarantee) {
+			rc = -EINVAL;
+			break;
+		}
+
+		ckrm_lock_hier(res->pcore);
+		spin_lock(&res->reslock);
+		rc = set_shares(shares, &res->shares, NULL);
+		if (!rc) {
+			list_for_each_entry(chnode,
+					&res->core->hnode.children,siblings){
+				child=hnode_2_core(chnode)->res_class[my_resid];
+				laq_set_aq_values(child,res,(child->my_id==1));
+			}
+		}
+		spin_unlock(&res->reslock);
+		ckrm_unlock_hier(res->pcore);
+		break;
+
+	case 3: // accept queue itself. Check against parent.
+		ckrm_lock_hier(parent->pcore);
+		spin_lock(&parent->reslock);
+		rc = set_shares(shares, &res->shares, &parent->shares);
+		if (!rc) {
+			laq_set_aq_values(res,parent,1);
+		}
+		spin_unlock(&parent->reslock);
+		ckrm_unlock_hier(parent->pcore);
+		break;
+	}
+
+	return rc;
+}
+
+static int
+laq_get_share_values(void *my_res, struct ckrm_shares *shares)
+{
+	ckrm_laq_res_t *res = my_res;
+
+	if (!res) 
+		return -EINVAL;
+	*shares = res->shares;
+	return 0;
+}
+
+/**************************************************************************
+ * 			STATS						***
+ **************************************************************************/
+
+void
+laq_print_aq_stats(struct seq_file *sfile, struct tcp_acceptq_info *taq, int i)
+{
+	seq_printf(sfile, "Class %d connections:\n\taccepted: %u\n\t"
+			  "queued: %u\n\twait_time: %lu\n\t",
+			  i, taq->acceptq_count, taq->acceptq_qcount,
+			  taq->acceptq_wait_time);
+
+	if (i)
+		return;
+
+	for (i = 1; i < NUM_ACCEPT_QUEUES; i++) {
+		taq[0].acceptq_wait_time += taq[i].acceptq_wait_time;
+		taq[0].acceptq_qcount += taq[i].acceptq_qcount;
+		taq[0].acceptq_count += taq[i].acceptq_count;
+	}
+
+	seq_printf(sfile, "Totals :\n\taccepted: %u\n\t"
+			  "queued: %u\n\twait_time: %lu\n",
+			   taq->acceptq_count, taq->acceptq_qcount,
+			  taq->acceptq_wait_time);
+
+	return;
+}
+
+void
+laq_get_aq_stats(ckrm_laq_res_t *pres, ckrm_laq_res_t *mres, 
+					struct tcp_acceptq_info *taq)
+{
+	struct ckrm_net_struct *ns;
+	struct ckrm_core_class *core = pres->core;
+	struct tcp_opt *tp;
+	int a = mres->my_id;
+	int z;
+
+	if (a == 0)
+		z = NUM_ACCEPT_QUEUES;
+	else
+		z = a+1;
+
+	// XXX Instead of holding a  class_lock introduce a rw
+	// lock to be write locked by listen callbacks and read locked here.
+	// - VK
+	class_lock(pres->core);
+	list_for_each_entry(ns, &core->objlist,ckrm_link) { 
+		tp = tcp_sk(ns->ns_sk);
+		for (; a< z; a++) {
+			taq->acceptq_wait_time += tp->acceptq[a].aq_wait_time;
+			taq->acceptq_qcount += tp->acceptq[a].aq_qcount;
+			taq->acceptq_count += tp->acceptq[a].aq_count;
+			taq++;
+		}
+	}
+	class_unlock(pres->core);
+}
+
+
+static int  
+laq_get_stats(void *my_res, struct seq_file *sfile)
+{
+	ckrm_laq_res_t *res = my_res;
+	ckrm_laq_res_t *parent;
+	struct tcp_acceptq_info taq[NUM_ACCEPT_QUEUES];
+	int rc = 0;
+
+	if (!res) 
+		return -EINVAL;
+	
+	if (!res->pcore) { 
+		// something is badly wrong
+		printk(KERN_ERR "socketaq internal inconsistency\n");
+		return -EBADF;
+	}
+
+	parent = ckrm_get_res_class(res->pcore, my_resid, ckrm_laq_res_t);
+	if (!parent) {	// socket_class does not have a stat interface
+		printk(KERN_ERR "socketaq internal fs inconsistency\n");
+		return -EINVAL;
+	}
+
+	memset(taq, 0, sizeof(struct tcp_acceptq_info) * NUM_ACCEPT_QUEUES);
+
+	switch (res->my_depth) {
+
+	default:
+	case 0: printk(KERN_ERR "socket class bad entry\n");
+		rc = -EBADF;
+		break;
+
+	case 1: // can't be read from. this is internal default.
+		// return -EINVAL
+		rc = -EINVAL;
+		break;
+
+	case 2: // return the default and total
+		ckrm_lock_hier(res->core);	// block any deletes
+		laq_get_aq_stats(res, res, &taq[0]);
+		laq_print_aq_stats(sfile, &taq[0], 0);
+		ckrm_unlock_hier(res->core);	// block any deletes
+		break;
+
+	case 3: 
+		ckrm_lock_hier(parent->core);	// block any deletes
+		laq_get_aq_stats(parent, res, &taq[res->my_id]);
+		laq_print_aq_stats(sfile, &taq[res->my_id], res->my_id);
+		ckrm_unlock_hier(parent->core);	// block any deletes
+		break;
+	}
+
+	return rc;
+}
+
+/*
+ * The network connection is reclassified to this class. Update its shares.
+ * The socket lock is held. 
+ */
+static void
+laq_change_resclass(void *n, void *old, void *r)
+{
+	struct ckrm_net_struct *ns = (struct ckrm_net_struct *)n;
+	struct ckrm_laq_res *res = (struct ckrm_laq_res *)r;
+	struct ckrm_hnode  *chnode = NULL;
+
+
+	if (res->my_depth != 2) 
+		return;	
+
+	// a change to my_depth == 3 ie. the accept classes cannot happen.
+	// there is no target file
+	if (res->my_depth == 2) { // it is one of the socket classes
+		struct ckrm_laq_res *reschild;
+		struct sock *sk = ns->ns_sk; 
+		struct tcp_opt *tp = tcp_sk(sk);
+
+		// share rule: hold parent resource lock. then self.
+		// However, since my_depth == 1 is a generic class it is not
+		// needed here. Self lock is enough.
+		spin_lock(&res->reslock);
+		tp->acceptq[0].aq_ratio = res->shares.total_guarantee/
+				res->shares.unused_guarantee;
+		list_for_each_entry(chnode,&res->core->hnode.children,siblings){
+			reschild = hnode_2_core(chnode)->res_class[my_resid];
+
+			spin_lock(&reschild->reslock);
+			tp->acceptq[reschild->my_id].aq_ratio=
+				reschild->shares.total_guarantee/
+					res->shares.my_guarantee;
+			spin_unlock(&reschild->reslock);
+		}
+		spin_unlock(&res->reslock);
+	}
+	
+	return;
+}
+
+struct ckrm_res_ctlr laq_rcbs = {
+	.res_name          = "laq",
+	.resid		   = -1 , // dynamically assigned
+	.res_alloc         = laq_res_alloc,
+	.res_free          = laq_res_free,
+	.set_share_values  = laq_set_share_values,
+	.get_share_values  = laq_get_share_values,
+	.get_stats         = laq_get_stats,
+	.change_resclass   = laq_change_resclass,
+	//	.res_initcls       = laq_res_initcls,         // LAQ_HUBERTUS: no need for this !!
+};
+
+int __init
+init_ckrm_laq_res(void)
+{
+	struct ckrm_classtype *clstype;
+	int resid;
+
+	clstype = ckrm_find_classtype_by_name("socket_class");
+	if (clstype == NULL) {
+		printk(KERN_INFO " Unknown ckrm classtype<socket_class>");
+		return -ENOENT;
+	}
+
+	if (my_resid == -1) {
+		resid = ckrm_register_res_ctlr(clstype,&laq_rcbs);
+		if (resid >= 0)
+			my_resid = resid;
+		printk("........init_ckrm_listen_aq_res -> %d\n",my_resid);
+	}
+	return 0;
+
+}	
+
+void __exit
+exit_ckrm_laq_res(void)
+{
+	ckrm_unregister_res_ctlr(&laq_rcbs);
+	my_resid = -1;
+}
+
+
+module_init(init_ckrm_laq_res)
+module_exit(exit_ckrm_laq_res)
+
+MODULE_LICENSE("GPL");
+
diff -Nru --exclude='*SCCS*' a/kernel/ckrm/ckrm_sockc.c b/kernel/ckrm/ckrm_sockc.c
--- a/kernel/ckrm/ckrm_sockc.c	1969-12-31 19:00:00.000000000 -0500
+++ b/kernel/ckrm/ckrm_sockc.c	2004-04-28 22:57:42.000000000 -0400
@@ -0,0 +1,554 @@
+/* ckrm_sock.c - Class-based Kernel Resource Management (CKRM)
+ *
+ * Copyright (C) Hubertus Franke, IBM Corp. 2003,2004
+ *           (C) Shailabh Nagar,  IBM Corp. 2003
+ *           (C) Chandra Seetharaman,  IBM Corp. 2003
+ *	     (C) Vivek Kashyap,	IBM Corp. 2004
+ * 
+ * 
+ * Provides kernel API of CKRM for in-kernel,per-resource controllers 
+ * (one each for cpu, memory, io, network) and callbacks for 
+ * classification modules.
+ *
+ * Latest version, more details at http://ckrm.sf.net
+ * 
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ */
+
+/* Changes
+ *
+ * 28 Aug 2003
+ *        Created.
+ * 06 Nov 2003
+ *        Made modifications to suit the new RBCE module.
+ * 10 Nov 2003
+ *        Fixed a bug in fork and exit callbacks. Added callbacks_active and
+ *        surrounding logic. Added task paramter for all CE callbacks.
+ * 23 Mar 2004
+ *        moved to referenced counted class objects and correct locking
+ * 12 Apr 2004
+ *        introduced adopted to emerging classtype interface
+ */
+
+#include <linux/config.h>
+#include <linux/init.h>
+#include <linux/linkage.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <asm/uaccess.h>
+#include <linux/mm.h>
+#include <asm/errno.h>
+#include <linux/string.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/module.h>
+#include <linux/ckrm_rc.h>
+#include <linux/parser.h>
+#include <net/tcp.h>
+
+#include <linux/ckrm_net.h>
+
+struct ckrm_sock_class {
+	struct ckrm_core_class core;
+};
+
+static struct ckrm_sock_class  sockclass_dflt_class = {
+};
+
+#define SOCKET_CLASS_TYPE_NAME  "socket_class"
+
+const char *dflt_sockclass_name = SOCKET_CLASS_TYPE_NAME;
+
+static struct ckrm_core_class *sock_alloc_class(struct ckrm_core_class *parent, const char *name);
+static int  sock_free_class(struct ckrm_core_class *core);
+
+static int  sock_forced_reclassify(ckrm_core_class_t *target, const char *resname);
+static int  sock_show_members(struct ckrm_core_class *core, struct seq_file *seq);
+static void sock_add_resctrl(struct ckrm_core_class *core, int resid);
+static void sock_reclassify_class(struct ckrm_sock_class *cls);
+
+struct ckrm_classtype CT_sockclass = {
+	.mfidx          = 1,
+	.name           = SOCKET_CLASS_TYPE_NAME,
+	.typeID         = CKRM_CLASSTYPE_SOCKET_CLASS, 
+	.maxdepth       = 3,                           
+	.resid_reserved = 0,                           
+	.max_res_ctlrs  = CKRM_MAX_RES_CTLRS,        
+	.max_resid      = 0,
+	.bit_res_ctlrs  = 0L,
+	.res_ctlrs_lock = SPIN_LOCK_UNLOCKED,
+	.classes        = LIST_HEAD_INIT(CT_sockclass.classes),
+
+	.default_class  = &sockclass_dflt_class.core,
+	
+	// private version of functions 
+	.alloc          = &sock_alloc_class,
+	.free           = &sock_free_class,
+	.show_members   = &sock_show_members,
+	.forced_reclassify = &sock_forced_reclassify,
+
+	// use of default functions 
+	.show_shares    = &ckrm_class_show_shares,
+	.show_stats     = &ckrm_class_show_stats,
+	.show_config    = &ckrm_class_show_config,
+	.set_config     = &ckrm_class_set_config,
+	.set_shares     = &ckrm_class_set_shares,
+	.reset_stats    = &ckrm_class_reset_stats,
+
+	// mandatory private version .. no dflt available
+	.add_resctrl    = &sock_add_resctrl,	
+};
+
+/* helper functions */
+
+void
+ckrm_ns_hold(struct ckrm_net_struct *ns)
+{
+        atomic_inc(&ns->ns_refcnt);
+        return;
+}
+
+void
+ckrm_ns_put(struct ckrm_net_struct *ns)
+{
+        if (atomic_dec_and_test(&ns->ns_refcnt))
+                kfree(ns);
+
+        return;
+}
+/*
+ * Change the class of a netstruct 
+ *
+ * Change the task's task class  to "newcls" if the task's current 
+ * class (task->taskclass) is same as given "oldcls", if it is non-NULL.
+ *
+ */
+
+static void
+sock_set_class(struct ckrm_net_struct *ns, struct ckrm_sock_class *newcls,
+	      struct ckrm_sock_class *oldcls, enum ckrm_event event)
+{
+	int i;
+	struct ckrm_res_ctlr *rcbs;
+	struct ckrm_classtype *clstype;
+	void  *old_res_class, *new_res_class;
+
+	if ((newcls == oldcls) || (newcls == NULL)) {
+		ns->core = (void *)oldcls;
+		return;
+	}
+
+	class_lock(class_core(newcls));
+	ns->core = newcls;
+	list_add(&ns->ckrm_link, &class_core(newcls)->objlist);
+	class_unlock(class_core(newcls));
+
+	clstype = class_isa(newcls);                 
+	for (i = 0; i < clstype->max_resid; i++) {
+		atomic_inc(&clstype->nr_resusers[i]);
+		old_res_class = oldcls ? class_core(oldcls)->res_class[i] : NULL;
+		new_res_class = newcls ? class_core(newcls)->res_class[i] : NULL;
+		rcbs = clstype->res_ctlrs[i];
+		if (rcbs && rcbs->change_resclass && (old_res_class != new_res_class)) 
+			(*rcbs->change_resclass)(ns, old_res_class, new_res_class);
+		atomic_dec(&clstype->nr_resusers[i]);
+	}
+	return;
+}
+
+static void
+sock_add_resctrl(struct ckrm_core_class *core, int resid)
+{
+	struct ckrm_net_struct *ns;
+	struct ckrm_res_ctlr *rcbs;
+
+	if ((resid < 0) || (resid >= CKRM_MAX_RES_CTLRS) || ((rcbs = core->classtype->res_ctlrs[resid]) == NULL)) 
+		return;
+
+	spin_lock(&core->ckrm_lock);
+	list_for_each_entry(ns, &core->objlist, ckrm_link) {
+		if (rcbs->change_resclass)
+			(*rcbs->change_resclass)(ns, NULL, core->res_class[resid]);
+	}
+	spin_unlock(&core->ckrm_lock);
+}
+
+
+/**************************************************************************
+ *                   Functions called from classification points          *
+ **************************************************************************/
+
+static void
+cb_sockclass_listen_start(struct sock *sk)
+{
+	struct ckrm_net_struct *ns = NULL;
+	struct ckrm_sock_class *newcls = NULL;
+	struct ckrm_res_ctlr *rcbs;
+	struct ckrm_classtype *clstype;
+	int i = 0;
+
+	// XXX - TBD ipv6
+	if (sk->sk_family == IPPROTO_IPV6)
+		return;
+
+	// to store the socket address
+	ns = (struct ckrm_net_struct *)
+		kmalloc(sizeof(struct ckrm_net_struct), GFP_ATOMIC);
+	if (!ns)
+		return;
+
+	memset(ns,0, sizeof(ns));
+	INIT_LIST_HEAD(&ns->ckrm_link);
+
+	ns->ns_family = sk->sk_family;
+	if (ns->ns_family == IPPROTO_IPV6)	// IPv6 not supported yet.
+		return;
+
+	ns->ns_daddrv4 = inet_sk(sk)->rcv_saddr;
+	ns->ns_dport = inet_sk(sk)->num;
+		
+	ns->ns_pid = current->pid;
+	ns->ns_tgid = current->tgid;
+
+	ce_protect(&CT_sockclass);
+	CE_CLASSIFY_RET(newcls,&CT_sockclass,CKRM_EVENT_LISTEN_START,ns,current);
+	ce_release(&CT_sockclass);
+
+	if (newcls == NULL)  {
+		newcls = &sockclass_dflt_class;
+		ckrm_core_grab(class_core(newcls));
+	}
+
+	class_lock(class_core(newcls));
+	list_add(&ns->ckrm_link, &class_core(newcls)->objlist);
+	ckrm_ns_put(ns);
+	ns->core = newcls;
+	class_unlock(class_core(newcls));
+	
+
+	// the socket is already locked
+	// take a reference on socket on our behalf
+	sock_hold(sk);
+	sk->sk_ns = (void *)ns;
+	ns->ns_sk = sk;
+
+	// modify its shares
+	clstype = class_isa(newcls);
+	for (i = 0; i < clstype->max_resid; i++) {
+		atomic_inc(&clstype->nr_resusers[i]);
+		rcbs = clstype->res_ctlrs[i];
+		if (rcbs && rcbs->change_resclass) {
+			(*rcbs->change_resclass)((void *)ns, 
+					 NULL,class_core(newcls)->res_class[i]);
+		}
+		atomic_dec(&clstype->nr_resusers[i]);
+	}
+	return;
+}
+
+static void
+cb_sockclass_listen_stop(struct sock *sk)
+{
+	struct ckrm_net_struct *ns = NULL;
+	struct ckrm_sock_class *newcls = NULL;
+
+	// XXX - TBD ipv6
+	if (sk->sk_family == IPPROTO_IPV6)
+		return;
+
+	ns =  (struct ckrm_net_struct *)sk->sk_ns;
+	if (!ns) // listen_start called before socket_aq was loaded
+		return;
+
+	newcls = ns->core;
+	if (newcls) {
+		class_lock(class_core(newcls));
+		list_del(&ns->ckrm_link);
+		INIT_LIST_HEAD(&ns->ckrm_link);
+		ckrm_core_drop(class_core(newcls));
+		class_unlock(class_core(newcls));
+	}
+
+	// the socket is already locked
+	sk->sk_ns = NULL;
+	sock_put(sk);
+
+	// Should be the last count and free it
+	ckrm_ns_put(ns);
+	return;
+}
+
+static struct ckrm_event_spec sock_events_callbacks[] = {
+	CKRM_EVENT_SPEC( LISTEN_START, cb_sockclass_listen_start  ),
+	CKRM_EVENT_SPEC( LISTEN_STOP,  cb_sockclass_listen_stop  ),
+	{ -1 }
+};
+
+/**************************************************************************
+ *                  Class Object Creation / Destruction
+ **************************************************************************/
+
+static struct ckrm_core_class *
+sock_alloc_class(struct ckrm_core_class *parent, const char *name)
+{
+	struct ckrm_sock_class *sockcls;
+	sockcls = kmalloc(sizeof(struct ckrm_sock_class), GFP_KERNEL);
+	if (sockcls == NULL) 
+		return NULL;
+
+	ckrm_init_core_class(&CT_sockclass,class_core(sockcls),parent,name);
+
+	ce_protect(&CT_sockclass);
+	if (CT_sockclass.ce_cb_active && CT_sockclass.ce_callbacks.class_add)
+		(*CT_sockclass.ce_callbacks.class_add)(name,sockcls);
+	ce_release(&CT_sockclass);
+
+	return class_core(sockcls);
+}
+
+static int
+sock_free_class(struct ckrm_core_class *core)
+{
+	struct ckrm_sock_class *sockcls;
+
+	if (!ckrm_is_core_valid(core)) {
+		// Invalid core
+		return (-EINVAL);
+	}
+	if (core == core->classtype->default_class) {
+		// reset the name tag
+		core->name = dflt_sockclass_name;
+ 		return 0;
+	}
+
+	sockcls = class_type(struct ckrm_sock_class, core);
+
+	ce_protect(&CT_sockclass);
+
+	if (CT_sockclass.ce_cb_active && CT_sockclass.ce_callbacks.class_delete)
+		(*CT_sockclass.ce_callbacks.class_delete)(core->name,sockcls);
+
+	sock_reclassify_class ( sockcls );
+
+	ce_release(&CT_sockclass);
+
+	ckrm_release_core_class(core);  // Hubertus .... could just drop the class .. error message
+	return 0;
+}
+
+
+static int                      
+sock_show_members(struct ckrm_core_class *core, struct seq_file *seq) 
+{
+	struct list_head *lh;
+	struct ckrm_net_struct *ns = NULL;
+
+	class_lock(core);
+	list_for_each(lh, &core->objlist) {
+		ns = container_of(lh, struct ckrm_net_struct,ckrm_link);
+		seq_printf(seq, "%d.%d.%d.%d\\%d\n", 
+			   NIPQUAD(ns->ns_daddrv4),ns->ns_dport);
+	}
+	class_unlock(core);
+
+	return 0;
+}
+
+static int
+sock_forced_reclassify_ns(struct ckrm_net_struct *tns, struct ckrm_core_class *core)
+{
+	struct ckrm_net_struct *ns = NULL;
+	struct sock *sk = NULL;
+	struct ckrm_sock_class *oldcls, *newcls;
+	int rc = -EINVAL;
+
+	if (!ckrm_is_core_valid(core)) {
+		return rc;
+	}
+
+	newcls = class_type(struct ckrm_sock_class, core);
+	// lookup the listening sockets
+	// returns with a reference count set on socket
+	sk = tcp_v4_lookup_listener(tns->ns_daddrv4,tns->ns_dport,0);
+	if (!sk) {
+		printk(KERN_INFO "No such listener 0x%x:%d\n",
+				tns->ns_daddrv4, tns->ns_dport);
+		return rc;
+	}
+	lock_sock(sk);
+	if (!sk->sk_ns) {
+		goto out;
+	}
+	ns = sk->sk_ns;
+	ckrm_ns_hold(ns);
+	oldcls = ns->core;
+	if ((oldcls == NULL) || (oldcls == newcls)) {
+		ckrm_ns_put(ns);
+		goto out;
+	}
+
+	// remove the net_struct from the current class
+	class_lock(class_core(oldcls));
+	list_del(&ns->ckrm_link);
+	INIT_LIST_HEAD(&ns->ckrm_link);
+	ns->core = NULL;
+	class_unlock(class_core(oldcls));
+
+	sock_set_class(ns, newcls, oldcls, CKRM_EVENT_MANUAL);
+	ckrm_ns_put(ns);
+	rc = 0;
+out:
+	release_sock(sk);
+	sock_put(sk);
+
+	return rc;
+
+} 
+
+enum sock_target_token_t {
+        IPV4, IPV6, SOCKC_TARGET_ERR
+};
+
+static match_table_t sock_target_tokens = {
+	{IPV4, "ipv4=%s"},
+	{IPV6, "ipv6=%s"},
+        {SOCKC_TARGET_ERR, NULL},
+};
+
+char *
+v4toi(char *s, char c, __u32 *v)
+{
+	unsigned int  k = 0, n = 0;
+
+	while(*s && (*s != c)) {
+		if (*s == '.') {
+			n <<= 8;
+			n |= k;
+			k = 0;
+		}
+		else 
+			k = k *10 + *s - '0';
+		s++;
+	}
+
+	n <<= 8;
+	*v = n | k;
+
+	return s;
+}
+
+static int
+sock_forced_reclassify(struct ckrm_core_class *target,const char *options)
+{	
+	char *p,*p2;
+	struct ckrm_net_struct ns;
+	__u32 v4addr, tmp;
+
+	if (!options)
+		return 1;
+	
+	while ((p = strsep((char**)&options, ",")) != NULL) {
+		substring_t args[MAX_OPT_ARGS];
+		int token;
+		
+		if (!*p)
+			continue;
+		token = match_token(p, sock_target_tokens, args);
+		switch (token) {
+
+		case IPV4:
+
+			p2 = p;
+			while(*p2 && (*p2 != '='))
+				++p2;
+			p2++;
+			p2 = v4toi(p2, '\\',&(v4addr));
+			ns.ns_daddrv4 = htonl(v4addr);
+			ns.ns_family = 4; //IPPROTO_IPV4
+			p2 = v4toi(++p2, ':',&tmp); ns.ns_dport = (__u16)tmp;
+			p2 = v4toi(++p2,'\0',&ns.ns_pid);
+			
+			sock_forced_reclassify_ns(&ns,target);
+			break;
+
+		case IPV6:
+			printk(KERN_INFO "rcfs: IPV6 not supported yet\n");
+			return 0;	
+		default:
+			return 0;
+		}
+	}
+	return 1;
+}	
+
+/*
+ * Listen_aq reclassification.
+ */
+static void
+sock_reclassify_class(struct ckrm_sock_class *cls)
+{
+	struct ckrm_net_struct *ns, *tns;
+	struct ckrm_core_class *core = class_core(cls);
+	LIST_HEAD(local_list);
+
+	if (!cls)
+		return;
+
+	if (!ckrm_validate_and_grab_core(core))
+		return;
+
+	class_lock(core);
+	// we have the core refcnt
+	if (list_empty(&core->objlist)) {
+		class_unlock(core);
+		ckrm_core_drop(core);
+		return;
+	}
+
+	INIT_LIST_HEAD(&local_list);
+	list_splice_init(&core->objlist, &local_list);
+	class_unlock(core);
+	ckrm_core_drop(core);
+	
+	list_for_each_entry_safe(ns, tns, &local_list, ckrm_link) {
+		ckrm_ns_hold(ns);
+		list_del(&ns->ckrm_link);
+		if (ns->ns_sk) {
+			lock_sock(ns->ns_sk);
+			sock_set_class(ns, &sockclass_dflt_class, NULL, CKRM_EVENT_MANUAL);
+			release_sock(ns->ns_sk);
+		}
+		ckrm_ns_put(ns);
+	}
+	return ;
+}
+
+void __init
+ckrm_meta_init_sockclass(void)
+{
+	printk("...... Initializing ClassType<%s> ........\n",CT_sockclass.name);
+	// intialize the default class
+	ckrm_init_core_class(&CT_sockclass, class_core(&sockclass_dflt_class),
+			     NULL,dflt_sockclass_name);
+
+	// register classtype and initialize default task class
+	ckrm_register_classtype(&CT_sockclass);
+	ckrm_register_event_set(sock_events_callbacks);
+
+	// note registeration of all resource controllers will be done later dynamically 
+	// as these are specified as modules
+}
+
+
+
+#if 1
+
+/***************************************************************************************
+ * Debugging Network Classes:  Utility functions
+ **************************************************************************************/
+
+#endif
diff -Nru --exclude='*SCCS*' a/kernel/ckrm/ckrm_socketaq.c b/kernel/ckrm/ckrm_socketaq.c
--- a/kernel/ckrm/ckrm_socketaq.c	2004-04-26 12:35:37.000000000 -0400
+++ b/kernel/ckrm/ckrm_socketaq.c	1969-12-31 19:00:00.000000000 -0500
@@ -1,379 +0,0 @@
-/* ckrm_socketaq.c - Socket accept queue resource controller
- *
- * Copyright (C) Vivek Kashyap,      IBM Corp. 2004
- * 
- * Provides a Dummy Resource controller for CKRM
- *
- * Latest version, more details at http://ckrm.sf.net
- * 
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- *
- */
-
-/* Changes
- * Initial version
- */
-
-/* Code Description: TBD
- *
- */
-
-#include <linux/module.h>
-#include <linux/init.h>
-#include <linux/slab.h>
-#include <asm/errno.h>
-#include <linux/list.h>
-#include <linux/spinlock.h>
-#include <linux/ckrm.h>
-#include <linux/ckrm_rc.h>
-#include <net/tcp.h>
-#include <linux/rcfs.h>
-
-#define DEBUG_CKRM_SAQ 1
-
-#define SAQ_MAX_HIERARCHY 0 	/*the user is not allowed to create a subclass*/
-
-typedef struct ckrm_saq_res {
-	struct ckrm_hnode  hnode;    /* build our own hierarchy */
-	struct ckrm_shares shares;
-	struct ckrm_core_class *core;
-	int my_depth;
-} ckrm_saq_res_t;
-
-static int my_resid = -1;
-
-#define get_parent(res) (container_of((res)->hnode.parent,ckrm_saq_res_t,hnode))
-
-
-extern struct dentry * 
-rcfs_create_internal(struct dentry *parent, const char *name, int mfmode, int magic) ;
-
-
-/* Initialize rescls values
- */
-
-#if 0
-static void
-saq_res_initcls_zero(void *my_res)
-{
-	ckrm_saq_res_t *res = my_res;
-
-	res->shares.my_guarantee     = CKRM_SHARE_DONTCARE;
-	res->shares.my_limit         = CKRM_SHARE_DONTCARE;
-	res->shares.total_guarantee  = 0; 
-	res->shares.total_limit      = 0;
-	res->shares.unused_guarantee = 0;
-	res->shares.unused_limit     = 0;
-	
-	/* Don't initiate propagation to children here, caller will do it if needed */
-}
-#endif
-
-static void
-saq_res_initcls_one(void *my_res)
-{
-	ckrm_saq_res_t *res = my_res;
-
-	res->shares.my_guarantee     = CKRM_SHARE_DONTCARE;
-	res->shares.my_limit         = CKRM_SHARE_DONTCARE;
-	res->shares.total_guarantee  = CKRM_SHARE_DFLT_TOTAL_GUARANTEE;
-	res->shares.total_limit      = CKRM_SHARE_DFLT_TOTAL_LIMIT;
-	res->shares.unused_guarantee = CKRM_SHARE_DFLT_TOTAL_GUARANTEE;
-	res->shares.unused_limit     = CKRM_SHARE_DFLT_TOTAL_LIMIT;
-	
-	/* Don't initiate propagation to children here, caller will do it if needed */
-}
-static void
-saq_res_initcls(void *my_res)
-{
-	ckrm_saq_res_t *res = my_res;
-
-	res->shares.my_guarantee     = CKRM_SHARE_DONTCARE;
-	res->shares.my_limit         = CKRM_SHARE_DONTCARE;
-	res->shares.total_guarantee  = CKRM_SHARE_DFLT_TOTAL_GUARANTEE;
-	res->shares.total_limit      = CKRM_SHARE_DFLT_TOTAL_LIMIT;
-	res->shares.unused_guarantee = CKRM_SHARE_DFLT_TOTAL_GUARANTEE;
-	res->shares.unused_limit     = CKRM_SHARE_DFLT_TOTAL_LIMIT;
-	
-	/* Don't initiate propagation to children here, caller will do it if needed */
-}
-
-static void *
-saq_res_alloc(struct ckrm_core_class *core, struct ckrm_core_class *parent)
-{
-	ckrm_saq_res_t *res;
-
-	res = kmalloc(sizeof(ckrm_saq_res_t), GFP_KERNEL);
-	if (res) {
-		struct ckrm_hnode *parhnode = NULL;
-		ckrm_saq_res_t  *parres = NULL;
-
-		memset(res, 0, sizeof(res));
-
-		if (parent) {
-			parres = ckrm_get_res_class(parent,my_resid,
-							ckrm_saq_res_t);
-			printk(KERN_WARNING "net root %p %p %p\n",
-					&ckrm_net_root, parent, parres);
-			parhnode = &parres->hnode;
-		}
-
-		if (parhnode)
-			res->my_depth = parres->my_depth + 1;
-		else
-			res->my_depth = 1;
-
-		res->core = core;
-		ckrm_hnode_add(&res->hnode, parhnode);
-
-		// rescls in place, now initialize contents other than 
-		// hierarchy pointers
-		saq_res_initcls_one(res);
-	}
-	return res;
-}
-
-static void
-saq_res_free(void *my_res)
-{
-	ckrm_saq_res_t *res = my_res;
-	ckrm_saq_res_t *parent;
-
-	if (!res) 
-		return;
-
-	parent = get_parent(res);
-	ckrm_hnode_remove(&res->hnode);
-
-	// return child's limit/guarantee to parent node
-	if (parent) {
-		if (res->shares.my_guarantee >= 0)
-			parent->shares.unused_guarantee += res->shares.my_guarantee;
-		if (res->shares.my_limit >= 0)
-			parent->shares.unused_limit += res->shares.my_limit;
-	}
-	kfree(res);
-	return;
-}
-
-int
-saq_set_aq_values(ckrm_saq_res_t *my_res, struct ckrm_shares *shares)
-{
-
-	int i = 0; 
-	int j;
-	char name[4];
-	struct dentry *parent = my_res->core->dentry;
-	struct dentry *sp = NULL;
-
-#ifdef CONFIG_ACCEPT_QUEUES
-	int cnt = NUM_ACCEPT_QUEUES;
-#else
-	int cnt = 8;
-#endif
-	printk(KERN_WARNING "setting values for core %p empty %d %d\n",
-			my_res, list_empty(&my_res->hnode.children),
-			my_res->my_depth);
-
-	if ((my_res->my_depth == 2) && list_empty(&my_res->hnode.children)) {
-		for( i = 1; i < cnt; i++) {
-			j = sprintf(name, "%d",i);
-			name[j] = '\0';
-			// Now create all the socket accept queue 
-			// subdirectories and their share files.
-			sp = rcfs_create_internal(parent,name, 
-					parent->d_inode->i_mode,0);
-			rcfs_make_core(sp, my_res->core);
-			(void)rcfs_create_internal(sp, "shares",
-			   S_IFREG | S_IRUSR | S_IWUSR | S_IRGRP | S_IROTH,0);
-			(void)rcfs_create_internal(sp, "stats",
-			   S_IFREG | S_IRUSR | S_IWUSR | S_IRGRP | S_IROTH,0);
-		}
-	}
-
-	return 0;
-	/*
- 	* Set the shares
-	*/
-	// USEME
-	// for the recalculated shares
-	// 	find each socket in members
-	//		update its accept queue weights
-	//
-}
-
-
-
-static int
-saq_set_share_values(void *my_res, struct ckrm_shares *shares)
-{
-	ckrm_saq_res_t *res = my_res;
-	ckrm_saq_res_t *parent;
-	int reduce_by;
-	int rc = EINVAL;
-
-	if (!res) 
-		return -EINVAL;
-
-	parent = get_parent(res);
-
-	// we have to ensure that the set of parameters is OK
-	// ensure that lim/guarantees are ok wrt to parent total values 
-	// don't have to consider negative special values
-
-	if (parent) {
-		if ((shares->my_guarantee > parent->shares.unused_guarantee) ||
-		     (shares->my_limit > parent->shares.unused_limit))
-			goto set_share_err;
-	}
-
-	// translate UNCHANGED to existing values
-	if (shares->total_guarantee == CKRM_SHARE_UNCHANGED)
-		shares->total_guarantee = res->shares.total_guarantee;
-	if (shares->total_limit == CKRM_SHARE_UNCHANGED)
-		shares->total_limit = res->shares.total_limit;
-
-	// we don't allow DONTCARE for totals
-	if ((shares->total_guarantee <= CKRM_SHARE_DONTCARE) || 
-			(shares->total_limit <= CKRM_SHARE_DONTCARE))
-		goto set_share_err;
-
-	// check whether total shares exceeds sum of children(total - unused)
-	if (((reduce_by = shares->total_guarantee - res->shares.total_guarantee) > 0) &&
-	    (reduce_by > res->shares.unused_guarantee))
-		goto set_share_err;
-	if (((reduce_by = shares->total_limit - res->shares.total_limit) > 0) &&
-	    (reduce_by > res->shares.unused_limit))
-		goto set_share_err;
-
-
-	/* Need more sanity checks (first two not being enforced currently):
-	 *  1. guarantee < limit
-         *  2. my_* < tot_*
-	 *  etc. Do later.
-	 */
-	// values are OK now enforce them 
-
-	if (shares->my_guarantee > 0) {
-		if (parent) { 
-			parent->shares.unused_guarantee -= shares->my_guarantee;
-			if (res->shares.my_guarantee >= 0)
-				parent->shares.unused_guarantee += 
-						res->shares.my_guarantee;
-		}
-		res->shares.my_guarantee = shares->my_guarantee;
-	} else if (shares->my_guarantee == CKRM_SHARE_DONTCARE) {
-		if (parent) 
-			parent->shares.unused_guarantee += 
-						res->shares.my_guarantee;
-		res->shares.my_guarantee = CKRM_SHARE_DONTCARE;
-	}
-
-	if (shares->my_limit > 0) {
-		if (parent) { 
-			parent->shares.unused_limit -= shares->my_limit;
-			if (res->shares.my_limit >= 0)
-				parent->shares.unused_limit += res->shares.my_limit;
-		}
-		res->shares.my_limit = shares->my_limit;
-	} else if (shares->my_limit == CKRM_SHARE_DONTCARE) {
-		if (parent) 
-			parent->shares.unused_limit += res->shares.my_limit;
-		res->shares.my_limit = CKRM_SHARE_DONTCARE;
-	}
-
-
-	res->shares.unused_guarantee += 
-			(shares->total_guarantee - res->shares.total_guarantee);
-	res->shares.unused_limit  += 
-			(shares->total_limit     - res->shares.total_limit);
-
-	res->shares.total_guarantee  = shares->total_guarantee;
-	res->shares.total_limit      = shares->total_limit;
-
-	/* Here we should force the propagation of share values */
-	rc = saq_set_aq_values(res,shares);
-	goto out;
-
-set_share_err:
-#ifdef DEBUG_CKRM_DUMMY
-	saq_res_initcls_one(res);
-	rc = 0;
-#endif
-out:
-	
-	return rc;
-}
-
-static int
-saq_get_share_values(void *my_res, struct ckrm_shares *shares)
-{
-	ckrm_saq_res_t *res = my_res;
-
-	if (!res) 
-		return -EINVAL;
-	*shares = res->shares;
-	return 0;
-}
-
-static int  
-saq_get_stats(void *my_res, struct seq_file *sfile)
-{
-	ckrm_saq_res_t *res = my_res;
-
-	if (!res) 
-		return -EINVAL;
-
-	// TODO
-	// If have children sum up the values are return.
-	// Otherwise update values from each of the sockets - as in the 
-	// ns list and update the shares. Then print them.
-	seq_printf(sfile, "res=saq: these are my stats <none>\n");
-
-	return 0;
-}
-
-static void
-saq_change_resclass(void *tsk, void *old, void *new)
-{
-	// need to move from olde core class to new core class
-	return;
-}
-
-ckrm_res_callback_t saq_rcbs = {
-	.res_name          = "saq",
-	.res_type	   = CKRM_NET_CLASS,
-	.resid		   = CKRM_RES_SAQ,
-	.res_hdepth        = SAQ_MAX_HIERARCHY,
-	.res_alloc         = saq_res_alloc,
-	.res_free          = saq_res_free,
-	.set_share_values  = saq_set_share_values,
-	.get_share_values  = saq_get_share_values,
-	.get_stats         = saq_get_stats,
-	.change_resclass   = saq_change_resclass,
-	.res_initcls       = saq_res_initcls,
-};
-
-int __init
-init_ckrm_saq_res(void)
-{
-	my_resid = ckrm_register_res_ctlr(&saq_rcbs);
-	printk(KERN_INFO "SAQ resid is %d\n",my_resid);
-	return 0;
-}	
-
-void __exit
-exit_ckrm_saq_res(void)
-{
-	ckrm_unregister_res_ctlr(my_resid);
-}
-
-
-module_init(init_ckrm_saq_res)
-module_exit(exit_ckrm_saq_res)
-
-MODULE_LICENSE("GPL");
-
diff -Nru --exclude='*SCCS*' a/kernel/ckrm/ckrmstub.c b/kernel/ckrm/ckrmstub.c
--- a/kernel/ckrm/ckrmstub.c	2004-04-26 12:35:35.000000000 -0400
+++ b/kernel/ckrm/ckrmstub.c	1969-12-31 19:00:00.000000000 -0500
@@ -1,30 +0,0 @@
-/* ckrmstub.c - Stub for Class-based Kernel Resource Management (CKRM)
- *
- * Copyright (C) Chandra Seetharaman,  IBM Corp. 2003
- * 
- * Provides system call stub for the CKRM system calls.
- *
- * Latest version, more details at http://ckrm.sf.net
- * 
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- *
- */
-
-/* Changes
- * 
- * 06 Nov 2003
- *        Created
- */
-
-#include <linux/linkage.h>
-#include <asm/errno.h>
-
-asmlinkage int
-sys_res_ctrl(unsigned int op, void *data)
-{
-	return -ENOSYS;
-}
-
diff -Nru --exclude='*SCCS*' a/kernel/ckrm/ckrm_tasks.c b/kernel/ckrm/ckrm_tasks.c
--- a/kernel/ckrm/ckrm_tasks.c	1969-12-31 19:00:00.000000000 -0500
+++ b/kernel/ckrm/ckrm_tasks.c	2004-04-28 22:57:42.000000000 -0400
@@ -0,0 +1,509 @@
+/* ckrm_numtasks.c - "Number of tasks" resource controller for CKRM
+ *
+ * Copyright (C) Chandra Seetharaman,  IBM Corp. 2003
+ * 
+ * Latest version, more details at http://ckrm.sf.net
+ * 
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ */
+
+/* Changes
+ * 
+ * 31 Mar 2004: Created
+ * 
+ */
+
+/*
+ * Code Description: TBD
+ */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/slab.h>
+#include <asm/errno.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/ckrm.h>
+#include <linux/ckrm_rc.h>
+#include <linux/ckrm_tc.h>
+
+#define TOTAL_NUM_TASKS (131072) // 128 K
+#define NUMTASKS_DEBUG
+#define NUMTASKS_NAME "numtasks"
+
+typedef struct ckrm_numtasks {
+	struct ckrm_core_class *core; // the core i am part of...
+	struct ckrm_core_class *parent; // parent of the core above.
+	struct ckrm_shares shares;
+	spinlock_t cnt_lock; // always grab parent's lock first and then child's
+	int cnt_guarantee; // num_tasks guarantee in local units
+	int cnt_unused; // has to borrow if more than this is needed
+	int cnt_limit; // no tasks over this limit.
+	atomic_t cnt_cur_alloc; // current alloc from self
+	atomic_t cnt_borrowed; // borrowed from the parent
+
+	int over_guarantee; //turn on/off when cur_alloc goes over/under guarantee
+
+	// internally maintained statictics to compare with max numbers
+	int limit_failures; // no. of failures 'cause the request was over the limit
+	int borrow_sucesses; // no. of successful borrows
+	int borrow_failures; // no. of borrow faileures
+
+	// Maximum the specific statictics has reached.
+	int max_limit_failures;
+	int max_borrow_sucesses;
+	int max_borrow_failures;
+
+	// Total number of specific statistics
+	int tot_limit_failures;
+	int tot_borrow_sucesses;
+	int tot_borrow_failures;
+} ckrm_numtasks_t;
+
+struct ckrm_res_ctlr numtasks_rcbs;
+
+/* Initialize rescls values
+ * May be called on each rcfs unmount or as part of error recovery
+ * to make share values sane.
+ * Does not traverse hierarchy reinitializing children.
+ */
+static void
+numtasks_res_initcls_one(ckrm_numtasks_t *res)
+{
+	res->shares.my_guarantee     = CKRM_SHARE_DONTCARE;
+	res->shares.my_limit         = CKRM_SHARE_DONTCARE;
+	res->shares.total_guarantee  = CKRM_SHARE_DFLT_TOTAL_GUARANTEE;
+	res->shares.max_limit        = CKRM_SHARE_DFLT_MAX_LIMIT;
+	res->shares.unused_guarantee = CKRM_SHARE_DFLT_TOTAL_GUARANTEE;
+	res->shares.cur_max_limit    = 0;
+
+	res->cnt_guarantee           = CKRM_SHARE_DONTCARE;
+	res->cnt_unused              = CKRM_SHARE_DONTCARE;
+	res->cnt_limit               = CKRM_SHARE_DONTCARE;
+
+	res->over_guarantee          = 0;
+
+	res->limit_failures          = 0;
+	res->borrow_sucesses         = 0;
+	res->borrow_failures         = 0;
+
+	res->max_limit_failures      = 0;
+	res->max_borrow_sucesses     = 0;
+	res->max_borrow_failures     = 0;
+
+	res->tot_limit_failures      = 0;
+	res->tot_borrow_sucesses     = 0;
+	res->tot_borrow_failures     = 0;
+
+	atomic_set(&res->cnt_cur_alloc, 0);
+	atomic_set(&res->cnt_borrowed, 0);
+	return;
+}
+
+#if 0	
+static void
+numtasks_res_initcls(void *my_res)
+{
+	ckrm_numtasks_t *res = my_res;
+
+	/* Write a version which propagates values all the way down 
+	   and replace rcbs callback with that version */
+	
+}
+#endif
+
+int
+numtasks_get_ref(void *arg, int force)
+{
+	int rc, resid = numtasks_rcbs.resid;
+	ckrm_numtasks_t *res;
+	ckrm_core_class_t *core = arg;
+
+	if ((resid < 0) || (core == NULL))
+		return 1;
+
+	res = ckrm_get_res_class(core, resid, ckrm_numtasks_t);
+	if (res == NULL) 
+		return 1;
+
+	atomic_inc(&res->cnt_cur_alloc);
+
+	rc = 1;
+	if (((res->parent) && (res->cnt_unused == CKRM_SHARE_DONTCARE)) ||
+			(atomic_read(&res->cnt_cur_alloc) > res->cnt_unused)) {
+
+		rc = 0;
+		if (!force && (res->cnt_limit != CKRM_SHARE_DONTCARE) && 
+				(atomic_read(&res->cnt_cur_alloc) > res->cnt_limit)) {
+			res->limit_failures++;
+			res->tot_limit_failures++;
+		} else if (res->parent != NULL) {
+			if ((rc = numtasks_get_ref(res->parent, force)) == 1) {
+				atomic_inc(&res->cnt_borrowed);
+				res->borrow_sucesses++;
+				res->tot_borrow_sucesses++;
+				res->over_guarantee = 1;
+			} else {
+				res->borrow_failures++;
+				res->tot_borrow_failures++;
+			}
+		} else {
+			rc = force;
+		}
+	} else if (res->over_guarantee) {
+		res->over_guarantee = 0;
+
+		if (res->max_limit_failures < res->limit_failures) {
+			res->max_limit_failures = res->limit_failures;
+		}
+		if (res->max_borrow_sucesses < res->borrow_sucesses) {
+			res->max_borrow_sucesses = res->borrow_sucesses;
+		}
+		if (res->max_borrow_failures < res->borrow_failures) {
+			res->max_borrow_failures = res->borrow_failures;
+		}
+		res->limit_failures = 0;
+		res->borrow_sucesses = 0;
+		res->borrow_failures = 0;
+	}
+
+	if (!rc) {
+		atomic_dec(&res->cnt_cur_alloc);
+	}
+	return rc;
+}
+
+void
+numtasks_put_ref(void *arg)
+{
+	int resid = numtasks_rcbs.resid;
+	ckrm_numtasks_t *res;
+	ckrm_core_class_t *core = arg;
+
+	if ((resid == -1) || (core == NULL)) {
+		return;
+	}
+
+	res = ckrm_get_res_class(core, resid, ckrm_numtasks_t);
+	if (res == NULL) 
+		return;
+	atomic_dec(&res->cnt_cur_alloc);
+	if (atomic_read(&res->cnt_borrowed) > 0) {
+		atomic_dec(&res->cnt_borrowed);
+		numtasks_put_ref(res->parent);
+	}
+	return;
+}
+
+static void *
+numtasks_res_alloc(struct ckrm_core_class *core, struct ckrm_core_class *parent)
+{
+	ckrm_numtasks_t *res;
+	
+	res = kmalloc(sizeof(ckrm_numtasks_t), GFP_ATOMIC);
+	
+	if (res) {
+		res->core = core;
+		res->parent = parent;
+		numtasks_res_initcls_one(res);
+		res->cnt_lock = SPIN_LOCK_UNLOCKED;
+		if (parent == NULL) {
+			// I am part of root class. so set the max tasks to available
+			// default
+			res->cnt_guarantee = TOTAL_NUM_TASKS;
+			res->cnt_unused =  TOTAL_NUM_TASKS;
+			res->cnt_limit = TOTAL_NUM_TASKS;
+		}
+	} else {
+		printk(KERN_ERR "numtasks_res_alloc: failed GFP_ATOMIC alloc\n");
+	}
+	return res;
+}
+
+/*
+ * No locking of this resource class object necessary as we are not
+ * supposed to be assigned (or used) when/after this function is called.
+ */
+static void
+numtasks_res_free(void *my_res)
+{
+	ckrm_numtasks_t *res = my_res, *parres, *childres;
+	ckrm_core_class_t *child = NULL;
+	int i, borrowed, maxlimit, resid = numtasks_rcbs.resid;
+
+	if (!res) 
+		return;
+
+	// Assuming there will be no children when this function is called
+	
+	parres = ckrm_get_res_class(res->parent, resid, ckrm_numtasks_t);
+
+	if (unlikely(atomic_read(&res->cnt_cur_alloc) != 0 ||
+				atomic_read(&res->cnt_borrowed))) {
+		printk(KERN_ERR "numtasks_res_free: resource still alloc'd %p\n", res);
+		if ((borrowed = atomic_read(&res->cnt_borrowed)) > 0) {
+			for (i = 0; i < borrowed; i++) {
+				numtasks_put_ref(parres->core);
+			}
+		}
+	}
+
+	// return child's limit/guarantee to parent node
+	spin_lock(&parres->cnt_lock);
+	child_guarantee_changed(&parres->shares, res->shares.my_guarantee, 0);
+
+	// run thru parent's children and get the new max_limit of the parent
+	ckrm_lock_hier(parres->core);
+	maxlimit = 0;
+	while ((child = ckrm_get_next_child(parres->core, child)) != NULL) {
+		childres = ckrm_get_res_class(child, resid, ckrm_numtasks_t);
+		if (maxlimit < childres->shares.my_limit) {
+			maxlimit = childres->shares.my_limit;
+		}
+	}
+	ckrm_unlock_hier(parres->core);
+	if (parres->shares.cur_max_limit < maxlimit) {
+		parres->shares.cur_max_limit = maxlimit;
+	}
+
+	spin_unlock(&parres->cnt_lock);
+	kfree(res);
+	return;
+}
+/*
+ * Recalculate the guarantee and limit in real units... and propagate the
+ * same to children.
+ * Caller is responsible for protecting res and for the integrity of parres
+ */
+static void
+recalc_and_propagate(ckrm_numtasks_t *res, ckrm_numtasks_t *parres)
+{
+	ckrm_core_class_t *child = NULL;
+	ckrm_numtasks_t *childres;
+	int resid = numtasks_rcbs.resid;
+
+	if (parres) {
+		struct ckrm_shares *par = &parres->shares;
+		struct ckrm_shares *self = &res->shares;
+
+		// calculate cnt_guarantee and cnt_limit
+		//
+		if (parres->cnt_guarantee == CKRM_SHARE_DONTCARE) {
+			res->cnt_guarantee = CKRM_SHARE_DONTCARE;
+		} else {
+			res->cnt_guarantee = (self->my_guarantee * parres->cnt_guarantee) 
+					/ par->total_guarantee;
+		}
+		if (parres->cnt_limit == CKRM_SHARE_DONTCARE) {
+			res->cnt_limit = CKRM_SHARE_DONTCARE;
+		} else {
+			res->cnt_limit = (self->my_limit * parres->cnt_limit)
+					/ par->max_limit;
+		}
+
+		// Calculate unused units
+		if (res->cnt_guarantee == CKRM_SHARE_DONTCARE) {
+			res->cnt_unused = CKRM_SHARE_DONTCARE;
+		} else {
+			res->cnt_unused = (self->unused_guarantee *
+					res->cnt_guarantee) / self->total_guarantee;
+		}
+	}
+
+	// propagate to children
+	ckrm_lock_hier(res->core);
+	while ((child = ckrm_get_next_child(res->core, child)) != NULL) {
+		childres = ckrm_get_res_class(child, resid, ckrm_numtasks_t);
+
+		spin_lock(&childres->cnt_lock);
+		recalc_and_propagate(childres, res);
+		spin_unlock(&childres->cnt_lock);
+	}
+	ckrm_unlock_hier(res->core);
+	return;
+}
+
+static int
+numtasks_set_share_values(void *my_res, struct ckrm_shares *new)
+{
+	ckrm_numtasks_t *parres, *res = my_res;
+	struct ckrm_shares *cur = &res->shares, *par;
+	int rc = -EINVAL, resid = numtasks_rcbs.resid;
+
+	if (!res) 
+		return rc;
+
+	if (res->parent) {
+		parres = ckrm_get_res_class(res->parent, resid, ckrm_numtasks_t);
+		spin_lock(&parres->cnt_lock);
+		spin_lock(&res->cnt_lock);
+		par = &parres->shares;
+	} else {
+		spin_lock(&res->cnt_lock);
+		par = NULL;
+		parres = NULL;
+	}
+
+	rc = set_shares(new, cur, par);
+
+	if ((rc == 0) && parres) {
+		// Calculate parent's unused units
+		if (parres->cnt_guarantee == CKRM_SHARE_DONTCARE) {
+			parres->cnt_unused = CKRM_SHARE_DONTCARE;
+		} else {
+			parres->cnt_unused = (par->unused_guarantee *
+					parres->cnt_guarantee) / par->total_guarantee;
+		}
+
+		recalc_and_propagate(res, parres);
+	}
+	spin_unlock(&res->cnt_lock);
+	if (res->parent) {
+		spin_unlock(&parres->cnt_lock);
+	}
+	return rc;
+}
+
+
+static int
+numtasks_get_share_values(void *my_res, struct ckrm_shares *shares)
+{
+	ckrm_numtasks_t *res = my_res;
+
+	if (!res) 
+		return -EINVAL;
+	*shares = res->shares;
+	return 0;
+}
+
+static int  
+numtasks_get_stats(void *my_res, struct seq_file *sfile)
+{
+	ckrm_numtasks_t *res = my_res;
+
+	if (!res) 
+		return -EINVAL;
+
+	seq_printf(sfile, "Number of tasks resource:\n");
+	seq_printf(sfile, "Total Over limit failures: %d\n",
+			res->tot_limit_failures);
+	seq_printf(sfile, "Total Over guarantee sucesses: %d\n",
+			res->tot_borrow_sucesses);
+	seq_printf(sfile, "Total Over guarantee failures: %d\n",
+			res->tot_borrow_failures);
+
+	seq_printf(sfile, "Maximum Over limit failures: %d\n",
+			res->max_limit_failures);
+	seq_printf(sfile, "Maximum Over guarantee sucesses: %d\n",
+			res->max_borrow_sucesses);
+	seq_printf(sfile, "Maximum Over guarantee failures: %d\n",
+			res->max_borrow_failures);
+#ifdef NUMTASKS_DEBUG
+	seq_printf(sfile, "cur_alloc %d; borrowed %d; cnt_guar %d; cnt_limit %d "
+			"unused_guarantee %d, cur_max_limit %d\n",
+			atomic_read(&res->cnt_cur_alloc),
+			atomic_read(&res->cnt_borrowed),
+			res->cnt_guarantee,
+			res->cnt_limit,
+			res->shares.unused_guarantee,
+			res->shares.cur_max_limit);
+#endif
+
+	return 0;
+}
+
+static int  
+numtasks_show_config(void *my_res, struct seq_file *sfile)
+{
+	ckrm_numtasks_t *res = my_res;
+
+	if (!res) 
+		return -EINVAL;
+
+	seq_printf(sfile, "res=%s,parameter=somevalue\n",NUMTASKS_NAME);
+	return 0;
+}
+
+static int  
+numtasks_set_config(void *my_res, const char *cfgstr)
+{
+	ckrm_numtasks_t *res = my_res;
+
+	if (!res) 
+		return -EINVAL;
+	printk("numtasks config='%s'\n",cfgstr);
+	return 0;
+}
+
+static void
+numtasks_change_resclass(void *task, void *old, void *new)
+{
+	ckrm_numtasks_t *oldres = old;
+	ckrm_numtasks_t *newres = new;
+
+	if (oldres != (void *) -1) {
+		struct task_struct *tsk = task;
+		if (!oldres) {
+			struct ckrm_core_class *old_core = &(tsk->parent->taskclass->core);
+			oldres = ckrm_get_res_class(old_core, numtasks_rcbs.resid,
+					ckrm_numtasks_t);
+		}
+		numtasks_put_ref(oldres->core);
+	}
+	if (newres) {
+		(void) numtasks_get_ref(newres->core, 1);
+	}
+}
+
+struct ckrm_res_ctlr numtasks_rcbs = {
+	.res_name          = NUMTASKS_NAME,
+	.res_hdepth        = 1,
+	.resid             = -1,
+	.res_alloc         = numtasks_res_alloc,
+	.res_free          = numtasks_res_free,
+	.set_share_values  = numtasks_set_share_values,
+	.get_share_values  = numtasks_get_share_values,
+	.get_stats         = numtasks_get_stats,
+	.show_config       = numtasks_show_config,
+	.set_config        = numtasks_set_config,
+	.change_resclass   = numtasks_change_resclass,
+};
+
+int __init
+init_ckrm_numtasks_res(void)
+{
+	struct ckrm_classtype *clstype;
+	int resid = numtasks_rcbs.resid;
+
+	clstype = ckrm_find_classtype_by_name("taskclass");
+	if (clstype == NULL) {
+		printk(KERN_INFO " Unknown ckrm classtype<taskclass>");
+		return -ENOENT;
+	}
+
+	if (resid == -1) {
+		resid = ckrm_register_res_ctlr(clstype,&numtasks_rcbs);
+		printk("........init_ckrm_numtasks_res -> %d\n",resid);
+	}
+	return 0;
+}	
+
+void __exit
+exit_ckrm_numtasks_res(void)
+{
+	ckrm_unregister_res_ctlr(&numtasks_rcbs);
+	numtasks_rcbs.resid = -1;
+}
+
+module_init(init_ckrm_numtasks_res)
+module_exit(exit_ckrm_numtasks_res)
+
+EXPORT_SYMBOL(numtasks_get_ref);
+EXPORT_SYMBOL(numtasks_put_ref);
+
+MODULE_LICENSE("GPL");
+
diff -Nru --exclude='*SCCS*' a/kernel/ckrm/ckrm_tc.c b/kernel/ckrm/ckrm_tc.c
--- a/kernel/ckrm/ckrm_tc.c	1969-12-31 19:00:00.000000000 -0500
+++ b/kernel/ckrm/ckrm_tc.c	2004-04-28 22:57:42.000000000 -0400
@@ -0,0 +1,770 @@
+/* ckrm_tc.c - Class-based Kernel Resource Management (CKRM)
+ *
+ * Copyright (C) Hubertus Franke, IBM Corp. 2003,2004
+ *           (C) Shailabh Nagar,  IBM Corp. 2003
+ *           (C) Chandra Seetharaman,  IBM Corp. 2003
+ *	     (C) Vivek Kashyap,	IBM Corp. 2004
+ * 
+ * 
+ * Provides kernel API of CKRM for in-kernel,per-resource controllers 
+ * (one each for cpu, memory, io, network) and callbacks for 
+ * classification modules.
+ *
+ * Latest version, more details at http://ckrm.sf.net
+ * 
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ */
+
+/* Changes
+ *
+ * 28 Aug 2003
+ *        Created.
+ * 06 Nov 2003
+ *        Made modifications to suit the new RBCE module.
+ * 10 Nov 2003
+ *        Fixed a bug in fork and exit callbacks. Added callbacks_active and
+ *        surrounding logic. Added task paramter for all CE callbacks.
+ * 23 Mar 2004
+ *        moved to referenced counted class objects and correct locking
+ * 12 Apr 2004
+ *        introduced adopted to emerging classtype interface
+ */
+
+#include <linux/config.h>
+#include <linux/init.h>
+#include <linux/linkage.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <asm/uaccess.h>
+#include <linux/mm.h>
+#include <asm/errno.h>
+#include <linux/string.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/module.h>
+#include <linux/ckrm_rc.h>
+
+#include <linux/ckrm_tc.h>
+
+static struct ckrm_task_class  taskclass_dflt_class = {
+};
+
+const char *dflt_taskclass_name = TASK_CLASS_TYPE_NAME;
+
+static struct ckrm_core_class *ckrm_alloc_task_class(struct ckrm_core_class *parent, const char *name);
+static int ckrm_free_task_class(struct ckrm_core_class *core);
+
+static int  tc_forced_reclassify(ckrm_core_class_t *target, const char *resname);
+static int  tc_show_members(struct ckrm_core_class *core, struct seq_file *seq);
+static void tc_add_resctrl(struct ckrm_core_class *core, int resid);
+
+struct ckrm_classtype CT_taskclass = {
+	.mfidx          = TC_MF_IDX,
+	.name           = TASK_CLASS_TYPE_NAME,
+	.typeID         = CKRM_CLASSTYPE_TASK_CLASS, 
+	.maxdepth       = 3,                           // Hubertus .. just to start 
+	.resid_reserved = 4,                           // Hubertus .. reservation
+	.max_res_ctlrs  = CKRM_MAX_RES_CTLRS,        
+	.max_resid      = 0,
+	.bit_res_ctlrs  = 0L,
+	.res_ctlrs_lock = SPIN_LOCK_UNLOCKED,
+	.classes        = LIST_HEAD_INIT(CT_taskclass.classes),
+
+	.default_class  = &taskclass_dflt_class.core,
+	
+	// private version of functions 
+	.alloc          = &ckrm_alloc_task_class,
+	.free           = &ckrm_free_task_class,
+	.show_members   = &tc_show_members,
+	.forced_reclassify = &tc_forced_reclassify,
+
+	// use of default functions 
+	.show_shares    = &ckrm_class_show_shares,
+	.show_stats     = &ckrm_class_show_stats,
+	.show_config    = &ckrm_class_show_config,
+	.set_config     = &ckrm_class_set_config,
+	.set_shares     = &ckrm_class_set_shares,
+	.reset_stats    = &ckrm_class_reset_stats,
+
+	// mandatory private version .. no dflt available
+	.add_resctrl    = &tc_add_resctrl,	
+};
+
+/**************************************************************************
+ *                   Helper Functions                                     *
+ **************************************************************************/
+
+static inline void
+ckrm_init_task_lock(struct task_struct *tsk)
+{
+	tsk->ckrm_tsklock = SPIN_LOCK_UNLOCKED;
+}
+
+// Hubertus .. following functions should move to ckrm_rc.h
+
+static inline void
+ckrm_task_lock(struct task_struct *tsk)
+{
+  	spin_lock(&tsk->ckrm_tsklock);
+}
+
+static inline void
+ckrm_task_unlock(struct task_struct *tsk)
+{
+  	spin_unlock(&tsk->ckrm_tsklock);
+}
+
+/*
+ * Change the task class of the given task.
+ *
+ * Change the task's task class  to "newcls" if the task's current 
+ * class (task->taskclass) is same as given "oldcls", if it is non-NULL.
+ *
+ * Caller is responsible to make sure the task structure stays put through
+ * this function.
+ *
+ * This function should be called with the following locks NOT held
+ * 	- tsk->ckrm_task_lock
+ * 	- core->ckrm_lock, if core is NULL then ckrm_dflt_class.ckrm_lock
+ * 	- tsk->taskclass->ckrm_lock 
+ * 
+ * Function is also called with a ckrm_core_grab on the new core, hence
+ * it needs to be dropped if no assignment takes place.
+ */
+
+static void
+ckrm_set_taskclass(struct task_struct *tsk, ckrm_task_class_t *newcls, 
+		   ckrm_task_class_t *oldcls, enum ckrm_event event)
+{
+	int i;
+	ckrm_classtype_t  *clstype;
+	ckrm_res_ctlr_t   *rcbs;
+	ckrm_task_class_t *curcls;
+	void *old_res_class, *new_res_class;
+	int drop_old_cls;
+
+	ckrm_task_lock(tsk);
+	curcls = tsk->taskclass;
+
+	// check whether compare_and_exchange should
+	if (oldcls && (oldcls != curcls)) {
+		ckrm_task_unlock(tsk);
+		if (newcls) {
+			/* compensate for previous grab */
+			printk("ckrm_set_taskclass(%s:%d): Race-condition caught <%s> %d\n",
+				tsk->comm,tsk->pid,class_core(newcls)->name,event);
+			ckrm_core_drop(class_core(newcls));
+		}
+		return;
+	}
+
+	// make sure we have a real destination core
+	if (!newcls) {
+		newcls = &taskclass_dflt_class;
+		ckrm_core_grab(class_core(newcls));
+	}
+
+	// take out of old class 
+	// remember that we need to drop the oldcore
+	if ((drop_old_cls = (curcls != NULL))) {
+		class_lock(class_core(curcls));
+		if (newcls == curcls) {
+			// we are already in the destination class.
+			// we still need to drop oldcore
+			class_unlock(class_core(curcls));
+			ckrm_task_unlock(tsk);
+			goto out;
+		}
+		list_del(&tsk->taskclass_link);
+		INIT_LIST_HEAD(&tsk->taskclass_link);
+		tsk->taskclass = NULL;
+		class_unlock(class_core(curcls));
+	}	
+
+	// put into new class 
+	class_lock(class_core(newcls));
+	tsk->taskclass = newcls;
+	list_add(&tsk->taskclass_link, &class_core(newcls)->objlist);
+	class_unlock(class_core(newcls));
+
+	if (newcls == curcls) {
+		ckrm_task_unlock(tsk);
+		goto out;
+	}
+
+	CE_NOTIFY(&CT_taskclass,event,newcls,tsk);
+
+	ckrm_task_unlock(tsk);
+
+	clstype = class_isa(newcls);                      // Hubertus .. can hardcode ckrm_CT_taskclass
+	for (i = 0; i < clstype->max_resid; i++) {
+		atomic_inc(&clstype->nr_resusers[i]);
+		old_res_class = curcls ? class_core(curcls)->res_class[i] : NULL;
+		new_res_class = newcls ? class_core(newcls)->res_class[i] : NULL;
+		rcbs = clstype->res_ctlrs[i];
+		if (rcbs && rcbs->change_resclass && (old_res_class != new_res_class)) 
+			(*rcbs->change_resclass)(tsk, old_res_class, new_res_class);
+		atomic_dec(&clstype->nr_resusers[i]);
+	}
+
+ out:
+	if (drop_old_cls) 
+		ckrm_core_drop(class_core(curcls));
+	return;
+}
+
+// HF SUGGEST: we could macro-tize this for other types DEF_FUNC_ADD_RESCTRL(funcname,link)
+//          would DEF_FUNC_ADD_RESCTRL(tc_add_resctrl,taskclass_link)
+
+static void
+tc_add_resctrl(struct ckrm_core_class *core, int resid)
+{
+	struct task_struct *tsk;
+	struct ckrm_res_ctlr *rcbs;
+
+	if ((resid < 0) || (resid >= CKRM_MAX_RES_CTLRS) || ((rcbs = core->classtype->res_ctlrs[resid]) == NULL)) 
+		return;
+
+	spin_lock(&core->ckrm_lock);
+	list_for_each_entry(tsk, &core->objlist, taskclass_link) {
+		if (rcbs->change_resclass)
+			(*rcbs->change_resclass)(tsk, (void *) -1, core->res_class[resid]);
+	}
+	spin_unlock(&core->ckrm_lock);
+}
+
+
+/**************************************************************************
+ *                   Functions called from classification points          *
+ **************************************************************************/
+
+#define ECB_PRINTK(fmt, args...) // do { if (CT_taskclass.ce_regd) printk("%s: " fmt, __FUNCTION__ , ## args); } while (0)
+
+#define CE_CLASSIFY_TASK(event, tsk)						\
+do {										\
+	struct ckrm_task_class *newcls = NULL, *oldcls = tsk->taskclass;	\
+										\
+	CE_CLASSIFY_RET(newcls,&CT_taskclass,event,tsk);			\
+	if (newcls) {								\
+		/* called synchrously. no need to get task struct */		\
+		ckrm_set_taskclass(tsk, newcls, oldcls, event);			\
+	}									\
+} while (0)
+
+#define CE_CLASSIFY_TASK_PROTECT(event, tsk)	\
+do {						\
+	ce_protect(&CT_taskclass);		\
+	CE_CLASSIFY_TASK(event,tsk);		\
+	ce_release(&CT_taskclass);              \
+} while (0)
+
+
+
+
+static void
+cb_taskclass_newtask(struct task_struct *tsk)
+{
+	tsk->taskclass = NULL;
+	INIT_LIST_HEAD(&tsk->taskclass_link);
+}
+
+
+static void
+cb_taskclass_fork(struct task_struct *tsk)
+{
+	struct ckrm_task_class *cls = NULL;
+
+	ECB_PRINTK("%p:%d:%s\n",tsk,tsk->pid,tsk->comm);
+
+	ce_protect(&CT_taskclass);
+	CE_CLASSIFY_RET(cls,&CT_taskclass,CKRM_EVENT_FORK,tsk);	     
+	if (cls == NULL) {
+		ckrm_task_lock(tsk->parent);
+		cls = tsk->parent->taskclass;
+		ckrm_core_grab(class_core(cls));
+		ckrm_task_unlock(tsk->parent);
+	}
+	if (!list_empty(&tsk->taskclass_link))
+		printk("BUG in cb_fork.. tsk (%s:%d> already linked\n",
+			tsk->comm,tsk->pid);
+
+	ckrm_set_taskclass(tsk, cls, NULL, CKRM_EVENT_FORK);
+	ce_release(&CT_taskclass);
+}
+
+static void
+cb_taskclass_exit(struct task_struct *tsk)
+{
+	ckrm_task_class_t *cls;
+
+	// Remove the task from the current core class
+	
+	ECB_PRINTK("%p:%d:%s\n",tsk,tsk->pid,tsk->comm);
+	ckrm_task_lock(tsk);
+
+	CE_CLASSIFY_NORET( &CT_taskclass, CKRM_EVENT_EXIT, tsk);
+
+	if ((cls = tsk->taskclass) != NULL) {
+		class_lock(class_core(cls));
+		tsk->taskclass = NULL;
+		list_del(&tsk->taskclass_link);
+		class_unlock(class_core(cls));
+		ckrm_core_drop(class_core(cls));
+	} else {
+		INIT_LIST_HEAD(&tsk->taskclass_link);
+	}
+	ckrm_task_unlock(tsk);
+}
+
+static void
+cb_taskclass_exec(const char *filename)
+{
+	ECB_PRINTK("%p:%d:%s <%s>\n",current,current->pid,current->comm,filename);
+	CE_CLASSIFY_TASK_PROTECT(CKRM_EVENT_EXEC, current);
+}
+
+static void
+cb_taskclass_uid(void)
+{
+	ECB_PRINTK("%p:%d:%s\n",current,current->pid,current->comm);
+	CE_CLASSIFY_TASK_PROTECT(CKRM_EVENT_UID, current);
+}
+
+static void
+cb_taskclass_gid(void)
+{
+	ECB_PRINTK("%p:%d:%s\n",current,current->pid,current->comm);
+	CE_CLASSIFY_TASK_PROTECT(CKRM_EVENT_GID, current);
+}
+
+static struct ckrm_event_spec taskclass_events_callbacks[] = {
+	CKRM_EVENT_SPEC( NEWTASK, cb_taskclass_newtask ),
+	CKRM_EVENT_SPEC( EXEC   , cb_taskclass_exec ),
+	CKRM_EVENT_SPEC( FORK   , cb_taskclass_fork ),
+	CKRM_EVENT_SPEC( EXIT   , cb_taskclass_exit ),
+	CKRM_EVENT_SPEC( UID    , cb_taskclass_uid  ),
+	CKRM_EVENT_SPEC( GID    , cb_taskclass_gid  ),
+	{ -1 }
+};
+
+/***********************************************************************
+ *
+ * Asynchronous callback functions   (driven by RCFS)
+ * 
+ *    Async functions force a setting of the task structure
+ *    synchronous callbacks are protected against race conditions 
+ *    by using a cmpxchg on the core before setting it.
+ *    Async calls need to be serialized to ensure they can't 
+ *    race against each other 
+ *
+ ***********************************************************************/
+
+DECLARE_MUTEX(async_serializer);    // serialize all async functions
+
+
+/*
+ * Go through the task list and reclassify all tasks according to the current
+ * classification rules.
+ *
+ * We have the problem that we can not hold any lock (including the 
+ * tasklist_lock) while classifying. Two methods possible
+ *
+ * (a) go through entire pidrange (0..pidmax) and if a task exists at 
+ *     that pid then reclassify it
+ * (b) go several time through task list and build a bitmap for a particular 
+ *     subrange of pid otherwise the memory requirements ight be too much.
+ * 
+ * We use a hybrid by comparing ratio nr_threads/pidmax
+ */
+
+static void
+ckrm_reclassify_all_tasks(void)
+{
+	extern int pid_max;
+
+	struct task_struct *proc, *thread;
+	int i;
+	int curpidmax = pid_max;
+	int ratio;
+	int use_bitmap;
+
+
+	ratio = curpidmax / nr_threads;
+	if (curpidmax <= PID_MAX_DEFAULT) {
+	     use_bitmap = 1;
+	} else {
+	     use_bitmap = (ratio >= 2);
+	}
+
+	ce_protect(&CT_taskclass);
+
+ retry:		
+	if (use_bitmap == 0) {
+		// go through it in one walk
+		read_lock(&tasklist_lock);
+		for ( i=0 ; i<curpidmax ; i++ ) {
+			if ((thread = find_task_by_pid(i)) == NULL) 
+				continue;
+			get_task_struct(thread);
+			read_unlock(&tasklist_lock);
+			CE_CLASSIFY_TASK(CKRM_EVENT_RECLASSIFY, thread);
+			put_task_struct(thread);
+			read_lock(&tasklist_lock);
+		}
+		read_unlock(&tasklist_lock);
+	} else {
+		unsigned long *bitmap;
+		int bitmapsize;
+		int order = 0;
+		int num_loops;
+		int pid, do_next;
+
+
+		bitmap = (unsigned long*) __get_free_pages(GFP_KERNEL,order);
+		if (bitmap == NULL) {
+			use_bitmap = 0;
+			goto retry;
+		}
+
+		bitmapsize = 8 * (1 << (order + PAGE_SHIFT));
+		num_loops  = (curpidmax + bitmapsize - 1) / bitmapsize;
+
+		do_next = 1;
+		for ( i=0 ; i < num_loops && do_next; i++) {
+			int pid_start = i*bitmapsize; 
+			int pid_end   = pid_start + bitmapsize;
+			int num_found = 0;
+			int pos;
+
+			memset(bitmap, 0, bitmapsize/8); // start afresh
+			do_next = 0;
+
+			read_lock(&tasklist_lock);
+			do_each_thread(proc, thread) {
+				pid = thread->pid;
+				if ((pid < pid_start) || (pid >= pid_end)) {
+					if (pid >= pid_end) {
+						do_next = 1;
+					}
+					continue;
+				}
+				pid -= pid_start;
+				set_bit(pid, bitmap);
+				num_found++;
+			} while_each_thread(proc, thread);
+			read_unlock(&tasklist_lock);
+		
+			if (num_found == 0) 
+				continue;
+
+			pos = 0;
+			for ( ; num_found-- ; ) {
+				pos = find_next_bit(bitmap, bitmapsize, pos);
+				pid = pos + pid_start;
+
+				read_lock(&tasklist_lock);
+				if ((thread = find_task_by_pid(pid)) != NULL) {
+					get_task_struct(thread);
+					read_unlock(&tasklist_lock);
+					CE_CLASSIFY_TASK(CKRM_EVENT_RECLASSIFY, thread);
+					put_task_struct(thread);
+				} else {
+					read_unlock(&tasklist_lock);
+				}
+			}
+		}
+
+	}
+	ce_release(&CT_taskclass);
+}
+
+int
+ckrm_reclassify(int pid)
+{
+	struct task_struct *tsk;
+	int rc = 0;
+
+	down(&async_serializer);   // protect again race condition
+	if (pid < 0) {
+		// do we want to treat this as process group .. should YES ToDo
+		 rc = -EINVAL;
+	} else if (pid == 0) {
+		// reclassify all tasks in the system
+		ckrm_reclassify_all_tasks();
+	} else {
+		// reclassify particular pid
+		read_lock(&tasklist_lock);
+		if ((tsk = find_task_by_pid(pid)) != NULL) {
+			get_task_struct(tsk);
+			read_unlock(&tasklist_lock);
+			CE_CLASSIFY_TASK_PROTECT(CKRM_EVENT_RECLASSIFY, tsk);
+			put_task_struct(tsk);
+		} else {
+			read_unlock(&tasklist_lock);
+			rc = -EINVAL;
+		}
+	}
+	up(&async_serializer);
+	return rc;
+}
+
+/*
+ * Reclassify all tasks in the given core class.
+ */
+
+static void
+ckrm_reclassify_class_tasks(struct ckrm_task_class *cls)
+{
+	int ce_regd;
+	struct ckrm_hnode *cnode;
+	struct ckrm_task_class *parcls;
+
+	if (!ckrm_validate_and_grab_core(&cls->core))
+		return;
+
+	down(&async_serializer);   // protect again race condition
+
+
+	printk("\t%s: start %p:%s:%d\n",__FUNCTION__,cls,cls->core.name, cls->core.refcnt);
+	// If no CE registered for this classtype, following will be needed repeatedly;
+	ce_regd =  class_core(cls)->classtype->ce_regd;
+	cnode = &(class_core(cls)->hnode);
+	parcls = class_type(ckrm_task_class_t, cnode->parent);
+
+next_task:
+	class_lock(class_core(cls));
+	if (!list_empty(&class_core(cls)->objlist)) {
+		struct ckrm_task_class *newcls = NULL;
+		struct task_struct *tsk = 
+			list_entry(class_core(cls)->objlist.next,
+				   struct task_struct, taskclass_link);
+		
+		get_task_struct(tsk);
+		class_unlock(class_core(cls));
+
+		if (ce_regd) {
+			CE_CLASSIFY_RET(newcls,&CT_taskclass,CKRM_EVENT_RECLASSIFY,tsk); 
+		} else {
+			newcls = parcls;
+			ckrm_core_grab(class_core(newcls));
+		}
+
+		if (cls == newcls) {
+			// don't allow reclassifying to the same class
+			// as we are in the process of cleaning up this class
+			ckrm_core_drop(class_core(newcls)); // to compensate CE's grab
+			newcls = NULL;
+		}
+		ckrm_set_taskclass(tsk, newcls, cls, CKRM_EVENT_RECLASSIFY);
+		put_task_struct(tsk);
+		goto next_task;
+	}
+	printk("\t%s: stop  %p:%s:%d\n",__FUNCTION__,cls,cls->core.name, cls->core.refcnt);
+	ckrm_core_drop(class_core(cls));
+	class_unlock(class_core(cls));
+
+	up(&async_serializer);
+
+	return ;
+}
+
+/*
+ * Change the core class of the given task.
+ */
+
+int 
+ckrm_forced_reclassify_pid(pid_t pid, struct ckrm_task_class *cls)
+{
+	struct task_struct *tsk;
+
+	if (!ckrm_validate_and_grab_core(class_core(cls)))
+		return - EINVAL;
+
+	read_lock(&tasklist_lock);
+	if ((tsk = find_task_by_pid(pid)) == NULL) {
+		read_unlock(&tasklist_lock);
+		return -EINVAL;
+	}
+	get_task_struct(tsk);
+	read_unlock(&tasklist_lock);
+	
+	down(&async_serializer);   // protect again race condition
+	
+	ce_protect(&CT_taskclass);
+	ckrm_set_taskclass(tsk, cls, NULL, CKRM_EVENT_MANUAL);
+	ce_release(&CT_taskclass);
+	put_task_struct(tsk);
+	
+	up(&async_serializer);
+	return 0;
+}
+
+static struct ckrm_core_class *
+ckrm_alloc_task_class(struct ckrm_core_class *parent, const char *name)
+{
+	struct ckrm_task_class *taskcls;
+	taskcls = kmalloc(sizeof(struct ckrm_task_class), GFP_KERNEL);
+	if (taskcls == NULL) 
+		return NULL;
+
+	ckrm_init_core_class(&CT_taskclass,
+			     class_core(taskcls),parent,name);
+
+	ce_protect(&CT_taskclass);
+	if (CT_taskclass.ce_cb_active && CT_taskclass.ce_callbacks.class_add)
+		(*CT_taskclass.ce_callbacks.class_add)(name,taskcls);
+	ce_release(&CT_taskclass);
+
+	return class_core(taskcls);
+}
+
+static int
+ckrm_free_task_class(struct ckrm_core_class *core)
+{
+	struct ckrm_task_class *taskcls;
+
+	if (!ckrm_is_core_valid(core)) {
+		// Invalid core
+		return (-EINVAL);
+	}
+	if (core == core->classtype->default_class) {
+		// reset the name tag
+		core->name = dflt_taskclass_name;
+ 		return 0;
+	}
+
+	printk("%s: stop  %p:%s:%d\n",__FUNCTION__,core,core->name, core->refcnt);
+	taskcls = class_type(struct ckrm_task_class, core);
+
+	ce_protect(&CT_taskclass);
+
+	if (CT_taskclass.ce_cb_active && CT_taskclass.ce_callbacks.class_delete)
+		(*CT_taskclass.ce_callbacks.class_delete)(core->name,taskcls);
+	ckrm_reclassify_class_tasks( taskcls );
+
+	ce_release(&CT_taskclass);
+
+	ckrm_release_core_class(core);  // Hubertus .... could just drop the class .. error message
+	return 0;
+}
+
+
+void __init
+ckrm_meta_init_taskclass(void)
+{
+	printk("...... Initializing ClassType<%s> ........\n",CT_taskclass.name);
+	// intialize the default class
+	ckrm_init_core_class(&CT_taskclass, class_core(&taskclass_dflt_class),
+			     NULL,dflt_taskclass_name);
+
+	// register classtype and initialize default task class
+	ckrm_register_classtype(&CT_taskclass);
+	ckrm_register_event_set(taskclass_events_callbacks);
+
+	// note registeration of all resource controllers will be done later dynamically 
+	// as these are specified as modules
+}
+
+
+
+static int                      
+tc_show_members(struct ckrm_core_class *core, struct seq_file *seq) 
+{
+	struct list_head *lh;
+	struct task_struct *tsk;
+
+	spin_lock(&core->ckrm_lock);
+	list_for_each(lh, &core->objlist) {	
+		tsk = container_of(lh, struct task_struct, taskclass_link);
+		seq_printf(seq,"%ld\n", (long)tsk->pid);
+	}
+	spin_unlock(&core->ckrm_lock);
+
+	return 0;
+}
+
+static int
+tc_forced_reclassify(struct ckrm_core_class *target,const char *obj)
+{	
+	pid_t pid;
+	int rc = -EINVAL;
+
+	pid = (pid_t) simple_strtoul(obj,NULL,10);
+	if (pid > 0) {
+		rc = ckrm_forced_reclassify_pid(pid,
+				class_type(ckrm_task_class_t,target));
+	}
+	return rc;
+} 
+	
+#if 1
+
+/***************************************************************************************
+ * Debugging Task Classes:  Utility functions
+ **************************************************************************************/
+
+void
+check_tasklist_sanity(struct ckrm_task_class *cls)
+{
+	struct ckrm_core_class *core = class_core(cls);
+	struct list_head *lh1, *lh2;
+	int count = 0;
+
+	if (core) {
+		class_lock(core);
+		if (list_empty(&core->objlist)) {
+			class_lock(core);
+			printk("check_tasklist_sanity: class %s empty list\n",
+					core->name);
+			return;
+		}
+		list_for_each_safe(lh1, lh2, &core->objlist) {
+			struct task_struct *tsk = container_of(lh1, struct task_struct, taskclass_link);
+			if (count++ > 20000) {
+				printk("list is CORRUPTED\n");
+				break;
+			}
+			if (tsk->taskclass != cls) {
+				const char *tclsname;
+				tclsname = (tsk->taskclass) ? class_core(tsk->taskclass)->name 
+					                    : "NULL";
+				printk("sanity: task %s:%d has ckrm_core |%s| but in list |%s|\n",
+				       tsk->comm,tsk->pid,tclsname,core->name);
+			}
+		}
+		class_unlock(core);
+	}
+}
+
+void 
+ckrm_debug_free_task_class(struct ckrm_task_class *tskcls)
+{
+	struct task_struct *proc, *thread;
+	int count = 0;
+
+	printk("Analyze Error <%s> %d\n",
+	       class_core(tskcls)->name,atomic_read(&(class_core(tskcls)->refcnt)));
+
+	read_lock(&tasklist_lock);
+	class_lock(class_core(tskcls));
+	do_each_thread(proc, thread) {
+		count += (tskcls == thread->taskclass);
+		if ((thread->taskclass == tskcls) || (tskcls == NULL)) {
+			const char *tclsname;
+			tclsname = (thread->taskclass) ? class_core(thread->taskclass)->name : "NULL";
+			printk("%d thread=<%s:%d>  -> <%s> <%lx>\n",
+			       count,thread->comm,thread->pid,tclsname, thread->flags & PF_EXITING);
+		}
+	} while_each_thread(proc, thread);
+	class_unlock(class_core(tskcls));
+	read_unlock(&tasklist_lock);
+
+	printk("End Analyze Error <%s> %d\n",
+	       class_core(tskcls)->name,atomic_read(&(class_core(tskcls)->refcnt)));
+} 
+
+#endif
diff -Nru --exclude='*SCCS*' a/kernel/ckrm/ckrmutils.c b/kernel/ckrm/ckrmutils.c
--- a/kernel/ckrm/ckrmutils.c	2004-04-26 12:35:37.000000000 -0400
+++ b/kernel/ckrm/ckrmutils.c	2004-04-28 22:57:42.000000000 -0400
@@ -21,11 +21,11 @@
  *        Created
  */
 
-#include <linux/ckrm.h>
 #include <linux/mm.h>
 #include <linux/err.h>
 #include <linux/mount.h>
 #include <linux/module.h>
+#include <linux/ckrm_rc.h>
 
 int
 get_exe_path_name(struct task_struct *tsk, char *buf, int buflen)
@@ -66,68 +66,142 @@
 	return rc;
 }
 
-EXPORT_SYMBOL(get_exe_path_name);
 
+/*
+ * must be called with cnt_lock of parres held
+ * Caller is responsible for making sure that the new guarantee doesn't
+ * overflow parent's total guarantee.
+ */
+void
+child_guarantee_changed(struct ckrm_shares *parent, int cur, int new)
+{
+	if (new == cur || !parent) {
+		return;
+	}
+	if (new != CKRM_SHARE_DONTCARE) {
+		parent->unused_guarantee -= new;
+	}
+	if (cur != CKRM_SHARE_DONTCARE) {
+		parent->unused_guarantee += cur;
+	}
+	return;
+}
 
+/*
+ * must be called with cnt_lock of parres held
+ * Caller is responsible for making sure that the new limit is not more 
+ * than parent's max_limit
+ */
+void
+child_maxlimit_changed(struct ckrm_shares *parent, int new_limit)
+{
+	if (parent && parent->cur_max_limit < new_limit) {
+		parent->cur_max_limit = new_limit;
+	}
+	return;
+}
 
 /*
- *  functions that come in handy for debugging 
+ * Caller is responsible for holding any lock to protect the data
+ * structures passed to this function
  */
+int
+set_shares(struct ckrm_shares *new, struct ckrm_shares *cur,
+		struct ckrm_shares *par)
+{
+	int rc = -EINVAL;
+	int cur_usage_guar = cur->total_guarantee - cur->unused_guarantee;
+	int increase_by = new->my_guarantee - cur->my_guarantee;
+
+	// Check total_guarantee for correctness
+	if (new->total_guarantee <= CKRM_SHARE_DONTCARE) {
+		goto set_share_err;
+	} else if (new->total_guarantee == CKRM_SHARE_UNCHANGED) {
+		;// do nothing
+	} else if (cur_usage_guar > new->total_guarantee) {
+		goto set_share_err;
+	}
 
-#if 0
+	// Check max_limit for correctness
+	if (new->max_limit <= CKRM_SHARE_DONTCARE) {
+		goto set_share_err;
+	} else if (new->max_limit == CKRM_SHARE_UNCHANGED) {
+		; // do nothing
+	} else if (cur->cur_max_limit > new->max_limit) {
+		goto set_share_err;
+	}
 
-#include <linux/ckrm_rc.h>
+	// Check my_guarantee for correctness
+	if (new->my_guarantee == CKRM_SHARE_UNCHANGED) {
+		; // do nothing
+	} else if (new->my_guarantee == CKRM_SHARE_DONTCARE) {
+		; // do nothing
+	} else if (par && increase_by > par->unused_guarantee) {
+		goto set_share_err;
+	}
 
-void
-check_tasklist_sanity(struct ckrm_core_class *core)
-{
-	struct list_head *lh1, *lh2;
-	int count = 0;
+	// Check my_limit for correctness
+	if (new->my_limit == CKRM_SHARE_UNCHANGED) {
+		; // do nothing
+	} else if (new->my_limit == CKRM_SHARE_DONTCARE) {
+		; // do nothing
+	} else if (par && new->my_limit > par->max_limit) {
+		// I can't get more limit than my parent's limit
+		goto set_share_err;
+		
+	}
 
-	if (core) {
-		spin_lock(&core->ckrm_lock);
-		if (list_empty(&core->tasklist)) {
-			spin_unlock(&core->ckrm_lock);
-			printk("check_tasklist_sanity: class %s empty list\n",
-					core->name);
-			return;
+	// make sure guarantee is lesser than limit
+	if (new->my_limit == CKRM_SHARE_DONTCARE) {
+		; // do nothing
+	} else if (new->my_limit == CKRM_SHARE_UNCHANGED) {
+		if (new->my_guarantee == CKRM_SHARE_DONTCARE) {
+			; // do nothing
+		} else if (new->my_guarantee == CKRM_SHARE_UNCHANGED) {
+			; // do nothing earlier setting would 've taken care of it
+		} else if (new->my_guarantee > cur->my_limit) {
+			goto set_share_err;
 		}
-		list_for_each_safe(lh1, lh2, &core->tasklist) {
-			struct task_struct *tsk = container_of(lh1, struct task_struct, ckrm_link);
-			if (count++ > 20000) {
-				printk("list is CORRUPTED\n");
-				break;
-			}
-			if (tsk->ckrm_core != core) {
-				ckrm_core_class_t *tcore = tsk->ckrm_core;
-				printk("sanity: task %s:%d has ckrm_core |%s| but in list |%s|\n",
-						tsk->comm,tsk->pid,
-						tcore ? tcore->name: "NULL",
-						core->name);
+	} else { // new->my_limit has a valid value
+		if (new->my_guarantee == CKRM_SHARE_DONTCARE) {
+			; // do nothing
+		} else if (new->my_guarantee == CKRM_SHARE_UNCHANGED) {
+			if (cur->my_guarantee > new->my_limit) {
+				goto set_share_err;
 			}
+		} else if (new->my_guarantee > new->my_limit) {
+			goto set_share_err;
 		}
-		spin_unlock(&core->ckrm_lock);
 	}
+
+	if (new->my_guarantee != CKRM_SHARE_UNCHANGED) {
+		child_guarantee_changed(par, cur->my_guarantee,
+				new->my_guarantee);
+		cur->my_guarantee = new->my_guarantee;
+	}
+
+	if (new->my_limit != CKRM_SHARE_UNCHANGED) {
+		child_maxlimit_changed(par, new->my_limit);
+		cur->my_limit = new->my_limit;
+	}
+
+	if (new->total_guarantee != CKRM_SHARE_UNCHANGED) {
+		cur->unused_guarantee = new->total_guarantee - cur_usage_guar;
+		cur->total_guarantee = new->total_guarantee;
+	}
+
+	if (new->max_limit != CKRM_SHARE_UNCHANGED) {
+		cur->max_limit = new->max_limit;
+	}
+
+	rc = 0;
+set_share_err:
+	return rc;
 }
 
-void 
-ckrm_debug_free_core_class(struct ckrm_core_class *core)
-{
-	struct task_struct *proc, *thread;
-	int count = 0;
+EXPORT_SYMBOL(get_exe_path_name);
+EXPORT_SYMBOL(child_guarantee_changed);
+EXPORT_SYMBOL(child_maxlimit_changed);
+EXPORT_SYMBOL(set_shares);
 
-	printk("Analyze Error <%s> %d\n",core->name,atomic_read(&core->refcnt));
-	read_lock(&tasklist_lock);
-	spin_lock(&core->ckrm_lock);
-	do_each_thread(proc, thread) {
-		struct ckrm_core_class *tcore = (struct ckrm_core_class*) (thread->ckrm_core);
-		count += (core == thread->ckrm_core);
-		printk("%d thread=<%s:%d>  -> <%s> <%lx>\n",
-			count,thread->comm,thread->pid,tcore ? tcore->name : "NULL", thread->flags & PF_EXITING);
-	} while_each_thread(proc, thread);
-	spin_unlock(&core->ckrm_lock);
-	read_unlock(&tasklist_lock);
-	printk("End Analyze Error <%s> %d\n",core->name,atomic_read(&core->refcnt));
-} 
 
-#endif
diff -Nru --exclude='*SCCS*' a/kernel/ckrm/Makefile b/kernel/ckrm/Makefile
--- a/kernel/ckrm/Makefile	2004-04-26 12:35:37.000000000 -0400
+++ b/kernel/ckrm/Makefile	2004-04-28 22:57:42.000000000 -0400
@@ -1,13 +1,14 @@
 #
-# Makefile for the ckrm Module
+# Makefile for CKRM 
 #
 
 ifeq ($(CONFIG_CKRM),y)
-	obj-y = ckrm.o ckrmutils.o
-else
-	obj-y = ckrmstub.o
+	obj-y = ckrm.o ckrmutils.o 
 endif
 
-obj-$(CONFIG_CKRM_RES_DUMMY) += ckrm_dummy.o 
-obj-$(CONFIG_CKRM_RES_SOCKETAQ) += ckrm_socketaq.o
+obj-$(CONFIG_CKRM_TYPE_TASKCLASS) += ckrm_tc.o 
+obj-$(CONFIG_CKRM_RES_NUMTASKS) += ckrm_tasks.o
+
+obj-$(CONFIG_CKRM_TYPE_SOCKETCLASS) += ckrm_sockc.o        
+obj-$(CONFIG_CKRM_RES_LISTENAQ) += ckrm_listenaq.o  
 
