From: Intel
Subject: TCP offload to I/OAT
References: 142510

Locks down user pages and sets up for the DMA in tcp_recvmsg, then calls
dma_async_try_early_copy in tcp_v4_do_rcv

Acked-by: okir@suse.de

 include/linux/sysctl.h     |    1 
 net/ipv4/sysctl_net_ipv4.c |   10 +++
 net/ipv4/tcp.c             |  113 +++++++++++++++++++++++++++++++++++++++------
 net/ipv4/tcp_input.c       |   62 +++++++++++++++++++++++-
 net/ipv4/tcp_ipv4.c        |   17 ++++++
 net/ipv4/tcp_minisocks.c   |    1 
 net/ipv6/tcp_ipv6.c        |    1 
 7 files changed, 187 insertions(+), 18 deletions(-)

Index: build/include/linux/sysctl.h
===================================================================
--- build.orig/include/linux/sysctl.h
+++ build/include/linux/sysctl.h
@@ -395,6 +395,7 @@ enum
 	NET_TCP_CONG_CONTROL=110,
 	NET_TCP_ABC=111,
 	NET_IPV4_IPFRAG_MAX_DIST=112,
+	NET_TCP_DMA_COPYBREAK=0xf00baa, /* not in mainline yet; picking random value */
 };
 
 enum {
Index: build/net/ipv4/sysctl_net_ipv4.c
===================================================================
--- build.orig/net/ipv4/sysctl_net_ipv4.c
+++ build/net/ipv4/sysctl_net_ipv4.c
@@ -664,6 +664,16 @@ ctl_table ipv4_table[] = {
 		.mode		= 0644,
 		.proc_handler	= &proc_dointvec,
 	},
+#ifdef CONFIG_NET_DMA
+	{
+		.ctl_name	= NET_TCP_DMA_COPYBREAK,
+		.procname	= "tcp_dma_copybreak",
+		.data		= &sysctl_tcp_dma_copybreak,
+		.maxlen		= sizeof(int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec
+	},
+#endif
 
 	{ .ctl_name = 0 }
 };
Index: build/net/ipv4/tcp.c
===================================================================
--- build.orig/net/ipv4/tcp.c
+++ build/net/ipv4/tcp.c
@@ -941,7 +941,7 @@ void tcp_cleanup_rbuf(struct sock *sk, i
 	struct tcp_sock *tp = tcp_sk(sk);
 	int time_to_ack = 0;
 
-#if TCP_DEBUG
+#if TCP_DEBUG && !defined(CONFIG_NET_DMA)
 	struct sk_buff *skb = skb_peek(&sk->sk_receive_queue);
 
 	BUG_TRAP(!skb || before(tp->copied_seq, TCP_SKB_CB(skb)->end_seq));
@@ -1132,6 +1132,21 @@ int tcp_recvmsg(struct kiocb *iocb, stru
 
 	target = sock_rcvlowat(sk, flags & MSG_WAITALL, len);
 
+#ifdef CONFIG_NET_DMA
+	tp->ucopy.dma_chan = NULL;
+	if ((len > sysctl_tcp_dma_copybreak) && !(flags & MSG_PEEK) && !sysctl_tcp_low_latency) {
+		tp->ucopy.dma_chan = get_softnet_dma();
+		if (tp->ucopy.dma_chan) {
+			if (dma_lock_iovec_pages(msg->msg_iov, len,
+			    &tp->ucopy.locked_list)) {
+				/* fallback to no-dma */
+				tp->ucopy.dma_chan = NULL;
+				dma_chan_put(tp->ucopy.dma_chan);
+			}
+		}
+	}
+#endif
+
 	do {
 		struct sk_buff *skb;
 		u32 offset;
@@ -1161,7 +1176,12 @@ int tcp_recvmsg(struct kiocb *iocb, stru
 				       "seq %X\n", *seq, TCP_SKB_CB(skb)->seq);
 				break;
 			}
-			offset = *seq - TCP_SKB_CB(skb)->seq;
+
+			if (!skb->copied_early)
+				offset = *seq - TCP_SKB_CB(skb)->seq;
+			else
+				offset = 0;
+
 			if (skb->h.th->syn)
 				offset--;
 			if (offset < skb->len)
@@ -1306,8 +1326,12 @@ do_prequeue:
 		}
 		continue;
 
-	found_ok_skb:
+found_ok_skb:
 		/* Ok so how much can we use? */
+		if (skb->copied_early) {
+			used = skb->len;
+			goto skip_copy;
+		}
 		used = skb->len - offset;
 		if (len < used)
 			used = len;
@@ -1330,13 +1354,33 @@ do_prequeue:
 		}
 
 		if (!(flags & MSG_TRUNC)) {
-			err = skb_copy_datagram_iovec(skb, offset,
-						      msg->msg_iov, used);
-			if (err) {
-				/* Exception. Bailout! */
-				if (!copied)
-					copied = -EFAULT;
-				break;
+			if (tp->ucopy.dma_chan) {
+				tp->ucopy.dma_cookie = dma_skb_copy_datagram_iovec(
+					tp->ucopy.dma_chan, skb, offset,
+					msg->msg_iov, used,
+					tp->ucopy.locked_list);
+
+				if (tp->ucopy.dma_cookie < 0) {
+
+					printk(KERN_ALERT "dma_cookie < 0\n");
+
+					/* Exception. Bailout! */
+					if (!copied)
+						copied = -EFAULT;
+					break;
+				}
+				if ((offset + used) == skb->len)
+					skb->copied_early = 1;
+
+			} else {
+				err = skb_copy_datagram_iovec(skb, offset,
+						msg->msg_iov, used);
+				if (err) {
+					/* Exception. Bailout! */
+					if (!copied)
+						copied = -EFAULT;
+					break;
+				}
 			}
 		}
 
@@ -1356,15 +1400,27 @@ skip_copy:
 
 		if (skb->h.th->fin)
 			goto found_fin_ok;
-		if (!(flags & MSG_PEEK))
-			sk_eat_skb(sk, skb);
+		if (!(flags & MSG_PEEK)) {
+			if (!skb->copied_early)
+				sk_eat_skb(sk, skb);
+			else {
+				__skb_unlink(skb, &sk->sk_receive_queue);
+				__skb_queue_tail(&tp->async_wait_queue, skb);
+			}
+		}
 		continue;
 
 	found_fin_ok:
 		/* Process the FIN. */
 		++*seq;
-		if (!(flags & MSG_PEEK))
-			sk_eat_skb(sk, skb);
+		if (!(flags & MSG_PEEK)) {
+			if (!skb->copied_early)
+				sk_eat_skb(sk, skb);
+			else {
+				__skb_unlink(skb, &sk->sk_receive_queue);
+				__skb_queue_tail(&tp->async_wait_queue, skb);
+			}
+		}
 		break;
 	} while (len > 0);
 
@@ -1387,6 +1443,35 @@ skip_copy:
 		tp->ucopy.len = 0;
 	}
 
+
+#ifdef CONFIG_NET_DMA
+	if (tp->ucopy.dma_chan) {
+		struct sk_buff *skb;
+		dma_cookie_t done, used;
+
+		dma_async_memcpy_issue_pending(tp->ucopy.dma_chan);
+
+		while (dma_async_memcpy_complete(tp->ucopy.dma_chan,
+		                                 tp->ucopy.dma_cookie, &done,
+		                                 &used) == DMA_IN_PROGRESS) {
+			/* do partial cleanup of async_wait_queue */
+			while ((skb = skb_peek(&tp->async_wait_queue)) &&
+			       (dma_async_is_complete(skb->dma_cookie, done,
+			                              used) == DMA_SUCCESS)) {
+				__skb_dequeue(&tp->async_wait_queue);
+				kfree_skb(skb);
+			}
+		}
+
+		/* Safe to free early-copied skbs now */
+		__skb_queue_purge(&tp->async_wait_queue);
+		dma_unlock_iovec_pages(tp->ucopy.locked_list);
+		dma_chan_put(tp->ucopy.dma_chan);
+		tp->ucopy.dma_chan = NULL;
+		tp->ucopy.locked_list = NULL;
+	}
+#endif
+
 	/* According to UNIX98, msg_name/msg_namelen are ignored
 	 * on connected socket. I was just happy when found this 8) --ANK
 	 */
Index: build/net/ipv4/tcp_input.c
===================================================================
--- build.orig/net/ipv4/tcp_input.c
+++ build/net/ipv4/tcp_input.c
@@ -71,6 +71,7 @@
 #include <net/inet_common.h>
 #include <linux/ipsec.h>
 #include <asm/unaligned.h>
+#include <net/netdma.h>
 
 int sysctl_tcp_timestamps = 1;
 int sysctl_tcp_window_scaling = 1;
@@ -3761,10 +3762,12 @@ static inline int tcp_checksum_complete_
  *	the rest is checked inline. Fast processing is turned on in 
  *	tcp_data_queue when everything is OK.
  */
+
 int tcp_rcv_established(struct sock *sk, struct sk_buff *skb,
 			struct tcphdr *th, unsigned len)
 {
 	struct tcp_sock *tp = tcp_sk(sk);
+	int eaten = 0;
 
 	/*
 	 *	Header prediction.
@@ -3853,12 +3856,11 @@ int tcp_rcv_established(struct sock *sk,
 				goto discard;
 			}
 		} else {
-			int eaten = 0;
-
 			if (tp->ucopy.task == current &&
 			    tp->copied_seq == tp->rcv_nxt &&
 			    len - tcp_header_len <= tp->ucopy.len &&
-			    sock_owned_by_user(sk)) {
+			    sock_owned_by_user(sk) &&
+			    !skb->copied_early) {
 				__set_current_state(TASK_RUNNING);
 
 				if (!tcp_copy_to_iovec(sk, skb, tcp_header_len)) {
@@ -3905,6 +3907,9 @@ int tcp_rcv_established(struct sock *sk,
 				__skb_queue_tail(&sk->sk_receive_queue, skb);
 				sk_stream_set_owner_r(skb, sk);
 				tp->rcv_nxt = TCP_SKB_CB(skb)->end_seq;
+
+				if (skb->copied_early)
+					tcp_cleanup_rbuf(sk, skb->len);
 			}
 
 			tcp_event_data_recv(sk, tp, skb);
@@ -3919,6 +3924,9 @@ int tcp_rcv_established(struct sock *sk,
 
 			__tcp_ack_snd_check(sk, 0);
 no_ack:
+			if (skb->copied_early)
+				return 0;
+
 			if (eaten)
 				__kfree_skb(skb);
 			else
@@ -4002,6 +4010,54 @@ discard:
 	return 0;
 }
 
+#ifdef CONFIG_NET_DMA
+void dma_async_try_early_copy(struct sock *sk, struct sk_buff *skb)
+{
+	struct tcp_sock *tp = tcp_sk(sk);
+	int dma_cookie;
+	int chunk = skb->len - tp->tcp_header_len;
+	struct tcphdr *th = skb->h.th;
+
+	if ((tp->ucopy.task == NULL) || (tp->ucopy.dma_chan == NULL) || tp->ucopy.wakeup)
+          	return;
+
+	if ((tcp_flag_word(th) & TCP_HP_BITS) != tp->pred_flags ||
+		TCP_SKB_CB(skb)->seq != tp->rcv_nxt)
+		return;
+
+	if (tp->ucopy.dma_chan &&
+	    chunk > 0 &&
+	    chunk <= (tp->ucopy.len) &&
+	    tp->copied_seq == tp->rcv_nxt &&
+	    skb->ip_summed == CHECKSUM_UNNECESSARY) {
+
+		dma_cookie = dma_skb_copy_datagram_iovec(tp->ucopy.dma_chan,
+			skb, tp->tcp_header_len,
+			tp->ucopy.iov, chunk, tp->ucopy.locked_list);
+
+		if (dma_cookie < 0)
+			return;
+
+		tp->ucopy.dma_cookie = dma_cookie;
+		skb->copied_early = 1;
+
+		tp->copied_seq += chunk;
+		tp->ucopy.len -= chunk;
+		tcp_rcv_space_adjust(sk);
+
+		if ((tp->ucopy.len == 0) || (tcp_flag_word(skb->h.th) & TCP_FLAG_PSH)) {
+			tp->ucopy.wakeup = 1;
+			wake_up_interruptible(sk->sk_sleep);
+		}
+	} else if (chunk > 0) {
+		tp->ucopy.wakeup = 1;
+		wake_up_interruptible(sk->sk_sleep);
+	}
+}
+
+EXPORT_SYMBOL(dma_async_try_early_copy);
+#endif /* CONFIG_NET_DMA */
+
 static int tcp_rcv_synsent_state_process(struct sock *sk, struct sk_buff *skb,
 					 struct tcphdr *th, unsigned len)
 {
Index: build/net/ipv4/tcp_ipv4.c
===================================================================
--- build.orig/net/ipv4/tcp_ipv4.c
+++ build/net/ipv4/tcp_ipv4.c
@@ -71,6 +71,7 @@
 #include <net/inet_common.h>
 #include <net/timewait_sock.h>
 #include <net/xfrm.h>
+#include <net/netdma.h>
 
 #include <linux/inet.h>
 #include <linux/ipv6.h>
@@ -981,6 +982,11 @@ int tcp_v4_do_rcv(struct sock *sk, struc
 {
 	if (sk->sk_state == TCP_ESTABLISHED) { /* Fast path */
 		TCP_CHECK_TIMER(sk);
+
+#ifdef CONFIG_NET_DMA
+		dma_async_try_early_copy(sk, skb);
+#endif
+
 		if (tcp_rcv_established(sk, skb, skb->h.th, skb->len))
 			goto reset;
 		TCP_CHECK_TIMER(sk);
@@ -1090,10 +1096,19 @@ process:
 	bh_lock_sock(sk);
 	ret = 0;
 	if (!sock_owned_by_user(sk)) {
-		if (!tcp_prequeue(sk, skb))
+#ifdef CONFIG_NET_DMA
+		struct tcp_sock *tp = tcp_sk(sk);
+		if (tp->ucopy.dma_chan)
 			ret = tcp_v4_do_rcv(sk, skb);
+		else
+#endif
+		{
+			if (!tcp_prequeue(sk, skb))
+			ret = tcp_v4_do_rcv(sk, skb);
+		}
 	} else
 		sk_add_backlog(sk, skb);
+
 	bh_unlock_sock(sk);
 
 	sock_put(sk);
Index: build/net/ipv4/tcp_minisocks.c
===================================================================
--- build.orig/net/ipv4/tcp_minisocks.c
+++ build/net/ipv4/tcp_minisocks.c
@@ -392,6 +392,7 @@ struct sock *tcp_create_openreq_child(st
 		tcp_set_ca_state(newsk, TCP_CA_Open);
 		tcp_init_xmit_timers(newsk);
 		skb_queue_head_init(&newtp->out_of_order_queue);
+		skb_queue_head_init(&newtp->async_wait_queue);
 		newtp->rcv_wup = treq->rcv_isn + 1;
 		newtp->write_seq = treq->snt_isn + 1;
 		newtp->pushed_seq = newtp->write_seq;
Index: build/net/ipv6/tcp_ipv6.c
===================================================================
--- build.orig/net/ipv6/tcp_ipv6.c
+++ build/net/ipv6/tcp_ipv6.c
@@ -1340,6 +1340,7 @@ static int tcp_v6_init_sock(struct sock 
 	struct tcp_sock *tp = tcp_sk(sk);
 
 	skb_queue_head_init(&tp->out_of_order_queue);
+	skb_queue_head_init(&tp->async_wait_queue);
 	tcp_init_xmit_timers(sk);
 	tcp_prequeue_init(tp);
 
