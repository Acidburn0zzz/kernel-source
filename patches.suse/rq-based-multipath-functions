From c504bf141e7d559beef724bb2f3f0ab4c1780098 Mon Sep 17 00:00:00 2001
From: Hannes Reinecke <hare@suse.de>
Date: Fri, 8 Aug 2008 08:33:12 +0200
Subject: rq-based multipathing: request-based functions to multipath
References: FATE#302108

Request-based multipathing can be activated by selecting the
feature 'rq-based'. Default is normal (bio-based) multipathing.

Signed-off-by: Hannes Reinecke <hare@suse.de>
---
 drivers/md/dm-mpath.c |  258 ++++++++++++++++++++++++++++++++++++++++++++++----
 1 file changed, 240 insertions(+), 18 deletions(-)

--- a/drivers/md/dm-mpath.c
+++ b/drivers/md/dm-mpath.c
@@ -82,7 +82,8 @@ struct multipath {
 	unsigned pg_init_count;		/* Number of times pg_init called */
 
 	struct work_struct process_queued_ios;
-	struct bio_list queued_ios;
+	struct bio_list queued_bios;
+	struct list_head queued_reqs;
 	unsigned queue_size;
 
 	struct work_struct trigger_event;
@@ -185,6 +186,7 @@ static struct multipath *alloc_multipath
 	m = kzalloc(sizeof(*m), GFP_KERNEL);
 	if (m) {
 		INIT_LIST_HEAD(&m->priority_groups);
+		INIT_LIST_HEAD(&m->queued_reqs);
 		spin_lock_init(&m->lock);
 		m->queue_io = 1;
 		INIT_WORK(&m->process_queued_ios, process_queued_ios);
@@ -309,7 +311,7 @@ static int __must_push_back(struct multi
 		dm_noflush_suspending(m->ti));
 }
 
-static int map_io(struct multipath *m, struct bio *bio,
+static int map_bio(struct multipath *m, struct bio *bio,
 		  struct dm_mpath_io *mpio, unsigned was_queued)
 {
 	int r = DM_MAPIO_REMAPPED;
@@ -331,7 +333,7 @@ static int map_io(struct multipath *m, s
 	if ((pgpath && m->queue_io) ||
 	    (!pgpath && m->queue_if_no_path)) {
 		/* Queue for the daemon to resubmit */
-		bio_list_add(&m->queued_ios, bio);
+		bio_list_add(&m->queued_bios, bio);
 		m->queue_size++;
 		if ((m->pg_init_required && !m->pg_init_in_progress) ||
 		    !m->queue_io)
@@ -352,6 +354,59 @@ static int map_io(struct multipath *m, s
 	return r;
 }
 
+static int map_req(struct multipath *m, struct request *clone,
+		   struct dm_mpath_io *mpio, unsigned was_queued)
+{
+	int r = DM_MAPIO_REMAPPED;
+	unsigned long flags;
+	struct pgpath *pgpath;
+	struct block_device *bdev;
+
+	spin_lock_irqsave(&m->lock, flags);
+
+	/* Do we need to select a new pgpath? */
+	if (!m->current_pgpath ||
+	    (!m->queue_io && (m->repeat_count && --m->repeat_count == 0)))
+		__choose_pgpath(m);
+
+	pgpath = m->current_pgpath;
+
+	if (was_queued)
+		m->queue_size--;
+
+	if ((pgpath && m->queue_io) ||
+	    (!pgpath && m->queue_if_no_path)) {
+		/* Queue for the daemon to resubmit */
+		list_add_tail(&clone->queuelist, &m->queued_reqs);
+		m->queue_size++;
+		if ((m->pg_init_required && !m->pg_init_in_progress) ||
+		    !m->queue_io)
+			queue_work(kmultipathd, &m->process_queued_ios);
+		pgpath = NULL;
+		clone->q = NULL;
+		clone->rq_disk = NULL;
+		r = DM_MAPIO_SUBMITTED;
+	} else if (pgpath) {
+		bdev = pgpath->path.dev->bdev;
+		clone->q = bdev_get_queue(bdev);
+		clone->rq_disk = bdev->bd_disk;
+	} else if (__must_push_back(m)) {
+		clone->q = NULL;
+		clone->rq_disk = NULL;
+		r = DM_MAPIO_REQUEUE;
+	} else {
+		clone->q = NULL;
+		clone->rq_disk = NULL;
+		r = -EIO;	/* Failed */
+	}
+
+	mpio->pgpath = pgpath;
+
+	spin_unlock_irqrestore(&m->lock, flags);
+
+	return r;
+}
+
 /*
  * If we run out of usable paths, should we queue I/O or error it?
  */
@@ -379,7 +434,7 @@ static int queue_if_no_path(struct multi
  * The multipath daemon is responsible for resubmitting queued ios.
  *---------------------------------------------------------------*/
 
-static void dispatch_queued_ios(struct multipath *m)
+static void dispatch_queued_bios(struct multipath *m)
 {
 	int r;
 	unsigned long flags;
@@ -388,7 +443,7 @@ static void dispatch_queued_ios(struct m
 	union map_info *info;
 
 	spin_lock_irqsave(&m->lock, flags);
-	bio = bio_list_get(&m->queued_ios);
+	bio = bio_list_get(&m->queued_bios);
 	spin_unlock_irqrestore(&m->lock, flags);
 
 	while (bio) {
@@ -398,7 +453,7 @@ static void dispatch_queued_ios(struct m
 		info = dm_get_mapinfo(bio);
 		mpio = info->ptr;
 
-		r = map_io(m, bio, mpio, 1);
+		r = map_bio(m, bio, mpio, 1);
 		if (r < 0)
 			bio_endio(bio, r);
 		else if (r == DM_MAPIO_REMAPPED)
@@ -410,6 +465,36 @@ static void dispatch_queued_ios(struct m
 	}
 }
 
+static void dispatch_queued_reqs(struct multipath *m)
+{
+	int r;
+	unsigned long flags;
+	struct dm_mpath_io *mpio;
+	union map_info *info;
+	struct request *clone, *n;
+	LIST_HEAD(cl);
+
+	spin_lock_irqsave(&m->lock, flags);
+	list_splice_init(&m->queued_reqs, &cl);
+	spin_unlock_irqrestore(&m->lock, flags);
+
+	list_for_each_entry_safe(clone, n, &cl, queuelist) {
+		list_del_init(&clone->queuelist);
+
+		info = dm_get_rq_mapinfo(clone);
+		mpio = info->ptr;
+
+		r = map_req(m, clone, mpio, 1);
+		if (r < 0 || r == DM_MAPIO_REQUEUE) {
+			mempool_free(mpio, m->mpio_pool);
+			if (r == DM_MAPIO_REQUEUE)
+				r = DM_ENDIO_REQUEUE;
+			dm_end_request(clone, r);
+		} else if (r == DM_MAPIO_REMAPPED)
+			dm_dispatch_request(clone);
+	}
+}
+
 static void process_queued_ios(struct work_struct *work)
 {
 	struct multipath *m =
@@ -446,8 +531,12 @@ out:
 	if (init_required)
 		queue_work(kmpath_handlerd, &m->activate_path);
 
-	if (!must_queue)
-		dispatch_queued_ios(m);
+	if (!must_queue) {
+		if (dm_table_request_based(m->ti->table))
+			dispatch_queued_reqs(m);
+		else
+			dispatch_queued_bios(m);
+	}
 }
 
 /*
@@ -705,7 +794,7 @@ static int parse_features(struct arg_set
 	const char *param_name;
 
 	static struct param _params[] = {
-		{0, 3, "invalid number of feature args"},
+		{0, 4, "invalid number of feature args"},
 		{1, 50, "pg_init_retries must be between 1 and 50"},
 	};
 
@@ -733,6 +822,11 @@ static int parse_features(struct arg_set
 			continue;
 		}
 
+		if (!strnicmp(param_name, MESG_STR("rq_based"))) {
+			dm_table_set_request_based(ti->table);
+			continue;
+		}
+
 		ti->error = "Unrecognised multipath feature request";
 		r = -EINVAL;
 	} while (argc && !r);
@@ -821,10 +915,10 @@ static void multipath_dtr(struct dm_targ
 }
 
 /*
- * Map bios, recording original fields for later in case we have to resubmit
+ * Map cloned requests
  */
-static int multipath_map(struct dm_target *ti, struct bio *bio,
-			 union map_info *map_context)
+static int multipath_map_bio(struct dm_target *ti, struct bio *bio,
+			     union map_info *map_context)
 {
 	int r;
 	struct dm_mpath_io *mpio;
@@ -835,7 +929,30 @@ static int multipath_map(struct dm_targe
 
 	map_context->ptr = mpio;
 	bio->bi_rw |= (1 << BIO_RW_FAILFAST_TRANSPORT);
-	r = map_io(m, bio, mpio, 0);
+	r = map_bio(m, bio, mpio, 0);
+	if (r < 0 || r == DM_MAPIO_REQUEUE)
+		mempool_free(mpio, m->mpio_pool);
+
+	return r;
+}
+
+static int multipath_map_req(struct dm_target *ti, struct request *clone,
+			     union map_info *map_context)
+{
+	int r;
+	struct dm_mpath_io *mpio;
+	struct multipath *m = (struct multipath *) ti->private;
+
+	mpio = mempool_alloc(m->mpio_pool, GFP_ATOMIC);
+	if (!mpio)
+		/* ENOMEM, requeue */
+		return DM_MAPIO_REQUEUE;
+	memset(mpio, 0, sizeof(*mpio));
+
+	map_context->ptr = mpio;
+	clone->cmd_flags |= REQ_FAILFAST_TRANSPORT;
+
+	r = map_req(m, clone, mpio, 0);
 	if (r < 0 || r == DM_MAPIO_REQUEUE)
 		mempool_free(mpio, m->mpio_pool);
 
@@ -1148,18 +1265,48 @@ static int do_end_io(struct multipath *m
 	if (mpio->pgpath)
 		fail_path(mpio->pgpath);
 
-      requeue:
+requeue:
 	dm_bio_restore(&mpio->details, bio);
 
 	/* queue for the daemon to resubmit or fail */
 	spin_lock_irqsave(&m->lock, flags);
-	bio_list_add(&m->queued_ios, bio);
+	bio_list_add(&m->queued_bios, bio);
 	m->queue_size++;
 	if (!m->queue_io)
 		queue_work(kmultipathd, &m->process_queued_ios);
 	spin_unlock_irqrestore(&m->lock, flags);
 
-	return DM_ENDIO_INCOMPLETE;	/* io not complete */
+	return DM_ENDIO_INCOMPLETE;     /* io not complete */
+}
+
+static int do_end_req(struct multipath *m, struct request *clone,
+		     int error, struct dm_mpath_io *mpio)
+{
+	unsigned long flags;
+	int r;
+
+	if (!error && !clone->errors)
+		return 0;	/* I/O complete */
+
+	if (error == -EOPNOTSUPP)
+		return error;
+
+	spin_lock_irqsave(&m->lock, flags);
+	if (!m->nr_valid_paths) {
+		if (__must_push_back(m) || m->queue_if_no_path)
+			r = DM_ENDIO_REQUEUE;
+		else
+			r = -EIO;
+
+		spin_unlock_irqrestore(&m->lock, flags);
+		return r;
+	}
+	spin_unlock_irqrestore(&m->lock, flags);
+
+	if (mpio->pgpath)
+		fail_path(mpio->pgpath);
+
+	return DM_ENDIO_REQUEUE;
 }
 
 static int multipath_end_io(struct dm_target *ti, struct bio *bio,
@@ -1183,6 +1330,26 @@ static int multipath_end_io(struct dm_ta
 	return r;
 }
 
+static int multipath_end_req(struct dm_target *ti, struct request *clone,
+			    int error, union map_info *map_context)
+{
+	struct multipath *m = ti->private;
+	struct dm_mpath_io *mpio = map_context->ptr;
+	struct pgpath *pgpath = mpio->pgpath;
+	struct path_selector *ps;
+	int r;
+
+	r  = do_end_req(m, clone, error, mpio);
+	if (pgpath) {
+		ps = &pgpath->pg->ps;
+		if (ps->type->end_io)
+			ps->type->end_io(ps, &pgpath->path);
+	}
+	mempool_free(mpio, m->mpio_pool);
+
+	return r;
+}
+
 /*
  * Suspend can't complete until all the I/O is processed so if
  * the last path fails we must error any remaining I/O.
@@ -1242,12 +1409,16 @@ static int multipath_status(struct dm_ta
 	if (type == STATUSTYPE_INFO)
 		DMEMIT("2 %u %u ", m->queue_size, m->pg_init_count);
 	else {
-		DMEMIT("%u ", m->queue_if_no_path +
+		int rq_based = dm_table_request_based(ti->table);
+
+		DMEMIT("%u ", m->queue_if_no_path + rq_based +
 			      (m->pg_init_retries > 0) * 2);
 		if (m->queue_if_no_path)
 			DMEMIT("queue_if_no_path ");
 		if (m->pg_init_retries)
 			DMEMIT("pg_init_retries %u ", m->pg_init_retries);
+		if (rq_based)
+			DMEMIT("rq_based ");
 	}
 
 	if (!m->hw_handler_name || type == STATUSTYPE_INFO)
@@ -1414,6 +1585,54 @@ static int multipath_ioctl(struct dm_tar
 					 bdev->bd_disk, cmd, arg);
 }
 
+static int __pgpath_congested(struct pgpath *pgpath)
+{
+	struct request_queue *q = bdev_get_queue(pgpath->path.dev->bdev);
+
+	if (dm_underlying_device_congested(q))
+		return 1;
+
+	return 0;
+}
+
+static int multipath_congested(struct dm_target *ti)
+{
+	int congested = 0;
+	struct multipath *m = (struct multipath *) ti->private;
+	unsigned long flags;
+
+	spin_lock_irqsave(&m->lock, flags);
+
+	if (m->current_pgpath && m->repeat_count > 1) {
+		/* m->current_pgpath is surely used at next mapping time. */
+		if (__pgpath_congested(m->current_pgpath))
+			congested = 1;
+
+		goto out;
+	}
+
+	/*
+	 * We are here means that path selection will be executed
+	 * at next mapping time.
+	 * We run the path selection here and check congestion status
+	 * of the next path.
+	 * And increment repeat_count to avoid path selection again
+	 * in map_io().
+	 */
+	__choose_pgpath(m);
+	if (m->current_pgpath) {
+		if (__pgpath_congested(m->current_pgpath))
+			congested = 1;
+
+		m->repeat_count++;
+	}
+
+out:
+	spin_unlock_irqrestore(&m->lock, flags);
+
+	return congested;
+}
+
 /*-----------------------------------------------------------------
  * Module setup
  *---------------------------------------------------------------*/
@@ -1423,13 +1642,16 @@ static struct target_type multipath_targ
 	.module = THIS_MODULE,
 	.ctr = multipath_ctr,
 	.dtr = multipath_dtr,
-	.map = multipath_map,
+	.map = multipath_map_bio,
 	.end_io = multipath_end_io,
+	.map_rq = multipath_map_req,
+	.rq_end_io = multipath_end_req,
 	.presuspend = multipath_presuspend,
 	.resume = multipath_resume,
 	.status = multipath_status,
 	.message = multipath_message,
 	.ioctl  = multipath_ioctl,
+	.congested = multipath_congested,
 };
 
 static int __init dm_multipath_init(void)
