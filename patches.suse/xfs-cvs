diff -uNrp linux-2.6.5/fs/Kconfig linux-2.6.5.xfs/fs/Kconfig
--- linux-2.6.5/fs/Kconfig	2004-04-05 00:57:43.000000000 +0200
+++ linux-2.6.5.xfs/fs/Kconfig	2004-04-05 00:58:15.000000000 +0200
@@ -362,7 +362,7 @@ config XFS_RT
 	  If unsure, say N.
 
 config XFS_QUOTA
-	bool "Quota support"
+	tristate "Quota support"
 	depends on XFS_FS
 	help
 	  If you say Y here, you will be able to set limits for disk usage on
@@ -402,6 +402,43 @@ config XFS_POSIX_ACL
 
 	  If you don't know what Access Control Lists are, say N.
 
+config XFS_DMAPI
+	tristate "DMAPI support"
+	depends on XFS_FS
+	help
+	  The Data Management API is a system interface used to implement
+	  the interface defined in the X/Open document:
+	    "Systems Management: Data Storage Management (XDSM) API",
+	  dated February 1997.  This interface is used by hierarchical
+	  storage management systems.
+
+	  If unsure, say N.
+
+config XFS_DEBUG
+	bool "Debugging support (EXPERIMENTAL)"
+	depends on XFS_FS && EXPERIMENTAL
+	help
+	  Say Y here to get an XFS build with many debugging features,
+	  including ASSERT checks, function wrappers around macros,
+	  and extra sanity-checking functions in various code paths.
+
+	  Note that the resulting code will be HUGE and SLOW, and probably
+	  not useful unless you are debugging a particular problem.
+
+	  Say N unless you are an XFS developer, or you play one on TV.
+
+config XFS_TRACE
+	bool "Tracing support (EXPERIMENTAL)"
+	depends on XFS_FS && EXPERIMENTAL
+	help
+	  Say Y here to get an XFS build with activity tracing enabled.
+	  Enabling this option will attach historical information to XFS
+	  inodes, pagebufs, certain locks, the log, the IO path, and a      
+	  few other key areas within XFS.  These traces can be examined
+	  using the kdb kernel debugger.
+
+	  Say N unless you are an XFS developer.              
+
 config MINIX_FS
 	tristate "Minix fs support"
 	help
diff -uNrp linux-2.6.5/fs/xfs/Makefile linux-2.6.5.xfs/fs/xfs/Makefile
--- linux-2.6.5/fs/xfs/Makefile	2004-04-04 05:37:37.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/Makefile	2004-04-05 00:58:26.000000000 +0200
@@ -1,5 +1,5 @@
 #
-# Copyright (c) 2000-2003 Silicon Graphics, Inc.  All Rights Reserved.
+# Copyright (c) 2000-2004 Silicon Graphics, Inc.  All Rights Reserved.
 #
 # This program is free software; you can redistribute it and/or modify it
 # under the terms of version 2 of the GNU General Public License as
@@ -30,7 +30,9 @@
 # http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
 #
 
-EXTRA_CFLAGS +=	 -Ifs/xfs -Ifs/xfs/linux -funsigned-char
+EXTRA_CFLAGS +=	 -Ifs/xfs -Ifs/xfs/linux-2.6 -funsigned-char
+
+XFS_LINUX := linux-2.6
 
 ifeq ($(CONFIG_XFS_DEBUG),y)
 	EXTRA_CFLAGS += -g -DSTATIC="" -DDEBUG -DXFSDEBUG
@@ -53,31 +55,21 @@ ifeq ($(CONFIG_XFS_TRACE),y)
 endif
 
 obj-$(CONFIG_XFS_FS)		+= xfs.o
-
-xfs-$(CONFIG_XFS_QUOTA)		+= $(addprefix quota/, \
-				   xfs_dquot.o \
-				   xfs_dquot_item.o \
-				   xfs_trans_dquot.o \
-				   xfs_qm_syscalls.o \
-				   xfs_qm_bhv.o \
-				   xfs_qm.o)
-ifeq ($(CONFIG_XFS_QUOTA),y)
-xfs-$(CONFIG_PROC_FS)		+= quota/xfs_qm_stats.o
-endif
+obj-$(CONFIG_XFS_DMAPI)		+= dmapi/
+obj-$(CONFIG_XFS_QUOTA)		+= quota/
 
 xfs-$(CONFIG_XFS_RT)		+= xfs_rtalloc.o
 xfs-$(CONFIG_XFS_POSIX_ACL)	+= xfs_acl.o
 xfs-$(CONFIG_XFS_POSIX_CAP)	+= xfs_cap.o
 xfs-$(CONFIG_XFS_POSIX_MAC)	+= xfs_mac.o
-xfs-$(CONFIG_PROC_FS)		+= linux/xfs_stats.o
-xfs-$(CONFIG_SYSCTL)		+= linux/xfs_sysctl.o
+xfs-$(CONFIG_PROC_FS)		+= $(XFS_LINUX)/xfs_stats.o
+xfs-$(CONFIG_SYSCTL)		+= $(XFS_LINUX)/xfs_sysctl.o
 
 
 xfs-y				+= xfs_alloc.o \
 				   xfs_alloc_btree.o \
 				   xfs_attr.o \
 				   xfs_attr_leaf.o \
-				   xfs_behavior.o \
 				   xfs_bit.o \
 				   xfs_bmap.o \
 				   xfs_bmap_btree.o \
@@ -101,7 +93,6 @@ xfs-y				+= xfs_alloc.o \
 				   xfs_inode.o \
 				   xfs_inode_item.o \
 				   xfs_iocore.o \
-				   xfs_iomap.o \
 				   xfs_itable.o \
 				   xfs_dfrag.o \
 				   xfs_log.o \
@@ -119,15 +110,17 @@ xfs-y				+= xfs_alloc.o \
 				   xfs_vfsops.o \
 				   xfs_vnodeops.o \
 				   xfs_rw.o \
+				   xfs_behavior.o \
+				   xfs_iomap.o \
 				   xfs_dmops.o \
 				   xfs_qmops.o
 
 xfs-$(CONFIG_XFS_TRACE)		+= xfs_dir2_trace.o
 
+
 # Objects in linux/
-xfs-y				+= $(addprefix linux/, \
+xfs-y				+= $(addprefix $(XFS_LINUX)/, \
 				   xfs_aops.o \
-				   xfs_buf.o \
 				   xfs_file.o \
 				   xfs_fs_subr.o \
 				   xfs_globals.o \
@@ -136,7 +129,9 @@ xfs-y				+= $(addprefix linux/, \
 				   xfs_lrw.o \
 				   xfs_super.o \
 				   xfs_vfs.o \
-				   xfs_vnode.o)
+				   xfs_vnode.o \
+				   xfs_buf.o \
+				   xfs_ksyms.o)
 
 # Objects in support/
 xfs-y				+= $(addprefix support/, \
@@ -147,3 +142,17 @@ xfs-y				+= $(addprefix support/, \
 
 xfs-$(CONFIG_XFS_TRACE)		+= support/ktrace.o
 
+# If both xfs and kdb modules are built in then xfsidbg is built in.  If xfs is
+# a module and kdb modules are being compiled then xfsidbg must be a module, to
+# follow xfs.  If xfs is built in then xfsidbg tracks the kdb module state.
+# This must come after the main xfs code so xfs initialises before xfsidbg.
+# KAO
+ifneq ($(CONFIG_KDB_MODULES),)
+  ifeq ($(CONFIG_XFS_FS),y)
+    obj-$(CONFIG_KDB_MODULES)	+= xfsidbg.o
+  else
+    obj-$(CONFIG_XFS_FS)	+= xfsidbg.o
+  endif
+endif
+
+CFLAGS_xfsidbg.o += -Iarch/$(ARCH)/kdb
diff -uNrp linux-2.6.5/fs/xfs/dmapi/Makefile linux-2.6.5.xfs/fs/xfs/dmapi/Makefile
--- linux-2.6.5/fs/xfs/dmapi/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/dmapi/Makefile	2004-04-05 00:58:15.000000000 +0200
@@ -0,0 +1,55 @@
+#
+# Copyright (c) 2000-2003 Silicon Graphics, Inc.  All Rights Reserved.
+#
+# This program is free software; you can redistribute it and/or modify it
+# under the terms of version 2 of the GNU General Public License as
+# published by the Free Software Foundation.
+#
+# This program is distributed in the hope that it would be useful, but
+# WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+#
+# Further, this software is distributed without any warranty that it is
+# free of the rightful claim of any third person regarding infringement
+# or the like.	Any license provided herein, whether implied or
+# otherwise, applies only to this software file.  Patent licenses, if
+# any, provided herein do not apply to combinations of this program with
+# other software, or any other product whatsoever.
+#
+# You should have received a copy of the GNU General Public License along
+# with this program; if not, write the Free Software Foundation, Inc., 59
+# Temple Place - Suite 330, Boston MA 02111-1307, USA.
+#
+# Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+# Mountain View, CA  94043, or:
+#
+# http://www.sgi.com
+#
+# For further information regarding this notice, see:
+#
+# http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+#
+
+EXTRA_CFLAGS += -I $(TOPDIR)/fs/xfs -I $(TOPDIR)/fs/xfs/linux-2.6
+
+ifeq ($(CONFIG_XFS_DEBUG),y)
+	EXTRA_CFLAGS += -g -DDEBUG -DXFSDEBUG
+endif
+
+obj-$(CONFIG_XFS_DMAPI)		+= xfs_dmapi.o
+
+xfs_dmapi-y			+= dmapi_sysent.o \
+				   dmapi_attr.o \
+				   dmapi_config.o \
+				   dmapi_bulkattr.o \
+				   dmapi_dmattr.o \
+				   dmapi_event.o \
+				   dmapi_handle.o \
+				   dmapi_hole.o \
+				   dmapi_io.o \
+				   dmapi_mountinfo.o \
+				   dmapi_region.o \
+				   dmapi_register.o \
+				   dmapi_right.o \
+				   dmapi_session.o \
+				   dmapi_xfs.o
diff -uNrp linux-2.6.5/fs/xfs/dmapi/Status linux-2.6.5.xfs/fs/xfs/dmapi/Status
--- linux-2.6.5/fs/xfs/dmapi/Status	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/dmapi/Status	2004-04-05 00:58:15.000000000 +0200
@@ -0,0 +1,128 @@
+Jan21,04 - dm_get_bulkall is now implemented.  roehrich
+
+for linux:
+
+
+68 external interfaces in libdm
+
+   56 of those interfaces go through to dmi(), the kernel side of DMAPI
+
+
+
+Functions known to work
+----------------------------------------------
+
+dm_create_session
+dm_create_userevent
+dm_destroy_session
+dm_getall_sessions
+dm_getall_tokens
+dm_get_allocinfo
+dm_get_bulkall
+dm_get_bulkattr
+dm_get_config_events
+dm_get_dmattr
+dm_get_eventlist
+dm_get_events
+dm_get_fileattr
+dm_get_region
+dm_handle_free
+dm_init_attrloc
+dm_init_service
+dm_obj_ref_hold
+dm_obj_ref_query
+dm_obj_ref_rele
+dm_path_to_fshandle
+dm_path_to_handle
+dm_punch_hole
+dm_query_session
+dm_read_invis
+dm_remove_dmattr
+dm_respond_event
+dm_send_msg
+dm_set_disp
+dm_set_dmattr
+dm_set_eventlist
+dm_set_fileattr
+dm_set_region
+dm_sync_by_handle
+dm_write_invis
+35
+
+Functions that seem to work (would like more rigorous test case)
+------------------------------------------
+
+dm_pending
+dm_probe_hole		- one test case of test_hole.c fails
+dm_request_right
+3
+
+Functions untested but probably work
+----------------------------------------------
+
+dm_find_eventmsg
+dm_handle_cmp
+dm_handle_to_fshandle
+dm_handle_to_ino
+dm_release_right
+5
+
+Functions that do not work
+-----------------------------------------
+
+dm_get_dioinfo		- directio not implemented
+1
+
+Functions not supported in SGI DMAPI
+-------------------------------------------------------------
+
+dm_clear_inherit
+dm_create_by_handle
+dm_getall_inherit
+dm_mkdir_by_handle
+dm_set_inherit
+dm_symlink_by_handle
+
+
+
+
+Functions that seem to work (would like more rigorous test case)
+----------------------------------------------------------------
+
+dm_get_config
+dm_downgrade_right
+dm_get_mountinfo
+dm_set_return_on_destory
+dm_upgrade_right
+
+
+
+Functions that do not work
+-----------------------------------------------------------------
+
+dm_fd_to_handle		- Irix getf not implemented on linux
+dm_get_dirattrs		- null pointer reference
+dm_handle_to_path
+dm_getall_dmattr	- needs a copy_from_user in place of useracc
+
+
+Functions that are untested, but probably work
+-----------------------------------------------------------------
+
+dm_getall_disp
+dm_handle_hash
+dm_handle_is_valid
+dm_handle_to_fsid
+dm_handle_to_igen
+dm_make_fshandle
+dm_make_handle
+dm_move_event
+dm_query_right
+
+
+
+Other things not working
+----------------------------------
+
+- read/write events for memory-mapped I/O?
+
diff -uNrp linux-2.6.5/fs/xfs/dmapi/dmapi.h linux-2.6.5.xfs/fs/xfs/dmapi/dmapi.h
--- linux-2.6.5/fs/xfs/dmapi/dmapi.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/dmapi/dmapi.h	2004-04-05 00:58:15.000000000 +0200
@@ -0,0 +1,1053 @@
+/*
+ * Copyright (c) 1995-2003 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2.1 of the GNU Lesser General Public License
+ * as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.	 Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with this program; if not, write the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston MA 02111-1307,
+ * USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+
+#ifndef __DMAPI_H__
+#define __DMAPI_H__
+
+#ifdef	__cplusplus
+extern	"C" {
+#endif
+
+#ifndef __KERNEL__
+#include <sys/types.h>
+#endif
+#include <linux/types.h>
+
+/**************************************************************************
+ *									  *
+ * The SGI implementation of DMAPI is based upon the X/Open document	  *
+ *	Systems Management: Data Storage Managment (XDSM) API		  *
+ * dated February 1997.	 Not all DMAPI functions and structure fields	  *
+ * have been implemented.  Most importantly, the DMAPI functions	  *
+ * dm_request_right, dm_release_right, dm_query_right, dm_upgrade_right	  *
+ * and dm_downgrade_right do not work as described in the specification.  *
+ *									  *
+ * The XFS filesystem currently does not allow its locking mechanisms to  *
+ * be externally accessed from user space.  While the above-mentioned	  *
+ * dm_xxx_right functions exist and can be called by applications, they	  *
+ * always return successfully without actually obtaining any locks	  *
+ * within the filesystem.						  *
+ *									  *
+ * Applications which do not need full rights support and which only	  *
+ * make dm_xxx_right calls in order to satisfy the input requirements of  *
+ * other DMAPI calls should be able to use these routines to avoid	  *
+ * having to implement special-case code for SGI platforms.  Applications *
+ * which truely need the capabilities of a full implementation of rights  *
+ * will unfortunately have to come up with alternate software solutions	  *
+ * until such time as rights can be completely implemented.		  *
+ *									  *
+ * Functions and structure fields defined within this file which are not  *
+ * supported in the SGI implementation of DMAPI are indicated by comments *
+ * following their definitions such as "not supported", or "not		  *
+ * completely supported".  Any function or field not so marked may be	  *
+ * assumed to work exactly according to the spec.			  *
+ *									  *
+ **************************************************************************/
+
+
+
+/* The first portion of this file contains defines and typedefs that are
+   DMAPI implementation-dependent, and could be different on other platforms.
+*/
+
+typedef __s64		dm_attrloc_t;
+typedef unsigned int	dm_boolean_t;
+typedef __u64		dm_eventset_t;
+typedef __u64		dm_fsid_t;
+typedef __u64		dm_ino_t;
+typedef __u32		dm_igen_t;
+typedef __s64		dm_off_t;
+typedef unsigned int	dm_sequence_t;
+typedef int		dm_sessid_t;
+typedef __u64		dm_size_t;
+typedef __s64		dm_ssize_t;
+typedef int		dm_token_t;
+
+/* XXX dev_t, mode_t, and nlink_t are not the same size in kernel space
+   and user space.  This affects the field offsets for dm_stat_t.
+   The following solution is temporary.
+
+   user space sizes:  dev_t=8  mode_t=4	 nlink_t=4
+   kernel space	   :  dev_t=2  mode_t=2	 nlink_t=2
+
+*/
+typedef __s64		dm_dev_t;
+typedef int		dm_mode_t;
+typedef int		dm_nlink_t;
+
+
+#define DM_REGION_NOEVENT	0x0
+#define DM_REGION_READ		0x1
+#define DM_REGION_WRITE		0x2
+#define DM_REGION_TRUNCATE	0x4
+
+/* Values for the mask argument used with dm_get_fileattr, dm_get_bulkattr,
+   dm_get_dirattrs, and dm_set_fileattr.
+*/
+
+#define DM_AT_MODE	0x0001
+#define DM_AT_UID	0x0002
+#define DM_AT_GID	0x0004
+#define DM_AT_ATIME	0x0008
+#define DM_AT_MTIME	0x0010
+#define DM_AT_CTIME	0x0020
+#define DM_AT_SIZE	0x0040
+#define DM_AT_DTIME	0x0080
+#define DM_AT_HANDLE	0x0100
+#define DM_AT_EMASK	0x0200
+#define DM_AT_PMANR	0x0400
+#define DM_AT_PATTR	0x0800
+#define DM_AT_STAT	0x1000
+#define DM_AT_CFLAG	0x2000
+
+#define DM_EV_WAIT	0x1		/* used in dm_get_events() */
+
+#define DM_MOUNT_RDONLY 0x1		/* me_mode field in dm_mount_event_t */
+
+#define DM_RR_WAIT	0x1
+
+#define DM_UNMOUNT_FORCE 0x1		/* ne_mode field in dm_namesp_event_t */
+
+#define DM_WRITE_SYNC	0x1		/* used in dm_write_invis() */
+
+#define DM_SESSION_INFO_LEN	256
+#define DM_NO_SESSION		0
+#define DM_TRUE			1
+#define DM_FALSE		0
+#define DM_INVALID_TOKEN	0
+#define DM_NO_TOKEN		(-1)
+#define DM_INVALID_HANP		NULL
+#define DM_INVALID_HLEN		0
+#define DM_GLOBAL_HANP		((void *)(1LL))
+#define DM_GLOBAL_HLEN		((size_t)(1))
+#define DM_VER_STR_CONTENTS	"SGI DMAPI (XDSM) API, Release 1.0."
+
+
+#define DMEV_SET(event_type, event_list) \
+	((event_list) |= (1 << (event_type)))
+#define DMEV_CLR(event_type, event_list) \
+	((event_list) &= ~(1 << (event_type)))
+#define DMEV_ISSET(event_type, event_list) \
+	(int)(((event_list) & (1 << (event_type))) != 0)
+#define DMEV_ZERO(event_list) \
+	(event_list) = 0
+
+
+typedef struct {
+	int	vd_offset;	/* offset from start of containing struct */
+	unsigned int	vd_length;	/* length of data starting at vd_offset */
+} dm_vardata_t;
+
+#define DM_GET_VALUE(p, field, type) \
+	((type) ((char *)(p) + (p)->field.vd_offset))
+
+#define DM_GET_LEN(p, field) \
+	((p)->field.vd_length)
+
+#define DM_STEP_TO_NEXT(p, type) \
+	((type) ((p)->_link ? (char *)(p) + (p)->_link : NULL))
+
+
+
+
+/* The remainder of this include file contains defines, typedefs, and
+   structures which are strictly defined by the DMAPI 2.3 specification.
+
+   (The _link field which appears in several structures is an
+   implementation-specific way to implement DM_STEP_TO_NEXT, and
+   should not be referenced directly by application code.)
+*/
+
+
+#define DM_ATTR_NAME_SIZE	8
+
+
+struct dm_attrname {
+	unsigned char	an_chars[DM_ATTR_NAME_SIZE];
+};
+typedef struct dm_attrname	dm_attrname_t;
+
+
+struct dm_attrlist {
+	int		_link;
+	dm_attrname_t	al_name;
+	dm_vardata_t	al_data;
+};
+typedef struct dm_attrlist	dm_attrlist_t;
+
+
+typedef enum {
+	DM_CONFIG_INVALID,
+	DM_CONFIG_BULKALL,
+	DM_CONFIG_CREATE_BY_HANDLE,
+	DM_CONFIG_DTIME_OVERLOAD,
+	DM_CONFIG_LEGACY,
+	DM_CONFIG_LOCK_UPGRADE,
+	DM_CONFIG_MAX_ATTR_ON_DESTROY,
+	DM_CONFIG_MAX_ATTRIBUTE_SIZE,
+	DM_CONFIG_MAX_HANDLE_SIZE,
+	DM_CONFIG_MAX_MANAGED_REGIONS,
+	DM_CONFIG_MAX_MESSAGE_DATA,
+	DM_CONFIG_OBJ_REF,
+	DM_CONFIG_PENDING,
+	DM_CONFIG_PERS_ATTRIBUTES,
+	DM_CONFIG_PERS_EVENTS,
+	DM_CONFIG_PERS_INHERIT_ATTRIBS,
+	DM_CONFIG_PERS_MANAGED_REGIONS,
+	DM_CONFIG_PUNCH_HOLE,
+	DM_CONFIG_TOTAL_ATTRIBUTE_SPACE,
+	DM_CONFIG_WILL_RETRY
+} dm_config_t;
+
+
+struct	dm_dioinfo {			/* non-standard SGI addition */
+	unsigned int	d_mem;
+	unsigned int	d_miniosz;
+	unsigned int	d_maxiosz;
+	dm_boolean_t	d_dio_only;
+};
+typedef struct dm_dioinfo	dm_dioinfo_t;
+
+
+struct dm_dispinfo {
+	int		_link;
+	unsigned int	di_pad1;		/* reserved; do not reference */
+	dm_vardata_t	di_fshandle;
+	dm_eventset_t	di_eventset;
+};
+typedef struct dm_dispinfo	dm_dispinfo_t;
+
+
+#ifndef HAVE_DM_EVENTTYPE_T
+#define HAVE_DM_EVENTTYPE_T
+typedef enum {
+	DM_EVENT_INVALID	= -1,
+	DM_EVENT_CANCEL		= 0,		/* not supported */
+	DM_EVENT_MOUNT		= 1,
+	DM_EVENT_PREUNMOUNT	= 2,
+	DM_EVENT_UNMOUNT	= 3,
+	DM_EVENT_DEBUT		= 4,		/* not supported */
+	DM_EVENT_CREATE		= 5,
+	DM_EVENT_CLOSE		= 6,		/* not supported */
+	DM_EVENT_POSTCREATE	= 7,
+	DM_EVENT_REMOVE		= 8,
+	DM_EVENT_POSTREMOVE	= 9,
+	DM_EVENT_RENAME		= 10,
+	DM_EVENT_POSTRENAME	= 11,
+	DM_EVENT_LINK		= 12,
+	DM_EVENT_POSTLINK	= 13,
+	DM_EVENT_SYMLINK	= 14,
+	DM_EVENT_POSTSYMLINK	= 15,
+	DM_EVENT_READ		= 16,
+	DM_EVENT_WRITE		= 17,
+	DM_EVENT_TRUNCATE	= 18,
+	DM_EVENT_ATTRIBUTE	= 19,
+	DM_EVENT_DESTROY	= 20,
+	DM_EVENT_NOSPACE	= 21,
+	DM_EVENT_USER		= 22,
+	DM_EVENT_MAX		= 23
+} dm_eventtype_t;
+#endif
+
+
+struct dm_eventmsg {
+	int		_link;
+	dm_eventtype_t	ev_type;
+	dm_token_t	ev_token;
+	dm_sequence_t	ev_sequence;
+	dm_vardata_t	ev_data;
+};
+typedef struct dm_eventmsg	dm_eventmsg_t;
+
+
+struct dm_cancel_event {			/* not supported */
+	dm_sequence_t	ce_sequence;
+	dm_token_t	ce_token;
+};
+typedef struct dm_cancel_event	dm_cancel_event_t;
+
+
+struct dm_data_event {
+	dm_vardata_t	de_handle;
+	dm_off_t	de_offset;
+	dm_size_t	de_length;
+};
+typedef struct dm_data_event dm_data_event_t;
+
+struct dm_destroy_event {
+	dm_vardata_t		ds_handle;
+	dm_attrname_t		ds_attrname;
+	dm_vardata_t		ds_attrcopy;
+};
+typedef struct dm_destroy_event dm_destroy_event_t;
+
+struct dm_mount_event {
+	dm_mode_t	me_mode;
+	dm_vardata_t	me_handle1;
+	dm_vardata_t	me_handle2;
+	dm_vardata_t	me_name1;
+	dm_vardata_t	me_name2;
+	dm_vardata_t	me_roothandle;
+};
+typedef struct dm_mount_event dm_mount_event_t;
+
+struct dm_namesp_event {
+	dm_mode_t	ne_mode;
+	dm_vardata_t	ne_handle1;
+	dm_vardata_t	ne_handle2;
+	dm_vardata_t	ne_name1;
+	dm_vardata_t	ne_name2;
+	int		ne_retcode;
+};
+typedef struct dm_namesp_event dm_namesp_event_t;
+
+
+typedef enum {
+	DM_EXTENT_INVALID,
+	DM_EXTENT_RES,
+	DM_EXTENT_HOLE
+} dm_extenttype_t;
+
+
+struct dm_extent {
+	dm_extenttype_t ex_type;
+	unsigned int	ex_pad1;		/* reserved; do not reference */
+	dm_off_t	ex_offset;
+	dm_size_t	ex_length;
+};
+typedef struct dm_extent dm_extent_t;
+
+struct dm_fileattr {
+	dm_mode_t	fa_mode;
+	uid_t		fa_uid;
+	gid_t		fa_gid;
+	time_t		fa_atime;
+	time_t		fa_mtime;
+	time_t		fa_ctime;
+	time_t		fa_dtime;
+	unsigned int	fa_pad1;		/* reserved; do not reference */
+	dm_off_t	fa_size;
+};
+typedef struct dm_fileattr dm_fileattr_t;
+
+
+struct dm_inherit {				/* not supported */
+	dm_attrname_t	ih_name;
+	dm_mode_t	ih_filetype;
+};
+typedef struct dm_inherit dm_inherit_t;
+
+
+typedef enum {
+	DM_MSGTYPE_INVALID,
+	DM_MSGTYPE_SYNC,
+	DM_MSGTYPE_ASYNC
+} dm_msgtype_t;
+
+
+struct dm_region {
+	dm_off_t	rg_offset;
+	dm_size_t	rg_size;
+	unsigned int	rg_flags;
+	unsigned int	rg_pad1;		/* reserved; do not reference */
+};
+typedef struct dm_region dm_region_t;
+
+
+typedef enum {
+	DM_RESP_INVALID,
+	DM_RESP_CONTINUE,
+	DM_RESP_ABORT,
+	DM_RESP_DONTCARE
+} dm_response_t;
+
+
+#ifndef HAVE_DM_RIGHT_T
+#define HAVE_DM_RIGHT_T
+typedef enum {
+	DM_RIGHT_NULL,
+	DM_RIGHT_SHARED,
+	DM_RIGHT_EXCL
+} dm_right_t;
+#endif
+
+
+struct dm_stat {
+	int		_link;
+	dm_vardata_t	dt_handle;
+	dm_vardata_t	dt_compname;
+	int		dt_nevents;
+	dm_eventset_t	dt_emask;
+	int		dt_pers;		/* field not supported */
+	int		dt_pmanreg;
+	time_t		dt_dtime;
+	unsigned int	dt_change;		/* field not supported */
+	unsigned int	dt_pad1;		/* reserved; do not reference */
+	dm_dev_t	dt_dev;
+	dm_ino_t	dt_ino;
+	dm_mode_t	dt_mode;
+	dm_nlink_t	dt_nlink;
+	uid_t		dt_uid;
+	gid_t		dt_gid;
+	dm_dev_t	dt_rdev;
+	unsigned int	dt_pad2;		/* reserved; do not reference */
+	dm_off_t	dt_size;
+	time_t		dt_atime;
+	time_t		dt_mtime;
+	time_t		dt_ctime;
+	unsigned int	dt_blksize;
+	dm_size_t	dt_blocks;
+
+	/* Non-standard filesystem-specific fields.  Currently XFS is the only
+	   supported filesystem type.
+	*/
+
+	__u64	dt_pad3;	/* reserved; do not reference */
+	int		dt_fstype;	/* filesystem index; see sysfs(2) */
+	union	{
+		struct	{
+			dm_igen_t	igen;
+			unsigned int	xflags;
+			unsigned int	extsize;
+			unsigned int	extents;
+			unsigned short	aextents;
+			unsigned short	dmstate;
+		} sgi_xfs;
+	} fsys_dep;
+};
+typedef struct dm_stat	dm_stat_t;
+
+#define dt_xfs_igen	fsys_dep.sgi_xfs.igen
+#define dt_xfs_xflags	fsys_dep.sgi_xfs.xflags
+#define dt_xfs_extsize	fsys_dep.sgi_xfs.extsize
+#define dt_xfs_extents	fsys_dep.sgi_xfs.extents
+#define dt_xfs_aextents fsys_dep.sgi_xfs.aextents
+#define dt_xfs_dmstate	fsys_dep.sgi_xfs.dmstate
+
+/* Flags for the non-standard dt_xfs_xflags field. */
+
+#define DM_XFLAG_REALTIME	0x1
+#define DM_XFLAG_PREALLOC	0x2
+#define DM_XFLAG_IMMUTABLE	0x3
+#define DM_XFLAG_APPEND		0x4
+#define DM_XFLAG_SYNC		0x5
+#define DM_XFLAG_NOATIME	0x6
+#define DM_XFLAG_NODUMP		0x7
+#define DM_XFLAG_HASATTR	0x80000000
+
+
+struct	dm_timestruct {
+	time_t		dm_tv_sec;
+	int		dm_tv_nsec;
+};
+typedef struct dm_timestruct dm_timestruct_t;
+
+
+struct	dm_xstat {				/* not supported */
+	dm_stat_t	dx_statinfo;
+	dm_vardata_t	dx_attrdata;
+};
+typedef struct dm_xstat dm_xstat_t;
+
+
+
+/* The following list provides the prototypes for all functions defined in
+   the DMAPI interface.
+*/
+
+extern int
+dm_clear_inherit(				/* not supported */
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_attrname_t	*attrnamep);
+
+extern int
+dm_create_by_handle(				/* not supported */
+	dm_sessid_t	sid,
+	void		*dirhanp,
+	size_t		dirhlen,
+	dm_token_t	token,
+	void		*hanp,
+	size_t		hlen,
+	char		*cname);
+
+extern int
+dm_create_session(
+	dm_sessid_t	oldsid,
+	char		*sessinfop,
+	dm_sessid_t	*newsidp);
+
+extern int
+dm_create_userevent(
+	dm_sessid_t	sid,
+	size_t		msglen,
+	void		*msgdatap,
+	dm_token_t	*tokenp);
+
+extern int
+dm_destroy_session(
+	dm_sessid_t	sid);
+
+extern int
+dm_downgrade_right(		/* not completely supported; see caveat above */
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token);
+
+extern int
+dm_fd_to_handle(
+	int		fd,
+	void		**hanpp,
+	size_t		*hlenp);
+
+extern int
+dm_find_eventmsg(
+	dm_sessid_t	sid,
+	dm_token_t	token,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp);
+
+extern int
+dm_get_allocinfo(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_off_t	*offp,
+	unsigned int	nelem,
+	dm_extent_t	*extentp,
+	unsigned int	*nelemp);
+
+extern int
+dm_get_bulkall(					/* not supported */
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	unsigned int	mask,
+	dm_attrname_t	*attrnamep,
+	dm_attrloc_t	*locp,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp);
+
+extern int
+dm_get_bulkattr(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	unsigned int	mask,
+	dm_attrloc_t	*locp,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp);
+
+extern int
+dm_get_config(
+	void		*hanp,
+	size_t		hlen,
+	dm_config_t	flagname,
+	dm_size_t	*retvalp);
+
+extern int
+dm_get_config_events(
+	void		*hanp,
+	size_t		hlen,
+	unsigned int	nelem,
+	dm_eventset_t	*eventsetp,
+	unsigned int	*nelemp);
+
+extern int
+dm_get_dirattrs(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	unsigned int	mask,
+	dm_attrloc_t	*locp,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp);
+
+extern int
+dm_get_dmattr(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_attrname_t	*attrnamep,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp);
+
+extern int
+dm_get_eventlist(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	unsigned int	nelem,
+	dm_eventset_t	*eventsetp,
+	unsigned int	*nelemp);
+
+extern int
+dm_get_events(
+	dm_sessid_t	sid,
+	unsigned int	maxmsgs,
+	unsigned int	flags,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp);
+
+extern int
+dm_get_fileattr(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	unsigned int	mask,
+	dm_stat_t	*statp);
+
+extern int
+dm_get_mountinfo(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp);
+
+extern int
+dm_get_region(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	unsigned int	nelem,
+	dm_region_t	*regbufp,
+	unsigned int	*nelemp);
+
+extern int
+dm_getall_disp(
+	dm_sessid_t	sid,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp);
+
+extern int
+dm_getall_dmattr(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp);
+
+extern int
+dm_getall_inherit(				/* not supported */
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	unsigned int	nelem,
+	dm_inherit_t	*inheritbufp,
+	unsigned int	*nelemp);
+
+extern int
+dm_getall_sessions(
+	unsigned int	nelem,
+	dm_sessid_t	*sidbufp,
+	unsigned int	*nelemp);
+
+extern int
+dm_getall_tokens(
+	dm_sessid_t	sid,
+	unsigned int	nelem,
+	dm_token_t	*tokenbufp,
+	unsigned int	*nelemp);
+
+extern int
+dm_handle_cmp(
+	void		*hanp1,
+	size_t		hlen1,
+	void		*hanp2,
+	size_t		hlen2);
+
+extern void
+dm_handle_free(
+	void		*hanp,
+	size_t		hlen);
+
+extern u_int
+dm_handle_hash(
+	void		*hanp,
+	size_t		hlen);
+
+extern dm_boolean_t
+dm_handle_is_valid(
+	void		*hanp,
+	size_t		hlen);
+
+extern int
+dm_handle_to_fshandle(
+	void		*hanp,
+	size_t		hlen,
+	void		**fshanpp,
+	size_t		*fshlenp);
+
+extern int
+dm_handle_to_fsid(
+	void		*hanp,
+	size_t		hlen,
+	dm_fsid_t	*fsidp);
+
+extern int
+dm_handle_to_igen(
+	void		*hanp,
+	size_t		hlen,
+	dm_igen_t	*igenp);
+
+extern int
+dm_handle_to_ino(
+	void		*hanp,
+	size_t		hlen,
+	dm_ino_t	*inop);
+
+extern int
+dm_handle_to_path(
+	void		*dirhanp,
+	size_t		dirhlen,
+	void		*targhanp,
+	size_t		targhlen,
+	size_t		buflen,
+	char		*pathbufp,
+	size_t		*rlenp);
+
+extern int
+dm_init_attrloc(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_attrloc_t	*locp);
+
+extern int
+dm_init_service(
+	char		**versionstrpp);
+
+extern int
+dm_make_handle(
+	dm_fsid_t	*fsidp,
+	dm_ino_t	*inop,
+	dm_igen_t	*igenp,
+	void		**hanpp,
+	size_t		*hlenp);
+
+extern int
+dm_make_fshandle(
+	dm_fsid_t	*fsidp,
+	void		**hanpp,
+	size_t		*hlenp);
+
+extern int
+dm_mkdir_by_handle(				/* not supported */
+	dm_sessid_t	sid,
+	void		*dirhanp,
+	size_t		dirhlen,
+	dm_token_t	token,
+	void		*hanp,
+	size_t		hlen,
+	char		*cname);
+
+extern int
+dm_move_event(
+	dm_sessid_t	srcsid,
+	dm_token_t	token,
+	dm_sessid_t	targetsid,
+	dm_token_t	*rtokenp);
+
+extern int
+dm_obj_ref_hold(
+	dm_sessid_t	sid,
+	dm_token_t	token,
+	void		*hanp,
+	size_t		hlen);
+
+extern int
+dm_obj_ref_query(
+	dm_sessid_t	sid,
+	dm_token_t	token,
+	void		*hanp,
+	size_t		hlen);
+
+extern int
+dm_obj_ref_rele(
+	dm_sessid_t	sid,
+	dm_token_t	token,
+	void		*hanp,
+	size_t		hlen);
+
+extern int
+dm_path_to_fshandle(
+	char		*path,
+	void		**hanpp,
+	size_t		*hlenp);
+
+extern int
+dm_path_to_handle(
+	char		*path,
+	void		**hanpp,
+	size_t		*hlenp);
+
+extern int
+dm_pending(
+	dm_sessid_t	sid,
+	dm_token_t	token,
+	dm_timestruct_t *delay);
+
+extern int
+dm_probe_hole(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_off_t	off,
+	dm_size_t	len,
+	dm_off_t	*roffp,
+	dm_size_t	*rlenp);
+
+extern int
+dm_punch_hole(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_off_t	off,
+	dm_size_t	len);
+
+extern int
+dm_query_right(			/* not completely supported; see caveat above */
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_right_t	*rightp);
+
+extern int
+dm_query_session(
+	dm_sessid_t	sid,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp);
+
+extern dm_ssize_t
+dm_read_invis(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_off_t	off,
+	dm_size_t	len,
+	void		*bufp);
+
+extern int
+dm_release_right(		/* not completely supported; see caveat above */
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token);
+
+extern int
+dm_remove_dmattr(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	int		setdtime,
+	dm_attrname_t	*attrnamep);
+
+extern int
+dm_request_right(		/* not completely supported; see caveat above */
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	unsigned int	flags,
+	dm_right_t	right);
+
+extern int
+dm_respond_event(
+	dm_sessid_t	sid,
+	dm_token_t	token,
+	dm_response_t	response,
+	int		reterror,
+	size_t		buflen,
+	void		*respbufp);
+
+extern int
+dm_send_msg(
+	dm_sessid_t	targetsid,
+	dm_msgtype_t	msgtype,
+	size_t		buflen,
+	void		*bufp);
+
+extern int
+dm_set_disp(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_eventset_t	*eventsetp,
+	unsigned int	maxevent);
+
+extern int
+dm_set_dmattr(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_attrname_t	*attrnamep,
+	int		setdtime,
+	size_t		buflen,
+	void		*bufp);
+
+extern int
+dm_set_eventlist(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_eventset_t	*eventsetp,
+	unsigned int	maxevent);
+
+extern int
+dm_set_fileattr(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	unsigned int	mask,
+	dm_fileattr_t	*attrp);
+
+extern int
+dm_set_inherit(					/* not supported */
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_attrname_t	*attrnamep,
+	mode_t		mode);
+
+extern int
+dm_set_region(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	unsigned int	nelem,
+	dm_region_t	*regbufp,
+	dm_boolean_t	*exactflagp);
+
+extern int
+dm_set_return_on_destroy(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_attrname_t	*attrnamep,
+	dm_boolean_t	enable);
+
+extern int
+dm_symlink_by_handle(				/* not supported */
+	dm_sessid_t	sid,
+	void		*dirhanp,
+	size_t		dirhlen,
+	dm_token_t	token,
+	void		*hanp,
+	size_t		hlen,
+	char		*cname,
+	char		*path);
+
+extern int
+dm_sync_by_handle(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token);
+
+extern int
+dm_upgrade_right(		/* not completely supported; see caveat above */
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token);
+
+extern dm_ssize_t
+dm_write_invis(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	int		flags,
+	dm_off_t	off,
+	dm_size_t	len,
+	void		*bufp);
+
+/* Non-standard SGI additions to the DMAPI interface. */
+
+int
+dm_open_by_handle(
+	void		*hanp,
+	size_t		hlen,
+	int		mode);
+
+extern int
+dm_get_dioinfo(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_dioinfo_t	*diop);
+
+#ifdef	__cplusplus
+}
+#endif
+
+#endif /* __DMAPI_H__ */
diff -uNrp linux-2.6.5/fs/xfs/dmapi/dmapi_attr.c linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_attr.c
--- linux-2.6.5/fs/xfs/dmapi/dmapi_attr.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_attr.c	2004-04-05 00:58:15.000000000 +0200
@@ -0,0 +1,94 @@
+/*
+ * Copyright (c) 2000 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.	 Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+
+#include "xfs.h"
+#include "dmapi.h"
+#include "dmapi_kern.h"
+#include "dmapi_private.h"
+
+
+/* Retrieve attributes for a single file, directory or symlink. */
+
+int
+dm_get_fileattr(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	u_int		mask,
+	dm_stat_t	*statp)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_VNO,
+		DM_RIGHT_SHARED, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->get_fileattr(tdp->td_vp, tdp->td_right,
+		mask, statp);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+/* Set one or more file attributes of a file, directory, or symlink. */
+
+int
+dm_set_fileattr(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	u_int		mask,
+	dm_fileattr_t	*attrp)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_VNO,
+		DM_RIGHT_EXCL, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->set_fileattr(tdp->td_vp, tdp->td_right,
+		mask, attrp);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
diff -uNrp linux-2.6.5/fs/xfs/dmapi/dmapi_bulkattr.c linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_bulkattr.c
--- linux-2.6.5/fs/xfs/dmapi/dmapi_bulkattr.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_bulkattr.c	2004-04-05 00:58:15.000000000 +0200
@@ -0,0 +1,171 @@
+/*
+ * Copyright (c) 2000 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.	 Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+
+#include "xfs.h"
+#include "dmapi.h"
+#include "dmapi_kern.h"
+#include "dmapi_private.h"
+
+
+int
+dm_init_attrloc(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_attrloc_t	*locp)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_VFS|DM_TDT_DIR,
+		DM_RIGHT_SHARED, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->init_attrloc(tdp->td_vp, tdp->td_right, locp);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+/*
+ * Retrieves both standard and DM specific file attributes for the file
+ * system indicated by the handle. (The FS has to be mounted).
+ * Syscall returns 1 to indicate SUCCESS and more information is available.
+ * -1 is returned on error, and errno will be set appropriately.
+ * 0 is returned upon successful completion.
+ */
+
+int
+dm_get_bulkattr_rvp(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	u_int		mask,
+	dm_attrloc_t	*locp,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp,
+	int		*rvp)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_VFS,
+		DM_RIGHT_SHARED, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->get_bulkattr_rvp(tdp->td_vp, tdp->td_right,
+			mask, locp, buflen, bufp, rlenp, rvp);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+/*
+ * Retrieves attributes of directory entries given a handle to that
+ * directory. Iterative.
+ * Syscall returns 1 to indicate SUCCESS and more information is available.
+ * -1 is returned on error, and errno will be set appropriately.
+ * 0 is returned upon successful completion.
+ */
+
+int
+dm_get_dirattrs_rvp(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	u_int		mask,
+	dm_attrloc_t	*locp,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp,
+	int		*rvp)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_DIR,
+		DM_RIGHT_SHARED, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->get_dirattrs_rvp(tdp->td_vp, tdp->td_right,
+		mask, locp, buflen, bufp, rlenp, rvp);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+int
+dm_get_bulkall_rvp(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	u_int		mask,
+	dm_attrname_t	*attrnamep,
+	dm_attrloc_t	*locp,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp,
+	int		*rvp)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_VFS,
+		DM_RIGHT_SHARED, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->get_bulkall_rvp(tdp->td_vp, tdp->td_right,
+		mask, attrnamep, locp, buflen, bufp, rlenp, rvp);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
diff -uNrp linux-2.6.5/fs/xfs/dmapi/dmapi_config.c linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_config.c
--- linux-2.6.5/fs/xfs/dmapi/dmapi_config.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_config.c	2004-04-05 00:58:15.000000000 +0200
@@ -0,0 +1,117 @@
+/*
+ * Copyright (c) 2000 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.	 Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+
+#include "xfs.h"
+#include "dmapi.h"
+#include "dmapi_kern.h"
+#include "dmapi_private.h"
+
+int
+dm_get_config(
+	void		*hanp,
+	size_t		hlen,
+	dm_config_t	flagname,
+	dm_size_t	*retvalp)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	dm_size_t	retval;
+	int		system = 1;
+	int		error;
+
+	/* Trap and process configuration parameters which are system-wide. */
+
+	switch (flagname) {
+	case DM_CONFIG_LEGACY:
+	case DM_CONFIG_PENDING:
+	case DM_CONFIG_OBJ_REF:
+		retval = DM_TRUE;
+		break;
+	case DM_CONFIG_MAX_MESSAGE_DATA:
+		retval = DM_MAX_MSG_DATA;
+		break;
+	default:
+		system = 0;
+		break;
+	}
+	if (system) {
+		if (copy_to_user(retvalp, &retval, sizeof(retval)))
+			return(EFAULT);
+		return(0);
+	}
+
+	/* Must be filesystem-specific.	 Convert the handle into a vnode. */
+
+	if ((error = dm_get_config_tdp(hanp, hlen, &tdp)) != 0)
+		return(error);
+
+	/* Now call the filesystem-specific routine to determine the
+	   value of the configuration option for that filesystem.
+	*/
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->get_config(tdp->td_vp, tdp->td_right,
+		flagname, retvalp);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+int
+dm_get_config_events(
+	void		*hanp,
+	size_t		hlen,
+	u_int		nelem,
+	dm_eventset_t	*eventsetp,
+	u_int		*nelemp)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	/* Convert the handle into a vnode. */
+
+	if ((error = dm_get_config_tdp(hanp, hlen, &tdp)) != 0)
+		return(error);
+
+	/* Now call the filesystem-specific routine to determine the
+	   events supported by that filesystem.
+	*/
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->get_config_events(tdp->td_vp, tdp->td_right,
+		nelem, eventsetp, nelemp);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
diff -uNrp linux-2.6.5/fs/xfs/dmapi/dmapi_dmattr.c linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_dmattr.c
--- linux-2.6.5/fs/xfs/dmapi/dmapi_dmattr.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_dmattr.c	2004-04-05 00:58:15.000000000 +0200
@@ -0,0 +1,229 @@
+/*
+ * Copyright (c) 2000 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.	 Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+#include "xfs.h"
+#include "dmapi.h"
+#include "dmapi_kern.h"
+#include "dmapi_private.h"
+
+
+int
+dm_clear_inherit(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_attrname_t	*attrnamep)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_VFS,
+		DM_RIGHT_EXCL, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->clear_inherit(tdp->td_vp, tdp->td_right,
+		attrnamep);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+int
+dm_get_dmattr(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_attrname_t	*attrnamep,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_VNO,
+		DM_RIGHT_SHARED, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->get_dmattr(tdp->td_vp, tdp->td_right,
+		attrnamep, buflen, bufp, rlenp);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+int
+dm_getall_dmattr(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_VNO,
+		DM_RIGHT_SHARED, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->getall_dmattr(tdp->td_vp, tdp->td_right,
+		buflen, bufp, rlenp);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+int
+dm_getall_inherit(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	u_int		nelem,
+	dm_inherit_t	*inheritbufp,
+	u_int		*nelemp)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_VFS,
+		DM_RIGHT_SHARED, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->getall_inherit(tdp->td_vp, tdp->td_right,
+		nelem, inheritbufp, nelemp);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+int
+dm_remove_dmattr(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	int		setdtime,
+	dm_attrname_t	*attrnamep)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_VNO,
+		DM_RIGHT_EXCL, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->remove_dmattr(tdp->td_vp, tdp->td_right,
+		setdtime, attrnamep);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+int
+dm_set_dmattr(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_attrname_t	*attrnamep,
+	int		setdtime,
+	size_t		buflen,
+	void		*bufp)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_VNO,
+		DM_RIGHT_EXCL, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->set_dmattr(tdp->td_vp, tdp->td_right,
+		attrnamep, setdtime, buflen, bufp);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+int
+dm_set_inherit(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_attrname_t	*attrnamep,
+	mode_t		mode)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_VFS,
+		DM_RIGHT_EXCL, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->set_inherit(tdp->td_vp, tdp->td_right,
+		attrnamep, mode);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
diff -uNrp linux-2.6.5/fs/xfs/dmapi/dmapi_event.c linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_event.c
--- linux-2.6.5/fs/xfs/dmapi/dmapi_event.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_event.c	2004-04-05 00:58:15.000000000 +0200
@@ -0,0 +1,849 @@
+/*
+ * Copyright (c) 2000 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.	 Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+#include "xfs.h"
+#include "dmapi.h"
+#include "dmapi_kern.h"
+#include "dmapi_private.h"
+
+/* The "rights" portion of the DMAPI spec is not currently implemented.	 A
+   framework for rights is provided in the code, but turns out to be a noop
+   in practice.	 The following comments are a brain dump to serve as input to
+   the poor soul that eventually has to get DMAPI rights working in IRIX.
+
+   A DMAPI right is similar but not identical to the mrlock_t mechanism
+   already used within the kernel.  The similarities are that it is a
+   sleeping lock, and that a multiple-reader, single-writer protocol is used.
+   How locks are obtained and dropped are different however.  With a mrlock_t,
+   a thread grabs the lock, does some stuff, then drops the lock, and all other
+   threads block in the meantime (assuming a write lock).  There is a one-to-
+   one relationship between the lock and the thread which obtained the lock.
+   Not so with DMAPI right locks.  A DMAPI lock is associated with a particular
+   session/token/hanp/hlen quad; since there is a dm_tokdata_t structure for
+   each such quad, you can think of it as a one-to-one relationship between the
+   lock and a dm_tokdata_t.  Any application thread which presents the correct
+   quad is entitled to grab or release the lock, or to use the rights
+   associated with that lock.  The thread that grabs the lock does not have to
+   be the one to use the lock, nor does it have to be the thread which drops
+   the lock.  The lock can be held for very long periods of time, even across
+   multiple systems calls by multiple application threads.  The idea is that a
+   coordinated group of DMAPI application threads can grab the lock, issue a
+   series of inode accesses and/or updates, then drop the lock, and be assured
+   that no other thread in the system could be modifying the inode at the same
+   time.  The kernel is expected to blindly trust that the application will
+   not forget to unlock inodes it has locked, and will not deadlock itself
+   against the kernel.
+
+   There are two types of DMAPI rights, file object (inode) and filesystem
+   object (superblock?).  An inode right is the equivalent of the combination
+   of both the XFS ilock and iolock; if held exclusively, no data or metadata
+   within the file can be changed by non-lock-holding threads.	The filesystem
+   object lock is a little fuzzier; I think that if it is held, things like
+   unmounts can be blocked, plus there is an event mask associated with the
+   filesystem which can't be updated without the lock.	(By the way, that
+   event mask is supposed to be persistent in the superblock; add that to
+   your worklist :-)
+
+   All events generated by XFS currently arrive with no rights, i.e.
+   DM_RIGHT_NULL, and return to the filesystem with no rights.	It would be
+   smart to leave it this way if possible, because it otherwise becomes more
+   likely that an application thread will deadlock against the kernel if the
+   one responsible for calling dm_get_events() happens to touch a file which
+   was locked at the time the event was queued.	 Since the thread is blocked,
+   it can't read the event in order to find and drop the lock.	Catch-22.  If
+   you do have events that arrive with non-null rights, then dm_enqueue() needs
+   to have code added for synchronous events which atomically switches the
+   right from being a thread-based right to a dm_tokdata_t-based right without
+   allowing the lock to drop in between.  You will probably have to add a new
+   dm_fsys_vector entry point to do this.  The lock can't be lost during the
+   switch, or other threads might change the inode or superblock in between.
+   Likewise, if you need to return to the filesystem holding a right, then
+   you need a DMAPI-to-thread atomic switch to occur, most likely in
+   dm_change_right().  Again, the lock must not be lost during the switch; the
+   DMAPI spec spends a couple of pages stressing this.	Another dm_fsys_vector
+   entry point is probably the answer.
+
+   There are several assumptions implied in the current layout of the code.
+   First of all, if an event returns to the filesystem with a return value of
+   zero, then the filesystem can assume that any locks (rights) held at the
+   start of the event are still in effect at the end of the event.  (Note that
+   the application could have temporarily dropped and reaquired the right
+   while the event was outstanding, however).  If the event returns to the
+   filesystem with an errno, then the filesystem must assume that it has lost
+   any and all rights associated with any of the objects in the event.	This
+   was done for a couple of reasons.  First of all, since an errno is being
+   returned, most likely the filesystem is going to immediately drop all the
+   locks anyway.  If the DMAPI code was required to unconditionally reobtain
+   all locks before returning to the filesystem, then dm_pending() wouldn't
+   work for NFS server threads because the process would block indefinitely
+   trying to get its thread-based rights back, because the DMAPI-rights
+   associated with the dm_tokdata_t in the outstanding event would prevent
+   the rights from being obtained.  That would be a bad thing.	We wouldn't
+   be able to let users Cntl-C out of read/write/truncate events either.
+
+   If a case should ever surface where the thread has lost its rights even
+   though it has a zero return status, or where the thread has rights even
+   though it is returning with an errno, then this logic will have to be
+   reworked.  This could be done by changing the 'right' parameters on all
+   the event calls to (dm_right_t *), so that they could serve both as IN
+   and OUT parameters.
+
+   Some events such as DM_EVENT_DESTROY arrive without holding a vnode
+   reference; if you don't have a vnode reference, you can't have a right
+   on the file.
+
+   One more quirk.  The DM_EVENT_UNMOUNT event is defined to be synchronous
+   when it's behavior is asynchronous.	If an unmount event arrives with
+   rights, the event should return with the same rights and should NOT leave
+   any rights in the dm_tokdata_t where the application could use them.
+*/
+
+
+#define GETNEXTOFF(vdat)	((vdat).vd_offset + (vdat).vd_length)
+#define HANDLE_SIZE(tdp)	\
+	((tdp)->td_type & DM_TDT_VFS ? FSHSIZE : XFS_HSIZE((tdp)->td_handle))
+
+
+/* Given a vnode pointer in a filesystem known to support DMAPI,
+   build a tdp structure for the corresponding vnode.
+*/
+
+static dm_tokdata_t *
+dm_vp_data(
+	vnode_t		*vp,
+	dm_right_t	right,
+	int		referenced)	/* != 0, caller holds vnode reference */
+{
+	int		error;
+	dm_tokdata_t	*tdp;
+
+	tdp = kmem_cache_alloc(dm_tokdata_cachep, SLAB_KERNEL);
+	if (tdp == NULL) {
+		printk("%s/%d: kmem_cache_alloc(dm_tokdata_cachep) returned NULL\n", __FUNCTION__, __LINE__);
+		return NULL;
+	}
+
+	tdp->td_next = NULL;
+	tdp->td_tevp = NULL;
+	tdp->td_app_ref = 0;
+	tdp->td_orig_right = right;
+	tdp->td_right = right;
+	tdp->td_flags = DM_TDF_ORIG;
+	if (referenced) {
+		tdp->td_flags |= DM_TDF_EVTREF;
+	}
+
+	if (vp->v_type == VREG) {
+		tdp->td_type = DM_TDT_REG;
+	} else if (vp->v_type == VDIR) {
+		tdp->td_type = DM_TDT_DIR;
+	} else if (vp->v_type == VLNK) {
+		tdp->td_type = DM_TDT_LNK;
+	} else {
+		tdp->td_type = DM_TDT_OTH;
+	}
+
+	if (referenced) {
+		tdp->td_vp = vp;
+	} else {
+		tdp->td_vp = NULL;
+	}
+	tdp->td_vcount = 0;
+
+	if ((error = dm_vp_to_handle(vp, &tdp->td_handle)) != 0) {
+		panic("dm_vp_data: dm_vp_to_handle failed for vp %p in "
+			"a DMAPI filesystem, errno %d\n", vp, error);
+	}
+
+	return(tdp);
+}
+
+
+/* Given a vfs pointer to a filesystem known to support DMAPI, build a tdp
+   structure for that vfsp.
+*/
+static dm_tokdata_t *
+dm_vfs_data(
+	vfs_t		*vfsp,
+	vnode_t		*vp,		/* will be NULL for DM_EVENT_UNMOUNT */
+	dm_right_t	right)
+{
+	dm_tokdata_t	*tdp;
+
+	tdp = kmem_cache_alloc(dm_tokdata_cachep, SLAB_KERNEL);
+	if (tdp == NULL) {
+		printk("%s/%d: kmem_cache_alloc(dm_tokdata_cachep) returned NULL\n", __FUNCTION__, __LINE__);
+		return NULL;
+	}
+
+	tdp->td_next = NULL;
+	tdp->td_tevp = NULL;
+	tdp->td_app_ref = 0;
+	tdp->td_orig_right = right;
+	tdp->td_right = right;
+	tdp->td_flags = DM_TDF_ORIG;
+	if (vp) {
+		tdp->td_flags |= DM_TDF_EVTREF;
+	}
+	tdp->td_type = DM_TDT_VFS;
+	if (vp) {
+		tdp->td_vp = vp;
+	} else {
+		tdp->td_vp = NULL;
+	}
+	tdp->td_vcount = 0;
+
+	memcpy(&tdp->td_handle.ha_fsid, vfsp->vfs_altfsid, sizeof(xfs_fsid_t));
+	memset((char *)&tdp->td_handle.ha_fsid + sizeof(xfs_fsid_t), 0,
+		sizeof(tdp->td_handle) - sizeof(xfs_fsid_t));
+
+	return(tdp);
+}
+
+
+/* Link a tdp structure into the tevp. */
+
+static void
+dm_add_handle_to_event(
+	dm_tokevent_t	*tevp,
+	dm_tokdata_t	*tdp)
+{
+	tdp->td_next = tevp->te_tdp;
+	tevp->te_tdp = tdp;
+	tdp->td_tevp = tevp;
+}
+
+
+/* Generate the given data event for the vnode, and wait for a reply.  The
+   caller must guarantee that the vnode's reference count is greater than zero
+   so that the filesystem can't disappear while the request is outstanding.
+*/
+
+int
+dm_send_data_event(
+	dm_eventtype_t	event,
+	vnode_t		*vp,
+	dm_right_t	vp_right,	/* current right for vp */
+	xfs_off_t	offset,
+	size_t		length,
+	int		flags)		/* 0 or DM_FLAGS_NDELAY */
+{
+	dm_data_event_t *datap;
+	dm_tokevent_t	*tevp;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	tdp = dm_vp_data(vp, vp_right, /* reference held */ 1);
+	if (tdp == NULL)
+		return ENOMEM;
+
+	/* Calculate the size of the event in bytes, create an event structure
+	   for it, and insert the file's handle into the event.
+	*/
+
+	tevp = dm_evt_create_tevp(event, HANDLE_SIZE(tdp), (void **)&datap);
+	if (tevp == NULL) {
+		kmem_cache_free(dm_tokdata_cachep, tdp);
+		return(ENOMEM);
+	}
+	dm_add_handle_to_event(tevp, tdp);
+
+	/* Now fill in all the dm_data_event_t fields. */
+
+	datap->de_handle.vd_offset = sizeof(*datap);
+	datap->de_handle.vd_length = HANDLE_SIZE(tdp);
+	memcpy((char *)datap + datap->de_handle.vd_offset, &tdp->td_handle,
+		datap->de_handle.vd_length);
+	datap->de_offset = offset;
+	datap->de_length = length;
+
+	/* Queue the message and wait for the reply. */
+
+	error = dm_enqueue_normal_event(vp->v_vfsp, tevp, flags);
+
+	/* If no errors occurred, we must leave with the same rights we had
+	   upon entry.	If errors occurred, we must leave with no rights.
+	*/
+
+	dm_evt_rele_tevp(tevp, error);
+
+	return(error);
+}
+
+
+/* Generate the destroy event for the vnode and wait until the request has been
+   queued.  The caller does not hold a vnode reference or a right on the vnode,
+   but it must otherwise lock down the vnode such that the filesystem can't
+   disappear while the request is waiting to be queued.	 While waiting to be
+   queued, the vnode must not be referenceable either by path or by a call
+   to dm_handle_to_vp().
+*/
+
+int
+dm_send_destroy_event(
+	vnode_t		*vp,
+	dm_right_t	vp_right)	/* always DM_RIGHT_NULL */
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokevent_t	*tevp;
+	dm_tokdata_t	*tdp;
+	dm_destroy_event_t *destp;
+	dm_attrname_t	attrname;
+	char		*value;
+	int		value_len;
+	int		error;
+
+	tdp = dm_vp_data(vp, vp_right, /* no reference held */ 0);
+	if (tdp == NULL)
+		return ENOMEM;
+
+	if ((error = dm_waitfor_destroy_attrname(vp->v_vfsp, &attrname)) != 0)
+		return(error);
+
+	/* If a return-on-destroy attribute name exists for this filesystem,
+	   see if the object being deleted has this attribute.	If the object
+	   doesn't have the attribute or if we encounter an error, then send
+	   the event without the attribute.
+	 */
+
+	value_len = -1;		/* because zero is a valid attribute length */
+	if (attrname.an_chars[0] != '\0') {
+		fsys_vector = dm_fsys_vector(vp);
+		error = fsys_vector->get_destroy_dmattr(vp, vp_right, &attrname,
+			&value, &value_len);
+		if (error)
+			return error;
+	}
+
+	/* Now that we know the size of the attribute value, if any, calculate
+	   the size of the event in bytes, create an event structure for it,
+	   and insert the handle into the event.
+	*/
+
+	tevp = dm_evt_create_tevp(DM_EVENT_DESTROY,
+		HANDLE_SIZE(tdp) + (value_len >= 0 ? value_len : 0),
+		(void **)&destp);
+	if (tevp == NULL) {
+		kmem_cache_free(dm_tokdata_cachep, tdp);
+		if (value_len > 0)
+			kfree(value);
+		return(ENOMEM);
+	}
+	dm_add_handle_to_event(tevp, tdp);
+
+	/* Now fill in all the dm_destroy_event_t fields. */
+
+	destp->ds_handle.vd_offset = sizeof(*destp);
+	destp->ds_handle.vd_length = HANDLE_SIZE(tdp);
+	memcpy((char *)destp + destp->ds_handle.vd_offset, &tdp->td_handle,
+		destp->ds_handle.vd_length);
+	if (value_len >= 0) {
+		destp->ds_attrname = attrname;
+		destp->ds_attrcopy.vd_length = value_len;
+		if (value_len == 0) {
+			destp->ds_attrcopy.vd_offset = 0;
+		} else {
+			destp->ds_attrcopy.vd_offset = GETNEXTOFF(destp->ds_handle);
+			memcpy((char *)destp + destp->ds_attrcopy.vd_offset, value,
+				value_len);
+			kfree(value);
+		}
+	}
+
+	/* Queue the message asynchronously. */
+
+	error = dm_enqueue_normal_event(vp->v_vfsp, tevp, 0);
+
+	/* Since we had no rights upon entry, we have none to reobtain before
+	   leaving.
+	*/
+
+	dm_evt_rele_tevp(tevp, 1);
+
+	return(error);
+}
+
+
+/* The dm_mount_event_t event is sent in turn to all sessions that have asked
+   for it until one either rejects it or accepts it.  The filesystem is not
+   going anywhere because the mount is blocked until the event is answered.
+*/
+
+int
+dm_send_mount_event(
+	vfs_t		*vfsp,		/* filesystem being mounted */
+	dm_right_t	vfsp_right,
+	vnode_t		*vp,		/* mounted on directory */
+	dm_right_t	vp_right,
+	vnode_t		*rootvp,
+	dm_right_t	rootvp_right,
+	char		*name1,		/* mount path */
+	char		*name2)		/* filesystem device name */
+{
+	int		error;
+	dm_tokevent_t	*tevp = NULL;
+	dm_tokdata_t	*tdp1 = NULL;	/* filesystem handle for event */
+	dm_tokdata_t	*tdp2 = NULL;	/* file handle for mounted-on dir. */
+	dm_tokdata_t	*tdp3 = NULL;	/* file handle for root vnode */
+	dm_mount_event_t *mp;
+	size_t		nextoff;
+
+	/* Convert the vfsp to a filesystem handle, and vp and rootvp into
+	   file handles.  vp (the mounted-on directory) may not have a handle
+	   if it is a different filesystem type such as EFS which does not
+	   support DMAPI.
+	*/
+
+	tdp1 = dm_vfs_data(vfsp, rootvp, vfsp_right);
+	if (tdp1 == NULL)
+		goto out_nomem;
+
+	if ((vp == NULL) || dm_check_dmapi_vp(vp)) {
+		vp = NULL;	/* assume we are mounting on non XFS */
+	} else {
+		tdp2 = dm_vp_data(vp, vp_right, /* reference held */ 1);
+		if (tdp2 == NULL)
+			goto out_nomem;
+	}
+
+	tdp3 = dm_vp_data(rootvp, rootvp_right, /* reference held */ 1);
+	if (tdp3 == NULL)
+		goto out_nomem;
+
+	/* Calculate the size of the event in bytes, create an event structure
+	   for it, and insert the handles into the event.
+	*/
+
+	tevp = dm_evt_create_tevp(DM_EVENT_MOUNT,
+			HANDLE_SIZE(tdp1) + (vp ? HANDLE_SIZE(tdp2) : 0) +
+			HANDLE_SIZE(tdp3) + strlen(name1) + 1 +
+			strlen(name2) + 1, (void **)&mp);
+	if (tevp == NULL)
+		goto out_nomem;
+
+	dm_add_handle_to_event(tevp, tdp1);
+	if (vp)
+		dm_add_handle_to_event(tevp, tdp2);
+	dm_add_handle_to_event(tevp, tdp3);
+
+	/* Now fill in all the dm_mount_event_t fields. */
+
+	mp->me_handle1.vd_offset = sizeof(*mp);
+	mp->me_handle1.vd_length = HANDLE_SIZE(tdp1);
+	memcpy((char *) mp + mp->me_handle1.vd_offset, &tdp1->td_handle,
+			mp->me_handle1.vd_length);
+	nextoff = GETNEXTOFF(mp->me_handle1);
+
+	if (vp) {
+		mp->me_handle2.vd_offset = nextoff;
+		mp->me_handle2.vd_length = HANDLE_SIZE(tdp2);
+		memcpy((char *)mp + mp->me_handle2.vd_offset, &tdp2->td_handle,
+			mp->me_handle2.vd_length);
+		nextoff = GETNEXTOFF(mp->me_handle2);
+	}
+
+	mp->me_name1.vd_offset = nextoff;
+	mp->me_name1.vd_length = strlen(name1) + 1;
+	memcpy((char *)mp + mp->me_name1.vd_offset, name1, mp->me_name1.vd_length);
+	nextoff = GETNEXTOFF(mp->me_name1);
+
+	mp->me_name2.vd_offset = nextoff;
+	mp->me_name2.vd_length = strlen(name2) + 1;
+	memcpy((char *)mp + mp->me_name2.vd_offset, name2, mp->me_name2.vd_length);
+	nextoff = GETNEXTOFF(mp->me_name2);
+
+	mp->me_roothandle.vd_offset = nextoff;
+	mp->me_roothandle.vd_length = HANDLE_SIZE(tdp3);
+	memcpy((char *)mp + mp->me_roothandle.vd_offset, &tdp3->td_handle,
+			mp->me_roothandle.vd_length);
+
+	mp->me_mode = (vfsp->vfs_flag & VFS_RDONLY ? DM_MOUNT_RDONLY : 0);
+
+	/* Queue the message and wait for the reply. */
+
+	error = dm_enqueue_mount_event(vfsp, tevp);
+
+	/* If no errors occurred, we must leave with the same rights we had
+	   upon entry.	If errors occurred, we must leave with no rights.
+	*/
+
+	dm_evt_rele_tevp(tevp, error);
+
+	return(error);
+
+out_nomem:
+	if (tevp)
+		kfree(tevp);
+	if (tdp1)
+		kmem_cache_free(dm_tokdata_cachep, tdp1);
+	if (tdp2)
+		kmem_cache_free(dm_tokdata_cachep, tdp2);
+	if (tdp3)
+		kmem_cache_free(dm_tokdata_cachep, tdp3);
+	return ENOMEM;
+}
+
+
+/* Generate an DM_EVENT_UNMOUNT event and wait for a reply.  The 'retcode'
+   field indicates whether this is a successful or unsuccessful unmount.
+   If successful, the filesystem is already unmounted, and any pending handle
+   reference to the filesystem will be failed.	If the unmount was
+   unsuccessful, then the filesystem will be placed back into full service.
+
+   The DM_EVENT_UNMOUNT event should really be asynchronous, because the
+   application has no control over whether or not the unmount succeeds.	 (The
+   DMAPI spec defined it that way because asynchronous events aren't always
+   guaranteed to be delivered.)
+
+   Since the filesystem is already unmounted in the successful case, the
+   DM_EVENT_UNMOUNT event can't make available any vnode to be used in
+   subsequent sid/hanp/hlen/token calls by the application.  The event will
+   hang around until the application does a DM_RESP_CONTINUE, but the handle
+   within the event is unusable by the application.
+*/
+
+void
+dm_send_unmount_event(
+	vfs_t		*vfsp,
+	vnode_t		*vp,		/* NULL if unmount successful */
+	dm_right_t	vfsp_right,
+	mode_t		mode,
+	int		retcode,	/* errno, if unmount failed */
+	int		flags)
+{
+	dm_namesp_event_t	*np;
+	dm_tokevent_t	*tevp;
+	dm_tokdata_t	*tdp1;
+
+	/* If the unmount failed, put the filesystem back into full service,
+	   allowing blocked handle references to finish.  If it succeeded, put
+	   the filesystem into the DM_STATE_UNMOUNTED state and fail all
+	   blocked DM_NO_TOKEN handle accesses.
+	*/
+
+	if (retcode != 0) {	/* unmount was unsuccessful */
+		dm_change_fsys_entry(vfsp, DM_STATE_MOUNTED);
+	} else {
+		dm_change_fsys_entry(vfsp, DM_STATE_UNMOUNTED);
+	}
+
+	/* If the event wasn't in the filesystem dm_eventset_t, just remove
+	   the filesystem from the list of DMAPI filesystems and return.
+	*/
+
+	if (flags & DM_FLAGS_UNWANTED) {
+		if (retcode == 0)
+			dm_remove_fsys_entry(vfsp);
+		return;
+	}
+
+	/* Calculate the size of the event in bytes and allocate zeroed memory
+	   for it.
+	*/
+
+	tdp1 = dm_vfs_data(vfsp, vp, vfsp_right);
+	if (tdp1 == NULL)
+		return;
+
+	tevp = dm_evt_create_tevp(DM_EVENT_UNMOUNT, HANDLE_SIZE(tdp1),
+		(void **)&np);
+	if (tevp == NULL) {
+		kmem_cache_free(dm_tokdata_cachep, tdp1);
+		return;
+	}
+
+	dm_add_handle_to_event(tevp, tdp1);
+
+	/* Now copy in all the dm_namesp_event_t specific fields. */
+
+	np->ne_handle1.vd_offset = sizeof(*np);
+	np->ne_handle1.vd_length = HANDLE_SIZE(tdp1);
+	memcpy((char *) np + np->ne_handle1.vd_offset, &tdp1->td_handle,
+			np->ne_handle1.vd_length);
+	np->ne_mode = mode;
+	np->ne_retcode = retcode;
+
+	/* Since DM_EVENT_UNMOUNT is effectively asynchronous, queue the
+	   message and ignore any error return for DM_EVENT_UNMOUNT.
+	*/
+
+	(void)dm_enqueue_normal_event(vfsp, tevp, flags);
+
+	if (retcode == 0)
+		dm_remove_fsys_entry(vfsp);
+
+	dm_evt_rele_tevp(tevp, 0);
+}
+
+
+/* Generate the given namespace event and wait for a reply (if synchronous) or
+   until the event has been queued (asynchronous).  The caller must guarantee
+   that at least one vnode within the filesystem has had its reference count
+   bumped so that the filesystem can't disappear while the event is
+   outstanding.
+*/
+
+int
+dm_send_namesp_event(
+	dm_eventtype_t	event,
+	vnode_t		*vp1,
+	dm_right_t	vp1_right,
+	vnode_t		*vp2,
+	dm_right_t	vp2_right,
+	char		*name1,
+	char		*name2,
+	mode_t		mode,
+	int		retcode,
+	int		flags)
+{
+	dm_namesp_event_t	*np;
+	dm_tokevent_t	*tevp;
+	dm_tokdata_t	*tdp1 = NULL;	/* primary handle for event */
+	dm_tokdata_t	*tdp2 = NULL;	/* additional handle for event */
+	vfs_t		*sidvfsp;	/* vfs event must be registered on */
+	size_t		nextoff;
+	int		error;
+
+	sidvfsp = vp1->v_vfsp;
+
+	switch (event) {
+	case DM_EVENT_PREUNMOUNT:
+		/*
+		 *  PREUNMOUNT - Send the file system handle in handle1,
+		 *  and the handle for the root dir in the second.  Otherwise
+		 *  it's a normal sync message; i.e. succeeds or fails
+		 *  depending on the app's return code.
+		 *	vp1 and vp2 are both the root dir of mounted FS
+		 *	vp1_right is the filesystem right.
+		 *	vp2_right is the root inode right.
+		 */
+
+		if (flags & DM_FLAGS_UNWANTED) {
+			dm_change_fsys_entry(sidvfsp, DM_STATE_UNMOUNTING);
+			return(0);
+		}
+		tdp1 = dm_vfs_data(sidvfsp, vp1, vp1_right);
+		if (tdp1 == NULL)
+			return ENOMEM;
+		tdp2 = dm_vp_data(vp2, vp2_right, /* reference held */ 1);
+		if (tdp2 == NULL) {
+			kmem_cache_free(dm_tokdata_cachep, tdp1);
+			return ENOMEM;
+		}
+		break;
+
+	case DM_EVENT_NOSPACE:
+		/* vp1_right is the filesystem right. */
+
+		tdp1 = dm_vfs_data(sidvfsp, vp1, vp1_right);
+		if (tdp1 == NULL)
+			return ENOMEM;
+		tdp2 = dm_vp_data(vp2, vp2_right, /* reference held */ 1); /* additional info - not in the spec */
+		if (tdp2 == NULL) {
+			kmem_cache_free(dm_tokdata_cachep, tdp1);
+			return ENOMEM;
+		}
+		break;
+
+	default:
+		/* All other events only pass in vnodes and don't require any
+		   special cases.
+		*/
+
+		tdp1 = dm_vp_data(vp1, vp1_right, /* reference held */ 1);
+		if (tdp1 == NULL)
+			return ENOMEM;
+		if (vp2) {
+			tdp2 = dm_vp_data(vp2, vp2_right, /* reference held */ 1);
+			if (tdp2 == NULL) {
+				kmem_cache_free(dm_tokdata_cachep, tdp1);
+				return ENOMEM;
+			}
+		}
+	}
+
+	/* Calculate the size of the event in bytes and allocate zeroed memory
+	   for it.
+	*/
+
+	tevp = dm_evt_create_tevp(event,
+		HANDLE_SIZE(tdp1) + (vp2 ? HANDLE_SIZE(tdp2) : 0) +
+		(name1 ? strlen(name1) + 1 : 0) +
+		(name2 ? strlen(name2) + 1 : 0), (void **)&np);
+	if (tevp == NULL) {
+		if (tdp1)
+			kmem_cache_free(dm_tokdata_cachep, tdp1);
+		if (tdp2)
+			kmem_cache_free(dm_tokdata_cachep, tdp2);
+		return(ENOMEM);
+	}
+
+	dm_add_handle_to_event(tevp, tdp1);
+	if (vp2)
+		dm_add_handle_to_event(tevp, tdp2);
+
+	/* Now copy in all the dm_namesp_event_t specific fields. */
+
+	np->ne_handle1.vd_offset = sizeof(*np);
+	np->ne_handle1.vd_length = HANDLE_SIZE(tdp1);
+	memcpy((char *) np + np->ne_handle1.vd_offset, &tdp1->td_handle,
+			np->ne_handle1.vd_length);
+	nextoff = GETNEXTOFF(np->ne_handle1);
+	if (vp2) {
+		np->ne_handle2.vd_offset = nextoff;
+		np->ne_handle2.vd_length = HANDLE_SIZE(tdp2);
+		memcpy((char *)np + np->ne_handle2.vd_offset, &tdp2->td_handle,
+				np->ne_handle2.vd_length);
+		nextoff = GETNEXTOFF(np->ne_handle2);
+	}
+	if (name1) {
+		np->ne_name1.vd_offset = nextoff;
+		np->ne_name1.vd_length = strlen(name1) + 1;
+		memcpy((char *)np + np->ne_name1.vd_offset, name1,
+				np->ne_name1.vd_length);
+		nextoff = GETNEXTOFF(np->ne_name1);
+	}
+	if (name2) {
+		np->ne_name2.vd_offset = nextoff;
+		np->ne_name2.vd_length = strlen(name2) + 1;
+		memcpy((char *)np + np->ne_name2.vd_offset, name2,
+				np->ne_name2.vd_length);
+	}
+	np->ne_mode = mode;
+	np->ne_retcode = retcode;
+
+	/* Queue the message and wait for the reply. */
+
+	error = dm_enqueue_normal_event(sidvfsp, tevp, flags);
+
+	/* If no errors occurred, we must leave with the same rights we had
+	   upon entry.	If errors occurred, we must leave with no rights.
+	*/
+
+	dm_evt_rele_tevp(tevp, error);
+
+	if (!error && event == DM_EVENT_PREUNMOUNT) {
+		dm_change_fsys_entry(sidvfsp, DM_STATE_UNMOUNTING);
+	}
+
+	return(error);
+}
+
+
+/*
+ *  Send a message of type "DM_EVENT_USER".  Since no vnode is involved, we
+ *  don't have to worry about rights here.
+ */
+
+int
+dm_send_msg(
+	dm_sessid_t	targetsid,
+	dm_msgtype_t	msgtype,		/* SYNC or ASYNC */
+	size_t		buflen,
+	void		*bufp)
+{
+	dm_tokevent_t	*tevp;
+	int		sync;
+	void		*msgp;
+	int		error;
+
+	if (buflen > DM_MAX_MSG_DATA)
+		return(E2BIG);
+	if (msgtype == DM_MSGTYPE_ASYNC) {
+		sync = 0;
+	} else if (msgtype == DM_MSGTYPE_SYNC) {
+		sync = 1;
+	} else {
+		return(EINVAL);
+	}
+
+	tevp = dm_evt_create_tevp(DM_EVENT_USER, buflen, (void **)&msgp);
+	if (tevp == NULL)
+		return ENOMEM;
+
+	if (buflen && copy_from_user(msgp, bufp, buflen)) {
+		dm_evt_rele_tevp(tevp, 0);
+		return(EFAULT);
+	}
+
+	/* Enqueue the request and wait for the reply. */
+
+	error = dm_enqueue_sendmsg_event(targetsid, tevp, sync);
+
+	/* Destroy the tevp and return the reply.  (dm_pending is not
+	   supported here.)
+	*/
+
+	dm_evt_rele_tevp(tevp, error);
+
+	return(error);
+}
+
+
+/*
+ *  Send a message of type "DM_EVENT_USER".  Since no vnode is involved, we
+ *  don't have to worry about rights here.
+ */
+
+int
+dm_create_userevent(
+	dm_sessid_t	sid,
+	size_t		msglen,
+	void		*msgdatap,
+	dm_token_t	*tokenp)		/* return token created */
+{
+	dm_tokevent_t	*tevp;
+	dm_token_t	token;
+	int		error;
+	void		*msgp;
+
+	if (msglen > DM_MAX_MSG_DATA)
+		return(E2BIG);
+
+	tevp = dm_evt_create_tevp(DM_EVENT_USER, msglen, (void **)&msgp);
+	if (tevp == NULL)
+		return(ENOMEM);
+
+	if (msglen && copy_from_user(msgp, msgdatap, msglen)) {
+		dm_evt_rele_tevp(tevp, 0);
+		return(EFAULT);
+	}
+
+	/* Queue the message.  If that didn't work, free the tevp structure. */
+
+	if ((error = dm_enqueue_user_event(sid, tevp, &token)) != 0)
+		dm_evt_rele_tevp(tevp, 0);
+
+	if (!error && copy_to_user(tokenp, &token, sizeof(token)))
+		error = EFAULT;
+
+	return(error);
+}
diff -uNrp linux-2.6.5/fs/xfs/dmapi/dmapi_handle.c linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_handle.c
--- linux-2.6.5/fs/xfs/dmapi/dmapi_handle.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_handle.c	2004-04-05 00:58:15.000000000 +0200
@@ -0,0 +1,120 @@
+/*
+ * Copyright (c) 2000 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.	 Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+#include "xfs.h"
+#include "dmapi.h"
+#include "dmapi_kern.h"
+#include "dmapi_private.h"
+
+
+int
+dm_create_by_handle(
+	dm_sessid_t	sid,
+	void		*dirhanp,
+	size_t		dirhlen,
+	dm_token_t	token,
+	void		*hanp,
+	size_t		hlen,
+	char		*cname)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, dirhanp, dirhlen, token, DM_TDT_DIR,
+		DM_RIGHT_EXCL, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->create_by_handle(tdp->td_vp, tdp->td_right,
+		hanp, hlen, cname);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+int
+dm_mkdir_by_handle(
+	dm_sessid_t	sid,
+	void		*dirhanp,
+	size_t		dirhlen,
+	dm_token_t	token,
+	void		*hanp,
+	size_t		hlen,
+	char		*cname)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, dirhanp, dirhlen, token, DM_TDT_DIR,
+		DM_RIGHT_EXCL, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->mkdir_by_handle(tdp->td_vp, tdp->td_right,
+		hanp, hlen, cname);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+int
+dm_symlink_by_handle(
+	dm_sessid_t	sid,
+	void		*dirhanp,
+	size_t		dirhlen,
+	dm_token_t	token,
+	void		*hanp,
+	size_t		hlen,
+	char		*cname,
+	char		*path)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, dirhanp, dirhlen, token, DM_TDT_DIR,
+		DM_RIGHT_EXCL, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->symlink_by_handle(tdp->td_vp, tdp->td_right,
+		hanp, hlen, cname, path);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
diff -uNrp linux-2.6.5/fs/xfs/dmapi/dmapi_hole.c linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_hole.c
--- linux-2.6.5/fs/xfs/dmapi/dmapi_hole.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_hole.c	2004-04-05 00:58:15.000000000 +0200
@@ -0,0 +1,120 @@
+/*
+ * Copyright (c) 2000 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.	 Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+#include "xfs.h"
+#include "dmapi.h"
+#include "dmapi_kern.h"
+#include "dmapi_private.h"
+
+
+int
+dm_get_allocinfo_rvp(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_off_t	*offp,
+	u_int		nelem,
+	dm_extent_t	*extentp,
+	u_int		*nelemp,
+	int		*rvp)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_REG,
+		DM_RIGHT_SHARED, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->get_allocinfo_rvp(tdp->td_vp, tdp->td_right,
+		offp, nelem, extentp, nelemp, rvp);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+int
+dm_probe_hole(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_off_t	off,
+	dm_size_t	len,
+	dm_off_t	*roffp,
+	dm_size_t	*rlenp)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_REG,
+		DM_RIGHT_SHARED, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->probe_hole(tdp->td_vp, tdp->td_right,
+		off, len, roffp, rlenp);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+int
+dm_punch_hole(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_off_t	off,
+	dm_size_t	len)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_REG,
+		DM_RIGHT_EXCL, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->punch_hole(tdp->td_vp, tdp->td_right, off, len);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
diff -uNrp linux-2.6.5/fs/xfs/dmapi/dmapi_io.c linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_io.c
--- linux-2.6.5/fs/xfs/dmapi/dmapi_io.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_io.c	2004-04-05 00:58:15.000000000 +0200
@@ -0,0 +1,143 @@
+/*
+ * Copyright (c) 2000 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.	 Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+#include "xfs.h"
+#include "dmapi.h"
+#include "dmapi_kern.h"
+#include "dmapi_private.h"
+
+
+int
+dm_read_invis_rvp(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_off_t	off,
+	dm_size_t	len,
+	void		*bufp,
+	int		*rvp)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_REG,
+		DM_RIGHT_SHARED, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->read_invis_rvp(tdp->td_vp, tdp->td_right,
+		off, len, bufp, rvp);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+int
+dm_write_invis_rvp(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	int		flags,
+	dm_off_t	off,
+	dm_size_t	len,
+	void		*bufp,
+	int		*rvp)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_REG,
+		DM_RIGHT_EXCL, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->write_invis_rvp(tdp->td_vp, tdp->td_right,
+		flags, off, len, bufp, rvp);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+int
+dm_sync_by_handle (
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_REG,
+		DM_RIGHT_EXCL, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->sync_by_handle(tdp->td_vp, tdp->td_right);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+int
+dm_get_dioinfo (
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_dioinfo_t	*diop)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_REG,
+		DM_RIGHT_SHARED, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->get_dioinfo(tdp->td_vp, tdp->td_right, diop);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
diff -uNrp linux-2.6.5/fs/xfs/dmapi/dmapi_kern.h linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_kern.h
--- linux-2.6.5/fs/xfs/dmapi/dmapi_kern.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_kern.h	2004-04-05 00:58:15.000000000 +0200
@@ -0,0 +1,563 @@
+/*
+ * Copyright (c) 2000-2002 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.	 Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+
+#ifndef __DMAPI_KERN_H__
+#define __DMAPI_KERN_H__
+
+
+union sys_dmapi_uarg {
+	void *p;
+	__u64 u;
+};
+typedef union sys_dmapi_uarg sys_dmapi_u;
+
+struct sys_dmapi_args {
+	sys_dmapi_u uarg1, uarg2, uarg3, uarg4, uarg5, uarg6, uarg7, uarg8,
+		uarg9, uarg10, uarg11;
+};
+typedef struct sys_dmapi_args sys_dmapi_args_t;
+
+#define DM_Uarg(uap,i)	uap->uarg##i.u
+#define DM_Parg(uap,i)	uap->uarg##i.p
+
+#ifdef __KERNEL__
+
+struct xfs_handle_t;
+
+/* The first group of definitions and prototypes define the filesystem's
+   interface into the DMAPI code.
+*/
+
+
+/* Definitions used for the flags field on dm_send_data_event(),
+   dm_send_unmount_event(), and dm_send_namesp_event() calls.
+*/
+
+#define DM_FLAGS_NDELAY		0x001	/* return EAGAIN after dm_pending() */
+#define DM_FLAGS_UNWANTED	0x002	/* event not in fsys dm_eventset_t */
+
+/* Possible code levels reported by dm_code_level(). */
+
+#define DM_CLVL_INIT	0	/* DMAPI prior to X/Open compliance */
+#define DM_CLVL_XOPEN	1	/* X/Open compliant DMAPI */
+
+
+/* Prototypes used outside of the DMI module/directory. */
+
+int		dm_send_data_event(
+			dm_eventtype_t	event,
+			struct vnode	*vp,
+			dm_right_t	vp_right,
+			xfs_off_t	off,
+			size_t		len,
+			int		flags);
+
+int		dm_send_destroy_event(
+			struct vnode	*vp,
+			dm_right_t	vp_right);
+
+int		dm_send_mount_event(
+			struct vfs	*vfsp,
+			dm_right_t	vfsp_right,
+			struct vnode	*vp,
+			dm_right_t	vp_right,
+			struct vnode	*rootvp,
+			dm_right_t	rootvp_right,
+			char		*name1,
+			char		*name2);
+
+int		dm_send_namesp_event(
+			dm_eventtype_t	event,
+			struct vnode	*vp1,
+			dm_right_t	vp1_right,
+			struct vnode	*vp2,
+			dm_right_t	vp2_right,
+			char		*name1,
+			char		*name2,
+			mode_t		mode,
+			int		retcode,
+			int		flags);
+
+void		dm_send_unmount_event(
+			struct vfs	*vfsp,
+			struct vnode	*vp,
+			dm_right_t	vfsp_right,
+			mode_t		mode,
+			int		retcode,
+			int		flags);
+
+int		dm_code_level(void);
+
+int		dm_vp_to_handle (
+			struct vnode	*vp,
+			xfs_handle_t	*handlep);
+
+/* The following prototypes and definitions are used by DMAPI as its
+   interface into the filesystem code.	Communication between DMAPI and the
+   filesystem are established as follows:
+   1. DMAPI uses the VFS_DMAPI_FSYS_VECTOR to ask for the addresses
+      of all the functions within the filesystem that it may need to call.
+   2. The filesystem returns an array of function name/address pairs which
+      DMAPI builds into a function vector.
+   The VFS_DMAPI_FSYS_VECTOR call is only made one time for a particular
+   filesystem type.  From then on, DMAPI uses its function vector to call the
+   filesystem functions directly.  Functions in the array which DMAPI doesn't
+   recognize are ignored.  A dummy function which returns ENOSYS is used for
+   any function that DMAPI needs but which was not provided by the filesystem.
+   If XFS doesn't recognize the VFS_DMAPI_FSYS_VECTOR, DMAPI assumes that it
+   doesn't have the X/Open support code; in this case DMAPI uses the XFS-code
+   originally bundled within DMAPI.
+
+   The goal of this interface is allow incremental changes to be made to
+   both the filesystem and to DMAPI while minimizing inter-patch dependencies,
+   and to eventually allow DMAPI to support multiple filesystem types at the
+   same time should that become necessary.
+*/
+
+typedef enum {
+	DM_FSYS_CLEAR_INHERIT		=  0,
+	DM_FSYS_CREATE_BY_HANDLE	=  1,
+	DM_FSYS_DOWNGRADE_RIGHT		=  2,
+	DM_FSYS_GET_ALLOCINFO_RVP	=  3,
+	DM_FSYS_GET_BULKALL_RVP		=  4,
+	DM_FSYS_GET_BULKATTR_RVP	=  5,
+	DM_FSYS_GET_CONFIG		=  6,
+	DM_FSYS_GET_CONFIG_EVENTS	=  7,
+	DM_FSYS_GET_DESTROY_DMATTR	=  8,
+	DM_FSYS_GET_DIOINFO		=  9,
+	DM_FSYS_GET_DIRATTRS_RVP	= 10,
+	DM_FSYS_GET_DMATTR		= 11,
+	DM_FSYS_GET_EVENTLIST		= 12,
+	DM_FSYS_GET_FILEATTR		= 13,
+	DM_FSYS_GET_REGION		= 14,
+	DM_FSYS_GETALL_DMATTR		= 15,
+	DM_FSYS_GETALL_INHERIT		= 16,
+	DM_FSYS_INIT_ATTRLOC		= 17,
+	DM_FSYS_MKDIR_BY_HANDLE		= 18,
+	DM_FSYS_PROBE_HOLE		= 19,
+	DM_FSYS_PUNCH_HOLE		= 20,
+	DM_FSYS_READ_INVIS_RVP		= 21,
+	DM_FSYS_RELEASE_RIGHT		= 22,
+	DM_FSYS_REMOVE_DMATTR		= 23,
+	DM_FSYS_REQUEST_RIGHT		= 24,
+	DM_FSYS_SET_DMATTR		= 25,
+	DM_FSYS_SET_EVENTLIST		= 26,
+	DM_FSYS_SET_FILEATTR		= 27,
+	DM_FSYS_SET_INHERIT		= 28,
+	DM_FSYS_SET_REGION		= 29,
+	DM_FSYS_SYMLINK_BY_HANDLE	= 30,
+	DM_FSYS_SYNC_BY_HANDLE		= 31,
+	DM_FSYS_UPGRADE_RIGHT		= 32,
+	DM_FSYS_WRITE_INVIS_RVP		= 33,
+	DM_FSYS_OBJ_REF_HOLD		= 34,
+	DM_FSYS_MAX			= 35
+} dm_fsys_switch_t;
+
+
+#define DM_FSYS_OBJ	0x1		/* object refers to a fsys handle */
+
+
+/*
+ *  Prototypes for filesystem-specific functions.
+ */
+
+typedef int	(*dm_fsys_clear_inherit_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			dm_attrname_t	*attrnamep);
+
+typedef int	(*dm_fsys_create_by_handle_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			void		*hanp,
+			size_t		hlen,
+			char		*cname);
+
+typedef int	(*dm_fsys_downgrade_right_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			u_int		type);		/* DM_FSYS_OBJ or zero */
+
+typedef int	(*dm_fsys_get_allocinfo_rvp_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			dm_off_t	*offp,
+			u_int		nelem,
+			dm_extent_t	*extentp,
+			u_int		*nelemp,
+			int		*rvalp);
+
+typedef int	(*dm_fsys_get_bulkall_rvp_t)(
+			vnode_t		*vp,		/* root vnode */
+			dm_right_t	right,
+			u_int		mask,
+			dm_attrname_t	*attrnamep,
+			dm_attrloc_t	*locp,
+			size_t		buflen,
+			void		*bufp,
+			size_t		*rlenp,
+			int		*rvalp);
+
+typedef int	(*dm_fsys_get_bulkattr_rvp_t)(
+			vnode_t		*vp,		/* root vnode */
+			dm_right_t	right,
+			u_int		mask,
+			dm_attrloc_t	*locp,
+			size_t		buflen,
+			void		*bufp,
+			size_t		*rlenp,
+			int		*rvalp);
+
+typedef int	(*dm_fsys_get_config_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			dm_config_t	flagname,
+			dm_size_t	*retvalp);
+
+typedef int	(*dm_fsys_get_config_events_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			u_int		nelem,
+			dm_eventset_t	*eventsetp,
+			u_int		*nelemp);
+
+typedef int	(*dm_fsys_get_destroy_dmattr_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			dm_attrname_t	*attrnamep,
+			char		**valuepp,
+			int		*vlenp);
+
+typedef int	(*dm_fsys_get_dioinfo_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			dm_dioinfo_t	*diop);
+
+typedef int	(*dm_fsys_get_dirattrs_rvp_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			u_int		mask,
+			dm_attrloc_t	*locp,
+			size_t		buflen,
+			void		*bufp,
+			size_t		*rlenp,
+			int		*rvalp);
+
+typedef int	(*dm_fsys_get_dmattr_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			dm_attrname_t	*attrnamep,
+			size_t		buflen,
+			void		*bufp,
+			size_t		*rlenp);
+
+typedef int	(*dm_fsys_get_eventlist_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			u_int		type,
+			u_int		nelem,
+			dm_eventset_t	*eventsetp,	/* in kernel space! */
+			u_int		*nelemp);		/* in kernel space! */
+
+typedef int	(*dm_fsys_get_fileattr_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			u_int		mask,
+			dm_stat_t	*statp);
+
+typedef int	(*dm_fsys_get_region_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			u_int		nelem,
+			dm_region_t	*regbufp,
+			u_int		*nelemp);
+
+typedef int	(*dm_fsys_getall_dmattr_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			size_t		buflen,
+			void		*bufp,
+			size_t		*rlenp);
+
+typedef int	(*dm_fsys_getall_inherit_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			u_int		nelem,
+			dm_inherit_t	*inheritbufp,
+			u_int		*nelemp);
+
+typedef int	(*dm_fsys_init_attrloc_t)(
+			vnode_t		*vp,	/* sometimes root vnode */
+			dm_right_t	right,
+			dm_attrloc_t	*locp);
+
+typedef int	(*dm_fsys_mkdir_by_handle_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			void		*hanp,
+			size_t		hlen,
+			char		*cname);
+
+typedef int	(*dm_fsys_probe_hole_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			dm_off_t	off,
+			dm_size_t	len,
+			dm_off_t	*roffp,
+			dm_size_t	*rlenp);
+
+typedef int	(*dm_fsys_punch_hole_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			dm_off_t	off,
+			dm_size_t	len);
+
+typedef int	(*dm_fsys_read_invis_rvp_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			dm_off_t	off,
+			dm_size_t	len,
+			void		*bufp,
+			int		*rvp);
+
+typedef int	(*dm_fsys_release_right_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			u_int		type);
+
+typedef int	(*dm_fsys_remove_dmattr_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			int		setdtime,
+			dm_attrname_t	*attrnamep);
+
+typedef int	(*dm_fsys_request_right_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			u_int		type,	/* DM_FSYS_OBJ or zero */
+			u_int		flags,
+			dm_right_t	newright);
+
+typedef int	(*dm_fsys_set_dmattr_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			dm_attrname_t	*attrnamep,
+			int		setdtime,
+			size_t		buflen,
+			void		*bufp);
+
+typedef int	(*dm_fsys_set_eventlist_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			u_int		type,
+			dm_eventset_t	*eventsetp,	/* in kernel space! */
+			u_int		maxevent);
+
+typedef int	(*dm_fsys_set_fileattr_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			u_int		mask,
+			dm_fileattr_t	*attrp);
+
+typedef int	(*dm_fsys_set_inherit_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			dm_attrname_t	*attrnamep,
+			mode_t		mode);
+
+typedef int	(*dm_fsys_set_region_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			u_int		nelem,
+			dm_region_t	*regbufp,
+			dm_boolean_t	*exactflagp);
+
+typedef int	(*dm_fsys_symlink_by_handle_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			void		*hanp,
+			size_t		hlen,
+			char		*cname,
+			char		*path);
+
+typedef int	(*dm_fsys_sync_by_handle_t)(
+			vnode_t		*vp,
+			dm_right_t	right);
+
+typedef int	(*dm_fsys_upgrade_right_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			u_int		type);		/* DM_FSYS_OBJ or zero */
+
+typedef int	(*dm_fsys_write_invis_rvp_t)(
+			vnode_t		*vp,
+			dm_right_t	right,
+			int		flags,
+			dm_off_t	off,
+			dm_size_t	len,
+			void		*bufp,
+			int		*rvp);
+
+typedef void	(*dm_fsys_obj_ref_hold_t)(
+			vnode_t		*vp);
+
+
+/* Structure definitions used by the VFS_DMAPI_FSYS_VECTOR call. */
+
+typedef struct {
+	dm_fsys_switch_t  func_no;	/* function number */
+	union {
+		dm_fsys_clear_inherit_t clear_inherit;
+		dm_fsys_create_by_handle_t create_by_handle;
+		dm_fsys_downgrade_right_t downgrade_right;
+		dm_fsys_get_allocinfo_rvp_t get_allocinfo_rvp;
+		dm_fsys_get_bulkall_rvp_t get_bulkall_rvp;
+		dm_fsys_get_bulkattr_rvp_t get_bulkattr_rvp;
+		dm_fsys_get_config_t get_config;
+		dm_fsys_get_config_events_t get_config_events;
+		dm_fsys_get_destroy_dmattr_t get_destroy_dmattr;
+		dm_fsys_get_dioinfo_t get_dioinfo;
+		dm_fsys_get_dirattrs_rvp_t get_dirattrs_rvp;
+		dm_fsys_get_dmattr_t get_dmattr;
+		dm_fsys_get_eventlist_t get_eventlist;
+		dm_fsys_get_fileattr_t get_fileattr;
+		dm_fsys_get_region_t get_region;
+		dm_fsys_getall_dmattr_t getall_dmattr;
+		dm_fsys_getall_inherit_t getall_inherit;
+		dm_fsys_init_attrloc_t init_attrloc;
+		dm_fsys_mkdir_by_handle_t mkdir_by_handle;
+		dm_fsys_probe_hole_t probe_hole;
+		dm_fsys_punch_hole_t punch_hole;
+		dm_fsys_read_invis_rvp_t read_invis_rvp;
+		dm_fsys_release_right_t release_right;
+		dm_fsys_remove_dmattr_t remove_dmattr;
+		dm_fsys_request_right_t request_right;
+		dm_fsys_set_dmattr_t set_dmattr;
+		dm_fsys_set_eventlist_t set_eventlist;
+		dm_fsys_set_fileattr_t set_fileattr;
+		dm_fsys_set_inherit_t set_inherit;
+		dm_fsys_set_region_t set_region;
+		dm_fsys_symlink_by_handle_t symlink_by_handle;
+		dm_fsys_sync_by_handle_t sync_by_handle;
+		dm_fsys_upgrade_right_t upgrade_right;
+		dm_fsys_write_invis_rvp_t write_invis_rvp;
+		dm_fsys_obj_ref_hold_t obj_ref_hold;
+	} u_fc;
+} fsys_function_vector_t;
+
+struct dm_fcntl_vector {
+	int	code_level;
+	int	count;		/* Number of functions in the vector */
+	fsys_function_vector_t *vecp;
+};
+typedef struct dm_fcntl_vector dm_fcntl_vector_t;
+
+struct dm_fcntl_mapevent {
+	size_t	length;			/* length of transfer */
+	dm_eventtype_t	max_event;	/* Maximum (WRITE or READ)  event */
+	int	error;			/* returned error code */
+};
+typedef struct dm_fcntl_mapevent dm_fcntl_mapevent_t;
+
+#endif	/* __KERNEL__ */
+
+
+/* The following definitions are needed both by the kernel and by the
+   library routines.
+*/
+
+#define DM_MAX_HANDLE_SIZE	56	/* maximum size for a file handle */
+
+
+/*
+ *  Opcodes for dmapi ioctl.
+ */
+
+#define DM_CLEAR_INHERIT	1
+#define DM_CREATE_BY_HANDLE	2
+#define DM_CREATE_SESSION	3
+#define DM_CREATE_USEREVENT	4
+#define DM_DESTROY_SESSION	5
+#define DM_DOWNGRADE_RIGHT	6
+#define DM_FD_TO_HANDLE		7
+#define DM_FIND_EVENTMSG	8
+#define DM_GET_ALLOCINFO	9
+#define DM_GET_BULKALL		10
+#define DM_GET_BULKATTR		11
+#define DM_GET_CONFIG		12
+#define DM_GET_CONFIG_EVENTS	13
+#define DM_GET_DIOINFO		14
+#define DM_GET_DIRATTRS		15
+#define DM_GET_DMATTR		16
+#define DM_GET_EVENTLIST	17
+#define DM_GET_EVENTS		18
+#define DM_GET_FILEATTR		19
+#define DM_GET_MOUNTINFO	20
+#define DM_GET_REGION		21
+#define DM_GETALL_DISP		22
+#define DM_GETALL_DMATTR	23
+#define DM_GETALL_INHERIT	24
+#define DM_GETALL_SESSIONS	25
+#define DM_GETALL_TOKENS	26
+#define DM_INIT_ATTRLOC		27
+#define DM_MKDIR_BY_HANDLE	28
+#define DM_MOVE_EVENT		29
+#define DM_OBJ_REF_HOLD		30
+#define DM_OBJ_REF_QUERY	31
+#define DM_OBJ_REF_RELE		32
+#define DM_PATH_TO_FSHANDLE	33
+#define DM_PATH_TO_HANDLE	34
+#define DM_PENDING		35
+#define DM_PROBE_HOLE		36
+#define DM_PUNCH_HOLE		37
+#define DM_QUERY_RIGHT		38
+#define DM_QUERY_SESSION	39
+#define DM_READ_INVIS		40
+#define DM_RELEASE_RIGHT	41
+#define DM_REMOVE_DMATTR	42
+#define DM_REQUEST_RIGHT	43
+#define DM_RESPOND_EVENT	44
+#define DM_SEND_MSG		45
+#define DM_SET_DISP		46
+#define DM_SET_DMATTR		47
+#define DM_SET_EVENTLIST	48
+#define DM_SET_FILEATTR		49
+#define DM_SET_INHERIT		50
+#define DM_SET_REGION		51
+#define DM_SET_RETURN_ON_DESTROY 52
+#define DM_SYMLINK_BY_HANDLE	53
+#define DM_SYNC_BY_HANDLE	54
+#define DM_UPGRADE_RIGHT	55
+#define DM_WRITE_INVIS		56
+#define DM_OPEN_BY_HANDLE	57
+
+#endif /* __DMAPI_KERN_H__ */
diff -uNrp linux-2.6.5/fs/xfs/dmapi/dmapi_mountinfo.c linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_mountinfo.c
--- linux-2.6.5/fs/xfs/dmapi/dmapi_mountinfo.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_mountinfo.c	2004-04-05 00:58:15.000000000 +0200
@@ -0,0 +1,366 @@
+/*
+ * Copyright (c) 2000-2003 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.	 Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+#include "xfs.h"
+#include "dmapi.h"
+#include "dmapi_kern.h"
+#include "dmapi_private.h"
+
+#ifdef linux
+/* XXX */
+#define vfsmax 1
+#endif
+
+typedef struct	{
+	int	support_type;
+	char	name[16];
+	dm_fsys_vector_t	*vptr;
+} dm_vector_map_t;
+
+/* Values for the support_type field. */
+
+#define DM_SUPPORT_UNKNOWN	0
+#define DM_SUPPORT_AVAIL	1
+
+
+dm_vector_map_t *dm_fsys_map = NULL;
+
+
+int
+dm_code_level(void)
+{
+	return(DM_CLVL_XOPEN);	/* initial X/Open compliant release */
+}
+
+
+/* Dummy routine which is stored in each function vector slot for which the
+   filesystem provides no function of its own.	If an application calls the
+   function, he will just get ENOSYS.
+*/
+
+static int
+dm_enosys(void)
+{
+	return(ENOSYS);		/* function not supported by filesystem */
+}
+
+
+/* dm_query_fsys_for_vector() asks a filesystem for its list of supported
+   DMAPI functions, and builds a dm_vector_map_t structure based upon the
+   reply.  We ignore functions supported by the filesystem which we do not
+   know about, and we substitute the subroutine 'dm_enosys' for each function
+   we know about but the filesystem does not support.
+*/
+
+static void
+dm_query_fsys_for_vector(
+	vnode_t		*vp)
+{
+	fsys_function_vector_t	*vecp;
+	dm_fcntl_vector_t	vecrq;
+	dm_fsys_vector_t	*vptr;
+	dm_vector_map_t	*map;
+	struct vfs	*vfsp = vp->v_vfsp;
+	int		fstype;
+	int		error;
+	int		i;
+
+	/* XXX fstype = vfsp->vfs_fstype */
+	fstype = 0;
+	map = &dm_fsys_map[fstype];
+
+	/* Clear out any information left from a previous filesystem that was
+	   in this slot and initialize it for the new filesystem.
+	*/
+
+	if (map->vptr) {
+		kfree(map->vptr);
+		map->vptr = NULL;
+	}
+
+	/* XXX	strcpy(map->name, vfssw[fstype].vsw_name); */
+	strcpy(map->name, XFS_NAME);
+
+	map->support_type = DM_SUPPORT_AVAIL;
+
+	/* Next allocate a function vector and initialize all fields with a
+	   dummy function that returns ENOSYS.
+	*/
+
+	vptr = map->vptr = kmalloc(sizeof(*map->vptr), SLAB_KERNEL);
+	if (vptr == NULL) {
+		printk("%s/%d: kmalloc returned NULL\n", __FUNCTION__, __LINE__);
+		return;
+	}
+
+	strncpy(vptr->fsys_name, map->name, sizeof(vptr->fsys_name));
+	vptr->code_level = 0;
+	vptr->clear_inherit = (dm_fsys_clear_inherit_t)dm_enosys;
+	vptr->create_by_handle = (dm_fsys_create_by_handle_t)dm_enosys;
+	vptr->downgrade_right = (dm_fsys_downgrade_right_t)dm_enosys;
+	vptr->get_allocinfo_rvp = (dm_fsys_get_allocinfo_rvp_t)dm_enosys;
+	vptr->get_bulkall_rvp = (dm_fsys_get_bulkall_rvp_t)dm_enosys;
+	vptr->get_bulkattr_rvp = (dm_fsys_get_bulkattr_rvp_t)dm_enosys;
+	vptr->get_config = (dm_fsys_get_config_t)dm_enosys;
+	vptr->get_config_events = (dm_fsys_get_config_events_t)dm_enosys;
+	vptr->get_destroy_dmattr = (dm_fsys_get_destroy_dmattr_t)dm_enosys;
+	vptr->get_dioinfo = (dm_fsys_get_dioinfo_t)dm_enosys;
+	vptr->get_dirattrs_rvp = (dm_fsys_get_dirattrs_rvp_t)dm_enosys;
+	vptr->get_dmattr = (dm_fsys_get_dmattr_t)dm_enosys;
+	vptr->get_eventlist = (dm_fsys_get_eventlist_t)dm_enosys;
+	vptr->get_fileattr = (dm_fsys_get_fileattr_t)dm_enosys;
+	vptr->get_region = (dm_fsys_get_region_t)dm_enosys;
+	vptr->getall_dmattr = (dm_fsys_getall_dmattr_t)dm_enosys;
+	vptr->getall_inherit = (dm_fsys_getall_inherit_t)dm_enosys;
+	vptr->init_attrloc = (dm_fsys_init_attrloc_t)dm_enosys;
+	vptr->mkdir_by_handle = (dm_fsys_mkdir_by_handle_t)dm_enosys;
+	vptr->probe_hole = (dm_fsys_probe_hole_t)dm_enosys;
+	vptr->punch_hole = (dm_fsys_punch_hole_t)dm_enosys;
+	vptr->read_invis_rvp = (dm_fsys_read_invis_rvp_t)dm_enosys;
+	vptr->release_right = (dm_fsys_release_right_t)dm_enosys;
+	vptr->request_right = (dm_fsys_request_right_t)dm_enosys;
+	vptr->remove_dmattr = (dm_fsys_remove_dmattr_t)dm_enosys;
+	vptr->set_dmattr = (dm_fsys_set_dmattr_t)dm_enosys;
+	vptr->set_eventlist = (dm_fsys_set_eventlist_t)dm_enosys;
+	vptr->set_fileattr = (dm_fsys_set_fileattr_t)dm_enosys;
+	vptr->set_inherit = (dm_fsys_set_inherit_t)dm_enosys;
+	vptr->set_region = (dm_fsys_set_region_t)dm_enosys;
+	vptr->symlink_by_handle = (dm_fsys_symlink_by_handle_t)dm_enosys;
+	vptr->sync_by_handle = (dm_fsys_sync_by_handle_t)dm_enosys;
+	vptr->upgrade_right = (dm_fsys_upgrade_right_t)dm_enosys;
+	vptr->write_invis_rvp = (dm_fsys_write_invis_rvp_t)dm_enosys;
+	vptr->obj_ref_hold = (dm_fsys_obj_ref_hold_t)dm_enosys;
+
+	/* Issue a call to the filesystem in order to obtain
+	   its vector of filesystem-specific DMAPI routines.
+	*/
+
+	vecrq.count = 0;
+	vecrq.vecp = NULL;
+
+	VFS_DMAPIOPS(vfsp, (caddr_t)&vecrq, error);
+
+	/* If we still have an error at this point, then the filesystem simply
+	   does not support DMAPI, so we give up with all functions set to
+	   ENOSYS.
+	*/
+
+	if (error || vecrq.count == 0)
+		return;
+
+	/* The request succeeded and we were given a vector which we need to
+	   map to our current level.  Overlay the dummy function with every
+	   filesystem function we understand.
+	*/
+
+	vptr->code_level = vecrq.code_level;
+	vecp = vecrq.vecp;
+	for (i = 0; i < vecrq.count; i++) {
+		switch (vecp[i].func_no) {
+		case DM_FSYS_CLEAR_INHERIT:
+			vptr->clear_inherit = vecp[i].u_fc.clear_inherit;
+			break;
+		case DM_FSYS_CREATE_BY_HANDLE:
+			vptr->create_by_handle = vecp[i].u_fc.create_by_handle;
+			break;
+		case DM_FSYS_DOWNGRADE_RIGHT:
+			vptr->downgrade_right = vecp[i].u_fc.downgrade_right;
+			break;
+		case DM_FSYS_GET_ALLOCINFO_RVP:
+			vptr->get_allocinfo_rvp = vecp[i].u_fc.get_allocinfo_rvp;
+			break;
+		case DM_FSYS_GET_BULKALL_RVP:
+			vptr->get_bulkall_rvp = vecp[i].u_fc.get_bulkall_rvp;
+			break;
+		case DM_FSYS_GET_BULKATTR_RVP:
+			vptr->get_bulkattr_rvp = vecp[i].u_fc.get_bulkattr_rvp;
+			break;
+		case DM_FSYS_GET_CONFIG:
+			vptr->get_config = vecp[i].u_fc.get_config;
+			break;
+		case DM_FSYS_GET_CONFIG_EVENTS:
+			vptr->get_config_events = vecp[i].u_fc.get_config_events;
+			break;
+		case DM_FSYS_GET_DESTROY_DMATTR:
+			vptr->get_destroy_dmattr = vecp[i].u_fc.get_destroy_dmattr;
+			break;
+		case DM_FSYS_GET_DIOINFO:
+			vptr->get_dioinfo = vecp[i].u_fc.get_dioinfo;
+			break;
+		case DM_FSYS_GET_DIRATTRS_RVP:
+			vptr->get_dirattrs_rvp = vecp[i].u_fc.get_dirattrs_rvp;
+			break;
+		case DM_FSYS_GET_DMATTR:
+			vptr->get_dmattr = vecp[i].u_fc.get_dmattr;
+			break;
+		case DM_FSYS_GET_EVENTLIST:
+			vptr->get_eventlist = vecp[i].u_fc.get_eventlist;
+			break;
+		case DM_FSYS_GET_FILEATTR:
+			vptr->get_fileattr = vecp[i].u_fc.get_fileattr;
+			break;
+		case DM_FSYS_GET_REGION:
+			vptr->get_region = vecp[i].u_fc.get_region;
+			break;
+		case DM_FSYS_GETALL_DMATTR:
+			vptr->getall_dmattr = vecp[i].u_fc.getall_dmattr;
+			break;
+		case DM_FSYS_GETALL_INHERIT:
+			vptr->getall_inherit = vecp[i].u_fc.getall_inherit;
+			break;
+		case DM_FSYS_INIT_ATTRLOC:
+			vptr->init_attrloc = vecp[i].u_fc.init_attrloc;
+			break;
+		case DM_FSYS_MKDIR_BY_HANDLE:
+			vptr->mkdir_by_handle = vecp[i].u_fc.mkdir_by_handle;
+			break;
+		case DM_FSYS_PROBE_HOLE:
+			vptr->probe_hole = vecp[i].u_fc.probe_hole;
+			break;
+		case DM_FSYS_PUNCH_HOLE:
+			vptr->punch_hole = vecp[i].u_fc.punch_hole;
+			break;
+		case DM_FSYS_READ_INVIS_RVP:
+			vptr->read_invis_rvp = vecp[i].u_fc.read_invis_rvp;
+			break;
+		case DM_FSYS_RELEASE_RIGHT:
+			vptr->release_right = vecp[i].u_fc.release_right;
+			break;
+		case DM_FSYS_REMOVE_DMATTR:
+			vptr->remove_dmattr = vecp[i].u_fc.remove_dmattr;
+			break;
+		case DM_FSYS_REQUEST_RIGHT:
+			vptr->request_right = vecp[i].u_fc.request_right;
+			break;
+		case DM_FSYS_SET_DMATTR:
+			vptr->set_dmattr = vecp[i].u_fc.set_dmattr;
+			break;
+		case DM_FSYS_SET_EVENTLIST:
+			vptr->set_eventlist = vecp[i].u_fc.set_eventlist;
+			break;
+		case DM_FSYS_SET_FILEATTR:
+			vptr->set_fileattr = vecp[i].u_fc.set_fileattr;
+			break;
+		case DM_FSYS_SET_INHERIT:
+			vptr->set_inherit = vecp[i].u_fc.set_inherit;
+			break;
+		case DM_FSYS_SET_REGION:
+			vptr->set_region = vecp[i].u_fc.set_region;
+			break;
+		case DM_FSYS_SYMLINK_BY_HANDLE:
+			vptr->symlink_by_handle = vecp[i].u_fc.symlink_by_handle;
+			break;
+		case DM_FSYS_SYNC_BY_HANDLE:
+			vptr->sync_by_handle = vecp[i].u_fc.sync_by_handle;
+			break;
+		case DM_FSYS_UPGRADE_RIGHT:
+			vptr->upgrade_right = vecp[i].u_fc.upgrade_right;
+			break;
+		case DM_FSYS_WRITE_INVIS_RVP:
+			vptr->write_invis_rvp = vecp[i].u_fc.write_invis_rvp;
+			break;
+		case DM_FSYS_OBJ_REF_HOLD:
+			vptr->obj_ref_hold = vecp[i].u_fc.obj_ref_hold;
+			break;
+		default:		/* ignore ones we don't understand */
+			break;
+		}
+	}
+}
+
+
+/* Given a behavior pointer, dm_fsys_vector() returns a pointer to the DMAPI
+   function vector to be used for the corresponding vnode.  There is one possible
+   function vector for each filesystem type, although currently XFS is the
+   only filesystem that actually supports DMAPI.
+*/
+
+dm_fsys_vector_t *
+dm_fsys_vector(
+	vnode_t		*vp)
+{
+	dm_vector_map_t *map;
+	int		fstype;
+
+	/* XXX fstype = vp->v_vfsp->vfs_fstype */
+	fstype = 0;
+
+	/* If this is the first call, initialize the filesystem function
+	   vector map.
+	*/
+
+	if (dm_fsys_map == NULL) {
+		int	size = vfsmax * sizeof(*dm_fsys_map);
+
+		dm_fsys_map = (dm_vector_map_t *)kmalloc(size, GFP_KERNEL);
+		if (dm_fsys_map == NULL) {
+			printk("%s/%d: kmalloc returned NULL\n", __FUNCTION__, __LINE__);
+			return NULL;
+		}
+		memset(dm_fsys_map, 0, size);
+	}
+	map = &dm_fsys_map[fstype];
+
+	/* If a new filesystem has been dynamically loaded into a slot
+	   previously held by another filesystem, then treat it as a
+	   DM_SUPPORT_UNKNOWN.
+	*/
+
+	/* XXX if (strcmp(map->name, vfssw[fstype].vsw_name)) */
+	if (strcmp(map->name, XFS_NAME))
+		map->support_type = DM_SUPPORT_UNKNOWN;
+
+	/* If we don't yet know what the filesystem supports, ask it. */
+
+	if (map->support_type == DM_SUPPORT_UNKNOWN)
+		dm_query_fsys_for_vector(vp);
+
+	/* Now return the function vector. */
+
+	return(map->vptr);
+}
+
+
+void
+dm_fsys_vector_free(void)
+{
+	dm_vector_map_t *map;
+	int i;
+
+	if (dm_fsys_map) {
+		for (i = 0; i < vfsmax; i++){
+			map = &dm_fsys_map[i];
+			if (map->vptr)
+				kfree(map->vptr);
+		}
+		kfree(dm_fsys_map);
+	}
+}
diff -uNrp linux-2.6.5/fs/xfs/dmapi/dmapi_private.h linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_private.h
--- linux-2.6.5/fs/xfs/dmapi/dmapi_private.h	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_private.h	2004-04-05 00:58:15.000000000 +0200
@@ -0,0 +1,597 @@
+/*
+ * Copyright (c) 2000-2003 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.	 Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+#ifndef _DMAPI_PRIVATE_H
+#define _DMAPI_PRIVATE_H
+
+
+#ifdef CONFIG_PROC_FS
+#define DMAPI_PROCFS		"fs/xfs_dmapi_v2" /* DMAPI device in /proc. */
+#define DMAPI_DBG_PROCFS	"fs/xfs_dmapi_d" /* DMAPI debugging dir */
+#endif
+
+extern struct kmem_cache_s	*dm_fsreg_cachep;
+extern struct kmem_cache_s	*dm_tokdata_cachep;
+extern struct kmem_cache_s	*dm_session_cachep;
+
+
+
+typedef struct dm_tokdata {
+	struct dm_tokdata *td_next;
+	struct dm_tokevent *td_tevp;	/* pointer to owning tevp */
+	int		td_app_ref;	/* # app threads currently active */
+	dm_right_t	td_orig_right;	/* original right held when created */
+	dm_right_t	td_right;	/* current right held for this handle */
+	short		td_flags;
+	short		td_type;	/* object type */
+	int		td_vcount;	/* # of current application VN_HOLDs */
+	vnode_t		*td_vp;		/* vnode pointer */
+	xfs_handle_t	td_handle;	/* handle for vp or vfsp */
+} dm_tokdata_t;
+
+/* values for td_type */
+
+#define DM_TDT_NONE	0x00		/* td_handle is empty */
+#define DM_TDT_VFS	0x01		/* td_handle points to a vfs */
+#define DM_TDT_REG	0x02		/* td_handle points to a file */
+#define DM_TDT_DIR	0x04		/* td_handle points to a directory */
+#define DM_TDT_LNK	0x08		/* td_handle points to a symlink */
+#define DM_TDT_OTH	0x10		/* some other object eg. pipe, socket */
+
+#define DM_TDT_VNO	(DM_TDT_REG|DM_TDT_DIR|DM_TDT_LNK|DM_TDT_OTH)
+#define DM_TDT_ANY	(DM_TDT_VFS|DM_TDT_REG|DM_TDT_DIR|DM_TDT_LNK|DM_TDT_OTH)
+
+/* values for td_flags */
+
+#define DM_TDF_ORIG	0x0001		/* part of the original event */
+#define DM_TDF_EVTREF	0x0002		/* event thread holds vnode reference */
+#define DM_TDF_STHREAD	0x0004		/* only one app can use this handle */
+#define DM_TDF_RIGHT	0x0008		/* vcount bumped for dm_request_right */
+#define DM_TDF_HOLD	0x0010		/* vcount bumped for dm_obj_ref_hold */
+
+
+/* Because some events contain __u64 fields, we force te_msg and te_event
+   to always be 8-byte aligned.	 In order to send more than one message in
+   a single dm_get_events() call, we also ensure that each message is an
+   8-byte multiple.
+*/
+
+typedef struct dm_tokevent {
+	struct dm_tokevent  *te_next;
+	struct dm_tokevent  *te_hashnext; /* hash chain */
+	lock_t		te_lock;	/* lock for all fields but te_*next.
+					 * te_next and te_hashnext are
+					 * protected by the session lock.
+					 */
+	short		te_flags;
+	short		te_allocsize;	/* alloc'ed size of this structure */
+	sv_t		te_evt_queue;	/* queue waiting for dm_respond_event */
+	sv_t		te_app_queue;	/* queue waiting for handle access */
+	int		te_evt_ref;	/* number of event procs using token */
+	int		te_app_ref;	/* number of app procs using token */
+	int		te_app_slp;	/* number of app procs sleeping */
+	int		te_reply;	/* return errno for sync messages */
+	dm_tokdata_t	*te_tdp;	/* list of handle/right pairs */
+	union {
+		__u64		align;	/* force alignment of te_msg */
+		dm_eventmsg_t	te_msg;		/* user visible part */
+	} te_u;
+	__u64		te_event;	/* start of dm_xxx_event_t message */
+} dm_tokevent_t;
+
+#define te_msg	te_u.te_msg
+
+/* values for te_flags */
+
+#define DM_TEF_LOCKED	0x0001		/* event "locked" by dm_get_events() */
+#define DM_TEF_INTERMED 0x0002		/* a dm_pending reply was received */
+#define DM_TEF_FINAL	0x0004		/* dm_respond_event has been received */
+#define DM_TEF_HASHED	0x0010		/* event is on hash chain  */
+
+
+#ifdef DEBUG
+#define DM_SHASH_DEBUG
+#endif
+
+typedef struct dm_sesshash {
+	dm_tokevent_t	*h_next;	/* ptr to chain of tokevents */
+#ifdef DM_SHASH_DEBUG
+	int		maxlength;
+	int		curlength;
+	int		num_adds;
+	int		num_dels;
+	int		dup_hits;
+#endif
+} dm_sesshash_t;
+
+
+typedef struct dm_eventq {
+	dm_tokevent_t	*eq_head;
+	dm_tokevent_t	*eq_tail;
+	int		eq_count;	/* size of queue */
+} dm_eventq_t;
+
+
+typedef struct dm_session {
+	struct dm_session	*sn_next;	/* sessions linkage */
+	dm_sessid_t	sn_sessid;	/* user-visible session number */
+	u_int		sn_flags;
+	lock_t		sn_qlock;	/* lock for newq/delq related fields */
+	sv_t		sn_readerq;	/* waiting for message on sn_newq */
+	sv_t		sn_writerq;	/* waiting for room on sn_newq */
+	u_int		sn_readercnt;	/* count of waiting readers */
+	u_int		sn_writercnt;	/* count of waiting readers */
+	dm_eventq_t	sn_newq;	/* undelivered event queue */
+	dm_eventq_t	sn_delq;	/* delivered event queue */
+	dm_eventq_t	sn_evt_writerq; /* events of thrds in sn_writerq */
+	dm_sesshash_t	*sn_sesshash;	/* buckets for tokevent hash chains */
+#ifdef DM_SHASH_DEBUG
+	int		sn_buckets_in_use;
+	int		sn_max_buckets_in_use;
+#endif
+	char		sn_info[DM_SESSION_INFO_LEN];	/* user-supplied info */
+} dm_session_t;
+
+/* values for sn_flags */
+
+#define DM_SN_WANTMOUNT 0x0001		/* session wants to get mount events */
+
+
+typedef enum {
+	DM_STATE_MOUNTING,
+	DM_STATE_MOUNTED,
+	DM_STATE_UNMOUNTING,
+	DM_STATE_UNMOUNTED
+} dm_fsstate_t;
+
+
+typedef struct dm_fsreg {
+	struct dm_fsreg *fr_next;
+	vfs_t		*fr_vfsp;	/* filesystem pointer */
+	dm_tokevent_t	*fr_tevp;
+	xfs_fsid_t	fr_fsid;	/* filesystem ID */
+	void		*fr_msg;	/* dm_mount_event_t for filesystem */
+	int		fr_msgsize;	/* size of dm_mount_event_t */
+	dm_fsstate_t	fr_state;
+	sv_t		fr_dispq;
+	int		fr_dispcnt;
+	dm_eventq_t	fr_evt_dispq;	/* events of thrds in fr_dispq */
+	sv_t		fr_queue;	/* queue for hdlcnt/vfscnt/unmount */
+	lock_t		fr_lock;
+	int		fr_hdlcnt;	/* threads blocked during unmount */
+	int		fr_vfscnt;	/* threads in VFS_VGET or VFS_ROOT */
+	int		fr_unmount;	/* if non-zero, umount is sleeping */
+	dm_attrname_t	fr_rattr;	/* dm_set_return_on_destroy attribute */
+	dm_session_t	*fr_sessp [DM_EVENT_MAX];
+} dm_fsreg_t;
+
+
+
+
+/* events valid in dm_set_disp() when called with a filesystem handle. */
+
+#define DM_VALID_DISP_EVENTS		( \
+	(1 << DM_EVENT_PREUNMOUNT)	| \
+	(1 << DM_EVENT_UNMOUNT)		| \
+	(1 << DM_EVENT_NOSPACE)		| \
+	(1 << DM_EVENT_DEBUT)		| \
+	(1 << DM_EVENT_CREATE)		| \
+	(1 << DM_EVENT_POSTCREATE)	| \
+	(1 << DM_EVENT_REMOVE)		| \
+	(1 << DM_EVENT_POSTREMOVE)	| \
+	(1 << DM_EVENT_RENAME)		| \
+	(1 << DM_EVENT_POSTRENAME)	| \
+	(1 << DM_EVENT_LINK)		| \
+	(1 << DM_EVENT_POSTLINK)	| \
+	(1 << DM_EVENT_SYMLINK)		| \
+	(1 << DM_EVENT_POSTSYMLINK)	| \
+	(1 << DM_EVENT_READ)		| \
+	(1 << DM_EVENT_WRITE)		| \
+	(1 << DM_EVENT_TRUNCATE)	| \
+	(1 << DM_EVENT_ATTRIBUTE)	| \
+	(1 << DM_EVENT_DESTROY)		)
+
+
+/* isolate the read/write/trunc events of a dm_tokevent_t */
+
+#define DM_EVENT_RDWRTRUNC(tevp)			(  \
+	((tevp)->te_msg.ev_type == DM_EVENT_READ)	|| \
+	((tevp)->te_msg.ev_type == DM_EVENT_WRITE)	|| \
+	((tevp)->te_msg.ev_type == DM_EVENT_TRUNCATE)	)
+
+
+/*
+ *  Global handle hack isolation.
+ */
+
+#define DM_GLOBALHAN(hanp, hlen)	(((hanp) == DM_GLOBAL_HANP) && \
+					 ((hlen) == DM_GLOBAL_HLEN))
+
+
+#define DM_MAX_MSG_DATA		3960
+
+
+
+/* Supported filesystem function vector functions. */
+
+
+typedef struct {
+	int				code_level;
+	char				fsys_name[16];
+	dm_fsys_clear_inherit_t		clear_inherit;
+	dm_fsys_create_by_handle_t	create_by_handle;
+	dm_fsys_downgrade_right_t	downgrade_right;
+	dm_fsys_get_allocinfo_rvp_t	get_allocinfo_rvp;
+	dm_fsys_get_bulkall_rvp_t	get_bulkall_rvp;
+	dm_fsys_get_bulkattr_rvp_t	get_bulkattr_rvp;
+	dm_fsys_get_config_t		get_config;
+	dm_fsys_get_config_events_t	get_config_events;
+	dm_fsys_get_destroy_dmattr_t	get_destroy_dmattr;
+	dm_fsys_get_dioinfo_t		get_dioinfo;
+	dm_fsys_get_dirattrs_rvp_t	get_dirattrs_rvp;
+	dm_fsys_get_dmattr_t		get_dmattr;
+	dm_fsys_get_eventlist_t		get_eventlist;
+	dm_fsys_get_fileattr_t		get_fileattr;
+	dm_fsys_get_region_t		get_region;
+	dm_fsys_getall_dmattr_t		getall_dmattr;
+	dm_fsys_getall_inherit_t	getall_inherit;
+	dm_fsys_init_attrloc_t		init_attrloc;
+	dm_fsys_mkdir_by_handle_t	mkdir_by_handle;
+	dm_fsys_probe_hole_t		probe_hole;
+	dm_fsys_punch_hole_t		punch_hole;
+	dm_fsys_read_invis_rvp_t	read_invis_rvp;
+	dm_fsys_release_right_t		release_right;
+	dm_fsys_remove_dmattr_t		remove_dmattr;
+	dm_fsys_request_right_t		request_right;
+	dm_fsys_set_dmattr_t		set_dmattr;
+	dm_fsys_set_eventlist_t		set_eventlist;
+	dm_fsys_set_fileattr_t		set_fileattr;
+	dm_fsys_set_inherit_t		set_inherit;
+	dm_fsys_set_region_t		set_region;
+	dm_fsys_symlink_by_handle_t	symlink_by_handle;
+	dm_fsys_sync_by_handle_t	sync_by_handle;
+	dm_fsys_upgrade_right_t		upgrade_right;
+	dm_fsys_write_invis_rvp_t	write_invis_rvp;
+	dm_fsys_obj_ref_hold_t		obj_ref_hold;
+} dm_fsys_vector_t;
+
+
+extern	dm_session_t	*dm_sessions;		/* head of session list */
+extern	dm_fsreg_t	*dm_registers;
+extern	lock_t		dm_reg_lock;		/* lock for registration list */
+
+/*
+ *  Kernel only prototypes.
+ */
+
+int		dm_find_session_and_lock(
+			dm_sessid_t	sid,
+			dm_session_t	**sessionpp,
+			unsigned long	*lcp);
+
+int		dm_find_msg_and_lock(
+			dm_sessid_t	sid,
+			dm_token_t	token,
+			dm_tokevent_t	**tevpp,
+			unsigned long	*lcp);
+
+dm_tokevent_t * dm_evt_create_tevp(
+			dm_eventtype_t	event,
+			int		variable_size,
+			void		**msgpp);
+
+int		dm_app_get_tdp(
+			dm_sessid_t	sid,
+			void		*hanp,
+			size_t		hlen,
+			dm_token_t	token,
+			short		types,
+			dm_right_t	right,
+			dm_tokdata_t	**tdpp);
+
+int		dm_get_config_tdp(
+			void		*hanp,
+			size_t		hlen,
+			dm_tokdata_t	**tdpp);
+
+void		dm_app_put_tdp(
+			dm_tokdata_t	*tdp);
+
+void		dm_put_tevp(
+			dm_tokevent_t	*tevp,
+			dm_tokdata_t	*tdp);
+
+void		dm_evt_rele_tevp(
+			dm_tokevent_t	*tevp,
+			int		droprights);
+
+int		dm_enqueue_normal_event(
+			vfs_t		*vfsp,
+			dm_tokevent_t	*tevp,
+			int		flags);
+
+int		dm_enqueue_mount_event(
+			vfs_t		*vfsp,
+			dm_tokevent_t	*tevp);
+
+int		dm_enqueue_sendmsg_event(
+			dm_sessid_t	targetsid,
+			dm_tokevent_t	*tevp,
+			int		synch);
+
+int		dm_enqueue_user_event(
+			dm_sessid_t	sid,
+			dm_tokevent_t	*tevp,
+			dm_token_t	*tokenp);
+
+int		dm_obj_ref_query_rvp(
+			dm_sessid_t	sid,
+			dm_token_t	token,
+			void		*hanp,
+			size_t		hlen,
+			int		*rvp);
+
+int		dm_read_invis_rvp(
+			dm_sessid_t	sid,
+			void		*hanp,
+			size_t		hlen,
+			dm_token_t	token,
+			dm_off_t	off,
+			dm_size_t	len,
+			void		*bufp,
+			int		*rvp);
+
+int		dm_write_invis_rvp(
+			dm_sessid_t	sid,
+			void		*hanp,
+			size_t		hlen,
+			dm_token_t	token,
+			int		flags,
+			dm_off_t	off,
+			dm_size_t	len,
+			void		*bufp,
+			int		*rvp);
+
+int		dm_get_bulkattr_rvp(
+			dm_sessid_t	sid,
+			void		*hanp,
+			size_t		hlen,
+			dm_token_t	token,
+			u_int		mask,
+			dm_attrloc_t	*locp,
+			size_t		buflen,
+			void		*bufp,
+			size_t		*rlenp,
+			int		*rvp);
+
+int		dm_get_bulkall_rvp(
+			dm_sessid_t	sid,
+			void		*hanp,
+			size_t		hlen,
+			dm_token_t	token,
+			u_int		mask,
+			dm_attrname_t	*attrnamep,
+			dm_attrloc_t	*locp,
+			size_t		buflen,
+			void		*bufp,
+			size_t		*rlenp,
+			int		*rvp);
+
+int		dm_get_dirattrs_rvp(
+			dm_sessid_t	sid,
+			void		*hanp,
+			size_t		hlen,
+			dm_token_t	token,
+			u_int		mask,
+			dm_attrloc_t	*locp,
+			size_t		buflen,
+			void		*bufp,
+			size_t		*rlenp,
+			int		*rvp);
+
+int		dm_get_allocinfo_rvp(
+			dm_sessid_t	sid,
+			void		*hanp,
+			size_t		hlen,
+			dm_token_t	token,
+			dm_off_t	*offp,
+			u_int		nelem,
+			dm_extent_t	*extentp,
+			u_int		*nelemp,
+			int		*rvp);
+
+int		dm_waitfor_destroy_attrname(
+			vfs_t		*vfsp,
+			dm_attrname_t	*attrnamep);
+
+void		dm_clear_fsreg(
+			dm_session_t	*s);
+
+int		dm_add_fsys_entry(
+			vfs_t		*vfsp,
+			dm_tokevent_t	*tevp);
+
+void		dm_change_fsys_entry(
+			vfs_t		*vfsp,
+			dm_fsstate_t	newstate);
+
+void		dm_remove_fsys_entry(
+			vfs_t		*vfsp);
+
+dm_fsys_vector_t *dm_fsys_vector(
+			vnode_t		*vp);
+
+void		dm_fsys_vector_free(void);
+
+int		dm_waitfor_disp_session(
+			vfs_t		*vfsp,
+			dm_tokevent_t	*tevp,
+			dm_session_t	**sessionpp,
+			unsigned long	*lcp);
+
+vnode_t *	dm_handle_to_vp (
+			xfs_handle_t	*handlep,
+			short		*typep);
+
+int		dm_check_dmapi_vp(
+			vnode_t		*vp);
+
+dm_tokevent_t * dm_find_mount_tevp_and_lock(
+			xfs_fsid_t	*fsidp,
+			unsigned long	*lcp);
+
+int		dm_path_to_hdl(
+			char		*path,
+			void		*hanp,
+			size_t		*hlenp);
+
+int		dm_path_to_fshdl(
+			char		*path,
+			void		*hanp,
+			size_t		*hlenp);
+
+int		dm_fd_to_hdl(
+			int		fd,
+			void		*hanp,
+			size_t		*hlenp);
+
+int		dm_upgrade_right(
+			dm_sessid_t	sid,
+			void		*hanp,
+			size_t		hlen,
+			dm_token_t	token);
+
+int		dm_downgrade_right(
+			dm_sessid_t	sid,
+			void		*hanp,
+			size_t		hlen,
+			dm_token_t	token);
+
+int		dm_request_right(
+			dm_sessid_t	sid,
+			void		*hanp,
+			size_t		hlen,
+			dm_token_t	token,
+			u_int		flags,
+			dm_right_t	right);
+
+int		dm_release_right(
+			dm_sessid_t	sid,
+			void		*hanp,
+			size_t		hlen,
+			dm_token_t	token);
+
+int		dm_query_right(
+			dm_sessid_t	sid,
+			void		*hanp,
+			size_t		hlen,
+			dm_token_t	token,
+			dm_right_t	*rightp);
+
+
+int		dm_set_eventlist(
+			dm_sessid_t	sid,
+			void		*hanp,
+			size_t		hlen,
+			dm_token_t	token,
+			dm_eventset_t	*eventsetp,
+			u_int		maxevent);
+
+int		dm_obj_ref_hold(
+			dm_sessid_t	sid,
+			dm_token_t	token,
+			void		*hanp,
+			size_t		hlen);
+
+int		dm_obj_ref_rele(
+			dm_sessid_t	sid,
+			dm_token_t	token,
+			void		*hanp,
+			size_t		hlen);
+
+int		dm_get_eventlist(
+			dm_sessid_t	sid,
+			void		*hanp,
+			size_t		hlen,
+			dm_token_t	token,
+			u_int		nelem,
+			dm_eventset_t	*eventsetp,
+			u_int		*nelemp);
+
+
+int		dm_set_disp(
+			dm_sessid_t	sid,
+			void		*hanp,
+			size_t		hlen,
+			dm_token_t	token,
+			dm_eventset_t	*eventsetp,
+			u_int		maxevent);
+
+
+int		dm_set_return_on_destroy(
+			dm_sessid_t	sid,
+			void		*hanp,
+			size_t		hlen,
+			dm_token_t	token,
+			dm_attrname_t	*attrnamep,
+			dm_boolean_t	enable);
+
+
+int		dm_get_mountinfo(
+			dm_sessid_t	sid,
+			void		*hanp,
+			size_t		hlen,
+			dm_token_t	token,
+			size_t		buflen,
+			void		*bufp,
+			size_t		*rlenp);
+
+void		dm_link_event(
+			dm_tokevent_t	*tevp,
+			dm_eventq_t	*queue);
+
+void		dm_unlink_event(
+			dm_tokevent_t	*tevp,
+			dm_eventq_t	*queue);
+
+int		dm_open_by_handle_rvp(
+			unsigned int	fd,
+			void		*hanp,
+			size_t		hlen,
+			int		mode,
+			int		*rvp);
+
+int		dm_copyin_handle(
+			void		*hanp,
+			size_t		hlen,
+			xfs_handle_t	*handlep);
+
+#endif	/* _DMAPI_PRIVATE_H */
diff -uNrp linux-2.6.5/fs/xfs/dmapi/dmapi_region.c linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_region.c
--- linux-2.6.5/fs/xfs/dmapi/dmapi_region.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_region.c	2004-04-05 00:58:15.000000000 +0200
@@ -0,0 +1,92 @@
+/*
+ * Copyright (c) 2000 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.	 Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+#include "xfs.h"
+#include "dmapi.h"
+#include "dmapi_kern.h"
+#include "dmapi_private.h"
+
+
+int
+dm_get_region(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	u_int		nelem,
+	dm_region_t	*regbufp,
+	u_int		*nelemp)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_REG,
+		DM_RIGHT_SHARED, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->get_region(tdp->td_vp, tdp->td_right,
+		nelem, regbufp, nelemp);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+
+int
+dm_set_region(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	u_int		nelem,
+	dm_region_t	*regbufp,
+	dm_boolean_t	*exactflagp)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_REG,
+		DM_RIGHT_EXCL, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->set_region(tdp->td_vp, tdp->td_right,
+		nelem, regbufp, exactflagp);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
diff -uNrp linux-2.6.5/fs/xfs/dmapi/dmapi_register.c linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_register.c
--- linux-2.6.5/fs/xfs/dmapi/dmapi_register.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_register.c	2004-04-05 00:58:15.000000000 +0200
@@ -0,0 +1,1540 @@
+/*
+ * Copyright (c) 2000 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.	 Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+
+#include <linux/version.h>
+#include <linux/mm.h>
+#include <linux/proc_fs.h>
+#include <linux/module.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,0)
+#include <linux/mount.h>
+#include <linux/namei.h>
+#endif
+#include "xfs.h"
+#include "dmapi.h"
+#include "dmapi_kern.h"
+#include "dmapi_private.h"
+#include "xfs_sb.h"
+
+/* LOOKUP_POSTIVE was removed in Linux 2.6 */
+#ifndef LOOKUP_POSITIVE
+#define LOOKUP_POSITIVE	0
+#endif
+
+dm_fsreg_t	*dm_registers;	/* head of filesystem registration list */
+int		dm_fsys_cnt;	/* number of filesystems on dm_registers list */
+lock_t		dm_reg_lock = SPIN_LOCK_UNLOCKED;/* lock for dm_registers */
+
+
+
+#ifdef CONFIG_PROC_FS
+static int
+fsreg_read_pfs(char *buffer, char **start, off_t offset,
+		 int count, int *eof, void *data)
+{
+	int len;
+	int i;
+	dm_fsreg_t	*fsrp = (dm_fsreg_t*)data;
+	char		statebuf[30];
+
+#define CHKFULL if(len >= count) break;
+#define ADDBUF(a,b)	len += sprintf(buffer + len, a, b); CHKFULL;
+
+	switch (fsrp->fr_state) {
+	case DM_STATE_MOUNTING:		sprintf(statebuf, "mounting"); break;
+	case DM_STATE_MOUNTED:		sprintf(statebuf, "mounted"); break;
+	case DM_STATE_UNMOUNTING:	sprintf(statebuf, "unmounting"); break;
+	case DM_STATE_UNMOUNTED:	sprintf(statebuf, "unmounted"); break;
+	default:
+		sprintf(statebuf, "unknown:%d", (int)fsrp->fr_state);
+		break;
+	}
+
+	len=0;
+	while(1){
+		ADDBUF("fsrp=0x%p\n", fsrp);
+		ADDBUF("fr_next=0x%p\n", fsrp->fr_next);
+		ADDBUF("fr_vfsp=0x%p\n", fsrp->fr_vfsp);
+		ADDBUF("fr_tevp=0x%p\n", fsrp->fr_tevp);
+		ADDBUF("fr_fsid=%c\n", '?');
+		ADDBUF("fr_msg=0x%p\n", fsrp->fr_msg);
+		ADDBUF("fr_msgsize=%d\n", fsrp->fr_msgsize);
+		ADDBUF("fr_state=%s\n", statebuf);
+		ADDBUF("fr_dispq=%c\n", '?');
+		ADDBUF("fr_dispcnt=%d\n", fsrp->fr_dispcnt);
+
+		ADDBUF("fr_evt_dispq.eq_head=0x%p\n", fsrp->fr_evt_dispq.eq_head);
+		ADDBUF("fr_evt_dispq.eq_tail=0x%p\n", fsrp->fr_evt_dispq.eq_tail);
+		ADDBUF("fr_evt_dispq.eq_count=%d\n", fsrp->fr_evt_dispq.eq_count);
+
+		ADDBUF("fr_queue=%c\n", '?');
+		ADDBUF("fr_lock=%c\n", '?');
+		ADDBUF("fr_hdlcnt=%d\n", fsrp->fr_hdlcnt);
+		ADDBUF("fr_vfscnt=%d\n", fsrp->fr_vfscnt);
+		ADDBUF("fr_unmount=%d\n", fsrp->fr_unmount);
+
+		len += sprintf(buffer + len, "fr_rattr=");
+		CHKFULL;
+		for(i = 0; i <= DM_ATTR_NAME_SIZE; ++i){
+			ADDBUF("%c", fsrp->fr_rattr.an_chars[i]);
+		}
+		CHKFULL;
+		len += sprintf(buffer + len, "\n");
+		CHKFULL;
+
+		for(i = 0; i < DM_EVENT_MAX; i++){
+			if( fsrp->fr_sessp[i] != NULL ){
+				ADDBUF("fr_sessp[%d]=", i);
+				ADDBUF("0x%p\n", fsrp->fr_sessp[i]);
+			}
+		}
+		CHKFULL;
+
+		break;
+	}
+
+	if (offset >= len) {
+		*start = buffer;
+		*eof = 1;
+		return 0;
+	}
+	*start = buffer + offset;
+	if ((len -= offset) > count)
+		return count;
+	*eof = 1;
+
+	return len;
+}
+#endif
+
+
+/* Returns a pointer to the filesystem structure for the filesystem
+   referenced by vfsp.	The caller is responsible for obtaining dm_reg_lock
+   before calling this routine.
+*/
+
+static dm_fsreg_t *
+dm_find_fsreg(
+	xfs_fsid_t	*fsidp)
+{
+	dm_fsreg_t	*fsrp;
+
+	for (fsrp = dm_registers; fsrp; fsrp = fsrp->fr_next) {
+		if (!memcmp(&fsrp->fr_fsid, fsidp, sizeof(*fsidp)))
+			break;
+	}
+	return(fsrp);
+}
+
+
+/* Given a fsid_t, dm_find_fsreg_and_lock() finds the dm_fsreg_t structure
+   for that filesytem if one exists, and returns a pointer to the structure
+   after obtaining its 'fr_lock' so that the caller can safely modify the
+   dm_fsreg_t.	The caller is responsible for releasing 'fr_lock'.
+*/
+
+static dm_fsreg_t *
+dm_find_fsreg_and_lock(
+	xfs_fsid_t	*fsidp,
+	unsigned long	*lcp)		/* address of returned lock cookie */
+{
+	dm_fsreg_t	*fsrp;
+
+	for (;;) {
+		*lcp = mutex_spinlock(&dm_reg_lock);
+
+		if ((fsrp = dm_find_fsreg(fsidp)) == NULL) {
+			mutex_spinunlock(&dm_reg_lock, *lcp);
+			return(NULL);
+		}
+		if (spin_trylock(&fsrp->fr_lock)) {
+			nested_spinunlock(&dm_reg_lock);
+			return(fsrp);	/* success */
+		}
+
+		/* If the second lock is not available, drop the first and
+		   start over.	This gives the CPU a chance to process any
+		   interrupts, and also allows processes which want a fr_lock
+		   for a different filesystem to proceed.
+		*/
+
+		mutex_spinunlock(&dm_reg_lock, *lcp);
+	}
+}
+
+
+/* dm_add_fsys_entry() is called when a DM_EVENT_MOUNT event is about to be
+   sent.  It creates a dm_fsreg_t structure for the filesystem and stores a
+   pointer to a copy of the mount event within that structure so that it is
+   available for subsequent dm_get_mountinfo() calls.
+*/
+
+int
+dm_add_fsys_entry(
+	vfs_t		*vfsp,
+	dm_tokevent_t	*tevp)
+{
+	dm_fsreg_t	*fsrp;
+	int		msgsize;
+	void		*msg;
+	unsigned long	lc;			/* lock cookie */
+
+	/* Allocate and initialize a dm_fsreg_t structure for the filesystem. */
+
+	msgsize = tevp->te_allocsize - offsetof(dm_tokevent_t, te_event);
+	msg = kmalloc(msgsize, GFP_KERNEL);
+	if (msg == NULL) {
+		printk("%s/%d: kmalloc returned NULL\n", __FUNCTION__, __LINE__);
+		return ENOMEM;
+	}
+	memcpy(msg, &tevp->te_event, msgsize);
+
+	fsrp = kmem_cache_alloc(dm_fsreg_cachep, SLAB_KERNEL);
+	if (fsrp == NULL) {
+		kfree(msg);
+		printk("%s/%d: kmem_cache_alloc(dm_fsreg_cachep) returned NULL\n", __FUNCTION__, __LINE__);
+		return ENOMEM;
+	}
+	memset(fsrp, 0, sizeof(*fsrp));
+
+	fsrp->fr_vfsp = vfsp;
+	fsrp->fr_tevp = tevp;
+	fsrp->fr_fsid = *vfsp->vfs_altfsid;
+	fsrp->fr_msg = msg;
+	fsrp->fr_msgsize = msgsize;
+	fsrp->fr_state = DM_STATE_MOUNTING;
+	sv_init(&fsrp->fr_dispq, SV_DEFAULT, "fr_dispq");
+	sv_init(&fsrp->fr_queue, SV_DEFAULT, "fr_queue");
+	spinlock_init(&fsrp->fr_lock, "fr_lock");
+
+	/* If no other mounted DMAPI filesystem already has this same
+	   fsid_t, then add this filesystem to the list.
+	*/
+
+	lc = mutex_spinlock(&dm_reg_lock);
+
+	if (!dm_find_fsreg(vfsp->vfs_altfsid)) {
+		fsrp->fr_next = dm_registers;
+		dm_registers = fsrp;
+		dm_fsys_cnt++;
+#ifdef CONFIG_PROC_FS
+		{
+		char buf[100];
+		struct proc_dir_entry *entry;
+
+		sprintf(buf, DMAPI_DBG_PROCFS "/fsreg/0x%p", fsrp);
+		entry = create_proc_read_entry(buf, 0, 0, fsreg_read_pfs, fsrp);
+		entry->owner = THIS_MODULE;
+		}
+#endif
+		mutex_spinunlock(&dm_reg_lock, lc);
+		return(0);
+	}
+
+	/* A fsid_t collision occurred, so prevent this new filesystem from
+	   mounting.
+	*/
+
+	mutex_spinunlock(&dm_reg_lock, lc);
+
+	sv_destroy(&fsrp->fr_dispq);
+	sv_destroy(&fsrp->fr_queue);
+	spinlock_destroy(&fsrp->fr_lock);
+	kfree(msg);
+	kmem_cache_free(dm_fsreg_cachep, fsrp);
+	return(EBUSY);
+}
+
+
+/* dm_change_fsys_entry() is called whenever a filesystem's mount state is
+   about to change.  The state is changed to DM_STATE_MOUNTED after a
+   successful DM_EVENT_MOUNT event or after a failed unmount.  It is changed
+   to DM_STATE_UNMOUNTING after a successful DM_EVENT_PREUNMOUNT event.
+   Finally, the state is changed to DM_STATE_UNMOUNTED after a successful
+   unmount.  It stays in this state until the DM_EVENT_UNMOUNT event is
+   queued, at which point the filesystem entry is removed.
+*/
+
+void
+dm_change_fsys_entry(
+	vfs_t		*vfsp,
+	dm_fsstate_t	newstate)
+{
+	dm_fsreg_t	*fsrp;
+	int		seq_error;
+	unsigned long	lc;			/* lock cookie */
+
+	/* Find the filesystem referenced by the vfsp's fsid_t.	 This should
+	   always succeed.
+	*/
+
+	if ((fsrp = dm_find_fsreg_and_lock(vfsp->vfs_altfsid, &lc)) == NULL) {
+		panic("dm_change_fsys_entry: can't find DMAPI fsrp for "
+			"vfsp %p\n", vfsp);
+	}
+
+	/* Make sure that the new state is acceptable given the current state
+	   of the filesystem.  Any error here is a major DMAPI/filesystem
+	   screwup.
+	*/
+
+	seq_error = 0;
+	switch (newstate) {
+	case DM_STATE_MOUNTED:
+		if (fsrp->fr_state != DM_STATE_MOUNTING &&
+		    fsrp->fr_state != DM_STATE_UNMOUNTING) {
+			seq_error++;
+		}
+		break;
+	case DM_STATE_UNMOUNTING:
+		if (fsrp->fr_state != DM_STATE_MOUNTED)
+			seq_error++;
+		break;
+	case DM_STATE_UNMOUNTED:
+		if (fsrp->fr_state != DM_STATE_UNMOUNTING)
+			seq_error++;
+		break;
+	default:
+		seq_error++;
+		break;
+	}
+	if (seq_error) {
+		panic("dm_change_fsys_entry: DMAPI sequence error: old state "
+			"%d, new state %d, fsrp %p\n", fsrp->fr_state,
+			newstate, fsrp);
+	}
+
+	/* If the old state was DM_STATE_UNMOUNTING, then processes could be
+	   sleeping in dm_handle_to_vp() waiting for their DM_NO_TOKEN handles
+	   to be translated to vnodes.	Wake them up so that they either
+	   continue (new state is DM_STATE_MOUNTED) or fail (new state is
+	   DM_STATE_UNMOUNTED).
+	*/
+
+	if (fsrp->fr_state == DM_STATE_UNMOUNTING) {
+		if (fsrp->fr_hdlcnt)
+			sv_broadcast(&fsrp->fr_queue);
+	}
+
+	/* Change the filesystem's mount state to its new value. */
+
+	fsrp->fr_state = newstate;
+	fsrp->fr_tevp = NULL;		/* not valid after DM_STATE_MOUNTING */
+
+	/* If the new state is DM_STATE_UNMOUNTING, wait until any application
+	   threads currently in the process of making VFS_VGET and VFS_ROOT
+	   calls are done before we let this unmount thread continue the
+	   unmount.  (We want to make sure that the unmount will see these
+	   vnode references during its scan.)
+	*/
+
+	if (newstate == DM_STATE_UNMOUNTING) {
+		while (fsrp->fr_vfscnt) {
+			fsrp->fr_unmount++;
+			sv_wait(&fsrp->fr_queue, 1, &fsrp->fr_lock, lc);
+			lc = mutex_spinlock(&fsrp->fr_lock);
+			fsrp->fr_unmount--;
+		}
+	}
+
+	mutex_spinunlock(&fsrp->fr_lock, lc);
+}
+
+
+/* dm_remove_fsys_entry() gets called after a failed mount or after an
+   DM_EVENT_UNMOUNT event has been queued.  (The filesystem entry must stay
+   until the DM_EVENT_UNMOUNT reply is queued so that the event can use the
+   'fr_sessp' list to see which session to send the event to.)
+*/
+
+void
+dm_remove_fsys_entry(
+	vfs_t		*vfsp)
+{
+	dm_fsreg_t	**fsrpp;
+	dm_fsreg_t	*fsrp;
+	unsigned long	lc;			/* lock cookie */
+
+	/* Find the filesystem referenced by the vfsp's fsid_t and dequeue
+	   it after verifying that the fr_state shows a filesystem that is
+	   either mounting or unmounted.
+	*/
+
+	lc = mutex_spinlock(&dm_reg_lock);
+
+	fsrpp = &dm_registers;
+	while ((fsrp = *fsrpp) != NULL) {
+		if (!memcmp(&fsrp->fr_fsid, vfsp->vfs_altfsid, sizeof(fsrp->fr_fsid)))
+			break;
+		fsrpp = &fsrp->fr_next;
+	}
+	if (fsrp == NULL) {
+		mutex_spinunlock(&dm_reg_lock, lc);
+		panic("dm_remove_fsys_entry: can't find DMAPI fsrp for "
+			"vfsp %p\n", vfsp);
+	}
+
+	nested_spinlock(&fsrp->fr_lock);
+
+	/* Verify that it makes sense to remove this entry. */
+
+	if (fsrp->fr_state != DM_STATE_MOUNTING &&
+	    fsrp->fr_state != DM_STATE_UNMOUNTED) {
+		nested_spinunlock(&fsrp->fr_lock);
+		mutex_spinunlock(&dm_reg_lock, lc);
+		panic("dm_remove_fsys_entry: DMAPI sequence error: old state "
+			"%d, fsrp %p\n", fsrp->fr_state, fsrp);
+	}
+
+	*fsrpp = fsrp->fr_next;
+	dm_fsys_cnt--;
+
+	nested_spinunlock(&dm_reg_lock);
+
+	/* Since the filesystem is about to finish unmounting, we must be sure
+	   that no vnodes are being referenced within the filesystem before we
+	   let this event thread continue.  If the filesystem is currently in
+	   state DM_STATE_MOUNTING, then we know by definition that there can't
+	   be any references.  If the filesystem is DM_STATE_UNMOUNTED, then
+	   any application threads referencing handles with DM_NO_TOKEN should
+	   have already been awakened by dm_change_fsys_entry and should be
+	   long gone by now.  Just in case they haven't yet left, sleep here
+	   until they are really gone.
+	*/
+
+	while (fsrp->fr_hdlcnt) {
+		fsrp->fr_unmount++;
+		sv_wait(&fsrp->fr_queue, 1, &fsrp->fr_lock, lc);
+		lc = mutex_spinlock(&fsrp->fr_lock);
+		fsrp->fr_unmount--;
+	}
+	mutex_spinunlock(&fsrp->fr_lock, lc);
+
+	/* Release all memory. */
+
+#ifdef CONFIG_PROC_FS
+	{
+	char buf[100];
+	sprintf(buf, DMAPI_DBG_PROCFS "/fsreg/0x%p", fsrp);
+	remove_proc_entry(buf, NULL);
+	}
+#endif
+	sv_destroy(&fsrp->fr_dispq);
+	sv_destroy(&fsrp->fr_queue);
+	spinlock_destroy(&fsrp->fr_lock);
+	kfree(fsrp->fr_msg);
+	kmem_cache_free(dm_fsreg_cachep, fsrp);
+}
+
+
+/* Get a vnode for the object referenced by handlep.  We cannot use
+   altgetvfs() because it fails if the VFS_OFFLINE bit is set, which means
+   that any call to dm_handle_to_vp() while a umount is in progress would
+   return an error, even if the umount can't possibly succeed because users
+   are in the filesystem.  The requests would start to fail as soon as the
+   umount begins, even before the application receives the DM_EVENT_PREUNMOUNT
+   event.
+
+   dm_handle_to_vp() emulates the behavior of lookup() while an unmount is
+   in progress.	 Any call to dm_handle_to_vp() while the filesystem is in the
+   DM_STATE_UNMOUNTING state will block.  If the unmount eventually succeeds,
+   the requests will wake up and fail.	If the unmount fails, the requests will
+   wake up and complete normally.
+
+   While a filesystem is in state DM_STATE_MOUNTING, dm_handle_to_vp() will
+   fail all requests.  Per the DMAPI spec, the only handles in the filesystem
+   which are valid during a mount event are the handles within the event
+   itself.
+*/
+
+vnode_t *
+dm_handle_to_vp(
+	xfs_handle_t	*handlep,
+	short		*typep)
+{
+	dm_fsreg_t	*fsrp;
+	vnode_t		*vp;
+	short		type;
+	unsigned long	lc;			/* lock cookie */
+	int		error;
+	fid_t		*fidp;
+
+	if ((fsrp = dm_find_fsreg_and_lock(&handlep->ha_fsid, &lc)) == NULL)
+		return(NULL);
+
+	fidp = (fid_t*)&handlep->ha_fid;
+	/* If mounting, and we are not asking for a filesystem handle,
+	 * then fail the request.  (fid_len==0 for fshandle)
+	 */
+	if ((fsrp->fr_state == DM_STATE_MOUNTING) &&
+	    (fidp->fid_len != 0)) {
+		mutex_spinunlock(&fsrp->fr_lock, lc);
+		return(NULL);
+	}
+
+	for (;;) {
+		if (fsrp->fr_state == DM_STATE_MOUNTING)
+			break;
+		if (fsrp->fr_state == DM_STATE_MOUNTED)
+			break;
+		if (fsrp->fr_state == DM_STATE_UNMOUNTED) {
+			if (fsrp->fr_unmount && fsrp->fr_hdlcnt == 0)
+				sv_broadcast(&fsrp->fr_queue);
+			mutex_spinunlock(&fsrp->fr_lock, lc);
+			return(NULL);
+		}
+
+		/* Must be DM_STATE_UNMOUNTING. */
+
+		fsrp->fr_hdlcnt++;
+		sv_wait(&fsrp->fr_queue, 1, &fsrp->fr_lock, lc);
+		lc = mutex_spinlock(&fsrp->fr_lock);
+		fsrp->fr_hdlcnt--;
+	}
+
+	fsrp->fr_vfscnt++;
+	mutex_spinunlock(&fsrp->fr_lock, lc);
+
+	/* Now that the mutex is released, wait until we have access to the
+	   vnode.
+	*/
+
+	if (fidp->fid_len == 0) {	/* filesystem handle */
+		VFS_ROOT(fsrp->fr_vfsp, &vp, error);
+	} else {				/* file object handle */
+		VFS_VGET(fsrp->fr_vfsp, &vp, fidp, error);
+	}
+
+	lc = mutex_spinlock(&fsrp->fr_lock);
+
+	fsrp->fr_vfscnt--;
+	if (fsrp->fr_unmount && fsrp->fr_vfscnt == 0)
+		sv_broadcast(&fsrp->fr_queue);
+
+	mutex_spinunlock(&fsrp->fr_lock, lc);
+	if (error || vp == NULL)
+		return(NULL);
+
+	if (fidp->fid_len == 0) {
+		type = DM_TDT_VFS;
+	} else if (vp->v_type == VREG) {
+		type = DM_TDT_REG;
+	} else if (vp->v_type == VDIR) {
+		type = DM_TDT_DIR;
+	} else if (vp->v_type == VLNK) {
+		type = DM_TDT_LNK;
+	} else {
+		type = DM_TDT_OTH;
+	}
+	*typep = type;
+	return(vp);
+}
+
+
+int
+dm_vp_to_handle(
+	vnode_t		*vp,
+	xfs_handle_t	*handlep)
+{
+	int		error;
+	struct	fid	fid;
+	int		hsize;
+
+	if (vp->v_vfsp->vfs_altfsid == NULL)
+		return(EINVAL);
+
+	VOP_FID2(vp, &fid, error);
+	if (error)
+		return(error);
+
+	memcpy(&handlep->ha_fsid, vp->v_vfsp->vfs_altfsid, sizeof(xfs_fsid_t));
+	memcpy(&handlep->ha_fid, &fid, fid.fid_len + sizeof fid.fid_len);
+	hsize = XFS_HSIZE(*handlep);
+	memset((char *)handlep + hsize, 0, sizeof(*handlep) - hsize);
+	return(0);
+}
+
+
+/* Given a vnode, check if that vnode resides in filesystem that supports
+   DMAPI.  Returns zero if the vnode is in a DMAPI filesystem, otherwise
+   returns an errno.
+*/
+
+int
+dm_check_dmapi_vp(
+	vnode_t		*vp)
+{
+	xfs_handle_t	handle;
+	/* REFERENCED */
+	dm_fsreg_t	*fsrp;
+	int		error;
+	unsigned long	lc;			/* lock cookie */
+
+	if ((error = dm_vp_to_handle(vp, &handle)) != 0)
+		return(error);
+
+	if ((fsrp = dm_find_fsreg_and_lock(&handle.ha_fsid, &lc)) == NULL)
+		return(EBADF);
+	mutex_spinunlock(&fsrp->fr_lock, lc);
+	return(0);
+}
+
+
+/* Return a pointer to the DM_EVENT_MOUNT event while a mount is still in
+   progress.  This is only called by dm_get_config and dm_get_config_events
+   which need to access the filesystem during a mount but which don't have
+   a session and token to use.
+*/
+
+dm_tokevent_t *
+dm_find_mount_tevp_and_lock(
+	xfs_fsid_t		*fsidp,
+	unsigned long	*lcp)		/* address of returned lock cookie */
+{
+	dm_fsreg_t	*fsrp;
+
+	if ((fsrp = dm_find_fsreg_and_lock(fsidp, lcp)) == NULL)
+		return(NULL);
+
+	if (!fsrp->fr_tevp || fsrp->fr_state != DM_STATE_MOUNTING) {
+		mutex_spinunlock(&fsrp->fr_lock, *lcp);
+		return(NULL);
+	}
+	nested_spinlock(&fsrp->fr_tevp->te_lock);
+	nested_spinunlock(&fsrp->fr_lock);
+	return(fsrp->fr_tevp);
+}
+
+
+/* Wait interruptibly until a session registers disposition for 'event' in
+   filesystem 'vfsp'.  Upon successful exit, both the filesystem's dm_fsreg_t
+   structure and the session's dm_session_t structure are locked.  The caller
+   is responsible for unlocking both structures using the returned cookies.
+
+   Warning: The locks can be dropped in any order, but the 'lc2p' cookie MUST
+   BE USED FOR THE FIRST UNLOCK, and the lc1p cookie must be used for the
+   second unlock.  If this is not done, the CPU will be interruptible while
+   holding a mutex, which could deadlock the machine!
+*/
+
+static int
+dm_waitfor_disp(
+	vfs_t		*vfsp,
+	dm_tokevent_t	*tevp,
+	dm_fsreg_t	**fsrpp,
+	unsigned long	*lc1p,		/* addr of first returned lock cookie */
+	dm_session_t	**sessionpp,
+	unsigned long	*lc2p)		/* addr of 2nd returned lock cookie */
+{
+	dm_eventtype_t	event = tevp->te_msg.ev_type;
+	dm_session_t	*s;
+	dm_fsreg_t	*fsrp;
+
+	if ((fsrp = dm_find_fsreg_and_lock(vfsp->vfs_altfsid, lc1p)) == NULL)
+		return(ENOENT);
+
+	/* If no session is registered for this event in the specified
+	   filesystem, then sleep interruptibly until one does.
+	*/
+
+	for (;;) {
+		int	rc = 0;
+
+		/* The dm_find_session_and_lock() call is needed because a
+		   session that is in the process of being removed might still
+		   be in the dm_fsreg_t structure but won't be in the
+		   dm_sessions list.
+		*/
+
+		if ((s = fsrp->fr_sessp[event]) != NULL &&
+		    dm_find_session_and_lock(s->sn_sessid, &s, lc2p) == 0) {
+			break;
+		}
+
+		/* Noone is currently registered.  DM_EVENT_UNMOUNT events
+		   don't wait for anyone to register because the unmount is
+		   already past the point of no return.
+		*/
+
+		if (event == DM_EVENT_UNMOUNT) {
+			mutex_spinunlock(&fsrp->fr_lock, *lc1p);
+			return(ENOENT);
+		}
+
+		/* Wait until a session registers for disposition of this
+		   event.
+		*/
+
+		fsrp->fr_dispcnt++;
+		dm_link_event(tevp, &fsrp->fr_evt_dispq);
+
+		sv_wait_sig(&fsrp->fr_dispq, 1, &fsrp->fr_lock, *lc1p);
+		rc = signal_pending(current);
+
+		*lc1p = mutex_spinlock(&fsrp->fr_lock);
+		fsrp->fr_dispcnt--;
+		dm_unlink_event(tevp, &fsrp->fr_evt_dispq);
+		if (rc) {		/* if signal was received */
+			mutex_spinunlock(&fsrp->fr_lock, *lc1p);
+			return(EINTR);
+		}
+	}
+	*sessionpp = s;
+	*fsrpp = fsrp;
+	return(0);
+}
+
+
+/* Returns the session pointer for the session registered for an event
+   in the given vfsp.  If successful, the session is locked upon return.  The
+   caller is responsible for releasing the lock.  If no session is currently
+   registered for the event, dm_waitfor_disp_session() will sleep interruptibly
+   until a registration occurs.
+*/
+
+int
+dm_waitfor_disp_session(
+	vfs_t		*vfsp,
+	dm_tokevent_t	*tevp,
+	dm_session_t	**sessionpp,
+	unsigned long	*lcp)
+{
+	dm_fsreg_t	*fsrp;
+	unsigned long	lc2;
+	int		error;
+
+	if (tevp->te_msg.ev_type < 0 || tevp->te_msg.ev_type > DM_EVENT_MAX)
+		return(EIO);
+
+	error = dm_waitfor_disp(vfsp, tevp, &fsrp, lcp, sessionpp, &lc2);
+	if (!error)
+		mutex_spinunlock(&fsrp->fr_lock, lc2);	/* rev. cookie order*/
+	return(error);
+}
+
+
+/* Find the session registered for the DM_EVENT_DESTROY event on the specified
+   filesystem, sleeping if necessary until registration occurs.	 Once found,
+   copy the session's return-on-destroy attribute name, if any, back to the
+   caller.
+*/
+
+int
+dm_waitfor_destroy_attrname(
+	vfs_t		*vfsp,
+	dm_attrname_t	*attrnamep)
+{
+	dm_tokevent_t	*tevp;
+	dm_session_t	*s;
+	dm_fsreg_t	*fsrp;
+	int		error;
+	unsigned long	lc1;		/* first lock cookie */
+	unsigned long	lc2;		/* second lock cookie */
+	void		*msgp;
+
+	tevp = dm_evt_create_tevp(DM_EVENT_DESTROY, 1, (void**)&msgp);
+	error = dm_waitfor_disp(vfsp, tevp, &fsrp, &lc1, &s, &lc2);
+	if (!error) {
+		*attrnamep = fsrp->fr_rattr;		/* attribute or zeros */
+		mutex_spinunlock(&s->sn_qlock, lc2);	/* rev. cookie order */
+		mutex_spinunlock(&fsrp->fr_lock, lc1);
+	}
+	dm_evt_rele_tevp(tevp,0);
+	return(error);
+}
+
+
+/* Unregisters the session for the disposition of all events on all
+   filesystems.	 This routine is not called until the session has been
+   dequeued from the session list and its session lock has been dropped,
+   but before the actual structure is freed, so it is safe to grab the
+   'dm_reg_lock' here.	If dm_waitfor_disp_session() happens to be called
+   by another thread, it won't find this session on the session list and
+   will wait until a new session registers.
+*/
+
+void
+dm_clear_fsreg(
+	dm_session_t	*s)
+{
+	dm_fsreg_t	*fsrp;
+	int		event;
+	unsigned long	lc;			/* lock cookie */
+
+	lc = mutex_spinlock(&dm_reg_lock);
+
+	for (fsrp = dm_registers; fsrp != NULL; fsrp = fsrp->fr_next) {
+		nested_spinlock(&fsrp->fr_lock);
+		for (event = 0; event < DM_EVENT_MAX; event++) {
+			if (fsrp->fr_sessp[event] != s)
+				continue;
+			fsrp->fr_sessp[event] = NULL;
+			if (event == DM_EVENT_DESTROY)
+				memset(&fsrp->fr_rattr, 0, sizeof(fsrp->fr_rattr));
+		}
+		nested_spinunlock(&fsrp->fr_lock);
+	}
+
+	mutex_spinunlock(&dm_reg_lock, lc);
+}
+
+
+/*
+ *  Return the handle for the object named by path.
+ */
+
+int
+dm_path_to_hdl(
+	char		*path,		/* any path name */
+	void		*hanp,		/* user's data buffer */
+	size_t		*hlenp)		/* set to size of data copied */
+{
+	/* REFERENCED */
+	dm_fsreg_t	*fsrp;
+	xfs_handle_t	handle;
+	vnode_t		*vp;
+	size_t		hlen;
+	int		error;
+	unsigned long	lc;		/* lock cookie */
+	struct nameidata nd;
+	struct inode *inode;
+	size_t		len;
+	char		*name;
+
+	/* XXX get things straightened out so getname() works here? */
+	len = strnlen_user(path, 2000);
+	name = kmalloc(len, GFP_KERNEL);
+	if (name == NULL) {
+		printk("%s/%d: kmalloc returned NULL\n", __FUNCTION__, __LINE__);
+		return(ENOMEM);
+	}
+	if (copy_from_user(name, path, len)) {
+		kfree(name);
+		return(EFAULT);
+	}
+
+	error = path_lookup(name, LOOKUP_POSITIVE, &nd);
+	kfree(name);
+	if (error)
+		return -error; /* path_lookup returned negative error */
+
+	ASSERT(nd.dentry);
+	ASSERT(nd.dentry->d_inode);
+	inode = igrab(nd.dentry->d_inode);
+	path_release(&nd);
+
+	if (inode->i_sb->s_magic != XFS_SB_MAGIC) {
+		/* we're not in XFS anymore, Toto */
+		iput(inode);
+		return EINVAL;
+	}
+
+	/* we need the vnode */
+	vp = LINVFS_GET_VP(inode);
+	error = dm_vp_to_handle(vp, &handle);
+	iput(inode);
+	if (error)
+		return(error);
+
+	if ((fsrp = dm_find_fsreg_and_lock(&handle.ha_fsid, &lc)) == NULL)
+		return(EBADF);
+	mutex_spinunlock(&fsrp->fr_lock, lc);
+
+	hlen = XFS_HSIZE(handle);
+
+	if (copy_to_user(hanp, &handle, (int)hlen))
+		return(EFAULT);
+	if (put_user(hlen,hlenp))
+		return(EFAULT);
+	return 0;
+}
+
+
+/*
+ *  Return the handle for the file system containing the object named by path.
+ */
+
+int
+dm_path_to_fshdl(
+	char		*path,		/* any path name */
+	void		*hanp,		/* user's data buffer */
+	size_t		*hlenp)		/* set to size of data copied */
+{
+	/* REFERENCED */
+	dm_fsreg_t	*fsrp;
+	xfs_handle_t	handle;
+	vnode_t		*vp;
+	size_t		hlen;
+	int		error;
+	unsigned long	lc;		/* lock cookie */
+	struct nameidata nd;
+	struct inode *inode;
+	size_t		len;
+	char		*name;
+
+	/* XXX get things straightened out so getname() works here? */
+	len = strnlen_user(path, 2000);
+	name = kmalloc(len, GFP_KERNEL);
+	if (name == NULL) {
+		printk("%s/%d: kmalloc returned NULL\n", __FUNCTION__, __LINE__);
+		return(ENOMEM);
+	}
+	if (copy_from_user(name, path, len)) {
+		kfree(name);
+		return(EFAULT);
+	}
+
+	error = path_lookup(name, LOOKUP_POSITIVE|LOOKUP_FOLLOW, &nd);
+	kfree(name);
+	if (error)
+		return -error; /* path_lookup returned negative error */
+
+	ASSERT(nd.dentry);
+	ASSERT(nd.dentry->d_inode);
+
+	inode = igrab(nd.dentry->d_inode);
+	path_release(&nd);
+
+	if (inode->i_sb->s_magic != XFS_SB_MAGIC) {
+		/* we're not in XFS anymore, Toto */
+		iput(inode);
+		return EINVAL;
+	}
+
+	/* we need the vnode */
+	vp = LINVFS_GET_VP(inode);
+	error = dm_vp_to_handle(vp, &handle);
+	iput(inode);
+
+	if (error)
+		return(error);
+
+	if ((fsrp = dm_find_fsreg_and_lock(&handle.ha_fsid, &lc)) == NULL)
+		return(EBADF);
+	mutex_spinunlock(&fsrp->fr_lock, lc);
+
+	hlen = FSHSIZE;
+	if(copy_to_user(hanp, &handle, (int)hlen))
+		return(EFAULT);
+	if(put_user(hlen,hlenp))
+		return(EFAULT);
+	return 0;
+}
+
+
+int
+dm_fd_to_hdl(
+	int		fd,		/* any file descriptor */
+	void		*hanp,		/* user's data buffer */
+	size_t		*hlenp)		/* set to size of data copied */
+{
+	/* REFERENCED */
+	dm_fsreg_t	*fsrp;
+	xfs_handle_t	handle;
+	size_t		hlen;
+	int		error;
+	unsigned long	lc;		/* lock cookie */
+	struct file *filep = fget(fd);
+
+	if (!filep)
+		return(EBADF);
+	if ((error = dm_vp_to_handle(LINVFS_GET_VP(filep->f_dentry->d_inode), &handle)) != 0)
+		return(error);
+
+	if ((fsrp = dm_find_fsreg_and_lock(&handle.ha_fsid, &lc)) == NULL)
+		return(EBADF);
+	mutex_spinunlock(&fsrp->fr_lock, lc);
+
+	hlen = XFS_HSIZE(handle);
+	if (copy_to_user(hanp, &handle, (int)hlen))
+		return(EFAULT);
+	fput(filep);
+	if(put_user(hlen, hlenp))
+		return(EFAULT);
+	return 0;
+}
+
+
+/* Enable events on an object. */
+
+int
+dm_set_eventlist(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_eventset_t	*eventsetp,
+	u_int		maxevent)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_eventset_t	eventset;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	if (copy_from_user(&eventset, eventsetp, sizeof(eventset)))
+		return(EFAULT);
+
+	/* Do some minor sanity checking. */
+
+	if (maxevent == 0 || maxevent > DM_EVENT_MAX)
+		return(EINVAL);
+
+	/* Access the specified object. */
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_ANY,
+		DM_RIGHT_EXCL, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->set_eventlist(tdp->td_vp, tdp->td_right,
+		(tdp->td_type == DM_TDT_VFS ? DM_FSYS_OBJ : 0),
+		&eventset, maxevent);
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+/* Return the list of enabled events for an object. */
+
+int
+dm_get_eventlist(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	u_int		nelem,
+	dm_eventset_t	*eventsetp,
+	u_int		*nelemp)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	dm_eventset_t	eventset;
+	u_int		elem;
+	int		error;
+
+	if (nelem == 0)
+		return(EINVAL);
+
+	/* Access the specified object. */
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_ANY,
+		DM_RIGHT_SHARED, &tdp);
+	if (error != 0)
+		return(error);
+
+	/* Get the object's event list. */
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->get_eventlist(tdp->td_vp, tdp->td_right,
+		(tdp->td_type == DM_TDT_VFS ? DM_FSYS_OBJ : 0),
+		nelem, &eventset, &elem);
+
+	dm_app_put_tdp(tdp);
+
+	if (error)
+		return(error);
+
+	if (copy_to_user(eventsetp, &eventset, sizeof(eventset)))
+		return(EFAULT);
+	if (put_user(nelem, nelemp))
+		return(EFAULT);
+	return(0);
+}
+
+
+/* Register for disposition of events.	The handle must either be the
+   global handle or must be the handle of a file system.  The list of events
+   is pointed to by eventsetp.
+*/
+
+int
+dm_set_disp(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_eventset_t	*eventsetp,
+	u_int		maxevent)
+{
+	dm_session_t	*s;
+	dm_fsreg_t	*fsrp;
+	dm_tokdata_t	*tdp;
+	dm_eventset_t	eventset;
+	int		error;
+	unsigned long	lc1;		/* first lock cookie */
+	unsigned long	lc2;		/* second lock cookie */
+	u_int		i;
+
+	/* Copy in and validate the event mask.	 Only the lower maxevent bits
+	   are meaningful, so clear any bits set above maxevent.
+	*/
+
+	if (maxevent == 0 || maxevent > DM_EVENT_MAX)
+		return(EINVAL);
+	if (copy_from_user(&eventset, eventsetp, sizeof(eventset)))
+		return(EFAULT);
+	eventset &= (1 << maxevent) - 1;
+
+	/* If the caller specified the global handle, then the only valid token
+	   is DM_NO_TOKEN, and the only valid event in the event mask is
+	   DM_EVENT_MOUNT.  If it is set, add the session to the list of
+	   sessions that want to receive mount events.	If it is clear, remove
+	   the session from the list.  Since DM_EVENT_MOUNT events never block
+	   waiting for a session to register, there is noone to wake up if we
+	   do add the session to the list.
+	*/
+
+	if (DM_GLOBALHAN(hanp, hlen)) {
+		if (token != DM_NO_TOKEN)
+			return(EINVAL);
+		if ((error = dm_find_session_and_lock(sid, &s, &lc1)) != 0)
+			return(error);
+		if (eventset == 0) {
+			s->sn_flags &= ~DM_SN_WANTMOUNT;
+			error = 0;
+		} else if (eventset == 1 << DM_EVENT_MOUNT) {
+			s->sn_flags |= DM_SN_WANTMOUNT;
+			error = 0;
+		} else {
+			error = EINVAL;
+		}
+		mutex_spinunlock(&s->sn_qlock, lc1);
+		return(error);
+	}
+
+	/* Since it's not the global handle, it had better be a filesystem
+	   handle.  Verify that the first 'maxevent' events in the event list
+	   are all valid for a filesystem handle.
+	*/
+
+	if (eventset & ~DM_VALID_DISP_EVENTS)
+		return(EINVAL);
+
+	/* Verify that the session is valid, that the handle is a filesystem
+	   handle, and that the filesystem is capable of sending events.  (If
+	   a dm_fsreg_t structure exists, then the filesystem can issue events.)
+	*/
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_VFS,
+		DM_RIGHT_EXCL, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsrp = dm_find_fsreg_and_lock(&tdp->td_handle.ha_fsid, &lc1);
+	if (fsrp == NULL) {
+		dm_app_put_tdp(tdp);
+		return(EINVAL);
+	}
+
+	/* Now that we own 'fsrp->fr_lock', get the lock on the session so that
+	   it can't disappear while we add it to the filesystem's event mask.
+	*/
+
+	if ((error = dm_find_session_and_lock(sid, &s, &lc2)) != 0) {
+		mutex_spinunlock(&fsrp->fr_lock, lc1);
+		dm_app_put_tdp(tdp);
+		return(error);
+	}
+
+	/* Update the event disposition array for this filesystem, adding
+	   and/or removing the session as appropriate.	If this session is
+	   dropping registration for DM_EVENT_DESTROY, or is overriding some
+	   other session's registration for DM_EVENT_DESTROY, then clear any
+	   any attr-on-destroy attribute name also.
+	*/
+
+	for (i = 0; i < DM_EVENT_MAX; i++) {
+		if (DMEV_ISSET(i, eventset)) {
+			if (i == DM_EVENT_DESTROY && fsrp->fr_sessp[i] != s)
+				memset(&fsrp->fr_rattr, 0, sizeof(fsrp->fr_rattr));
+			fsrp->fr_sessp[i] = s;
+		} else if (fsrp->fr_sessp[i] == s) {
+			if (i == DM_EVENT_DESTROY)
+				memset(&fsrp->fr_rattr, 0, sizeof(fsrp->fr_rattr));
+			fsrp->fr_sessp[i] = NULL;
+		}
+	}
+	mutex_spinunlock(&s->sn_qlock, lc2);	/* reverse cookie order */
+
+	/* Wake up all processes waiting for a disposition on this filesystem
+	   in case any of them happen to be waiting for an event which we just
+	   added.
+	*/
+
+	if (fsrp->fr_dispcnt)
+		sv_broadcast(&fsrp->fr_dispq);
+
+	mutex_spinunlock(&fsrp->fr_lock, lc1);
+
+	dm_app_put_tdp(tdp);
+	return(0);
+}
+
+
+/*
+ *	Register a specific attribute name with a filesystem.  The value of
+ *	the attribute is to be returned with an asynchronous destroy event.
+ */
+
+int
+dm_set_return_on_destroy(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_attrname_t	*attrnamep,
+	dm_boolean_t	enable)
+{
+	dm_attrname_t	attrname;
+	dm_tokdata_t	*tdp;
+	dm_fsreg_t	*fsrp;
+	dm_session_t	*s;
+	int		error;
+	unsigned long	lc1;		/* first lock cookie */
+	unsigned long	lc2;		/* second lock cookie */
+
+	/* If a dm_attrname_t is provided, copy it in and validate it. */
+
+	if (enable && (error = copy_from_user(&attrname, attrnamep, sizeof(attrname))) != 0)
+		return(-error); /* copy_from_user returns negative error */
+
+	/* Validate the filesystem handle and use it to get the filesystem's
+	   disposition structure.
+	*/
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_VFS,
+		DM_RIGHT_EXCL, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsrp = dm_find_fsreg_and_lock(&tdp->td_handle.ha_fsid, &lc1);
+	if (fsrp == NULL) {
+		dm_app_put_tdp(tdp);
+		return(EINVAL);
+	}
+
+	/* Now that we own 'fsrp->fr_lock', get the lock on the session so that
+	   it can't disappear while we add it to the filesystem's event mask.
+	*/
+
+	if ((error = dm_find_session_and_lock(sid, &s, &lc2)) != 0) {
+		mutex_spinunlock(&fsrp->fr_lock, lc1);
+		dm_app_put_tdp(tdp);
+		return(error);
+	}
+
+	/* A caller cannot disable return-on-destroy if he is not registered
+	   for DM_EVENT_DESTROY.  Enabling return-on-destroy is an implicit
+	   dm_set_disp() for DM_EVENT_DESTROY; we wake up all processes
+	   waiting for a disposition in case any was waiting for a
+	   DM_EVENT_DESTROY event.
+	*/
+
+	error = 0;
+	if (enable) {
+		fsrp->fr_sessp[DM_EVENT_DESTROY] = s;
+		fsrp->fr_rattr = attrname;
+		if (fsrp->fr_dispcnt)
+			sv_broadcast(&fsrp->fr_dispq);
+	} else if (fsrp->fr_sessp[DM_EVENT_DESTROY] != s) {
+		error = EINVAL;
+	} else {
+		memset(&fsrp->fr_rattr, 0, sizeof(fsrp->fr_rattr));
+	}
+	mutex_spinunlock(&s->sn_qlock, lc2);	/* reverse cookie order */
+	mutex_spinunlock(&fsrp->fr_lock, lc1);
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+int
+dm_get_mountinfo(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp)
+{
+	dm_fsreg_t	*fsrp;
+	dm_tokdata_t	*tdp;
+	int		error;
+	unsigned long	lc;		/* lock cookie */
+
+	/* Make sure that the caller's buffer is 8-byte aligned. */
+
+	if (((__psint_t)bufp & (sizeof(__u64) - 1)) != 0)
+		return(EFAULT);
+
+	/* Verify that the handle is a filesystem handle, and that the
+	   filesystem is capable of sending events.  If not, return an error.
+	*/
+
+	error = dm_app_get_tdp(sid, hanp, hlen, token, DM_TDT_VFS,
+		DM_RIGHT_SHARED, &tdp);
+	if (error != 0)
+		return(error);
+
+	/* Find the filesystem entry.  This should always succeed as the
+	   dm_app_get_tdp call created a filesystem reference.	Once we find
+	   the entry, drop the lock.  The mountinfo message is never modified,
+	   the filesystem entry can't disappear, and we don't want to hold a
+	   spinlock while doing copyout calls.
+	*/
+
+	fsrp = dm_find_fsreg_and_lock(&tdp->td_handle.ha_fsid, &lc);
+	if (fsrp == NULL) {
+		dm_app_put_tdp(tdp);
+		return(EINVAL);
+	}
+	mutex_spinunlock(&fsrp->fr_lock, lc);
+
+	/* Copy the message into the user's buffer and update his 'rlenp'. */
+
+	if (put_user(fsrp->fr_msgsize, rlenp)) {
+		error = EFAULT;
+	} else if (fsrp->fr_msgsize > buflen) { /* user buffer not big enough */
+		error = E2BIG;
+	} else if (copy_to_user(bufp, fsrp->fr_msg, fsrp->fr_msgsize)) {
+		error = EFAULT;
+	} else {
+		error = 0;
+	}
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+int
+dm_getall_disp(
+	dm_sessid_t	sid,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp)
+{
+	dm_session_t	*s;		/* pointer to session given by sid */
+	unsigned long	lc1;		/* first lock cookie */
+	unsigned long	lc2;		/* second lock cookie */
+	int		totalsize;
+	int		msgsize;
+	int		fsyscnt;
+	dm_dispinfo_t	*prevmsg;
+	dm_fsreg_t	*fsrp;
+	int		error;
+	char		*kbuf;
+
+	int tmp3;
+	int tmp4;
+
+	/* Because the dm_getall_disp structure contains a __u64 field,
+	   make sure that the buffer provided by the caller is aligned so
+	   that he can read such fields successfully.
+	*/
+
+	if (((__psint_t)bufp & (sizeof(__u64) - 1)) != 0)
+		return(EFAULT);
+
+	/* Compute the size of a dm_dispinfo structure, rounding up to an
+	   8-byte boundary so that any subsequent structures will also be
+	   aligned.
+	*/
+
+#if 0
+	/* XXX	ug, what is going on here? */
+	msgsize = (sizeof(dm_dispinfo_t) + FSHSIZE + sizeof(uint64_t) - 1) &
+		~(sizeof(uint64_t) - 1);
+#else
+	tmp3 = sizeof(dm_dispinfo_t) + FSHSIZE;
+	tmp3 += sizeof(__u64);
+	tmp3 -= 1;
+	tmp4 = ~((int)sizeof(__u64) - 1);
+	msgsize = tmp3 & tmp4;
+#endif
+
+	/* Loop until we can get the right amount of temp space, being careful
+	   not to hold a mutex during the allocation.  Usually only one trip.
+	*/
+
+	for (;;) {
+		if ((fsyscnt = dm_fsys_cnt) == 0) {
+			/*if (dm_cpoutsizet(rlenp, 0))*/
+			if (put_user(0,rlenp))
+				return(EFAULT);
+			return(0);
+		}
+		kbuf = kmalloc(fsyscnt * msgsize, GFP_KERNEL);
+		if (kbuf == NULL) {
+			printk("%s/%d: kmalloc returned NULL\n", __FUNCTION__, __LINE__);
+			return ENOMEM;
+		}
+
+		lc1 = mutex_spinlock(&dm_reg_lock);
+		if (fsyscnt == dm_fsys_cnt)
+			break;
+
+		mutex_spinunlock(&dm_reg_lock, lc1);
+		kfree(kbuf);
+	}
+
+	/* Find the indicated session and lock it. */
+
+	if ((error = dm_find_session_and_lock(sid, &s, &lc2)) != 0) {
+		mutex_spinunlock(&dm_reg_lock, lc1);
+		kfree(kbuf);
+		return(error);
+	}
+
+	/* Create a dm_dispinfo structure for each filesystem in which
+	   this session has at least one event selected for disposition.
+	*/
+
+	totalsize = 0;		/* total bytes to transfer to the user */
+	prevmsg = NULL;
+
+	for (fsrp = dm_registers; fsrp; fsrp = fsrp->fr_next) {
+		dm_dispinfo_t	*disp;
+		int		event;
+		int		found;
+
+		disp = (dm_dispinfo_t *)(kbuf + totalsize);
+
+		DMEV_ZERO(disp->di_eventset);
+
+		for (event = 0, found = 0; event < DM_EVENT_MAX; event++) {
+			if (fsrp->fr_sessp[event] != s)
+				continue;
+			DMEV_SET(event, disp->di_eventset);
+			found++;
+		}
+		if (!found)
+			continue;
+
+		disp->_link = 0;
+		disp->di_fshandle.vd_offset = sizeof(dm_dispinfo_t);
+		disp->di_fshandle.vd_length = FSHSIZE;
+
+		memcpy((char *)disp + disp->di_fshandle.vd_offset,
+			&fsrp->fr_fsid, disp->di_fshandle.vd_length);
+
+		if (prevmsg)
+			prevmsg->_link = msgsize;
+
+		prevmsg = disp;
+		totalsize += msgsize;
+	}
+	mutex_spinunlock(&s->sn_qlock, lc2);	/* reverse cookie order */
+	mutex_spinunlock(&dm_reg_lock, lc1);
+
+	if (put_user(totalsize, rlenp)) {
+		error = EFAULT;
+	} else if (totalsize > buflen) {	/* no more room */
+		error = E2BIG;
+	} else if (totalsize && copy_to_user(bufp, kbuf, totalsize)) {
+		error = EFAULT;
+	} else {
+		error = 0;
+	}
+
+	kfree(kbuf);
+	return(error);
+}
+
+int
+dm_open_by_handle_rvp(
+	unsigned int	fd,
+	void		*hanp,
+	size_t		hlen,
+	int		flags,
+	int		*rvp)
+{
+	xfs_handle_t	handle;
+	int		error;
+	vnode_t		*vp;
+	short		td_type;
+	struct dentry	*dentry;
+	struct inode	*inodep;
+	int		new_fd;
+	struct file	*mfilp;
+	struct file	*filp;
+
+	if ((error = dm_copyin_handle(hanp, hlen, &handle)) != 0) {
+		return(error);
+	}
+
+	if ((vp = dm_handle_to_vp(&handle, &td_type)) == NULL) {
+		return(EBADF);
+	}
+	inodep = LINVFS_GET_IP(vp);
+	if ((td_type == DM_TDT_VFS) || (td_type == DM_TDT_OTH)) {
+		iput(inodep);
+		return(EBADF);
+	}
+
+	if ((new_fd = get_unused_fd()) < 0) {
+		iput(inodep);
+		return(EMFILE);
+	}
+
+	dentry = d_alloc_anon(inodep);
+	if (dentry == NULL) {
+		iput(inodep);
+		put_unused_fd(new_fd);
+		return(ENOMEM);
+	}
+
+	mfilp = fget(fd);
+	if (!mfilp) {
+		dput(dentry);
+		put_unused_fd(new_fd);
+		return(EBADF);
+	}
+
+	mntget(mfilp->f_vfsmnt);
+
+	/* Create file pointer */
+	filp = dentry_open(dentry, mfilp->f_vfsmnt, flags);
+	if (IS_ERR(filp)) {
+		put_unused_fd(new_fd);
+		fput(mfilp);
+		return -PTR_ERR(filp); /* dentry_open returned negative err */
+	}
+
+	if (td_type == DM_TDT_REG)
+		filp->f_op = &linvfs_invis_file_operations;
+	fd_install(new_fd, filp);
+	fput(mfilp);
+	*rvp = new_fd;
+	return 0;
+}
diff -uNrp linux-2.6.5/fs/xfs/dmapi/dmapi_right.c linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_right.c
--- linux-2.6.5/fs/xfs/dmapi/dmapi_right.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_right.c	2004-04-05 00:58:15.000000000 +0200
@@ -0,0 +1,1258 @@
+/*
+ * Copyright (c) 2000 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.	 Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+#include "xfs.h"
+#include "dmapi.h"
+#include "dmapi_kern.h"
+#include "dmapi_private.h"
+
+
+#define DM_FG_STHREAD		0x001	/* keep other threads from using tdp */
+#define DM_FG_MUSTEXIST		0x002	/* handle must exist in the event */
+#define DM_FG_DONTADD		0x004	/* don't add handle if not in event */
+
+/* Get a handle of the form (void *, size_t) from user space and convert it to
+   a handle_t.	Do as much validation of the result as possible; any error
+   other than a bad address should return EBADF per the DMAPI spec.
+*/
+
+int
+dm_copyin_handle(
+	void		*hanp,		/* input,  handle data */
+	size_t		hlen,		/* input,  size of handle data */
+	xfs_handle_t	*handlep)	/* output, copy of data */
+{
+	u_short		len;
+	fid_t		*fidp;
+
+	fidp = (fid_t*)&handlep->ha_fid;
+
+	if (hlen < sizeof(handlep->ha_fsid) || hlen > sizeof(*handlep))
+		return(EBADF);
+
+	if (copy_from_user(handlep, hanp, hlen))
+		return(EFAULT);
+
+	if (hlen < sizeof(*handlep))
+		memset((char *)handlep + hlen, 0, sizeof(*handlep) - hlen);
+
+	if (hlen == sizeof(handlep->ha_fsid))
+		return(0);	/* FS handle, nothing more to check */
+
+	len = hlen - sizeof(handlep->ha_fsid) - sizeof(fidp->fid_len);
+
+	if (fidp->fid_len != len ||
+	    *((short *) fidp->fid_data)) {
+		return(EBADF);
+	}
+	return(0);
+}
+
+/* Allocate and initialize a tevp structure.  Called from both application and
+   event threads.
+*/
+
+static dm_tokevent_t *
+dm_init_tevp(
+	int		ev_size,	/* size of event structure */
+	int		var_size)	/* size of variable-length data */
+{
+	dm_tokevent_t	*tevp;
+	int		msgsize;
+
+	/* Calculate the size of the event in bytes and allocate memory for it.
+	   Zero all but the variable portion of the message, which will be
+	   eventually overlaid by the caller with data.
+	*/
+
+	msgsize = offsetof(dm_tokevent_t, te_event) + ev_size + var_size;
+	tevp = kmalloc(msgsize, GFP_KERNEL);
+	if (tevp == NULL) {
+		printk("%s/%d: kmalloc returned NULL\n", __FUNCTION__, __LINE__);
+		return NULL;
+	}
+	memset(tevp, 0, msgsize - var_size);
+
+	/* Now initialize all the non-zero fields. */
+
+	spinlock_init(&tevp->te_lock, "te_lock");
+	sv_init(&tevp->te_evt_queue, SV_DEFAULT, "te_evt_queue");
+	sv_init(&tevp->te_app_queue, SV_DEFAULT, "te_app_queue");
+	tevp->te_allocsize = msgsize;
+	tevp->te_msg.ev_type = DM_EVENT_INVALID;
+	tevp->te_flags = 0;
+
+	return(tevp);
+}
+
+
+/* Given the event type and the number of bytes of variable length data that
+   will follow the event, dm_evt_create_tevp() creates a dm_tokevent_t
+   structure to hold the event and initializes all the common event fields.
+
+   No locking is required for this routine because the caller is an event
+   thread, and is therefore the only thread that can see the event.
+*/
+
+dm_tokevent_t *
+dm_evt_create_tevp(
+	dm_eventtype_t	event,
+	int		variable_size,
+	void		**msgpp)
+{
+	dm_tokevent_t	*tevp;
+	int		evsize;
+
+	switch (event) {
+	case DM_EVENT_READ:
+	case DM_EVENT_WRITE:
+	case DM_EVENT_TRUNCATE:
+		evsize = sizeof(dm_data_event_t);
+		break;
+
+	case DM_EVENT_DESTROY:
+		evsize = sizeof(dm_destroy_event_t);
+		break;
+
+	case DM_EVENT_MOUNT:
+		evsize = sizeof(dm_mount_event_t);
+		break;
+
+	case DM_EVENT_PREUNMOUNT:
+	case DM_EVENT_UNMOUNT:
+	case DM_EVENT_NOSPACE:
+	case DM_EVENT_CREATE:
+	case DM_EVENT_REMOVE:
+	case DM_EVENT_RENAME:
+	case DM_EVENT_SYMLINK:
+	case DM_EVENT_LINK:
+	case DM_EVENT_POSTCREATE:
+	case DM_EVENT_POSTREMOVE:
+	case DM_EVENT_POSTRENAME:
+	case DM_EVENT_POSTSYMLINK:
+	case DM_EVENT_POSTLINK:
+	case DM_EVENT_ATTRIBUTE:
+	case DM_EVENT_DEBUT:		/* currently not supported */
+	case DM_EVENT_CLOSE:		/* currently not supported */
+		evsize = sizeof(dm_namesp_event_t);
+		break;
+
+	case DM_EVENT_CANCEL:		/* currently not supported */
+		evsize = sizeof(dm_cancel_event_t);
+		break;
+
+	case DM_EVENT_USER:
+		evsize = 0;
+		break;
+
+	default:
+		panic("dm_create_tevp: called with unknown event type %d\n",
+			event);
+	}
+
+	/* Allocate and initialize an event structure of the correct size. */
+
+	tevp = dm_init_tevp(evsize, variable_size);
+	if (tevp == NULL)
+		return NULL;
+	tevp->te_evt_ref = 1;
+
+	/* Fields ev_token, ev_sequence, and _link are all filled in when the
+	   event is queued onto a session.  Initialize all other fields here.
+	*/
+
+	tevp->te_msg.ev_type = event;
+	tevp->te_msg.ev_data.vd_offset = offsetof(dm_tokevent_t, te_event) -
+		offsetof(dm_tokevent_t, te_msg);
+	tevp->te_msg.ev_data.vd_length = evsize + variable_size;
+
+	/* Give the caller a pointer to the event-specific structure. */
+
+	*msgpp = ((char *)&tevp->te_msg + tevp->te_msg.ev_data.vd_offset);
+	return(tevp);
+}
+
+
+/* Given a pointer to an event (tevp) and a pointer to a handle_t, look for a
+   tdp structure within the event which contains the handle_t.	Either verify
+   that the event contains the tdp, or optionally add the tdp to the
+   event.  Called only from application threads.
+
+   On entry, tevp->te_lock is held; it is dropped prior to return.
+*/
+
+static int
+dm_app_lookup_tdp(
+	xfs_handle_t	*handlep,	/* the handle we are looking for */
+	dm_tokevent_t	*tevp,		/* the event to search for the handle */
+	unsigned long	*lcp,		/* address of active lock cookie */
+	short		types,		/* acceptable object types */
+	dm_right_t	right,		/* minimum right the object must have */
+	u_int		flags,
+	dm_tokdata_t	**tdpp)		/* if ! NULL, pointer to matching tdp */
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	vnode_t		*vp;
+	int		error;
+
+	/* Bump the tevp application reference counter so that the event
+	   can't disappear in case we have to drop the lock for a while.
+	*/
+
+	tevp->te_app_ref++;
+	*tdpp = NULL;		/* assume failure */
+
+	for (;;) {
+		/* Look for a matching tdp in the tevp. */
+
+		for (tdp = tevp->te_tdp; tdp; tdp = tdp->td_next) {
+			if (XFS_HANDLE_CMP(&tdp->td_handle, handlep) == 0)
+				break;
+		}
+
+		/* If the tdp exists, but either we need single-thread access
+		   to the handle and can't get it, or some other thread already
+		   has single-thread access, then sleep until we can try again.
+		*/
+
+		if (tdp != NULL && tdp->td_app_ref &&
+		    ((flags & DM_FG_STHREAD) ||
+		     (tdp->td_flags & DM_TDF_STHREAD))) {
+			tevp->te_app_slp++;
+			sv_wait(&tevp->te_app_queue, 1,
+				&tevp->te_lock, *lcp);
+			*lcp = mutex_spinlock(&tevp->te_lock);
+			tevp->te_app_slp--;
+			continue;
+		}
+
+		if (tdp != NULL &&
+		    (tdp->td_vcount > 0 || tdp->td_flags & DM_TDF_EVTREF)) {
+			/* We have an existing tdp with a non-zero vnode
+			   reference count.  If it's the wrong type, return
+			   an appropriate errno.
+			*/
+
+			if (!(tdp->td_type & types)) {
+				mutex_spinunlock(&tevp->te_lock, *lcp);
+				dm_put_tevp(tevp, NULL); /* no destroy events */
+				return(EOPNOTSUPP);
+			}
+
+			/* If the current access right isn't high enough,
+			   complain.
+			*/
+
+			if (tdp->td_right < right) {
+				mutex_spinunlock(&tevp->te_lock, *lcp);
+				dm_put_tevp(tevp, NULL); /* no destroy events */
+				return(EACCES);
+			}
+
+			/* The handle is acceptable.  Increment the tdp
+			   application and vnode references and mark the tdp
+			   as single-threaded if necessary.
+			*/
+
+			tdp->td_app_ref++;
+			if (flags & DM_FG_STHREAD)
+				tdp->td_flags |= DM_TDF_STHREAD;
+			tdp->td_vcount++;
+
+			fsys_vector = dm_fsys_vector(tdp->td_vp);
+			(void)fsys_vector->obj_ref_hold(tdp->td_vp);
+
+			mutex_spinunlock(&tevp->te_lock, *lcp);
+			*tdpp = tdp;
+			return(0);
+		}
+
+		/* If the tdp is not in the tevp or does not have a vnode
+		   reference, check to make sure it is okay to add/update it.
+		*/
+
+		if (flags & DM_FG_MUSTEXIST) {
+			mutex_spinunlock(&tevp->te_lock, *lcp);
+			dm_put_tevp(tevp, NULL);	/* no destroy events */
+			return(EACCES);		/* i.e. an insufficient right */
+		}
+		if (flags & DM_FG_DONTADD) {
+			tevp->te_app_ref--;
+			mutex_spinunlock(&tevp->te_lock, *lcp);
+			return(0);
+		}
+
+		/* If a tdp structure doesn't yet exist, create one and link
+		   it into the tevp.  Drop the lock while we are doing this as
+		   zallocs can go to sleep.  Once we have the memory, make
+		   sure that another thread didn't simultaneously add the same
+		   handle to the same event.  If so, toss ours and start over.
+		*/
+
+		if (tdp == NULL) {
+			dm_tokdata_t	*tmp;
+
+			mutex_spinunlock(&tevp->te_lock, *lcp);
+
+			tdp = kmem_cache_alloc(dm_tokdata_cachep, SLAB_KERNEL);
+			if (tdp == NULL){
+				printk("%s/%d: kmem_cache_alloc(dm_tokdata_cachep) returned NULL\n", __FUNCTION__, __LINE__);
+				return(ENOMEM);
+			}
+			memset(tdp, 0, sizeof(*tdp));
+
+			*lcp = mutex_spinlock(&tevp->te_lock);
+
+			for (tmp = tevp->te_tdp; tmp; tmp = tmp->td_next) {
+				if (XFS_HANDLE_CMP(&tmp->td_handle, handlep) == 0)
+					break;
+			}
+			if (tmp) {
+				kmem_cache_free(dm_tokdata_cachep, tdp);
+				continue;
+			}
+
+			tdp->td_next = tevp->te_tdp;
+			tevp->te_tdp = tdp;
+			tdp->td_tevp = tevp;
+			tdp->td_handle = *handlep;
+		}
+
+		/* Temporarily single-thread access to the tdp so that other
+		   threads don't touch it while we are filling the rest of the
+		   fields in.
+		*/
+
+		tdp->td_app_ref = 1;
+		tdp->td_flags |= DM_TDF_STHREAD;
+
+		/* Drop the spinlock while we access, validate, and obtain the
+		   proper rights to the object.	 This can take a very long time
+		   if the vnode is not in memory, if the filesystem is
+		   unmounting,	or if the request_right() call should block
+		   because some other tdp or kernel thread is holding a right.
+		*/
+
+		mutex_spinunlock(&tevp->te_lock, *lcp);
+
+		if ((vp = dm_handle_to_vp(handlep, &tdp->td_type)) == NULL) {
+			error = EBADF;
+		} else {
+			tdp->td_vcount = 1;
+			tdp->td_vp = vp;
+
+			/* The handle is usable.  Check that the type of the
+			   object matches one of the types that the caller
+			   will accept.
+			*/
+
+			if (!(types & tdp->td_type)) {
+				error = EOPNOTSUPP;
+			} else if (right > DM_RIGHT_NULL) {
+				/* Attempt to get the rights required by the
+				   caller.  If rights can't be obtained, return
+				   an error.
+				*/
+
+				fsys_vector = dm_fsys_vector(tdp->td_vp);
+				error = fsys_vector->request_right(tdp->td_vp,
+					DM_RIGHT_NULL,
+					(tdp->td_type == DM_TDT_VFS ?
+					DM_FSYS_OBJ : 0),
+					DM_RR_WAIT, right);
+				if (!error) {
+					tdp->td_right = right;
+				}
+			} else {
+				error = 0;
+			}
+		}
+		if (error != 0) {
+			dm_put_tevp(tevp, tdp); /* destroy event risk, although tiny */
+			return(error);
+		}
+
+		*lcp = mutex_spinlock(&tevp->te_lock);
+
+		/* Wake up any threads which may have seen our tdp while we
+		   were filling it in.
+		*/
+
+		if (!(flags & DM_FG_STHREAD)) {
+			tdp->td_flags &= ~DM_TDF_STHREAD;
+			if (tevp->te_app_slp)
+				sv_broadcast(&tevp->te_app_queue);
+		}
+
+		mutex_spinunlock(&tevp->te_lock, *lcp);
+		*tdpp = tdp;
+		return(0);
+	}
+}
+
+
+/* dm_app_get_tdp_by_token() is called whenever the application request
+   contains a session ID and contains a token other than DM_NO_TOKEN.
+   Most of the callers provide a right that is either DM_RIGHT_SHARED or
+   DM_RIGHT_EXCL, but a few of the callers such as dm_obj_ref_hold() may
+   specify a right of DM_RIGHT_NULL.
+*/
+
+static int
+dm_app_get_tdp_by_token(
+	dm_sessid_t	sid,		/* an existing session ID */
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,		/* an existing token */
+	short		types,		/* acceptable object types */
+	dm_right_t	right,		/* minimum right the object must have */
+	u_int		flags,
+	dm_tokdata_t	**tdpp)
+{
+	dm_tokevent_t	*tevp;
+	xfs_handle_t	handle;
+	int		error;
+	unsigned long	lc;		/* lock cookie */
+
+	if (right < DM_RIGHT_NULL || right > DM_RIGHT_EXCL)
+		return(EINVAL);
+
+	if ((error = dm_copyin_handle(hanp, hlen, &handle)) != 0)
+		return(error);
+
+	/* Find and lock the event which corresponds to the specified
+	   session/token pair.
+	*/
+
+	if ((error = dm_find_msg_and_lock(sid, token, &tevp, &lc)) != 0)
+		return(error);
+
+	return(dm_app_lookup_tdp(&handle, tevp, &lc, types,
+		right, flags, tdpp));
+}
+
+
+/* Function dm_app_get_tdp() must ONLY be called from routines associated with
+   application calls, e.g. dm_read_invis, dm_set_disp, etc.  It must not be
+   called by a thread responsible for generating an event such as
+   dm_send_data_event()!
+
+   dm_app_get_tdp() is the interface used by all application calls other than
+   dm_get_events, dm_respond_event, dm_get_config, dm_get_config_events, and by
+   the dm_obj_ref_* and dm_*_right families of requests.
+
+   dm_app_get_tdp() converts a sid/hanp/hlen/token quad into a tdp pointer,
+   increments the number of active application threads in the event, and
+   increments the number of active application threads using the tdp.  The
+   'right' parameter must be either DM_RIGHT_SHARED or DM_RIGHT_EXCL.  The
+   token may either be DM_NO_TOKEN, or can be a token received in a synchronous
+   event.
+*/
+
+int
+dm_app_get_tdp(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	short		types,
+	dm_right_t	right,		/* minimum right */
+	dm_tokdata_t	**tdpp)
+{
+	dm_session_t	*s;
+	xfs_handle_t	handle;
+	dm_tokevent_t	*tevp;
+	int		error;
+	unsigned long	lc;		/* lock cookie */
+
+	ASSERT(right >= DM_RIGHT_SHARED);
+
+	/* If a token other than DM_NO_TOKEN is specified, find the event on
+	   this session which owns the token and increment its reference count.
+	*/
+
+	if (token != DM_NO_TOKEN) {	/* look up existing tokevent struct */
+		return(dm_app_get_tdp_by_token(sid, hanp, hlen, token, types,
+			right, DM_FG_MUSTEXIST, tdpp));
+	}
+
+	/* The token is DM_NO_TOKEN.  In this case we only want to verify that
+	   the session ID is valid, and do not need to continue holding the
+	   session lock after we know that to be true.
+	*/
+
+	if ((error = dm_copyin_handle(hanp, hlen, &handle)) != 0)
+		return(error);
+
+	if ((error = dm_find_session_and_lock(sid, &s, &lc)) != 0)
+		return(error);
+	mutex_spinunlock(&s->sn_qlock, lc);
+
+	/* When DM_NO_TOKEN is used, we simply block until we can obtain the
+	   right that we want (since the tevp contains no tdp structures).
+	   The blocking when we eventually support it will occur within
+	   fsys_vector->request_right().
+	*/
+
+	tevp = dm_init_tevp(0, 0);
+	lc = mutex_spinlock(&tevp->te_lock);
+
+	return(dm_app_lookup_tdp(&handle, tevp, &lc, types, right, 0, tdpp));
+}
+
+
+/* dm_get_config_tdp() is only called by dm_get_config() and
+   dm_get_config_events(), which neither have a session ID nor a token.
+   Both of these calls are supposed to work even if the filesystem is in the
+   process of being mounted, as long as the caller only uses handles within
+   the mount event.
+*/
+
+int
+dm_get_config_tdp(
+	void		*hanp,
+	size_t		hlen,
+	dm_tokdata_t	**tdpp)
+{
+	xfs_handle_t	handle;
+	dm_tokevent_t	*tevp;
+	int		error;
+	unsigned long	lc;		/* lock cookie */
+
+	if ((error = dm_copyin_handle(hanp, hlen, &handle)) != 0)
+		return(error);
+
+	tevp = dm_init_tevp(0, 0);
+	lc = mutex_spinlock(&tevp->te_lock);
+
+	/* Try to use the handle provided by the caller and assume DM_NO_TOKEN.
+	   This will fail if the filesystem is in the process of being mounted.
+	*/
+
+	error = dm_app_lookup_tdp(&handle, tevp, &lc, DM_TDT_ANY,
+		DM_RIGHT_NULL, 0, tdpp);
+
+	if (!error) {
+		return(0);
+	}
+
+	/* Perhaps the filesystem is still mounting, in which case we need to
+	   see if this is one of the handles in the DM_EVENT_MOUNT tevp.
+	*/
+
+	if ((tevp = dm_find_mount_tevp_and_lock(&handle.ha_fsid, &lc)) == NULL)
+		return(EBADF);
+
+	return(dm_app_lookup_tdp(&handle, tevp, &lc, DM_TDT_ANY,
+		DM_RIGHT_NULL, DM_FG_MUSTEXIST, tdpp));
+}
+
+
+/* dm_put_tdp() is called to release any right held on the vnode, and to
+   VN_RELE() all references held on the vnode.	It is the caller's
+   responsibility to ensure that no other application threads are using the
+   tdp, and if necessary to unlink the tdp from the tevp before calling
+   this routine and to free the tdp afterwards.
+*/
+
+static void
+dm_put_tdp(
+	dm_tokdata_t	*tdp)
+{
+	ASSERT(tdp->td_app_ref <= 1);
+
+	/* If the application thread is holding a right, or if the event
+	   thread had a right but it has disappeared because of a dm_pending
+	   or Cntl-C, then we need to release it here.
+	*/
+
+	if (tdp->td_right != DM_RIGHT_NULL) {
+		dm_fsys_vector_t *fsys_vector;
+
+		fsys_vector = dm_fsys_vector(tdp->td_vp);
+		(void)fsys_vector->release_right(tdp->td_vp, tdp->td_right,
+			(tdp->td_type == DM_TDT_VFS ? DM_FSYS_OBJ : 0));
+		tdp->td_right = DM_RIGHT_NULL;
+	}
+
+	/* Given that we wouldn't be here if there was still an event thread,
+	   this VN_RELE loop has the potential of generating a DM_EVENT_DESTROY
+	   event if some other thread has unlinked the file.
+	*/
+
+	while (tdp->td_vcount > 0) {
+		iput(LINVFS_GET_IP(tdp->td_vp));
+		tdp->td_vcount--;
+	}
+
+	tdp->td_flags &= ~(DM_TDF_HOLD|DM_TDF_RIGHT);
+	tdp->td_vp = NULL;
+}
+
+
+/* Function dm_put_tevp() must ONLY be called from routines associated with
+   application threads, e.g. dm_read_invis, dm_get_events, etc.	 It must not be
+   called by a thread responsible for generating an event, such as
+   dm_send_data_event.
+
+   PLEASE NOTE: It is possible for this routine to generate DM_EVENT_DESTROY
+   events, because its calls to dm_put_tdp drop vnode references, and another
+   thread may have already unlinked a file whose vnode we are de-referencing.
+   This sets the stage for various types of deadlock if the thread calling
+   dm_put_tevp is the same thread that calls dm_respond_event!	In particular,
+   the dm_sent_destroy_event routine needs to obtain the dm_reg_lock,
+   dm_session_lock, and sn_qlock in order to queue the destroy event.  No
+   caller of dm_put_tevp can hold any of these locks!
+
+   Other possible deadlocks are that dm_send_destroy_event could block waiting
+   for a thread to register for the event using	 dm_set_disp() and/or
+   dm_set_return_on_destroy, or it could block because the session's sn_newq
+   is at the dm_max_queued_msgs event limit.  The only safe solution
+   (unimplemented) is to have a separate kernel thread for each filesystem
+   whose only job is to do the vnode-dereferencing.  That way dm_respond_event
+   will not block, so the application can keep calling dm_get_events to read
+   events even if the filesystem thread should block.  (If the filesystem
+   thread blocks, so will all subsequent destroy events for the same
+   filesystem.)
+*/
+
+void
+dm_put_tevp(
+	dm_tokevent_t	*tevp,
+	dm_tokdata_t	*tdp)
+{
+	int		free_tdp = 0;
+	unsigned long	lc;		/* lock cookie */
+
+	lc = mutex_spinlock(&tevp->te_lock);
+
+	if (tdp != NULL) {
+		if (tdp->td_vcount > 1 || (tdp->td_flags & DM_TDF_EVTREF)) {
+			ASSERT(tdp->td_app_ref > 0);
+
+			iput(LINVFS_GET_IP(tdp->td_vp));
+			tdp->td_vcount--;
+		} else {
+			ASSERT(tdp->td_app_ref == 1);
+
+			/* The vnode reference count is either already at
+			   zero (e.g. a failed dm_handle_to_vp() call in
+			   dm_app_lookup_tdp()) or is going to zero.  We can't
+			   hold the lock while we decrement the count because
+			   we could potentially end up being busy for a long
+			   time in VOP_INACTIVATE.  Use single-threading to
+			   lock others out while we clean house.
+			*/
+
+			tdp->td_flags |= DM_TDF_STHREAD;
+
+			/* WARNING - A destroy event is possible here if we are
+			   giving up the last reference on a vnode which has
+			   been previously unlinked by some other thread!
+			*/
+
+			mutex_spinunlock(&tevp->te_lock, lc);
+			dm_put_tdp(tdp);
+			lc = mutex_spinlock(&tevp->te_lock);
+
+			/* If this tdp is not one of the original tdps in the
+			   event, then remove it from the tevp.
+			*/
+
+			if (!(tdp->td_flags & DM_TDF_ORIG)) {
+				dm_tokdata_t	**tdpp = &tevp->te_tdp;
+
+				while (*tdpp && *tdpp != tdp) {
+					tdpp = &(*tdpp)->td_next;
+				}
+				if (*tdpp == NULL) {
+					panic("dm_remove_tdp_from_tevp: tdp "
+						"%p not in tevp %p\n", tdp,
+						tevp);
+				}
+				*tdpp = tdp->td_next;
+				free_tdp++;
+			}
+		}
+
+		/* If this is the last app thread actively using the tdp, clear
+		   any single-threading and wake up any other app threads who
+		   might be waiting to use this tdp, single-threaded or
+		   otherwise.
+		*/
+
+		if (--tdp->td_app_ref == 0) {
+			if (tdp->td_flags & DM_TDF_STHREAD) {
+				tdp->td_flags &= ~DM_TDF_STHREAD;
+				if (tevp->te_app_slp)
+					sv_broadcast(&tevp->te_app_queue);
+			}
+		}
+
+		if (free_tdp) {
+			kmem_cache_free(dm_tokdata_cachep, tdp);
+		}
+	}
+
+	/* If other application threads are using this token/event, they will
+	   do the cleanup.
+	*/
+
+	if (--tevp->te_app_ref > 0) {
+		mutex_spinunlock(&tevp->te_lock, lc);
+		return;
+	}
+
+	/* If event generation threads are waiting for this thread to go away,
+	   wake them up and let them do the cleanup.
+	*/
+
+	if (tevp->te_evt_ref > 0) {
+		sv_broadcast(&tevp->te_evt_queue);
+		mutex_spinunlock(&tevp->te_lock, lc);
+		return;
+	}
+
+	/* This thread is the last active thread using the token/event.	 No
+	   lock can be held while we disassemble the tevp because we could
+	   potentially end up being busy for a long time in VOP_INACTIVATE.
+	*/
+
+	mutex_spinunlock(&tevp->te_lock, lc);
+
+	/* WARNING - One or more destroy events are possible here if we are
+	   giving up references on vnodes which have been previously unlinked
+	   by other kernel threads!
+	*/
+
+	while ((tdp = tevp->te_tdp) != NULL) {
+		tevp->te_tdp = tdp->td_next;
+		dm_put_tdp(tdp);
+		kmem_cache_free(dm_tokdata_cachep, tdp);
+	}
+	spinlock_destroy(&tevp->te_lock);
+	sv_destroy(&tevp->te_evt_queue);
+	sv_destroy(&tevp->te_app_queue);
+	kfree(tevp);
+}
+
+
+/* No caller of dm_app_put_tevp can hold either of the locks dm_reg_lock,
+   dm_session_lock, or any sn_qlock!  (See dm_put_tevp for details.)
+*/
+
+void
+dm_app_put_tdp(
+	dm_tokdata_t	*tdp)
+{
+	dm_put_tevp(tdp->td_tevp, tdp);
+}
+
+
+/* dm_change_right is only called if the event thread is the one doing the
+   cleanup on a completed event.  It looks at the current rights of a tdp
+   and compares that with the rights it had on the tdp when the event was
+   created.  If different, it reaquires the original rights, then transfers
+   the rights back to being thread-based.
+*/
+
+static void
+dm_change_right(
+	dm_tokdata_t	*tdp)
+{
+#ifdef HAVE_DMAPI_RIGHTS
+	dm_fsys_vector_t *fsys_vector;
+	int		error;
+	u_int		type;
+#endif
+
+	/* If the event doesn't have a vnode reference, if the original right
+	   was DM_RIGHT_NULL, or if the rights were never switched from being
+	   thread-based to tdp-based, then there is nothing to do.
+	*/
+
+	if (!(tdp->td_flags & DM_TDF_EVTREF))
+		return;
+
+	if (tdp->td_orig_right == DM_RIGHT_NULL)
+		return;
+
+	/* DEBUG - Need a check here for event-based rights. */
+
+#ifdef HAVE_DMAPI_RIGHTS
+	/* The "rights" vectors are stubs now anyway.  When they are
+	 * implemented then bhv locking will have to be sorted out.
+	 */
+
+	/* If the current right is not the same as it was when the event was
+	   created, first get back the original right.
+	*/
+
+	if (tdp->td_right != tdp->td_orig_right) {
+		fsys_vector = dm_fsys_vector(tdp->td_vp);
+		type = (tdp->td_type == DM_TDT_VFS ? DM_FSYS_OBJ : 0);
+
+		switch (tdp->td_orig_right) {
+		case DM_RIGHT_SHARED:
+			if (tdp->td_right == DM_RIGHT_EXCL) {
+				error = fsys_vector->downgrade_right(
+					tdp->td_vp, tdp->td_right, type);
+				if (!error)
+					break;
+				(void)fsys_vector->release_right(tdp->td_vp,
+					tdp->td_right, type);
+			}
+			(void)fsys_vector->request_right(tdp->td_vp,
+				tdp->td_right, type, DM_RR_WAIT,
+				tdp->td_orig_right);
+			break;
+
+		case DM_RIGHT_EXCL:
+			if (tdp->td_right == DM_RIGHT_SHARED) {
+				error = fsys_vector->upgrade_right(tdp->td_vp,
+					tdp->td_right, type);
+				if (!error)
+					break;
+				(void)fsys_vector->release_right(tdp->td_vp,
+					tdp->td_right, type);
+			}
+			(void)fsys_vector->request_right(tdp->td_vp,
+				tdp->td_right, type, DM_RR_WAIT,
+				tdp->td_orig_right);
+			break;
+		case DM_RIGHT_NULL:
+			break;
+		}
+	}
+#endif
+
+	/* We now have back the same level of rights as we had when the event
+	   was generated.  Now transfer the rights from being tdp-based back
+	   to thread-based.
+	*/
+
+	/* DEBUG - Add a call here to transfer rights back to thread-based. */
+
+	/* Finally, update the tdp so that we don't mess with the rights when
+	   we eventually call dm_put_tdp.
+	*/
+
+	tdp->td_right = DM_RIGHT_NULL;
+}
+
+
+/* This routine is only called by event threads.  The calls to dm_put_tdp
+   are not a deadlock risk here because this is an event thread, and it is
+   okay for such a thread to block on an induced destroy event.	 Okay, maybe
+   there is a slight risk; say that the event contains three vnodes all of
+   which have DM_RIGHT_EXCL, and say that we are at the dm_max_queued_msgs
+   limit, and that the first vnode is already unlinked.	 In that case the
+   destroy event will block waiting to be queued, and the application thread
+   could happen to reference one of the other locked vnodes.  Deadlock.
+*/
+
+void
+dm_evt_rele_tevp(
+	dm_tokevent_t	*tevp,
+	int		droprights)	/* non-zero, evt thread loses rights */
+{
+	dm_tokdata_t	*tdp;
+	unsigned long	lc;		/* lock cookie */
+
+	lc = mutex_spinlock(&tevp->te_lock);
+
+	/* If we are here without DM_TEF_FINAL set and with at least one
+	   application reference still remaining, then one of several
+	   possibilities is true:
+	   1. This is an asynchronous event which has been queued but has not
+	      yet been delivered, or which is in the process of being delivered.
+	   2. This is an unmount event (pseudo-asynchronous) yet to be
+	      delivered or in the process of being delivered.
+	   3. This event had DM_FLAGS_NDELAY specified, and the application
+	      has sent a dm_pending() reply for the event.
+	   4. This is a DM_EVENT_READ, DM_EVENT_WRITE, or DM_EVENT_TRUNCATE
+	      event and the user typed a Cntl-C.
+	   In all of these cases, the correct behavior is to leave the
+	   responsibility of releasing any rights to the application threads
+	   when they are done.
+	*/
+
+	if (tevp->te_app_ref > 0 && !(tevp->te_flags & DM_TEF_FINAL)) {
+		tevp->te_evt_ref--;
+		for (tdp = tevp->te_tdp; tdp; tdp = tdp->td_next) {
+			if (tdp->td_flags & DM_TDF_EVTREF) {
+				tdp->td_flags &= ~DM_TDF_EVTREF;
+				if (tdp->td_vcount == 0) {
+					tdp->td_vp = NULL;
+				}
+			}
+		}
+		mutex_spinunlock(&tevp->te_lock, lc);
+		return;		/* not the last thread */
+	}
+
+	/* If the application reference count is non-zero here, that can only
+	   mean that dm_respond_event() has been called, but the application
+	   still has one or more threads in the kernel that haven't let go of
+	   the tevp.  In these cases, the event thread must wait until all
+	   application threads have given up their references, and their
+	   rights to handles within the event.
+	*/
+
+	while (tevp->te_app_ref) {
+		sv_wait(&tevp->te_evt_queue, 1, &tevp->te_lock, lc);
+		lc = mutex_spinlock(&tevp->te_lock);
+	}
+
+	/* This thread is the last active thread using the token/event.	 Reset
+	   the rights of any vnode that was part of the original event back
+	   to their initial values before returning to the filesystem.	The
+	   exception is if the event failed (droprights is non-zero), in which
+	   case we chose to return to the filesystem with all rights released.
+	   Release the rights on any vnode that was not part of the original
+	   event.  Give up all remaining application vnode references
+	   regardless of whether or not the vnode was part of the original
+	   event.
+	*/
+
+	mutex_spinunlock(&tevp->te_lock, lc);
+
+	while ((tdp = tevp->te_tdp) != NULL) {
+		tevp->te_tdp = tdp->td_next;
+		if ((tdp->td_flags & DM_TDF_ORIG) &&
+		    (tdp->td_flags & DM_TDF_EVTREF) &&
+		    (!droprights)) {
+			dm_change_right(tdp);
+		}
+		dm_put_tdp(tdp);
+		kmem_cache_free(dm_tokdata_cachep, tdp);
+	}
+	spinlock_destroy(&tevp->te_lock);
+	sv_destroy(&tevp->te_evt_queue);
+	sv_destroy(&tevp->te_app_queue);
+	kfree(tevp);
+}
+
+
+/* dm_obj_ref_hold() is just a fancy way to get a vnode reference on an object
+   to hold it in kernel memory.
+*/
+
+int
+dm_obj_ref_hold(
+	dm_sessid_t	sid,
+	dm_token_t	token,
+	void		*hanp,
+	size_t		hlen)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp_by_token(sid, hanp, hlen, token, DM_TDT_VNO,
+		DM_RIGHT_NULL, DM_FG_STHREAD, &tdp);
+
+	/* The tdp is single-threaded, so no mutex lock needed for update. */
+
+	if (error == 0) {
+		if (tdp->td_flags & DM_TDF_HOLD) {	/* if already held */
+			error = EBUSY;
+		} else {
+			tdp->td_flags |= DM_TDF_HOLD;
+			tdp->td_vcount++;
+
+			fsys_vector = dm_fsys_vector(tdp->td_vp);
+			(void)fsys_vector->obj_ref_hold(tdp->td_vp);
+		}
+		dm_app_put_tdp(tdp);
+	}
+	return(error);
+}
+
+
+int
+dm_obj_ref_rele(
+	dm_sessid_t	sid,
+	dm_token_t	token,
+	void		*hanp,
+	size_t		hlen)
+{
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp_by_token(sid, hanp, hlen, token, DM_TDT_VNO,
+		DM_RIGHT_NULL, DM_FG_MUSTEXIST|DM_FG_STHREAD, &tdp);
+
+	/* The tdp is single-threaded, so no mutex lock needed for update. */
+
+	if (error == 0) {
+		if (!(tdp->td_flags & DM_TDF_HOLD)) {	/* if not held */
+			error = EACCES; /* use the DM_FG_MUSTEXIST errno */
+		} else {
+			tdp->td_flags &= ~DM_TDF_HOLD;
+			iput(LINVFS_GET_IP(tdp->td_vp));
+			tdp->td_vcount--;
+		}
+		dm_app_put_tdp(tdp);
+	}
+	return(error);
+}
+
+
+int
+dm_obj_ref_query_rvp(
+	dm_sessid_t	sid,
+	dm_token_t	token,
+	void		*hanp,
+	size_t		hlen,
+	int		*rvp)
+{
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp_by_token(sid, hanp, hlen, token, DM_TDT_VNO,
+		DM_RIGHT_NULL, DM_FG_DONTADD|DM_FG_STHREAD, &tdp);
+	if (error != 0)
+		return(error);
+
+	/* If the request is valid but the handle just isn't present in the
+	   event or the hold flag isn't set, return zero, else return one.
+	*/
+
+	if (tdp) {
+		if (tdp->td_flags & DM_TDF_HOLD) {	/* if held */
+			*rvp = 1;
+		} else {
+			*rvp = 0;
+		}
+		dm_app_put_tdp(tdp);
+	} else {
+		*rvp = 0;
+	}
+	return(0);
+}
+
+
+int
+dm_downgrade_right(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp_by_token(sid, hanp, hlen, token, DM_TDT_ANY,
+		DM_RIGHT_EXCL, DM_FG_MUSTEXIST|DM_FG_STHREAD, &tdp);
+	if (error != 0)
+		return(error);
+
+	/* Attempt the downgrade.  Filesystems which support rights but not
+	   the downgrading of rights will return ENOSYS.
+	*/
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->downgrade_right(tdp->td_vp, tdp->td_right,
+		(tdp->td_type == DM_TDT_VFS ? DM_FSYS_OBJ : 0));
+
+	/* The tdp is single-threaded, so no mutex lock needed for update. */
+
+	if (error == 0)
+		tdp->td_right = DM_RIGHT_SHARED;
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+int
+dm_query_right(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	dm_right_t	*rightp)
+{
+	dm_tokdata_t	*tdp;
+	dm_right_t	right;
+	int		error;
+
+	error = dm_app_get_tdp_by_token(sid, hanp, hlen, token, DM_TDT_ANY,
+		DM_RIGHT_NULL, DM_FG_DONTADD|DM_FG_STHREAD, &tdp);
+	if (error != 0)
+		return(error);
+
+	/* Get the current right and copy it to the caller.  The tdp is
+	   single-threaded, so no mutex lock is needed.	 If the tdp is not in
+	   the event we are supposed to return DM_RIGHT_NULL in order to be
+	   compatible with Veritas.
+	*/
+
+	if (tdp) {
+		right = tdp->td_right;
+		dm_app_put_tdp(tdp);
+	} else {
+		right = DM_RIGHT_NULL;
+	}
+	if (copy_to_user(rightp, &right, sizeof(right)))
+		return(EFAULT);
+	return(0);
+}
+
+
+int
+dm_release_right(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp_by_token(sid, hanp, hlen, token, DM_TDT_ANY,
+		DM_RIGHT_SHARED, DM_FG_MUSTEXIST|DM_FG_STHREAD, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->release_right(tdp->td_vp, tdp->td_right,
+		(tdp->td_type == DM_TDT_VFS ? DM_FSYS_OBJ : 0));
+
+	/* The tdp is single-threaded, so no mutex lock needed for update. */
+
+	if (error == 0) {
+		tdp->td_right = DM_RIGHT_NULL;
+		if (tdp->td_flags & DM_TDF_RIGHT) {
+			tdp->td_flags &= ~DM_TDF_RIGHT;
+			iput(LINVFS_GET_IP(tdp->td_vp));
+			tdp->td_vcount--;
+		}
+	}
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+int
+dm_request_right(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token,
+	u_int		flags,
+	dm_right_t	right)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp_by_token(sid, hanp, hlen, token, DM_TDT_ANY,
+		DM_RIGHT_NULL, DM_FG_STHREAD, &tdp);
+	if (error != 0)
+		return(error);
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->request_right(tdp->td_vp, tdp->td_right,
+		(tdp->td_type == DM_TDT_VFS ? DM_FSYS_OBJ : 0), flags, right);
+
+	/* The tdp is single-threaded, so no mutex lock is needed for update.
+
+	   If this is the first dm_request_right call for this vnode, then we
+	   need to bump the vnode reference count for two reasons.  First of
+	   all, it is supposed to be impossible for the file to disappear or
+	   for the filesystem to be unmounted while a right is held on a file;
+	   bumping the file's vnode reference count ensures this.  Second, if
+	   rights are ever actually implemented, it will most likely be done
+	   without changes to the on-disk inode, which means that we can't let
+	   the vnode become unreferenced while a right on it is held.
+	*/
+
+	if (error == 0) {
+		if (!(tdp->td_flags & DM_TDF_RIGHT)) {	/* if first call */
+			tdp->td_flags |= DM_TDF_RIGHT;
+			tdp->td_vcount++;
+			(void)fsys_vector->obj_ref_hold(tdp->td_vp);
+		}
+		tdp->td_right = right;
+	}
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
+
+
+int
+dm_upgrade_right(
+	dm_sessid_t	sid,
+	void		*hanp,
+	size_t		hlen,
+	dm_token_t	token)
+{
+	dm_fsys_vector_t *fsys_vector;
+	dm_tokdata_t	*tdp;
+	int		error;
+
+	error = dm_app_get_tdp_by_token(sid, hanp, hlen, token, DM_TDT_ANY,
+		DM_RIGHT_SHARED, DM_FG_MUSTEXIST|DM_FG_STHREAD, &tdp);
+	if (error != 0)
+		return(error);
+
+	/* If the object already has the DM_RIGHT_EXCL right, no need to
+	   attempt an upgrade.
+	*/
+
+	if (tdp->td_right == DM_RIGHT_EXCL) {
+		dm_app_put_tdp(tdp);
+		return(0);
+	}
+
+	/* Attempt the upgrade.	 Filesystems which support rights but not
+	   the upgrading of rights will return ENOSYS.
+	*/
+
+	fsys_vector = dm_fsys_vector(tdp->td_vp);
+	error = fsys_vector->upgrade_right(tdp->td_vp, tdp->td_right,
+		(tdp->td_type == DM_TDT_VFS ? DM_FSYS_OBJ : 0));
+
+	/* The tdp is single-threaded, so no mutex lock needed for update. */
+
+	if (error == 0)
+		tdp->td_right = DM_RIGHT_EXCL;
+
+	dm_app_put_tdp(tdp);
+	return(error);
+}
diff -uNrp linux-2.6.5/fs/xfs/dmapi/dmapi_session.c linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_session.c
--- linux-2.6.5/fs/xfs/dmapi/dmapi_session.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_session.c	2004-04-05 00:58:15.000000000 +0200
@@ -0,0 +1,1515 @@
+/*
+ * Copyright (c) 2000 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.	 Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+
+#include <linux/init.h>
+#include <linux/proc_fs.h>
+#include "xfs.h"
+#include "dmapi.h"
+#include "dmapi_kern.h"
+#include "dmapi_private.h"
+
+dm_session_t	*dm_sessions = NULL;	/* head of session list */
+u_int		dm_sessions_active = 0; /* # sessions currently active */
+dm_sessid_t	dm_next_sessid = 1;	/* next session ID to use */
+lock_t		dm_session_lock = SPIN_LOCK_UNLOCKED;/* lock for session list */
+
+dm_token_t	dm_next_token = 1;	/* next token ID to use */
+dm_sequence_t	dm_next_sequence = 1;	/* next sequence number to use */
+lock_t		dm_token_lock = SPIN_LOCK_UNLOCKED;/* dm_next_token/dm_next_sequence lock */
+
+int	dm_max_queued_msgs = 2048;	/* max # undelivered msgs/session */
+
+int	dm_hash_buckets = 1009;		/* prime -- number of buckets */
+
+#define DM_SHASH(sess,inodenum)	\
+		((sess)->sn_sesshash + do_mod((inodenum), dm_hash_buckets))
+
+
+#ifdef CONFIG_PROC_FS
+static int
+sessions_read_pfs(char *buffer, char **start, off_t offset,
+		 int count, int *eof, void *data)
+{
+	int len;
+	dm_session_t	*sessp = (dm_session_t*)data;
+
+#define CHKFULL if(len >= count) break;
+#define ADDBUF(a,b)	len += sprintf(buffer + len, a, b); CHKFULL;
+
+	len=0;
+	while(1){
+		ADDBUF("sessp=0x%p\n", sessp);
+		ADDBUF("sn_next=0x%p\n", sessp->sn_next);
+		ADDBUF("sn_sessid=%d\n", sessp->sn_sessid);
+		ADDBUF("sn_flags=%x\n", sessp->sn_flags);
+		ADDBUF("sn_qlock=%c\n", '?');
+		ADDBUF("sn_readerq=%c\n", '?');
+		ADDBUF("sn_writerq=%c\n", '?');
+		ADDBUF("sn_readercnt=%u\n", sessp->sn_readercnt);
+		ADDBUF("sn_writercnt=%u\n", sessp->sn_writercnt);
+
+		ADDBUF("sn_newq.eq_head=0x%p\n", sessp->sn_newq.eq_head);
+		ADDBUF("sn_newq.eq_tail=0x%p\n", sessp->sn_newq.eq_tail);
+		ADDBUF("sn_newq.eq_count=%d\n", sessp->sn_newq.eq_count);
+
+		ADDBUF("sn_delq.eq_head=0x%p\n", sessp->sn_delq.eq_head);
+		ADDBUF("sn_delq.eq_tail=0x%p\n", sessp->sn_delq.eq_tail);
+		ADDBUF("sn_delq.eq_count=%d\n", sessp->sn_delq.eq_count);
+
+		ADDBUF("sn_evt_writerq.eq_head=0x%p\n", sessp->sn_evt_writerq.eq_head);
+		ADDBUF("sn_evt_writerq.eq_tail=0x%p\n", sessp->sn_evt_writerq.eq_tail);
+		ADDBUF("sn_evt_writerq.eq_count=%d\n", sessp->sn_evt_writerq.eq_count);
+
+		ADDBUF("sn_info=\"%s\"\n", sessp->sn_info);
+
+		break;
+	}
+
+	if (offset >= len) {
+		*start = buffer;
+		*eof = 1;
+		return 0;
+	}
+	*start = buffer + offset;
+	if ((len -= offset) > count)
+		return count;
+	*eof = 1;
+
+	return len;
+}
+#endif
+
+
+/* Link a session to the end of the session list.  New sessions are always
+   added at the end of the list so that dm_enqueue_mount_event() doesn't
+   miss a session.  The caller must have obtained dm_session_lock before
+   calling this routine.
+*/
+
+static void
+link_session(
+	dm_session_t	*s)
+{
+	dm_session_t	*tmp;
+
+	if ((tmp = dm_sessions) == NULL) {
+		dm_sessions = s;
+	} else {
+		while (tmp->sn_next != NULL)
+			tmp = tmp->sn_next;
+		tmp->sn_next = s;
+	}
+	s->sn_next = NULL;
+	dm_sessions_active++;
+}
+
+
+/* Remove a session from the session list.  The caller must have obtained
+   dm_session_lock before calling this routine.	 unlink_session() should only
+   be used in situations where the session is known to be on the dm_sessions
+   list; otherwise it panics.
+*/
+
+static void
+unlink_session(
+	dm_session_t	*s)
+{
+	dm_session_t	*tmp;
+
+	if (dm_sessions == s) {
+		dm_sessions = dm_sessions->sn_next;
+	} else {
+		for (tmp = dm_sessions; tmp; tmp = tmp->sn_next) {
+			if (tmp->sn_next == s)
+				break;
+		}
+		if (tmp == NULL) {
+			panic("unlink_session: corrupt DMAPI session list, "
+				"dm_sessions %p, session %p\n",
+				dm_sessions, s);
+		}
+		tmp->sn_next = s->sn_next;
+	}
+	s->sn_next = NULL;
+	dm_sessions_active--;
+}
+
+
+/* Link an event to the end of an event queue.	The caller must have obtained
+   the session's sn_qlock before calling this routine.
+*/
+
+void
+dm_link_event(
+	dm_tokevent_t	*tevp,
+	dm_eventq_t	*queue)
+{
+	if (queue->eq_tail) {
+		queue->eq_tail->te_next = tevp;
+		queue->eq_tail = tevp;
+	} else {
+		queue->eq_head = queue->eq_tail = tevp;
+	}
+	tevp->te_next = NULL;
+	queue->eq_count++;
+}
+
+
+/* Remove an event from an event queue.	 The caller must have obtained the
+   session's sn_qlock before calling this routine.  dm_unlink_event() should
+   only be used in situations where the event is known to be on the queue;
+   otherwise it panics.
+*/
+
+void
+dm_unlink_event(
+	dm_tokevent_t	*tevp,
+	dm_eventq_t	*queue)
+{
+	dm_tokevent_t	*tmp;
+
+	if (queue->eq_head == tevp) {
+		queue->eq_head = tevp->te_next;
+		if (queue->eq_head == NULL)
+			queue->eq_tail = NULL;
+	} else {
+		tmp = queue->eq_head;
+		while (tmp && tmp->te_next != tevp)
+			tmp = tmp->te_next;
+		if (tmp == NULL) {
+			panic("dm_unlink_event: corrupt DMAPI queue %p, "
+				"tevp %p\n", queue, tevp);
+		}
+		tmp->te_next = tevp->te_next;
+		if (tmp->te_next == NULL)
+			queue->eq_tail = tmp;
+	}
+	tevp->te_next = NULL;
+	queue->eq_count--;
+}
+
+/* Link a regular file event to a hash bucket.	The caller must have obtained
+   the session's sn_qlock before calling this routine.
+   The tokevent must be for a regular file object--DM_TDT_REG.
+*/
+
+static void
+hash_event(
+	dm_session_t	*s,
+	dm_tokevent_t	*tevp)
+{
+	dm_sesshash_t	*sh;
+	xfs_ino_t	ino;
+
+	if (s->sn_sesshash == NULL)
+		s->sn_sesshash = kmem_zalloc(dm_hash_buckets * sizeof(dm_sesshash_t), KM_SLEEP);
+
+	ino = ((xfs_fid2_t*)&tevp->te_tdp->td_handle.ha_fid)->fid_ino;
+	sh = DM_SHASH(s, ino);
+
+#ifdef DM_SHASH_DEBUG
+	if (sh->h_next == NULL) {
+		s->sn_buckets_in_use++;
+		if (s->sn_buckets_in_use > s->sn_max_buckets_in_use)
+			s->sn_max_buckets_in_use++;
+	}
+	sh->maxlength++;
+	sh->curlength++;
+	sh->num_adds++;
+#endif
+
+	tevp->te_flags |= DM_TEF_HASHED;
+	tevp->te_hashnext = sh->h_next;
+	sh->h_next = tevp;
+}
+
+
+/* Remove a regular file event from a hash bucket.  The caller must have
+   obtained the session's sn_qlock before calling this routine.
+   The tokevent must be for a regular file object--DM_TDT_REG.
+*/
+
+static void
+unhash_event(
+	dm_session_t	*s,
+	dm_tokevent_t	*tevp)
+{
+	dm_sesshash_t	*sh;
+	dm_tokevent_t	*tmp;
+	xfs_ino_t	ino;
+
+	if (s->sn_sesshash == NULL)
+		return;
+
+	ino = ((xfs_fid2_t*)&tevp->te_tdp->td_handle.ha_fid)->fid_ino;
+	sh = DM_SHASH(s, ino);
+
+	if (sh->h_next == tevp) {
+		sh->h_next = tevp->te_hashnext; /* leap frog */
+	} else {
+		tmp = sh->h_next;
+		while (tmp->te_hashnext != tevp) {
+			tmp = tmp->te_hashnext;
+		}
+		tmp->te_hashnext = tevp->te_hashnext; /* leap frog */
+	}
+	tevp->te_hashnext = NULL;
+	tevp->te_flags &= ~DM_TEF_HASHED;
+
+#ifdef DM_SHASH_DEBUG
+	if (sh->h_next == NULL)
+		s->sn_buckets_in_use--;
+	sh->curlength--;
+	sh->num_dels++;
+#endif
+}
+
+
+/* Determine if this is a repeat event.	 The caller MUST be holding
+   the session lock.
+   The tokevent must be for a regular file object--DM_TDT_REG.
+   Returns:
+	0 == match not found
+	1 == match found
+*/
+
+static int
+repeated_event(
+	dm_session_t	*s,
+	dm_tokevent_t	*tevp)
+{
+	dm_sesshash_t	*sh;
+	dm_data_event_t *d_event1;
+	dm_data_event_t *d_event2;
+	dm_tokevent_t	*tevph;
+	xfs_ino_t	ino1;
+	xfs_ino_t	ino2;
+
+	if ((!s->sn_newq.eq_tail) && (!s->sn_delq.eq_tail)) {
+		return(0);
+	}
+	if (s->sn_sesshash == NULL) {
+		return(0);
+	}
+
+	ino1 = ((xfs_fid2_t*)&tevp->te_tdp->td_handle.ha_fid)->fid_ino;
+	sh = DM_SHASH(s, ino1);
+
+	if (sh->h_next == NULL) {
+		/* bucket is empty, no match here */
+		return(0);
+	}
+
+	d_event1 = (dm_data_event_t *)((char *)&tevp->te_msg + tevp->te_msg.ev_data.vd_offset);
+	tevph = sh->h_next;
+	while (tevph) {
+		/* find something with the same event type and handle type */
+		if ((tevph->te_msg.ev_type == tevp->te_msg.ev_type) &&
+		    (tevph->te_tdp->td_type == tevp->te_tdp->td_type)) {
+
+			ino2 = ((xfs_fid2_t*)&tevp->te_tdp->td_handle.ha_fid)->fid_ino;
+			d_event2 = (dm_data_event_t *)((char *)&tevph->te_msg + tevph->te_msg.ev_data.vd_offset);
+
+			/* If the two events are operating on the same file,
+			   and the same part of that file, then we have a
+			   match.
+			*/
+			if ((ino1 == ino2) &&
+			    (d_event2->de_offset == d_event1->de_offset) &&
+			    (d_event2->de_length == d_event1->de_length)) {
+				/* found a match */
+#ifdef DM_SHASH_DEBUG
+				sh->dup_hits++;
+#endif
+				return(1);
+			}
+		}
+		tevph = tevph->te_hashnext;
+	}
+
+	/* No match found */
+	return(0);
+}
+
+
+/* Return a pointer to a session given its session ID, or EINVAL if no session
+   has the session ID (per the DMAPI spec).  The caller must have obtained
+   dm_session_lock before calling this routine.
+*/
+
+static int
+dm_find_session(
+	dm_sessid_t	sid,
+	dm_session_t	**sessionpp)
+{
+	dm_session_t	*s;
+
+	for (s = dm_sessions; s; s = s->sn_next) {
+		if (s->sn_sessid == sid) {
+			*sessionpp = s;
+			return(0);
+		}
+	}
+	return(EINVAL);
+}
+
+
+/* Return a pointer to a locked session given its session ID.  '*lcp' is
+   used to obtain the session's sn_qlock.  Caller is responsible for eventually
+   unlocking it.
+*/
+
+int
+dm_find_session_and_lock(
+	dm_sessid_t	sid,
+	dm_session_t	**sessionpp,
+	unsigned long	*lcp)		/* addr of returned lock cookie */
+{
+	int		error;
+
+	for (;;) {
+		*lcp = mutex_spinlock(&dm_session_lock);
+
+		if ((error = dm_find_session(sid, sessionpp)) != 0) {
+			mutex_spinunlock(&dm_session_lock, *lcp);
+			return(error);
+		}
+		if (spin_trylock(&(*sessionpp)->sn_qlock)) {
+			nested_spinunlock(&dm_session_lock);
+			return(0);	/* success */
+		}
+
+		/* If the second lock is not available, drop the first and
+		   start over.	This gives the CPU a chance to process any
+		   interrupts, and also allows processes which want a sn_qlock
+		   for a different session to proceed.
+		*/
+
+		mutex_spinunlock(&dm_session_lock, *lcp);
+	}
+}
+
+
+/* Return a pointer to the event on the specified session's sn_delq which
+   contains the given token.  The caller must have obtained the session's
+   sn_qlock before calling this routine.
+*/
+
+static int
+dm_find_msg(
+	dm_session_t	*s,
+	dm_token_t	token,
+	dm_tokevent_t	**tevpp)
+{
+	dm_tokevent_t	*tevp;
+
+	if (token <= DM_INVALID_TOKEN)
+		return(EINVAL);
+
+	for (tevp = s->sn_delq.eq_head; tevp; tevp = tevp->te_next) {
+		if (tevp->te_msg.ev_token == token) {
+			*tevpp = tevp;
+			return(0);
+		}
+	}
+	return(ESRCH);
+}
+
+
+/* Given a session ID and token, find the tevp on the specified session's
+   sn_delq which corresponds to that session ID/token pair.  If a match is
+   found, lock the tevp's te_lock and return a pointer to the tevp.
+   '*lcp' is used to obtain the tevp's te_lock.	 The caller is responsible
+   for eventually unlocking it.
+*/
+
+int
+dm_find_msg_and_lock(
+	dm_sessid_t	sid,
+	dm_token_t	token,
+	dm_tokevent_t	**tevpp,
+	unsigned long	*lcp)		/* address of returned lock cookie */
+{
+	dm_session_t	*s;
+	int		error;
+
+	if ((error = dm_find_session_and_lock(sid, &s, lcp)) != 0)
+		return(error);
+
+	if ((error = dm_find_msg(s, token, tevpp)) != 0) {
+		mutex_spinunlock(&s->sn_qlock, *lcp);
+		return(error);
+	}
+	nested_spinlock(&(*tevpp)->te_lock);
+	nested_spinunlock(&s->sn_qlock);
+	return(0);
+}
+
+
+/* Create a new session, or resume an old session if one is given. */
+
+int
+dm_create_session(
+	dm_sessid_t	old,
+	char		*info,
+	dm_sessid_t	*new)
+{
+	dm_session_t	*s;
+	dm_sessid_t	sid;
+	char		sessinfo[DM_SESSION_INFO_LEN];
+	size_t		len;
+	int		error;
+	unsigned long		lc;		/* lock cookie */
+
+	len = strnlen_user(info, DM_SESSION_INFO_LEN-1);
+	if (copy_from_user(sessinfo, info, len))
+		return(EFAULT);
+	lc = mutex_spinlock(&dm_session_lock);
+	sid = dm_next_sessid++;
+	mutex_spinunlock(&dm_session_lock, lc);
+	if (copy_to_user(new, &sid, sizeof(sid)))
+		return(EFAULT);
+
+	if (old == DM_NO_SESSION) {
+		s = kmem_cache_alloc(dm_session_cachep, SLAB_KERNEL);
+		if (s == NULL) {
+			printk("%s/%d: kmem_cache_alloc(dm_session_cachep) returned NULL\n", __FUNCTION__, __LINE__);
+			return ENOMEM;
+		}
+		memset(s, 0, sizeof(*s));
+
+		sv_init(&s->sn_readerq, SV_DEFAULT, "dmreadq");
+		sv_init(&s->sn_writerq, SV_DEFAULT, "dmwritq");
+		spinlock_init(&s->sn_qlock, "sn_qlock");
+		lc = mutex_spinlock(&dm_session_lock);
+	} else {
+		lc = mutex_spinlock(&dm_session_lock);
+		if ((error = dm_find_session(old, &s)) != 0) {
+			mutex_spinunlock(&dm_session_lock, lc);
+			return(error);
+		}
+#ifdef CONFIG_PROC_FS
+		{
+		char buf[100];
+		sprintf(buf, DMAPI_DBG_PROCFS "/sessions/0x%p", s);
+		remove_proc_entry(buf, NULL);
+		}
+#endif
+		unlink_session(s);
+	}
+	memcpy(s->sn_info, sessinfo, len);
+	s->sn_info[len-1] = 0;		/* if not NULL, then now 'tis */
+	s->sn_sessid = sid;
+	link_session(s);
+#ifdef CONFIG_PROC_FS
+	{
+	char buf[100];
+	struct proc_dir_entry *entry;
+
+	sprintf(buf, DMAPI_DBG_PROCFS "/sessions/0x%p", s);
+	entry = create_proc_read_entry(buf, 0, 0, sessions_read_pfs, s);
+	entry->owner = THIS_MODULE;
+	}
+#endif
+	mutex_spinunlock(&dm_session_lock, lc);
+	return(0);
+}
+
+
+int
+dm_destroy_session(
+	dm_sessid_t	sid)
+{
+	dm_session_t	*s;
+	int		error;
+	unsigned long		lc;		/* lock cookie */
+
+	/* The dm_session_lock must be held until the session is unlinked. */
+
+	lc = mutex_spinlock(&dm_session_lock);
+
+	if ((error = dm_find_session(sid, &s)) != 0) {
+		mutex_spinunlock(&dm_session_lock, lc);
+		return(error);
+	}
+	nested_spinlock(&s->sn_qlock);
+
+	/* The session exists.	Check to see if it is still in use.  If any
+	   messages still exist on the sn_newq or sn_delq, or if any processes
+	   are waiting for messages to arrive on the session, then the session
+	   must not be destroyed.
+	*/
+
+	if (s->sn_newq.eq_head || s->sn_readercnt || s->sn_delq.eq_head) {
+		nested_spinunlock(&s->sn_qlock);
+		mutex_spinunlock(&dm_session_lock, lc);
+		return(EBUSY);
+	}
+
+#ifdef CONFIG_PROC_FS
+	{
+	char buf[100];
+	sprintf(buf, DMAPI_DBG_PROCFS "/sessions/0x%p", s);
+	remove_proc_entry(buf, NULL);
+	}
+#endif
+
+	/* The session is not in use.  Dequeue it from the session chain. */
+
+	unlink_session(s);
+	nested_spinunlock(&s->sn_qlock);
+	mutex_spinunlock(&dm_session_lock, lc);
+
+	/* Now clear the sessions's disposition registration, and then destroy
+	   the session structure.
+	*/
+
+	dm_clear_fsreg(s);
+
+	spinlock_destroy(&s->sn_qlock);
+	sv_destroy(&s->sn_readerq);
+	sv_destroy(&s->sn_writerq);
+	if (s->sn_sesshash)
+		kmem_free(s->sn_sesshash, dm_hash_buckets * sizeof(dm_sesshash_t));
+	kmem_cache_free(dm_session_cachep, s);
+	return(0);
+}
+
+
+/*
+ *  Return a list of all active sessions.
+ */
+
+int
+dm_getall_sessions(
+	u_int		nelem,
+	dm_sessid_t	*sidp,
+	u_int		*nelemp)
+{
+	dm_session_t	*s;
+	u_int		sesscnt;
+	dm_sessid_t	*sesslist;
+	unsigned long		lc;		/* lock cookie */
+	int		error;
+	int		i;
+
+	/* Loop until we can get the right amount of temp space, being careful
+	   not to hold a mutex during the allocation.  Usually only one trip.
+	*/
+
+	for (;;) {
+		if ((sesscnt = dm_sessions_active) == 0) {
+			/*if (suword(nelemp, 0))*/
+			if (put_user(0, nelemp))
+				return(EFAULT);
+			return(0);
+		}
+		sesslist = kmalloc(sesscnt * sizeof(*sidp), GFP_KERNEL);
+		if (sesslist == NULL) {
+			printk("%s/%d: kmalloc returned NULL\n", __FUNCTION__, __LINE__);
+			return ENOMEM;
+		}
+
+		lc = mutex_spinlock(&dm_session_lock);
+		if (sesscnt == dm_sessions_active)
+			break;
+
+		mutex_spinunlock(&dm_session_lock, lc);
+		kfree(sesslist);
+	}
+
+	/* Make a temp copy of the data, then release the mutex. */
+
+	for (i = 0, s = dm_sessions; i < sesscnt; i++, s = s->sn_next)
+		sesslist[i] = s->sn_sessid;
+
+	mutex_spinunlock(&dm_session_lock, lc);
+
+	/* Now copy the data to the user. */
+
+	if(put_user(sesscnt, nelemp)) {
+		error = EFAULT;
+	} else if (sesscnt > nelem) {
+		error = E2BIG;
+	} else if (copy_to_user(sidp, sesslist, sesscnt * sizeof(*sidp))) {
+		error = EFAULT;
+	} else {
+		error = 0;
+	}
+	kfree(sesslist);
+	return(error);
+}
+
+
+/*
+ *  Return the descriptive string associated with a session.
+ */
+
+int
+dm_query_session(
+	dm_sessid_t	sid,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp)
+{
+	dm_session_t	*s;		/* pointer to session given by sid */
+	int		len;		/* length of session info string */
+	int		error;
+	char		sessinfo[DM_SESSION_INFO_LEN];
+	unsigned long	lc;		/* lock cookie */
+
+	if ((error = dm_find_session_and_lock(sid, &s, &lc)) != 0)
+		return(error);
+
+	len = strlen(s->sn_info) + 1;	/* NULL terminated when created */
+	memcpy(sessinfo, s->sn_info, len);
+
+	mutex_spinunlock(&s->sn_qlock, lc);
+
+	/* Now that the mutex is released, copy the sessinfo to the user. */
+
+	if (put_user(len, rlenp)) {
+		error = EFAULT;
+	} else if (len > buflen) {
+		error = E2BIG;
+	} else if (copy_to_user(bufp, sessinfo, len)) {
+		error = EFAULT;
+	} else {
+		error = 0;
+	}
+	return(error);
+}
+
+
+/*
+ *  Return all of the previously delivered tokens (that is, their IDs)
+ *  for the given session.
+ */
+
+int
+dm_getall_tokens(
+	dm_sessid_t	sid,		/* session obtaining tokens from */
+	u_int		nelem,		/* size of tokenbufp */
+	dm_token_t	*tokenbufp,	/* buffer to copy token IDs to */
+	u_int		*nelemp)	/* return number copied to tokenbufp */
+{
+	dm_session_t	*s;		/* pointer to session given by sid */
+	dm_tokevent_t	*tevp;		/* event message queue traversal */
+	unsigned long	lc;		/* lock cookie */
+	int		tokcnt;
+	dm_token_t	*toklist;
+	int		error;
+	int		i;
+
+	/* Loop until we can get the right amount of temp space, being careful
+	   not to hold a mutex during the allocation.  Usually only one trip.
+	*/
+
+	for (;;) {
+		if ((error = dm_find_session_and_lock(sid, &s, &lc)) != 0)
+			return(error);
+		tokcnt = s->sn_delq.eq_count;
+		mutex_spinunlock(&s->sn_qlock, lc);
+
+		if (tokcnt == 0) {
+			/*if (suword(nelemp, 0))*/
+			if (put_user(0, nelemp))
+				return(EFAULT);
+			return(0);
+		}
+		toklist = kmalloc(tokcnt * sizeof(*tokenbufp), GFP_KERNEL);
+		if (toklist == NULL) {
+			printk("%s/%d: kmalloc returned NULL\n", __FUNCTION__, __LINE__);
+			return ENOMEM;
+		}
+
+		if ((error = dm_find_session_and_lock(sid, &s, &lc)) != 0) {
+			kfree(toklist);
+			return(error);
+		}
+
+		if (tokcnt == s->sn_delq.eq_count)
+			break;
+
+		mutex_spinunlock(&s->sn_qlock, lc);
+		kfree(toklist);
+	}
+
+	/* Make a temp copy of the data, then release the mutex. */
+
+	tevp = s->sn_delq.eq_head;
+	for (i = 0; i < tokcnt; i++, tevp = tevp->te_next)
+		toklist[i] = tevp->te_msg.ev_token;
+
+	mutex_spinunlock(&s->sn_qlock, lc);
+
+	/* Now copy the data to the user. */
+
+	if (put_user(tokcnt, nelemp)) {
+		error = EFAULT;
+	} else if (tokcnt > nelem) {
+		error = E2BIG;
+	} else if (copy_to_user(tokenbufp,toklist,tokcnt*sizeof(*tokenbufp))) {
+		error = EFAULT;
+	} else {
+		error = 0;
+	}
+	kfree(toklist);
+	return(error);
+}
+
+
+/*
+ *  Return the message identified by token.
+ */
+
+int
+dm_find_eventmsg(
+	dm_sessid_t	sid,
+	dm_token_t	token,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp)
+{
+	dm_tokevent_t	*tevp;		/* message identified by token */
+	int		msgsize;	/* size of message to copy out */
+	void		*msg;
+	int		error;
+	unsigned long	lc;		/* lock cookie */
+
+	/* Because some of the events (dm_data_event_t in particular) contain
+	   __u64 fields, we need to make sure that the buffer provided by the
+	   caller is aligned such that he can read those fields successfully.
+	*/
+
+	if (((__psint_t)bufp & (sizeof(__u64) - 1)) != 0)
+		return(EFAULT);
+
+	/* Allocate the right amount of temp space, being careful not to hold
+	   a mutex during the allocation.
+	*/
+
+	if ((error = dm_find_msg_and_lock(sid, token, &tevp, &lc)) != 0)
+		return(error);
+	msgsize = tevp->te_allocsize - offsetof(dm_tokevent_t, te_msg);
+	mutex_spinunlock(&tevp->te_lock, lc);
+
+	msg = kmalloc(msgsize, GFP_KERNEL);
+	if (msg == NULL) {
+		printk("%s/%d: kmalloc returned NULL\n", __FUNCTION__, __LINE__);
+		return ENOMEM;
+	}
+
+	if ((error = dm_find_msg_and_lock(sid, token, &tevp, &lc)) != 0) {
+		kfree(msg);
+		return(error);
+	}
+
+	/* Make a temp copy of the data, then release the mutex. */
+
+	memcpy(msg, &tevp->te_msg, msgsize);
+	mutex_spinunlock(&tevp->te_lock, lc);
+
+	/* Now copy the data to the user. */
+
+	if (put_user(msgsize,rlenp)) {
+		error = EFAULT;
+	} else if (msgsize > buflen) {		/* user buffer not big enough */
+		error = E2BIG;
+	} else if (copy_to_user( bufp, msg, msgsize )) {
+		error = EFAULT;
+	} else {
+		error = 0;
+	}
+	kfree(msg);
+	return(error);
+}
+
+
+int
+dm_move_event(
+	dm_sessid_t	srcsid,
+	dm_token_t	token,
+	dm_sessid_t	targetsid,
+	dm_token_t	*rtokenp)
+{
+	dm_session_t	*s1;
+	dm_session_t	*s2;
+	dm_tokevent_t	*tevp;
+	int		error;
+	unsigned long		lc;		/* lock cookie */
+	int		hash_it = 0;
+
+	lc = mutex_spinlock(&dm_session_lock);
+
+	if ((error = dm_find_session(srcsid, &s1)) != 0 ||
+	    (error = dm_find_session(targetsid, &s2)) != 0 ||
+	    (error = dm_find_msg(s1, token, &tevp)) != 0) {
+		mutex_spinunlock(&dm_session_lock, lc);
+		return(error);
+	}
+	dm_unlink_event(tevp, &s1->sn_delq);
+	if (tevp->te_flags & DM_TEF_HASHED) {
+		unhash_event(s1, tevp);
+		hash_it = 1;
+	}
+	dm_link_event(tevp, &s2->sn_delq);
+	if (hash_it)
+		hash_event(s2, tevp);
+	mutex_spinunlock(&dm_session_lock, lc);
+
+	if (copy_to_user(rtokenp, &token, sizeof(token)))
+		return(EFAULT);
+	return(0);
+}
+
+
+/* ARGSUSED */
+int
+dm_pending(
+	dm_sessid_t	sid,
+	dm_token_t	token,
+	dm_timestruct_t *delay)		/* unused */
+{
+	dm_tokevent_t	*tevp;
+	int		error;
+	unsigned long	lc;		/* lock cookie */
+
+	if ((error = dm_find_msg_and_lock(sid, token, &tevp, &lc)) != 0)
+		return(error);
+
+	tevp->te_flags |= DM_TEF_INTERMED;
+	if (tevp->te_evt_ref > 0)	/* if event generation threads exist */
+		sv_broadcast(&tevp->te_evt_queue);
+
+	mutex_spinunlock(&tevp->te_lock, lc);
+	return(0);
+}
+
+
+int
+dm_get_events(
+	dm_sessid_t	sid,
+	u_int		maxmsgs,
+	u_int		flags,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp)
+{
+	dm_session_t	*s;		/* pointer to session given by sid */
+	dm_tokevent_t	*tevp;		/* next event message on queue */
+	int		error;
+	unsigned long	lc1;		/* first lock cookie */
+	unsigned long	lc2 = 0;	/* second lock cookie */
+	int		totalsize;
+	int		msgsize;
+	dm_eventmsg_t	*prevmsg;
+	int		prev_msgsize = 0;
+	u_int		msgcnt;
+
+	/* Because some of the events (dm_data_event_t in particular) contain
+	   __u64 fields, we need to make sure that the buffer provided by the
+	   caller is aligned such that he can read those fields successfully.
+	*/
+
+	if (((__psint_t)bufp & (sizeof(__u64) - 1)) != 0)
+		return(EFAULT);
+
+	/* Find the indicated session and lock it. */
+
+	if ((error = dm_find_session_and_lock(sid, &s, &lc1)) != 0)
+		return(error);
+
+	/* Check for messages on sn_newq.  If there aren't any that haven't
+	   already been grabbed by another process, and if we are supposed to
+	   to wait until one shows up, then go to sleep interruptibly on the
+	   sn_readerq semaphore.  The session can't disappear out from under
+	   us as long as sn_readerq is non-zero.
+	*/
+
+	for (;;) {
+		int		rc;
+
+		for (tevp = s->sn_newq.eq_head; tevp; tevp = tevp->te_next) {
+			lc2 = mutex_spinlock(&tevp->te_lock);
+			if (!(tevp->te_flags & DM_TEF_LOCKED))
+				break;
+			mutex_spinunlock(&tevp->te_lock, lc2);
+		}
+		if (tevp)
+			break;		/* got one! */
+
+		if (!(flags & DM_EV_WAIT)) {
+			mutex_spinunlock(&s->sn_qlock, lc1);
+			return(EAGAIN);
+		}
+		s->sn_readercnt++;
+
+		sv_wait_sig(&s->sn_readerq, 1, &s->sn_qlock, lc1);
+		rc = signal_pending(current);
+
+		lc1 = mutex_spinlock(&s->sn_qlock);
+		s->sn_readercnt--;
+		if (rc) {	/* if signal was received */
+			mutex_spinunlock(&s->sn_qlock, lc1);
+			return(EINTR);
+		}
+	}
+
+	/* At least one message is available for delivery, and we have both the
+	   session lock and event lock.	 Mark the event so that it is not
+	   grabbed by other daemons, then drop both locks prior copying the
+	   data to the caller's buffer.	 Leaving the event on the queue in a
+	   marked state prevents both the session and the event from
+	   disappearing out from under us while we don't have the locks.
+	*/
+
+	tevp->te_flags |= DM_TEF_LOCKED;
+	mutex_spinunlock(&tevp->te_lock, lc2);	/* reverse cookie order */
+	mutex_spinunlock(&s->sn_qlock, lc1);
+
+	/* Continue to deliver messages until there are no more, the
+	   user's buffer becomes full, or we hit his maxmsgs limit.
+	*/
+
+	totalsize = 0;		/* total bytes transferred to the user */
+	prevmsg = NULL;
+	msgcnt = 0;
+
+	while (tevp) {
+		/* Compute the number of bytes to be moved, rounding up to an
+		   8-byte boundary so that any subsequent messages will also be
+		   aligned.
+		*/
+
+		msgsize = tevp->te_allocsize - offsetof(dm_tokevent_t, te_msg);
+		msgsize = (msgsize + sizeof(__u64) - 1) & ~(sizeof(__u64) - 1);
+		totalsize += msgsize;
+
+		/* If it fits, copy the message into the user's buffer and
+		   update his 'rlenp'.	Update the _link pointer for any
+		   previous message.
+		*/
+
+		if (totalsize > buflen) {	/* no more room */
+			error = E2BIG;
+		} else if (put_user(totalsize, rlenp)) {
+			error = EFAULT;
+		} else if (copy_to_user(bufp, &tevp->te_msg, msgsize)) {
+			error = EFAULT;
+		} else if (prevmsg && put_user(prev_msgsize, &prevmsg->_link)) {
+			error = EFAULT;
+		} else {
+			error = 0;
+		}
+
+		/* If an error occurred, just unmark the event and leave it on
+		   the queue for someone else.	Note that other daemons may
+		   have gone to sleep because this event was marked, so wake
+		   them up.  Also, if at least one message has already been
+		   delivered, then an error here is not really an error.
+		*/
+
+		lc1 = mutex_spinlock(&s->sn_qlock);
+		lc2 = mutex_spinlock(&tevp->te_lock);
+		tevp->te_flags &= ~DM_TEF_LOCKED;	/* drop the mark */
+
+		if (error) {
+			if (s->sn_readercnt)
+				sv_signal(&s->sn_readerq);
+
+			mutex_spinunlock(&tevp->te_lock, lc2);	/* rev. order */
+			mutex_spinunlock(&s->sn_qlock, lc1);
+			if (prevmsg)
+				return(0);
+			if (error == E2BIG && put_user(totalsize,rlenp))
+				error = EFAULT;
+			return(error);
+		}
+
+		/* The message was successfully delivered.  Unqueue it. */
+
+		dm_unlink_event(tevp, &s->sn_newq);
+
+		/* Wake up the first of any processes waiting for room on the
+		   sn_newq.
+		*/
+
+		if (s->sn_writercnt)
+			sv_signal(&s->sn_writerq);
+
+		/* If the message is synchronous, add it to the sn_delq while
+		   still holding the lock.  If it is asynchronous, free it.
+		*/
+
+		if (tevp->te_msg.ev_token != DM_INVALID_TOKEN) { /* synch */
+			dm_link_event(tevp, &s->sn_delq);
+			mutex_spinunlock(&tevp->te_lock, lc2);
+		} else {
+			tevp->te_flags |= DM_TEF_FINAL;
+			if (tevp->te_flags & DM_TEF_HASHED)
+				unhash_event(s, tevp);
+			mutex_spinunlock(&tevp->te_lock, lc2);
+			dm_put_tevp(tevp, NULL);/* can't cause destroy events */
+		}
+
+		/* Update our notion of where we are in the user's buffer.  If
+		   he doesn't want any more messages, then stop.
+		*/
+
+		prevmsg = (dm_eventmsg_t *)bufp;
+		prev_msgsize = msgsize;
+		bufp = (char *)bufp + msgsize;
+
+		msgcnt++;
+		if (maxmsgs && msgcnt >= maxmsgs) {
+			mutex_spinunlock(&s->sn_qlock, lc1);
+			break;
+		}
+
+		/* While still holding the sn_qlock,  see if any additional
+		   messages are available for delivery.
+		*/
+
+		for (tevp = s->sn_newq.eq_head; tevp; tevp = tevp->te_next) {
+			lc2 = mutex_spinlock(&tevp->te_lock);
+			if (!(tevp->te_flags & DM_TEF_LOCKED)) {
+				tevp->te_flags |= DM_TEF_LOCKED;
+				mutex_spinunlock(&tevp->te_lock, lc2);
+				break;
+			}
+			mutex_spinunlock(&tevp->te_lock, lc2);
+		}
+		mutex_spinunlock(&s->sn_qlock, lc1);
+	}
+	return(0);
+}
+
+
+/*
+ *  Remove an event message from the delivered queue, set the returned
+ *  error where the event generator wants it, and wake up the generator.
+ *  Also currently have the user side release any locks it holds...
+ */
+
+/* ARGSUSED */
+int
+dm_respond_event(
+	dm_sessid_t	sid,
+	dm_token_t	token,
+	dm_response_t	response,
+	int		reterror,
+	size_t		buflen,		/* unused */
+	void		*respbufp)	/* unused */
+{
+	dm_session_t	*s;		/* pointer to session given by sid */
+	dm_tokevent_t	*tevp;		/* event message queue traversal */
+	int		error;
+	unsigned long	lc;		/* lock cookie */
+
+	/* Sanity check the input parameters. */
+
+	switch (response) {
+	case DM_RESP_CONTINUE:	/* continue must have reterror == 0 */
+		if (reterror != 0)
+			return(EINVAL);
+		break;
+	case DM_RESP_ABORT:	/* abort must have errno set */
+		if (reterror <= 0)
+			return(EINVAL);
+		break;
+	case DM_RESP_DONTCARE:
+		if (reterror > 0)
+			return(EINVAL);
+		reterror = -1;	/* to distinguish DM_RESP_DONTCARE */
+		break;
+	default:
+		return(EINVAL);
+	}
+
+	/* Hold session lock until the event is unqueued. */
+
+	if ((error = dm_find_session_and_lock(sid, &s, &lc)) != 0)
+		return(error);
+
+	if ((error = dm_find_msg(s, token, &tevp)) != 0) {
+		mutex_spinunlock(&s->sn_qlock, lc);
+		return(error);
+	}
+	nested_spinlock(&tevp->te_lock);
+
+	if (reterror == -1 && tevp->te_msg.ev_type != DM_EVENT_MOUNT) {
+		error = EINVAL;
+		nested_spinunlock(&tevp->te_lock);
+		mutex_spinunlock(&s->sn_qlock, lc);
+	} else {
+		dm_unlink_event(tevp, &s->sn_delq);
+		if (tevp->te_flags & DM_TEF_HASHED)
+			unhash_event(s, tevp);
+		tevp->te_reply = reterror;
+		tevp->te_flags |= DM_TEF_FINAL;
+		if (tevp->te_evt_ref)
+			sv_broadcast(&tevp->te_evt_queue);
+		nested_spinunlock(&tevp->te_lock);
+		mutex_spinunlock(&s->sn_qlock, lc);
+		error = 0;
+
+		/* Absolutely no locks can be held when calling dm_put_tevp! */
+
+		dm_put_tevp(tevp, NULL);  /* this can generate destroy events */
+	}
+	return(error);
+}
+
+
+/* Queue the filled in event message pointed to by tevp on the session s, and
+   (if a synchronous event) wait for the reply from the DMAPI application.
+   The caller MUST be holding the session lock before calling this routine!
+   The session lock is always released upon exit.
+   Returns:
+	 -1 == don't care
+	  0 == success (or async event)
+	> 0 == errno describing reason for failure
+*/
+
+static int
+dm_enqueue(
+	dm_session_t	*s,
+	unsigned long	lc,		/* input lock cookie */
+	dm_tokevent_t	*tevp,		/* in/out parameter */
+	int		sync,
+	int		flags,
+	int		interruptable)
+{
+	int		is_unmount = 0;
+	int		is_hashable = 0;
+	int		reply;
+
+	/* If the caller isn't planning to stick around for the result
+	   and this request is identical to one that is already on the
+	   queues then just give the caller an EAGAIN.	Release the
+	   session lock before returning.
+
+	   We look only at NDELAY requests with an event type of READ,
+	   WRITE, or TRUNCATE on objects that are regular files.
+	*/
+
+	if ((flags & DM_FLAGS_NDELAY) && DM_EVENT_RDWRTRUNC(tevp) &&
+	    (tevp->te_tdp->td_type == DM_TDT_REG)) {
+		if (repeated_event(s, tevp)) {
+			mutex_spinunlock(&s->sn_qlock, lc);
+			return(EAGAIN);
+		}
+		is_hashable = 1;
+	}
+
+	if (tevp->te_msg.ev_type == DM_EVENT_UNMOUNT)
+		is_unmount = 1;
+
+	/* Check for room on sn_newq.  If there is no room for new messages,
+	   then go to sleep on the sn_writerq semaphore.  The
+	   session cannot disappear out from under us as long as sn_writercnt
+	   is non-zero.
+	*/
+
+	while (s->sn_newq.eq_count >= dm_max_queued_msgs) {	/* no room */
+		s->sn_writercnt++;
+		dm_link_event(tevp, &s->sn_evt_writerq);
+		if (interruptable) {
+			sv_wait_sig(&s->sn_writerq, 1, &s->sn_qlock, lc);
+			if (signal_pending(current)) {
+				s->sn_writercnt--;
+				return(EINTR);
+			}
+		} else {
+			sv_wait(&s->sn_writerq, 1, &s->sn_qlock, lc);
+		}
+		lc = mutex_spinlock(&s->sn_qlock);
+		s->sn_writercnt--;
+		dm_unlink_event(tevp, &s->sn_evt_writerq);
+	}
+
+	/* Assign a sequence number and token to the event and bump the
+	   application reference count by one.	We don't need 'te_lock' here
+	   because this thread is still the only thread that can see the event.
+	*/
+
+	nested_spinlock(&dm_token_lock);
+	tevp->te_msg.ev_sequence = dm_next_sequence++;
+	if (sync) {
+		tevp->te_msg.ev_token = dm_next_token++;
+	} else {
+		tevp->te_msg.ev_token = DM_INVALID_TOKEN;
+	}
+	nested_spinunlock(&dm_token_lock);
+
+	tevp->te_app_ref++;
+
+	/* Room exists on the sn_newq queue, so add this request.  If the
+	   queue was previously empty, wake up the first of any processes
+	   that are waiting for an event.
+	*/
+
+	dm_link_event(tevp, &s->sn_newq);
+	if (is_hashable)
+		hash_event(s, tevp);
+
+	if (s->sn_readercnt)
+		sv_signal(&s->sn_readerq);
+
+	mutex_spinunlock(&s->sn_qlock, lc);
+
+	/* Now that the message is queued, processes issuing asynchronous
+	   events or DM_EVENT_UNMOUNT events are ready to continue.
+	*/
+
+	if (!sync || is_unmount)
+		return(0);
+
+	/* Synchronous requests wait until a final reply is received.  If the
+	   caller supplied the DM_FLAGS_NDELAY flag, the process will return
+	   EAGAIN if dm_pending() sets DM_TEF_INTERMED.	 We also let users
+	   Cntl-C out of a read, write, and truncate requests.
+	*/
+
+	lc = mutex_spinlock(&tevp->te_lock);
+
+	while (!(tevp->te_flags & DM_TEF_FINAL)) {
+		if ((tevp->te_flags & DM_TEF_INTERMED) &&
+		    (flags & DM_FLAGS_NDELAY)) {
+			mutex_spinunlock(&tevp->te_lock, lc);
+			return(EAGAIN);
+		}
+		if (tevp->te_msg.ev_type == DM_EVENT_READ ||
+		    tevp->te_msg.ev_type == DM_EVENT_WRITE ||
+		    tevp->te_msg.ev_type == DM_EVENT_TRUNCATE) {
+			sv_wait_sig(&tevp->te_evt_queue, 1, &tevp->te_lock, lc);
+			if (signal_pending(current)){
+				return(EINTR);
+			}
+		} else {
+			sv_wait(&tevp->te_evt_queue, 1, &tevp->te_lock, lc);
+		}
+		lc = mutex_spinlock(&tevp->te_lock);
+	}
+
+	/* Return both the tevp and the reply which was stored in the tevp by
+	   dm_respond_event.  The tevp structure has already been removed from
+	   the reply queue by this point in dm_respond_event().
+	*/
+
+	reply = tevp->te_reply;
+	mutex_spinunlock(&tevp->te_lock, lc);
+	return(reply);
+}
+
+
+/* The filesystem is guaranteed to stay mounted while this event is
+   outstanding.
+*/
+
+int
+dm_enqueue_normal_event(
+	vfs_t		*vfsp,
+	dm_tokevent_t	*tevp,
+	int		flags)
+{
+	dm_session_t	*s;
+	int		error;
+	int		sync;
+	unsigned long	lc;		/* lock cookie */
+
+	switch (tevp->te_msg.ev_type) {
+	case DM_EVENT_READ:
+	case DM_EVENT_WRITE:
+	case DM_EVENT_TRUNCATE:
+	case DM_EVENT_PREUNMOUNT:
+	case DM_EVENT_UNMOUNT:
+	case DM_EVENT_NOSPACE:
+	case DM_EVENT_CREATE:
+	case DM_EVENT_REMOVE:
+	case DM_EVENT_RENAME:
+	case DM_EVENT_SYMLINK:
+	case DM_EVENT_LINK:
+	case DM_EVENT_DEBUT:		/* not currently supported */
+		sync = 1;
+		break;
+
+	case DM_EVENT_DESTROY:
+	case DM_EVENT_POSTCREATE:
+	case DM_EVENT_POSTREMOVE:
+	case DM_EVENT_POSTRENAME:
+	case DM_EVENT_POSTSYMLINK:
+	case DM_EVENT_POSTLINK:
+	case DM_EVENT_ATTRIBUTE:
+	case DM_EVENT_CLOSE:		/* not currently supported */
+	case DM_EVENT_CANCEL:		/* not currently supported */
+		sync = 0;
+		break;
+
+	default:
+		return(EIO);		/* garbage event number */
+	}
+
+	/* Wait until a session selects disposition for the event.  The session
+	   is locked upon return from dm_waitfor_disp_session().
+	*/
+
+	if ((error = dm_waitfor_disp_session(vfsp, tevp, &s, &lc)) != 0)
+		return(error);
+
+	return(dm_enqueue(s, lc, tevp, sync, flags, 0));
+}
+
+
+/* Traverse the session list checking for sessions with the WANTMOUNT flag
+   set.	 When one is found, send it the message.  Possible responses to the
+   message are one of DONTCARE, CONTINUE, or ABORT.  The action taken in each
+   case is:
+	DONTCARE (-1)  - Send the event to the next session with WANTMOUNT set
+	CONTINUE ( 0) - Proceed with the mount, errno zero.
+	ABORT	 (>0) - Fail the mount, return the returned errno.
+
+   The mount request is sent to sessions in ascending session ID order.
+   Since the session list can change dramatically while this process is
+   sleeping in dm_enqueue(), this routine must use session IDs rather than
+   session pointers when keeping track of where it is in the list.  Since
+   new sessions are always added at the end of the queue, and have increasing
+   session ID values, we don't have to worry about missing any session.
+*/
+
+int
+dm_enqueue_mount_event(
+	vfs_t		*vfsp,
+	dm_tokevent_t	*tevp)
+{
+	dm_session_t	*s;
+	dm_sessid_t	sid;
+	int		error;
+	unsigned long	lc;		/* lock cookie */
+
+	/* Make the mounting filesystem visible to other DMAPI calls. */
+
+	if ((error = dm_add_fsys_entry(vfsp, tevp)) != 0){
+		return(error);
+	}
+
+	/* Walk through the session list presenting the mount event to each
+	   session that is interested until a session accepts or rejects it,
+	   or until all sessions ignore it.
+	*/
+
+	for (sid = DM_NO_SESSION, error = -1; error < 0; sid = s->sn_sessid) {
+
+		lc = mutex_spinlock(&dm_session_lock);
+		for (s = dm_sessions; s; s = s->sn_next) {
+			if (s->sn_sessid > sid && s->sn_flags & DM_SN_WANTMOUNT) {
+				nested_spinlock(&s->sn_qlock);
+				nested_spinunlock(&dm_session_lock);
+				break;
+			}
+		}
+		if (s == NULL) {
+			mutex_spinunlock(&dm_session_lock, lc);
+			break;		/* noone wants it; proceed with mount */
+		}
+		error = dm_enqueue(s, lc, tevp, 1, 0, 0);
+	}
+
+	/* If the mount will be allowed to complete, then update the fsrp entry
+	   accordingly.	 If the mount is to be aborted, remove the fsrp entry.
+	*/
+
+	if (error <= 0) {
+		dm_change_fsys_entry(vfsp, DM_STATE_MOUNTED);
+		error = 0;
+	} else {
+		dm_remove_fsys_entry(vfsp);
+	}
+	return(error);
+}
+
+int
+dm_enqueue_sendmsg_event(
+	dm_sessid_t	targetsid,
+	dm_tokevent_t	*tevp,
+	int		sync)
+{
+	dm_session_t	*s;
+	int		error;
+	unsigned long	lc;		/* lock cookie */
+
+	if ((error = dm_find_session_and_lock(targetsid, &s, &lc)) != 0)
+		return(error);
+
+	return(dm_enqueue(s, lc, tevp, sync, 0, 1));
+}
+
+
+dm_token_t
+dm_enqueue_user_event(
+	dm_sessid_t	sid,
+	dm_tokevent_t	*tevp,
+	dm_token_t	*tokenp)
+{
+	dm_session_t	*s;
+	int		error;
+	unsigned long	lc;		/* lock cookie */
+
+	/* Atomically find and lock the session whose session id is 'sid'. */
+
+	if ((error = dm_find_session_and_lock(sid, &s, &lc)) != 0)
+		return(error);
+
+	/* Assign a sequence number and token to the event, bump the
+	   application reference count by one, and decrement the event
+	   count because the caller gives up all ownership of the event.
+	   We don't need 'te_lock' here because this thread is still the
+	   only thread that can see the event.
+	*/
+
+	nested_spinlock(&dm_token_lock);
+	tevp->te_msg.ev_sequence = dm_next_sequence++;
+	*tokenp = tevp->te_msg.ev_token = dm_next_token++;
+	nested_spinunlock(&dm_token_lock);
+
+	tevp->te_flags &= ~(DM_TEF_INTERMED|DM_TEF_FINAL);
+	tevp->te_app_ref++;
+	tevp->te_evt_ref--;
+
+	/* Add the request to the tail of the sn_delq.	Now it's visible. */
+
+	dm_link_event(tevp, &s->sn_delq);
+	mutex_spinunlock(&s->sn_qlock, lc);
+
+	return(0);
+}
diff -uNrp linux-2.6.5/fs/xfs/dmapi/dmapi_sysent.c linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_sysent.c
--- linux-2.6.5/fs/xfs/dmapi/dmapi_sysent.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_sysent.c	2004-04-05 00:58:15.000000000 +0200
@@ -0,0 +1,783 @@
+/*
+ * Copyright (c) 2000 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.	 Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+
+/* Data Migration API (DMAPI)
+ */
+
+
+/* We're using MISC_MAJOR / MISC_DYNAMIC_MINOR. */
+
+#include <linux/version.h>
+#include <linux/kernel.h>
+#include <linux/miscdevice.h>
+#include <linux/major.h>
+#include <linux/init.h>
+#include <linux/proc_fs.h>
+#include <linux/module.h>
+
+#include <asm/uaccess.h>
+
+#include "xfs.h"
+#include "dmapi.h"
+#include "dmapi_kern.h"
+#include "dmapi_private.h"
+
+kmem_cache_t	*dm_fsreg_cachep = NULL;
+kmem_cache_t	*dm_tokdata_cachep = NULL;
+kmem_cache_t	*dm_session_cachep = NULL;
+
+extern void __init xfs_dm_init(void);
+extern void __exit xfs_dm_exit(void);
+
+static int
+dmapi_ioctl(struct inode *inode, struct file *file, unsigned int cmd,
+	    unsigned long arg)
+{
+	sys_dmapi_args_t kargs;
+	sys_dmapi_args_t *uap = &kargs;
+	int error = 0;
+	int rvp = -ENOSYS;
+	int use_rvp = 0;
+
+	if (!capable(CAP_MKNOD))
+		return(-EPERM);
+
+	if( copy_from_user( &kargs, (sys_dmapi_args_t*)arg,
+			   sizeof(sys_dmapi_args_t) ) )
+		return -EFAULT;
+
+	switch (cmd) {
+	case DM_CLEAR_INHERIT:
+		error = dm_clear_inherit(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(dm_attrname_t *) DM_Parg(uap,5));/* attrnamep */
+		break;
+	case DM_CREATE_BY_HANDLE:
+		error = dm_create_by_handle(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* dirhanp */
+				(size_t)	DM_Uarg(uap,3), /* dirhlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(void *)	DM_Parg(uap,5), /* hanp */
+				(size_t)	DM_Uarg(uap,6), /* hlen */
+				(char *)	DM_Parg(uap,7));/* cname */
+		break;
+	case DM_CREATE_SESSION:
+		error = dm_create_session(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* oldsid */
+				(char *)	DM_Parg(uap,2), /* sessinfop */
+				(dm_sessid_t *) DM_Parg(uap,3));/* newsidp */
+		break;
+	case DM_CREATE_USEREVENT:
+		error = dm_create_userevent(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(size_t)	DM_Uarg(uap,2), /* msglen */
+				(void *)	DM_Parg(uap,3), /* msgdatap */
+				(dm_token_t *)	DM_Parg(uap,4));/* tokenp */
+		break;
+	case DM_DESTROY_SESSION:
+		error = dm_destroy_session(
+				(dm_sessid_t)	DM_Uarg(uap,1));/* sid */
+		break;
+	case DM_DOWNGRADE_RIGHT:
+		error = dm_downgrade_right(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4));/* token */
+		break;
+	case DM_FD_TO_HANDLE:
+		error = dm_fd_to_hdl(
+				(int)		DM_Uarg(uap,1), /* fd */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t *)	DM_Parg(uap,3));/* hlenp */
+		break;
+	case DM_FIND_EVENTMSG:
+		error = dm_find_eventmsg(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(dm_token_t)	DM_Uarg(uap,2), /* token */
+				(size_t)	DM_Uarg(uap,3), /* buflen */
+				(void *)	DM_Parg(uap,4), /* bufp */
+				(size_t *)	DM_Parg(uap,5));/* rlenp */
+		break;
+	case DM_GET_ALLOCINFO:
+		use_rvp = 1;
+		error = dm_get_allocinfo_rvp(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(dm_off_t *)	DM_Parg(uap,5), /* offp */
+				(u_int)		DM_Uarg(uap,6), /* nelem */
+				(dm_extent_t *) DM_Parg(uap,7), /* extentp */
+				(u_int *)	DM_Parg(uap,8), /* nelemp */
+						&rvp);
+		break;
+	case DM_GET_BULKALL:
+		use_rvp = 1;
+		error = dm_get_bulkall_rvp(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(u_int)		DM_Uarg(uap,5), /* mask */
+				(dm_attrname_t *) DM_Parg(uap,6),/* attrnamep */
+				(dm_attrloc_t *) DM_Parg(uap,7),/* locp */
+				(size_t)	DM_Uarg(uap,8), /* buflen */
+				(void *)	DM_Parg(uap,9), /* bufp */
+				(size_t *)	DM_Parg(uap,10),/* rlenp */
+						&rvp);
+		break;
+	case DM_GET_BULKATTR:
+		use_rvp = 1;
+		error = dm_get_bulkattr_rvp(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(u_int)		DM_Uarg(uap,5), /* mask */
+				(dm_attrloc_t *)DM_Parg(uap,6), /* locp */
+				(size_t)	DM_Uarg(uap,7), /* buflen */
+				(void *)	DM_Parg(uap,8), /* bufp */
+				(size_t *)	DM_Parg(uap,9), /* rlenp */
+						&rvp);
+		break;
+	case DM_GET_CONFIG:
+		error = dm_get_config(
+				(void *)	DM_Parg(uap,1), /* hanp */
+				(size_t)	DM_Uarg(uap,2), /* hlen */
+				(dm_config_t)	DM_Uarg(uap,3), /* flagname */
+				(dm_size_t *)	DM_Parg(uap,4));/* retvalp */
+		break;
+	case DM_GET_CONFIG_EVENTS:
+		error = dm_get_config_events(
+				(void *)	DM_Parg(uap,1), /* hanp */
+				(size_t)	DM_Uarg(uap,2), /* hlen */
+				(u_int)		DM_Uarg(uap,3), /* nelem */
+				(dm_eventset_t *) DM_Parg(uap,4),/* eventsetp */
+				(u_int *)	DM_Parg(uap,5));/* nelemp */
+		break;
+	case DM_GET_DIOINFO:
+		error = dm_get_dioinfo(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(dm_dioinfo_t *)DM_Parg(uap,5));/* diop */
+		break;
+	case DM_GET_DIRATTRS:
+		use_rvp = 1;
+		error = dm_get_dirattrs_rvp(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(u_int)		DM_Uarg(uap,5), /* mask */
+				(dm_attrloc_t *)DM_Parg(uap,6), /* locp */
+				(size_t)	DM_Uarg(uap,7), /* buflen */
+				(void *)	DM_Parg(uap,8), /* bufp */
+				(size_t *)	DM_Parg(uap,9), /* rlenp */
+						&rvp);
+		break;
+	case DM_GET_DMATTR:
+		error = dm_get_dmattr(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(dm_attrname_t *) DM_Parg(uap,5),/* attrnamep */
+				(size_t)	DM_Uarg(uap,6), /* buflen */
+				(void *)	DM_Parg(uap,7), /* bufp */
+				(size_t *)	DM_Parg(uap,8));/* rlenp */
+
+		break;
+	case DM_GET_EVENTLIST:
+		error = dm_get_eventlist(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(u_int)		DM_Uarg(uap,5), /* nelem */
+				(dm_eventset_t *) DM_Parg(uap,6),/* eventsetp */
+				(u_int *)	DM_Parg(uap,7));/* nelemp */
+		break;
+	case DM_GET_EVENTS:
+		error = dm_get_events(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(u_int)		DM_Uarg(uap,2), /* maxmsgs */
+				(u_int)		DM_Uarg(uap,3), /* flags */
+				(size_t)	DM_Uarg(uap,4), /* buflen */
+				(void *)	DM_Parg(uap,5), /* bufp */
+				(size_t *)	DM_Parg(uap,6));/* rlenp */
+		break;
+	case DM_GET_FILEATTR:
+		error = dm_get_fileattr(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(u_int)		DM_Uarg(uap,5), /* mask */
+				(dm_stat_t *)	DM_Parg(uap,6));/* statp */
+		break;
+	case DM_GET_MOUNTINFO:
+		error = dm_get_mountinfo(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(size_t)	DM_Uarg(uap,5), /* buflen */
+				(void *)	DM_Parg(uap,6), /* bufp */
+				(size_t *)	DM_Parg(uap,7));/* rlenp */
+		break;
+	case DM_GET_REGION:
+		error = dm_get_region(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(u_int)		DM_Uarg(uap,5), /* nelem */
+				(dm_region_t *) DM_Parg(uap,6), /* regbufp */
+				(u_int *)	DM_Parg(uap,7));/* nelemp */
+		break;
+	case DM_GETALL_DISP:
+		error = dm_getall_disp(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(size_t)	DM_Uarg(uap,2), /* buflen */
+				(void *)	DM_Parg(uap,3), /* bufp */
+				(size_t *)	DM_Parg(uap,4));/* rlenp */
+		break;
+	case DM_GETALL_DMATTR:
+		error = dm_getall_dmattr(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(size_t)	DM_Uarg(uap,5), /* buflen */
+				(void *)	DM_Parg(uap,6), /* bufp */
+				(size_t *)	DM_Parg(uap,7));/* rlenp */
+		break;
+	case DM_GETALL_INHERIT:
+		error = dm_getall_inherit(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(u_int)		DM_Uarg(uap,5), /* nelem */
+				(dm_inherit_t *)DM_Parg(uap,6), /* inheritbufp*/
+				(u_int *)	DM_Parg(uap,7));/* nelemp */
+		break;
+	case DM_GETALL_SESSIONS:
+		error = dm_getall_sessions(
+				(u_int)		DM_Uarg(uap,1), /* nelem */
+				(dm_sessid_t *) DM_Parg(uap,2), /* sidbufp */
+				(u_int *)	DM_Parg(uap,3));/* nelemp */
+		break;
+	case DM_GETALL_TOKENS:
+		error = dm_getall_tokens(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(u_int)		DM_Uarg(uap,2), /* nelem */
+				(dm_token_t *)	DM_Parg(uap,3), /* tokenbufp */
+				(u_int *)	DM_Parg(uap,4));/* nelemp */
+		break;
+	case DM_INIT_ATTRLOC:
+		error = dm_init_attrloc(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(dm_attrloc_t *) DM_Parg(uap,5));/* locp */
+		break;
+	case DM_MKDIR_BY_HANDLE:
+		error = dm_mkdir_by_handle(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* dirhanp */
+				(size_t)	DM_Uarg(uap,3), /* dirhlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(void *)	DM_Parg(uap,5), /* hanp */
+				(size_t)	DM_Uarg(uap,6), /* hlen */
+				(char *)	DM_Parg(uap,7));/* cname */
+		break;
+	case DM_MOVE_EVENT:
+		error = dm_move_event(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* srcsid */
+				(dm_token_t)	DM_Uarg(uap,2), /* token */
+				(dm_sessid_t)	DM_Uarg(uap,3), /* targetsid */
+				(dm_token_t *)	DM_Parg(uap,4));/* rtokenp */
+		break;
+	case DM_OBJ_REF_HOLD:
+		error = dm_obj_ref_hold(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(dm_token_t)	DM_Uarg(uap,2), /* token */
+				(void *)	DM_Parg(uap,3), /* hanp */
+				(size_t)	DM_Uarg(uap,4));/* hlen */
+		break;
+	case DM_OBJ_REF_QUERY:
+		use_rvp = 1;
+		error = dm_obj_ref_query_rvp(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(dm_token_t)	DM_Uarg(uap,2), /* token */
+				(void *)	DM_Parg(uap,3), /* hanp */
+				(size_t)	DM_Uarg(uap,4), /* hlen */
+						&rvp);
+		break;
+	case DM_OBJ_REF_RELE:
+		error = dm_obj_ref_rele(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(dm_token_t)	DM_Uarg(uap,2), /* token */
+				(void *)	DM_Parg(uap,3), /* hanp */
+				(size_t)	DM_Uarg(uap,4));/* hlen */
+		break;
+	case DM_PATH_TO_FSHANDLE:
+		error = dm_path_to_fshdl(
+				(char *)	DM_Parg(uap,1), /* path */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t *)	DM_Parg(uap,3));/* hlenp */
+		break;
+	case DM_PATH_TO_HANDLE:
+		error = dm_path_to_hdl(
+				(char *)	DM_Parg(uap,1), /* path */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t *)	DM_Parg(uap,3));/* hlenp */
+		break;
+	case DM_PENDING:
+		error = dm_pending(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(dm_token_t)	DM_Uarg(uap,2), /* token */
+				(dm_timestruct_t *) DM_Parg(uap,3));/* delay */
+		break;
+	case DM_PROBE_HOLE:
+		error = dm_probe_hole(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(dm_off_t)	DM_Uarg(uap,5), /* off */
+				(dm_size_t)	DM_Uarg(uap,6), /* len */
+				(dm_off_t *)	DM_Parg(uap,7), /* roffp */
+				(dm_size_t *)	DM_Parg(uap,8));/* rlenp */
+		break;
+	case DM_PUNCH_HOLE:
+		error = dm_punch_hole(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(dm_off_t)	DM_Uarg(uap,5), /* off */
+				(dm_size_t)	DM_Uarg(uap,6));/* len */
+		break;
+	case DM_QUERY_RIGHT:
+		error = dm_query_right(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(dm_right_t *)	DM_Parg(uap,5));/* rightp */
+		break;
+	case DM_QUERY_SESSION:
+		error = dm_query_session(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(size_t)	DM_Uarg(uap,2), /* buflen */
+				(void *)	DM_Parg(uap,3), /* bufp */
+				(size_t *)	DM_Parg(uap,4));/* rlenp */
+		break;
+	case DM_READ_INVIS:
+		use_rvp = 1;
+		error = dm_read_invis_rvp(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(dm_off_t)	DM_Uarg(uap,5), /* off */
+				(dm_size_t)	DM_Uarg(uap,6), /* len */
+				(void *)	DM_Parg(uap,7), /* bufp */
+						&rvp);
+		break;
+	case DM_RELEASE_RIGHT:
+		error = dm_release_right(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4));/* token */
+		break;
+	case DM_REMOVE_DMATTR:
+		error = dm_remove_dmattr(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(int)		DM_Uarg(uap,5), /* setdtime */
+				(dm_attrname_t *) DM_Parg(uap,6));/* attrnamep */
+		break;
+	case DM_REQUEST_RIGHT:
+		error = dm_request_right(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(u_int)		DM_Uarg(uap,5), /* flags */
+				(dm_right_t)	DM_Uarg(uap,6));/* right */
+		break;
+	case DM_RESPOND_EVENT:
+		error = dm_respond_event(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(dm_token_t)	DM_Uarg(uap,2), /* token */
+				(dm_response_t) DM_Uarg(uap,3), /* response */
+				(int)		DM_Uarg(uap,4), /* reterror */
+				(size_t)	DM_Uarg(uap,5), /* buflen */
+				(void *)	DM_Parg(uap,6));/* respbufp */
+		break;
+	case DM_SEND_MSG:
+		error = dm_send_msg(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* targetsid */
+				(dm_msgtype_t)	DM_Uarg(uap,2), /* msgtype */
+				(size_t)	DM_Uarg(uap,3), /* buflen */
+				(void *)	DM_Parg(uap,4));/* bufp */
+		break;
+	case DM_SET_DISP:
+		error = dm_set_disp(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(dm_eventset_t *) DM_Parg(uap,5),/* eventsetp */
+				(u_int)		DM_Uarg(uap,6));/* maxevent */
+		break;
+	case DM_SET_DMATTR:
+		error = dm_set_dmattr(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(dm_attrname_t *) DM_Parg(uap,5),/* attrnamep */
+				(int)		DM_Uarg(uap,6), /* setdtime */
+				(size_t)	DM_Uarg(uap,7), /* buflen */
+				(void *)	DM_Parg(uap,8));/* bufp */
+		break;
+	case DM_SET_EVENTLIST:
+		error = dm_set_eventlist(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(dm_eventset_t *) DM_Parg(uap,5),/* eventsetp */
+				(u_int)		DM_Uarg(uap,6));/* maxevent */
+		break;
+	case DM_SET_FILEATTR:
+		error = dm_set_fileattr(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(u_int)		DM_Uarg(uap,5), /* mask */
+				(dm_fileattr_t *)DM_Parg(uap,6));/* attrp */
+		break;
+	case DM_SET_INHERIT:
+		error = dm_set_inherit(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(dm_attrname_t *)DM_Parg(uap,5),/* attrnamep */
+				(mode_t)	DM_Uarg(uap,6));/* mode */
+		break;
+	case DM_SET_REGION:
+		error = dm_set_region(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(u_int)		DM_Uarg(uap,5), /* nelem */
+				(dm_region_t *) DM_Parg(uap,6), /* regbufp */
+				(dm_boolean_t *) DM_Parg(uap,7));/* exactflagp */
+		break;
+	case DM_SET_RETURN_ON_DESTROY:
+		error = dm_set_return_on_destroy(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(dm_attrname_t *) DM_Parg(uap,5),/* attrnamep */
+				(dm_boolean_t)	DM_Uarg(uap,6));/* enable */
+		break;
+	case DM_SYMLINK_BY_HANDLE:
+		error = dm_symlink_by_handle(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* dirhanp */
+				(size_t)	DM_Uarg(uap,3), /* dirhlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(void *)	DM_Parg(uap,5), /* hanp */
+				(size_t)	DM_Uarg(uap,6), /* hlen */
+				(char *)	DM_Parg(uap,7), /* cname */
+				(char *)	DM_Parg(uap,8));/* path */
+		break;
+	case DM_SYNC_BY_HANDLE:
+		error = dm_sync_by_handle(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4));/* token */
+		break;
+	case DM_UPGRADE_RIGHT:
+		error = dm_upgrade_right(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4));/* token */
+		break;
+	case DM_WRITE_INVIS:
+		use_rvp = 1;
+		error = dm_write_invis_rvp(
+				(dm_sessid_t)	DM_Uarg(uap,1), /* sid */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(dm_token_t)	DM_Uarg(uap,4), /* token */
+				(int)		DM_Uarg(uap,5), /* flags */
+				(dm_off_t)	DM_Uarg(uap,6), /* off */
+				(dm_size_t)	DM_Uarg(uap,7), /* len */
+				(void *)	DM_Parg(uap,8), /* bufp */
+						&rvp);
+		break;
+	case DM_OPEN_BY_HANDLE:
+		use_rvp = 1;
+		error = dm_open_by_handle_rvp(
+				(unsigned int)	DM_Uarg(uap,1), /* fd */
+				(void *)	DM_Parg(uap,2), /* hanp */
+				(size_t)	DM_Uarg(uap,3), /* hlen */
+				(int)		DM_Uarg(uap,4), /* flags */
+						&rvp);
+		break;
+	default:
+		error = ENOSYS;
+		break;
+	}
+	/* If it was an *_rvp() function, then
+		if error==0, return |rvp|
+	*/
+	if( use_rvp && (error == 0) )
+		return rvp;
+	else
+		return -error;
+}
+
+
+
+static int
+dmapi_open(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+
+static int
+dmapi_release(struct inode *inode, struct file *file)
+{
+	return 0;
+}
+
+
+/* say hello, and let me know the device is hooked up */
+static ssize_t
+dmapi_dump(struct file *file, char *buf, size_t count, loff_t *ppos)
+{
+	char tmp[50];
+	int len;
+	if( *ppos == 0 ){
+		len = sprintf( tmp, "# " DM_VER_STR_CONTENTS "\n" );
+		if( copy_to_user(buf, tmp, len) )
+			return -EFAULT;
+		*ppos += 1;
+		return len;
+	}
+	return 0;
+}
+
+static struct file_operations dmapi_fops = {
+	open:		dmapi_open,
+	ioctl:		dmapi_ioctl,
+	read:		dmapi_dump,
+	release:	dmapi_release
+};
+
+static struct miscdevice dmapi_dev = {
+	minor:	MISC_DYNAMIC_MINOR,
+	name:	"dmapi",
+	fops:	&dmapi_fops
+};
+
+
+
+#ifdef CONFIG_PROC_FS
+static int
+dmapi_summary(char *buffer, char **start, off_t offset,
+		 int count, int *eof, void *data)
+{
+	int len;
+
+	extern u_int dm_sessions_active;
+	extern dm_sessid_t dm_next_sessid;
+	extern dm_token_t dm_next_token;
+	extern dm_sequence_t dm_next_sequence;
+	extern int dm_fsys_cnt;
+
+#define CHKFULL if(len >= count) break;
+#define ADDBUF(a,b)	len += sprintf(buffer + len, a, b); CHKFULL;
+
+	len=0;
+	while(1){
+		ADDBUF("dm_sessions_active=%u\n", dm_sessions_active);
+		ADDBUF("dm_next_sessid=%d\n", (int)dm_next_sessid);
+		ADDBUF("dm_next_token=%d\n", (int)dm_next_token);
+		ADDBUF("dm_next_sequence=%u\n", (u_int)dm_next_sequence);
+		ADDBUF("dm_fsys_cnt=%d\n", dm_fsys_cnt);
+
+		break;
+	}
+
+	if (offset >= len) {
+		*start = buffer;
+		*eof = 1;
+		return 0;
+	}
+	*start = buffer + offset;
+	if ((len -= offset) > count)
+		return count;
+	*eof = 1;
+
+	return len;
+}
+#endif
+
+
+static void __init
+dmapi_init_procfs(int dmapi_minor)
+{
+#ifdef CONFIG_PROC_FS
+	struct proc_dir_entry *entry;
+
+	if ((entry = proc_mkdir( DMAPI_DBG_PROCFS, 0)) == NULL )
+		return;
+	entry->owner = THIS_MODULE;
+	entry->mode = S_IFDIR | S_IRUSR | S_IXUSR;
+
+	if ((entry = proc_mkdir( DMAPI_DBG_PROCFS "/fsreg", 0)) == NULL )
+		return;
+	entry->owner = THIS_MODULE;
+
+	if ((entry = proc_mkdir( DMAPI_DBG_PROCFS "/sessions", 0)) == NULL )
+		return;
+	entry->owner = THIS_MODULE;
+
+	entry = create_proc_read_entry( DMAPI_DBG_PROCFS "/summary", 0, 0, dmapi_summary, NULL);
+	entry->owner = THIS_MODULE;
+
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,0)
+	entry = proc_mknod( DMAPI_PROCFS, S_IFCHR | S_IRUSR | S_IWUSR,
+			   NULL, mk_kdev(MISC_MAJOR,dmapi_minor));
+	if( entry == NULL )
+		return;
+	entry->owner = THIS_MODULE;
+#endif
+#endif
+}
+
+static void __exit
+dmapi_cleanup_procfs(void)
+{
+#ifdef CONFIG_PROC_FS
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,0)
+	remove_proc_entry( DMAPI_PROCFS, NULL);
+#endif
+	remove_proc_entry( DMAPI_DBG_PROCFS "/summary", NULL);
+	remove_proc_entry( DMAPI_DBG_PROCFS "/fsreg", NULL);
+	remove_proc_entry( DMAPI_DBG_PROCFS "/sessions", NULL);
+	remove_proc_entry( DMAPI_DBG_PROCFS, NULL);
+#endif
+}
+
+
+int __init dmapi_init(void)
+{
+	int ret;
+
+	dm_tokdata_cachep = kmem_cache_create("dm_tokdata",
+				sizeof(struct dm_tokdata), 0, 0, NULL, NULL);
+	if (dm_tokdata_cachep == NULL)
+		return -ENOMEM;
+
+	dm_fsreg_cachep = kmem_cache_create("dm_fsreg",
+				sizeof(struct dm_fsreg), 0, 0, NULL, NULL);
+	if (dm_fsreg_cachep == NULL) {
+		kmem_cache_destroy(dm_tokdata_cachep);
+		return -ENOMEM;
+	}
+
+	dm_session_cachep = kmem_cache_create("dm_session",
+				sizeof(struct dm_session), 0, 0, NULL, NULL);
+	if (dm_session_cachep == NULL) {
+		kmem_cache_destroy(dm_tokdata_cachep);
+		kmem_cache_destroy(dm_fsreg_cachep);
+		return -ENOMEM;
+	}
+
+	ret = misc_register(&dmapi_dev);
+	if( ret != 0 )
+		printk(KERN_ERR "dmapi_init: misc_register returned %d\n", ret);
+	dmapi_init_procfs(dmapi_dev.minor);
+	xfs_dm_init();
+	return(0);
+}
+
+void __exit dmapi_uninit(void)
+{
+	xfs_dm_exit();
+	misc_deregister(&dmapi_dev);
+	dmapi_cleanup_procfs();
+	kmem_cache_destroy(dm_tokdata_cachep);
+	kmem_cache_destroy(dm_fsreg_cachep);
+	kmem_cache_destroy(dm_session_cachep);
+	dm_fsys_vector_free();
+}
+
+module_init(dmapi_init);
+module_exit(dmapi_uninit);
+
+MODULE_AUTHOR("Silicon Graphics, Inc.");
+MODULE_DESCRIPTION("SGI XFS data migration subsystem");
+MODULE_LICENSE("GPL");
+
+EXPORT_SYMBOL(dm_send_mount_event);
+EXPORT_SYMBOL(dm_send_namesp_event);
+EXPORT_SYMBOL(dm_send_unmount_event);
+EXPORT_SYMBOL(dm_send_data_event);
diff -uNrp linux-2.6.5/fs/xfs/dmapi/dmapi_xfs.c linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_xfs.c
--- linux-2.6.5/fs/xfs/dmapi/dmapi_xfs.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/dmapi/dmapi_xfs.c	2004-04-05 00:58:15.000000000 +0200
@@ -0,0 +1,3128 @@
+/*
+ * Copyright (c) 2000-2003 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.	 Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+
+#include <linux/version.h>
+#include <linux/init.h>
+#include <linux/spinlock.h>
+#include <linux/seq_file.h>
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,0)
+#include <linux/iobuf.h>
+#endif
+
+#include "xfs.h"
+#include "xfs_fs.h"
+#include "xfs_types.h"
+#include "xfs_inum.h"
+#include "xfs_log.h"
+#include "xfs_clnt.h"
+#include "xfs_trans.h"
+#include "xfs_sb.h"
+#include "xfs_dir.h"
+#include "xfs_dir2.h"
+#include "xfs_alloc.h"
+#include "xfs_dmapi.h"
+#include "xfs_mount.h"
+#include "xfs_alloc_btree.h"
+#include "xfs_bmap_btree.h"
+#include "xfs_ialloc_btree.h"
+#include "xfs_btree.h"
+#include "xfs_ialloc.h"
+#include "xfs_attr_sf.h"
+#include "xfs_dir_sf.h"
+#include "xfs_dir2_sf.h"
+#include "xfs_dinode.h"
+#include "xfs_inode.h"
+#include "xfs_bmap.h"
+#include "xfs_itable.h"
+#include "xfs_rw.h"
+#include "xfs_acl.h"
+#include "xfs_attr.h"
+#include "xfs_inode_item.h"
+#include "dmapi.h"
+#include "dmapi_kern.h"
+#include "dmapi_private.h"
+
+#define MAXNAMLEN MAXNAMELEN
+
+#define XFS_BHV_LOOKUP(vp, xbdp)  \
+	xbdp = vn_bhv_lookup(VN_BHV_HEAD(vp), &xfs_vnodeops); \
+	ASSERT(xbdp);
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,0)
+#define MAX_DIO_SIZE(mp)	(64 * PAGE_CACHE_SIZE)
+#define XFS_TO_LINUX_DEVT(dev)	(MKDEV(sysv_major(dev) & 0x1ff, sysv_minor(dev)))
+#else
+#define MAX_DIO_SIZE(mp)	XFS_B_TO_FSBT((mp), KIO_MAX_ATOMIC_IO << 10)
+#define XFS_TO_LINUX_DEVT(dev)	(kdev_t_to_nr(XFS_DEV_TO_KDEVT(dev)))
+
+static inline int
+open_private_file(struct file *file, struct dentry *dentry, int oflags)
+{
+	mode_t fmode = (oflags+1) & O_ACCMODE;
+	int error = init_private_file(file, dentry, fmode);
+
+	if (error == -EFBIG) {
+		/* try again */
+		file->f_flags = oflags;
+		error = file->f_op->open(dentry->d_inode, file);
+	}
+
+	if (!error)
+		 file->f_flags = oflags;
+	return error;
+}
+
+static inline void
+close_private_file(struct file *file)
+{
+	if (file->f_op->release)
+		file->f_op->release(file->f_dentry->d_inode, file);
+}
+#endif
+
+/* Structure used to hold the on-disk version of a dm_attrname_t.  All
+   on-disk attribute names start with the 8-byte string "SGI_DMI_".
+*/
+
+typedef struct	{
+	char	dan_chars[DMATTR_PREFIXLEN + DM_ATTR_NAME_SIZE + 1];
+} dm_dkattrname_t;
+
+/* Structure used by xfs_dm_get_bulkall(), used as the "private_data"
+ * that we want xfs_bulkstat to send to our formatter.
+ */
+typedef struct {
+	void		*laststruct;
+	dm_dkattrname_t	attrname;
+	int		bulkall;
+} dm_bulkstat_one_t;
+
+/* In the on-disk inode, DMAPI attribute names consist of the user-provided
+   name with the DMATTR_PREFIXSTRING pre-pended.  This string must NEVER be
+   changed!
+*/
+
+STATIC	const	char	dmattr_prefix[DMATTR_PREFIXLEN + 1] = DMATTR_PREFIXSTRING;
+
+STATIC	dm_size_t  dm_min_dio_xfer = 0; /* direct I/O disabled for now */
+
+
+/* See xfs_dm_get_dmattr() for a description of why this is needed. */
+
+#define XFS_BUG_KLUDGE	256	/* max size of an in-inode attribute value */
+
+#define DM_MAX_ATTR_BYTES_ON_DESTROY	256
+
+#define DM_STAT_SIZE(dmtype,namelen)	\
+	(sizeof(dmtype) + sizeof(xfs_handle_t) + namelen)
+#define MAX_DIRENT_SIZE		(sizeof(dirent_t) + MAXNAMELEN)
+
+#define DM_STAT_ALIGN		(sizeof(__uint64_t))
+
+/* DMAPI's E2BIG == EA's ERANGE */
+#define DM_EA_XLATE_ERR(err) { if (err == ERANGE) err = E2BIG; }
+
+/*
+ *	xfs_dm_send_data_event()
+ *
+ *	Send data event to DMAPI.  Drop IO lock (if specified) before
+ *	the dm_send_data_event() call and reacquire it afterwards.
+ */
+int
+xfs_dm_send_data_event(
+	dm_eventtype_t	event,
+	vnode_t		*vp,
+	xfs_off_t	offset,
+	size_t		length,
+	int		flags,
+	vrwlock_t	*locktype)
+{
+	int		error;
+	bhv_desc_t	*bdp;
+	xfs_inode_t	*ip;
+	uint16_t	dmstate;
+	struct inode	*inode = LINVFS_GET_IP(vp);
+
+	XFS_BHV_LOOKUP(vp, bdp);
+	ip = XFS_BHVTOI(bdp);
+	do {
+		dmstate = ip->i_iocore.io_dmstate;
+		if (locktype)
+			xfs_rwunlock(bdp, *locktype);
+
+		if (flags & DM_FLAGS_ISEM)
+			up(&inode->i_sem);
+#ifdef DM_FLAGS_IALLOCSEM_WR
+		if (flags & DM_FLAGS_IALLOCSEM_WR)
+			up_write(&inode->i_alloc_sem);
+#endif
+#ifdef DM_FLAGS_IALLOCSEM_RD
+		if (flags & DM_FLAGS_IALLOCSEM_RD)
+			up_read(&inode->i_alloc_sem);
+#endif
+
+		error = dm_send_data_event(event, vp, DM_RIGHT_NULL,
+				offset, length, flags);
+
+		if (flags & DM_FLAGS_ISEM)
+			down(&inode->i_sem);
+#ifdef DM_FLAGS_IALLOCSEM_WR
+		if (flags & DM_FLAGS_IALLOCSEM_WR)
+			down_write(&inode->i_alloc_sem);
+#endif
+#ifdef DM_FLAGS_IALLOCSEM_RD
+		if (flags & DM_FLAGS_IALLOCSEM_RD)
+			down_read(&inode->i_alloc_sem);
+#endif
+
+		if (locktype)
+			xfs_rwlock(bdp, *locktype);
+	} while (!error && (ip->i_iocore.io_dmstate != dmstate));
+
+	return error;
+}
+
+/*	prohibited_mr_events
+ *
+ *	Return event bits representing any events which cannot have managed
+ *	region events set due to memory mapping of the file.  If the maximum
+ *	protection allowed in any pregion includes PROT_WRITE, and the region
+ *	is shared and not text, then neither READ nor WRITE events can be set.
+ *	Otherwise if the file is memory mapped, no READ event can be set.
+ *
+ */
+
+STATIC int
+prohibited_mr_events(
+	vnode_t		*vp)
+{
+	struct address_space *mapping = LINVFS_GET_IP(vp)->i_mapping;
+	int prohibited = (1 << DM_EVENT_READ);
+	struct vm_area_struct *vma;
+
+	if (!VN_MAPPED(vp))
+		return 0;
+
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,0)
+	down(&mapping->i_shared_sem);
+	list_for_each_entry(vma, &mapping->i_mmap_shared, shared) {
+		if (!(vma->vm_flags & VM_DENYWRITE)) {
+			prohibited |= (1 << DM_EVENT_WRITE);
+			break;
+		}
+	}
+	up(&mapping->i_shared_sem);
+#else
+	spin_lock(&mapping->i_shared_lock);
+	for (vma = mapping->i_mmap_shared; vma; vma = vma->vm_next) {
+		if (!(vma->vm_flags & VM_DENYWRITE)) {
+			prohibited |= (1 << DM_EVENT_WRITE);
+			break;
+		}
+	}
+	spin_unlock(&mapping->i_shared_lock);
+#endif
+
+	return prohibited;
+}
+
+
+#ifdef	DEBUG_RIGHTS
+STATIC int
+xfs_bdp_to_hexhandle(
+	bhv_desc_t	*bdp,
+	u_int		type,
+	char		*buffer)
+{
+	xfs_handle_t	handle;
+	vnode_t		*vp;
+	u_char		*ip;
+	int		length;
+	int		error;
+	int		i;
+
+	vp = BHV_TO_VNODE(bdp);
+
+	if ((error = dm_vp_to_handle(vp, &handle)))
+		return(error);
+
+	if (type == DM_FSYS_OBJ) {	/* a filesystem handle */
+		length = FSHSIZE;
+	} else {
+		length = XFS_HSIZE(handle);
+	}
+	for (ip = (u_char *)&handle, i = 0; i < length; i++) {
+		*buffer++ = "0123456789abcdef"[ip[i] >> 4];
+		*buffer++ = "0123456789abcdef"[ip[i] & 0xf];
+	}
+	*buffer = '\0';
+	return(0);
+}
+#endif	/* DEBUG_RIGHTS */
+
+
+
+
+/* Copy in and validate an attribute name from user space.  It should be a
+   string of at least one and at most DM_ATTR_NAME_SIZE characters.  Because
+   the dm_attrname_t structure doesn't provide room for the trailing NULL
+   byte, we just copy in one extra character and then zero it if it
+   happens to be non-NULL.
+*/
+
+STATIC int
+xfs_copyin_attrname(
+	dm_attrname_t	*from,		/* dm_attrname_t in user space */
+	dm_dkattrname_t *to)		/* name buffer in kernel space */
+{
+	int		error;
+	size_t len;
+
+	strcpy(to->dan_chars, dmattr_prefix);
+
+	len = strnlen_user((char*)from, DM_ATTR_NAME_SIZE);
+	error = copy_from_user(&to->dan_chars[DMATTR_PREFIXLEN], from, len);
+	/* copy_from_user returns negative error */
+
+	if (!error && (to->dan_chars[DMATTR_PREFIXLEN] == '\0'))
+		error = -EINVAL;
+	if (error == -EFAULT) {
+		to->dan_chars[sizeof(to->dan_chars) - 1] = '\0';
+		error = 0;
+	}
+	return(-error); /* return it positive */
+}
+
+
+/* This copies selected fields in an inode into a dm_stat structure.  Because
+   these fields must return the same values as they would in stat(), the
+   majority of this code was copied directly from xfs_getattr().  Any future
+   changes to xfs_gettattr() must also be reflected here.
+
+   The inode must be kept locked SHARED by the caller.
+*/
+
+STATIC void
+xfs_ip_to_stat(
+	xfs_mount_t	*mp,
+	dm_stat_t	*buf,
+	xfs_inode_t	*ip)
+{
+	vnode_t		*vp = XFS_ITOV(ip);
+
+	buf->dt_size = ip->i_d.di_size;
+	buf->dt_dev = ip->i_mount->m_dev;
+
+	buf->dt_ino = ip->i_ino;
+#if XFS_BIG_INUMS
+	buf->dt_ino += mp->m_inoadd;
+#endif
+	/*
+	 * Copy from in-core inode.
+	 */
+	buf->dt_mode = VTTOIF(vp->v_type) | (ip->i_d.di_mode & MODEMASK);
+	buf->dt_uid = ip->i_d.di_uid;
+	buf->dt_gid = ip->i_d.di_gid;
+	buf->dt_nlink = ip->i_d.di_nlink;
+	/*
+	 * Minor optimization, check the common cases first.
+	 */
+	if ((vp->v_type == VREG) || (vp->v_type == VDIR)) {
+		buf->dt_rdev = 0;
+	} else if ((vp->v_type == VCHR) || (vp->v_type == VBLK) ) {
+		buf->dt_rdev = XFS_TO_LINUX_DEVT(ip->i_df.if_u2.if_rdev);
+	} else {
+		buf->dt_rdev = 0;	/* not a b/c spec. */
+	}
+
+	buf->dt_atime = ip->i_d.di_atime.t_sec;
+	buf->dt_mtime = ip->i_d.di_mtime.t_sec;
+	buf->dt_ctime = ip->i_d.di_ctime.t_sec;
+
+	switch (ip->i_d.di_mode & S_IFMT) {
+	  case S_IFBLK:
+	  case S_IFCHR:
+		buf->dt_blksize = BLKDEV_IOSIZE;
+		break;
+	  default:
+		/*
+		 * We use the read buffer size as a recommended I/O
+		 * size.  This should always be larger than the
+		 * write buffer size, so it should be OK.
+		 * The value returned is in bytes.
+		 */
+		buf->dt_blksize = 1 << mp->m_readio_log;
+		break;
+	}
+
+	/*
+	 * XXX : truncate to 32 bits for now.
+	 */
+	buf->dt_blocks =
+		XFS_FSB_TO_BB(mp, ip->i_d.di_nblocks + ip->i_delayed_blks);
+
+	/*
+	 * XFS-added attributes
+	 */
+
+	/*
+	 * convert di_flags to xflags
+	 */
+	buf->dt_xfs_xflags = 0;
+	if (ip->i_d.di_flags & XFS_DIFLAG_REALTIME)
+		buf->dt_xfs_xflags |= DM_XFLAG_REALTIME;
+	if (ip->i_d.di_flags & XFS_DIFLAG_PREALLOC)
+		buf->dt_xfs_xflags |= DM_XFLAG_PREALLOC;
+	if (ip->i_d.di_flags & XFS_DIFLAG_IMMUTABLE)
+		buf->dt_xfs_xflags |= DM_XFLAG_IMMUTABLE;
+	if (ip->i_d.di_flags & XFS_DIFLAG_APPEND)
+		buf->dt_xfs_xflags |= DM_XFLAG_APPEND;
+	if (ip->i_d.di_flags & XFS_DIFLAG_SYNC)
+		buf->dt_xfs_xflags |= DM_XFLAG_SYNC;
+	if (ip->i_d.di_flags & XFS_DIFLAG_NOATIME)
+		buf->dt_xfs_xflags |= DM_XFLAG_NOATIME;
+	if (ip->i_d.di_flags & XFS_DIFLAG_NODUMP)
+		buf->dt_xfs_xflags |= DM_XFLAG_NODUMP;
+	if (XFS_IFORK_Q(ip))
+		buf->dt_xfs_xflags |= DM_XFLAG_HASATTR;
+	buf->dt_xfs_extsize = ip->i_d.di_extsize << mp->m_sb.sb_blocklog;
+	buf->dt_xfs_extents = (ip->i_df.if_flags & XFS_IFEXTENTS) ?
+		ip->i_df.if_bytes / sizeof(xfs_bmbt_rec_t) :
+		ip->i_d.di_nextents;
+	if (ip->i_afp != NULL) {
+		buf->dt_xfs_aextents =
+			(ip->i_afp->if_flags & XFS_IFEXTENTS) ?
+			 ip->i_afp->if_bytes / sizeof(xfs_bmbt_rec_t) :
+			 ip->i_d.di_anextents;
+	} else {
+		buf->dt_xfs_aextents = 0;
+	}
+
+	/* Now fill in the fields that xfs_getattr() doesn't do. */
+
+	buf->dt_emask = ip->i_d.di_dmevmask;
+	buf->dt_nevents = DM_EVENT_MAX;
+	buf->dt_pers = 0;
+	buf->dt_change = 0;
+	buf->dt_dtime = ip->i_d.di_ctime.t_sec;
+	buf->dt_xfs_dmstate = ip->i_d.di_dmstate;
+	buf->dt_xfs_igen = ip->i_d.di_gen;
+
+	/* Set if one of READ, WRITE or TRUNCATE bits is set in emask */
+
+	buf->dt_pmanreg = ( DMEV_ISSET(DM_EVENT_READ, buf->dt_emask) ||
+			DMEV_ISSET(DM_EVENT_WRITE, buf->dt_emask) ||
+			DMEV_ISSET(DM_EVENT_TRUNCATE, buf->dt_emask) ) ? 1 : 0;
+}
+
+
+/*
+ * This is used by dm_get_bulkattr() as well as dm_get_dirattrs().
+ * Given a inumber, it igets the inode and fills the given buffer
+ * with the dm_stat structure for the file.
+ */
+/* ARGSUSED */
+STATIC int
+xfs_dm_bulkstat_one(
+	xfs_mount_t	*mp,		/* mount point for filesystem */
+	xfs_trans_t	*tp,		/* transaction pointer */
+	xfs_ino_t	ino,		/* inode number to get data for */
+	void		*buffer,	/* buffer to place output in */
+	int		ubsize,		/* size of buffer */
+	void		*private_data,	/* my private data */
+	xfs_daddr_t	bno,		/* starting block of inode cluster */
+	int		*ubused,	/* amount of buffer we used */
+	void		*dip,		/* on-disk inode pointer */
+	int		*res)		/* bulkstat result code */
+{
+	xfs_inode_t	*ip;
+	dm_stat_t	*sbuf;
+	dm_xstat_t	*xbuf = NULL;
+	xfs_handle_t	handle;
+	u_int		stat_sz;
+	int		error;
+	dm_bulkstat_one_t *dmb = (dm_bulkstat_one_t*)private_data;
+
+	if (dmb && dmb->bulkall) {
+		xbuf = (dm_xstat_t *)buffer;
+		sbuf = &xbuf->dx_statinfo;
+		stat_sz = DM_STAT_SIZE(*xbuf, 0);
+	}
+	else {
+		sbuf = (dm_stat_t *)buffer;
+		stat_sz = DM_STAT_SIZE(*sbuf, 0);
+	}
+
+	stat_sz = (stat_sz+(DM_STAT_ALIGN-1)) & ~(DM_STAT_ALIGN-1);
+	if (stat_sz > ubsize) {
+		*res = BULKSTAT_RV_NOTHING;
+		return ENOMEM;
+	}
+
+	if (ino == mp->m_sb.sb_rbmino || ino == mp->m_sb.sb_rsumino) {
+		*res = BULKSTAT_RV_NOTHING;
+		return EINVAL;
+	}
+	error = xfs_iget(mp, tp, ino, XFS_ILOCK_SHARED, &ip, bno);
+	if (error) {
+		*res = BULKSTAT_RV_NOTHING;
+		return(error);
+	}
+	if (ip->i_d.di_mode == 0) {
+		xfs_iput_new(ip, XFS_ILOCK_SHARED);
+		*res = BULKSTAT_RV_NOTHING;
+		return(ENOENT);
+	}
+
+	/*
+	 * copy everything to the dm_stat buffer
+	 */
+	xfs_ip_to_stat(mp, sbuf, ip);
+
+	/*
+	 * Make the handle and put it at the end of the stat buffer.
+	 */
+	dm_vp_to_handle(XFS_ITOV(ip), &handle);
+	/* Handle follows outer struct, but ptr is always in dm_stat_t */
+	if (xbuf) {
+		memcpy(xbuf+1, &handle, sizeof(handle));
+		sbuf->dt_handle.vd_offset = (ssize_t) sizeof(dm_xstat_t);
+	}
+	else {
+		memcpy(sbuf+1, &handle, sizeof(handle));
+		sbuf->dt_handle.vd_offset = (ssize_t) sizeof(dm_stat_t);
+	}
+	sbuf->dt_handle.vd_length = (size_t) XFS_HSIZE(handle);
+
+	/*
+	 * This is unused in bulkstat - so we zero it out.
+	 */
+	memset((void *) &sbuf->dt_compname, 0, sizeof(dm_vardata_t));
+
+  	/*
+	 * Do not hold ILOCK_SHARED during VOP_ATTR_GET.
+  	 */
+	xfs_iunlock(ip, XFS_ILOCK_SHARED);
+
+  	/*
+	 * For dm_xstat, get attr.
+	 */
+	if (xbuf) {
+		caddr_t	bufp = (caddr_t)xbuf;
+		int	value_len;
+
+		/* Determine place to drop attr value, and available space. */
+		bufp += stat_sz;
+		value_len = ubsize - stat_sz;
+
+		/* Sanity check value_len */
+		if (value_len < XFS_BUG_KLUDGE) {
+			VN_RELE(XFS_ITOV(ip));
+			*res = BULKSTAT_RV_NOTHING;
+			return ENOMEM;
+		}
+		if (value_len > ATTR_MAX_VALUELEN)
+			value_len = ATTR_MAX_VALUELEN;
+
+		memset((void *) &xbuf->dx_attrdata, 0, sizeof(dm_vardata_t));
+		VOP_ATTR_GET(XFS_ITOV(ip), dmb->attrname.dan_chars, bufp,
+			     &value_len, ATTR_ROOT, sys_cred, error);
+
+		DM_EA_XLATE_ERR(error);
+		if (error && (error != ENOATTR)) {
+			VN_RELE(XFS_ITOV(ip));
+			*res = BULKSTAT_RV_NOTHING;
+			return (error == E2BIG ? ENOMEM : error);
+		}
+
+		/* How much space in the attr? */
+		if (error != ENOATTR) {
+			xbuf->dx_attrdata.vd_offset = stat_sz;
+			xbuf->dx_attrdata.vd_length = value_len;
+			value_len = (value_len+(DM_STAT_ALIGN-1)) & ~(DM_STAT_ALIGN-1);
+			stat_sz += value_len;
+		}
+	}
+
+	/*
+	 * Finished with the vnode
+  	 */
+	VN_RELE(XFS_ITOV(ip));
+
+	/*
+	 *  Update link in dm_stat_t to point to next struct.
+	 */
+	sbuf->_link = stat_sz;
+
+	*res = BULKSTAT_RV_DIDONE;
+	if (ubused)
+		*ubused = stat_sz;
+	if (dmb) {
+		if (xbuf)
+			dmb->laststruct = (void*)xbuf;
+		else
+			dmb->laststruct = (void*)sbuf;
+	}
+	return(0);
+}
+
+
+STATIC int
+xfs_get_dirents(
+	xfs_inode_t	*dirp,
+	void		*bufp,
+	size_t		bufsz,
+	xfs_off_t	*locp,
+	size_t		*nreadp)
+{
+	int		sink;
+	struct uio	auio;
+	struct iovec	aiov;
+	int		rval;
+
+	*nreadp = 0;
+
+	aiov.iov_base = bufp;
+	aiov.iov_len = bufsz;
+	auio.uio_iov = &aiov;
+	auio.uio_iovcnt = 1;
+	auio.uio_offset = *locp;
+	auio.uio_segflg = UIO_SYSSPACE;
+	auio.uio_resid = bufsz;
+
+	rval = XFS_DIR_GETDENTS(dirp->i_mount, NULL, dirp, &auio, &sink);
+	if (! rval) {
+		*locp = auio.uio_offset;
+
+		/*
+		 * number of bytes read into the dirent buffer
+		 */
+		*nreadp = bufsz - auio.uio_resid;
+	}
+	return(rval);
+}
+
+
+STATIC int
+xfs_dirents_to_stats(
+	xfs_mount_t	*mp,
+	xfs_dirent_t	*direntp,	/* array of dirent structs */
+	void		*bufp,		/* buffer to fill */
+	size_t		direntbufsz,	/* sz of filled part of dirent buf */
+	size_t		*spaceleftp,	/* IO - space left in user buffer */
+	size_t		*nwrittenp,	/* number of bytes written to 'bufp' */
+	xfs_off_t	*locp)
+{
+	xfs_dirent_t	*p;
+	dm_stat_t	*statp;
+	size_t		reclen;
+	size_t		namelen;
+	size_t		spaceleft;
+	xfs_off_t	prevoff;
+	int		res;
+
+	spaceleft = *spaceleftp;
+	*spaceleftp = 0;
+	*nwrittenp = 0;
+	prevoff = 0;  /* sizeof this getdents record */
+
+	/*
+	 * Go thru all the dirent records, making dm_stat structures from
+	 * them, one by one, until dirent buffer is empty or stat buffer
+	 * is full.
+	 */
+	p = direntp;
+	statp = (dm_stat_t *) bufp;
+	for (reclen = (size_t) p->d_reclen; direntbufsz > 0;
+					direntbufsz -= reclen,
+					p = (xfs_dirent_t *) ((char *) p + reclen),
+					reclen = (size_t) p->d_reclen) {
+
+		namelen = strlen(p->d_name) + 1;
+
+		/*
+		 * Make sure we have enough space.
+		 */
+		if (spaceleft <= DM_STAT_SIZE(*statp, namelen)) {
+			/*
+			 * d_off field in dirent_t points at the next entry.
+			 */
+			if (prevoff)	/* did at least one; update location */
+				*locp = prevoff;
+			*spaceleftp = 0;
+
+			/*
+			 * The last link is NULL.
+			 */
+			statp->_link = 0;
+			return(0);
+		}
+
+		statp = (dm_stat_t *) bufp;
+
+		(void)xfs_dm_bulkstat_one(mp, NULL, (xfs_ino_t)p->d_ino,
+					  statp, sizeof(*statp), NULL,
+					  0, NULL, NULL, &res);
+		if (res != BULKSTAT_RV_DIDONE)
+			continue;
+
+		/*
+		 * On return from bulkstat_one(), stap->_link points
+		 * at the end of the handle in the stat structure.
+		 */
+		statp->dt_compname.vd_offset = statp->_link;
+		statp->dt_compname.vd_length = namelen;
+		/*
+		 * Directory entry name is guaranteed to be
+		 * null terminated; the copy gets the '\0' too.
+		 */
+		memcpy((char *) statp + statp->_link, p->d_name, namelen);
+
+		/* Word-align the record */
+		statp->_link = (statp->_link + namelen + (DM_STAT_ALIGN - 1))
+			& ~(DM_STAT_ALIGN - 1);
+
+		spaceleft -= statp->_link;
+		*nwrittenp += statp->_link;
+		bufp = (char *)statp + statp->_link;
+
+		/*
+		 * We need to rollback to this position if something happens.
+		 * So we remember it.
+		 */
+		prevoff = p->d_off;
+	}
+	statp->_link = 0;
+
+	/*
+	 * If there's space left to put in more, caller should know that..
+	 */
+	if (spaceleft > DM_STAT_SIZE(*statp, MAXNAMLEN)) {
+		*spaceleftp = spaceleft;
+	}
+	return(0);
+}
+
+
+/* xfs_dm_f_get_eventlist - return the dm_eventset_t mask for inode vp. */
+
+STATIC int
+xfs_dm_f_get_eventlist(
+	bhv_desc_t	*bdp,
+	dm_right_t	right,
+	u_int		nelem,
+	dm_eventset_t	*eventsetp,		/* in kernel space! */
+	u_int		*nelemp)		/* in kernel space! */
+{
+	dm_eventset_t	eventset;
+	xfs_inode_t	*ip;
+
+	if (right < DM_RIGHT_SHARED)
+		return(EACCES);
+
+	/* Note that we MUST return a regular file's managed region bits as
+	   part of the mask because dm_get_eventlist is supposed to return the
+	   union of all managed region flags in those bits.  Since we only
+	   support one region, we can just return the bits as they are.	 For
+	   all other object types, the bits will already be zero.  Handy, huh?
+	*/
+
+	ip = XFS_BHVTOI(bdp);
+	eventset = ip->i_d.di_dmevmask;
+
+	/* Now copy the event mask and event count back to the caller.	We
+	   return the lesser of nelem and DM_EVENT_MAX.
+	*/
+
+	if (nelem > DM_EVENT_MAX)
+		nelem = DM_EVENT_MAX;
+	eventset &= (1 << nelem) - 1;
+
+	*eventsetp = eventset;
+	*nelemp = nelem;
+	return(0);
+}
+
+
+/* xfs_dm_f_set_eventlist - update the dm_eventset_t mask in the inode vp.  Only the
+   bits from zero to maxevent-1 are being replaced; higher bits are preserved.
+*/
+
+STATIC int
+xfs_dm_f_set_eventlist(
+	bhv_desc_t	*bdp,
+	dm_right_t	right,
+	dm_eventset_t	*eventsetp,	/* in kernel space! */
+	u_int		maxevent)
+{
+	dm_eventset_t	eventset;
+	dm_eventset_t	max_mask;
+	dm_eventset_t	valid_events;
+	vnode_t		*vp;
+	xfs_inode_t	*ip;
+	xfs_trans_t	*tp;
+	xfs_mount_t	*mp;
+	int		error;
+
+	if (right < DM_RIGHT_EXCL)
+		return(EACCES);
+
+	eventset = *eventsetp;
+	if (maxevent >= sizeof(ip->i_d.di_dmevmask) * NBBY)
+		return(EINVAL);
+	max_mask = (1 << maxevent) - 1;
+
+	vp = BHV_TO_VNODE(bdp);
+	if (vp->v_type == VDIR) {
+		valid_events = DM_XFS_VALID_DIRECTORY_EVENTS;
+	} else {	/* file or symlink */
+		valid_events = DM_XFS_VALID_FILE_EVENTS;
+	}
+	if ((eventset & max_mask) & ~valid_events)
+		return(EINVAL);
+
+	/* Adjust the event mask so that the managed region bits will not
+	   be altered.
+	*/
+
+	max_mask &= ~(1 <<DM_EVENT_READ);	/* preserve current MR bits */
+	max_mask &= ~(1 <<DM_EVENT_WRITE);
+	max_mask &= ~(1 <<DM_EVENT_TRUNCATE);
+
+	ip = XFS_BHVTOI(bdp);
+	mp = ip->i_mount;
+	tp = xfs_trans_alloc(mp, XFS_TRANS_SET_DMATTRS);
+	error = xfs_trans_reserve(tp, 0, XFS_ICHANGE_LOG_RES(mp), 0, 0, 0);
+	if (error) {
+		xfs_trans_cancel(tp, 0);
+		return(error);
+	}
+	xfs_ilock(ip, XFS_ILOCK_EXCL);
+	xfs_trans_ijoin(tp, ip, XFS_ILOCK_EXCL);
+
+	ip->i_d.di_dmevmask = (eventset & max_mask) | (ip->i_d.di_dmevmask & ~max_mask);
+	ip->i_iocore.io_dmevmask = ip->i_d.di_dmevmask;
+
+	xfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);
+	VN_HOLD(vp);
+	xfs_trans_commit(tp, 0, NULL);
+
+	return(0);
+}
+
+
+/* xfs_dm_fs_get_eventlist - return the dm_eventset_t mask for filesystem vfsp. */
+
+STATIC int
+xfs_dm_fs_get_eventlist(
+	bhv_desc_t	*bdp,
+	dm_right_t	right,
+	u_int		nelem,
+	dm_eventset_t	*eventsetp,		/* in kernel space! */
+	u_int		*nelemp)		/* in kernel space! */
+{
+	dm_eventset_t	eventset;
+	xfs_mount_t	*mp;
+
+	if (right < DM_RIGHT_SHARED)
+		return(EACCES);
+
+	mp = XFS_BHVTOI(bdp)->i_mount;
+	eventset = mp->m_dmevmask;
+
+	/* Now copy the event mask and event count back to the caller.	We
+	   return the lesser of nelem and DM_EVENT_MAX.
+	*/
+
+	if (nelem > DM_EVENT_MAX)
+		nelem = DM_EVENT_MAX;
+	eventset &= (1 << nelem) - 1;
+
+	*eventsetp = eventset;
+	*nelemp = nelem;
+	return(0);
+}
+
+
+/* xfs_dm_fs_set_eventlist - update the dm_eventset_t mask in the mount structure for
+   filesystem vfsp.  Only the bits from zero to maxevent-1 are being replaced;
+   higher bits are preserved.
+*/
+
+STATIC int
+xfs_dm_fs_set_eventlist(
+	bhv_desc_t	*bdp,
+	dm_right_t	right,
+	dm_eventset_t	*eventsetp,	/* in kernel space! */
+	u_int		maxevent)
+{
+	dm_eventset_t	eventset;
+	dm_eventset_t	max_mask;
+	xfs_mount_t	*mp;
+
+	if (right < DM_RIGHT_EXCL)
+		return(EACCES);
+
+	eventset = *eventsetp;
+
+	mp = XFS_BHVTOI(bdp)->i_mount;
+	if (maxevent >= sizeof(mp->m_dmevmask) * NBBY)
+		return(EINVAL);
+	max_mask = (1 << maxevent) - 1;
+
+	if ((eventset & max_mask) & ~DM_XFS_VALID_FS_EVENTS)
+		return(EINVAL);
+
+	mp->m_dmevmask = (eventset & max_mask) | (mp->m_dmevmask & ~max_mask);
+	return(0);
+}
+
+
+/* Code in this routine must exactly match the logic in xfs_diordwr() in
+   order for this to work!
+*/
+
+STATIC int
+xfs_dm_direct_ok(
+	bhv_desc_t	*bdp,
+	dm_off_t	off,
+	dm_size_t	len,
+	void		*bufp)
+{
+	xfs_mount_t	*mp;
+	xfs_inode_t	*ip;
+
+	ip = XFS_BHVTOI(bdp);
+	mp = ip->i_mount;
+
+	/* Realtime files can ONLY do direct I/O. */
+
+	if (ip->i_d.di_flags & XFS_DIFLAG_REALTIME)
+		return(1);
+
+	/* If direct I/O is disabled, or if the request is too small, use
+	   buffered I/O.
+	*/
+
+	if (!dm_min_dio_xfer || len < dm_min_dio_xfer)
+		return(0);
+
+#if 0
+	/* If the request is not well-formed or is too large, use
+	   buffered I/O.
+	*/
+
+	if ((__psint_t)bufp & scache_linemask)	/* if buffer not aligned */
+		return(0);
+	if (off & mp->m_blockmask)		/* if file offset not aligned */
+		return(0);
+	if (len & mp->m_blockmask)		/* if xfer length not aligned */
+		return(0);
+	if (len > ctooff(v.v_maxdmasz - 1))	/* if transfer too large */
+		return(0);
+
+	/* A valid direct I/O candidate. */
+
+	return(1);
+#else
+	return(0);
+#endif
+}
+
+
+/* We need to be able to select various combinations of FINVIS, O_NONBLOCK,
+   O_DIRECT, and O_SYNC, yet we don't have a file descriptor and we don't have
+   the file's pathname.	 All we have is a handle.
+*/
+
+STATIC int
+xfs_dm_rdwr(
+	vnode_t		*vp,
+	uint		fflag,
+	mode_t		fmode,
+	dm_off_t	off,
+	dm_size_t	len,
+	void		*bufp,
+	int		*rvp)
+{
+	int		error;
+	int		oflags;
+	int		ioflags;
+	ssize_t		xfer;
+	struct file	file;
+	struct inode	*ip;
+	struct dentry	*dentry;
+	bhv_desc_t	*xbdp;
+
+	if (off < 0 || vp->v_type != VREG)
+		return(EINVAL);
+
+	if (fmode & FMODE_READ) {
+		oflags = O_RDONLY;
+	} else {
+		oflags = O_WRONLY;
+	}
+
+	/*
+	 * Build file descriptor flags and I/O flags.  O_NONBLOCK is needed so
+	 * that we don't block on mandatory file locks.	 IO_INVIS is needed so
+	 * that we don't change any file timestamps.
+	 */
+
+	oflags |= O_LARGEFILE | O_NONBLOCK;
+	ioflags = IO_INVIS;
+	XFS_BHV_LOOKUP(vp, xbdp);
+	if (xfs_dm_direct_ok(xbdp, off, len, bufp)) {
+		ioflags |= IO_ISDIRECT;
+		oflags |= O_DIRECT;
+	}
+
+	if (fflag & O_SYNC)
+		oflags |= O_SYNC;
+
+	ip = LINVFS_GET_IP(vp);
+	if (ip->i_fop == NULL) {
+		/* no iput; caller did get, and will do put */
+		return(EINVAL);
+	}
+
+	igrab(ip);
+
+	dentry = d_alloc_anon(ip);
+	if (dentry == NULL) {
+		iput(ip);
+		return ENOMEM;
+	}
+
+	if (fmode & FMODE_WRITE) {
+		error = get_write_access(ip);
+		if (error) {
+			error = -error;
+			goto dput;
+		}
+	}
+
+	error = open_private_file(&file, dentry, oflags);
+	if (error) {
+		error = EINVAL;
+		goto put_access;
+	}
+	file.f_op = &linvfs_invis_file_operations;
+
+	if (fmode & FMODE_READ) {
+		xfer = file.f_op->read(&file, bufp, len, (loff_t*)&off);
+	} else {
+		xfer = file.f_op->write(&file, bufp, len, (loff_t*)&off);
+	}
+
+	if (xfer >= 0) {
+		*rvp = xfer;
+		error = 0;
+	} else {
+		/* xfs_read/xfs_write return negative error--flip it */
+		error = -(int)xfer;
+	}
+
+	close_private_file(&file);
+ put_access:
+	if (fmode & FMODE_WRITE)
+		put_write_access(ip);
+ dput:
+	dput(dentry);
+	return error;
+}
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_clear_inherit(
+	vnode_t		*vp,
+	dm_right_t	right,
+	dm_attrname_t	*attrnamep)
+{
+	return(ENOSYS);
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_create_by_handle(
+	vnode_t		*vp,
+	dm_right_t	right,
+	void		*hanp,
+	size_t		hlen,
+	char		*cname)
+{
+	return(ENOSYS);
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_downgrade_right(
+	vnode_t		*vp,
+	dm_right_t	right,
+	u_int		type)		/* DM_FSYS_OBJ or zero */
+{
+#ifdef	DEBUG_RIGHTS
+	char		buffer[sizeof(xfs_handle_t) * 2 + 1];
+	bhv_desc_t	*bdp;
+
+	XFS_BHV_LOOKUP(vp, bdp);
+
+	if (!xfs_bdp_to_hexhandle(bdp, type, buffer)) {
+		printf("dm_downgrade_right: old %d new %d type %d handle %s\n",
+			right, DM_RIGHT_SHARED, type, buffer);
+	} else {
+		printf("dm_downgrade_right: old %d new %d type %d handle "
+			"<INVALID>\n", right, DM_RIGHT_SHARED, type);
+	}
+#endif	/* DEBUG_RIGHTS */
+	return(0);
+}
+
+
+/* Note: xfs_dm_get_allocinfo() makes no attempt to coalesce two adjacent
+   extents when both are of type DM_EXTENT_RES; this is left to the caller.
+   XFS guarantees that there will never be two adjacent DM_EXTENT_HOLE extents.
+
+   In order to provide the caller with all extents in a file including
+   those beyond the file's last byte offset, we have to use the xfs_bmapi()
+   interface.  (VOP_BMAP won't let us see past EOF, and xfs_getbmap is too
+   buggy.)
+*/
+
+STATIC int
+xfs_dm_get_allocinfo_rvp(
+	vnode_t		*vp,
+	dm_right_t	right,
+	dm_off_t	*offp,
+	u_int		nelem,
+	dm_extent_t	*extentp,
+	u_int		*nelemp,
+	int		*rvp)
+{
+	xfs_inode_t	*ip;		/* xfs incore inode pointer */
+	xfs_mount_t	*mp;		/* file system mount point */
+	xfs_fileoff_t	fsb_offset;
+	xfs_filblks_t	fsb_length;
+	dm_off_t	startoff;
+	int		elem;
+	bhv_desc_t	*xbdp;
+
+	if (right < DM_RIGHT_SHARED)
+		return(EACCES);
+
+	if (copy_from_user( &startoff, offp, sizeof(startoff)))
+		return(EFAULT);
+
+	XFS_BHV_LOOKUP(vp, xbdp);
+
+	ip = XFS_BHVTOI(xbdp);
+	mp = ip->i_mount;
+
+	if (startoff > XFS_MAXIOFFSET(mp))
+		return(EINVAL);
+
+	if (nelem == 0) {
+		if (put_user(1, nelemp))
+			return(EFAULT);
+		return(E2BIG);
+	}
+
+	/* Convert the caller's starting offset into filesystem allocation
+	   units as required by xfs_bmapi().  Round the offset down so that
+	   it is sure to be included in the reply.
+	*/
+
+	fsb_offset = XFS_B_TO_FSBT(mp, startoff);
+	fsb_length = XFS_B_TO_FSB(mp, XFS_MAXIOFFSET(mp)) - fsb_offset;
+	elem = 0;
+
+	while (fsb_length && elem < nelem) {
+		xfs_bmbt_irec_t bmp[50];
+		dm_extent_t	extent;
+		xfs_filblks_t	fsb_bias;
+		dm_size_t	bias;
+		int		error;
+		int		lock;
+		int		num;
+		int		i;
+
+		/* Compute how many getbmap structures to use on the xfs_bmapi
+		   call.
+		*/
+
+		num = MIN((u_int)(nelem - elem), (u_int)(sizeof(bmp) / sizeof(bmp[0])));
+
+		xfs_ilock(ip, XFS_IOLOCK_SHARED);
+		lock = xfs_ilock_map_shared(ip);
+
+		error = xfs_bmapi(NULL, ip, fsb_offset, fsb_length,
+			XFS_BMAPI_ENTIRE, NULL, 0, bmp, &num, NULL);
+
+		xfs_iunlock_map_shared(ip, lock);
+		xfs_iunlock(ip, XFS_IOLOCK_SHARED);
+
+		if (error)
+			return(error);
+
+		/* Fill in the caller's extents, adjusting the bias in the
+		   first entry if necessary.
+		*/
+
+		for (i = 0; i < num; i++, extentp++) {
+			bias = startoff - XFS_FSB_TO_B(mp, bmp[i].br_startoff);
+			extent.ex_offset = startoff;
+			extent.ex_length =
+				XFS_FSB_TO_B(mp, bmp[i].br_blockcount) - bias;
+			if (bmp[i].br_startblock == HOLESTARTBLOCK) {
+				extent.ex_type = DM_EXTENT_HOLE;
+			} else {
+				extent.ex_type = DM_EXTENT_RES;
+			}
+			startoff = extent.ex_offset + extent.ex_length;
+
+			if (copy_to_user( extentp, &extent, sizeof(extent)))
+				return(EFAULT);
+
+			fsb_bias = fsb_offset - bmp[i].br_startoff;
+			fsb_offset += bmp[i].br_blockcount - fsb_bias;
+			fsb_length -= bmp[i].br_blockcount - fsb_bias;
+			elem++;
+		}
+	}
+
+	if (fsb_length == 0) {
+		startoff = 0;
+	}
+	if (copy_to_user( offp, &startoff, sizeof(startoff)))
+		return(EFAULT);
+
+	if (copy_to_user( nelemp, &elem, sizeof(elem)))
+		return(EFAULT);
+
+	*rvp = (fsb_length == 0 ? 0 : 1);
+
+	return(0);
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_get_bulkall_rvp(
+	vnode_t		*vp,
+	dm_right_t	right,
+	u_int		mask,
+	dm_attrname_t	*attrnamep,
+	dm_attrloc_t	*locp,
+	size_t		buflen,
+	void		*bufp,		/* address of buffer in user space */
+	size_t		*rlenp,		/* user space address */
+	int		*rvalp)
+{
+	int		error, done;
+	int		nelems;
+	u_int		statstruct_sz;
+	dm_attrloc_t	loc;
+	bhv_desc_t	*mp_bdp;
+	xfs_mount_t	*mp;
+	vfs_t		*vfsp = vp->v_vfsp;
+	dm_bulkstat_one_t dmb;
+	dm_xstat_t	*dxs;
+
+	if (attrnamep->an_chars[0] == '\0')
+		return(EINVAL);
+
+	if (right < DM_RIGHT_SHARED)
+		return(EACCES);
+
+	if (copy_from_user( &loc, locp, sizeof(loc)))
+		return(EFAULT);
+
+	/* Because we will write directly to the user's buffer, make sure that
+	   the buffer is properly aligned.
+	*/
+
+	if (((__psint_t)bufp & (DM_STAT_ALIGN - 1)) != 0)
+		return(EFAULT);
+
+	/* Size of the handle is constant for this function.
+	 * If there are no files with attributes, then this will be the
+	 * maximum number of inodes we can get.
+	 */
+
+	statstruct_sz = DM_STAT_SIZE(dm_xstat_t, 0);
+	statstruct_sz = (statstruct_sz+(DM_STAT_ALIGN-1)) & ~(DM_STAT_ALIGN-1);
+
+	nelems = buflen / statstruct_sz; 
+	if (nelems < 1) {
+		if (put_user( statstruct_sz, rlenp ))
+			return(EFAULT);
+		return(E2BIG);
+	} 
+
+	mp_bdp = bhv_lookup(VFS_BHVHEAD(vfsp), &xfs_vfsops);
+	ASSERT(mp_bdp);
+	mp = XFS_BHVTOM(mp_bdp);
+
+
+	/* Build the on-disk version of the attribute name. */
+	strcpy(dmb.attrname.dan_chars, dmattr_prefix);
+	strncpy(&dmb.attrname.dan_chars[DMATTR_PREFIXLEN],
+		(char *)attrnamep->an_chars, DM_ATTR_NAME_SIZE + 1);
+	dmb.attrname.dan_chars[sizeof(dmb.attrname.dan_chars) - 1] = '\0';
+
+	/*
+	 * fill the buffer with dm_xstat_t's 
+	 */
+
+	dmb.laststruct = NULL;
+	dmb.bulkall = 1;
+	error = xfs_bulkstat(mp, NULL, (xfs_ino_t *)&loc,
+			     &nelems, 
+			     xfs_dm_bulkstat_one,
+			     (void*)&dmb,
+			     statstruct_sz,
+			     bufp, 
+			     BULKSTAT_FG_IGET,
+			     &done);
+
+	if (error)
+		return(error);
+	if (!done)
+		*rvalp = 1;
+	else
+		*rvalp = 0;
+	if (put_user( statstruct_sz * nelems, rlenp ))
+		return(EFAULT);
+
+	if (copy_to_user( locp, &loc, sizeof(loc)))
+		return(EFAULT);
+	/*
+	 *  If we didn't do any, we must not have any more to do.
+	 */
+	if (nelems < 1)
+		return(0);
+	/* set _link in the last struct to zero */
+	dxs = (dm_xstat_t*)dmb.laststruct;
+	if (dxs) {
+		dm_stat_t *ds = &dxs->dx_statinfo;
+		if (put_user(0, &ds->_link))
+			return(EFAULT);
+	}
+	return(0);
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_get_bulkattr_rvp(
+	vnode_t		*vp,
+	dm_right_t	right,
+	u_int		mask,
+	dm_attrloc_t	*locp,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp,
+	int		*rvalp)
+{
+	int		error, done;
+	int		nelems;
+	u_int		statstruct_sz;
+	dm_attrloc_t	loc;
+	bhv_desc_t	*mp_bdp;
+	xfs_mount_t	*mp;
+	vfs_t		*vfsp = vp->v_vfsp;
+
+	if (right < DM_RIGHT_SHARED)
+		return(EACCES);
+
+	if (copy_from_user( &loc, locp, sizeof(loc)))
+		return(EFAULT);
+
+	/* Because we will write directly to the user's buffer, make sure that
+	   the buffer is properly aligned.
+	*/
+
+	if (((__psint_t)bufp & (DM_STAT_ALIGN - 1)) != 0)
+		return(EFAULT);
+
+	/* size of the handle is constant for this function */
+
+	statstruct_sz = DM_STAT_SIZE(dm_stat_t, 0);
+	statstruct_sz = (statstruct_sz+(DM_STAT_ALIGN-1)) & ~(DM_STAT_ALIGN-1);
+
+	nelems = buflen / statstruct_sz;
+	if (nelems < 1) {
+		if (put_user( statstruct_sz, rlenp ))
+			return(EFAULT);
+		return(E2BIG);
+	}
+
+	mp_bdp = bhv_lookup(VFS_BHVHEAD(vfsp), &xfs_vfsops);
+	ASSERT(mp_bdp);
+	mp = XFS_BHVTOM(mp_bdp);
+
+
+	/*
+	 * fill the buffer with dm_stat_t's
+	 */
+
+	error = xfs_bulkstat(mp, NULL,
+			     (xfs_ino_t *)&loc,
+			     &nelems,
+			     xfs_dm_bulkstat_one,
+			     NULL,
+			     statstruct_sz,
+			     bufp,
+			     BULKSTAT_FG_IGET,
+			     &done);
+	if (error)
+		return(error);
+	if (!done) {
+		*rvalp = 1;
+	} else {
+		*rvalp = 0;
+	}
+
+	if (put_user( statstruct_sz * nelems, rlenp ))
+		return(EFAULT);
+
+	if (copy_to_user( locp, &loc, sizeof(loc)))
+		return(EFAULT);
+
+	/*
+	 *  If we didn't do any, we must not have any more to do.
+	 */
+	if (nelems < 1)
+		return(0);
+	/* set _link in the last struct to zero */
+	if (put_user( 0,
+	    &((dm_stat_t *)((char *)bufp + statstruct_sz*(nelems-1)))->_link)
+	   )
+		return(EFAULT);
+	return(0);
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_get_config(
+	vnode_t		*vp,
+	dm_right_t	right,
+	dm_config_t	flagname,
+	dm_size_t	*retvalp)
+{
+	dm_size_t	retval;
+
+	switch (flagname) {
+	case DM_CONFIG_DTIME_OVERLOAD:
+	case DM_CONFIG_PERS_ATTRIBUTES:
+	case DM_CONFIG_PERS_EVENTS:
+	case DM_CONFIG_PERS_MANAGED_REGIONS:
+	case DM_CONFIG_PUNCH_HOLE:
+	case DM_CONFIG_WILL_RETRY:
+		retval = DM_TRUE;
+		break;
+
+	case DM_CONFIG_CREATE_BY_HANDLE:	/* these will never be done */
+	case DM_CONFIG_LOCK_UPGRADE:
+	case DM_CONFIG_PERS_INHERIT_ATTRIBS:
+		retval = DM_FALSE;
+		break;
+
+	case DM_CONFIG_BULKALL:
+		retval = DM_TRUE;
+		break;
+	case DM_CONFIG_MAX_ATTR_ON_DESTROY:
+		retval = DM_MAX_ATTR_BYTES_ON_DESTROY;
+		break;
+
+	case DM_CONFIG_MAX_ATTRIBUTE_SIZE:
+		retval = ATTR_MAX_VALUELEN;
+		break;
+
+	case DM_CONFIG_MAX_HANDLE_SIZE:
+		retval = DM_MAX_HANDLE_SIZE;
+		break;
+
+	case DM_CONFIG_MAX_MANAGED_REGIONS:
+		retval = 1;
+		break;
+
+	case DM_CONFIG_TOTAL_ATTRIBUTE_SPACE:
+		retval = 0x7fffffff;	/* actually it's unlimited */
+		break;
+
+	default:
+		return(EINVAL);
+	}
+
+	/* Copy the results back to the user. */
+
+	if (copy_to_user( retvalp, &retval, sizeof(retval)))
+		return(EFAULT);
+	return(0);
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_get_config_events(
+	vnode_t		*vp,
+	dm_right_t	right,
+	u_int		nelem,
+	dm_eventset_t	*eventsetp,
+	u_int		*nelemp)
+{
+	dm_eventset_t	eventset;
+
+	if (nelem == 0)
+		return(EINVAL);
+
+	eventset = DM_XFS_SUPPORTED_EVENTS;
+
+	/* Now copy the event mask and event count back to the caller.	We
+	   return the lesser of nelem and DM_EVENT_MAX.
+	*/
+
+	if (nelem > DM_EVENT_MAX)
+		nelem = DM_EVENT_MAX;
+	eventset &= (1 << nelem) - 1;
+
+	if (copy_to_user( eventsetp, &eventset, sizeof(eventset)))
+		return(EFAULT);
+
+	if (put_user(nelem, nelemp))
+		return(EFAULT);
+	return(0);
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_get_destroy_dmattr(
+	vnode_t		*vp,
+	dm_right_t	right,
+	dm_attrname_t	*attrnamep,
+	char		**valuepp,
+	int		*vlenp)
+{
+	char		buffer[XFS_BUG_KLUDGE];
+	dm_dkattrname_t dkattrname;
+	int		alloc_size;
+	int		value_len;
+	char		*value;
+	int		error;
+
+	*vlenp = -1;		/* assume failure by default */
+
+	if (attrnamep->an_chars[0] == '\0')
+		return(EINVAL);
+
+	/* Build the on-disk version of the attribute name. */
+
+	strcpy(dkattrname.dan_chars, dmattr_prefix);
+	strncpy(&dkattrname.dan_chars[DMATTR_PREFIXLEN],
+		(char *)attrnamep->an_chars, DM_ATTR_NAME_SIZE + 1);
+	dkattrname.dan_chars[sizeof(dkattrname.dan_chars) - 1] = '\0';
+
+	/* VOP_ATTR_GET will not return anything if the buffer is too small,
+	   and we don't know how big to make the buffer, so this may take
+	   two tries to get it right.  The initial try must use a buffer of
+	   at least XFS_BUG_KLUDGE bytes to prevent buffer overflow because
+	   of a bug in XFS.
+	*/
+
+	alloc_size = 0;
+	value_len = sizeof(buffer);	/* in/out parameter */
+	value = buffer;
+
+	VOP_ATTR_GET(vp, dkattrname.dan_chars, value, &value_len,
+			ATTR_ROOT, sys_cred, error);
+
+	if (error == ERANGE) {
+		alloc_size = value_len;
+		value = kmalloc(alloc_size, SLAB_KERNEL);
+		if (value == NULL) {
+			printk("%s/%d: kmalloc returned NULL\n", __FUNCTION__, __LINE__);
+			return(ENOMEM);
+		}
+
+		VOP_ATTR_GET(vp, dkattrname.dan_chars, value,
+			&value_len, ATTR_ROOT, sys_cred, error);
+	}
+	if (error) {
+		if (alloc_size)
+			kfree(value);
+		DM_EA_XLATE_ERR(error);
+		return(error);
+	}
+
+	/* The attribute exists and has a value.  Note that a value_len of
+	   zero is valid!
+	*/
+
+	if (value_len == 0) {
+		*vlenp = 0;
+		return(0);
+	}
+
+	if (!alloc_size) {
+		value = kmalloc(value_len, SLAB_KERNEL);
+		if (value == NULL) {
+			printk("%s/%d: kmalloc returned NULL\n", __FUNCTION__, __LINE__);
+			return(ENOMEM);
+		}
+		memcpy(value, buffer, value_len);
+	} else if (value_len > DM_MAX_ATTR_BYTES_ON_DESTROY) {
+		int	value_len2 = DM_MAX_ATTR_BYTES_ON_DESTROY;
+		char	*value2;
+
+		value2 = kmalloc(value_len2, SLAB_KERNEL);
+		if (value2 == NULL) {
+			printk("%s/%d: kmalloc returned NULL\n", __FUNCTION__, __LINE__);
+			kfree(value);
+			return(ENOMEM);
+		}
+		memcpy(value2, value, value_len2);
+		kfree(value);
+		value = value2;
+		value_len = value_len2;
+	}
+	*vlenp = value_len;
+	*valuepp = value;
+	return(0);
+}
+
+/* This code was taken from xfs_fcntl(F_DIOINFO) and modified slightly because
+   we don't have a flags parameter (no open file).
+   Taken from xfs_ioctl(XFS_IOC_DIOINFO) on Linux.
+*/
+
+STATIC int
+xfs_dm_get_dioinfo(
+	vnode_t		*vp,
+	dm_right_t	right,
+	dm_dioinfo_t	*diop)
+{
+	dm_dioinfo_t	dio;
+	xfs_mount_t	*mp;
+	xfs_inode_t	*ip;
+	bhv_desc_t	*xbdp;
+
+	if (right < DM_RIGHT_SHARED)
+		return(EACCES);
+
+	XFS_BHV_LOOKUP(vp, xbdp);
+
+	ip = XFS_BHVTOI(xbdp);
+	mp = ip->i_mount;
+
+	/*
+	 * this only really needs to be BBSIZE.
+	 * it is set to the file system block size to
+	 * avoid having to do block zeroing on short writes.
+	 */
+	dio.d_miniosz = mp->m_sb.sb_blocksize;
+	dio.d_maxiosz = MAX_DIO_SIZE(mp);
+	dio.d_mem = mp->m_sb.sb_blocksize;
+
+	if (ip->i_d.di_flags & XFS_DIFLAG_REALTIME) {
+		dio.d_dio_only = DM_TRUE;
+	} else {
+		dio.d_dio_only = DM_FALSE;
+	}
+
+	if (copy_to_user(diop, &dio, sizeof(dio)))
+		return(EFAULT);
+	return(0);
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_get_dirattrs_rvp(
+	vnode_t		*vp,
+	dm_right_t	right,
+	u_int		mask,
+	dm_attrloc_t	*locp,
+	size_t		buflen,
+	void		*bufp,		/* address of buffer in user space */
+	size_t		*rlenp,		/* user space address */
+	int		*rvp)
+{
+	xfs_inode_t	*dp;
+	xfs_mount_t	*mp;
+	size_t		direntbufsz, statbufsz;
+	size_t		nread, spaceleft, nwritten=0;
+	void		*direntp, *statbufp;
+	uint		lock_mode;
+	int		error;
+	dm_attrloc_t	loc;
+	bhv_desc_t	*xbdp;
+	bhv_desc_t	*mp_bdp;
+	vfs_t		*vfsp = vp->v_vfsp;
+
+	if (right < DM_RIGHT_SHARED)
+		return(EACCES);
+
+	if (copy_from_user( &loc, locp, sizeof(loc)))
+		return(EFAULT);
+
+	if ((buflen / DM_STAT_SIZE(dm_stat_t, MAXNAMLEN)) == 0) {
+		if (put_user( DM_STAT_SIZE(dm_stat_t, MAXNAMLEN), rlenp ))
+			return(EFAULT);
+		return(E2BIG);
+	}
+
+	mp_bdp = bhv_lookup(VFS_BHVHEAD(vfsp), &xfs_vfsops);
+	ASSERT(mp_bdp);
+	xbdp = vn_bhv_lookup(VN_BHV_HEAD(vp), &xfs_vnodeops);
+	ASSERT(xbdp);
+
+	mp = XFS_BHVTOM(mp_bdp);
+	dp = XFS_BHVTOI(xbdp);
+	if ((dp->i_d.di_mode & S_IFMT) != S_IFDIR)
+		return(ENOTDIR);
+
+	/*
+	 * Don't get more dirents than are guaranteed to fit.
+	 * The minimum that the stat buf holds is the buf size over
+	 * maximum entry size.	That times the minimum dirent size
+	 * is an overly conservative size for the dirent buf.
+	 */
+	statbufsz = NBPP;
+	direntbufsz = (NBPP / DM_STAT_SIZE(dm_stat_t, MAXNAMLEN)) * sizeof(xfs_dirent_t);
+
+	direntp = kmem_alloc(direntbufsz, KM_SLEEP);
+	statbufp = kmem_alloc(statbufsz, KM_SLEEP);
+	error = 0;
+	spaceleft = buflen;
+	/*
+	 * Keep getting dirents until the ubuffer is packed with
+	 * dm_stat structures.
+	 */
+	do {
+		ulong	dir_gen = 0;
+
+		lock_mode = xfs_ilock_map_shared(dp);
+		/* See if the directory was removed after it was opened. */
+		if (dp->i_d.di_nlink <= 0) {
+			xfs_iunlock_map_shared(dp, lock_mode);
+			error = ENOENT;
+			break;
+		}
+		if (dir_gen == 0)
+			dir_gen = dp->i_gen;
+		else if (dir_gen != dp->i_gen) {
+			/* if dir changed, quit.  May be overzealous... */
+			xfs_iunlock_map_shared(dp, lock_mode);
+			break;
+		}
+		error = xfs_get_dirents(dp, direntp, direntbufsz,
+						(xfs_off_t *)&loc, &nread);
+		xfs_iunlock_map_shared(dp, lock_mode);
+
+		if (error) {
+			break;
+		}
+		if (nread == 0)
+			break;
+		/*
+		 * Now iterate thru them and call bulkstat_one() on all
+		 * of them
+		 */
+		error = xfs_dirents_to_stats(mp,
+					  (xfs_dirent_t *) direntp,
+					  statbufp,
+					  nread,
+					  &spaceleft,
+					  &nwritten,
+					  (xfs_off_t *)&loc);
+		if (error) {
+			break;
+		}
+
+		if (nwritten) {
+			if (copy_to_user( bufp, statbufp, nwritten)) {
+				error = EFAULT;
+				break;
+			}
+			break;
+		}
+	} while (spaceleft);
+	/*
+	 *  If xfs_get_dirents found anything, there might be more to do.
+	 *  If it didn't read anything, signal all done (rval == 0).
+	 *  (Doesn't matter either way if there was an error.)
+	 */
+	if (nread) {
+		*rvp = 1;
+	} else {
+		*rvp = 0;
+	}
+
+	kmem_free(statbufp, statbufsz);
+	kmem_free(direntp, direntbufsz);
+	if (!error){
+		if (put_user( buflen - spaceleft, rlenp))
+			return(EFAULT);
+	}
+
+	if (!error && copy_to_user(locp, &loc, sizeof(loc)))
+		error = EFAULT;
+	return(error);
+}
+
+
+STATIC int
+xfs_dm_get_dmattr(
+	vnode_t		*vp,
+	dm_right_t	right,
+	dm_attrname_t	*attrnamep,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp)
+{
+	dm_dkattrname_t name;
+	char		*value;
+	int		value_len;
+	int		alloc_size;
+	int		error;
+
+	if (right < DM_RIGHT_SHARED)
+		return(EACCES);
+
+	if ((error = xfs_copyin_attrname(attrnamep, &name)) != 0)
+		return(error);
+
+	/* Allocate a buffer to receive the attribute's value.	We allocate
+	   at least one byte even if the caller specified a buflen of zero.
+	   (A buflen of zero is considered valid.)
+
+	   Allocating a minimum of XFS_BUG_KLUDGE bytes temporarily works
+	   around a bug within XFS in which in-inode attribute values are not
+	   checked to see if they will fit in the buffer before they are
+	   copied.  Since no in-core attribute value can be larger than 256
+	   bytes (an 8-bit size field), we allocate that minimum size here to
+	   prevent buffer overrun in both the kernel's and user's buffers.
+	*/
+
+	alloc_size = buflen;
+	if (alloc_size < XFS_BUG_KLUDGE)
+		alloc_size = XFS_BUG_KLUDGE;
+	if (alloc_size > ATTR_MAX_VALUELEN)
+		alloc_size = ATTR_MAX_VALUELEN;
+	value = kmem_alloc(alloc_size, KM_SLEEP);
+
+	/* Get the attribute's value. */
+
+	value_len = alloc_size;		/* in/out parameter */
+
+	VOP_ATTR_GET(vp, name.dan_chars, value, &value_len,
+			ATTR_ROOT, NULL, error);
+	DM_EA_XLATE_ERR(error);
+
+	/* DMAPI requires an errno of ENOENT if an attribute does not exist,
+	   so remap ENOATTR here.
+	*/
+
+	if (error == ENOATTR)
+		error = ENOENT;
+	if (!error && value_len > buflen)
+		error = E2BIG;
+	if (!error && copy_to_user(bufp, value, value_len))
+		error = EFAULT;
+	if (!error || error == E2BIG) {
+		if (put_user(value_len, rlenp))
+			error = EFAULT;
+	}
+
+	kmem_free(value, alloc_size);
+	return(error);
+}
+
+STATIC int
+xfs_dm_get_eventlist(
+	vnode_t		*vp,
+	dm_right_t	right,
+	u_int		type,
+	u_int		nelem,
+	dm_eventset_t	*eventsetp,
+	u_int		*nelemp)
+{
+	int		error;
+	bhv_desc_t	*xbdp;
+
+	XFS_BHV_LOOKUP(vp, xbdp);
+
+	if (type == DM_FSYS_OBJ) {
+		error = xfs_dm_fs_get_eventlist(xbdp, right, nelem,
+			eventsetp, nelemp);
+	} else {
+		error = xfs_dm_f_get_eventlist(xbdp, right, nelem,
+			eventsetp, nelemp);
+	}
+	return(error);
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_get_fileattr(
+	vnode_t		*vp,
+	dm_right_t	right,
+	u_int		mask,		/* not used; always return everything */
+	dm_stat_t	*statp)
+{
+	dm_stat_t	stat;
+	xfs_inode_t	*ip;
+	xfs_mount_t	*mp;
+	bhv_desc_t	*xbdp;
+
+	if (right < DM_RIGHT_SHARED)
+		return(EACCES);
+
+	XFS_BHV_LOOKUP(vp, xbdp);
+
+	/* Find the mount point. */
+
+	ip = XFS_BHVTOI(xbdp);
+	mp = ip->i_mount;
+
+	xfs_ilock(ip, XFS_ILOCK_SHARED);
+	xfs_ip_to_stat(mp, &stat, ip);
+	xfs_iunlock(ip, XFS_ILOCK_SHARED);
+
+	if (copy_to_user( statp, &stat, sizeof(stat)))
+		return(EFAULT);
+	return(0);
+}
+
+
+/* We currently only support a maximum of one managed region per file, and
+   use the DM_EVENT_READ, DM_EVENT_WRITE, and DM_EVENT_TRUNCATE events in
+   the file's dm_eventset_t event mask to implement the DM_REGION_READ,
+   DM_REGION_WRITE, and DM_REGION_TRUNCATE flags for that single region.
+*/
+
+STATIC int
+xfs_dm_get_region(
+	vnode_t		*vp,
+	dm_right_t	right,
+	u_int		nelem,
+	dm_region_t	*regbufp,
+	u_int		*nelemp)
+{
+	dm_eventset_t	evmask;
+	dm_region_t	region;
+	xfs_inode_t	*ip;
+	u_int		elem;
+	bhv_desc_t	*xbdp;
+
+	if (right < DM_RIGHT_SHARED)
+		return(EACCES);
+
+	XFS_BHV_LOOKUP(vp, xbdp);
+
+	ip = XFS_BHVTOI(xbdp);
+	evmask = ip->i_d.di_dmevmask;	/* read the mask "atomically" */
+
+	/* Get the file's current managed region flags out of the
+	   dm_eventset_t mask and use them to build a managed region that
+	   covers the entire file, i.e. set rg_offset and rg_size to zero.
+	*/
+
+	memset((char *)&region, 0, sizeof(region));
+
+	if (evmask & (1 << DM_EVENT_READ))
+		region.rg_flags |= DM_REGION_READ;
+	if (evmask & (1 << DM_EVENT_WRITE))
+		region.rg_flags |= DM_REGION_WRITE;
+	if (evmask & (1 << DM_EVENT_TRUNCATE))
+		region.rg_flags |= DM_REGION_TRUNCATE;
+
+	elem = (region.rg_flags ? 1 : 0);
+
+	if (copy_to_user( nelemp, &elem, sizeof(elem)))
+		return(EFAULT);
+	if (elem > nelem)
+		return(E2BIG);
+	if (elem && copy_to_user(regbufp, &region, sizeof(region)))
+		return(EFAULT);
+	return(0);
+}
+
+
+STATIC int
+xfs_dm_getall_dmattr(
+	vnode_t		*vp,
+	dm_right_t	right,
+	size_t		buflen,
+	void		*bufp,
+	size_t		*rlenp)
+{
+	attrlist_cursor_kern_t cursor;
+	attrlist_t	*attrlist;
+	dm_attrlist_t	*ulist;
+	int		*last_link;
+	int		alignment;
+	int		total_size;
+	int		list_size = 8192;	/* should be big enough */
+	int		error;
+
+	if (right < DM_RIGHT_SHARED)
+		return(EACCES);
+
+	/* Verify that the user gave us a buffer that is 4-byte aligned, lock
+	   it down, and work directly within that buffer.  As a side-effect,
+	   values of buflen < sizeof(int) return EINVAL.
+	*/
+
+	alignment = sizeof(int) - 1;
+	if (((__psint_t)bufp & alignment) != 0) {
+		return(EFAULT);
+	}
+	buflen &= ~alignment;		/* round down the alignment */
+
+#if defined(HAVE_USERACC)
+	if ((error = useracc(bufp, buflen, B_READ, NULL)) != 0)
+		return error;
+#endif
+
+	/* Initialize all the structures and variables for the main loop. */
+
+	memset(&cursor, 0, sizeof(cursor));
+	attrlist = (attrlist_t *)kmem_alloc(list_size, KM_SLEEP);
+	total_size = 0;
+	ulist = (dm_attrlist_t *)bufp;
+	last_link = NULL;
+
+	/* Use VOP_ATTR_LIST to get the names of DMAPI attributes, and use
+	   VOP_ATTR_GET to get their values.  There is a risk here that the
+	   DMAPI attributes could change between the VOP_ATTR_LIST and
+	   VOP_ATTR_GET calls.	If we can detect it, we return EIO to notify
+	   the user.
+	*/
+
+	do {
+		int	i;
+
+		/* Get a buffer full of attribute names.  If there aren't any
+		   more or if we encounter an error, then finish up.
+		*/
+
+		VOP_ATTR_LIST(vp, (char *)attrlist, list_size,
+			ATTR_ROOT, &cursor, NULL, error);
+		DM_EA_XLATE_ERR(error);
+
+		if (error || attrlist->al_count == 0)
+			break;
+
+		for (i = 0; i < attrlist->al_count; i++) {
+			attrlist_ent_t	*entry;
+			char		*user_name;
+			int		size_needed;
+			int		value_len;
+
+			/* Skip over all non-DMAPI attributes.	If the
+			   attribute name is too long, we assume it is
+			   non-DMAPI even if it starts with the correct
+			   prefix.
+			*/
+
+			entry = ATTR_ENTRY(attrlist, i);
+			if (strncmp(entry->a_name, dmattr_prefix, DMATTR_PREFIXLEN))
+				continue;
+			user_name = &entry->a_name[DMATTR_PREFIXLEN];
+			if (strlen(user_name) > DM_ATTR_NAME_SIZE)
+				continue;
+
+			/* We have a valid DMAPI attribute to return.  If it
+			   won't fit in the user's buffer, we still need to
+			   keep track of the number of bytes for the user's
+			   next call.
+			*/
+
+
+			size_needed = sizeof(*ulist) + entry->a_valuelen;
+			size_needed = (size_needed + alignment) & ~alignment;
+
+			total_size += size_needed;
+			if (total_size > buflen)
+				continue;
+
+			/* Start by filling in all the fields in the
+			   dm_attrlist_t structure.
+			*/
+
+			strncpy((char *)ulist->al_name.an_chars, user_name,
+				DM_ATTR_NAME_SIZE);
+			ulist->al_data.vd_offset = sizeof(*ulist);
+			ulist->al_data.vd_length = entry->a_valuelen;
+			ulist->_link =	size_needed;
+			last_link = &ulist->_link;
+
+			/* Next read the attribute's value into its correct
+			   location after the dm_attrlist structure.  Any sort
+			   of error indicates that the data is moving under us,
+			   so we return EIO to let the user know.
+			*/
+
+			value_len = entry->a_valuelen;
+
+			VOP_ATTR_GET(vp, entry->a_name,
+				(void *)(ulist + 1), &value_len,
+				ATTR_ROOT, NULL, error);
+			DM_EA_XLATE_ERR(error);
+
+			if (error || value_len != entry->a_valuelen) {
+				error = EIO;
+				break;
+			}
+
+			ulist = (dm_attrlist_t *)((char *)ulist + ulist->_link);
+		}
+	} while (!error && attrlist->al_more);
+	if (last_link)
+		*last_link = 0;
+
+	if (!error && total_size > buflen)
+		error = E2BIG;
+	if (!error || error == E2BIG) {
+		if (put_user(total_size, rlenp))
+			error = EFAULT;
+	}
+
+	kmem_free(attrlist, list_size);
+	return(error);
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_getall_inherit(
+	vnode_t		*vp,
+	dm_right_t	right,
+	u_int		nelem,
+	dm_inherit_t	*inheritbufp,
+	u_int		*nelemp)
+{
+	return(ENOSYS);
+}
+
+
+/* Initialize location pointer for subsequent dm_get_dirattrs,
+   dm_get_bulkattr, and dm_get_bulkall calls.  The same initialization must
+   work for vnode-based routines (dm_get_dirattrs) and filesystem-based
+   routines (dm_get_bulkattr and dm_get_bulkall).  Filesystem-based functions
+   call this routine using the filesystem's root vnode.
+*/
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_init_attrloc(
+	vnode_t		*vp,
+	dm_right_t	right,
+	dm_attrloc_t	*locp)
+{
+	dm_attrloc_t	loc = 0;
+
+	if (right < DM_RIGHT_SHARED)
+		return(EACCES);
+
+	if (copy_to_user( locp, &loc, sizeof(loc)))
+		return(EFAULT);
+	return(0);
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_mkdir_by_handle(
+	vnode_t		*vp,
+	dm_right_t	right,
+	void		*hanp,
+	size_t		hlen,
+	char		*cname)
+{
+	return(ENOSYS);
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_probe_hole(
+	vnode_t		*vp,
+	dm_right_t	right,
+	dm_off_t	off,
+	dm_size_t	len,		/* we ignore this for now */
+	dm_off_t	*roffp,
+	dm_size_t	*rlenp)
+{
+	dm_off_t	roff;
+	dm_size_t	rlen;
+	xfs_inode_t	*ip;
+	xfs_mount_t	*mp;
+	uint		lock_flags;
+	xfs_fsize_t	realsize;
+	u_int		bsize;
+	bhv_desc_t	*xbdp;
+
+	if (right < DM_RIGHT_SHARED)
+		return(EACCES);
+
+	XFS_BHV_LOOKUP(vp, xbdp);
+
+	ip = XFS_BHVTOI(xbdp);
+	if ((ip->i_d.di_mode & S_IFMT) != S_IFREG)
+		return(EINVAL);
+
+	mp = ip->i_mount;
+	bsize = mp->m_sb.sb_blocksize;
+
+	lock_flags = XFS_ILOCK_EXCL | XFS_IOLOCK_EXCL;
+	xfs_ilock(ip, lock_flags);
+	realsize = ip->i_d.di_size;
+	xfs_iunlock(ip, lock_flags);
+	if (off >= realsize)
+		return(E2BIG);
+
+	roff = (off + bsize-1) & ~(bsize-1);
+	rlen = 0;		/* Only support punches to EOF for now */
+	if (copy_to_user( roffp, &roff, sizeof(roff)))
+		return(EFAULT);
+	if (copy_to_user( rlenp, &rlen, sizeof(rlen)))
+		return(EFAULT);
+	return(0);
+}
+
+
+STATIC int
+xfs_dm_punch_hole(
+	vnode_t		*vp,
+	dm_right_t	right,
+	dm_off_t	off,
+	dm_size_t	len)
+{
+	xfs_inode_t	*ip;
+	xfs_trans_t	*tp;
+	xfs_trans_t	*tp2;
+	xfs_mount_t	*mp;
+	int		error;
+	uint		lock_flags;
+	uint		commit_flags;
+	xfs_fsize_t	realsize;
+	u_int		bsize;
+	bhv_desc_t	*xbdp;
+
+	if (right < DM_RIGHT_EXCL)
+		return(EACCES);
+
+	if (vp->v_type != VREG)
+		return(EINVAL);
+	if (len != 0)	/* Only support punches to EOF for now */
+		return(EAGAIN);
+	if (VN_MAPPED(vp))
+		return(EBUSY);
+
+	XFS_BHV_LOOKUP(vp, xbdp);
+
+	ip = XFS_BHVTOI(xbdp);
+	mp = ip->i_mount;
+	bsize = mp->m_sb.sb_blocksize;
+
+	if (off & (bsize-1))
+		return(EAGAIN);
+
+	lock_flags = XFS_ILOCK_EXCL | XFS_IOLOCK_EXCL;
+	xfs_ilock(ip, lock_flags);
+
+	realsize = ip->i_d.di_size;	/* saved size to restore to */
+	if (off >= realsize) {	/* also check block boundary */
+		xfs_iunlock(ip, lock_flags);
+		return(EINVAL);
+	}
+
+	/*
+	 * Before we join the inode to the transaction, take care of
+	 * the part of the truncation that must be done without the
+	 * inode lock.	This needs to be done before joining the inode
+	 * to the transaction, because the inode cannot be unlocked
+	 * once it is a part of the transaction.
+	 */
+	xfs_iunlock(ip, XFS_ILOCK_EXCL);
+	tp = xfs_trans_alloc(mp, XFS_TRANS_SETATTR_SIZE);
+	if ((error = xfs_trans_reserve(tp, 0,
+				     XFS_ITRUNCATE_LOG_RES(mp), 0,
+				     XFS_TRANS_PERM_LOG_RES,
+				     XFS_ITRUNCATE_LOG_COUNT))) {
+		xfs_trans_cancel(tp, 0);
+		xfs_iunlock(ip, XFS_IOLOCK_EXCL);
+		return(error);
+	}
+	commit_flags = XFS_TRANS_RELEASE_LOG_RES;
+	/* --- start of truncate --- */
+	xfs_itruncate_start(ip, XFS_ITRUNC_DEFINITE, (xfs_fsize_t) off);
+	xfs_ilock(ip, XFS_ILOCK_EXCL);
+	xfs_trans_ijoin(tp, ip, lock_flags);
+	xfs_trans_ihold(tp, ip);
+	xfs_itruncate_finish(&tp, ip, (xfs_fsize_t) off, XFS_DATA_FORK, 0);
+
+	/*
+	 * If this is a synchronous mount, make sure that the
+	 * transaction goes to disk before returning to the user.
+	 */
+	if (mp->m_flags & XFS_MOUNT_WSYNC) {
+		xfs_trans_set_sync(tp);
+	}
+	xfs_trans_commit(tp, commit_flags, NULL);
+	/* --- end of truncate --- */
+
+	/* --- start of grow --- */
+	tp2 = xfs_trans_alloc(mp, XFS_TRANS_SETATTR_SIZE);
+	if ((error = xfs_trans_reserve(tp2, 0,
+				     XFS_ITRUNCATE_LOG_RES(mp), 0,
+				     XFS_TRANS_PERM_LOG_RES,
+				     XFS_ITRUNCATE_LOG_COUNT))) {
+		xfs_trans_cancel(tp, 0);
+		xfs_trans_cancel(tp2, 0);
+		xfs_iunlock(ip, XFS_IOLOCK_EXCL);
+		return(error);
+	}
+	/* ip left locked after previous commit */
+	xfs_igrow_start(ip, realsize, NULL);
+	xfs_trans_ijoin(tp2, ip, lock_flags);
+	xfs_igrow_finish(tp2, ip, realsize, 0);
+
+	/* Let threads in send_data_event know we punched the file. */
+	ip->i_iocore.io_dmstate++;
+
+	VN_HOLD(vp);
+	if (mp->m_flags & XFS_MOUNT_WSYNC) {
+		xfs_trans_set_sync(tp2);
+	}
+	xfs_trans_commit(tp2, commit_flags, NULL);
+	/* --- end of grow --- */
+
+	/* ip unlocked during the commit */
+	return(0);
+}
+
+
+STATIC int
+xfs_dm_read_invis_rvp(
+	vnode_t		*vp,
+	dm_right_t	right,
+	dm_off_t	off,
+	dm_size_t	len,
+	void		*bufp,
+	int		*rvp)
+{
+	if (right < DM_RIGHT_SHARED)
+		return(EACCES);
+
+	return(xfs_dm_rdwr(vp, 0, FMODE_READ, off, len, bufp, rvp));
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_release_right(
+	vnode_t		*vp,
+	dm_right_t	right,
+	u_int		type)		/* DM_FSYS_OBJ or zero */
+{
+#ifdef	DEBUG_RIGHTS
+	char		buffer[sizeof(xfs_handle_t) * 2 + 1];
+	bhv_desc_t	*bdp;
+
+	XFS_BHV_LOOKUP(vp, bdp);
+
+	if (!xfs_bdp_to_hexhandle(bdp, type, buffer)) {
+		printf("dm_release_right: old %d type %d handle %s\n",
+			right, type, buffer);
+	} else {
+		printf("dm_release_right: old %d type %d handle "
+			" <INVALID>\n", right, type);
+	}
+#endif	/* DEBUG_RIGHTS */
+	return(0);
+}
+
+
+STATIC int
+xfs_dm_remove_dmattr(
+	vnode_t		*vp,
+	dm_right_t	right,
+	int		setdtime,
+	dm_attrname_t	*attrnamep)
+{
+	dm_dkattrname_t name;
+	int		error;
+
+	if (right < DM_RIGHT_EXCL)
+		return(EACCES);
+
+	if ((error = xfs_copyin_attrname(attrnamep, &name)) != 0)
+		return(error);
+
+	/* Remove the attribute from the object. */
+
+	VOP_ATTR_REMOVE(vp, name.dan_chars,
+			(setdtime ? ATTR_ROOT : ATTR_ROOT|ATTR_KERNOTIME),
+			NULL, error);
+	DM_EA_XLATE_ERR(error);
+
+	if (error == ENOATTR)
+		error = ENOENT;
+	return(error);
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_request_right(
+	vnode_t		*vp,
+	dm_right_t	right,
+	u_int		type,		/* DM_FSYS_OBJ or zero */
+	u_int		flags,
+	dm_right_t	newright)
+{
+#ifdef	DEBUG_RIGHTS
+	char		buffer[sizeof(xfs_handle_t) * 2 + 1];
+	bhv_desc_t	*bdp;
+
+	XFS_BHV_LOOKUP(vp, bdp);
+
+	if (!xfs_bdp_to_hexhandle(bdp, type, buffer)) {
+		printf("dm_request_right: old %d new %d type %d flags 0x%x "
+			"handle %s\n", right, newright, type, flags, buffer);
+	} else {
+		printf("dm_request_right: old %d new %d type %d flags 0x%x "
+			"handle <INVALID>\n", right, newright, type, flags);
+	}
+#endif	/* DEBUG_RIGHTS */
+	return(0);
+}
+
+
+STATIC int
+xfs_dm_set_dmattr(
+	vnode_t		*vp,
+	dm_right_t	right,
+	dm_attrname_t	*attrnamep,
+	int		setdtime,
+	size_t		buflen,
+	void		*bufp)
+{
+	dm_dkattrname_t name;
+	char		*value;
+	int		alloc_size;
+	int		error;
+
+	if (right < DM_RIGHT_EXCL)
+		return(EACCES);
+
+	if ((error = xfs_copyin_attrname(attrnamep, &name)) != 0)
+		return(error);
+	if (buflen > ATTR_MAX_VALUELEN)
+		return(E2BIG);
+
+	/* Copy in the attribute's value and store the <name,value> pair in
+	   the object.	We allocate a buffer of at least one byte even if the
+	   caller specified a buflen of zero.  (A buflen of zero is considered
+	   valid.)
+	*/
+
+	alloc_size = (buflen == 0) ? 1 : buflen;
+	value = kmem_alloc(alloc_size, KM_SLEEP);
+	if (copy_from_user( value, bufp, buflen)) {
+		error = EFAULT;
+	} else {
+		VOP_ATTR_SET(vp, name.dan_chars, value, buflen,
+			(setdtime ? ATTR_ROOT : ATTR_ROOT|ATTR_KERNOTIME),
+			NULL, error);
+		DM_EA_XLATE_ERR(error);
+	}
+	kmem_free(value, alloc_size);
+	return(error);
+}
+
+STATIC int
+xfs_dm_set_eventlist(
+	vnode_t		*vp,
+	dm_right_t	right,
+	u_int		type,
+	dm_eventset_t	*eventsetp,	/* in kernel space! */
+	u_int		maxevent)
+{
+	int		error;
+	bhv_desc_t	*xbdp;
+
+	XFS_BHV_LOOKUP(vp, xbdp);
+
+	if (type == DM_FSYS_OBJ) {
+		error = xfs_dm_fs_set_eventlist(xbdp, right, eventsetp, maxevent);
+	} else {
+		error = xfs_dm_f_set_eventlist(xbdp, right, eventsetp, maxevent);
+	}
+	return(error);
+}
+
+
+/*
+ *  This turned out not XFS-specific, but leave it here with get_fileattr.
+ */
+
+STATIC int
+xfs_dm_set_fileattr(
+	vnode_t		*vp,
+	dm_right_t	right,
+	u_int		mask,
+	dm_fileattr_t	*statp)
+{
+	dm_fileattr_t	stat;
+	vattr_t		vat;
+	int		error;
+
+	if (right < DM_RIGHT_EXCL)
+		return(EACCES);
+
+	if (copy_from_user( &stat, statp, sizeof(stat)))
+		return(EFAULT);
+
+	vat.va_mask = 0;
+
+	if (mask & DM_AT_MODE) {
+		vat.va_mask |= XFS_AT_MODE;
+		vat.va_mode = stat.fa_mode;
+	}
+	if (mask & DM_AT_UID) {
+		vat.va_mask |= XFS_AT_UID;
+		vat.va_uid = stat.fa_uid;
+	}
+	if (mask & DM_AT_GID) {
+		vat.va_mask |= XFS_AT_GID;
+		vat.va_gid = stat.fa_gid;
+	}
+	if (mask & DM_AT_ATIME) {
+		vat.va_mask |= XFS_AT_ATIME;
+		vat.va_atime.tv_sec = stat.fa_atime;
+		vat.va_atime.tv_nsec = 0;
+	}
+	if (mask & DM_AT_MTIME) {
+		vat.va_mask |= XFS_AT_MTIME;
+		vat.va_mtime.tv_sec = stat.fa_mtime;
+		vat.va_mtime.tv_nsec = 0;
+	}
+	if (mask & DM_AT_CTIME) {
+		vat.va_mask |= XFS_AT_CTIME;
+		vat.va_ctime.tv_sec = stat.fa_ctime;
+		vat.va_ctime.tv_nsec = 0;
+	}
+
+	/* DM_AT_DTIME only takes effect if DM_AT_CTIME is not specified.  We
+	   overload ctime to also act as dtime, i.e. DM_CONFIG_DTIME_OVERLOAD.
+	*/
+
+	if ((mask & DM_AT_DTIME) && !(mask & DM_AT_CTIME)) {
+		vat.va_mask |= XFS_AT_CTIME;
+		vat.va_ctime.tv_sec = stat.fa_dtime;
+		vat.va_ctime.tv_nsec = 0;
+	}
+	if (mask & DM_AT_SIZE) {
+		vat.va_mask |= XFS_AT_SIZE;
+		vat.va_size = stat.fa_size;
+	}
+
+	VOP_SETATTR(vp, &vat, ATTR_DMI, NULL, error);
+	return(error);
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_set_inherit(
+	vnode_t		*vp,
+	dm_right_t	right,
+	dm_attrname_t	*attrnamep,
+	mode_t		mode)
+{
+	return(ENOSYS);
+}
+
+
+STATIC int
+xfs_dm_set_region(
+	vnode_t		*vp,
+	dm_right_t	right,
+	u_int		nelem,
+	dm_region_t	*regbufp,
+	dm_boolean_t	*exactflagp)
+{
+	xfs_inode_t	*ip;
+	xfs_trans_t	*tp;
+	xfs_mount_t	*mp;
+	dm_region_t	region;
+	dm_eventset_t	new_mask;
+	dm_eventset_t	mr_mask;
+	int		error;
+	u_int		exactflag;
+	bhv_desc_t	*xbdp;
+
+	if (right < DM_RIGHT_EXCL)
+		return(EACCES);
+
+	/* If the caller gave us more than one dm_region_t structure, complain.
+	   (He has to call dm_get_config() to find out what our limit is.)
+	*/
+
+	if (nelem > 1)
+		return(E2BIG);
+
+	/* If the user provided a dm_region_t structure, then copy it in,
+	   validate it, and convert its flags to the corresponding bits in a
+	   dm_set_eventlist() event mask.  A call with zero regions is
+	   equivalent to clearing all region flags.
+	*/
+
+	new_mask = 0;
+	if (nelem == 1) {
+		if (copy_from_user( &region, regbufp, sizeof(region)))
+			return(EFAULT);
+
+		if (region.rg_flags & ~(DM_REGION_READ|DM_REGION_WRITE|DM_REGION_TRUNCATE))
+			return(EINVAL);
+		if (region.rg_flags & DM_REGION_READ)
+			new_mask |= 1 << DM_EVENT_READ;
+		if (region.rg_flags & DM_REGION_WRITE)
+			new_mask |= 1 << DM_EVENT_WRITE;
+		if (region.rg_flags & DM_REGION_TRUNCATE)
+			new_mask |= 1 << DM_EVENT_TRUNCATE;
+	}
+	if ((new_mask & prohibited_mr_events(vp)) != 0)
+		return(EBUSY);
+	mr_mask = (1 << DM_EVENT_READ) | (1 << DM_EVENT_WRITE) | (1 << DM_EVENT_TRUNCATE);
+
+	/* Get the file's existing event mask, clear the old managed region
+	   bits, add in the new ones, and update the file's mask.
+	*/
+
+	XFS_BHV_LOOKUP(vp, xbdp);
+
+	ip = XFS_BHVTOI(xbdp);
+	mp = ip->i_mount;
+	tp = xfs_trans_alloc(mp, XFS_TRANS_SET_DMATTRS);
+	error = xfs_trans_reserve(tp, 0, XFS_ICHANGE_LOG_RES (mp), 0, 0, 0);
+	if (error) {
+		xfs_trans_cancel(tp, 0);
+		return(error);
+	}
+	xfs_ilock(ip, XFS_ILOCK_EXCL);
+	xfs_trans_ijoin(tp, ip, XFS_ILOCK_EXCL);
+
+	ip->i_d.di_dmevmask = (ip->i_d.di_dmevmask & ~mr_mask) | new_mask;
+	ip->i_iocore.io_dmevmask = ip->i_d.di_dmevmask;
+
+	xfs_trans_log_inode(tp, ip, XFS_ILOG_CORE);
+	VN_HOLD(vp);
+	xfs_trans_commit(tp, 0, NULL);
+
+	/* Return the proper value for *exactflagp depending upon whether or not
+	   we "changed" the user's managed region.  In other words, if the user
+	   specified a non-zero value for either rg_offset or rg_size, we
+	   round each of those values back to zero.
+	*/
+
+	if (nelem && (region.rg_offset || region.rg_size)) {
+		exactflag = DM_FALSE;	/* user region was changed */
+	} else {
+		exactflag = DM_TRUE;	/* user region was unchanged */
+	}
+	if (copy_to_user( exactflagp, &exactflag, sizeof(exactflag)))
+		return(EFAULT);
+	return(0);
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_symlink_by_handle(
+	vnode_t		*vp,
+	dm_right_t	right,
+	void		*hanp,
+	size_t		hlen,
+	char		*cname,
+	char		*path)
+{
+	return(ENOSYS);
+}
+
+
+STATIC int
+xfs_dm_sync_by_handle (
+	vnode_t		*vp,
+	dm_right_t	right)
+{
+	int		error;
+
+	if (right < DM_RIGHT_EXCL)
+		return(EACCES);
+
+	VOP_FSYNC(vp, FSYNC_WAIT, NULL, (xfs_off_t)0, (xfs_off_t)-1, error);
+	return(error);
+}
+
+
+/* ARGSUSED */
+STATIC int
+xfs_dm_upgrade_right(
+	vnode_t		*vp,
+	dm_right_t	right,
+	u_int		type)		/* DM_FSYS_OBJ or zero */
+{
+#ifdef	DEBUG_RIGHTS
+	char		buffer[sizeof(xfs_handle_t) * 2 + 1];
+	bhv_desc_t	*bdp;
+
+	XFS_BHV_LOOKUP(vp, bdp);
+
+	if (!xfs_bdp_to_hexhandle(bdp, type, buffer)) {
+		printf("dm_upgrade_right: old %d new %d type %d handle %s\n",
+			right, DM_RIGHT_EXCL, type, buffer);
+	} else {
+		printf("dm_upgrade_right: old %d new %d type %d handle "
+			"<INVALID>\n", right, DM_RIGHT_EXCL, type);
+	}
+#endif	/* DEBUG_RIGHTS */
+	return(0);
+}
+
+
+STATIC int
+xfs_dm_write_invis_rvp(
+	vnode_t		*vp,
+	dm_right_t	right,
+	int		flags,
+	dm_off_t	off,
+	dm_size_t	len,
+	void		*bufp,
+	int		*rvp)
+{
+	int		fflag = 0;
+
+	if (right < DM_RIGHT_EXCL)
+		return(EACCES);
+
+	if (flags & DM_WRITE_SYNC)
+		fflag |= O_SYNC;
+	return(xfs_dm_rdwr(vp, fflag, FMODE_WRITE, off, len, bufp, rvp));
+}
+
+
+STATIC void
+xfs_dm_obj_ref_hold(
+	vnode_t		*vp)
+{
+	VN_HOLD(vp);
+}
+
+
+STATIC fsys_function_vector_t	xfs_fsys_vector[DM_FSYS_MAX];
+
+
+int
+xfs_dm_get_fsys_vector(
+	bhv_desc_t	*bdp,
+	caddr_t		addr)
+{
+	static	int		initialized = 0;
+	dm_fcntl_vector_t	*vecrq;
+	fsys_function_vector_t	*vecp;
+	int			i = 0;
+
+	vecrq = (dm_fcntl_vector_t *)addr;
+	vecrq->count =
+		sizeof(xfs_fsys_vector) / sizeof(xfs_fsys_vector[0]);
+	vecrq->vecp = xfs_fsys_vector;
+	if (initialized)
+		return(0);
+	vecrq->code_level = DM_CLVL_XOPEN;
+	vecp = xfs_fsys_vector;
+
+	vecp[i].func_no = DM_FSYS_CLEAR_INHERIT;
+	vecp[i++].u_fc.clear_inherit = xfs_dm_clear_inherit;
+	vecp[i].func_no = DM_FSYS_CREATE_BY_HANDLE;
+	vecp[i++].u_fc.create_by_handle = xfs_dm_create_by_handle;
+	vecp[i].func_no = DM_FSYS_DOWNGRADE_RIGHT;
+	vecp[i++].u_fc.downgrade_right = xfs_dm_downgrade_right;
+	vecp[i].func_no = DM_FSYS_GET_ALLOCINFO_RVP;
+	vecp[i++].u_fc.get_allocinfo_rvp = xfs_dm_get_allocinfo_rvp;
+	vecp[i].func_no = DM_FSYS_GET_BULKALL_RVP;
+	vecp[i++].u_fc.get_bulkall_rvp = xfs_dm_get_bulkall_rvp;
+	vecp[i].func_no = DM_FSYS_GET_BULKATTR_RVP;
+	vecp[i++].u_fc.get_bulkattr_rvp = xfs_dm_get_bulkattr_rvp;
+	vecp[i].func_no = DM_FSYS_GET_CONFIG;
+	vecp[i++].u_fc.get_config = xfs_dm_get_config;
+	vecp[i].func_no = DM_FSYS_GET_CONFIG_EVENTS;
+	vecp[i++].u_fc.get_config_events = xfs_dm_get_config_events;
+	vecp[i].func_no = DM_FSYS_GET_DESTROY_DMATTR;
+	vecp[i++].u_fc.get_destroy_dmattr = xfs_dm_get_destroy_dmattr;
+	vecp[i].func_no = DM_FSYS_GET_DIOINFO;
+	vecp[i++].u_fc.get_dioinfo = xfs_dm_get_dioinfo;
+	vecp[i].func_no = DM_FSYS_GET_DIRATTRS_RVP;
+	vecp[i++].u_fc.get_dirattrs_rvp = xfs_dm_get_dirattrs_rvp;
+	vecp[i].func_no = DM_FSYS_GET_DMATTR;
+	vecp[i++].u_fc.get_dmattr = xfs_dm_get_dmattr;
+	vecp[i].func_no = DM_FSYS_GET_EVENTLIST;
+	vecp[i++].u_fc.get_eventlist = xfs_dm_get_eventlist;
+	vecp[i].func_no = DM_FSYS_GET_FILEATTR;
+	vecp[i++].u_fc.get_fileattr = xfs_dm_get_fileattr;
+	vecp[i].func_no = DM_FSYS_GET_REGION;
+	vecp[i++].u_fc.get_region = xfs_dm_get_region;
+	vecp[i].func_no = DM_FSYS_GETALL_DMATTR;
+	vecp[i++].u_fc.getall_dmattr = xfs_dm_getall_dmattr;
+	vecp[i].func_no = DM_FSYS_GETALL_INHERIT;
+	vecp[i++].u_fc.getall_inherit = xfs_dm_getall_inherit;
+	vecp[i].func_no = DM_FSYS_INIT_ATTRLOC;
+	vecp[i++].u_fc.init_attrloc = xfs_dm_init_attrloc;
+	vecp[i].func_no = DM_FSYS_MKDIR_BY_HANDLE;
+	vecp[i++].u_fc.mkdir_by_handle = xfs_dm_mkdir_by_handle;
+	vecp[i].func_no = DM_FSYS_PROBE_HOLE;
+	vecp[i++].u_fc.probe_hole = xfs_dm_probe_hole;
+	vecp[i].func_no = DM_FSYS_PUNCH_HOLE;
+	vecp[i++].u_fc.punch_hole = xfs_dm_punch_hole;
+	vecp[i].func_no = DM_FSYS_READ_INVIS_RVP;
+	vecp[i++].u_fc.read_invis_rvp = xfs_dm_read_invis_rvp;
+	vecp[i].func_no = DM_FSYS_RELEASE_RIGHT;
+	vecp[i++].u_fc.release_right = xfs_dm_release_right;
+	vecp[i].func_no = DM_FSYS_REMOVE_DMATTR;
+	vecp[i++].u_fc.remove_dmattr = xfs_dm_remove_dmattr;
+	vecp[i].func_no = DM_FSYS_REQUEST_RIGHT;
+	vecp[i++].u_fc.request_right = xfs_dm_request_right;
+	vecp[i].func_no = DM_FSYS_SET_DMATTR;
+	vecp[i++].u_fc.set_dmattr = xfs_dm_set_dmattr;
+	vecp[i].func_no = DM_FSYS_SET_EVENTLIST;
+	vecp[i++].u_fc.set_eventlist = xfs_dm_set_eventlist;
+	vecp[i].func_no = DM_FSYS_SET_FILEATTR;
+	vecp[i++].u_fc.set_fileattr = xfs_dm_set_fileattr;
+	vecp[i].func_no = DM_FSYS_SET_INHERIT;
+	vecp[i++].u_fc.set_inherit = xfs_dm_set_inherit;
+	vecp[i].func_no = DM_FSYS_SET_REGION;
+	vecp[i++].u_fc.set_region = xfs_dm_set_region;
+	vecp[i].func_no = DM_FSYS_SYMLINK_BY_HANDLE;
+	vecp[i++].u_fc.symlink_by_handle = xfs_dm_symlink_by_handle;
+	vecp[i].func_no = DM_FSYS_SYNC_BY_HANDLE;
+	vecp[i++].u_fc.sync_by_handle = xfs_dm_sync_by_handle;
+	vecp[i].func_no = DM_FSYS_UPGRADE_RIGHT;
+	vecp[i++].u_fc.upgrade_right = xfs_dm_upgrade_right;
+	vecp[i].func_no = DM_FSYS_WRITE_INVIS_RVP;
+	vecp[i++].u_fc.write_invis_rvp = xfs_dm_write_invis_rvp;
+	vecp[i].func_no = DM_FSYS_OBJ_REF_HOLD;
+	vecp[i++].u_fc.obj_ref_hold = xfs_dm_obj_ref_hold;
+
+	return(0);
+}
+
+
+/*	xfs_dm_mapevent - send events needed for memory mapping a file.
+ *
+ *	xfs_dm_map is a workaround called for files that are about to be
+ *	mapped.	 DMAPI events are not being generated at a low enough level
+ *	in the kernel for page reads/writes to generate the correct events.
+ *	So for memory-mapped files we generate read  or write events for the
+ *	whole byte range being mapped.	If the mmap call can never cause a
+ *	write to the file, then only a read event is sent.
+ *
+ *	Code elsewhere prevents adding managed regions to a file while it
+ *	is still mapped.
+ */
+
+/* ARGSUSED */
+static int
+xfs_dm_mapevent(
+	bhv_desc_t	*bdp,
+	int		flags,
+	xfs_off_t	offset,
+	dm_fcntl_mapevent_t *mapevp)
+{
+	xfs_fsize_t	filesize;		/* event read/write "size" */
+	xfs_inode_t	*ip;
+	xfs_off_t	end_of_area, evsize;
+	vnode_t		*vp = BHV_TO_VNODE(bdp);
+	struct vfs	*vfsp = vp->v_vfsp;
+
+	/* exit immediately if not regular file in a DMAPI file system */
+
+	mapevp->error = 0;			/* assume success */
+
+	if ((vp->v_type != VREG) || !(vfsp->vfs_flag & VFS_DMI))
+		return 0;
+
+	if (mapevp->max_event != DM_EVENT_WRITE &&
+		mapevp->max_event != DM_EVENT_READ)
+			return 0;
+
+	/* Set file size to work with. */
+
+	ip = XFS_BHVTOI(bdp);
+	filesize = ip->i_iocore.io_new_size;
+	if (filesize < ip->i_d.di_size) {
+		filesize = ip->i_d.di_size;
+	}
+
+	/* Set first byte number beyond the map area. */
+
+	if (mapevp->length) {
+		end_of_area = offset + mapevp->length;
+		if (end_of_area > filesize)
+			end_of_area = filesize;
+	} else {
+		end_of_area = filesize;
+	}
+
+	/* Set the real amount being mapped. */
+	evsize = end_of_area - offset;
+	if (evsize < 0)
+		evsize = 0;
+
+	/* If write possible, try a DMAPI write event */
+	if (mapevp->max_event == DM_EVENT_WRITE &&
+		DM_EVENT_ENABLED (vp->v_vfsp, ip, DM_EVENT_WRITE)) {
+		mapevp->error = xfs_dm_send_data_event(DM_EVENT_WRITE, vp,
+				offset, evsize, 0, NULL);
+		return(0);
+	}
+
+	/* Try a read event if max_event was != DM_EVENT_WRITE or if it
+	 * was DM_EVENT_WRITE but the WRITE event was not enabled.
+	 */
+	if (DM_EVENT_ENABLED (vp->v_vfsp, ip, DM_EVENT_READ)) {
+		mapevp->error = xfs_dm_send_data_event(DM_EVENT_READ, vp,
+				offset, evsize, 0, NULL);
+	}
+
+	return 0;
+}
+
+
+int
+xfs_dm_send_mmap_event(
+	struct vm_area_struct *vma,
+	unsigned int	wantflag)
+{
+	vnode_t		*vp;
+	xfs_inode_t	*ip;
+	bhv_desc_t	*bdp;
+	int		ret = 0;
+	dm_fcntl_mapevent_t maprq;
+	dm_eventtype_t	max_event = DM_EVENT_READ;
+
+	if (!vma->vm_file)
+		return 0;
+
+	vp = LINVFS_GET_VP(vma->vm_file->f_dentry->d_inode);
+	ASSERT(vp);
+
+	if ((vp->v_type != VREG) || !(vp->v_vfsp->vfs_flag & VFS_DMI))
+		return 0;
+
+	/* If they specifically asked for 'read', then give it to them.
+	 * Otherwise, see if it's possible to give them 'write'.
+	 */
+	if( wantflag & VM_READ ){
+		max_event = DM_EVENT_READ;
+	}
+	else if( ! (vma->vm_flags & VM_DENYWRITE) ) {
+		if((wantflag & VM_WRITE) || (vma->vm_flags & VM_WRITE))
+			max_event = DM_EVENT_WRITE;
+	}
+
+	if( (wantflag & VM_WRITE) && (max_event != DM_EVENT_WRITE) ){
+		return -EACCES;
+	}
+
+	maprq.max_event = max_event;
+
+	/* Figure out how much of the file is being requested by the user. */
+	maprq.length = 0; /* whole file, for now */
+
+	XFS_BHV_LOOKUP(vp, bdp);
+	ip = XFS_BHVTOI(bdp);
+
+	if(DM_EVENT_ENABLED(vp->v_vfsp, ip, max_event)){
+		xfs_dm_mapevent(bdp, 0, 0, &maprq);
+		ret = maprq.error;
+	}
+
+	return -ret;
+}
+
+
+/*
+ * Data migration operations accessed by the rest of XFS.
+ * When DMAPI support is configured in, this vector is used.
+ */
+
+xfs_dmops_t	xfs_dmcore_xfs = {
+	.xfs_send_data		= xfs_dm_send_data_event,
+	.xfs_send_mmap		= xfs_dm_send_mmap_event,
+	.xfs_send_destroy	= dm_send_destroy_event,
+	.xfs_send_namesp	= dm_send_namesp_event,
+	.xfs_send_unmount	= dm_send_unmount_event,
+};
+
+
+/*
+ * DMAPI behavior module routines
+ */
+
+STATIC int
+xfs_dm_mount(
+	struct bhv_desc		*bhv,
+	struct xfs_mount_args	*args,
+	struct cred		*cr)
+{
+	struct bhv_desc		*rootbdp;
+	struct vnode		*rootvp;
+	struct vfs		*vfsp = bhvtovfs(bhv);
+	int			error = 0;
+
+	PVFS_MOUNT(BHV_NEXT(bhv), args, cr, error);
+	if (error)
+		return error;
+
+	if (args->flags & XFSMNT_DMAPI) {
+		VFS_ROOT(vfsp, &rootvp, error);
+		if (!error) {
+			rootbdp = vn_bhv_lookup_unlocked(
+					VN_BHV_HEAD(rootvp), &xfs_vnodeops);
+			VN_RELE(rootvp);
+			if (rootbdp != NULL) {
+			    vfsp->vfs_flag |= VFS_DMI;
+			    error = dm_send_mount_event(vfsp, DM_RIGHT_NULL,
+					NULL,
+					DM_RIGHT_NULL, rootvp, DM_RIGHT_NULL,
+					args->mtpt, args->fsname);
+			}
+		}
+	}
+
+	return error;
+}
+
+#define MNTOPT_DMAPI	"dmapi"		/* DMI enabled (DMAPI / XDSM) */
+#define MNTOPT_XDSM	"xdsm"		/* DMI enabled (DMAPI / XDSM) */
+#define MNTOPT_DMI	"dmi"		/* DMI enabled (DMAPI / XDSM) */
+
+STATIC int
+xfs_dm_parseargs(
+	struct bhv_desc		*bhv,
+	char			*options,
+	struct xfs_mount_args	*args,
+	int			update)
+{
+	size_t			length;
+	char			*local_options = options;
+	char			*this_char;
+	int			error;
+
+	while ((this_char = strsep(&local_options, ",")) != NULL) {
+		length = strlen(this_char);
+		if (local_options)
+			length++;
+
+		if (!strcmp(this_char, MNTOPT_DMAPI)) {
+			args->flags |= XFSMNT_DMAPI;
+		} else if (!strcmp(this_char, MNTOPT_XDSM)) {
+			args->flags |= XFSMNT_DMAPI;
+		} else if (!strcmp(this_char, MNTOPT_DMI)) {
+			args->flags |= XFSMNT_DMAPI;
+		} else {
+			if (local_options)
+				*(local_options-1) = ',';
+			continue;
+		}
+
+		while (length--)
+			*this_char++ = ',';
+	}
+
+	PVFS_PARSEARGS(BHV_NEXT(bhv), options, args, update, error);
+	if (!error && (args->flags & XFSMNT_DMAPI) && (*args->mtpt == '\0')) {
+		printk("XFS: %s option needs the mount point option as well\n",
+			MNTOPT_DMAPI);
+		error = EINVAL;
+	}
+	if (!error && !update && !(args->flags & XFSMNT_DMAPI))
+		bhv_remove_vfsops(bhvtovfs(bhv), VFS_POSITION_DM);
+	return error;
+}
+
+STATIC int
+xfs_dm_showargs(
+	struct bhv_desc		*bhv,
+	struct seq_file		*m)
+{
+	struct vfs		*vfsp = bhvtovfs(bhv);
+	int			error;
+
+	if (vfsp->vfs_flag & VFS_DMI)
+		seq_puts(m, "," MNTOPT_DMAPI);
+
+	PVFS_SHOWARGS(BHV_NEXT(bhv), m, error);
+	return error;
+}
+
+struct bhv_vfsops xfs_dmops = { {
+	BHV_IDENTITY_INIT(VFS_BHV_DM, VFS_POSITION_DM),
+	.vfs_mount		= xfs_dm_mount,
+	.vfs_parseargs		= xfs_dm_parseargs,
+	.vfs_showargs		= xfs_dm_showargs,
+	.vfs_dmapiops		= xfs_dm_get_fsys_vector, },
+};
+
+void __init
+xfs_dm_init(void)
+{
+	vfs_bhv_set_custom(&xfs_dmops, &xfs_dmcore_xfs);
+	bhv_module_init(XFS_DMOPS, THIS_MODULE, &xfs_dmops);
+}
+
+void __exit
+xfs_dm_exit(void)
+{
+	bhv_module_exit(XFS_DMOPS);
+	vfs_bhv_clr_custom(&xfs_dmops);
+}
diff -uNrp linux-2.6.5/fs/xfs/linux/xfs_aops.c linux-2.6.5.xfs/fs/xfs/linux/xfs_aops.c
--- linux-2.6.5/fs/xfs/linux/xfs_aops.c	2004-04-04 05:38:14.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/linux/xfs_aops.c	2004-04-05 01:08:11.000000000 +0200
@@ -108,7 +108,7 @@ linvfs_unwritten_done(
 	struct buffer_head	*bh,
 	int			uptodate)
 {
-	page_buf_t		*pb = (page_buf_t *)bh->b_private;
+	xfs_buf_t		*pb = (xfs_buf_t *)bh->b_private;
 
 	ASSERT(buffer_unwritten(bh));
 	bh->b_end_io = NULL;
@@ -265,9 +265,9 @@ xfs_map_at_offset(
 STATIC struct page *
 xfs_probe_unwritten_page(
 	struct address_space	*mapping,
-	unsigned long		index,
+	pgoff_t			index,
 	xfs_iomap_t		*iomapp,
-	page_buf_t		*pb,
+	xfs_buf_t		*pb,
 	unsigned long		max_offset,
 	unsigned long		*fsbs,
 	unsigned int            bbits)
@@ -316,7 +316,7 @@ out:
 STATIC unsigned int
 xfs_probe_unmapped_page(
 	struct address_space	*mapping,
-	unsigned long		index,
+	pgoff_t			index,
 	unsigned int		pg_offset)
 {
 	struct page		*page;
@@ -356,8 +356,8 @@ xfs_probe_unmapped_cluster(
 	struct buffer_head	*bh,
 	struct buffer_head	*head)
 {
-	unsigned long		tindex, tlast, tloff;
-	unsigned int		len, total = 0;
+	pgoff_t			tindex, tlast, tloff;
+	unsigned int		pg_offset, len, total = 0;
 	struct address_space	*mapping = inode->i_mapping;
 
 	/* First sum forwards in this page */
@@ -382,9 +382,9 @@ xfs_probe_unmapped_cluster(
 			total += len;
 		}
 		if (tindex == tlast &&
-		    (tloff = i_size_read(inode) & (PAGE_CACHE_SIZE - 1))) {
+		    (pg_offset = i_size_read(inode) & (PAGE_CACHE_SIZE - 1))) {
 			total += xfs_probe_unmapped_page(mapping,
-							tindex, tloff);
+							tindex, pg_offset);
 		}
 	}
 	return total;
@@ -398,7 +398,7 @@ xfs_probe_unmapped_cluster(
 STATIC struct page *
 xfs_probe_delalloc_page(
 	struct inode		*inode,
-	unsigned long		index)
+	pgoff_t			index)
 {
 	struct page		*page;
 
@@ -445,7 +445,7 @@ xfs_map_unwritten(
 {
 	struct buffer_head	*bh = curr;
 	xfs_iomap_t		*tmp;
-	page_buf_t		*pb;
+	xfs_buf_t		*pb;
 	loff_t			offset, size;
 	unsigned long		nblocks = 0;
 
@@ -497,8 +497,9 @@ xfs_map_unwritten(
 	 */
 	if (bh == head) {
 		struct address_space	*mapping = inode->i_mapping;
-		unsigned long		tindex, tloff, tlast, bs;
-		unsigned int		bbits = inode->i_blkbits;
+		pgoff_t			tindex, tloff, tlast;
+		unsigned long		bs;
+		unsigned int		pg_offset, bbits = inode->i_blkbits;
 		struct page		*page;
 
 		tlast = i_size_read(inode) >> PAGE_CACHE_SHIFT;
@@ -522,10 +523,10 @@ xfs_map_unwritten(
 		}
 
 		if (tindex == tlast &&
-		    (tloff = (i_size_read(inode) & (PAGE_CACHE_SIZE - 1)))) {
+		    (pg_offset = (i_size_read(inode) & (PAGE_CACHE_SIZE - 1)))) {
 			page = xfs_probe_unwritten_page(mapping,
 							tindex, iomapp, pb,
-							tloff, &bs, bbits);
+							pg_offset, &bs, bbits);
 			if (page) {
 				nblocks += bs;
 				atomic_add(bs, &pb->pb_io_remaining);
@@ -603,7 +604,8 @@ xfs_convert_page(
 {
 	struct buffer_head	*bh_arr[MAX_BUF_PER_PAGE], *bh, *head;
 	xfs_iomap_t		*mp = iomapp, *tmp;
-	unsigned long		end, offset, end_index;
+	unsigned long		end, offset;
+	pgoff_t			end_index;
 	int			i = 0, index = 0;
 	int			bbits = inode->i_blkbits;
 
@@ -671,12 +673,12 @@ xfs_convert_page(
 STATIC void
 xfs_cluster_write(
 	struct inode		*inode,
-	unsigned long		tindex,
+	pgoff_t			tindex,
 	xfs_iomap_t		*iomapp,
 	int			startio,
 	int			all_bh)
 {
-	unsigned long		tlast;
+	pgoff_t			tlast;
 	struct page		*page;
 
 	tlast = (iomapp->iomap_offset + iomapp->iomap_bsize) >> PAGE_CACHE_SHIFT;
@@ -716,7 +718,8 @@ xfs_page_state_convert(
 {
 	struct buffer_head	*bh_arr[MAX_BUF_PER_PAGE], *bh, *head;
 	xfs_iomap_t		*iomp, iomap;
-	unsigned long		p_offset = 0, end_index;
+	unsigned long		p_offset = 0;
+	pgoff_t			end_index;
 	loff_t			offset;
 	unsigned long long	end_offset;
 	int			len, err, i, cnt = 0, uptodate = 1;
diff -uNrp linux-2.6.5/fs/xfs/linux/xfs_buf.c linux-2.6.5.xfs/fs/xfs/linux/xfs_buf.c
--- linux-2.6.5/fs/xfs/linux/xfs_buf.c	2004-04-04 05:36:15.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/linux/xfs_buf.c	2004-04-05 00:58:23.000000000 +0200
@@ -31,14 +31,10 @@
  */
 
 /*
- *	page_buf.c
- *
- *	The page_buf module provides an abstract buffer cache model on top of
- *	the Linux page cache.  Cached metadata blocks for a file system are
- *	hashed to the inode for the block device.  The page_buf module
- *	assembles buffer (page_buf_t) objects on demand to aggregate such
- *	cached pages for I/O.
- *
+ *	The xfs_buf.c code provides an abstract buffer cache model on top
+ *	of the Linux page cache.  Cached metadata blocks for a file system
+ *	are hashed to the inode for the block device.  xfs_buf.c assembles
+ *	buffers (xfs_buf_t) on demand to aggregate such cached pages for I/O.
  *
  *      Written by Steve Lord, Jim Mostek, Russell Cattelan
  *		    and Rajagopal Ananthanarayanan ("ananth") at SGI.
@@ -71,7 +67,7 @@
 
 STATIC kmem_cache_t *pagebuf_cache;
 STATIC void pagebuf_daemon_wakeup(void);
-STATIC void pagebuf_delwri_queue(page_buf_t *, int);
+STATIC void pagebuf_delwri_queue(xfs_buf_t *, int);
 STATIC struct workqueue_struct *pagebuf_logio_workqueue;
 STATIC struct workqueue_struct *pagebuf_dataio_workqueue;
 
@@ -82,7 +78,7 @@ STATIC struct workqueue_struct *pagebuf_
 #ifdef PAGEBUF_TRACE
 void
 pagebuf_trace(
-	page_buf_t	*pb,
+	xfs_buf_t	*pb,
 	char		*id,
 	void		*data,
 	void		*ra)
@@ -169,7 +165,7 @@ _bhash(
  * Mapping of multi-page buffers into contiguous virtual space
  */
 
-STATIC void *pagebuf_mapout_locked(page_buf_t *);
+STATIC void *pagebuf_mapout_locked(xfs_buf_t *);
 
 typedef struct a_list {
 	void		*vm_addr;
@@ -229,8 +225,8 @@ purge_addresses(void)
 
 STATIC void
 _pagebuf_initialize(
-	page_buf_t		*pb,
-	pb_target_t		*target,
+	xfs_buf_t		*pb,
+	xfs_buftarg_t		*target,
 	loff_t			range_base,
 	size_t			range_length,
 	page_buf_flags_t	flags)
@@ -240,7 +236,7 @@ _pagebuf_initialize(
 	 */
 	flags &= ~(PBF_LOCK|PBF_MAPPED|PBF_DONT_BLOCK|PBF_READ_AHEAD);
 
-	memset(pb, 0, sizeof(page_buf_t));
+	memset(pb, 0, sizeof(xfs_buf_t));
 	atomic_set(&pb->pb_hold, 1);
 	init_MUTEX_LOCKED(&pb->pb_iodonesema);
 	INIT_LIST_HEAD(&pb->pb_list);
@@ -251,12 +247,12 @@ _pagebuf_initialize(
 	pb->pb_file_offset = range_base;
 	/*
 	 * Set buffer_length and count_desired to the same value initially.
-	 * IO routines should use count_desired, which will be the same in
+	 * I/O routines should use count_desired, which will be the same in
 	 * most cases but may be reset (e.g. XFS recovery).
 	 */
 	pb->pb_buffer_length = pb->pb_count_desired = range_length;
 	pb->pb_flags = flags | PBF_NONE;
-	pb->pb_bn = PAGE_BUF_DADDR_NULL;
+	pb->pb_bn = XFS_BUF_DADDR_NULL;
 	atomic_set(&pb->pb_pin_count, 0);
 	init_waitqueue_head(&pb->pb_waiters);
 
@@ -270,7 +266,7 @@ _pagebuf_initialize(
  */
 STATIC int
 _pagebuf_get_pages(
-	page_buf_t		*pb,
+	xfs_buf_t		*pb,
 	int			page_count,
 	page_buf_flags_t	flags)
 {
@@ -296,7 +292,7 @@ _pagebuf_get_pages(
  */
 STATIC inline void
 _pagebuf_freepages(
-	page_buf_t		*pb)
+	xfs_buf_t		*pb)
 {
 	int			buf_index;
 
@@ -318,7 +314,7 @@ _pagebuf_freepages(
  */
 void
 pagebuf_free(
-	page_buf_t		*pb)
+	xfs_buf_t		*pb)
 {
 	PB_TRACE(pb, "free", 0);
 	
@@ -334,10 +330,14 @@ pagebuf_free(
 
 	if (pb->pb_flags & _PBF_MEM_ALLOCATED) {
 		if (pb->pb_pages) {
-			/* release the pages in the address list */
-			if ((pb->pb_pages[0]) &&
-			    (pb->pb_flags & _PBF_MEM_SLAB)) {
-				kfree(pb->pb_addr);
+			if (pb->pb_flags & _PBF_MEM_SLAB) {
+				 /*
+				  * XXX: bp->pb_count_desired might be incorrect
+				  * (see pagebuf_associate_memory for details),
+				  * but fortunately the Linux version of
+				  * kmem_free ignores the len argument..
+				  */
+				kmem_free(pb->pb_addr, pb->pb_count_desired);
 			} else {
 				_pagebuf_freepages(pb);
 			}
@@ -352,104 +352,74 @@ pagebuf_free(
 }
 
 /*
- *	_pagebuf_lookup_pages
- *
- *	_pagebuf_lookup_pages finds all pages which match the buffer
- *	in question and the range of file offsets supplied,
- *	and builds the page list for the buffer, if the
- *	page list is not already formed or if not all of the pages are
- *	already in the list. Invalid pages (pages which have not yet been
- *	read in from disk) are assigned for any pages which are not found.
+ *	Finds all pages for buffer in question and builds it's page list.
  */
 STATIC int
 _pagebuf_lookup_pages(
-	page_buf_t		*pb,
-	struct address_space	*aspace,
-	page_buf_flags_t	flags)
+	xfs_buf_t		*bp,
+	uint			flags)
 {
-	loff_t			next_buffer_offset;
-	unsigned long		page_count, pi, index;
-	struct page		*page;
+	struct address_space	*mapping = bp->pb_target->pbr_mapping;
+	unsigned int		sectorshift = bp->pb_target->pbr_sshift;
+	size_t			blocksize = bp->pb_target->pbr_bsize;
+	size_t			size = bp->pb_count_desired;
+	size_t			nbytes, offset;
 	int			gfp_mask = pb_to_gfp(flags);
-	int			all_mapped, good_pages, nbytes, rval, retries;
-	unsigned int		blocksize, sectorshift;
-	size_t			size, offset;
-
-	next_buffer_offset = pb->pb_file_offset + pb->pb_buffer_length;
-	good_pages = page_count = (page_buf_btoc(next_buffer_offset) -
-				   page_buf_btoct(pb->pb_file_offset));
-
-	if (pb->pb_flags & _PBF_ALL_PAGES_MAPPED) {
-		/* Bring pages forward in cache */
-		for (pi = 0; pi < page_count; pi++) {
-			mark_page_accessed(pb->pb_pages[pi]);
-		}
-		if ((flags & PBF_MAPPED) && !(pb->pb_flags & PBF_MAPPED)) {
-			all_mapped = 1;
-			rval = 0;
-			goto mapit;
-		}
-		return 0;
-	}
+	unsigned short		page_count, i;
+	pgoff_t			first;
+	loff_t			end;
+	int			error;
 
-	/* Ensure pb_pages field has been initialised */
-	rval = _pagebuf_get_pages(pb, page_count, flags);
-	if (rval)
-		return rval;
+	end = bp->pb_file_offset + bp->pb_buffer_length;
+	page_count = page_buf_btoc(end) - page_buf_btoct(bp->pb_file_offset);
 
-	all_mapped = 1;
-	blocksize = pb->pb_target->pbr_bsize;
-	sectorshift = pb->pb_target->pbr_sshift;
-	size = pb->pb_count_desired;
-	offset = pb->pb_offset;
-
-	/* Enter the pages in the page list */
-	index = (pb->pb_file_offset - pb->pb_offset) >> PAGE_CACHE_SHIFT;
-	for (pi = 0; pi < page_count; pi++, index++) {
-		if (pb->pb_pages[pi] == 0) {
-			retries = 0;
-		      retry:
-			page = find_or_create_page(aspace, index, gfp_mask);
-			if (!page) {
-				if (flags & PBF_READ_AHEAD)
-					return -ENOMEM;
-				/*
-				 * This could deadlock.  But until all the
-				 * XFS lowlevel code is revamped to handle
-				 * buffer allocation failures we can't do
-				 * much.
-				 */
-				if (!(++retries % 100)) {
-					printk(KERN_ERR
-					       "possibly deadlocking in %s\n",
-					       __FUNCTION__);
-				}
-				XFS_STATS_INC(pb_page_retries);
-				pagebuf_daemon_wakeup();
-				current->state = TASK_UNINTERRUPTIBLE;
-				schedule_timeout(10);
-				goto retry;
+	error = _pagebuf_get_pages(bp, page_count, flags);
+	if (unlikely(error))
+		return error;
+
+	offset = bp->pb_offset;
+	first = bp->pb_file_offset >> PAGE_CACHE_SHIFT;
+
+	for (i = 0; i < bp->pb_page_count; i++) {
+		struct page	*page;
+		uint		retries = 0;
+
+	      retry:
+		page = find_or_create_page(mapping, first + i, gfp_mask);
+		if (unlikely(page == NULL)) {
+			if (flags & PBF_READ_AHEAD)
+				return -ENOMEM;
+
+			/*
+			 * This could deadlock.
+			 *
+			 * But until all the XFS lowlevel code is revamped to
+			 * handle buffer allocation failures we can't do much.
+			 */
+			if (!(++retries % 100)) {
+				printk(KERN_ERR "possibly deadlocking in %s\n",
+						__FUNCTION__);
 			}
-			XFS_STATS_INC(pb_page_found);
-			mark_page_accessed(page);
-			pb->pb_pages[pi] = page;
-		} else {
-			page = pb->pb_pages[pi];
-			lock_page(page);
+
+			XFS_STATS_INC(pb_page_retries);
+			pagebuf_daemon_wakeup();
+			current->state = TASK_UNINTERRUPTIBLE;
+			schedule_timeout(10);
+			goto retry;
 		}
 
-		nbytes = PAGE_CACHE_SIZE - offset;
-		if (nbytes > size)
-			nbytes = size;
+		XFS_STATS_INC(pb_page_found);
+
+		nbytes = min_t(size_t, size, PAGE_CACHE_SIZE - offset);
 		size -= nbytes;
 
 		if (!PageUptodate(page)) {
+			page_count--;
 			if (blocksize == PAGE_CACHE_SIZE) {
 				if (flags & PBF_READ)
-					pb->pb_locked = 1;
-				good_pages--;
+					bp->pb_locked = 1;
 			} else if (!PagePrivate(page)) {
-				unsigned long	i, range;
+				unsigned long	j, range;
 
 				/*
 				 * In this case page->private holds a bitmap
@@ -457,60 +427,62 @@ _pagebuf_lookup_pages(
 				 */
 				ASSERT(blocksize < PAGE_CACHE_SIZE);
 				range = (offset + nbytes) >> sectorshift;
-				for (i = offset >> sectorshift; i < range; i++)
-					if (!test_bit(i, &page->private))
+				for (j = offset >> sectorshift; j < range; j++)
+					if (!test_bit(j, &page->private))
 						break;
-				if (i != range)
-					good_pages--;
-			} else {
-				good_pages--;
+				if (j == range)
+					page_count++;
 			}
 		}
+
+		bp->pb_pages[i] = page;
 		offset = 0;
 	}
 
-	if (!pb->pb_locked) {
-		for (pi = 0; pi < page_count; pi++) {
-			if (pb->pb_pages[pi])
-				unlock_page(pb->pb_pages[pi]);
-		}
+	if (!bp->pb_locked) {
+		for (i = 0; i < bp->pb_page_count; i++)
+			unlock_page(bp->pb_pages[i]);
 	}
 
-	pb->pb_flags |= _PBF_PAGECACHE;
-mapit:
-	pb->pb_flags |= _PBF_MEM_ALLOCATED;
-	if (all_mapped) {
-		pb->pb_flags |= _PBF_ALL_PAGES_MAPPED;
-
-		/* A single page buffer is always mappable */
-		if (page_count == 1) {
-			pb->pb_addr = (caddr_t)
-				page_address(pb->pb_pages[0]) + pb->pb_offset;
-			pb->pb_flags |= PBF_MAPPED;
-		} else if (flags & PBF_MAPPED) {
-			if (as_list_len > 64)
-				purge_addresses();
-			pb->pb_addr = vmap(pb->pb_pages, page_count,
-					VM_MAP, PAGE_KERNEL);
-			if (pb->pb_addr == NULL)
-				return -ENOMEM;
-			pb->pb_addr += pb->pb_offset;
-			pb->pb_flags |= PBF_MAPPED | _PBF_ADDR_ALLOCATED;
-		}
-	}
-	/* If some pages were found with data in them
-	 * we are not in PBF_NONE state.
-	 */
-	if (good_pages != 0) {
-		pb->pb_flags &= ~(PBF_NONE);
-		if (good_pages != page_count) {
-			pb->pb_flags |= PBF_PARTIAL;
-		}
+	bp->pb_flags |= (_PBF_PAGECACHE|_PBF_MEM_ALLOCATED);
+
+	if (page_count) {
+		/* if we have any uptodate pages, mark that in the buffer */
+		bp->pb_flags &= ~PBF_NONE;
+
+		/* if some pages aren't uptodate, mark that in the buffer */
+		if (page_count != bp->pb_page_count)
+			bp->pb_flags |= PBF_PARTIAL;
 	}
 
-	PB_TRACE(pb, "lookup_pages", (long)good_pages);
+	PB_TRACE(bp, "lookup_pages", (long)page_count);
+	return error;
+}
 
-	return rval;
+/*
+ *	Map buffer into kernel address-space if nessecary.
+ */
+STATIC int
+_pagebuf_map_pages(
+	xfs_buf_t		*bp,
+	uint			flags)
+{
+	/* A single page buffer is always mappable */
+	if (bp->pb_page_count == 1) {
+		bp->pb_addr = page_address(bp->pb_pages[0]) + bp->pb_offset;
+		bp->pb_flags |= PBF_MAPPED;
+	} else if (flags & PBF_MAPPED) {
+		if (as_list_len > 64)
+			purge_addresses();
+		bp->pb_addr = vmap(bp->pb_pages, bp->pb_page_count,
+				VM_MAP, PAGE_KERNEL);
+		if (unlikely(bp->pb_addr == NULL))
+			return -ENOMEM;
+		bp->pb_addr += bp->pb_offset;
+		bp->pb_flags |= PBF_MAPPED | _PBF_ADDR_ALLOCATED;
+	}
+
+	return 0;
 }
 
 /*
@@ -527,20 +499,19 @@ mapit:
  *	which may imply that this call will block until those buffers
  *	are unlocked.  No I/O is implied by this call.
  */
-STATIC page_buf_t *
+STATIC xfs_buf_t *
 _pagebuf_find(				/* find buffer for block	*/
-	pb_target_t		*target,/* target for block		*/
+	xfs_buftarg_t		*target,/* target for block		*/
 	loff_t			ioff,	/* starting offset of range	*/
 	size_t			isize,	/* length of range		*/
 	page_buf_flags_t	flags,	/* PBF_TRYLOCK			*/
-	page_buf_t		*new_pb)/* newly allocated buffer	*/
+	xfs_buf_t		*new_pb)/* newly allocated buffer	*/
 {
 	loff_t			range_base;
 	size_t			range_length;
 	int			hval;
 	pb_hash_t		*h;
-	struct list_head	*p;
-	page_buf_t		*pb;
+	xfs_buf_t		*pb, *n;
 	int			not_locked;
 
 	range_base = (ioff << BBSHIFT);
@@ -556,9 +527,7 @@ _pagebuf_find(				/* find buffer for blo
 	h = &pbhash[hval];
 
 	spin_lock(&h->pb_hash_lock);
-	list_for_each(p, &h->pb_hash) {
-		pb = list_entry(p, page_buf_t, pb_hash_list);
-
+	list_for_each_entry_safe(pb, n, &h->pb_hash, pb_hash_list) {
 		if (pb->pb_target == target &&
 		    pb->pb_file_offset == range_base &&
 		    pb->pb_buffer_length == range_length) {
@@ -617,7 +586,6 @@ found:
 
 	if (pb->pb_flags & PBF_STALE)
 		pb->pb_flags &= PBF_MAPPED | \
-				_PBF_ALL_PAGES_MAPPED | \
 				_PBF_ADDR_ALLOCATED | \
 				_PBF_MEM_ALLOCATED | \
 				_PBF_MEM_SLAB;
@@ -637,10 +605,10 @@ found:
  *	pages are present in the buffer, not all of every page may be
  *	valid.
  */
-page_buf_t *
+xfs_buf_t *
 pagebuf_find(				/* find buffer for block	*/
 					/* if the block is in memory	*/
-	pb_target_t		*target,/* target for block		*/
+	xfs_buftarg_t		*target,/* target for block		*/
 	loff_t			ioff,	/* starting offset of range	*/
 	size_t			isize,	/* length of range		*/
 	page_buf_flags_t	flags)	/* PBF_TRYLOCK			*/
@@ -657,37 +625,48 @@ pagebuf_find(				/* find buffer for bloc
  *	although backing storage may not be.  If PBF_READ is set in
  *	flags, pagebuf_iostart is called also.
  */
-page_buf_t *
+xfs_buf_t *
 pagebuf_get(				/* allocate a buffer		*/
-	pb_target_t		*target,/* target for buffer		*/
+	xfs_buftarg_t		*target,/* target for buffer		*/
 	loff_t			ioff,	/* starting offset of range	*/
 	size_t			isize,	/* length of range		*/
 	page_buf_flags_t	flags)	/* PBF_TRYLOCK			*/
 {
-	page_buf_t		*pb, *new_pb;
-	int			error;
+	xfs_buf_t		*pb, *new_pb;
+	int			error = 0, i;
 
 	new_pb = pagebuf_allocate(flags);
 	if (unlikely(!new_pb))
-		return (NULL);
+		return NULL;
 
 	pb = _pagebuf_find(target, ioff, isize, flags, new_pb);
-	if (pb != new_pb) {
+	if (pb == new_pb) {
+		error = _pagebuf_lookup_pages(pb, flags);
+		if (unlikely(error)) {
+			printk(KERN_WARNING
+			       "pagebuf_get: failed to lookup pages\n");
+			goto no_buffer;
+		}
+	} else {
 		pagebuf_deallocate(new_pb);
-		if (unlikely(!pb))
-			return (NULL);
+		if (unlikely(pb == NULL))
+			return NULL;
 	}
 
-	XFS_STATS_INC(pb_get);
+	for (i = 0; i < pb->pb_page_count; i++)
+		mark_page_accessed(pb->pb_pages[i]);
 
-	/* fill in any missing pages */
-	error = _pagebuf_lookup_pages(pb, pb->pb_target->pbr_mapping, flags);
-	if (unlikely(error)) {
-		printk(KERN_WARNING
-			"pagebuf_get: warning, failed to lookup pages\n");
-		goto no_buffer;
+	if (!(pb->pb_flags & PBF_MAPPED)) {
+		error = _pagebuf_map_pages(pb, flags);
+		if (unlikely(error)) {
+			printk(KERN_WARNING
+			       "pagebuf_get: failed to map pages\n");
+			goto no_buffer;
+		}
 	}
 
+	XFS_STATS_INC(pb_get);
+
 	/*
 	 * Always fill in the block number now, the mapped cases can do
 	 * their own overlay of this later.
@@ -728,14 +707,14 @@ no_buffer:
 /*
  * Create a skeletal pagebuf (no pages associated with it).
  */
-page_buf_t *
+xfs_buf_t *
 pagebuf_lookup(
-	struct pb_target	*target,
+	xfs_buftarg_t		*target,
 	loff_t			ioff,
 	size_t			isize,
 	page_buf_flags_t	flags)
 {
-	page_buf_t		*pb;
+	xfs_buf_t		*pb;
 
 	pb = pagebuf_allocate(flags);
 	if (pb) {
@@ -750,7 +729,7 @@ pagebuf_lookup(
  */
 void
 pagebuf_readahead(
-	pb_target_t		*target,
+	xfs_buftarg_t		*target,
 	loff_t			ioff,
 	size_t			isize,
 	page_buf_flags_t	flags)
@@ -767,12 +746,12 @@ pagebuf_readahead(
 	pagebuf_get(target, ioff, isize, flags);
 }
 
-page_buf_t *
+xfs_buf_t *
 pagebuf_get_empty(
 	size_t			len,
-	pb_target_t		*target)
+	xfs_buftarg_t		*target)
 {
-	page_buf_t		*pb;
+	xfs_buf_t		*pb;
 
 	pb = pagebuf_allocate(0);
 	if (pb)
@@ -794,7 +773,7 @@ mem_to_page(
 
 int
 pagebuf_associate_memory(
-	page_buf_t		*pb,
+	xfs_buf_t		*pb,
 	void			*mem,
 	size_t			len)
 {
@@ -843,55 +822,55 @@ pagebuf_associate_memory(
 	return 0;
 }
 
-page_buf_t *
+xfs_buf_t *
 pagebuf_get_no_daddr(
 	size_t			len,
-	pb_target_t		*target)
+	xfs_buftarg_t		*target)
 {
-	int			rval;
-	void			*rmem = NULL;
-	page_buf_flags_t	flags = PBF_FORCEIO;
-	page_buf_t		*pb;
-	size_t			tlen = 0;
+	size_t			malloc_len = len;
+	xfs_buf_t		*bp;
+	void			*data;
+	int			error;
 
 	if (unlikely(len > 0x20000))
-		return NULL;
-
-	pb = pagebuf_allocate(flags);
-	if (!pb)
-		return NULL;
-
-	_pagebuf_initialize(pb, target, 0, len, flags);
-
-	do {
-		if (tlen == 0) {
-			tlen = len; /* first time */
-		} else {
-			kfree(rmem); /* free the mem from the previous try */
-			tlen <<= 1; /* double the size and try again */
-		}
-		if ((rmem = kmalloc(tlen, GFP_KERNEL)) == 0) {
-			pagebuf_free(pb);
-			return NULL;
-		}
-	} while ((size_t)rmem != ((size_t)rmem & ~target->pbr_smask));
+		goto fail;
 
-	if ((rval = pagebuf_associate_memory(pb, rmem, len)) != 0) {
-		kfree(rmem);
-		pagebuf_free(pb);
-		return NULL;
-	}
-	/* otherwise pagebuf_free just ignores it */
-	pb->pb_flags |= (_PBF_MEM_ALLOCATED | _PBF_MEM_SLAB);
-	PB_CLEAR_OWNER(pb);
-	up(&pb->pb_sema);	/* Return unlocked pagebuf */
-
-	PB_TRACE(pb, "no_daddr", rmem);
-
-	return pb;
+	bp = pagebuf_allocate(0);
+	if (unlikely(bp == NULL))
+		goto fail;
+	_pagebuf_initialize(bp, target, 0, len, PBF_FORCEIO);
+
+ try_again:
+	data = kmem_alloc(malloc_len, KM_SLEEP);
+	if (unlikely(data == NULL))
+		goto fail_free_buf;
+
+	/* check whether alignment matches.. */
+	if ((__psunsigned_t)data !=
+	    ((__psunsigned_t)data & ~target->pbr_smask)) {
+		/* .. else double the size and try again */
+		kmem_free(data, malloc_len);
+		malloc_len <<= 1;
+		goto try_again;
+	}
+
+	error = pagebuf_associate_memory(bp, data, len);
+	if (error)
+		goto fail_free_mem;
+	bp->pb_flags |= (_PBF_MEM_ALLOCATED | _PBF_MEM_SLAB);
+
+	pagebuf_unlock(bp);
+
+	PB_TRACE(bp, "no_daddr", data);
+	return bp;
+ fail_free_mem:
+	kmem_free(data, malloc_len);
+ fail_free_buf:
+	pagebuf_free(bp);
+ fail:
+	return NULL;
 }
 
-
 /*
  *	pagebuf_hold
  *
@@ -902,7 +881,7 @@ pagebuf_get_no_daddr(
  */
 void
 pagebuf_hold(
-	page_buf_t		*pb)
+	xfs_buf_t		*pb)
 {
 	atomic_inc(&pb->pb_hold);
 	PB_TRACE(pb, "hold", 0);
@@ -916,7 +895,7 @@ pagebuf_hold(
  */
 void
 pagebuf_rele(
-	page_buf_t		*pb)
+	xfs_buf_t		*pb)
 {
 	pb_hash_t		*hash = pb_hash(pb);
 
@@ -975,7 +954,7 @@ pagebuf_rele(
 int
 pagebuf_cond_lock(			/* lock buffer, if not locked	*/
 					/* returns -EBUSY if locked)	*/
-	page_buf_t		*pb)
+	xfs_buf_t		*pb)
 {
 	int			locked;
 
@@ -994,7 +973,7 @@ pagebuf_cond_lock(			/* lock buffer, if 
  */
 int
 pagebuf_lock_value(
-	page_buf_t		*pb)
+	xfs_buf_t		*pb)
 {
 	return(atomic_read(&pb->pb_sema.count));
 }
@@ -1009,7 +988,7 @@ pagebuf_lock_value(
  */
 int
 pagebuf_lock(
-	page_buf_t		*pb)
+	xfs_buf_t		*pb)
 {
 	PB_TRACE(pb, "lock", 0);
 	if (atomic_read(&pb->pb_io_remaining))
@@ -1029,7 +1008,7 @@ pagebuf_lock(
  */
 void
 pagebuf_unlock(				/* unlock buffer		*/
-	page_buf_t		*pb)	/* buffer to unlock		*/
+	xfs_buf_t		*pb)	/* buffer to unlock		*/
 {
 	PB_CLEAR_OWNER(pb);
 	up(&pb->pb_sema);
@@ -1057,7 +1036,7 @@ pagebuf_unlock(				/* unlock buffer		*/
  */
 void
 pagebuf_pin(
-	page_buf_t		*pb)
+	xfs_buf_t		*pb)
 {
 	atomic_inc(&pb->pb_pin_count);
 	PB_TRACE(pb, "pin", (long)pb->pb_pin_count.counter);
@@ -1072,7 +1051,7 @@ pagebuf_pin(
  */
 void
 pagebuf_unpin(
-	page_buf_t		*pb)
+	xfs_buf_t		*pb)
 {
 	if (atomic_dec_and_test(&pb->pb_pin_count)) {
 		wake_up_all(&pb->pb_waiters);
@@ -1082,7 +1061,7 @@ pagebuf_unpin(
 
 int
 pagebuf_ispin(
-	page_buf_t		*pb)
+	xfs_buf_t		*pb)
 {
 	return atomic_read(&pb->pb_pin_count);
 }
@@ -1096,7 +1075,7 @@ pagebuf_ispin(
  */
 static inline void
 _pagebuf_wait_unpin(
-	page_buf_t		*pb)
+	xfs_buf_t		*pb)
 {
 	DECLARE_WAITQUEUE	(wait, current);
 
@@ -1131,23 +1110,17 @@ void
 pagebuf_iodone_work(
 	void			*v)
 {
-	page_buf_t		*pb = (page_buf_t *)v;
-
-	if (pb->pb_iodone) {
-		(*(pb->pb_iodone)) (pb);
-		return;
-	}
+	xfs_buf_t		*bp = (xfs_buf_t *)v;
 
-	if (pb->pb_flags & PBF_ASYNC) {
-		if (!pb->pb_relse)
-			pagebuf_unlock(pb);
-		pagebuf_rele(pb);
-	}
+	if (bp->pb_iodone)
+		(*(bp->pb_iodone))(bp);
+	else if (bp->pb_flags & PBF_ASYNC)
+		xfs_buf_relse(bp);
 }
 
 void
 pagebuf_iodone(
-	page_buf_t		*pb,
+	xfs_buf_t		*pb,
 	int			dataio,
 	int			schedule)
 {
@@ -1178,7 +1151,7 @@ pagebuf_iodone(
  */
 void
 pagebuf_ioerror(			/* mark/clear buffer error flag */
-	page_buf_t		*pb,	/* buffer to mark		*/
+	xfs_buf_t		*pb,	/* buffer to mark		*/
 	unsigned int		error)	/* error to store (0 if none)	*/
 {
 	pb->pb_error = error;
@@ -1199,7 +1172,7 @@ pagebuf_ioerror(			/* mark/clear buffer 
  */
 int
 pagebuf_iostart(			/* start I/O on a buffer	  */
-	page_buf_t		*pb,	/* buffer to start		  */
+	xfs_buf_t		*pb,	/* buffer to start		  */
 	page_buf_flags_t	flags)	/* PBF_LOCK, PBF_ASYNC, PBF_READ, */
 					/* PBF_WRITE, PBF_DELWRI,	  */
 					/* PBF_DONT_BLOCK		  */
@@ -1220,7 +1193,7 @@ pagebuf_iostart(			/* start I/O on a buf
 	pb->pb_flags |= flags & (PBF_READ | PBF_WRITE | PBF_ASYNC | \
 			PBF_READ_AHEAD | PBF_RUN_QUEUES);
 
-	BUG_ON(pb->pb_bn == PAGE_BUF_DADDR_NULL);
+	BUG_ON(pb->pb_bn == XFS_BUF_DADDR_NULL);
 
 	/* For writes allow an alternate strategy routine to precede
 	 * the actual I/O request (which may not be issued at all in
@@ -1246,7 +1219,7 @@ pagebuf_iostart(			/* start I/O on a buf
 
 STATIC __inline__ int
 _pagebuf_iolocked(
-	page_buf_t		*pb)
+	xfs_buf_t		*pb)
 {
 	ASSERT(pb->pb_flags & (PBF_READ|PBF_WRITE));
 	if (pb->pb_flags & PBF_READ)
@@ -1256,7 +1229,7 @@ _pagebuf_iolocked(
 
 STATIC __inline__ void
 _pagebuf_iodone(
-	page_buf_t		*pb,
+	xfs_buf_t		*pb,
 	int			schedule)
 {
 	if (atomic_dec_and_test(&pb->pb_io_remaining) == 1) {
@@ -1271,7 +1244,7 @@ bio_end_io_pagebuf(
 	unsigned int		bytes_done,
 	int			error)
 {
-	page_buf_t		*pb = (page_buf_t *)bio->bi_private;
+	xfs_buf_t		*pb = (xfs_buf_t *)bio->bi_private;
 	unsigned int		i, blocksize = pb->pb_target->pbr_bsize;
 	unsigned int		sectorshift = pb->pb_target->pbr_sshift;
 	struct bio_vec		*bvec = bio->bi_io_vec;
@@ -1290,7 +1263,7 @@ bio_end_io_pagebuf(
 		} else if (blocksize == PAGE_CACHE_SIZE) {
 			SetPageUptodate(page);
 		} else if (!PagePrivate(page)) {
-			unsigned int	j, range;
+			unsigned long	j, range;
 
 			ASSERT(blocksize < PAGE_CACHE_SIZE);
 			range = (bvec->bv_offset + bvec->bv_len) >> sectorshift;
@@ -1312,7 +1285,7 @@ bio_end_io_pagebuf(
 
 void
 _pagebuf_ioapply(
-	page_buf_t		*pb)
+	xfs_buf_t		*pb)
 {
 	int			i, map_i, total_nr_pages, nr_pages;
 	struct bio		*bio;
@@ -1412,26 +1385,11 @@ submit_io:
 }
 
 /*
- *	pagebuf_iorequest
- *
- *	pagebuf_iorequest is the core I/O request routine.
- *	It assumes that the buffer is well-formed and
- *	mapped and ready for physical I/O, unlike
- *	pagebuf_iostart() and pagebuf_iophysio().  Those
- *	routines call the pagebuf_ioinitiate routine to start I/O,
- *	if it is present, or else call pagebuf_iorequest()
- *	directly if the pagebuf_ioinitiate routine is not present.
- *
- *	This function will be responsible for ensuring access to the
- *	pages is restricted whilst I/O is in progress - for locking
- *	pagebufs the pagebuf lock is the mediator, for non-locking
- *	pagebufs the pages will be locked. In the locking case we
- *	need to use the pagebuf lock as multiple meta-data buffers
- *	will reference the same page.
+ *	pagebuf_iorequest -- the core I/O request routine.
  */
 int
 pagebuf_iorequest(			/* start real I/O		*/
-	page_buf_t		*pb)	/* buffer to convey to device	*/
+	xfs_buf_t		*pb)	/* buffer to convey to device	*/
 {
 	PB_TRACE(pb, "iorequest", 0);
 
@@ -1467,7 +1425,7 @@ pagebuf_iorequest(			/* start real I/O		
  */
 int
 pagebuf_iowait(
-	page_buf_t		*pb)
+	xfs_buf_t		*pb)
 {
 	PB_TRACE(pb, "iowait", 0);
 	if (atomic_read(&pb->pb_io_remaining))
@@ -1479,7 +1437,7 @@ pagebuf_iowait(
 
 STATIC void *
 pagebuf_mapout_locked(
-	page_buf_t		*pb)
+	xfs_buf_t		*pb)
 {
 	void			*old_addr = NULL;
 
@@ -1498,7 +1456,7 @@ pagebuf_mapout_locked(
 
 caddr_t
 pagebuf_offset(
-	page_buf_t		*pb,
+	xfs_buf_t		*pb,
 	size_t			offset)
 {
 	struct page		*page;
@@ -1516,7 +1474,7 @@ pagebuf_offset(
  */
 void
 pagebuf_iomove(
-	page_buf_t		*pb,	/* buffer to process		*/
+	xfs_buf_t		*pb,	/* buffer to process		*/
 	size_t			boff,	/* starting buffer offset	*/
 	size_t			bsize,	/* length to copy		*/
 	caddr_t			data,	/* data address			*/
@@ -1560,10 +1518,12 @@ STATIC spinlock_t pbd_delwrite_lock = SP
 
 STATIC void
 pagebuf_delwri_queue(
-	page_buf_t		*pb,
+	xfs_buf_t		*pb,
 	int			unlock)
 {
 	PB_TRACE(pb, "delwri_q", (long)unlock);
+	ASSERT(pb->pb_flags & PBF_DELWRI);
+
 	spin_lock(&pbd_delwrite_lock);
 	/* If already in the queue, dequeue and place at tail */
 	if (!list_empty(&pb->pb_list)) {
@@ -1583,7 +1543,7 @@ pagebuf_delwri_queue(
 
 void
 pagebuf_delwri_dequeue(
-	page_buf_t		*pb)
+	xfs_buf_t		*pb)
 {
 	PB_TRACE(pb, "delwri_uq", 0);
 	spin_lock(&pbd_delwrite_lock);
@@ -1617,9 +1577,9 @@ STATIC int
 pagebuf_daemon(
 	void			*data)
 {
+	struct list_head	tmp;
+	xfs_buf_t		*pb, *n;
 	int			count;
-	page_buf_t		*pb;
-	struct list_head	*curr, *next, tmp;
 
 	/*  Set up the thread  */
 	daemonize("xfsbufd");
@@ -1638,16 +1598,13 @@ pagebuf_daemon(
 		set_current_state(TASK_INTERRUPTIBLE);
 		schedule_timeout(xfs_flush_interval);
 
-		spin_lock(&pbd_delwrite_lock);
-
 		count = 0;
-		list_for_each_safe(curr, next, &pbd_delwrite_queue) {
-			pb = list_entry(curr, page_buf_t, pb_list);
-
+		spin_lock(&pbd_delwrite_lock);
+		list_for_each_entry_safe(pb, n, &pbd_delwrite_queue, pb_list) {
 			PB_TRACE(pb, "walkq1", (long)pagebuf_ispin(pb));
+			ASSERT(pb->pb_flags & PBF_DELWRI);
 
-			if ((pb->pb_flags & PBF_DELWRI) &&
-			     !pagebuf_ispin(pb) && !pagebuf_cond_lock(pb)) {
+			if (!pagebuf_ispin(pb) && !pagebuf_cond_lock(pb)) {
 				if (!force_flush &&
 				    time_before(jiffies, pb->pb_flushtime)) {
 					pagebuf_unlock(pb);
@@ -1660,12 +1617,11 @@ pagebuf_daemon(
 				count++;
 			}
 		}
-
 		spin_unlock(&pbd_delwrite_lock);
+
 		while (!list_empty(&tmp)) {
-			pb = list_entry(tmp.next, page_buf_t, pb_list);
+			pb = list_entry(tmp.next, xfs_buf_t, pb_list);
 			list_del_init(&pb->pb_list);
-
 			pagebuf_iostrategy(pb);
 		}
 
@@ -1682,33 +1638,26 @@ pagebuf_daemon(
 
 void
 pagebuf_delwri_flush(
-	pb_target_t		*target,
-	u_long			flags,
+	xfs_buftarg_t		*target,
+	int			wait,
 	int			*pinptr)
 {
-	page_buf_t		*pb;
-	struct list_head	*curr, *next, tmp;
+	struct list_head	tmp;
+	xfs_buf_t		*pb, *n;
 	int			pincount = 0;
 	int			flush_cnt = 0;
 
 	pagebuf_runall_queues(pagebuf_dataio_workqueue);
 	pagebuf_runall_queues(pagebuf_logio_workqueue);
 
-	spin_lock(&pbd_delwrite_lock);
 	INIT_LIST_HEAD(&tmp);
+	spin_lock(&pbd_delwrite_lock);
+	list_for_each_entry_safe(pb, n, &pbd_delwrite_queue, pb_list) {
 
-	list_for_each_safe(curr, next, &pbd_delwrite_queue) {
-		pb = list_entry(curr, page_buf_t, pb_list);
-
-		/*
-		 * Skip other targets, markers and in progress buffers
-		 */
-
-		if ((pb->pb_flags == 0) || (pb->pb_target != target) ||
-		    !(pb->pb_flags & PBF_DELWRI)) {
+		if (pb->pb_target != target)
 			continue;
-		}
 
+		ASSERT(pb->pb_flags & PBF_DELWRI);
 		PB_TRACE(pb, "walkq2", (long)pagebuf_ispin(pb));
 		if (pagebuf_ispin(pb)) {
 			pincount++;
@@ -1719,17 +1668,16 @@ pagebuf_delwri_flush(
 		pb->pb_flags |= PBF_WRITE;
 		list_move(&pb->pb_list, &tmp);
 	}
-	/* ok found all the items that can be worked on 
-	 * drop the lock and process the private list */
 	spin_unlock(&pbd_delwrite_lock);
 
-	list_for_each_safe(curr, next, &tmp) {
-		pb = list_entry(curr, page_buf_t, pb_list);
-
-		if (flags & PBDF_WAIT)
+	/*
+	 * Dropped the delayed write list lock, now walk the temporary list
+	 */
+	list_for_each_entry_safe(pb, n, &tmp, pb_list) {
+		if (wait)
 			pb->pb_flags &= ~PBF_ASYNC;
 		else
-			list_del_init(curr);
+			list_del_init(&pb->pb_list);
 
 		pagebuf_lock(pb);
 		pagebuf_iostrategy(pb);
@@ -1741,14 +1689,15 @@ pagebuf_delwri_flush(
 
 	blk_run_queues();
 
+	/*
+	 * Remaining list items must be flushed before returning
+	 */
 	while (!list_empty(&tmp)) {
-		pb = list_entry(tmp.next, page_buf_t, pb_list);
+		pb = list_entry(tmp.next, xfs_buf_t, pb_list);
 
 		list_del_init(&pb->pb_list);
-		pagebuf_iowait(pb);
-		if (!pb->pb_relse)
-			pagebuf_unlock(pb);
-		pagebuf_rele(pb);
+		xfs_iowait(pb);
+		xfs_buf_relse(pb);
 	}
 
 	if (pinptr)
@@ -1804,7 +1753,7 @@ pagebuf_init(void)
 {
 	int			i;
 
-	pagebuf_cache = kmem_cache_create("page_buf_t", sizeof(page_buf_t), 0,
+	pagebuf_cache = kmem_cache_create("xfs_buf_t", sizeof(xfs_buf_t), 0,
 			SLAB_HWCACHE_ALIGN, NULL, NULL);
 	if (pagebuf_cache == NULL) {
 		printk("pagebuf: couldn't init pagebuf cache\n");
diff -uNrp linux-2.6.5/fs/xfs/linux/xfs_buf.h linux-2.6.5.xfs/fs/xfs/linux/xfs_buf.h
--- linux-2.6.5/fs/xfs/linux/xfs_buf.h	2004-04-04 05:37:23.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/linux/xfs_buf.h	2004-04-05 00:58:23.000000000 +0200
@@ -51,10 +51,7 @@
  *	Base types
  */
 
-/* daddr must be signed since -1 is used for bmaps that are not yet allocated */
-typedef loff_t page_buf_daddr_t;
-
-#define PAGE_BUF_DADDR_NULL ((page_buf_daddr_t) (-1LL))
+#define XFS_BUF_DADDR_NULL ((xfs_daddr_t) (-1LL))
 
 #define page_buf_ctob(pp)	((pp) * PAGE_CACHE_SIZE)
 #define page_buf_btoc(dd)	(((dd) + PAGE_CACHE_SIZE - 1) >> PAGE_CACHE_SHIFT)
@@ -87,7 +84,6 @@ typedef enum page_buf_flags_e {		/* pb_f
 
 	/* flags used only internally */
 	_PBF_PAGECACHE = (1 << 16),	/* backed by pagecache		   */
-	_PBF_ALL_PAGES_MAPPED = (1 << 18), /* all pages in range mapped	   */
 	_PBF_ADDR_ALLOCATED = (1 << 19), /* pb_addr space was allocated	   */
 	_PBF_MEM_ALLOCATED = (1 << 20), /* underlying pages are allocated  */
 	_PBF_MEM_SLAB = (1 << 21), /* underlying pages are slab allocated  */
@@ -103,17 +99,17 @@ typedef enum page_buf_flags_e {		/* pb_f
 #define PBF_NOT_DONE(pb) (((pb)->pb_flags & (PBF_PARTIAL|PBF_NONE)) != 0)
 #define PBF_DONE(pb) (((pb)->pb_flags & (PBF_PARTIAL|PBF_NONE)) == 0)
 
-typedef struct pb_target {
+typedef struct xfs_buftarg {
 	dev_t			pbr_dev;
 	struct block_device	*pbr_bdev;
 	struct address_space	*pbr_mapping;
 	unsigned int		pbr_bsize;
 	unsigned int		pbr_sshift;
 	size_t			pbr_smask;
-} pb_target_t;
+} xfs_buftarg_t;
 
 /*
- *	page_buf_t:  Buffer structure for page cache-based buffers
+ *	xfs_buf_t:  Buffer structure for page cache-based buffers
  *
  * This buffer structure is used by the page cache buffer management routines
  * to refer to an assembly of pages forming a logical buffer.  The actual
@@ -128,16 +124,16 @@ typedef struct pb_target {
  * to indicate which disk blocks in the page are not valid.
  */
 
-struct page_buf_s;
-typedef void (*page_buf_iodone_t)(struct page_buf_s *);
+struct xfs_buf;
+typedef void (*page_buf_iodone_t)(struct xfs_buf *);
 			/* call-back function on I/O completion */
-typedef void (*page_buf_relse_t)(struct page_buf_s *);
+typedef void (*page_buf_relse_t)(struct xfs_buf *);
 			/* call-back function on I/O completion */
-typedef int (*page_buf_bdstrat_t)(struct page_buf_s *);
+typedef int (*page_buf_bdstrat_t)(struct xfs_buf *);
 
 #define PB_PAGES	4
 
-typedef struct page_buf_s {
+typedef struct xfs_buf {
 	struct semaphore	pb_sema;	/* semaphore for lockables  */
 	unsigned long		pb_flushtime;	/* time to flush pagebuf    */
 	atomic_t		pb_pin_count;	/* pin count		    */
@@ -145,9 +141,9 @@ typedef struct page_buf_s {
 	struct list_head	pb_list;
 	page_buf_flags_t	pb_flags;	/* status flags */
 	struct list_head	pb_hash_list;
-	struct pb_target	*pb_target;	/* logical object */
+	xfs_buftarg_t		*pb_target;	/* logical object */
 	atomic_t		pb_hold;	/* reference count */
-	page_buf_daddr_t	pb_bn;		/* block number for I/O */
+	xfs_daddr_t		pb_bn;		/* block number for I/O */
 	loff_t			pb_file_offset;	/* offset in file */
 	size_t			pb_buffer_length; /* size of buffer in bytes */
 	size_t			pb_count_desired; /* desired transfer size */
@@ -171,52 +167,52 @@ typedef struct page_buf_s {
 #ifdef PAGEBUF_LOCK_TRACKING
 	int			pb_last_holder;
 #endif
-} page_buf_t;
+} xfs_buf_t;
 
 
 /* Finding and Reading Buffers */
 
-extern page_buf_t *pagebuf_find(	/* find buffer for block if	*/
+extern xfs_buf_t *pagebuf_find(	/* find buffer for block if	*/
 					/* the block is in memory	*/
-		struct pb_target *,	/* inode for block		*/
+		xfs_buftarg_t *,	/* inode for block		*/
 		loff_t,			/* starting offset of range	*/
 		size_t,			/* length of range		*/
 		page_buf_flags_t);	/* PBF_LOCK			*/
 
-extern page_buf_t *pagebuf_get(		/* allocate a buffer		*/
-		struct pb_target *,	/* inode for buffer		*/
+extern xfs_buf_t *pagebuf_get(		/* allocate a buffer		*/
+		xfs_buftarg_t *,	/* inode for buffer		*/
 		loff_t,			/* starting offset of range     */
 		size_t,			/* length of range              */
 		page_buf_flags_t);	/* PBF_LOCK, PBF_READ,		*/
 					/* PBF_ASYNC			*/
 
-extern page_buf_t *pagebuf_lookup(
-		struct pb_target *,
+extern xfs_buf_t *pagebuf_lookup(
+		xfs_buftarg_t *,
 		loff_t,			/* starting offset of range	*/
 		size_t,			/* length of range		*/
 		page_buf_flags_t);	/* PBF_READ, PBF_WRITE,		*/
 					/* PBF_FORCEIO, 		*/
 
-extern page_buf_t *pagebuf_get_empty(	/* allocate pagebuf struct with	*/
+extern xfs_buf_t *pagebuf_get_empty(	/* allocate pagebuf struct with	*/
 					/*  no memory or disk address	*/
 		size_t len,
-		struct pb_target *);	/* mount point "fake" inode	*/
+		xfs_buftarg_t *);	/* mount point "fake" inode	*/
 
-extern page_buf_t *pagebuf_get_no_daddr(/* allocate pagebuf struct	*/
+extern xfs_buf_t *pagebuf_get_no_daddr(/* allocate pagebuf struct	*/
 					/* without disk address		*/
 		size_t len,
-		struct pb_target *);	/* mount point "fake" inode	*/
+		xfs_buftarg_t *);	/* mount point "fake" inode	*/
 
 extern int pagebuf_associate_memory(
-		page_buf_t *,
+		xfs_buf_t *,
 		void *,
 		size_t);
 
 extern void pagebuf_hold(		/* increment reference count	*/
-		page_buf_t *);		/* buffer to hold		*/
+		xfs_buf_t *);		/* buffer to hold		*/
 
 extern void pagebuf_readahead(		/* read ahead into cache	*/
-		struct pb_target  *,	/* target for buffer (or NULL)	*/
+		xfs_buftarg_t  *,	/* target for buffer (or NULL)	*/
 		loff_t,			/* starting offset of range     */
 		size_t,			/* length of range              */
 		page_buf_flags_t);	/* additional read flags	*/
@@ -224,63 +220,63 @@ extern void pagebuf_readahead(		/* read 
 /* Releasing Buffers */
 
 extern void pagebuf_free(		/* deallocate a buffer		*/
-		page_buf_t *);		/* buffer to deallocate		*/
+		xfs_buf_t *);		/* buffer to deallocate		*/
 
 extern void pagebuf_rele(		/* release hold on a buffer	*/
-		page_buf_t *);		/* buffer to release		*/
+		xfs_buf_t *);		/* buffer to release		*/
 
 /* Locking and Unlocking Buffers */
 
 extern int pagebuf_cond_lock(		/* lock buffer, if not locked	*/
 					/* (returns -EBUSY if locked)	*/
-		page_buf_t *);		/* buffer to lock		*/
+		xfs_buf_t *);		/* buffer to lock		*/
 
 extern int pagebuf_lock_value(		/* return count on lock		*/
-		page_buf_t *);          /* buffer to check              */
+		xfs_buf_t *);          /* buffer to check              */
 
 extern int pagebuf_lock(		/* lock buffer                  */
-		page_buf_t *);          /* buffer to lock               */
+		xfs_buf_t *);          /* buffer to lock               */
 
 extern void pagebuf_unlock(		/* unlock buffer		*/
-		page_buf_t *);		/* buffer to unlock		*/
+		xfs_buf_t *);		/* buffer to unlock		*/
 
 /* Buffer Read and Write Routines */
 
 extern void pagebuf_iodone(		/* mark buffer I/O complete	*/
-		page_buf_t *,		/* buffer to mark		*/
+		xfs_buf_t *,		/* buffer to mark		*/
 		int,			/* use data/log helper thread.	*/
 		int);			/* run completion locally, or in
 					 * a helper thread.		*/
 
 extern void pagebuf_ioerror(		/* mark buffer in error	(or not) */
-		page_buf_t *,		/* buffer to mark		*/
+		xfs_buf_t *,		/* buffer to mark		*/
 		unsigned int);		/* error to store (0 if none)	*/
 
 extern int pagebuf_iostart(		/* start I/O on a buffer	*/
-		page_buf_t *,		/* buffer to start		*/
+		xfs_buf_t *,		/* buffer to start		*/
 		page_buf_flags_t);	/* PBF_LOCK, PBF_ASYNC,		*/
 					/* PBF_READ, PBF_WRITE,		*/
 					/* PBF_DELWRI			*/
 
 extern int pagebuf_iorequest(		/* start real I/O		*/
-		page_buf_t *);		/* buffer to convey to device	*/
+		xfs_buf_t *);		/* buffer to convey to device	*/
 
 extern int pagebuf_iowait(		/* wait for buffer I/O done	*/
-		page_buf_t *);		/* buffer to wait on		*/
+		xfs_buf_t *);		/* buffer to wait on		*/
 
 extern void pagebuf_iomove(		/* move data in/out of pagebuf	*/
-		page_buf_t *,		/* buffer to manipulate		*/
+		xfs_buf_t *,		/* buffer to manipulate		*/
 		size_t,			/* starting buffer offset	*/
 		size_t,			/* length in buffer		*/
 		caddr_t,		/* data pointer			*/
 		page_buf_rw_t);		/* direction			*/
 
-static inline int pagebuf_iostrategy(page_buf_t *pb)
+static inline int pagebuf_iostrategy(xfs_buf_t *pb)
 {
 	return pb->pb_strat ? pb->pb_strat(pb) : pagebuf_iorequest(pb);
 }
 
-static inline int pagebuf_geterror(page_buf_t *pb)
+static inline int pagebuf_geterror(xfs_buf_t *pb)
 {
 	return pb ? pb->pb_error : ENOMEM;
 }
@@ -288,30 +284,24 @@ static inline int pagebuf_geterror(page_
 /* Buffer Utility Routines */
 
 extern caddr_t pagebuf_offset(		/* pointer at offset in buffer	*/
-		page_buf_t *,		/* buffer to offset into	*/
+		xfs_buf_t *,		/* buffer to offset into	*/
 		size_t);		/* offset			*/
 
 /* Pinning Buffer Storage in Memory */
 
 extern void pagebuf_pin(		/* pin buffer in memory		*/
-		page_buf_t *);		/* buffer to pin		*/
+		xfs_buf_t *);		/* buffer to pin		*/
 
 extern void pagebuf_unpin(		/* unpin buffered data		*/
-		page_buf_t *);		/* buffer to unpin		*/
+		xfs_buf_t *);		/* buffer to unpin		*/
 
 extern int pagebuf_ispin(		/* check if buffer is pinned	*/
-		page_buf_t *);		/* buffer to check		*/
+		xfs_buf_t *);		/* buffer to check		*/
 
 /* Delayed Write Buffer Routines */
 
-#define PBDF_WAIT    0x01
-extern void pagebuf_delwri_flush(
-		pb_target_t *,
-		unsigned long,
-		int *);
-
-extern void pagebuf_delwri_dequeue(
-		page_buf_t *);
+extern void pagebuf_delwri_flush(xfs_buftarg_t *, int, int *);
+extern void pagebuf_delwri_dequeue(xfs_buf_t *);
 
 /* Buffer Daemon Setup Routines */
 
@@ -322,7 +312,7 @@ extern void pagebuf_terminate(void);
 #ifdef PAGEBUF_TRACE
 extern ktrace_t *pagebuf_trace_buf;
 extern void pagebuf_trace(
-		page_buf_t *,		/* buffer being traced		*/
+		xfs_buf_t *,		/* buffer being traced		*/
 		char *,			/* description of operation	*/
 		void *,			/* arbitrary diagnostic value	*/
 		void *);		/* return address		*/
@@ -369,7 +359,7 @@ extern void pagebuf_trace(
 #define XFS_BUF_MANAGE		PBF_FS_MANAGED
 #define XFS_BUF_UNMANAGE(x)	((x)->pb_flags &= ~PBF_FS_MANAGED)
 
-static inline void xfs_buf_undelay(page_buf_t *pb)
+static inline void xfs_buf_undelay(xfs_buf_t *pb)
 {
 	if (pb->pb_flags & PBF_DELWRI) {
 		if (pb->pb_list.next != &pb->pb_list) {
@@ -423,12 +413,6 @@ static inline void xfs_buf_undelay(page_
 
 #define XFS_BUF_BP_ISMAPPED(bp)	 1
 
-typedef struct page_buf_s xfs_buf_t;
-#define xfs_buf page_buf_s
-
-typedef struct pb_target xfs_buftarg_t;
-#define xfs_buftarg pb_target
-
 #define XFS_BUF_DATAIO(x)	((x)->pb_flags |= PBF_FS_DATAIOD)
 #define XFS_BUF_UNDATAIO(x)	((x)->pb_flags &= ~PBF_FS_DATAIOD)
 
@@ -461,7 +445,7 @@ typedef struct pb_target xfs_buftarg_t;
 
 #define XFS_BUF_PTR(bp)		(xfs_caddr_t)((bp)->pb_addr)
 
-extern inline xfs_caddr_t xfs_buf_offset(page_buf_t *bp, size_t offset)
+extern inline xfs_caddr_t xfs_buf_offset(xfs_buf_t *bp, size_t offset)
 {
 	if (bp->pb_flags & PBF_MAPPED)
 		return XFS_BUF_PTR(bp) + offset;
@@ -472,7 +456,7 @@ extern inline xfs_caddr_t xfs_buf_offset
 				pagebuf_associate_memory(bp, val, count)
 #define XFS_BUF_ADDR(bp)	((bp)->pb_bn)
 #define XFS_BUF_SET_ADDR(bp, blk)		\
-			((bp)->pb_bn = (page_buf_daddr_t)(blk))
+			((bp)->pb_bn = (blk))
 #define XFS_BUF_OFFSET(bp)	((bp)->pb_file_offset)
 #define XFS_BUF_SET_OFFSET(bp, off)		\
 			((bp)->pb_file_offset = (off))
@@ -517,7 +501,7 @@ extern inline xfs_caddr_t xfs_buf_offset
 #define xfs_buf_get_flags(target, blkno, len, flags) \
 		pagebuf_get((target), (blkno), (len), (flags))
 
-static inline int	xfs_bawrite(void *mp, page_buf_t *bp)
+static inline int	xfs_bawrite(void *mp, xfs_buf_t *bp)
 {
 	bp->pb_fspriv3 = mp;
 	bp->pb_strat = xfs_bdstrat_cb;
@@ -525,7 +509,7 @@ static inline int	xfs_bawrite(void *mp, 
 	return pagebuf_iostart(bp, PBF_WRITE | PBF_ASYNC | PBF_RUN_QUEUES);
 }
 
-static inline void	xfs_buf_relse(page_buf_t *bp)
+static inline void	xfs_buf_relse(xfs_buf_t *bp)
 {
 	if (!bp->pb_relse)
 		pagebuf_unlock(bp);
@@ -553,7 +537,7 @@ static inline void	xfs_buf_relse(page_bu
 	    pagebuf_iomove((pb), (off), (len), NULL, PBRW_ZERO)
 
 
-static inline int	XFS_bwrite(page_buf_t *pb)
+static inline int	XFS_bwrite(xfs_buf_t *pb)
 {
 	int	iowait = (pb->pb_flags & PBF_ASYNC) == 0;
 	int	error = 0;
@@ -573,7 +557,7 @@ static inline int	XFS_bwrite(page_buf_t 
 #define XFS_bdwrite(pb)		     \
 	    pagebuf_iostart(pb, PBF_DELWRI | PBF_ASYNC)
 
-static inline int xfs_bdwrite(void *mp, page_buf_t *bp)
+static inline int xfs_bdwrite(void *mp, xfs_buf_t *bp)
 {
 	bp->pb_strat = xfs_bdstrat_cb;
 	bp->pb_fspriv3 = mp;
diff -uNrp linux-2.6.5/fs/xfs/linux/xfs_globals.c linux-2.6.5.xfs/fs/xfs/linux/xfs_globals.c
--- linux-2.6.5/fs/xfs/linux/xfs_globals.c	2004-04-04 05:37:25.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/linux/xfs_globals.c	2004-04-05 00:58:25.000000000 +0200
@@ -57,6 +57,9 @@ xfs_param_t xfs_params = {
 	.panic_mask	= {	0,	0,	127	},
 	.error_level	= {	0,	3,	11	},
 	.sync_interval	= {	HZ,	30*HZ,	60*HZ	},
+	.probe_dmapi	= {	0,	1,	1	},
+	.probe_ioops	= {	0,	0,	1	},
+	.probe_quota	= {	0,	1,	1	},
 	.stats_clear	= {	0,	0,	1	},
 	.inherit_sync	= {	0,	1,	1	},
 	.inherit_nodump	= {	0,	1,	1	},
diff -uNrp linux-2.6.5/fs/xfs/linux/xfs_ioctl.c linux-2.6.5.xfs/fs/xfs/linux/xfs_ioctl.c
--- linux-2.6.5/fs/xfs/linux/xfs_ioctl.c	2004-04-04 05:37:40.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/linux/xfs_ioctl.c	2004-04-05 00:58:25.000000000 +0200
@@ -659,7 +659,7 @@ xfs_ioctl(
 
 	case XFS_IOC_DIOINFO: {
 		struct dioattr	da;
-		pb_target_t	*target =
+		xfs_buftarg_t	*target =
 			(ip->i_d.di_flags & XFS_DIFLAG_REALTIME) ?
 			mp->m_rtdev_targp : mp->m_ddev_targp;
 
diff -uNrp linux-2.6.5/fs/xfs/linux/xfs_ksyms.c linux-2.6.5.xfs/fs/xfs/linux/xfs_ksyms.c
--- linux-2.6.5/fs/xfs/linux/xfs_ksyms.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/linux/xfs_ksyms.c	2004-04-05 00:58:26.000000000 +0200
@@ -0,0 +1,414 @@
+/*
+ * Copyright (c) 2004 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.  Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+
+#include "xfs.h"
+
+#include "xfs_fs.h"
+#include "xfs_buf.h"
+#include "xfs_macros.h"
+#include "xfs_inum.h"
+#include "xfs_log.h"
+#include "xfs_clnt.h"
+#include "xfs_trans.h"
+#include "xfs_sb.h"
+#include "xfs_ag.h"
+#include "xfs_dir.h"
+#include "xfs_dir2.h"
+#include "xfs_imap.h"
+#include "xfs_alloc.h"
+#include "xfs_dmapi.h"
+#include "xfs_quota.h"
+#include "xfs_mount.h"
+#include "xfs_alloc_btree.h"
+#include "xfs_bmap_btree.h"
+#include "xfs_ialloc_btree.h"
+#include "xfs_btree.h"
+#include "xfs_ialloc.h"
+#include "xfs_attr_sf.h"
+#include "xfs_dir_sf.h"
+#include "xfs_dir2_sf.h"
+#include "xfs_dinode.h"
+#include "xfs_inode.h"
+#include "xfs_bmap.h"
+#include "xfs_bit.h"
+#include "xfs_rtalloc.h"
+#include "xfs_error.h"
+#include "xfs_itable.h"
+#include "xfs_rw.h"
+#include "xfs_refcache.h"
+#include "xfs_da_btree.h"
+#include "xfs_dir_leaf.h"
+#include "xfs_dir2_data.h"
+#include "xfs_dir2_leaf.h"
+#include "xfs_dir2_block.h"
+#include "xfs_dir2_node.h"
+#include "xfs_dir2_trace.h"
+#include "xfs_acl.h"
+#include "xfs_cap.h"
+#include "xfs_mac.h"
+#include "xfs_attr.h"
+#include "xfs_attr_leaf.h"
+#include "xfs_inode_item.h"
+#include "xfs_buf_item.h"
+#include "xfs_extfree_item.h"
+#include "xfs_log_priv.h"
+#include "xfs_trans_priv.h"
+#include "xfs_trans_space.h"
+#include "xfs_utils.h"
+#include "xfs_iomap.h"
+#include "support/ktrace.h"
+
+
+/*
+ * Export symbols used for XFS tracing
+ */
+
+#if defined(CONFIG_XFS_TRACE)
+EXPORT_SYMBOL(ktrace_enter);
+EXPORT_SYMBOL(ktrace_free);
+EXPORT_SYMBOL(ktrace_alloc);
+EXPORT_SYMBOL(ktrace_skip);
+EXPORT_SYMBOL(ktrace_nentries);
+EXPORT_SYMBOL(ktrace_first);
+EXPORT_SYMBOL(ktrace_next);
+#endif
+
+#ifdef XFS_ILOCK_TRACE
+EXPORT_SYMBOL(xfs_ilock_trace_buf);
+#endif
+#ifdef XFS_ALLOC_TRACE
+EXPORT_SYMBOL(xfs_alloc_trace_buf);
+#endif
+#ifdef XFS_BMAP_TRACE
+EXPORT_SYMBOL(xfs_bmap_trace_buf);
+#endif
+#ifdef XFS_BMBT_TRACE
+EXPORT_SYMBOL(xfs_bmbt_trace_buf);
+#endif
+#ifdef XFS_ATTR_TRACE
+EXPORT_SYMBOL(xfs_attr_trace_buf);
+#endif  
+#ifdef XFS_DIR2_TRACE
+EXPORT_SYMBOL(xfs_dir2_trace_buf);
+#endif   
+#ifdef XFS_DIR_TRACE
+EXPORT_SYMBOL(xfs_dir_trace_buf);
+#endif
+
+#ifdef PAGEBUF_TRACE
+extern ktrace_t *pagebuf_trace_buf;
+EXPORT_SYMBOL(pagebuf_trace_buf);
+#endif
+
+
+/*
+ * Export symbols used for XFS debugging
+ */
+EXPORT_SYMBOL(xfs_next_bit);
+EXPORT_SYMBOL(xfs_contig_bits);
+EXPORT_SYMBOL(xfs_bmbt_get_all);
+#if ARCH_CONVERT != ARCH_NOCONVERT
+EXPORT_SYMBOL(xfs_bmbt_disk_get_all);
+#endif
+
+#if defined(CONFIG_XFS_DEBUG)
+extern struct list_head pbd_delwrite_queue;
+EXPORT_SYMBOL(pbd_delwrite_queue);
+
+EXPORT_SYMBOL(xfs_fsb_to_agbno);
+EXPORT_SYMBOL(xfs_dir2_data_unused_tag_p_arch);
+EXPORT_SYMBOL(xfs_attr_leaf_name_remote);
+EXPORT_SYMBOL(xfs_lic_slot);
+EXPORT_SYMBOL(xfs_dir2_sf_firstentry);
+EXPORT_SYMBOL(xfs_ino_to_agno);
+EXPORT_SYMBOL(xfs_dir2_sf_get_inumber_arch);
+EXPORT_SYMBOL(xfs_dir2_data_entry_tag_p);
+EXPORT_SYMBOL(xfs_dir2_sf_inumberp);
+EXPORT_SYMBOL(xfs_dir2_data_entsize);
+EXPORT_SYMBOL(xfs_lic_isfree);
+EXPORT_SYMBOL(xfs_attr_leaf_name_local);
+EXPORT_SYMBOL(xfs_bmap_broot_ptr_addr);
+EXPORT_SYMBOL(xfs_dir_sf_get_dirino_arch);
+EXPORT_SYMBOL(xfs_ino_to_agbno);
+EXPORT_SYMBOL(xfs_dir2_leaf_bests_p_arch);
+EXPORT_SYMBOL(xfs_dir2_sf_get_offset_arch);
+EXPORT_SYMBOL(startblockval);
+EXPORT_SYMBOL(xfs_attr_sf_nextentry);
+EXPORT_SYMBOL(xfs_bmap_broot_key_addr);
+EXPORT_SYMBOL(xfs_dir2_block_leaf_p_arch);
+EXPORT_SYMBOL(xfs_dir_leaf_namestruct);
+EXPORT_SYMBOL(xfs_ino_to_offset);
+EXPORT_SYMBOL(isnullstartblock);
+EXPORT_SYMBOL(xfs_lic_are_all_free);
+EXPORT_SYMBOL(xfs_dir_sf_nextentry);
+EXPORT_SYMBOL(xfs_dir2_sf_nextentry);
+
+EXPORT_SYMBOL(xfs_da_cookie_entry);
+EXPORT_SYMBOL(xfs_da_cookie_bno);
+EXPORT_SYMBOL(xfs_da_cookie_hash);
+#endif
+
+
+/*
+ * Export symbols used by XFS behavior modules.
+ */
+
+EXPORT_SYMBOL(assfail);
+EXPORT_SYMBOL(cmn_err);
+EXPORT_SYMBOL(bhv_base);
+EXPORT_SYMBOL(bhv_head_destroy);
+EXPORT_SYMBOL(bhv_insert);
+EXPORT_SYMBOL(bhv_insert_initial);
+EXPORT_SYMBOL(bhv_lookup);
+EXPORT_SYMBOL(bhv_lookup_range);
+EXPORT_SYMBOL(bhv_remove_vfsops);
+EXPORT_SYMBOL(bhv_remove_all_vfsops);
+EXPORT_SYMBOL(bhv_remove_not_first);
+EXPORT_SYMBOL(doass);
+EXPORT_SYMBOL(fs_flush_pages);
+EXPORT_SYMBOL(fs_flushinval_pages);
+EXPORT_SYMBOL(fs_tosspages);
+EXPORT_SYMBOL(fs_noval);
+#if ((defined(DEBUG) || defined(INDUCE_IO_ERROR)) && !defined(NO_WANT_RANDOM))
+EXPORT_SYMBOL(get_thread_id);
+#endif
+EXPORT_SYMBOL(icmn_err);
+EXPORT_SYMBOL(kmem_alloc);
+EXPORT_SYMBOL(kmem_free);
+EXPORT_SYMBOL(kmem_realloc);
+EXPORT_SYMBOL(kmem_shake_deregister);
+EXPORT_SYMBOL(kmem_shake_register);
+EXPORT_SYMBOL(kmem_zalloc);
+EXPORT_SYMBOL(kmem_zone_alloc);
+EXPORT_SYMBOL(kmem_zone_free);
+EXPORT_SYMBOL(kmem_zone_init);
+EXPORT_SYMBOL(kmem_zone_zalloc);
+EXPORT_SYMBOL(linvfs_aops);
+EXPORT_SYMBOL(linvfs_dir_inode_operations);
+EXPORT_SYMBOL(linvfs_dir_operations);
+EXPORT_SYMBOL(linvfs_file_inode_operations);
+EXPORT_SYMBOL(linvfs_file_operations);
+EXPORT_SYMBOL(linvfs_invis_file_operations);
+EXPORT_SYMBOL(linvfs_symlink_inode_operations);
+EXPORT_SYMBOL(pagebuf_delwri_dequeue);
+EXPORT_SYMBOL(pagebuf_find);
+EXPORT_SYMBOL(pagebuf_iostart);
+EXPORT_SYMBOL(pagebuf_ispin);
+EXPORT_SYMBOL(pagebuf_lock_value);
+EXPORT_SYMBOL(pagebuf_offset);
+EXPORT_SYMBOL(pagebuf_rele);
+EXPORT_SYMBOL(pagebuf_readahead);
+EXPORT_SYMBOL(pagebuf_unlock);
+EXPORT_SYMBOL(qsort);
+#if ((defined(DEBUG) || defined(INDUCE_IO_ERROR)) && !defined(NO_WANT_RANDOM))
+EXPORT_SYMBOL(random);
+#endif
+EXPORT_SYMBOL(sys_cred);
+EXPORT_SYMBOL(uuid_create_nil);
+EXPORT_SYMBOL(uuid_equal);
+EXPORT_SYMBOL(uuid_getnodeuniq);
+EXPORT_SYMBOL(uuid_hash64);
+EXPORT_SYMBOL(uuid_is_nil);
+EXPORT_SYMBOL(uuid_table_remove);
+EXPORT_SYMBOL(vfs_mount);
+EXPORT_SYMBOL(vfs_parseargs);
+EXPORT_SYMBOL(vfs_showargs);
+EXPORT_SYMBOL(vfs_unmount);
+EXPORT_SYMBOL(vfs_mntupdate);
+EXPORT_SYMBOL(vfs_root);
+EXPORT_SYMBOL(vfs_statvfs);
+EXPORT_SYMBOL(vfs_sync);
+EXPORT_SYMBOL(vfs_vget);
+EXPORT_SYMBOL(vfs_dmapiops);
+EXPORT_SYMBOL(vfs_quotactl);
+EXPORT_SYMBOL(vfs_get_inode);
+EXPORT_SYMBOL(vfs_init_vnode);
+EXPORT_SYMBOL(vfs_force_shutdown);
+EXPORT_SYMBOL(vn_wait);
+EXPORT_SYMBOL(vn_get);
+EXPORT_SYMBOL(vn_hold);
+EXPORT_SYMBOL(vn_initialize);
+EXPORT_SYMBOL(vn_revalidate);
+EXPORT_SYMBOL(vn_purge);
+EXPORT_SYMBOL(vttoif_tab);
+
+#if defined(CONFIG_XFS_POSIX_ACL)
+EXPORT_SYMBOL(xfs_acl_vtoacl);
+EXPORT_SYMBOL(xfs_acl_inherit);
+#endif
+EXPORT_SYMBOL(xfs_alloc_buftarg);
+EXPORT_SYMBOL(xfs_flush_buftarg);
+EXPORT_SYMBOL(xfs_bdstrat_cb);
+EXPORT_SYMBOL(xfs_bmap_cancel);
+EXPORT_SYMBOL(xfs_bmap_do_search_extents);
+EXPORT_SYMBOL(xfs_bmap_finish);
+EXPORT_SYMBOL(xfs_bmapi);
+EXPORT_SYMBOL(xfs_bmapi_single);
+EXPORT_SYMBOL(xfs_bmbt_get_blockcount);
+EXPORT_SYMBOL(xfs_bmbt_get_state);
+EXPORT_SYMBOL(xfs_bmbt_get_startoff);
+EXPORT_SYMBOL(xfs_bmbt_set_all);
+EXPORT_SYMBOL(xfs_bmbt_set_allf);
+EXPORT_SYMBOL(xfs_bmbt_set_blockcount);
+EXPORT_SYMBOL(xfs_bmbt_set_startblock);
+EXPORT_SYMBOL(xfs_bmbt_set_startoff);
+EXPORT_SYMBOL(xfs_bmbt_set_state);
+EXPORT_SYMBOL(xfs_buf_attach_iodone);
+EXPORT_SYMBOL(xfs_bulkstat);
+EXPORT_SYMBOL(xfs_bunmapi);
+EXPORT_SYMBOL(xfs_bwrite);
+EXPORT_SYMBOL(xfs_change_file_space);
+EXPORT_SYMBOL(xfs_chashlist_zone);
+EXPORT_SYMBOL(xfs_dev_is_read_only);
+EXPORT_SYMBOL(xfs_dir_ialloc);
+EXPORT_SYMBOL(xfs_error_report);
+#ifdef DEBUG
+EXPORT_SYMBOL(xfs_error_trap);
+#endif
+EXPORT_SYMBOL(xfs_file_last_byte);
+EXPORT_SYMBOL(xfs_freesb);
+EXPORT_SYMBOL(xfs_fs_cmn_err);
+EXPORT_SYMBOL(xfs_highbit32);
+EXPORT_SYMBOL(xfs_highbit64);
+#if XFS_WANT_FUNCS || (XFS_WANT_SPACE && XFSSO_XFS_IALLOC_BLOCKS)
+EXPORT_SYMBOL(xfs_ialloc_blocks);
+#endif
+EXPORT_SYMBOL(xfs_idestroy);
+EXPORT_SYMBOL(xfs_iextract);
+EXPORT_SYMBOL(xfs_iflock);
+EXPORT_SYMBOL(xfs_iflock_nowait);
+EXPORT_SYMBOL(xfs_iflush);
+EXPORT_SYMBOL(xfs_iflush_all);
+EXPORT_SYMBOL(xfs_ifunlock);
+EXPORT_SYMBOL(xfs_iget);
+EXPORT_SYMBOL(xfs_igrow_start);
+EXPORT_SYMBOL(xfs_igrow_finish);
+EXPORT_SYMBOL(xfs_ilock);
+EXPORT_SYMBOL(xfs_ilock_map_shared);
+EXPORT_SYMBOL(xfs_ilock_nowait);
+EXPORT_SYMBOL(xfs_inode_lock_init);
+EXPORT_SYMBOL(xfs_iocore_inode_init);
+EXPORT_SYMBOL(xfs_iocore_xfs);
+EXPORT_SYMBOL(xfs_iomap);
+EXPORT_SYMBOL(xfs_iput);
+EXPORT_SYMBOL(xfs_iput_new);
+EXPORT_SYMBOL(xfs_iread);
+EXPORT_SYMBOL(xfs_iread_extents);
+EXPORT_SYMBOL(xfs_itruncate_start);
+EXPORT_SYMBOL(xfs_iunlock);
+EXPORT_SYMBOL(xfs_iunlock_map_shared);
+EXPORT_SYMBOL(xfs_itruncate_finish);
+EXPORT_SYMBOL(xfs_log_force);
+EXPORT_SYMBOL(xfs_log_force_umount);
+EXPORT_SYMBOL(xfs_log_unmount_dealloc);
+EXPORT_SYMBOL(xfs_log_unmount_write);
+EXPORT_SYMBOL(xfs_mod_sb);
+EXPORT_SYMBOL(xfs_mount_free);
+EXPORT_SYMBOL(xfs_mount_init);
+EXPORT_SYMBOL(xfs_mountfs);
+EXPORT_SYMBOL(xfs_qm_dqcheck);
+EXPORT_SYMBOL(xfs_readsb);
+EXPORT_SYMBOL(xfs_read_buf);
+EXPORT_SYMBOL(xfs_rwlock);
+EXPORT_SYMBOL(xfs_rwunlock);
+EXPORT_SYMBOL(xfs_setsize_buftarg);
+EXPORT_SYMBOL(xfs_syncsub);
+EXPORT_SYMBOL(xfs_trans_add_item);
+EXPORT_SYMBOL(xfs_trans_alloc);
+EXPORT_SYMBOL(xfs_trans_brelse);
+EXPORT_SYMBOL(xfs_trans_cancel);
+EXPORT_SYMBOL(xfs_trans_commit);
+EXPORT_SYMBOL(xfs_trans_delete_ail);
+EXPORT_SYMBOL(xfs_trans_dquot_buf);
+EXPORT_SYMBOL(xfs_trans_find_item);
+EXPORT_SYMBOL(xfs_trans_get_buf);
+EXPORT_SYMBOL(xfs_trans_iget);
+EXPORT_SYMBOL(xfs_trans_ihold);
+EXPORT_SYMBOL(xfs_trans_ijoin);
+EXPORT_SYMBOL(xfs_trans_log_buf);
+EXPORT_SYMBOL(xfs_trans_log_inode);
+EXPORT_SYMBOL(xfs_trans_mod_sb);
+EXPORT_SYMBOL(xfs_trans_read_buf);
+EXPORT_SYMBOL(xfs_trans_reserve);
+EXPORT_SYMBOL(xfs_trans_unlocked_item);
+EXPORT_SYMBOL(xfs_truncate_file);
+EXPORT_SYMBOL(xfs_unmount_flush);
+EXPORT_SYMBOL(xfs_unmountfs_writesb);
+EXPORT_SYMBOL(xfs_vfsops);
+EXPORT_SYMBOL(xfs_vnodeops);
+EXPORT_SYMBOL(xfs_write_clear_setuid);
+EXPORT_SYMBOL(xfs_zero_eof);
+EXPORT_SYMBOL(xlog_recover_process_iunlinks);
+
+
+#if XFS_WANT_FUNCS_C || (XFS_WANT_SPACE_C && XFSSO_XFS_BHVTOI)
+EXPORT_SYMBOL(xfs_bhvtoi);
+#endif
+#if XFS_WANT_FUNCS_C || (XFS_WANT_SPACE_C && XFSSO_XFS_VFSTOM)
+EXPORT_SYMBOL(xfs_vfstom);
+#endif
+#if XFS_WANT_FUNCS_C || (XFS_WANT_SPACE_C && XFSSO_XFS_BM_MAXLEVELS)
+EXPORT_SYMBOL(xfs_bm_maxlevels);
+#endif
+#if XFS_WANT_FUNCS_C || (XFS_WANT_SPACE_C && XFSSO_XFS_BMAP_INIT)
+EXPORT_SYMBOL(xfs_bmap_init);
+#endif
+#if XFS_WANT_FUNCS_C || (XFS_WANT_SPACE_C && XFSSO_XFS_FILBLKS_MIN)
+EXPORT_SYMBOL(xfs_filblks_min);
+#endif
+#if XFS_WANT_FUNCS_C || (XFS_WANT_SPACE_C && XFSSO_XFS_FSB_TO_DADDR)
+EXPORT_SYMBOL(xfs_fsb_to_daddr);
+#endif
+#if XFS_WANT_FUNCS_C || (XFS_WANT_SPACE_C && XFSSO_XFS_IFORK_PTR)
+EXPORT_SYMBOL(xfs_ifork_ptr);
+#endif
+#if XFS_WANT_FUNCS_C || (XFS_WANT_SPACE_C && XFSSO_XFS_IFORK_Q)
+EXPORT_SYMBOL(xfs_ifork_q);
+#endif
+#if XFS_WANT_FUNCS_C || (XFS_WANT_SPACE_C && XFSSO_XFS_IN_MAXLEVELS)
+EXPORT_SYMBOL(xfs_in_maxlevels);
+#endif
+#if XFS_WANT_FUNCS_C || (XFS_WANT_SPACE_C && XFSSO_XFS_ITOBHV)
+EXPORT_SYMBOL(xfs_itobhv);
+#endif
+#if XFS_WANT_FUNCS_C || (XFS_WANT_SPACE_C && XFSSO_XFS_ITOV)
+EXPORT_SYMBOL(xfs_itov);
+#endif
+#if XFS_WANT_FUNCS_C || (XFS_WANT_SPACE_C && XFSSO_XFS_MTOVFS)
+EXPORT_SYMBOL(xfs_mtovfs);
+#endif
+#if XFS_WANT_FUNCS_C || (XFS_WANT_SPACE_C && XFSSO_XFS_SB_VERSION_ADDQUOTA)
+EXPORT_SYMBOL(xfs_sb_version_addquota);
+#endif
+#if XFS_WANT_FUNCS_C || (XFS_WANT_SPACE_C && XFSSO_XFS_SB_VERSION_HASQUOTA)
+EXPORT_SYMBOL(xfs_sb_version_hasquota);
+#endif
diff -uNrp linux-2.6.5/fs/xfs/linux/xfs_linux.h linux-2.6.5.xfs/fs/xfs/linux/xfs_linux.h
--- linux-2.6.5/fs/xfs/linux/xfs_linux.h	2004-04-04 05:36:13.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/linux/xfs_linux.h	2004-04-05 00:58:26.000000000 +0200
@@ -135,6 +135,9 @@ static inline void set_buffer_unwritten_
 #define xfs_panic_mask		xfs_params.panic_mask.val
 #define xfs_error_level		xfs_params.error_level.val
 #define xfs_syncd_interval	xfs_params.sync_interval.val
+#define xfs_probe_dmapi		xfs_params.probe_dmapi.val
+#define xfs_probe_ioops		xfs_params.probe_ioops.val
+#define xfs_probe_quota		xfs_params.probe_quota.val
 #define xfs_stats_clear		xfs_params.stats_clear.val
 #define xfs_inherit_sync	xfs_params.inherit_sync.val
 #define xfs_inherit_nodump	xfs_params.inherit_nodump.val
diff -uNrp linux-2.6.5/fs/xfs/linux/xfs_lrw.c linux-2.6.5.xfs/fs/xfs/linux/xfs_lrw.c
--- linux-2.6.5/fs/xfs/linux/xfs_lrw.c	2004-04-04 05:36:10.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/linux/xfs_lrw.c	2004-04-05 00:58:26.000000000 +0200
@@ -301,7 +301,7 @@ xfs_read(
 	/* END copy & waste from filemap.c */
 
 	if (ioflags & IO_ISDIRECT) {
-		pb_target_t	*target =
+		xfs_buftarg_t	*target =
 			(ip->i_d.di_flags & XFS_DIFLAG_REALTIME) ?
 				mp->m_rtdev_targp : mp->m_ddev_targp;
 		if ((*offset & target->pbr_smask) ||
@@ -329,7 +329,8 @@ xfs_read(
 	 * does not really happen here, but is scheduled 
 	 * later?
 	 */
-	xfs_ilock(ip, XFS_IOLOCK_SHARED);
+	if (!(ioflags & IO_ISLOCKED))
+		xfs_ilock(ip, XFS_IOLOCK_SHARED);
 
 	if (DM_EVENT_ENABLED(vp->v_vfsp, ip, DM_EVENT_READ) &&
 	    !(ioflags & IO_INVIS)) {
@@ -339,7 +340,8 @@ xfs_read(
 					BHV_TO_VNODE(bdp), *offset, size,
 					FILP_DELAY_FLAG(file), &locktype);
 		if (ret) {
-			xfs_iunlock(ip, XFS_IOLOCK_SHARED);
+			if (!(ioflags & IO_ISLOCKED))
+				xfs_iunlock(ip, XFS_IOLOCK_SHARED);
 			return -ret;
 		}
 	}
@@ -347,7 +349,8 @@ xfs_read(
 	xfs_rw_enter_trace(XFS_READ_ENTER, &ip->i_iocore,
 				iovp, segs, *offset, ioflags);
 	ret = __generic_file_aio_read(iocb, iovp, segs, offset);
-	xfs_iunlock(ip, XFS_IOLOCK_SHARED);
+	if (!(ioflags & IO_ISLOCKED))
+		xfs_iunlock(ip, XFS_IOLOCK_SHARED);
 
 	if (ret > 0)
 		XFS_STATS_ADD(xs_read_bytes, ret);
@@ -391,7 +394,8 @@ xfs_sendfile(
 	if (XFS_FORCED_SHUTDOWN(ip->i_mount))
 		return -EIO;
 
-	xfs_ilock(ip, XFS_IOLOCK_SHARED);
+	if (!(ioflags & IO_ISLOCKED))
+		xfs_ilock(ip, XFS_IOLOCK_SHARED);
 
 	if (DM_EVENT_ENABLED(vp->v_vfsp, ip, DM_EVENT_READ) &&
 	    (!(ioflags & IO_INVIS))) {
@@ -401,14 +405,16 @@ xfs_sendfile(
 		error = XFS_SEND_DATA(mp, DM_EVENT_READ, BHV_TO_VNODE(bdp), *offset, count,
 				      FILP_DELAY_FLAG(filp), &locktype);
 		if (error) {
-			xfs_iunlock(ip, XFS_IOLOCK_SHARED);
+			if (!(ioflags & IO_ISLOCKED))
+				xfs_iunlock(ip, XFS_IOLOCK_SHARED);
 			return -error;
 		}
 	}
 	xfs_rw_enter_trace(XFS_SENDFILE_ENTER, &ip->i_iocore,
 				target, count, *offset, ioflags);
 	ret = generic_file_sendfile(filp, offset, count, actor, target);
-	xfs_iunlock(ip, XFS_IOLOCK_SHARED);
+	if (!(ioflags & IO_ISLOCKED))
+		xfs_iunlock(ip, XFS_IOLOCK_SHARED);
 
 	XFS_STATS_ADD(xs_read_bytes, ret);
 	xfs_ichgtime(ip, XFS_ICHGTIME_ACC);
@@ -689,7 +695,7 @@ xfs_write(
 	}
 
 	if (ioflags & IO_ISDIRECT) {
-		pb_target_t	*target =
+		xfs_buftarg_t	*target =
 			(xip->i_d.di_flags & XFS_DIFLAG_REALTIME) ?
 				mp->m_rtdev_targp : mp->m_ddev_targp;
 
@@ -704,6 +710,9 @@ xfs_write(
 		locktype = VRWLOCK_WRITE;
 	}
 
+	if (ioflags & IO_ISLOCKED)
+		iolock = 0;
+
 	xfs_ilock(xip, XFS_ILOCK_EXCL|iolock);
 
 	isize = xip->i_d.di_size;
@@ -737,7 +746,7 @@ start:
 				      *offset, size,
 				      dmflags, &locktype);
 		if (error) {
-			xfs_iunlock(xip, iolock);
+			if (iolock) xfs_iunlock(xip, iolock);
 			return -error;
 		}
 		xfs_ilock(xip, XFS_ILOCK_EXCL);
@@ -930,7 +939,9 @@ retry:
 		}
 	} /* (ioflags & O_SYNC) */
 
-	xfs_rwunlock(bdp, locktype);
+	if (iolock)
+		xfs_rwunlock(bdp, locktype);
+
 	return(ret);
 }
 
diff -uNrp linux-2.6.5/fs/xfs/linux/xfs_lrw.h linux-2.6.5.xfs/fs/xfs/linux/xfs_lrw.h
--- linux-2.6.5/fs/xfs/linux/xfs_lrw.h	2004-04-04 05:36:17.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/linux/xfs_lrw.h	2004-04-05 00:58:26.000000000 +0200
@@ -38,7 +38,7 @@ struct xfs_mount;
 struct xfs_iocore;
 struct xfs_inode;
 struct xfs_bmbt_irec;
-struct page_buf_s;
+struct xfs_buf;
 struct xfs_iomap;
 
 #if defined(XFS_RW_TRACE)
@@ -89,8 +89,8 @@ extern void xfs_inval_cached_trace(struc
 
 extern int xfs_bmap(struct bhv_desc *, xfs_off_t, ssize_t, int,
 			struct xfs_iomap *, int *);
-extern int xfsbdstrat(struct xfs_mount *, struct page_buf_s *);
-extern int xfs_bdstrat_cb(struct page_buf_s *);
+extern int xfsbdstrat(struct xfs_mount *, struct xfs_buf *);
+extern int xfs_bdstrat_cb(struct xfs_buf *);
 
 extern int xfs_zero_eof(struct vnode *, struct xfs_iocore *, xfs_off_t,
 				xfs_fsize_t, xfs_fsize_t);
diff -uNrp linux-2.6.5/fs/xfs/linux/xfs_super.c linux-2.6.5.xfs/fs/xfs/linux/xfs_super.c
--- linux-2.6.5/fs/xfs/linux/xfs_super.c	2004-04-04 05:38:15.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/linux/xfs_super.c	2004-04-05 00:58:26.000000000 +0200
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2000-2003 Silicon Graphics, Inc.  All Rights Reserved.
+ * Copyright (c) 2000-2004 Silicon Graphics, Inc.  All Rights Reserved.
  *
  * This program is free software; you can redistribute it and/or modify it
  * under the terms of version 2 of the GNU General Public License as
@@ -153,8 +153,7 @@ xfs_set_inodeops(
 			inode->i_mapping->a_ops = &linvfs_aops;
 	} else {
 		inode->i_op = &linvfs_file_inode_operations;
-		init_special_inode(inode, inode->i_mode,
-					inode->i_rdev);
+		init_special_inode(inode, inode->i_mode, inode->i_rdev);
 	}
 }
 
@@ -241,6 +240,19 @@ xfs_initialize_vnode(
 	}
 }
 
+struct inode *
+xfs_get_inode(
+	bhv_desc_t	*bdp,
+	xfs_ino_t	ino,
+	int		flags)
+{
+	struct vfs	*vfsp = bhvtovfs(bdp);
+
+	if (flags & IGET_NOALLOC)
+		return ilookup(vfsp->vfs_super, ino);
+	return iget_locked(vfsp->vfs_super, ino);
+}
+
 void
 xfs_flush_inode(
 	xfs_inode_t	*ip)
@@ -287,7 +299,7 @@ void
 xfs_flush_buftarg(
 	xfs_buftarg_t		*btp)
 {
-	pagebuf_delwri_flush(btp, PBDF_WAIT, NULL);
+	pagebuf_delwri_flush(btp, 1, NULL);
 }
 
 void
@@ -448,7 +460,8 @@ linvfs_clear_inode(
 #define SYNCD_FLAGS	(SYNC_FSDATA|SYNC_BDFLUSH|SYNC_ATTR)
 
 STATIC int
-syncd(void *arg)
+xfssyncd(
+	void			*arg)
 {
 	vfs_t			*vfsp = (vfs_t *) arg;
 	int			error;
@@ -480,11 +493,12 @@ syncd(void *arg)
 }
 
 STATIC int
-linvfs_start_syncd(vfs_t *vfsp)
+linvfs_start_syncd(
+	vfs_t			*vfsp)
 {
-	int pid;
+	int			pid;
 
-	pid = kernel_thread(syncd, (void *) vfsp,
+	pid = kernel_thread(xfssyncd, (void *) vfsp,
 			CLONE_VM | CLONE_FS | CLONE_FILES);
 	if (pid < 0)
 		return pid;
@@ -493,7 +507,8 @@ linvfs_start_syncd(vfs_t *vfsp)
 }
 
 STATIC void
-linvfs_stop_syncd(vfs_t *vfsp)
+linvfs_stop_syncd(
+	vfs_t			*vfsp)
 {
 	vfsp->vfs_flag |= VFS_UMOUNT;
 	wmb();
@@ -898,8 +913,6 @@ init_xfs_fs( void )
 	vn_init();
 	xfs_init();
 	uuid_init();
-	vfs_initdmapi();
-	vfs_initquota();
 
 	error = register_filesystem(&xfs_fs_type);
 	if (error)
@@ -921,8 +934,6 @@ exit_xfs_fs( void )
 {
 	unregister_filesystem(&xfs_fs_type);
 	xfs_cleanup();
-	vfs_exitquota();
-	vfs_exitdmapi();
 	pagebuf_terminate();
 	destroy_inodecache();
 	ktrace_uninit();
diff -uNrp linux-2.6.5/fs/xfs/linux/xfs_super.h linux-2.6.5.xfs/fs/xfs/linux/xfs_super.h
--- linux-2.6.5/fs/xfs/linux/xfs_super.h	2004-04-04 05:36:53.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/linux/xfs_super.h	2004-04-05 00:58:26.000000000 +0200
@@ -32,28 +32,6 @@
 #ifndef __XFS_SUPER_H__
 #define __XFS_SUPER_H__
 
-#ifdef CONFIG_XFS_DMAPI
-# define vfs_insertdmapi(vfs)	vfs_insertops(vfsp, &xfs_dmops)
-# define vfs_initdmapi()	dmapi_init()
-# define vfs_exitdmapi()	dmapi_uninit()
-#else
-# define vfs_insertdmapi(vfs)	do { } while (0)
-# define vfs_initdmapi()	do { } while (0)
-# define vfs_exitdmapi()	do { } while (0)
-#endif
-
-#ifdef CONFIG_XFS_QUOTA
-# define vfs_insertquota(vfs)	vfs_insertops(vfsp, &xfs_qmops)
-extern void xfs_qm_init(void);
-extern void xfs_qm_exit(void);
-# define vfs_initquota()	xfs_qm_init()
-# define vfs_exitquota()	xfs_qm_exit()
-#else
-# define vfs_insertquota(vfs)	do { } while (0)
-# define vfs_initquota()	do { } while (0)
-# define vfs_exitquota()	do { } while (0)
-#endif
-
 #ifdef CONFIG_XFS_POSIX_ACL
 # define XFS_ACL_STRING		"ACLs, "
 # define set_posix_acl_flag(sb)	((sb)->s_flags |= MS_POSIXACL)
@@ -112,11 +90,12 @@ extern void xfs_qm_exit(void);
 
 struct xfs_inode;
 struct xfs_mount;
-struct pb_target;
+struct xfs_buftarg;
 struct block_device;
 
 extern __uint64_t xfs_max_file_offset(unsigned int);
 
+extern struct inode *xfs_get_inode(bhv_desc_t *, xfs_ino_t, int);
 extern void xfs_initialize_vnode(bhv_desc_t *, vnode_t *, bhv_desc_t *, int);
 
 extern void xfs_flush_inode(struct xfs_inode *);
@@ -126,12 +105,12 @@ extern int  xfs_blkdev_get(struct xfs_mo
 				struct block_device **);
 extern void xfs_blkdev_put(struct block_device *);
 
-extern struct pb_target *xfs_alloc_buftarg(struct block_device *);
-extern void xfs_relse_buftarg(struct pb_target *);
-extern void xfs_free_buftarg(struct pb_target *);
-extern void xfs_flush_buftarg(struct pb_target *);
-extern int xfs_readonly_buftarg(struct pb_target *);
-extern void xfs_setsize_buftarg(struct pb_target *, unsigned int, unsigned int);
-extern unsigned int xfs_getsize_buftarg(struct pb_target *);
+extern struct xfs_buftarg *xfs_alloc_buftarg(struct block_device *);
+extern void xfs_relse_buftarg(struct xfs_buftarg *);
+extern void xfs_free_buftarg(struct xfs_buftarg *);
+extern void xfs_flush_buftarg(struct xfs_buftarg *);
+extern int xfs_readonly_buftarg(struct xfs_buftarg *);
+extern void xfs_setsize_buftarg(struct xfs_buftarg *, unsigned int, unsigned int);
+extern unsigned int xfs_getsize_buftarg(struct xfs_buftarg *);
 
 #endif	/* __XFS_SUPER_H__ */
diff -uNrp linux-2.6.5/fs/xfs/linux/xfs_sysctl.c linux-2.6.5.xfs/fs/xfs/linux/xfs_sysctl.c
--- linux-2.6.5/fs/xfs/linux/xfs_sysctl.c	2004-04-04 05:38:28.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/linux/xfs_sysctl.c	2004-04-05 00:58:26.000000000 +0200
@@ -103,6 +103,21 @@ STATIC ctl_table xfs_table[] = {
 	&sysctl_intvec, NULL, 
 	&xfs_params.sync_interval.min, &xfs_params.sync_interval.max},
 
+	{XFS_PROBE_DMAPI, "probe_dmapi", &xfs_params.probe_dmapi.val,
+	sizeof(int), 0644, NULL, &proc_dointvec_minmax,
+	&sysctl_intvec, NULL,
+	&xfs_params.probe_dmapi.min, &xfs_params.probe_dmapi.max},
+
+	{XFS_PROBE_IOOPS, "probe_ioops", &xfs_params.probe_ioops.val,
+	sizeof(int), 0644, NULL, &proc_dointvec_minmax,
+	&sysctl_intvec, NULL,
+	&xfs_params.probe_ioops.min, &xfs_params.probe_ioops.max},
+
+	{XFS_PROBE_QUOTA, "probe_quota", &xfs_params.probe_quota.val,
+	sizeof(int), 0644, NULL, &proc_dointvec_minmax,
+	&sysctl_intvec, NULL,
+	&xfs_params.probe_quota.min, &xfs_params.probe_quota.max},
+
 	{XFS_INHERIT_SYNC, "inherit_sync", &xfs_params.inherit_sync.val,
 	sizeof(int), 0644, NULL, &proc_dointvec_minmax,
 	&sysctl_intvec, NULL,
diff -uNrp linux-2.6.5/fs/xfs/linux/xfs_sysctl.h linux-2.6.5.xfs/fs/xfs/linux/xfs_sysctl.h
--- linux-2.6.5/fs/xfs/linux/xfs_sysctl.h	2004-04-04 05:36:13.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/linux/xfs_sysctl.h	2004-04-05 00:58:26.000000000 +0200
@@ -55,6 +55,9 @@ typedef struct xfs_param {
 	xfs_sysctl_val_t error_level;	/* Degree of reporting for problems  */
 	xfs_sysctl_val_t sync_interval;	/* time between sync calls           */
 	xfs_sysctl_val_t stats_clear;	/* Reset all XFS statistics to zero. */
+	xfs_sysctl_val_t probe_dmapi;	/* probe for DMAPI module on mount. */
+	xfs_sysctl_val_t probe_ioops;	/* probe for an IO module on mount. */
+	xfs_sysctl_val_t probe_quota;	/* probe for quota module on mount. */
 	xfs_sysctl_val_t inherit_sync;	/* Inherit the "sync" inode flag. */
 	xfs_sysctl_val_t inherit_nodump;/* Inherit the "nodump" inode flag. */
 	xfs_sysctl_val_t inherit_noatim;/* Inherit the "noatime" inode flag. */
@@ -86,6 +89,9 @@ enum {
 	XFS_PANIC_MASK = 6,
 	XFS_ERRLEVEL = 7,
 	XFS_SYNC_INTERVAL = 8,
+	XFS_PROBE_DMAPI = 9,
+	XFS_PROBE_IOOPS = 10,
+	XFS_PROBE_QUOTA = 11,
 	XFS_STATS_CLEAR = 12,
 	XFS_INHERIT_SYNC = 13,
 	XFS_INHERIT_NODUMP = 14,
diff -uNrp linux-2.6.5/fs/xfs/linux/xfs_version.h linux-2.6.5.xfs/fs/xfs/linux/xfs_version.h
--- linux-2.6.5/fs/xfs/linux/xfs_version.h	2004-04-04 05:38:18.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/linux/xfs_version.h	2004-04-05 00:58:26.000000000 +0200
@@ -39,6 +39,6 @@
 #ifndef __XFS_VERSION_H__
 #define __XFS_VERSION_H__
 
-#define XFS_VERSION_STRING "SGI XFS"
+#define XFS_VERSION_STRING "SGI-XFS CVS-2004-03-08_06:00_UTC"
 
 #endif /* __XFS_VERSION_H__ */
diff -uNrp linux-2.6.5/fs/xfs/linux/xfs_vfs.c linux-2.6.5.xfs/fs/xfs/linux/xfs_vfs.c
--- linux-2.6.5/fs/xfs/linux/xfs_vfs.c	2004-04-04 05:36:13.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/linux/xfs_vfs.c	2004-04-05 00:58:26.000000000 +0200
@@ -200,6 +200,19 @@ vfs_quotactl(
 	return ((*bhvtovfsops(next)->vfs_quotactl)(next, cmd, id, addr));
 }
 
+struct inode *
+vfs_get_inode(
+	struct bhv_desc		*bdp,
+	xfs_ino_t		ino,
+	int			fl)
+{
+	struct bhv_desc		*next = bdp;
+
+	while (! (bhvtovfsops(next))->vfs_get_inode)
+		next = BHV_NEXTNULL(next);
+	return ((*bhvtovfsops(next)->vfs_get_inode)(next, ino, fl));
+}
+
 void
 vfs_init_vnode(
 	struct bhv_desc		*bdp,
@@ -280,10 +293,14 @@ bhv_remove_vfsops(
 	struct bhv_desc		*bhv;
 
 	bhv = bhv_lookup_range(&vfsp->vfs_bh, pos, pos);
-	if (!bhv)
-		return;
-	bhv_remove(&vfsp->vfs_bh, bhv);
-	kmem_free(bhv, sizeof(*bhv));
+	if (bhv) {
+		struct bhv_module	*bm;
+
+		bm = (bhv_module_t *) BHV_PDATA(bhv);
+		bhv_remove(&vfsp->vfs_bh, bhv);
+		bhv_remove_module(bm->bm_name);
+		kmem_free(bhv, sizeof(*bhv));
+	}
 }
 
 void
@@ -295,6 +312,7 @@ bhv_remove_all_vfsops(
 
 	bhv_remove_vfsops(vfsp, VFS_POSITION_QM);
 	bhv_remove_vfsops(vfsp, VFS_POSITION_DM);
+	bhv_remove_vfsops(vfsp, VFS_POSITION_IO);
 	if (!freebase)
 		return;
 	mp = XFS_BHVTOM(bhv_lookup(VFS_BHVHEAD(vfsp), &xfs_vfsops));
@@ -302,6 +320,25 @@ bhv_remove_all_vfsops(
 	xfs_mount_free(mp, 0);
 }
 
+STATIC void
+bhv_get_vfsops(
+	struct vfs		*vfsp,
+	const char		*name,
+	const char		*module)
+{
+	struct bhv_vfsops	*ops;
+
+	ops = (struct bhv_vfsops *) bhv_lookup_module(name, module);
+	if (ops) {
+		struct bhv_module	*bm;
+
+		bm = kmem_alloc(sizeof(struct bhv_module), KM_SLEEP);
+		bm->bm_name = name;
+		bhv_desc_init(&bm->bm_desc, bm, vfsp, ops);
+		bhv_insert(&vfsp->vfs_bh, &bm->bm_desc);
+	}
+}
+
 void
 bhv_insert_all_vfsops(
 	struct vfs		*vfsp)
@@ -310,6 +347,10 @@ bhv_insert_all_vfsops(
 
 	mp = xfs_mount_init();
 	vfs_insertbhv(vfsp, &mp->m_bhv, &xfs_vfsops, mp);
-	vfs_insertdmapi(vfsp);
-	vfs_insertquota(vfsp);
+	bhv_get_vfsops(vfsp, XFS_DMOPS,
+		xfs_probe_dmapi ? XFS_DM_MODULE : NULL);
+	bhv_get_vfsops(vfsp, XFS_QMOPS,
+		xfs_probe_quota ? XFS_QM_MODULE : NULL);
+	bhv_get_vfsops(vfsp, XFS_IOOPS,
+		xfs_probe_ioops ? XFS_IO_MODULE : NULL);
 }
diff -uNrp linux-2.6.5/fs/xfs/linux/xfs_vfs.h linux-2.6.5.xfs/fs/xfs/linux/xfs_vfs.h
--- linux-2.6.5/fs/xfs/linux/xfs_vfs.h	2004-04-04 05:36:14.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/linux/xfs_vfs.h	2004-04-05 00:58:26.000000000 +0200
@@ -95,6 +95,8 @@ typedef enum {
 #define SYNC_REFCACHE		0x0040  /* prune some of the nfs ref cache */
 #define SYNC_REMOUNT		0x0080  /* remount readonly, no dummy LRs */
 
+#define IGET_NOALLOC		0x0001	/* vfs_get_inode may return NULL */
+
 typedef int	(*vfs_mount_t)(bhv_desc_t *,
 				struct xfs_mount_args *, struct cred *);
 typedef int	(*vfs_parseargs_t)(bhv_desc_t *, char *,
@@ -112,6 +114,7 @@ typedef int	(*vfs_quotactl_t)(bhv_desc_t
 typedef void	(*vfs_init_vnode_t)(bhv_desc_t *,
 				struct vnode *, bhv_desc_t *, int);
 typedef void	(*vfs_force_shutdown_t)(bhv_desc_t *, int, char *, int);
+typedef	struct inode * (*vfs_get_inode_t)(bhv_desc_t *, xfs_ino_t, int);
 
 typedef struct vfsops {
 	bhv_position_t		vf_position;	/* behavior chain position */
@@ -126,6 +129,7 @@ typedef struct vfsops {
 	vfs_vget_t		vfs_vget;	/* get vnode from fid */
 	vfs_dmapiops_t		vfs_dmapiops;	/* data migration */
 	vfs_quotactl_t		vfs_quotactl;	/* disk quota */
+	vfs_get_inode_t		vfs_get_inode;	/* bhv specific iget */
 	vfs_init_vnode_t	vfs_init_vnode;	/* initialize a new vnode */
 	vfs_force_shutdown_t	vfs_force_shutdown;	/* crash and burn */
 } vfsops_t;
@@ -145,6 +149,7 @@ typedef struct vfsops {
 #define VFS_VGET(v, vpp,fidp, rv)	((rv) = vfs_vget(VHEAD(v), vpp,fidp))
 #define VFS_DMAPIOPS(v, p, rv)		((rv) = vfs_dmapiops(VHEAD(v), p))
 #define VFS_QUOTACTL(v, c,id,p, rv)	((rv) = vfs_quotactl(VHEAD(v), c,id,p))
+#define VFS_GET_INODE(v, ino, fl)	( vfs_get_inode(VHEAD(v), ino,fl) )
 #define VFS_INIT_VNODE(v, vp,b,ul)	( vfs_init_vnode(VHEAD(v), vp,b,ul) )
 #define VFS_FORCE_SHUTDOWN(v, fl,f,l)	( vfs_force_shutdown(VHEAD(v), fl,f,l) )
 
@@ -162,6 +167,7 @@ typedef struct vfsops {
 #define PVFS_VGET(b, vpp,fidp, rv)	((rv) = vfs_vget(b, vpp,fidp))
 #define PVFS_DMAPIOPS(b, p, rv)		((rv) = vfs_dmapiops(b, p))
 #define PVFS_QUOTACTL(b, c,id,p, rv)	((rv) = vfs_quotactl(b, c,id,p))
+#define PVFS_GET_INODE(b, ino,fl)	( vfs_get_inode(b, ino,fl) )
 #define PVFS_INIT_VNODE(b, vp,b2,ul)	( vfs_init_vnode(b, vp,b2,ul) )
 #define PVFS_FORCE_SHUTDOWN(b, fl,f,l)	( vfs_force_shutdown(b, fl,f,l) )
 
@@ -176,14 +182,28 @@ extern int vfs_sync(bhv_desc_t *, int, s
 extern int vfs_vget(bhv_desc_t *, struct vnode **, struct fid *);
 extern int vfs_dmapiops(bhv_desc_t *, caddr_t);
 extern int vfs_quotactl(bhv_desc_t *, int, int, caddr_t);
+extern struct inode *vfs_get_inode(bhv_desc_t *, xfs_ino_t, int);
 extern void vfs_init_vnode(bhv_desc_t *, struct vnode *, bhv_desc_t *, int);
 extern void vfs_force_shutdown(bhv_desc_t *, int, char *, int);
 
+#define XFS_DMOPS		"xfs_dm_operations"	/* Data Migration */
+#define XFS_QMOPS		"xfs_qm_operations"	/* Quota Manager  */
+#define XFS_IOOPS		"xfs_io_operations"	/* I/O subsystem  */
+#define XFS_DM_MODULE		"xfs_dmapi"
+#define XFS_QM_MODULE		"xfs_quota"
+#define XFS_IO_MODULE		"xfs_ioops"
+
 typedef struct bhv_vfsops {
 	struct vfsops		bhv_common;
 	void *			bhv_custom;
 } bhv_vfsops_t;
 
+typedef struct bhv_module {
+	bhv_desc_t		bm_desc;
+	const char *		bm_name;
+	bhv_vfsops_t *		bm_ops;
+} bhv_module_t;
+
 #define vfs_bhv_lookup(v, id)	( bhv_lookup_range(&(v)->vfs_bh, (id), (id)) )
 #define vfs_bhv_custom(b)	( ((bhv_vfsops_t *)BHV_OPS(b))->bhv_custom )
 #define vfs_bhv_set_custom(b,o)	( (b)->bhv_custom = (void *)(o))
@@ -194,6 +214,13 @@ extern void vfs_deallocate(vfs_t *);
 extern void vfs_insertops(vfs_t *, bhv_vfsops_t *);
 extern void vfs_insertbhv(vfs_t *, bhv_desc_t *, vfsops_t *, void *);
 
+#define bhv_lookup_module(n,m)	( (m) ? \
+				inter_module_get_request(n, m) : \
+				inter_module_get(n) )
+#define bhv_remove_module(n)	inter_module_put(n)
+#define bhv_module_init(n,m,op)	inter_module_register(n,m,op)
+#define bhv_module_exit(n)	inter_module_unregister(n)
+
 extern void bhv_insert_all_vfsops(struct vfs *);
 extern void bhv_remove_all_vfsops(struct vfs *, int);
 extern void bhv_remove_vfsops(struct vfs *, int);
diff -uNrp linux-2.6.5/fs/xfs/linux/xfs_vnode.c linux-2.6.5.xfs/fs/xfs/linux/xfs_vnode.c
--- linux-2.6.5/fs/xfs/linux/xfs_vnode.c	2004-04-04 05:36:55.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/linux/xfs_vnode.c	2004-04-05 00:58:26.000000000 +0200
@@ -178,7 +178,7 @@ vn_get(
 	if (inode->i_state & I_FREEING)
 		return NULL;
 
-	inode = ilookup(vmap->v_vfsp->vfs_super, vmap->v_ino);
+	inode = VFS_GET_INODE(vmap->v_vfsp, vmap->v_ino, IGET_NOALLOC);
 	if (!inode)	/* Inode not present */
 		return NULL;
 
diff -uNrp linux-2.6.5/fs/xfs/linux/xfs_vnode.h linux-2.6.5.xfs/fs/xfs/linux/xfs_vnode.h
--- linux-2.6.5/fs/xfs/linux/xfs_vnode.h	2004-04-04 05:38:25.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/linux/xfs_vnode.h	2004-04-05 00:58:26.000000000 +0200
@@ -225,6 +225,8 @@ typedef int	(*vop_fid2_t)(bhv_desc_t *, 
 typedef int	(*vop_release_t)(bhv_desc_t *);
 typedef int	(*vop_rwlock_t)(bhv_desc_t *, vrwlock_t);
 typedef void	(*vop_rwunlock_t)(bhv_desc_t *, vrwlock_t);
+typedef	int	(*vop_frlock_t)(bhv_desc_t *, int, struct file_lock *,int,
+				xfs_off_t, struct cred *);
 typedef int	(*vop_bmap_t)(bhv_desc_t *, xfs_off_t, ssize_t, int,
 				struct xfs_iomap *, int *);
 typedef int	(*vop_reclaim_t)(bhv_desc_t *);
@@ -269,6 +271,7 @@ typedef struct vnodeops {
 	vop_fid2_t		vop_fid2;
 	vop_rwlock_t		vop_rwlock;
 	vop_rwunlock_t		vop_rwunlock;
+	vop_frlock_t		vop_frlock;
 	vop_bmap_t		vop_bmap;
 	vop_reclaim_t		vop_reclaim;
 	vop_attr_get_t		vop_attr_get;
@@ -381,6 +384,7 @@ typedef struct vnodeops {
  */
 #define IO_ISDIRECT	0x00004		/* bypass page cache */
 #define IO_INVIS	0x00020		/* don't update inode timestamps */
+#define IO_ISLOCKED	0x00800		/* don't do inode locking */
 
 /*
  * Flags for VOP_IFLUSH call
diff -uNrp linux-2.6.5/fs/xfs/quota/Makefile linux-2.6.5.xfs/fs/xfs/quota/Makefile
--- linux-2.6.5/fs/xfs/quota/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/quota/Makefile	2004-04-05 00:58:26.000000000 +0200
@@ -0,0 +1,53 @@
+#
+# Copyright (c) 2000-2003 Silicon Graphics, Inc.  All Rights Reserved.
+#
+# This program is free software; you can redistribute it and/or modify it
+# under the terms of version 2 of the GNU General Public License as
+# published by the Free Software Foundation.
+#
+# This program is distributed in the hope that it would be useful, but
+# WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+#
+# Further, this software is distributed without any warranty that it is
+# free of the rightful claim of any third person regarding infringement
+# or the like.	Any license provided herein, whether implied or
+# otherwise, applies only to this software file.  Patent licenses, if
+# any, provided herein do not apply to combinations of this program with
+# other software, or any other product whatsoever.
+#
+# You should have received a copy of the GNU General Public License along
+# with this program; if not, write the Free Software Foundation, Inc., 59
+# Temple Place - Suite 330, Boston MA 02111-1307, USA.
+#
+# Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+# Mountain View, CA  94043, or:
+#
+# http://www.sgi.com
+#
+# For further information regarding this notice, see:
+#
+# http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+#
+
+EXTRA_CFLAGS += -I $(TOPDIR)/fs/xfs -I $(TOPDIR)/fs/xfs/linux-2.6
+
+ifeq ($(CONFIG_XFS_DEBUG),y)
+	EXTRA_CFLAGS += -g -DDEBUG -DXFSDEBUG
+	#EXTRA_CFLAGS += -DQUOTADEBUG
+endif
+ifeq ($(CONFIG_XFS_TRACE),y)
+	EXTRA_CFLAGS += -DXFS_DQUOT_TRACE
+endif
+
+obj-$(CONFIG_XFS_QUOTA)		+= xfs_quota.o
+
+xfs_quota-y			+= xfs_dquot.o \
+				   xfs_dquot_item.o \
+				   xfs_trans_dquot.o \
+				   xfs_qm_syscalls.o \
+				   xfs_qm_bhv.o \
+				   xfs_qm_ksyms.o \
+				   xfs_qm.o
+
+xfs_quota-$(CONFIG_PROC_FS)	+= xfs_qm_stats.o
diff -uNrp linux-2.6.5/fs/xfs/quota/xfs_qm_bhv.c linux-2.6.5.xfs/fs/xfs/quota/xfs_qm_bhv.c
--- linux-2.6.5/fs/xfs/quota/xfs_qm_bhv.c	2004-04-04 05:36:14.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/quota/xfs_qm_bhv.c	2004-04-05 00:58:26.000000000 +0200
@@ -388,7 +388,7 @@ struct bhv_vfsops xfs_qmops = { {
 };
 
 
-void __init
+static int __init
 xfs_qm_init(void)
 {
 	static char	message[] __initdata =
@@ -397,16 +397,22 @@ xfs_qm_init(void)
 	printk(message);
 	mutex_init(&xfs_Gqm_lock, MUTEX_DEFAULT, "xfs_qmlock");
 	vfs_bhv_set_custom(&xfs_qmops, &xfs_qmcore_xfs);
+	bhv_module_init(XFS_QMOPS, THIS_MODULE, &xfs_qmops);
 	xfs_qm_init_procfs();
+	return 0;
 }
 
-void __exit
+static void __exit
 xfs_qm_exit(void)
 {
-	vfs_bhv_clr_custom(&xfs_qmops);
 	xfs_qm_cleanup_procfs();
+	bhv_module_exit(XFS_QMOPS);
+	vfs_bhv_clr_custom(&xfs_qmops);
 	if (qm_dqzone)
 		kmem_cache_destroy(qm_dqzone);
 	if (qm_dqtrxzone)
 		kmem_cache_destroy(qm_dqtrxzone);
 }
+
+module_init(xfs_qm_init);
+module_exit(xfs_qm_exit);
diff -uNrp linux-2.6.5/fs/xfs/quota/xfs_qm_ksyms.c linux-2.6.5.xfs/fs/xfs/quota/xfs_qm_ksyms.c
--- linux-2.6.5/fs/xfs/quota/xfs_qm_ksyms.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/quota/xfs_qm_ksyms.c	2004-04-05 00:58:26.000000000 +0200
@@ -0,0 +1,66 @@
+/*
+ * Copyright (c) 2004 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.	 Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+
+#include "xfs.h"
+#include "xfs_fs.h"
+#include "xfs_inum.h"
+#include "xfs_log.h"
+#include "xfs_trans.h"
+#include "xfs_sb.h"
+#include "xfs_ag.h"
+#include "xfs_dir.h"
+#include "xfs_dir2.h"
+#include "xfs_alloc.h"
+#include "xfs_dmapi.h"
+#include "xfs_quota.h"
+#include "xfs_mount.h"
+#include "xfs_alloc_btree.h"
+#include "xfs_bmap_btree.h"
+#include "xfs_ialloc_btree.h"
+#include "xfs_btree.h"
+#include "xfs_ialloc.h"
+#include "xfs_attr_sf.h"
+#include "xfs_dir_sf.h"
+#include "xfs_dir2_sf.h"
+#include "xfs_dinode.h"
+#include "xfs_inode.h"
+#include "xfs_qm.h"
+
+EXPORT_SYMBOL(xfs_Gqm);
+EXPORT_SYMBOL(xfs_qm_dqattach);
+EXPORT_SYMBOL(xfs_qm_dqpurge_all);
+EXPORT_SYMBOL(xfs_qm_dqrele_all_inodes);
+EXPORT_SYMBOL(xfs_trans_reserve_quota_bydquots);
+
+MODULE_AUTHOR("Silicon Graphics, Inc.");
+MODULE_DESCRIPTION("SGI XFS quota management subsystem");
+MODULE_LICENSE("GPL");
diff -uNrp linux-2.6.5/fs/xfs/support/Makefile linux-2.6.5.xfs/fs/xfs/support/Makefile
--- linux-2.6.5/fs/xfs/support/Makefile	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/support/Makefile	2004-04-05 00:58:26.000000000 +0200
@@ -0,0 +1,51 @@
+#
+# Copyright (c) 2000-2002 Silicon Graphics, Inc.  All Rights Reserved.
+#
+# This program is free software; you can redistribute it and/or modify it
+# under the terms of version 2 of the GNU General Public License as
+# published by the Free Software Foundation.
+#
+# This program is distributed in the hope that it would be useful, but
+# WITHOUT ANY WARRANTY; without even the implied warranty of
+# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+#
+# Further, this software is distributed without any warranty that it is
+# free of the rightful claim of any third person regarding infringement
+# or the like.  Any license provided herein, whether implied or
+# otherwise, applies only to this software file.  Patent licenses, if
+# any, provided herein do not apply to combinations of this program with
+# other software, or any other product whatsoever.
+#
+# You should have received a copy of the GNU General Public License along
+# with this program; if not, write the Free Software Foundation, Inc., 59
+# Temple Place - Suite 330, Boston MA 02111-1307, USA.
+#
+# Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+# Mountain View, CA  94043, or:
+#
+# http://www.sgi.com
+#
+# For further information regarding this notice, see:
+#
+# http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+#
+
+EXTRA_CFLAGS += -I.. -I $(TOPDIR)/fs/xfs/linux-2.4
+
+ifeq ($(CONFIG_XFS_DEBUG),y)
+	EXTRA_CFLAGS += -DDEBUG
+endif
+
+O_TARGET			:= support_xfs.o
+ifneq ($(MAKECMDGOALS),modules_install)
+  obj-m				:= $(O_TARGET)
+endif
+
+obj-y				:= debug.o \
+				   move.o \
+				   qsort.o \
+				   uuid.o
+
+obj-$(CONFIG_XFS_TRACE)		+= ktrace.o
+
+include $(TOPDIR)/Rules.make
diff -uNrp linux-2.6.5/fs/xfs/xfs.h linux-2.6.5.xfs/fs/xfs/xfs.h
--- linux-2.6.5/fs/xfs/xfs.h	2004-04-04 05:36:26.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/xfs.h	2004-04-05 00:58:26.000000000 +0200
@@ -32,7 +32,12 @@
 #ifndef __XFS_H__
 #define __XFS_H__
 
-#include <linux/xfs_linux.h>
+#include <linux/version.h>
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(2,6,0)
+#include <linux-2.6/xfs_linux.h>
+#else
+#include <linux-2.4/xfs_linux.h>
+#endif
 
 #include <xfs_fs.h> 
 
diff -uNrp linux-2.6.5/fs/xfs/xfs_acl.c linux-2.6.5.xfs/fs/xfs/xfs_acl.c
--- linux-2.6.5/fs/xfs/xfs_acl.c	2004-04-04 05:36:56.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/xfs_acl.c	2004-04-05 00:58:26.000000000 +0200
@@ -572,6 +572,7 @@ xfs_acl_access(
 
 	return xfs_acl_capability_check(md, cr, fap);
 }
+EXPORT_SYMBOL(xfs_acl_access);
 
 /*
  * ACL validity checker.
diff -uNrp linux-2.6.5/fs/xfs/xfs_iget.c linux-2.6.5.xfs/fs/xfs/xfs_iget.c
--- linux-2.6.5/fs/xfs/xfs_iget.c	2004-04-04 05:38:21.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/xfs_iget.c	2004-04-05 00:58:26.000000000 +0200
@@ -445,7 +445,7 @@ xfs_iget(
 retry:
 	XFS_STATS_INC(xs_ig_attempts);
 
-	if ((inode = iget_locked(XFS_MTOVFS(mp)->vfs_super, ino))) {
+	if ((inode = VFS_GET_INODE(XFS_MTOVFS(mp), ino, 0))) {
 		bhv_desc_t	*bdp;
 		xfs_inode_t	*ip;
 		int		newnode;
@@ -457,6 +457,7 @@ inode_allocate:
 			error = xfs_iget_core(vp, mp, tp, ino,
 						lock_flags, ipp, bno);
 			if (error) {
+				remove_inode_hash(inode);
 				make_bad_inode(inode);
 				if (inode->i_state & I_NEW)
 					unlock_new_inode(inode);
@@ -473,6 +474,11 @@ inode_allocate:
 				goto retry;
 			}
 
+			if (is_bad_inode(inode)) {
+				iput(inode);
+				return EIO;
+			}
+
 			bdp = vn_bhv_lookup(VN_BHV_HEAD(vp), &xfs_vnodeops);
 			if (bdp == NULL) {
 				XFS_STATS_INC(xs_ig_dup);
@@ -578,6 +584,7 @@ xfs_iput_new(xfs_inode_t	*ip,
 
 	/* We shouldn't get here without this being true, but just in case */
 	if (inode->i_state & I_NEW) {
+		remove_inode_hash(inode);
 		make_bad_inode(inode);
 		unlock_new_inode(inode);
 	}
diff -uNrp linux-2.6.5/fs/xfs/xfs_inode.c linux-2.6.5.xfs/fs/xfs/xfs_inode.c
--- linux-2.6.5/fs/xfs/xfs_inode.c	2004-04-04 05:38:20.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/xfs_inode.c	2004-04-05 01:12:06.000000000 +0200
@@ -3707,8 +3707,7 @@ xfs_iaccess(
 	 * Read/write DACs are always overridable.
 	 * Executable DACs are overridable if at least one exec bit is set.
 	 */
-	if (!(orgmode & S_IXUSR) ||
-	    (inode->i_mode & S_IXUGO) || S_ISDIR(inode->i_mode))
+	if ((orgmode & (S_IRUSR|S_IWUSR)) || (inode->i_mode & S_IXUGO))
 		if (capable_cred(cr, CAP_DAC_OVERRIDE))
 			return 0;
 
diff -uNrp linux-2.6.5/fs/xfs/xfs_refcache.c linux-2.6.5.xfs/fs/xfs/xfs_refcache.c
--- linux-2.6.5/fs/xfs/xfs_refcache.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/xfs_refcache.c	2004-04-05 00:58:29.000000000 +0200
@@ -0,0 +1,439 @@
+/*
+ * Copyright (c) 2000-2003 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.  Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+
+#include "xfs.h"
+#include "xfs_macros.h"
+#include "xfs_types.h"
+#include "xfs_inum.h"
+#include "xfs_log.h"
+#include "xfs_trans.h"
+#include "xfs_sb.h"
+#include "xfs_ag.h"
+#include "xfs_dir.h"
+#include "xfs_dir2.h"
+#include "xfs_dmapi.h"
+#include "xfs_mount.h"
+#include "xfs_alloc_btree.h"
+#include "xfs_bmap_btree.h"
+#include "xfs_ialloc_btree.h"
+#include "xfs_itable.h"
+#include "xfs_btree.h"
+#include "xfs_alloc.h"
+#include "xfs_ialloc.h"
+#include "xfs_attr.h"
+#include "xfs_attr_sf.h"
+#include "xfs_dir_sf.h"
+#include "xfs_dir2_sf.h"
+#include "xfs_dinode.h"
+#include "xfs_inode_item.h"
+#include "xfs_inode.h"
+#include "xfs_bmap.h"
+#include "xfs_error.h"
+#include "xfs_buf_item.h"
+#include "xfs_refcache.h"
+
+STATIC spinlock_t	xfs_refcache_lock = SPIN_LOCK_UNLOCKED;
+STATIC xfs_inode_t	**xfs_refcache;
+STATIC int		xfs_refcache_index;
+STATIC int		xfs_refcache_busy;
+STATIC int		xfs_refcache_count;
+
+/*
+ * Insert the given inode into the reference cache.
+ */
+void
+xfs_refcache_insert(
+	xfs_inode_t	*ip)
+{
+	vnode_t		*vp;
+	xfs_inode_t	*release_ip;
+	xfs_inode_t	**refcache;
+
+	ASSERT(ismrlocked(&(ip->i_iolock), MR_UPDATE));
+
+	/*
+	 * If an unmount is busy blowing entries out of the cache,
+	 * then don't bother.
+	 */
+	if (xfs_refcache_busy) {
+		return;
+	}
+
+	/*
+	 * If we tuned the refcache down to zero, don't do anything.
+	 */
+	 if (!xfs_refcache_size) {
+		return;
+	}
+
+	/*
+	 * The inode is already in the refcache, so don't bother
+	 * with it.
+	 */
+	if (ip->i_refcache != NULL) {
+		return;
+	}
+
+	vp = XFS_ITOV(ip);
+	/* ASSERT(vp->v_count > 0); */
+	VN_HOLD(vp);
+
+	/*
+	 * We allocate the reference cache on use so that we don't
+	 * waste the memory on systems not being used as NFS servers.
+	 */
+	if (xfs_refcache == NULL) {
+		refcache = (xfs_inode_t **)kmem_zalloc(XFS_REFCACHE_SIZE_MAX *
+						       sizeof(xfs_inode_t *),
+						       KM_SLEEP);
+	} else {
+		refcache = NULL;
+	}
+
+	spin_lock(&xfs_refcache_lock);
+
+	/*
+	 * If we allocated memory for the refcache above and it still
+	 * needs it, then use the memory we allocated.  Otherwise we'll
+	 * free the memory below.
+	 */
+	if (refcache != NULL) {
+		if (xfs_refcache == NULL) {
+			xfs_refcache = refcache;
+			refcache = NULL;
+		}
+	}
+
+	/*
+	 * If an unmount is busy clearing out the cache, don't add new
+	 * entries to it.
+	 */
+	if (xfs_refcache_busy) {
+		spin_unlock(&xfs_refcache_lock);
+		VN_RELE(vp);
+		/*
+		 * If we allocated memory for the refcache above but someone
+		 * else beat us to using it, then free the memory now.
+		 */
+		if (refcache != NULL) {
+			kmem_free(refcache,
+				  XFS_REFCACHE_SIZE_MAX * sizeof(xfs_inode_t *));
+		}
+		return;
+	}
+	release_ip = xfs_refcache[xfs_refcache_index];
+	if (release_ip != NULL) {
+		release_ip->i_refcache = NULL;
+		xfs_refcache_count--;
+		ASSERT(xfs_refcache_count >= 0);
+	}
+	xfs_refcache[xfs_refcache_index] = ip;
+	ASSERT(ip->i_refcache == NULL);
+	ip->i_refcache = &(xfs_refcache[xfs_refcache_index]);
+	xfs_refcache_count++;
+	ASSERT(xfs_refcache_count <= xfs_refcache_size);
+	xfs_refcache_index++;
+	if (xfs_refcache_index == xfs_refcache_size) {
+		xfs_refcache_index = 0;
+	}
+	spin_unlock(&xfs_refcache_lock);
+
+	/*
+	 * Save the pointer to the inode to be released so that we can
+	 * VN_RELE it once we've dropped our inode locks in xfs_rwunlock().
+	 * The pointer may be NULL, but that's OK.
+	 */
+	ip->i_release = release_ip;
+
+	/*
+	 * If we allocated memory for the refcache above but someone
+	 * else beat us to using it, then free the memory now.
+	 */
+	if (refcache != NULL) {
+		kmem_free(refcache,
+			  XFS_REFCACHE_SIZE_MAX * sizeof(xfs_inode_t *));
+	}
+}
+
+
+/*
+ * If the given inode is in the reference cache, purge its entry and
+ * release the reference on the vnode.
+ */
+void
+xfs_refcache_purge_ip(
+	xfs_inode_t	*ip)
+{
+	vnode_t	*vp;
+	int	error;
+
+	/*
+	 * If we're not pointing to our entry in the cache, then
+	 * we must not be in the cache.
+	 */
+	if (ip->i_refcache == NULL) {
+		return;
+	}
+
+	spin_lock(&xfs_refcache_lock);
+	if (ip->i_refcache == NULL) {
+		spin_unlock(&xfs_refcache_lock);
+		return;
+	}
+
+	/*
+	 * Clear both our pointer to the cache entry and its pointer
+	 * back to us.
+	 */
+	ASSERT(*(ip->i_refcache) == ip);
+	*(ip->i_refcache) = NULL;
+	ip->i_refcache = NULL;
+	xfs_refcache_count--;
+	ASSERT(xfs_refcache_count >= 0);
+	spin_unlock(&xfs_refcache_lock);
+
+	vp = XFS_ITOV(ip);
+	/* ASSERT(vp->v_count > 1); */
+	VOP_RELEASE(vp, error);
+	VN_RELE(vp);
+}
+
+
+/*
+ * This is called from the XFS unmount code to purge all entries for the
+ * given mount from the cache.  It uses the refcache busy counter to
+ * make sure that new entries are not added to the cache as we purge them.
+ */
+void
+xfs_refcache_purge_mp(
+	xfs_mount_t	*mp)
+{
+	vnode_t		*vp;
+	int		error, i;
+	xfs_inode_t	*ip;
+
+	if (xfs_refcache == NULL) {
+		return;
+	}
+
+	spin_lock(&xfs_refcache_lock);
+	/*
+	 * Bumping the busy counter keeps new entries from being added
+	 * to the cache.  We use a counter since multiple unmounts could
+	 * be in here simultaneously.
+	 */
+	xfs_refcache_busy++;
+
+	for (i = 0; i < xfs_refcache_size; i++) {
+		ip = xfs_refcache[i];
+		if ((ip != NULL) && (ip->i_mount == mp)) {
+			xfs_refcache[i] = NULL;
+			ip->i_refcache = NULL;
+			xfs_refcache_count--;
+			ASSERT(xfs_refcache_count >= 0);
+			spin_unlock(&xfs_refcache_lock);
+			vp = XFS_ITOV(ip);
+			VOP_RELEASE(vp, error);
+			VN_RELE(vp);
+			spin_lock(&xfs_refcache_lock);
+		}
+	}
+
+	xfs_refcache_busy--;
+	ASSERT(xfs_refcache_busy >= 0);
+	spin_unlock(&xfs_refcache_lock);
+}
+
+
+/*
+ * This is called from the XFS sync code to ensure that the refcache
+ * is emptied out over time.  We purge a small number of entries with
+ * each call.
+ */
+void
+xfs_refcache_purge_some(xfs_mount_t *mp)
+{
+	int		error, i;
+	xfs_inode_t	*ip;
+	int		iplist_index;
+	xfs_inode_t	**iplist;
+
+	if ((xfs_refcache == NULL) || (xfs_refcache_count == 0)) {
+		return;
+	}
+
+	iplist_index = 0;
+	iplist = (xfs_inode_t **)kmem_zalloc(xfs_refcache_purge_count *
+					  sizeof(xfs_inode_t *), KM_SLEEP);
+
+	spin_lock(&xfs_refcache_lock);
+
+	/*
+	 * Store any inodes we find in the next several entries
+	 * into the iplist array to be released after dropping
+	 * the spinlock.  We always start looking from the currently
+	 * oldest place in the cache.  We move the refcache index
+	 * forward as we go so that we are sure to eventually clear
+	 * out the entire cache when the system goes idle.
+	 */
+	for (i = 0; i < xfs_refcache_purge_count; i++) {
+		ip = xfs_refcache[xfs_refcache_index];
+		if (ip != NULL) {
+			xfs_refcache[xfs_refcache_index] = NULL;
+			ip->i_refcache = NULL;
+			xfs_refcache_count--;
+			ASSERT(xfs_refcache_count >= 0);
+			iplist[iplist_index] = ip;
+			iplist_index++;
+		}
+		xfs_refcache_index++;
+		if (xfs_refcache_index == xfs_refcache_size) {
+			xfs_refcache_index = 0;
+		}
+	}
+
+	spin_unlock(&xfs_refcache_lock);
+
+	/*
+	 * Now drop the inodes we collected.
+	 */
+	for (i = 0; i < iplist_index; i++) {
+		VOP_RELEASE(XFS_ITOV(iplist[i]), error);
+		VN_RELE(XFS_ITOV(iplist[i]));
+	}
+
+	kmem_free(iplist, xfs_refcache_purge_count *
+			  sizeof(xfs_inode_t *));
+}
+
+/*
+ * This is called when the refcache is dynamically resized
+ * via a sysctl.
+ *
+ * If the new size is smaller than the old size, purge all
+ * entries in slots greater than the new size, and move
+ * the index if necessary.
+ *
+ * If the refcache hasn't even been allocated yet, or the
+ * new size is larger than the old size, just set the value
+ * of xfs_refcache_size.
+ */
+
+void
+xfs_refcache_resize(int xfs_refcache_new_size)
+{
+	int		i;
+	xfs_inode_t	*ip;
+	int		iplist_index = 0;
+	xfs_inode_t	**iplist;
+	int		error;
+
+	/*
+	 * If the new size is smaller than the current size,
+	 * purge entries to create smaller cache, and
+	 * reposition index if necessary.
+	 * Don't bother if no refcache yet.
+	 */
+	if (xfs_refcache && (xfs_refcache_new_size < xfs_refcache_size)) {
+
+		iplist = (xfs_inode_t **)kmem_zalloc(XFS_REFCACHE_SIZE_MAX *
+				sizeof(xfs_inode_t *), KM_SLEEP);
+
+		spin_lock(&xfs_refcache_lock);
+
+		for (i = xfs_refcache_new_size; i < xfs_refcache_size; i++) {
+			ip = xfs_refcache[i];
+			if (ip != NULL) {
+				xfs_refcache[i] = NULL;
+				ip->i_refcache = NULL;
+				xfs_refcache_count--;
+				ASSERT(xfs_refcache_count >= 0);
+				iplist[iplist_index] = ip;
+				iplist_index++;
+			}
+		}
+
+		xfs_refcache_size = xfs_refcache_new_size;
+
+		/*
+		 * Move index to beginning of cache if it's now past the end
+		 */
+		if (xfs_refcache_index >= xfs_refcache_new_size)
+			xfs_refcache_index = 0;
+
+		spin_unlock(&xfs_refcache_lock);
+
+		/*
+		 * Now drop the inodes we collected.
+		 */
+		for (i = 0; i < iplist_index; i++) {
+			VOP_RELEASE(XFS_ITOV(iplist[i]), error);
+			VN_RELE(XFS_ITOV(iplist[i]));
+		}
+
+		kmem_free(iplist, XFS_REFCACHE_SIZE_MAX *
+				  sizeof(xfs_inode_t *));
+	} else {
+		spin_lock(&xfs_refcache_lock);
+		xfs_refcache_size = xfs_refcache_new_size;
+		spin_unlock(&xfs_refcache_lock);
+	}
+}
+
+void
+xfs_refcache_iunlock(
+	xfs_inode_t	*ip,
+	uint		lock_flags)
+{
+	xfs_inode_t	*release_ip;
+	int		error;
+
+	release_ip = ip->i_release;
+	ip->i_release = NULL;
+
+	xfs_iunlock(ip, lock_flags);
+
+	if (release_ip != NULL) {
+		VOP_RELEASE(XFS_ITOV(release_ip), error);
+		VN_RELE(XFS_ITOV(release_ip));
+	}
+}
+
+void
+xfs_refcache_destroy(void)
+{
+	if (xfs_refcache) {
+		kmem_free(xfs_refcache,
+			XFS_REFCACHE_SIZE_MAX * sizeof(xfs_inode_t *));
+		xfs_refcache = NULL;
+	}
+}
diff -uNrp linux-2.6.5/fs/xfs/xfs_vfsops.c linux-2.6.5.xfs/fs/xfs/xfs_vfsops.c
--- linux-2.6.5/fs/xfs/xfs_vfsops.c	2004-04-04 05:36:26.000000000 +0200
+++ linux-2.6.5.xfs/fs/xfs/xfs_vfsops.c	2004-04-05 00:58:31.000000000 +0200
@@ -213,9 +213,9 @@ xfs_cleanup(void)
  */
 STATIC int
 xfs_start_flags(
+	struct vfs		*vfs,
 	struct xfs_mount_args	*ap,
-	struct xfs_mount	*mp,
-	int			ronly)
+	struct xfs_mount	*mp)
 {
 	/* Values are in BBs */
 	if ((ap->flags & XFSMNT_NOALIGN) != XFSMNT_NOALIGN) {
@@ -305,7 +305,7 @@ xfs_start_flags(
 	 * no recovery flag requires a read-only mount
 	 */
 	if (ap->flags & XFSMNT_NORECOVERY) {
-		if (!ronly) {
+		if (!(vfs->vfs_flag & VFS_RDONLY)) {
 			cmn_err(CE_WARN,
 	"XFS: tried to mount a FS read-write without recovery!");
 			return XFS_ERROR(EINVAL);
@@ -327,10 +327,12 @@ xfs_start_flags(
  */
 STATIC int
 xfs_finish_flags(
+	struct vfs		*vfs,
 	struct xfs_mount_args	*ap,
-	struct xfs_mount	*mp,
-	int			ronly)
+	struct xfs_mount	*mp)
 {
+	int			ronly = (vfs->vfs_flag & VFS_RDONLY);
+
 	/* Fail a mount where the logbuf is smaller then the log stripe */
 	if (XFS_SB_VERSION_HASLOGV2(&mp->m_sb)) {
 		if ((ap->logbufsize == -1) &&
@@ -420,7 +422,6 @@ xfs_mount(
 	struct bhv_desc		*p;
 	struct xfs_mount	*mp = XFS_BHVTOM(bhvp);
 	struct block_device	*ddev, *logdev, *rtdev;
-	int			ronly = (vfsp->vfs_flag & VFS_RDONLY);
 	int			flags = 0, error;
 
 	ddev = vfsp->vfs_super->s_bdev;
@@ -472,13 +473,13 @@ xfs_mount(
 	/*
 	 * Setup flags based on mount(2) options and then the superblock
 	 */
-	error = xfs_start_flags(args, mp, ronly);
+	error = xfs_start_flags(vfsp, args, mp);
 	if (error)
 		goto error;
 	error = xfs_readsb(mp);
 	if (error)
 		goto error;
-	error = xfs_finish_flags(args, mp, ronly);
+	error = xfs_finish_flags(vfsp, args, mp);
 	if (error) {
 		xfs_freesb(mp);
 		goto error;
@@ -636,8 +637,7 @@ xfs_mntupdate(
 		 */ 
 		do {
 			VFS_SYNC(vfsp, REMOUNT_READONLY_FLAGS, NULL, error);
-			pagebuf_delwri_flush(mp->m_ddev_targp, PBDF_WAIT,
-								&pincount);
+			pagebuf_delwri_flush(mp->m_ddev_targp, 1, &pincount);
 			if(0 == pincount) { delay(50); count++; }
 		} while (count < 2);
 
@@ -773,7 +773,6 @@ xfs_statvfs(
 	xfs_mount_t	*mp;
 	xfs_sb_t	*sbp;
 	unsigned long	s;
-	u64 id;
 
 	mp = XFS_BHVTOM(bdp);
 	sbp = &(mp->m_sb);
@@ -801,9 +800,8 @@ xfs_statvfs(
 	statp->f_ffree = statp->f_files - (sbp->sb_icount - sbp->sb_ifree);
 	XFS_SB_UNLOCK(mp, s);
 
-	id = huge_encode_dev(mp->m_dev);
-	statp->f_fsid.val[0] = (u32)id;
-	statp->f_fsid.val[1] = (u32)(id >> 32);
+	statp->f_fsid.val[0] = mp->m_dev;
+	statp->f_fsid.val[1] = 0;
 	statp->f_namelen = MAXNAMELEN - 1;
 
 	return 0;
@@ -1872,6 +1870,7 @@ vfsops_t xfs_vfsops = {
 	.vfs_vget		= xfs_vget,
 	.vfs_dmapiops		= (vfs_dmapiops_t)fs_nosys,
 	.vfs_quotactl		= (vfs_quotactl_t)fs_nosys,
+	.vfs_get_inode		= xfs_get_inode,
 	.vfs_init_vnode		= xfs_initialize_vnode,
 	.vfs_force_shutdown	= xfs_do_force_shutdown,
 };
diff -uNrp linux-2.6.5/fs/xfs/xfsidbg.c linux-2.6.5.xfs/fs/xfs/xfsidbg.c
--- linux-2.6.5/fs/xfs/xfsidbg.c	1970-01-01 01:00:00.000000000 +0100
+++ linux-2.6.5.xfs/fs/xfs/xfsidbg.c	2004-04-05 00:58:26.000000000 +0200
@@ -0,0 +1,7776 @@
+/*
+ * Copyright (c) 2000-2004 Silicon Graphics, Inc.  All Rights Reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms of version 2 of the GNU General Public License as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it would be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
+ *
+ * Further, this software is distributed without any warranty that it is
+ * free of the rightful claim of any third person regarding infringement
+ * or the like.  Any license provided herein, whether implied or
+ * otherwise, applies only to this software file.  Patent licenses, if
+ * any, provided herein do not apply to combinations of this program with
+ * other software, or any other product whatsoever.
+ *
+ * You should have received a copy of the GNU General Public License along
+ * with this program; if not, write the Free Software Foundation, Inc., 59
+ * Temple Place - Suite 330, Boston MA 02111-1307, USA.
+ *
+ * Contact information: Silicon Graphics, Inc., 1600 Amphitheatre Pkwy,
+ * Mountain View, CA  94043, or:
+ *
+ * http://www.sgi.com
+ *
+ * For further information regarding this notice, see:
+ *
+ * http://oss.sgi.com/projects/GenInfo/SGIGPLNoticeExplan/
+ */
+
+#include "xfs.h"
+
+#include <linux/ctype.h>
+#include <linux/kdb.h>
+#include <linux/kdbprivate.h>
+#include <linux/mm.h>
+#include <linux/init.h>
+#include <linux/version.h>
+
+#include "xfs_macros.h"
+#include "xfs_types.h"
+#include "xfs_inum.h"
+#include "xfs_log.h"
+#include "xfs_trans.h"
+#include "xfs_sb.h"
+#include "xfs_dir.h"
+#include "xfs_dir2.h"
+#include "xfs_dmapi.h"
+#include "xfs_mount.h"
+#include "xfs_alloc.h"
+#include "xfs_ag.h"
+#include "xfs_alloc_btree.h"
+#include "xfs_bmap_btree.h"
+#include "xfs_ialloc_btree.h"
+#include "xfs_btree.h"
+#include "xfs_buf_item.h"
+#include "xfs_extfree_item.h"
+#include "xfs_inode_item.h"
+#include "xfs_attr_sf.h"
+#include "xfs_dir_sf.h"
+#include "xfs_dir2_sf.h"
+#include "xfs_dir2_trace.h"
+#include "xfs_dinode.h"
+#include "xfs_rw.h"
+#include "xfs_bmap.h"
+#include "xfs_inode.h"
+#include "xfs_da_btree.h"
+#include "xfs_attr.h"
+#include "xfs_attr_leaf.h"
+#include "xfs_dir_leaf.h"
+#include "xfs_dir2_data.h"
+#include "xfs_dir2_leaf.h"
+#include "xfs_dir2_block.h"
+#include "xfs_dir2_node.h"
+#include "xfs_log_priv.h"
+#include "xfs_log_recover.h"
+#include "xfs_bit.h"
+#include "xfs_quota.h"
+#include "quota/xfs_qm.h"
+#include "xfs_iomap.h"
+
+MODULE_AUTHOR("Silicon Graphics, Inc.");
+MODULE_DESCRIPTION("Additional kdb commands for debugging XFS");
+MODULE_LICENSE("GPL");
+
+#define qprintf	kdb_printf
+
+/*
+ * Command table functions. (tracing)
+ */
+#ifdef XFS_ALLOC_TRACE
+static void	xfsidbg_xalatrace(int);
+static void	xfsidbg_xalbtrace(xfs_agblock_t);
+static void	xfsidbg_xalgtrace(xfs_agnumber_t);
+static void	xfsidbg_xalmtrace(xfs_mount_t *);
+static void	xfsidbg_xalttrace(int);
+#endif
+#ifdef XFS_ATTR_TRACE
+static void	xfsidbg_xattrtrace(int);
+#endif
+#ifdef XFS_BLI_TRACE
+static void	xfsidbg_xblitrace(xfs_buf_log_item_t *);
+#endif
+#ifdef XFS_BMAP_TRACE
+static void	xfsidbg_xbmatrace(int);
+static void	xfsidbg_xbmitrace(xfs_inode_t *);
+static void	xfsidbg_xbmstrace(xfs_inode_t *);
+static void	xfsidbg_xbxatrace(int);
+static void	xfsidbg_xbxitrace(xfs_inode_t *);
+static void	xfsidbg_xbxstrace(xfs_inode_t *);
+#endif
+#ifdef XFS_ILOCK_TRACE
+static void	xfsidbg_xilock_trace(xfs_inode_t *);
+static void	xfsidbg_xailock_trace(int);
+#endif
+#ifdef XFS_DIR_TRACE
+static void	xfsidbg_xdirtrace(int);
+#endif
+#ifdef XFS_DIR2_TRACE
+static void	xfsidbg_xdir2atrace(int);
+static void	xfsidbg_xdir2itrace(xfs_inode_t *);
+#endif
+#ifdef XFS_LOG_TRACE
+static void	xfsidbg_xiclogtrace(xlog_in_core_t *);
+static void	xfsidbg_xlog_granttrace(xlog_t *);
+#endif
+#ifdef XFS_DQUOT_TRACE
+static void	xfsidbg_xqm_dqtrace(xfs_dquot_t *);
+#endif
+#ifdef XFS_RW_TRACE
+static void	xfsidbg_xrwtrace(xfs_inode_t *);
+#endif
+
+
+/*
+ * Command table functions.
+ */
+static void	xfsidbg_xagf(xfs_agf_t *);
+static void	xfsidbg_xagi(xfs_agi_t *);
+static void	xfsidbg_xaildump(xfs_mount_t *);
+static void	xfsidbg_xalloc(xfs_alloc_arg_t *);
+static void	xfsidbg_xattrcontext(xfs_attr_list_context_t *);
+static void	xfsidbg_xattrleaf(xfs_attr_leafblock_t *);
+static void	xfsidbg_xattrsf(xfs_attr_shortform_t *);
+static void	xfsidbg_xbirec(xfs_bmbt_irec_t *r);
+static void	xfsidbg_xbmalla(xfs_bmalloca_t *);
+static void	xfsidbg_xbrec(xfs_bmbt_rec_64_t *);
+static void	xfsidbg_xbroot(xfs_inode_t *);
+static void	xfsidbg_xbroota(xfs_inode_t *);
+static void	xfsidbg_xbtcur(xfs_btree_cur_t *);
+static void	xfsidbg_xbuf(xfs_buf_t *);
+static void	xfsidbg_xbuf_real(xfs_buf_t *, int);
+static void	xfsidbg_xarg(int);
+static void	xfsidbg_xchksum(uint *);
+static void	xfsidbg_xchash(xfs_mount_t *mp);
+static void	xfsidbg_xchashlist(xfs_chashlist_t *chl);
+static void	xfsidbg_xdaargs(xfs_da_args_t *);
+static void	xfsidbg_xdabuf(xfs_dabuf_t *);
+static void	xfsidbg_xdanode(xfs_da_intnode_t *);
+static void	xfsidbg_xdastate(xfs_da_state_t *);
+static void	xfsidbg_xdirleaf(xfs_dir_leafblock_t *);
+static void	xfsidbg_xdirsf(xfs_dir_shortform_t *);
+static void	xfsidbg_xdir2free(xfs_dir2_free_t *);
+static void	xfsidbg_xdir2sf(xfs_dir2_sf_t *);
+static void	xfsidbg_xexlist(xfs_inode_t *);
+static void	xfsidbg_xflist(xfs_bmap_free_t *);
+static void	xfsidbg_xhelp(void);
+static void	xfsidbg_xiclog(xlog_in_core_t *);
+static void	xfsidbg_xiclogall(xlog_in_core_t *);
+static void	xfsidbg_xiclogcb(xlog_in_core_t *);
+static void	xfsidbg_xihash(xfs_mount_t *mp);
+static void	xfsidbg_xinodes(xfs_mount_t *);
+static void	xfsidbg_delayed_blocks(xfs_mount_t *);
+static void	xfsidbg_xinodes_quiesce(xfs_mount_t *);
+static void	xfsidbg_xlog(xlog_t *);
+static void	xfsidbg_xlog_ritem(xlog_recover_item_t *);
+static void	xfsidbg_xlog_rtrans(xlog_recover_t *);
+static void	xfsidbg_xlog_rtrans_entire(xlog_recover_t *);
+static void	xfsidbg_xlog_tic(xlog_ticket_t *);
+static void	xfsidbg_xlogitem(xfs_log_item_t *);
+static void	xfsidbg_xmount(xfs_mount_t *);
+static void	xfsidbg_xnode(xfs_inode_t *ip);
+static void	xfsidbg_xcore(xfs_iocore_t *io);
+static void	xfsidbg_xperag(xfs_mount_t *);
+static void	xfsidbg_xqm_diskdq(xfs_disk_dquot_t *);
+static void	xfsidbg_xqm_dqattached_inos(xfs_mount_t *);
+static void	xfsidbg_xqm_dquot(xfs_dquot_t *);
+static void	xfsidbg_xqm_mplist(xfs_mount_t *);
+static void	xfsidbg_xqm_qinfo(xfs_mount_t *mp);
+static void	xfsidbg_xqm_tpdqinfo(xfs_trans_t *tp);
+static void	xfsidbg_xsb(xfs_sb_t *, int convert);
+static void	xfsidbg_xtp(xfs_trans_t *);
+static void	xfsidbg_xtrans_res(xfs_mount_t *);
+#ifdef CONFIG_XFS_QUOTA
+static void	xfsidbg_xqm(void);
+static void	xfsidbg_xqm_htab(void);
+static void	xfsidbg_xqm_freelist_print(xfs_frlist_t *qlist, char *title);
+static void	xfsidbg_xqm_freelist(void);
+#endif
+
+#ifdef XFS_BMAP_TRACE
+static void	xfs_convert_extent(xfs_bmbt_rec_32_t *, xfs_dfiloff_t *,
+				xfs_dfsbno_t *, xfs_dfilblks_t *, int *);
+#endif
+
+/* kdb wrappers */
+
+static int	kdbm_xfs_xagf(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xagf((xfs_agf_t *)addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xagi(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xagi((xfs_agi_t *)addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xaildump(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xaildump((xfs_mount_t *) addr);
+	return 0;
+}
+
+#ifdef XFS_ALLOC_TRACE
+static int	kdbm_xfs_xalatrace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+	
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xalatrace((int) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xalbtrace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xalbtrace((xfs_agblock_t) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xalgtrace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xalgtrace((xfs_agnumber_t) addr);
+	return 0;
+}
+#endif
+
+#ifdef XFS_ATTR_TRACE
+static int	kdbm_xfs_xattrtrace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xattrtrace((int) addr);
+	return 0;
+}
+#endif
+
+#ifdef XFS_BLI_TRACE
+static int	kdbm_xfs_xblitrace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xblitrace((xfs_buf_log_item_t *) addr);
+	return 0;
+}
+#endif
+
+#ifdef XFS_BMAP_TRACE
+static int	kdbm_xfs_xbmatrace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xbmatrace((int) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xbmitrace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xbmitrace((xfs_inode_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xbmstrace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xbmstrace((xfs_inode_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xbxatrace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xbxatrace((int) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xbxitrace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xbxitrace((xfs_inode_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xbxstrace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xbxstrace((xfs_inode_t *) addr);
+	return 0;
+}
+#endif
+
+#ifdef XFS_DIR2_TRACE
+static int	kdbm_xfs_xdir2atrace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xdir2atrace((int) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xdir2itrace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xdir2itrace((xfs_inode_t *) addr);
+	return 0;
+}
+#endif
+
+#ifdef XFS_DIR_TRACE
+static int	kdbm_xfs_xdirtrace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xdirtrace((int) addr);
+	return 0;
+}
+#endif
+
+#ifdef XFS_LOG_TRACE
+static int	kdbm_xfs_xiclogtrace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xiclogtrace((xlog_in_core_t *) addr);
+	return 0;
+}
+#endif
+
+#ifdef XFS_ILOCK_TRACE
+static int	kdbm_xfs_xilock_trace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xilock_trace((xfs_inode_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xailock_trace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xailock_trace((int) addr);
+	return 0;
+}
+#endif
+
+#ifdef XFS_LOG_TRACE
+static int	kdbm_xfs_xlog_granttrace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xlog_granttrace((xlog_t *) addr);
+	return 0;
+}
+#endif
+
+#ifdef XFS_DQUOT_TRACE
+static int	kdbm_xfs_xqm_dqtrace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xqm_dqtrace((xfs_dquot_t *) addr);
+	return 0;
+}
+#endif
+
+#ifdef XFS_RW_TRACE
+static int	kdbm_xfs_xrwtrace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xrwtrace((xfs_inode_t *) addr);
+	return 0;
+}
+#endif
+
+static int	kdbm_xfs_xalloc(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xalloc((xfs_alloc_arg_t *) addr);
+	return 0;
+}
+
+#ifdef XFS_ALLOC_TRACE
+static int	kdbm_xfs_xalmtrace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xalmtrace((xfs_mount_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xalttrace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xalttrace((int) addr);
+	return 0;
+}
+#endif /* XFS_ALLOC_TRACE */
+
+static int	kdbm_xfs_xattrcontext(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xattrcontext((xfs_attr_list_context_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xattrleaf(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xattrleaf((xfs_attr_leafblock_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xattrsf(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xattrsf((xfs_attr_shortform_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xbirec(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xbirec((xfs_bmbt_irec_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xbmalla(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xbmalla((xfs_bmalloca_t *)addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xbrec(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xbrec((xfs_bmbt_rec_64_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xbroot(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xbroot((xfs_inode_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xbroota(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xbroota((xfs_inode_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xbtcur(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xbtcur((xfs_btree_cur_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xbuf(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xbuf((xfs_buf_t *) addr);
+	return 0;
+}
+
+
+static int	kdbm_xfs_xarg(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xarg((int) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xchksum(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xchksum((uint *) addr);
+	return 0;
+}
+
+
+static int	kdbm_xfs_xchash(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xchash((xfs_mount_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xchashlist(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xchashlist((xfs_chashlist_t *) addr);
+	return 0;
+}
+
+
+static int	kdbm_xfs_xdaargs(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xdaargs((xfs_da_args_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xdabuf(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xdabuf((xfs_dabuf_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xdanode(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xdanode((xfs_da_intnode_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xdastate(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xdastate((xfs_da_state_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xdirleaf(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xdirleaf((xfs_dir_leafblock_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xdirsf(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xdirsf((xfs_dir_shortform_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xdir2free(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xdir2free((xfs_dir2_free_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xdir2sf(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xdir2sf((xfs_dir2_sf_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xexlist(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xexlist((xfs_inode_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xflist(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xflist((xfs_bmap_free_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xhelp(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	if (argc != 0)
+		return KDB_ARGCOUNT;
+
+	xfsidbg_xhelp();
+	return 0;
+}
+
+static int	kdbm_xfs_xiclog(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xiclog((xlog_in_core_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xiclogall(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xiclogall((xlog_in_core_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xiclogcb(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xiclogcb((xlog_in_core_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xihash(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xihash((xfs_mount_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xinodes(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xinodes((xfs_mount_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_delayed_blocks(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_delayed_blocks((xfs_mount_t *) addr);
+	return 0;
+}
+
+
+static int	kdbm_xfs_xinodes_quiesce(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xinodes_quiesce((xfs_mount_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xlog(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xlog((xlog_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xlog_ritem(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xlog_ritem((xlog_recover_item_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xlog_rtrans(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xlog_rtrans((xlog_recover_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xlog_rtrans_entire(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xlog_rtrans_entire((xlog_recover_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xlog_tic(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xlog_tic((xlog_ticket_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xlogitem(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xlogitem((xfs_log_item_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xmount(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xmount((xfs_mount_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xnode(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xnode((xfs_inode_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xcore(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xcore((xfs_iocore_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xperag(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xperag((xfs_mount_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xqm_diskdq(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xqm_diskdq((xfs_disk_dquot_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xqm_dqattached_inos(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xqm_dqattached_inos((xfs_mount_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xqm_dquot(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xqm_dquot((xfs_dquot_t *) addr);
+	return 0;
+}
+
+#ifdef	CONFIG_XFS_QUOTA
+static int	kdbm_xfs_xqm(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	if (argc != 0)
+		return KDB_ARGCOUNT;
+
+	xfsidbg_xqm();
+	return 0;
+}
+
+static int	kdbm_xfs_xqm_freelist(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	if (argc != 0)
+		return KDB_ARGCOUNT;
+
+	xfsidbg_xqm_freelist();
+	return 0;
+}
+
+static int	kdbm_xfs_xqm_htab(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	if (argc != 0)
+		return KDB_ARGCOUNT;
+
+	xfsidbg_xqm_htab();
+	return 0;
+}
+#endif
+
+static int	kdbm_xfs_xqm_mplist(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xqm_mplist((xfs_mount_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xqm_qinfo(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xqm_qinfo((xfs_mount_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xqm_tpdqinfo(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xqm_tpdqinfo((xfs_trans_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xsb(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	unsigned long convert=0;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1 && argc!=2)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+	if (argc==2) {
+	    /* extra argument - conversion flag */
+	    diag = kdbgetaddrarg(argc, argv, &nextarg, &convert, &offset, NULL, regs);
+	    if (diag)
+		    return diag;
+	}
+
+	xfsidbg_xsb((xfs_sb_t *) addr, (int)convert);
+	return 0;
+}
+
+static int	kdbm_xfs_xtp(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xtp((xfs_trans_t *) addr);
+	return 0;
+}
+
+static int	kdbm_xfs_xtrans_res(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	xfsidbg_xtrans_res((xfs_mount_t *) addr);
+	return 0;
+}
+
+/*
+ * Vnode descriptor dump.
+ * This table is a string version of all the flags defined in vnode.h.
+ */
+char *tab_vflags[] = {
+	/* local only flags */
+	"VINACT",		/*	 0x01 */
+	"VRECLM",		/*	 0x02 */
+	"VWAIT",		/*	 0x04 */
+	"VMODIFIED",		/*	 0x08 */
+	"INVALID0x10",		/*	 0x10 */
+	"INVALID0x20",		/*	 0x20 */
+	"INVALID0x40",		/*	 0x40 */
+	"INVALID0x80",		/*	 0x80 */
+	"INVALID0x100",		/*	0x100 */
+	"INVALID0x200",		/*	0x200 */
+	"INVALID0x400",		/*	0x400 */
+	"INVALID0x800",		/*	0x800 */
+	"INVALID0x1000",	/*     0x1000 */
+	"INVALID0x2000",	/*     0x2000 */
+	"INVALID0x4000",	/*     0x4000 */
+	"INVALID0x8000",	/*     0x8000 */
+	"INVALID0x10000",	/*    0x10000 */
+	"INVALID0x20000",	/*    0x20000 */
+	"INVALID0x40000",	/*    0x40000 */
+	"INVALID0x80000",	/*    0x80000 */
+	"VROOT",		/*   0x100000 */
+	"INVALID0x200000",	/*   0x200000 */
+	"INVALID00x400000",	/*   0x400000 */
+	"INVALID0x800000",	/*   0x800000 */
+	"INVALID0x1000000",	/*  0x1000000 */
+	"INVALID0x2000000",	/*  0x2000000 */
+	"VSHARE",		/*  0x4000000 */
+	"INVALID0x8000000",     /*  0x8000000 */
+	"VENF_LOCKING",		/* 0x10000000 */
+	"VOPLOCK",		/* 0x20000000 */
+	"VPURGE",		/* 0x40000000 */
+	"INVALID0x80000000",	/* 0x80000000 */
+	0
+};
+
+
+static char *vnode_type[] = {
+	"VNON", "VREG", "VDIR", "VBLK", "VLNK", "VFIFO", "VBAD", "VSOCK"
+};
+
+static void
+printflags(register uint64_t flags,
+	register char **strings,
+	register char *name)
+{
+	register uint64_t mask = 1;
+
+	if (name)
+		kdb_printf("%s 0x%llx <", name, (unsigned long long)flags);
+
+	while (flags != 0 && *strings) {
+		if (mask & flags) {
+			kdb_printf("%s ", *strings);
+			flags &= ~mask;
+		}
+		mask <<= 1;
+		strings++;
+	}
+
+	if (name)
+		kdb_printf("> ");
+
+	return;
+}
+
+
+static void printbhv(bhv_desc_t *bdp)
+{
+	kdb_symtab_t	 symtab;
+
+	if (bdp == NULL) {
+		kdb_printf("NULL bhv\n");
+		return;
+	}
+
+	kdb_printf("bhv at 0x%p\n", bdp);
+	while (bdp) {
+		if (kdbnearsym((unsigned long)bdp->bd_ops, &symtab))
+			kdb_printf("  ops %s", symtab.sym_name);
+		else
+			kdb_printf("  ops %s/0x%p", "???", (void *)bdp->bd_ops);
+
+		kdb_printf(" vobj 0x%p pdata 0x%p next 0x%p\n",
+			   bdp->bd_vobj, bdp->bd_pdata, bdp->bd_next);
+
+		bdp = bdp->bd_next;
+	}
+}
+
+
+static void	printvnode(vnode_t *vp, unsigned long addr)
+{
+	kdb_printf("vnode: 0x%lx type ", addr);
+	if ((size_t)vp->v_type >= sizeof(vnode_type)/sizeof(vnode_type[0]))
+		kdb_printf("out of range 0x%x", vp->v_type);
+	else
+		kdb_printf("%s", vnode_type[vp->v_type]);
+	kdb_printf(" v_bh 0x%p\n", &vp->v_bh);
+
+	printbhv(vp->v_fbhv);
+
+	printflags((__psunsigned_t)vp->v_flag, tab_vflags, "flag =");
+	kdb_printf("\n");
+
+#ifdef	XFS_VNODE_TRACE
+	kdb_printf("   v_trace 0x%p\n", vp->v_trace);
+#endif	/* XFS_VNODE_TRACE */
+
+	kdb_printf("   v_vfsp 0x%p v_number 0x%llx\n",
+		vp->v_vfsp, (unsigned long long)vp->v_number);
+}
+
+static int	kdbm_vnode(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+	vnode_t		vp;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+
+	if (diag)
+		return diag;
+
+	if ((diag = kdb_getarea(vp, addr)))
+		return diag;
+
+	printvnode(&vp, addr);
+
+	return 0;
+}
+
+static void
+print_vfs(vfs_t	*vfs, unsigned long addr)
+{
+	kdb_printf("vfsp at 0x%lx", addr);
+	kdb_printf(" vfs_flag 0x%x\n", vfs->vfs_flag);
+	kdb_printf(" vfs_super 0x%p", vfs->vfs_super);
+	kdb_printf(" vfs_bh 0x%p\n", &vfs->vfs_bh);
+
+	printbhv(vfs->vfs_fbhv);
+}
+
+static int	kdbm_bhv(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+	bhv_desc_t	*bh;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+
+	if (diag)
+		return diag;
+
+	bh = (bhv_desc_t *)addr;
+
+	printbhv(bh);
+
+	return 0;
+}
+
+static int	kdbm_vfs(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long addr;
+	int nextarg = 1;
+	long offset = 0;
+	int diag;
+	vfs_t		vfs;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+
+	if (diag)
+		return diag;
+
+	if ((diag = kdb_getarea(vfs, addr)))
+		return diag;
+
+	print_vfs(&vfs, addr);
+
+	return 0;
+}
+
+
+#ifdef	XFS_VNODE_TRACE
+/*
+ * Print a vnode trace entry.
+ */
+static int
+vn_trace_pr_entry(ktrace_entry_t *ktep)
+{
+	char		funcname[128];
+	kdb_symtab_t	symtab;
+
+
+	if ((__psint_t)ktep->val[0] == 0)
+		return 0;
+
+	if (kdbnearsym((unsigned int)ktep->val[8], &symtab)) {
+		unsigned long offval;
+
+		offval = (unsigned int)ktep->val[8] - symtab.sym_start;
+
+		if (offval)
+			sprintf(funcname, "%s+0x%lx", symtab.sym_name, offval);
+		else
+			sprintf(funcname, "%s", symtab.sym_name);
+	} else
+		funcname[0] = '\0';
+
+
+	switch ((__psint_t)ktep->val[0]) {
+	case VNODE_KTRACE_ENTRY:
+		kdb_printf("entry to %s i_count = %d",
+						(char *)ktep->val[1],
+						(__psint_t)ktep->val[3]);
+		break;
+
+	case VNODE_KTRACE_EXIT:
+		kdb_printf("exit from %s i_count = %d",
+						(char *)ktep->val[1],
+						(__psint_t)ktep->val[3]);
+		break;
+
+	case VNODE_KTRACE_HOLD:
+		if ((__psint_t)ktep->val[3] != 1)
+			kdb_printf("hold @%s:%d(%s) i_count %d => %d ",
+						(char *)ktep->val[1],
+						(__psint_t)ktep->val[2],
+						funcname,
+						(__psint_t)ktep->val[3] - 1,
+						(__psint_t)ktep->val[3]);
+		else
+			kdb_printf("get @%s:%d(%s) i_count = %d",
+						(char *)ktep->val[1],
+						(__psint_t)ktep->val[2],
+						funcname,
+						(__psint_t)ktep->val[3]);
+		break;
+
+	case VNODE_KTRACE_REF:
+		kdb_printf("ref @%s:%d(%s) i_count = %d",
+						(char *)ktep->val[1],
+						(__psint_t)ktep->val[2],
+						funcname,
+						(__psint_t)ktep->val[3]);
+		break;
+
+	case VNODE_KTRACE_RELE:
+		if ((__psint_t)ktep->val[3] != 1)
+			kdb_printf("rele @%s:%d(%s) i_count %d => %d ",
+						(char *)ktep->val[1],
+						(__psint_t)ktep->val[2],
+						funcname,
+						(__psint_t)ktep->val[3],
+						(__psint_t)ktep->val[3] - 1);
+		else
+			kdb_printf("free @%s:%d(%s) i_count = %d",
+						(char *)ktep->val[1],
+						(__psint_t)ktep->val[2],
+						funcname,
+						(__psint_t)ktep->val[3]);
+		break;
+
+	default:
+		kdb_printf("unknown vntrace record\n");
+		return 1;
+	}
+
+	kdb_printf("\n");
+
+	kdb_printf("  cpu = %d pid = %d ",
+			(__psint_t)ktep->val[6], (pid_t)ktep->val[7]);
+
+	printflags((__psunsigned_t)ktep->val[5], tab_vflags, "flag =");
+
+	if (kdbnearsym((unsigned int)ktep->val[4], &symtab)) {
+		unsigned long offval;
+
+		offval = (unsigned int)ktep->val[4] - symtab.sym_start;
+
+		if (offval)
+			kdb_printf("  ra = %s+0x%lx", symtab.sym_name, offval);
+		else
+			kdb_printf("  ra = %s", symtab.sym_name);
+	} else
+		kdb_printf("  ra = ?? 0x%p", (void *)ktep->val[4]);
+
+	return 1;
+}
+
+
+/*
+ * Print out the trace buffer attached to the given vnode.
+ */
+static int	kdbm_vntrace(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	int		diag;
+	int		nextarg = 1;
+	long		offset = 0;
+	unsigned long	addr;
+	vnode_t		*vp;
+	ktrace_entry_t	*ktep;
+	ktrace_snap_t	kts;
+
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+
+	if (diag)
+		return diag;
+
+	vp = (vnode_t *)addr;
+
+	if (vp->v_trace == NULL) {
+		kdb_printf("The vnode trace buffer is not initialized\n");
+
+		return 0;
+	}
+
+	kdb_printf("vntrace vp 0x%p\n", vp);
+
+	ktep = ktrace_first(vp->v_trace, &kts);
+
+	while (ktep != NULL) {
+		if (vn_trace_pr_entry(ktep))
+			kdb_printf("\n");
+
+		ktep = ktrace_next(vp->v_trace, &kts);
+	}
+
+	return 0;
+}
+/*
+ * Print out the trace buffer attached to the given vnode.
+ */
+static int	kdbm_vntraceaddr(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	int		diag;
+	int		nextarg = 1;
+	long		offset = 0;
+	unsigned long	addr;
+	struct ktrace	*kt;
+	ktrace_entry_t	*ktep;
+	ktrace_snap_t	kts;
+
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+
+	if (diag)
+		return diag;
+
+	kt = (struct ktrace *)addr;
+
+	kdb_printf("vntraceaddr kt 0x%p\n", kt);
+
+	ktep = ktrace_first(kt, &kts);
+
+	while (ktep != NULL) {
+		if (vn_trace_pr_entry(ktep))
+			kdb_printf("\n");
+
+		ktep = ktrace_next(kt, &kts);
+	}
+
+	return 0;
+}
+#endif	/* XFS_VNODE_TRACE */
+
+
+static void	printinode(struct inode *ip)
+{
+	unsigned long	addr;
+
+
+	if (ip == NULL)
+		return;
+
+	kdb_printf(" i_ino = %lu i_count = %u i_size %Ld\n",
+					ip->i_ino, atomic_read(&ip->i_count),
+					ip->i_size);
+#if LINUX_VERSION_CODE < KERNEL_VERSION(2,6,0)
+	kdb_printf(
+		" i_mode = 0x%x  i_nlink = %d  i_rdev = 0x%x i_state = 0x%lx\n",
+					ip->i_mode, ip->i_nlink,
+					kdev_t_to_nr(ip->i_rdev), ip->i_state);
+	kdb_printf(" i_hash.nxt = 0x%p i_hash.pprv = 0x%p\n",
+                                        ip->i_hash.next, ip->i_hash.prev);
+#else
+	kdb_printf(
+		" i_mode = 0x%x  i_nlink = %d  i_rdev = 0x%x i_state = 0x%lx\n",
+					ip->i_mode, ip->i_nlink,
+					ip->i_rdev, ip->i_state);
+	kdb_printf(" i_hash.nxt = 0x%p i_hash.pprv = 0x%p\n",
+                                        ip->i_hash.next, ip->i_hash.pprev);
+#endif
+	kdb_printf(" i_list.nxt = 0x%p i_list.prv = 0x%p\n",
+					ip->i_list.next, ip->i_list.prev);
+	kdb_printf(" i_dentry.nxt = 0x%p i_dentry.prv = 0x%p\n",
+					ip->i_dentry.next,
+					ip->i_dentry.prev);
+
+	addr = (unsigned long)ip;
+
+	kdb_printf(" i_sb = 0x%p i_op = 0x%p i_data = 0x%lx nrpages = %lu\n",
+					ip->i_sb, ip->i_op,
+					addr + offsetof(struct inode, i_data),
+					ip->i_data.nrpages);
+
+	kdb_printf("  vnode ptr 0x%p\n", LINVFS_GET_VP(ip));
+}
+
+
+static int	kdbm_vn(
+	int	argc,
+	const char **argv,
+	const char **envp,
+	struct pt_regs *regs)
+{
+	int		diag;
+	int		nextarg = 1;
+	long		offset = 0;
+	unsigned long	addr;
+	struct inode	*ip;
+	vnode_t		vp;
+#ifdef	XFS_VNODE_TRACE
+	ktrace_entry_t	*ktep;
+	ktrace_snap_t	kts;
+#endif
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs);
+	if (diag)
+		return diag;
+
+	if ((diag = kdb_getarea(vp, addr)))
+		return diag;
+
+	ip = LINVFS_GET_IP((vnode_t *)addr);
+	kdb_printf("--> Inode @ 0x%p\n", ip);
+	printinode(ip);
+
+	kdb_printf("--> Vnode @ 0x%lx\n", addr);
+	printvnode(&vp, addr);
+
+#ifdef	XFS_VNODE_TRACE
+	kdb_printf("--> Vntrace @ 0x%lx/0x%p\n", addr, vp.v_trace);
+	if (vp.v_trace == NULL)
+		return 0;
+	ktep = ktrace_first(vp.v_trace, &kts);
+	while (ktep != NULL) {
+		if (vn_trace_pr_entry(ktep))
+			kdb_printf("\n");
+
+		ktep = ktrace_next(vp.v_trace, &kts);
+	}
+#endif	/* XFS_VNODE_TRACE */
+	return 0;
+}
+
+
+/* pagebuf stuff */
+
+static char	*pb_flag_vals[] = {
+/*  0 */ "READ", "WRITE", "MAPPED", "PARTIAL", "ASYNC",
+/*  5 */ "NONE", "DELWRI", "INVALID0x07", "INVALID0x08", "INVALID0x09",
+/* 10 */ "STALE", "FS_MANAGED", "FS_DATAIOD", "LOCK", "TRYLOCK",
+/* 15 */ "DONT_BLOCK", "PAGECACHE", "PRIVATE_BH", "ALL_PAGES_MAPPED", 
+	 "ADDR_ALLOCATED",
+/* 20 */ "MEM_ALLOCATED", "MEM_SLAB", "FORCEIO", "FLUSH", "READ_AHEAD",
+/* 25 */ "RUN_QUEUES", "DIRECTIO",
+	 NULL };
+
+static char	*iomap_flag_vals[] = {
+	"EOF", "HOLE", "DELAY", "INVALID0x08",
+	"INVALID0x10", "UNWRITTEN", "NEW", "INVALID0x80",
+	NULL };
+
+
+static char	*map_flags(unsigned long flags, char *mapping[])
+{
+	static	char	buffer[256];
+	int	index;
+	int	offset = 12;
+
+	buffer[0] = '\0';
+
+	for (index = 0; flags && mapping[index]; flags >>= 1, index++) {
+		if (flags & 1) {
+			if ((offset + strlen(mapping[index]) + 1) >= 80) {
+				strcat(buffer, "\n	    ");
+				offset = 12;
+			} else if (offset > 12) {
+				strcat(buffer, " ");
+				offset++;
+			}
+			strcat(buffer, mapping[index]);
+			offset += strlen(mapping[index]);
+		}
+	}
+
+	return (buffer);
+}
+
+static char	*pb_flags(page_buf_flags_t pb_flag)
+{
+	return(map_flags((unsigned long) pb_flag, pb_flag_vals));
+}
+
+static int
+kdbm_pb_flags(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	unsigned long flags;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	diag = kdbgetularg(argv[1], &flags);
+	if (diag)
+		return diag;
+
+	kdb_printf("pb flags 0x%lx = %s\n", flags, pb_flags(flags));
+
+	return 0;
+}
+
+static void
+print_pagebuf(
+	xfs_buf_t	*pb,
+	unsigned long addr)
+{
+	kdb_printf("xfs_buf_t at 0x%lx\n", addr);
+	kdb_printf("  pb_flags %s\n", pb_flags(pb->pb_flags));
+	kdb_printf("  pb_target 0x%p pb_hold %d pb_next 0x%p pb_prev 0x%p\n",
+		   pb->pb_target, pb->pb_hold.counter,
+		   list_entry(pb->pb_list.next, xfs_buf_t, pb_list),
+		   list_entry(pb->pb_list.prev, xfs_buf_t, pb_list));
+	kdb_printf("  pb_hash_index %d pb_hash_next 0x%p pb_hash_prev 0x%p\n",
+		   pb->pb_hash_index,
+		   list_entry(pb->pb_hash_list.next, xfs_buf_t, pb_hash_list),
+		   list_entry(pb->pb_hash_list.prev, xfs_buf_t, pb_hash_list));
+	kdb_printf("  pb_file_offset 0x%llx pb_buffer_length 0x%llx pb_addr 0x%p\n",
+		   (unsigned long long) pb->pb_file_offset,
+		   (unsigned long long) pb->pb_buffer_length,
+		   pb->pb_addr);
+	kdb_printf("  pb_bn 0x%Lx pb_count_desired 0x%lx pb_locked %d\n",
+		   pb->pb_bn,
+		   (unsigned long) pb->pb_count_desired, (int)pb->pb_locked);
+	kdb_printf("  pb_flushtime %ld (%ld) pb_io_remaining %d pb_error %u\n",
+		   pb->pb_flushtime, pb->pb_flushtime - jiffies,
+		   pb->pb_io_remaining.counter, pb->pb_error);
+	kdb_printf("  pb_page_count %u pb_offset 0x%x pb_pages 0x%p\n",
+		pb->pb_page_count, pb->pb_offset,
+		pb->pb_pages);
+	kdb_printf("  pb_iodonesema (%d,%d) pb_sema (%d,%d) pincount (%d)\n",
+		   pb->pb_iodonesema.count.counter,
+		   pb->pb_iodonesema.sleepers,
+		   pb->pb_sema.count.counter, pb->pb_sema.sleepers,
+		   pb->pb_pin_count.counter);
+#ifdef PAGEBUF_LOCK_TRACKING
+	kdb_printf("  last holder %d\n", pb->pb_last_holder);
+#endif
+	if (pb->pb_fspriv || pb->pb_fspriv2) {
+		kdb_printf(  "  pb_fspriv 0x%p pb_fspriv2 0x%p\n",
+			   pb->pb_fspriv, pb->pb_fspriv2);
+	}
+}
+
+static int
+kdbm_pb(int argc, const char **argv, const char **envp, struct pt_regs *regs)
+{
+	xfs_buf_t bp;
+	unsigned long addr;
+	long	offset=0;
+	int nextarg;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	nextarg = 1;
+	if ((diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs)) ||
+	    (diag = kdb_getarea(bp, addr)))
+		return diag;
+
+	print_pagebuf(&bp, addr);
+
+	return 0;
+}
+
+static int
+kdbm_pbdelay(int argc, const char **argv, const char **envp,
+	struct pt_regs *regs)
+{
+#ifdef DEBUG
+	unsigned long	verbose = 0;
+	int	count = 0;
+	struct list_head	*curr, *next;
+	xfs_buf_t	bp;
+	unsigned long addr;
+	int diag;
+	extern struct list_head pbd_delwrite_queue;
+
+	if (argc > 1)
+		return KDB_ARGCOUNT;
+
+	if (argc == 1) {
+		if ((diag = kdbgetularg(argv[1], &verbose))) {
+			return diag;
+		}
+	}
+
+	if (!verbose) {
+		kdb_printf("index pb       pin   flushtime\n");
+	}
+
+	list_for_each_safe(curr, next, &pbd_delwrite_queue) {
+		addr = (unsigned long)list_entry(curr, xfs_buf_t, pb_list);
+		if ((diag = kdb_getarea(bp, addr)))
+			return diag;
+
+		if (verbose) {
+			print_pagebuf(&bp, addr);
+		} else {
+			kdb_printf("%4d  0x%lx   %d   %ld\n",
+				count++, addr, 
+				bp.pb_pin_count.counter,
+				bp.pb_flushtime - jiffies);
+		}
+	}
+#else
+	kdb_printf("pbd_delwrite_queue inaccessible (non-debug)\n");
+#endif
+	return 0;
+}
+
+static int
+kdbm_iomap(int argc, const char **argv, const char **envp,
+	struct pt_regs *regs)
+{
+	xfs_iomap_t iomap;
+	unsigned long addr;
+	long offset=0;
+	int nextarg;
+	int diag;
+
+	if (argc != 1)
+		return KDB_ARGCOUNT;
+
+	nextarg = 1;
+	if ((diag = kdbgetaddrarg(argc, argv, &nextarg, &addr, &offset, NULL, regs)) ||
+	    (diag = kdb_getarea(iomap, addr)))
+
+	kdb_printf("iomap_t at 0x%lx\n", addr);
+	kdb_printf("  iomap_bn 0x%llx iomap_offset 0x%Lx iomap_delta 0x%lx iomap_bsize 0x%llx\n",
+		(long long) iomap.iomap_bn, iomap.iomap_offset,
+		(unsigned long)iomap.iomap_delta, (long long)iomap.iomap_bsize);
+
+	kdb_printf("  iomap_flags %s\n", map_flags(iomap.iomap_flags, iomap_flag_vals));
+
+	return 0;
+}
+
+#ifdef PAGEBUF_TRACE
+static int pagebuf_trace_entry(ktrace_entry_t *ktep)
+{
+	unsigned long long daddr;
+
+	daddr = ((unsigned long long)(unsigned long)ktep->val[8] << 32)
+		| ((unsigned long long)(unsigned long)ktep->val[9]);
+
+	kdb_printf("pb 0x%p [%s] (hold %lu lock %ld) data 0x%p",
+		ktep->val[0],
+		(char *)ktep->val[1],
+		(unsigned long)ktep->val[3],
+		(long)ktep->val[4],
+		ktep->val[6]);
+	kdb_symbol_print((unsigned long)ktep->val[7], NULL,
+		KDB_SP_SPACEB|KDB_SP_PAREN|KDB_SP_NEWLINE);
+	kdb_printf("    offset 0x%llx size 0x%lx task 0x%p\n",
+		daddr, (long)ktep->val[10], ktep->val[5]);
+	kdb_printf("    flags: %s\n", pb_flags((int)(long)ktep->val[2]));
+	return 1;
+}
+
+static int
+kdbm_pbtrace_offset(int argc, const char **argv, const char **envp,
+	struct pt_regs *regs)
+{
+	long		mask = 0;
+	unsigned long	offset = 0;
+	int		diag;
+	ktrace_entry_t	*ktep;
+	ktrace_snap_t	kts;
+
+	if (argc > 2)
+		return KDB_ARGCOUNT;
+
+	if (argc > 0) {
+		diag = kdbgetularg(argv[1], &offset);
+		if (diag)
+			return diag;
+	}
+
+	if (argc > 1) {
+		diag = kdbgetularg(argv[1], &mask);	/* sign extent mask */
+		if (diag)
+			return diag;
+	}
+
+	ktep = ktrace_first(pagebuf_trace_buf, &kts);
+	while (ktep != NULL) {
+		unsigned long long daddr;
+
+		daddr = ((unsigned long long)(unsigned long)ktep->val[8] << 32)
+			| ((unsigned long long)(unsigned long)ktep->val[9]);
+		if (offset && ((daddr & ~mask) != offset))
+			continue;
+		if (pagebuf_trace_entry(ktep))
+			kdb_printf("\n");
+		ktep = ktrace_next(pagebuf_trace_buf, &kts);
+	}
+	return 0;
+}
+
+static int
+kdbm_pbtrace(int argc, const char **argv, const char **envp,
+	struct pt_regs *regs)
+{
+	unsigned long	addr = 0;
+	int		diag, nextarg;
+	long		offset = 0;
+	char		*event_match = NULL;
+	ktrace_entry_t	*ktep;
+	ktrace_snap_t	kts;
+
+	if (argc > 1)
+		return KDB_ARGCOUNT;
+
+	if (argc == 1) {
+		if (isupper(argv[1][0]) || islower(argv[1][0])) {
+			event_match = (char *)argv[1];
+			kdb_printf("event match on \"%s\"\n", event_match);
+			argc = 0;
+		} else {
+			nextarg = 1;
+			diag = kdbgetaddrarg(argc, argv,
+					&nextarg, &addr, &offset, NULL, regs);
+			if (diag) {
+				kdb_printf("non-numeric arg: %s\n", argv[1]);
+				return diag;
+			}
+		}
+	}
+
+	ktep = ktrace_first(pagebuf_trace_buf, &kts);
+	while (ktep != NULL) {
+		if (addr && (ktep->val[0] != (void *)addr))
+			continue;
+		if (event_match && strcmp((char *)ktep->val[1], event_match))
+			continue;
+		if (pagebuf_trace_entry(ktep))
+			qprintf("\n");
+		ktep = ktrace_next(pagebuf_trace_buf, &kts);
+	}
+	return 0;
+}
+#endif
+
+struct xif {
+	char	*name;
+	int	(*func)(int, const char **, const char **, struct pt_regs *);
+	char	*args;
+	char	*help;
+};
+
+static struct xif xfsidbg_funcs[] = {
+  {  "bhv",	kdbm_bhv,	"<bhv>", "Dump bhv chain"},
+  {  "vn",	kdbm_vn,	"<vnode>", "Dump inode/vnode/trace"},
+  {  "vnode",	kdbm_vnode,	"<vnode>", "Dump vnode"},
+  {  "vfs",	kdbm_vfs,	"<vfs>", "Dump vfs"},
+#ifdef XFS_VNODE_TRACE
+  {  "vntrace",	kdbm_vntrace,	"<vntrace>", "Dump vnode Trace"},
+  {  "vntraceaddr",	kdbm_vntraceaddr, "<vntrace>",
+				"Dump vnode Trace by Address"},
+#endif
+  {  "xagf",	kdbm_xfs_xagf,	"<agf>",
+				"Dump XFS allocation group freespace" },
+  {  "xagi",	kdbm_xfs_xagi,	"<agi>",
+				"Dump XFS allocation group inode" },
+  {  "xail",	kdbm_xfs_xaildump,	"<xfs_mount_t>",
+				"Dump XFS AIL for a mountpoint" },
+#ifdef XFS_ALLOC_TRACE
+  {  "xalatrc",	kdbm_xfs_xalatrace,	"<count>",
+				"Dump XFS alloc count trace" },
+  {  "xalbtrc",	kdbm_xfs_xalbtrace,	"<xfs_agblock_t>",
+				"Dump XFS alloc block trace" },
+  {  "xalgtrc",	kdbm_xfs_xalgtrace,	"<xfs_agnumber_t>",
+				"Dump XFS alloc alloc-group trace" },
+#endif
+
+  {  "xalloc",	kdbm_xfs_xalloc,	"<xfs_alloc_arg_t>",
+				"Dump XFS allocation args structure" },
+#ifdef XFS_ALLOC_TRACE
+  {  "xalmtrc",	kdbm_xfs_xalmtrace,	"<xfs_mount_t>",
+				"Dump XFS alloc mount-point trace" },
+  {  "xalttrc",	kdbm_xfs_xalttrace,	"<tag>",
+				"Dump XFS alloc trace by tag number" },
+#endif
+  {  "xarg",	kdbm_xfs_xarg,		"<value>",
+				"Input XFS argument for next function" },
+  {  "xattrcx",	kdbm_xfs_xattrcontext,	"<xfs_attr_list_context_t>",
+				"Dump XFS attr_list context struct"},
+  {  "xattrlf",	kdbm_xfs_xattrleaf,	"<xfs_attr_leafblock_t>",
+				"Dump XFS attribute leaf block"},
+  {  "xattrsf",	kdbm_xfs_xattrsf,	"<xfs_attr_shortform_t>",
+				"Dump XFS attribute shortform"},
+#ifdef XFS_ATTR_TRACE
+  {  "xattrtr",	kdbm_xfs_xattrtrace,	"<count>",
+				"Dump XFS attribute attr_list() trace" },
+#endif
+  {  "xbirec",	kdbm_xfs_xbirec,	"<xfs_bmbt_irec_t",
+				"Dump XFS bmap incore record"},
+#ifdef XFS_BLI_TRACE
+  {  "xblitrc",	kdbm_xfs_xblitrace,	"<xfs_buf_log_item_t>",
+				"Dump XFS buf log item trace" },
+#endif
+  {  "xbmalla",	kdbm_xfs_xbmalla,	"<xfs_bmalloca_t>",
+				"Dump XFS bmalloc args structure"},
+#ifdef XFS_BMAP_TRACE
+  {  "xbmatrc",	kdbm_xfs_xbmatrace,	"<count>",
+				"Dump XFS bmap btree count trace" },
+  {  "xbmitrc",	kdbm_xfs_xbmitrace,	"<xfs_inode_t>",
+				"Dump XFS bmap btree per-inode trace" },
+  {  "xbmstrc",	kdbm_xfs_xbmstrace,	"<xfs_inode_t>",
+				"Dump XFS bmap btree inode trace" },
+#endif
+  {  "xbrec",	kdbm_xfs_xbrec,		"<xfs_bmbt_rec_64_t>",
+				"Dump XFS bmap record"},
+  {  "xbroot",	kdbm_xfs_xbroot,	"<xfs_inode_t>",
+				"Dump XFS bmap btree root (data)"},
+  {  "xbroota",	kdbm_xfs_xbroota,	"<xfs_inode_t>",
+				"Dump XFS bmap btree root (attr)"},
+  {  "xbtcur",	kdbm_xfs_xbtcur,	"<xfs_btree_cur_t>",
+				"Dump XFS btree cursor"},
+  {  "xbuf",	kdbm_xfs_xbuf,		"<xfs_buf_t>",
+				"Dump XFS data from a buffer"},
+#ifdef XFS_BMAP_TRACE
+  {  "xbxatrc",	kdbm_xfs_xbxatrace,	"<count>",
+				"Dump XFS bmap extent count trace" },
+  {  "xbxitrc",	kdbm_xfs_xbxitrace,	"<xfs_inode_t>",
+				"Dump XFS bmap extent per-inode trace" },
+  {  "xbxstrc",	kdbm_xfs_xbxstrace,	"<xfs_inode_t>",
+				"Dump XFS bmap extent inode trace" },
+#endif
+  {  "xchash",	kdbm_xfs_xchash,	"<xfs_mount_t>",
+				"Dump XFS cluster hash"},
+  {  "xchlist",	kdbm_xfs_xchashlist,	"<xfs_chashlist_t>",
+				"Dump XFS cluster hash list"},
+  {  "xchksum",	kdbm_xfs_xchksum,	"<addr>", "Dump chksum" },
+#ifdef XFS_DIR2_TRACE
+  {  "xd2atrc",	kdbm_xfs_xdir2atrace,	"<count>",
+				"Dump XFS directory v2 count trace" },
+#endif
+  {  "xd2free",	kdbm_xfs_xdir2free,	"<xfs_dir2_free_t>",
+				"Dump XFS directory v2 freemap"},
+#ifdef XFS_DIR2_TRACE
+  {  "xd2itrc",	kdbm_xfs_xdir2itrace,	"<xfs_inode_t>",
+				"Dump XFS directory v2 per-inode trace" },
+#endif
+  {  "xdaargs",	kdbm_xfs_xdaargs,	"<xfs_da_args_t>",
+				"Dump XFS dir/attr args structure"},
+  {  "xdabuf",	kdbm_xfs_xdabuf,	"<xfs_dabuf_t>",
+				"Dump XFS dir/attr buf structure"},
+  {  "xdanode",	kdbm_xfs_xdanode,	"<xfs_da_intnode_t>",
+				"Dump XFS dir/attr node block"},
+  {  "xdastat",	kdbm_xfs_xdastate,	"<xfs_da_state_t>",
+				"Dump XFS dir/attr state_blk struct"},
+  {  "xdelay",	kdbm_xfs_delayed_blocks,	"<xfs_mount_t>",
+				"Dump delayed block totals"},
+  {  "xdirlf",	kdbm_xfs_xdirleaf,	"<xfs_dir_leafblock_t>",
+				"Dump XFS directory leaf block"},
+  {  "xdirsf",	kdbm_xfs_xdirsf,	"<xfs_dir_shortform_t>",
+				"Dump XFS directory shortform"},
+  {  "xdir2sf",	kdbm_xfs_xdir2sf,	"<xfs_dir2_sf_t>",
+				"Dump XFS directory v2 shortform"},
+#ifdef XFS_DIR_TRACE
+  {  "xdirtrc",	kdbm_xfs_xdirtrace,	"<count>",
+				"Dump XFS directory getdents() trace" },
+#endif
+  {  "xdiskdq",	kdbm_xfs_xqm_diskdq,	"<xfs_disk_dquot_t>",
+				"Dump XFS ondisk dquot (quota) struct"},
+  {  "xdqatt",	kdbm_xfs_xqm_dqattached_inos,	"<xfs_mount_t>",
+				 "All incore inodes with dquots"},
+  {  "xdqinfo",	kdbm_xfs_xqm_tpdqinfo,	"<xfs_trans_t>",
+				"Dump dqinfo structure of a trans"},
+#ifdef XFS_DQUOT_TRACE
+  {  "xdqtrace",kdbm_xfs_xqm_dqtrace,	"<xfs_dquot_t>",
+				"Dump trace of a given dquot" },
+#endif
+  {  "xdquot",	kdbm_xfs_xqm_dquot,	"<xfs_dquot_t>",
+				"Dump XFS dquot (quota) structure"},
+  {  "xexlist",	kdbm_xfs_xexlist,	"<xfs_inode_t>",
+				"Dump XFS bmap extents in inode"},
+  {  "xflist",	kdbm_xfs_xflist,	"<xfs_bmap_free_t>",
+				"Dump XFS to-be-freed extent list"},
+  {  "xhelp",	kdbm_xfs_xhelp,		"",
+				"Print idbg-xfs help"},
+  {  "xicall",	kdbm_xfs_xiclogall,	"<xlog_in_core_t>",
+				"Dump All XFS in-core logs"},
+  {  "xiclog",	kdbm_xfs_xiclog,	"<xlog_in_core_t>",
+				"Dump XFS in-core log"},
+#ifdef XFS_LOG_TRACE
+  {  "xictrc",	kdbm_xfs_xiclogtrace,	"<xlog_in_core_t>",
+				"Dump XFS in-core log trace" },
+#endif
+  {  "xihash",	kdbm_xfs_xihash,	"<xfs_mount_t>",
+				"Dump XFS inode hash statistics"},
+#ifdef XFS_ILOCK_TRACE
+  {  "xilocktrc",kdbm_xfs_xilock_trace,	"<xfs_inode_t>",
+				"Dump XFS ilock trace" },
+  {  "xailcktrc",kdbm_xfs_xailock_trace,"<count>",
+				"Dump XFS global ilock trace" },
+#endif
+  {  "xinodes",	kdbm_xfs_xinodes,	"<xfs_mount_t>",
+				"Dump XFS inodes per mount"},
+  {  "xquiesce",kdbm_xfs_xinodes_quiesce, "<xfs_mount_t>",
+				"Dump non-quiesced XFS inodes per mount"},
+#ifdef XFS_LOG_TRACE
+  {  "xl_grtr",	kdbm_xfs_xlog_granttrace,	"<xlog_t>",
+				"Dump XFS log grant trace" },
+#endif
+  {  "xl_rcit",	kdbm_xfs_xlog_ritem,	"<xlog_recover_item_t>",
+				"Dump XFS recovery item"},
+  {  "xl_rctr",	kdbm_xfs_xlog_rtrans,	"<xlog_recover_t>",
+				"Dump XFS recovery transaction"},
+  {  "xl_rctr2",kdbm_xfs_xlog_rtrans_entire,	"<xlog_recover_t>",
+				"Dump entire recovery transaction"},
+  {  "xl_tic",	kdbm_xfs_xlog_tic,	"<xlog_ticket_t>",
+				"Dump XFS log ticket"},
+  {  "xlog",	kdbm_xfs_xlog,	"<xlog_t>",
+				"Dump XFS log"},
+  {  "xlogcb",	kdbm_xfs_xiclogcb,	"<xlog_in_core_t>",
+				"Dump XFS in-core log callbacks"},
+  {  "xlogitm",	kdbm_xfs_xlogitem,	"<xfs_log_item_t>",
+				"Dump XFS log item structure"},
+  {  "xmount",	kdbm_xfs_xmount,	"<xfs_mount_t>",
+				"Dump XFS mount structure"},
+  {  "xnode",	kdbm_xfs_xnode,		"<xfs_inode_t>",
+				"Dump XFS inode"},
+  {  "xiocore",	kdbm_xfs_xcore,		"<xfs_iocore_t>",
+				"Dump XFS iocore"},
+  {  "xperag",	kdbm_xfs_xperag,	"<xfs_mount_t>",
+				"Dump XFS per-allocation group data"},
+  {  "xqinfo",  kdbm_xfs_xqm_qinfo,	"<xfs_mount_t>",
+				"Dump mount->m_quotainfo structure"},
+#ifdef	CONFIG_XFS_QUOTA
+  {  "xqm",	kdbm_xfs_xqm,		"",
+				"Dump XFS quota manager structure"},
+  {  "xqmfree",	kdbm_xfs_xqm_freelist,	"",
+				"Dump XFS global freelist of dquots"},
+  {  "xqmhtab",	kdbm_xfs_xqm_htab,	"",
+				"Dump XFS hashtable of dquots"},
+#endif	/* CONFIG_XFS_QUOTA */
+  {  "xqmplist",kdbm_xfs_xqm_mplist,	"<xfs_mount_t>",
+				"Dump XFS all dquots of a f/s"},
+#ifdef XFS_RW_TRACE
+  {  "xrwtrc",	kdbm_xfs_xrwtrace,	"<xfs_inode_t>",
+				"Dump XFS inode read/write trace" },
+#endif
+  {  "xsb",	kdbm_xfs_xsb,		"<xfs_sb_t> <cnv>",
+				"Dump XFS superblock"},
+  {  "xtp",	kdbm_xfs_xtp,		"<xfs_trans_t>",
+				"Dump XFS transaction structure"},
+  {  "xtrres",	kdbm_xfs_xtrans_res,	"<xfs_mount_t>",
+				"Dump XFS reservation values"},
+  {  0,		0,	0 }
+};
+
+static struct xif pb_funcs[] = {
+  {  "pb",	kdbm_pb,	"<vaddr>",	"Display xfs_buf_t" },
+  {  "pbflags",	kdbm_pb_flags,	"<flags>",	"Display page_buf flags" },
+  {  "iomapap",	kdbm_iomap,	"<iomap_t *>",	"Display IOmap" },
+  {  "pbdelay",	kdbm_pbdelay,	"0|1",		"Display delwri pagebufs" },
+#ifdef PAGEBUF_TRACE
+  {  "pbtrace",	kdbm_pbtrace,	"<vaddr>|<count>",	"xfs_buf_t trace" },
+  {  "pboffset",kdbm_pbtrace_offset, "<daddr> [<mask>]","xfs_buf_t trace" },
+#endif
+  {  0,		0,	0 }
+};
+
+static int
+__init xfsidbg_init(void)
+{
+	struct xif	*p;
+
+	for (p = xfsidbg_funcs; p->name; p++)
+		kdb_register(p->name, p->func, p->args, p->help, 0);
+	for (p = pb_funcs; p->name; p++)
+		kdb_register(p->name, p->func, p->args, p->help, 0);
+	return 0;
+}
+
+static void
+__exit xfsidbg_exit(void)
+{
+	struct xif	*p;
+
+	for (p = xfsidbg_funcs; p->name; p++)
+		kdb_unregister(p->name);
+	for (p = pb_funcs; p->name; p++)
+		kdb_unregister(p->name);
+}
+
+/*
+ * Argument to xfs_alloc routines, for allocation type.
+ */
+static char *xfs_alloctype[] = {
+	"any_ag", "first_ag", "start_ag", "this_ag",
+	"start_bno", "near_bno", "this_bno"
+};
+
+static int xargument = 0;
+
+/*
+ * Prototypes for static functions.
+ */
+#ifdef XFS_ALLOC_TRACE
+static int xfs_alloc_trace_entry(ktrace_entry_t *ktep);
+#endif
+#ifdef XFS_ATTR_TRACE
+static int xfs_attr_trace_entry(ktrace_entry_t *ktep);
+#endif
+#ifdef XFS_BMAP_TRACE
+static int xfs_bmap_trace_entry(ktrace_entry_t *ktep);
+#endif
+#ifdef XFS_BMAP_TRACE
+static int xfs_bmbt_trace_entry(ktrace_entry_t *ktep);
+#endif
+static void xfs_broot(xfs_inode_t *ip, xfs_ifork_t *f);
+static void xfs_btalloc(xfs_alloc_block_t *bt, int bsz);
+static void xfs_btbmap(xfs_bmbt_block_t *bt, int bsz);
+static void xfs_btino(xfs_inobt_block_t *bt, int bsz);
+static void xfs_buf_item_print(xfs_buf_log_item_t *blip, int summary);
+static void xfs_dastate_path(xfs_da_state_path_t *p);
+#ifdef XFS_DIR_TRACE
+static int xfs_dir_trace_entry(ktrace_entry_t *ktep);
+#endif
+#ifdef XFS_DIR2_TRACE
+static int xfs_dir2_trace_entry(ktrace_entry_t *ktep);
+#endif
+static void xfs_dir2data(void *addr, int size);
+static void xfs_dir2leaf(xfs_dir2_leaf_t *leaf, int size);
+static void xfs_dquot_item_print(xfs_dq_logitem_t *lip, int summary);
+static void xfs_efd_item_print(xfs_efd_log_item_t *efdp, int summary);
+static void xfs_efi_item_print(xfs_efi_log_item_t *efip, int summary);
+static char *xfs_fmtformat(xfs_dinode_fmt_t f);
+static char *xfs_fmtfsblock(xfs_fsblock_t bno, xfs_mount_t *mp);
+static char *xfs_fmtino(xfs_ino_t ino, xfs_mount_t *mp);
+static char *xfs_fmtlsn(xfs_lsn_t *lsnp);
+static char *xfs_fmtmode(int m);
+static char *xfs_fmtsize(size_t i);
+static char *xfs_fmtuuid(uuid_t *);
+static void xfs_inode_item_print(xfs_inode_log_item_t *ilip, int summary);
+static void xfs_inodebuf(xfs_buf_t *bp);
+static void xfs_prdinode(xfs_dinode_t *di, int coreonly, int convert);
+static void xfs_prdinode_core(xfs_dinode_core_t *dip, int convert);
+static void xfs_qoff_item_print(xfs_qoff_logitem_t *lip, int summary);
+#ifdef XFS_RW_TRACE
+static void xfs_rw_enter_trace_entry(ktrace_entry_t *ktep);
+static void xfs_page_trace_entry(ktrace_entry_t *ktep);
+static int xfs_rw_trace_entry(ktrace_entry_t *ktep);
+#endif
+static void xfs_xexlist_fork(xfs_inode_t *ip, int whichfork);
+static void xfs_xnode_fork(char *name, xfs_ifork_t *f);
+
+/*
+ * Static functions.
+ */
+
+#ifdef XFS_ALLOC_TRACE
+/*
+ * Print xfs alloc trace buffer entry.
+ */
+static int
+xfs_alloc_trace_entry(ktrace_entry_t *ktep)
+{
+	static char *modagf_flags[] = {
+		"magicnum",
+		"versionnum",
+		"seqno",
+		"length",
+		"roots",
+		"levels",
+		"flfirst",
+		"fllast",
+		"flcount",
+		"freeblks",
+		"longest",
+		NULL
+	};
+
+	if (((__psint_t)ktep->val[0] & 0xffff) == 0)
+		return 0;
+	switch ((long)ktep->val[0] & 0xffffL) {
+	case XFS_ALLOC_KTRACE_ALLOC:
+		kdb_printf("alloc %s[%s %ld] mp 0x%p\n",
+			(char *)ktep->val[1],
+			ktep->val[2] ? (char *)ktep->val[2] : "",
+			(long)ktep->val[0] >> 16,
+			(xfs_mount_t *)ktep->val[3]);
+		kdb_printf(
+	"agno %ld agbno %ld minlen %ld maxlen %ld mod %ld prod %ld minleft %ld\n",
+			(long)ktep->val[4],
+			(long)ktep->val[5],
+			(long)ktep->val[6],
+			(long)ktep->val[7],
+			(long)ktep->val[8],
+			(long)ktep->val[9],
+			(long)ktep->val[10]);
+		kdb_printf("total %ld alignment %ld len %ld type %s otype %s\n",
+			(long)ktep->val[11],
+			(long)ktep->val[12],
+			(long)ktep->val[13],
+			xfs_alloctype[((__psint_t)ktep->val[14]) >> 16],
+			xfs_alloctype[((__psint_t)ktep->val[14]) & 0xffff]);
+		kdb_printf("wasdel %d wasfromfl %d isfl %d userdata %d\n",
+			((__psint_t)ktep->val[15] & (1 << 3)) != 0,
+			((__psint_t)ktep->val[15] & (1 << 2)) != 0,
+			((__psint_t)ktep->val[15] & (1 << 1)) != 0,
+			((__psint_t)ktep->val[15] & (1 << 0)) != 0);
+		break;
+	case XFS_ALLOC_KTRACE_FREE:
+		kdb_printf("free %s[%s %ld] mp 0x%p\n",
+			(char *)ktep->val[1],
+			ktep->val[2] ? (char *)ktep->val[2] : "",
+			(long)ktep->val[0] >> 16,
+			(xfs_mount_t *)ktep->val[3]);
+		kdb_printf("agno %ld agbno %ld len %ld isfl %d\n",
+			(long)ktep->val[4],
+			(long)ktep->val[5],
+			(long)ktep->val[6],
+			(__psint_t)ktep->val[7] != 0);
+		break;
+	case XFS_ALLOC_KTRACE_MODAGF:
+		kdb_printf("modagf %s[%s %ld] mp 0x%p\n",
+			(char *)ktep->val[1],
+			ktep->val[2] ? (char *)ktep->val[2] : "",
+			(long)ktep->val[0] >> 16,
+			(xfs_mount_t *)ktep->val[3]);
+		printflags((__psint_t)ktep->val[4], modagf_flags, "modified");
+		kdb_printf("seqno %lu length %lu roots b %lu c %lu\n",
+			(unsigned long)ktep->val[5],
+			(unsigned long)ktep->val[6],
+			(unsigned long)ktep->val[7],
+			(unsigned long)ktep->val[8]);
+		kdb_printf("levels b %lu c %lu flfirst %lu fllast %lu flcount %lu\n",
+			(unsigned long)ktep->val[9],
+			(unsigned long)ktep->val[10],
+			(unsigned long)ktep->val[11],
+			(unsigned long)ktep->val[12],
+			(unsigned long)ktep->val[13]);
+		kdb_printf("freeblks %lu longest %lu\n",
+			(unsigned long)ktep->val[14],
+			(unsigned long)ktep->val[15]);
+		break;
+
+	case XFS_ALLOC_KTRACE_UNBUSY:
+		kdb_printf("unbusy %s [%s %ld] mp 0x%p\n",
+			(char *)ktep->val[1],
+			ktep->val[2] ? (char *)ktep->val[2] : "",
+			(long)ktep->val[0] >> 16,
+			(xfs_mount_t *)ktep->val[3]);
+		kdb_printf("      agno %lu slot %lu tp 0x%p\n",
+			(unsigned long)ktep->val[4],
+			(unsigned long)ktep->val[7],
+			(xfs_trans_t *)ktep->val[8]);
+		break;
+	case XFS_ALLOC_KTRACE_BUSY:
+		kdb_printf("busy %s [%s %ld] mp 0x%p\n",
+			(char *)ktep->val[1],
+			ktep->val[2] ? (char *)ktep->val[2] : "",
+			(long)ktep->val[0] >> 16,
+			(xfs_mount_t *)ktep->val[3]);
+		kdb_printf("      agno %lu agbno %lu len %lu slot %lu tp 0x%p\n",
+			(unsigned long)ktep->val[4],
+			(unsigned long)ktep->val[5],
+			(unsigned long)ktep->val[6],
+			(unsigned long)ktep->val[7],
+			(xfs_trans_t *)ktep->val[8]);
+		break;
+	case XFS_ALLOC_KTRACE_BUSYSEARCH:
+		kdb_printf("busy-search %s [%s %ld] mp 0x%p\n",
+			(char *)ktep->val[1],
+			ktep->val[2] ? (char *)ktep->val[2] : "",
+			(long)ktep->val[0] >> 16,
+			(xfs_mount_t *)ktep->val[3]);
+		kdb_printf("      agno %ld agbno %ld len %ld slot %ld tp 0x%p\n",
+			(unsigned long)ktep->val[4],
+			(unsigned long)ktep->val[5],
+			(unsigned long)ktep->val[6],
+			(unsigned long)ktep->val[7],
+			(xfs_trans_t *)ktep->val[8]);
+		break;
+	default:
+		kdb_printf("unknown alloc trace record\n");
+		break;
+	}
+	return 1;
+}
+#endif /* XFS_ALLOC_TRACE */
+
+#ifdef XFS_ATTR_TRACE
+/*
+ * Print an attribute trace buffer entry.
+ */
+static int
+xfs_attr_trace_entry(ktrace_entry_t *ktep)
+{
+	static char *attr_arg_flags[] = {
+		"DONTFOLLOW",	/* 0x0001 */
+		"ROOT",		/* 0x0002 */
+		"TRUSTED",	/* 0x0004 */
+		"SECURE",	/* 0x0008 */
+		"CREATE",	/* 0x0010 */
+		"REPLACE",	/* 0x0020 */
+		"?",		/* 0x0040 */
+		"?",		/* 0x0080 */
+		"SYSTEM",	/* 0x0100 */
+		"?",		/* 0x0200 */
+		"?",		/* 0x0400 */
+		"?",		/* 0x0800 */
+		"KERNOTIME",	/* 0x1000 */
+		"KERNOVAL",	/* 0x2000 */
+		"KERNAMELS",	/* 0x4000 */
+		"KERNFULLS",	/* 0x8000 */
+		NULL
+	};
+
+	if (!ktep->val[0])
+		return 0;
+
+	qprintf("-- %s: cursor h/b/o 0x%lx/0x%lx/%lu, dupcnt %lu, dp 0x%p\n",
+		 (char *)ktep->val[1],
+		 (unsigned long)ktep->val[3],
+		 (unsigned long)ktep->val[4],
+		 (unsigned long)ktep->val[5],
+		 (unsigned long)ktep->val[11],
+		 (xfs_inode_t *)ktep->val[2]);
+	qprintf("   alist 0x%p, size %lu, count %lu, firstu %lu, Llen %lu",
+		 (attrlist_t *)ktep->val[6],
+		 (unsigned long)ktep->val[7],
+		 (unsigned long)ktep->val[8],
+		 (unsigned long)ktep->val[9],
+		 (unsigned long)ktep->val[10]);
+	printflags((__psunsigned_t)(ktep->val[12]), attr_arg_flags, ", flags");
+	qprintf("\n");
+
+	switch ((__psint_t)ktep->val[0]) {
+	case XFS_ATTR_KTRACE_L_C:
+		break;
+	case XFS_ATTR_KTRACE_L_CN:
+		qprintf("   node: count %lu, 1st hash 0x%lx, last hash 0x%lx\n",
+			 (unsigned long)ktep->val[13],
+			 (unsigned long)ktep->val[14],
+			 (unsigned long)ktep->val[15]);
+		break;
+	case XFS_ATTR_KTRACE_L_CB:
+		qprintf("   btree: hash 0x%lx, blkno 0x%lx\n",
+			 (unsigned long)ktep->val[13],
+			 (unsigned long)ktep->val[14]);
+		break;
+	case XFS_ATTR_KTRACE_L_CL:
+		qprintf("   leaf: count %ld, 1st hash 0x%lx, last hash 0x%lx\n",
+			 (unsigned long)ktep->val[13],
+			 (unsigned long)ktep->val[14],
+			 (unsigned long)ktep->val[15]);
+		break;
+	default:
+		qprintf("   unknown attr trace record format\n");
+		break;
+	}
+	return 1;
+}
+#endif /* XFS_ATTR_TRACE */
+
+#ifdef XFS_BMAP_TRACE
+/*
+ * Print xfs bmap extent trace buffer entry.
+ */
+static int
+xfs_bmap_trace_entry(ktrace_entry_t *ktep)
+{
+	xfs_dfsbno_t	    b;
+	xfs_dfilblks_t	  c;
+	xfs_inode_t	     *ip;
+	xfs_ino_t	       ino;
+	xfs_dfiloff_t	   o;
+	int		     flag;
+	int		     opcode;
+	static char	     *ops[] = { "del", "ins", "pre", "post" };
+	xfs_bmbt_rec_32_t       r;
+	int		     whichfork;
+
+	opcode = ((__psint_t)ktep->val[0]) & 0xffff;
+	if (opcode == 0)
+		return 0;
+	whichfork = ((__psint_t)ktep->val[0]) >> 16;
+	ip = (xfs_inode_t *)ktep->val[3];
+	ino = ((xfs_ino_t)(unsigned long)ktep->val[6] << 32) |
+		((xfs_ino_t)(unsigned long)ktep->val[7]);
+	qprintf("%s %s:%s ip %p ino %s %cf\n",
+		ops[opcode - 1], (char *)ktep->val[1],
+		(char *)ktep->val[2], ip, xfs_fmtino(ino, ip->i_mount),
+		"da"[whichfork]);
+	r.l0 = (xfs_bmbt_rec_base_t)(unsigned long)ktep->val[8];
+	r.l1 = (xfs_bmbt_rec_base_t)(unsigned long)ktep->val[9];
+	r.l2 = (xfs_bmbt_rec_base_t)(unsigned long)ktep->val[10];
+	r.l3 = (xfs_bmbt_rec_base_t)(unsigned long)ktep->val[11];
+	xfs_convert_extent(&r, &o, &b, &c, &flag);
+	qprintf(" idx %ld offset %lld block %s",
+		(long)ktep->val[4], o,
+		xfs_fmtfsblock((xfs_fsblock_t)b, ip->i_mount));
+	qprintf(" count %lld flag %d\n", c, flag);
+	if ((__psint_t)ktep->val[5] != 2)
+		return 1;
+	r.l0 = (xfs_bmbt_rec_base_t)(unsigned long)ktep->val[12];
+	r.l1 = (xfs_bmbt_rec_base_t)(unsigned long)ktep->val[13];
+	r.l2 = (xfs_bmbt_rec_base_t)(unsigned long)ktep->val[14];
+	r.l3 = (xfs_bmbt_rec_base_t)(unsigned long)ktep->val[15];
+	xfs_convert_extent(&r, &o, &b, &c, &flag);
+	qprintf(" offset %lld block %s", o,
+		xfs_fmtfsblock((xfs_fsblock_t)b, ip->i_mount));
+	qprintf(" count %lld flag %d\n", c, flag);
+	return 1;
+}
+
+/*
+ * Print xfs bmap btree trace buffer entry.
+ */
+static int
+xfs_bmbt_trace_entry(
+	ktrace_entry_t	  *ktep)
+{
+	int			line;
+	xfs_bmbt_rec_32_t	r;
+	xfs_bmbt_irec_t		s;
+	int			type;
+	int			whichfork;
+
+	type = (__psint_t)ktep->val[0] & 0xff;
+	if (type == 0)
+		return 0;
+	whichfork = ((__psint_t)ktep->val[0] >> 8) & 0xff;
+	line = ((__psint_t)ktep->val[0] >> 16) & 0xffff;
+	qprintf("%s[%s@%d] ip 0x%p %cf cur 0x%p\n",
+		(char *)ktep->val[1],
+		(char *)ktep->val[2],
+		line,
+		(xfs_inode_t *)ktep->val[3],
+		"da"[whichfork],
+		(xfs_btree_cur_t *)ktep->val[4]);
+	switch (type) {
+	case XFS_BMBT_KTRACE_ARGBI:
+		qprintf(" buf 0x%p i %ld\n",
+			(xfs_buf_t *)ktep->val[5],
+			(long)ktep->val[6]);
+		break;
+	case XFS_BMBT_KTRACE_ARGBII:
+		qprintf(" buf 0x%p i0 %ld i1 %ld\n",
+			(xfs_buf_t *)ktep->val[5],
+			(long)ktep->val[6],
+			(long)ktep->val[7]);
+		break;
+	case XFS_BMBT_KTRACE_ARGFFFI:
+		qprintf(" o 0x%x%08x b 0x%x%08x i 0x%x%08x j %ld\n",
+			(unsigned int)(long)ktep->val[5],
+			(unsigned int)(long)ktep->val[6],
+			(unsigned int)(long)ktep->val[7],
+			(unsigned int)(long)ktep->val[8],
+			(unsigned int)(long)ktep->val[9],
+			(unsigned int)(long)ktep->val[10],
+			(long)ktep->val[11]);
+		break;
+	case XFS_BMBT_KTRACE_ARGI:
+		qprintf(" i 0x%lx\n",
+			(long)ktep->val[5]);
+		break;
+	case XFS_BMBT_KTRACE_ARGIFK:
+		qprintf(" i 0x%lx f 0x%x%08x o 0x%x%08x\n",
+			(long)ktep->val[5],
+			(unsigned int)(long)ktep->val[6],
+			(unsigned int)(long)ktep->val[7],
+			(unsigned int)(long)ktep->val[8],
+			(unsigned int)(long)ktep->val[9]);
+		break;
+	case XFS_BMBT_KTRACE_ARGIFR:
+		qprintf(" i 0x%lx f 0x%x%08x ",
+			(long)ktep->val[5],
+			(unsigned int)(long)ktep->val[6],
+			(unsigned int)(long)ktep->val[7]);
+		s.br_startoff = (xfs_fileoff_t)
+			(((xfs_dfiloff_t)(unsigned long)ktep->val[8] << 32) |
+				(xfs_dfiloff_t)(unsigned long)ktep->val[9]);
+		s.br_startblock = (xfs_fsblock_t)
+			(((xfs_dfsbno_t)(unsigned long)ktep->val[10] << 32) |
+				(xfs_dfsbno_t)(unsigned long)ktep->val[11]);
+		s.br_blockcount = (xfs_filblks_t)
+			(((xfs_dfilblks_t)(unsigned long)ktep->val[12] << 32) |
+				(xfs_dfilblks_t)(unsigned long)ktep->val[13]);
+		xfsidbg_xbirec(&s);
+		break;
+	case XFS_BMBT_KTRACE_ARGIK:
+		qprintf(" i 0x%lx o 0x%x%08x\n",
+			(long)ktep->val[5],
+			(unsigned int)(long)ktep->val[6],
+			(unsigned int)(long)ktep->val[7]);
+		break;
+	case XFS_BMBT_KTRACE_CUR:
+		qprintf(" nlevels %ld flags %ld allocated %ld ",
+			((long)ktep->val[5] >> 24) & 0xff,
+			((long)ktep->val[5] >> 16) & 0xff,
+			(long)ktep->val[5] & 0xffff);
+		r.l0 = (xfs_bmbt_rec_base_t)(unsigned long)ktep->val[6];
+		r.l1 = (xfs_bmbt_rec_base_t)(unsigned long)ktep->val[7];
+		r.l2 = (xfs_bmbt_rec_base_t)(unsigned long)ktep->val[8];
+		r.l3 = (xfs_bmbt_rec_base_t)(unsigned long)ktep->val[9];
+		xfsidbg_xbrec((xfs_bmbt_rec_64_t *)&r);
+		qprintf(" bufs 0x%p 0x%p 0x%p 0x%p ",
+			(xfs_buf_t *)ktep->val[10],
+			(xfs_buf_t *)ktep->val[11],
+			(xfs_buf_t *)ktep->val[12],
+			(xfs_buf_t *)ktep->val[13]);
+		qprintf("ptrs %ld %ld %ld %ld\n",
+			(long)ktep->val[14] >> 16,
+			(long)ktep->val[14] & 0xffff,
+			(long)ktep->val[15] >> 16,
+			(long)ktep->val[15] & 0xffff);
+		break;
+	default:
+		qprintf("unknown bmbt trace record\n");
+		break;
+	}
+	return 1;
+}
+#endif
+
+/*
+ * Print an xfs in-inode bmap btree root.
+ */
+static void
+xfs_broot(xfs_inode_t *ip, xfs_ifork_t *f)
+{
+	xfs_bmbt_block_t	*broot;
+	int			format;
+	int			i;
+	xfs_bmbt_key_t		*kp;
+	xfs_bmbt_ptr_t		*pp;
+
+	format = f == &ip->i_df ? ip->i_d.di_format : ip->i_d.di_aformat;
+	if ((f->if_flags & XFS_IFBROOT) == 0 ||
+	    format != XFS_DINODE_FMT_BTREE) {
+		kdb_printf("inode 0x%p not btree format\n", ip);
+		return;
+	}
+	broot = f->if_broot;
+	kdb_printf("block @0x%p magic %x level %d numrecs %d\n",
+		broot, INT_GET(broot->bb_magic, ARCH_CONVERT), INT_GET(broot->bb_level, ARCH_CONVERT), INT_GET(broot->bb_numrecs, ARCH_CONVERT));
+	kp = XFS_BMAP_BROOT_KEY_ADDR(broot, 1, f->if_broot_bytes);
+	pp = XFS_BMAP_BROOT_PTR_ADDR(broot, 1, f->if_broot_bytes);
+	for (i = 1; i <= INT_GET(broot->bb_numrecs, ARCH_CONVERT); i++)
+		kdb_printf("\t%d: startoff %Ld ptr %Lx %s\n",
+			i, INT_GET(kp[i - 1].br_startoff, ARCH_CONVERT), INT_GET(pp[i - 1], ARCH_CONVERT),
+			xfs_fmtfsblock(INT_GET(pp[i - 1], ARCH_CONVERT), ip->i_mount));
+}
+
+/*
+ * Print allocation btree block.
+ */
+static void
+xfs_btalloc(xfs_alloc_block_t *bt, int bsz)
+{
+	int i;
+
+	kdb_printf("magic 0x%x level %d numrecs %d leftsib 0x%x rightsib 0x%x\n",
+		INT_GET(bt->bb_magic, ARCH_CONVERT), INT_GET(bt->bb_level, ARCH_CONVERT), INT_GET(bt->bb_numrecs, ARCH_CONVERT),
+		INT_GET(bt->bb_leftsib, ARCH_CONVERT), INT_GET(bt->bb_rightsib, ARCH_CONVERT));
+	if (INT_ISZERO(bt->bb_level, ARCH_CONVERT)) {
+
+		for (i = 1; i <= INT_GET(bt->bb_numrecs, ARCH_CONVERT); i++) {
+			xfs_alloc_rec_t *r;
+
+			r = XFS_BTREE_REC_ADDR(bsz, xfs_alloc, bt, i, 0);
+			kdb_printf("rec %d startblock 0x%x blockcount %d\n",
+				i, INT_GET(r->ar_startblock, ARCH_CONVERT), INT_GET(r->ar_blockcount, ARCH_CONVERT));
+		}
+	} else {
+		int mxr;
+
+		mxr = XFS_BTREE_BLOCK_MAXRECS(bsz, xfs_alloc, 0);
+		for (i = 1; i <= INT_GET(bt->bb_numrecs, ARCH_CONVERT); i++) {
+			xfs_alloc_key_t *k;
+			xfs_alloc_ptr_t *p;
+
+			k = XFS_BTREE_KEY_ADDR(bsz, xfs_alloc, bt, i, mxr);
+			p = XFS_BTREE_PTR_ADDR(bsz, xfs_alloc, bt, i, mxr);
+			kdb_printf("key %d startblock 0x%x blockcount %d ptr 0x%x\n",
+				i, INT_GET(k->ar_startblock, ARCH_CONVERT), INT_GET(k->ar_blockcount, ARCH_CONVERT), *p);
+		}
+	}
+}
+
+/*
+ * Print a bmap btree block.
+ */
+static void
+xfs_btbmap(xfs_bmbt_block_t *bt, int bsz)
+{
+	int i;
+
+	kdb_printf("magic 0x%x level %d numrecs %d leftsib %Lx ",
+		INT_GET(bt->bb_magic, ARCH_CONVERT),
+		INT_GET(bt->bb_level, ARCH_CONVERT),
+		INT_GET(bt->bb_numrecs, ARCH_CONVERT),
+		INT_GET(bt->bb_leftsib, ARCH_CONVERT));
+	kdb_printf("rightsib %Lx\n", INT_GET(bt->bb_rightsib, ARCH_CONVERT));
+	if (INT_ISZERO(bt->bb_level, ARCH_CONVERT)) {
+		for (i = 1; i <= INT_GET(bt->bb_numrecs, ARCH_CONVERT); i++) {
+			xfs_bmbt_rec_t *r;
+			xfs_bmbt_irec_t	irec;
+
+			r = (xfs_bmbt_rec_t *)XFS_BTREE_REC_ADDR(bsz,
+				xfs_bmbt, bt, i, 0);
+
+			xfs_bmbt_disk_get_all((xfs_bmbt_rec_t *)r, &irec);
+			kdb_printf("rec %d startoff %Ld startblock %Lx blockcount %Ld flag %d\n",
+				i, irec.br_startoff,
+				(__uint64_t)irec.br_startblock,
+				irec.br_blockcount, irec.br_state);
+		}
+	} else {
+		int mxr;
+
+		mxr = XFS_BTREE_BLOCK_MAXRECS(bsz, xfs_bmbt, 0);
+		for (i = 1; i <= INT_GET(bt->bb_numrecs, ARCH_CONVERT); i++) {
+			xfs_bmbt_key_t *k;
+			xfs_bmbt_ptr_t *p;
+
+			k = XFS_BTREE_KEY_ADDR(bsz, xfs_bmbt, bt, i, mxr);
+			p = XFS_BTREE_PTR_ADDR(bsz, xfs_bmbt, bt, i, mxr);
+			kdb_printf("key %d startoff %Ld ",
+				i, INT_GET(k->br_startoff, ARCH_CONVERT));
+			kdb_printf("ptr %Lx\n", INT_GET(*p, ARCH_CONVERT));
+		}
+	}
+}
+
+/*
+ * Print an inode btree block.
+ */
+static void
+xfs_btino(xfs_inobt_block_t *bt, int bsz)
+{
+	int i;
+
+	kdb_printf("magic 0x%x level %d numrecs %d leftsib 0x%x rightsib 0x%x\n",
+		INT_GET(bt->bb_magic, ARCH_CONVERT),
+		INT_GET(bt->bb_level, ARCH_CONVERT),
+		INT_GET(bt->bb_numrecs, ARCH_CONVERT),
+		INT_GET(bt->bb_leftsib, ARCH_CONVERT),
+		INT_GET(bt->bb_rightsib, ARCH_CONVERT));
+	if (INT_ISZERO(bt->bb_level, ARCH_CONVERT)) {
+
+		for (i = 1; i <= INT_GET(bt->bb_numrecs, ARCH_CONVERT); i++) {
+			xfs_inobt_rec_t *r;
+
+			r = XFS_BTREE_REC_ADDR(bsz, xfs_inobt, bt, i, 0);
+			kdb_printf("rec %d startino 0x%x freecount %d, free %Lx\n",
+				i, INT_GET(r->ir_startino, ARCH_CONVERT),
+				INT_GET(r->ir_freecount, ARCH_CONVERT),
+				INT_GET(r->ir_free, ARCH_CONVERT));
+		}
+	} else {
+		int mxr;
+
+		mxr = XFS_BTREE_BLOCK_MAXRECS(bsz, xfs_inobt, 0);
+		for (i = 1; i <= INT_GET(bt->bb_numrecs, ARCH_CONVERT); i++) {
+			xfs_inobt_key_t *k;
+			xfs_inobt_ptr_t *p;
+
+			k = XFS_BTREE_KEY_ADDR(bsz, xfs_inobt, bt, i, mxr);
+			p = XFS_BTREE_PTR_ADDR(bsz, xfs_inobt, bt, i, mxr);
+			kdb_printf("key %d startino 0x%x ptr 0x%x\n",
+				i, INT_GET(k->ir_startino, ARCH_CONVERT),
+				INT_GET(*p, ARCH_CONVERT));
+		}
+	}
+}
+
+/*
+ * Print a buf log item.
+ */
+static void
+xfs_buf_item_print(xfs_buf_log_item_t *blip, int summary)
+{
+	static char *bli_flags[] = {
+		"hold",		/* 0x1 */
+		"dirty",	/* 0x2 */
+		"stale",	/* 0x4 */
+		"logged",	/* 0x8 */
+		"ialloc",	/* 0x10 */
+		"inode_stale",  /* 0x20 */
+		0
+		};
+	static char *blf_flags[] = {
+		"inode",	/* 0x1 */
+		"cancel",	/* 0x2 */
+		0
+		};
+
+	if (summary) {
+		kdb_printf("buf 0x%p blkno 0x%Lx ", blip->bli_buf,
+			     blip->bli_format.blf_blkno);
+		printflags(blip->bli_flags, bli_flags, "flags:");
+		kdb_printf("\n   ");
+		xfsidbg_xbuf_real(blip->bli_buf, 1);
+		return;
+	}
+	kdb_printf("buf 0x%p recur %d refcount %d flags:",
+		blip->bli_buf, blip->bli_recur,
+		atomic_read(&blip->bli_refcount));
+	printflags(blip->bli_flags, bli_flags, NULL);
+	kdb_printf("\n");
+	kdb_printf("size %d blkno 0x%Lx len 0x%x map size %d map 0x%p\n",
+		blip->bli_format.blf_size, blip->bli_format.blf_blkno,
+		(uint) blip->bli_format.blf_len, blip->bli_format.blf_map_size,
+		&(blip->bli_format.blf_data_map[0]));
+	kdb_printf("blf flags: ");
+	printflags((uint)blip->bli_format.blf_flags, blf_flags, NULL);
+#ifdef XFS_TRANS_DEBUG
+	kdb_printf("orig 0x%x logged 0x%x",
+		blip->bli_orig, blip->bli_logged);
+#endif
+	kdb_printf("\n");
+}
+
+#ifdef XFS_BMAP_TRACE
+/*
+ * Convert an external extent descriptor to internal form.
+ */
+static void
+xfs_convert_extent(xfs_bmbt_rec_32_t *rp, xfs_dfiloff_t *op, xfs_dfsbno_t *sp,
+		   xfs_dfilblks_t *cp, int *fp)
+{
+	xfs_dfiloff_t o;
+	xfs_dfsbno_t s;
+	xfs_dfilblks_t c;
+	int flag;
+
+	flag = (((xfs_dfiloff_t)rp->l0) >> 31) & 1;
+	o = ((((xfs_dfiloff_t)rp->l0) & 0x7fffffff) << 23) |
+	    (((xfs_dfiloff_t)rp->l1) >> 9);
+	s = (((xfs_dfsbno_t)(rp->l1 & 0x000001ff)) << 43) |
+	    (((xfs_dfsbno_t)rp->l2) << 11) |
+	    (((xfs_dfsbno_t)rp->l3) >> 21);
+	c = (xfs_dfilblks_t)(rp->l3 & 0x001fffff);
+	*op = o;
+	*sp = s;
+	*cp = c;
+	*fp = flag;
+}
+#endif
+
+#ifdef XFS_RW_TRACE
+/*
+ * Print itrunc entry trace.
+ */
+static void
+xfs_ctrunc_trace_entry(ktrace_entry_t	*ktep)
+{
+	qprintf("ip 0x%p cpu %ld\n",
+		(xfs_inode_t *)(unsigned long)ktep->val[1], (long)ktep->val[2]);
+}
+#endif
+
+/*
+ * Print an xfs_da_state_path structure.
+ */
+static void
+xfs_dastate_path(xfs_da_state_path_t *p)
+{
+	int i;
+
+	kdb_printf("active %d\n", p->active);
+	for (i = 0; i < XFS_DA_NODE_MAXDEPTH; i++) {
+		kdb_printf(" blk %d bp 0x%p blkno 0x%x",
+			i, p->blk[i].bp, p->blk[i].blkno);
+		kdb_printf(" index %d hashval 0x%x ",
+			p->blk[i].index, (uint_t)p->blk[i].hashval);
+		switch(p->blk[i].magic) {
+		case XFS_DA_NODE_MAGIC:		kdb_printf("NODE\n");	break;
+		case XFS_DIR_LEAF_MAGIC:	kdb_printf("DIR\n");	break;
+		case XFS_ATTR_LEAF_MAGIC:	kdb_printf("ATTR\n");	break;
+		case XFS_DIR2_LEAFN_MAGIC:	kdb_printf("DIR2\n");	break;
+		default:			kdb_printf("type ?\n");	break;
+		}
+	}
+}
+
+#ifdef XFS_DIR_TRACE
+/*
+ * Print a xfs directory trace buffer entry.
+ */
+static int
+xfs_dir_trace_entry(ktrace_entry_t *ktep)
+{
+	xfs_mount_t *mp;
+	__uint32_t hash;
+	xfs_off_t cookie;
+
+	if (!ktep->val[0] || !ktep->val[1])
+		return 0;
+
+	mp = (xfs_mount_t *)ktep->val[3];
+	cookie = (__psunsigned_t)ktep->val[4];
+	cookie <<= 32;
+	cookie |= (__psunsigned_t)ktep->val[5];
+	qprintf("%s -- dp=0x%p b/e/h=%ld/%ld/0x%08lx resid=0x%lx ",
+		    (char *)ktep->val[1],
+		    (xfs_inode_t *)ktep->val[2],
+		    (long)XFS_DA_COOKIE_BNO(mp, cookie),
+		    (long)XFS_DA_COOKIE_ENTRY(mp, cookie),
+		    (unsigned long)XFS_DA_COOKIE_HASH(mp, cookie),
+		    (long)ktep->val[6]);
+
+	switch ((__psint_t)ktep->val[0]) {
+	case XFS_DIR_KTRACE_G_DU:
+		break;
+	case XFS_DIR_KTRACE_G_DUB:
+		qprintf("bno=%ld", (long)ktep->val[7]);
+		break;
+	case XFS_DIR_KTRACE_G_DUN:
+		qprintf("forw=%ld, cnt=%ld, 0x%08lx - 0x%08lx",
+			      (long)ktep->val[7],
+			      (long)ktep->val[8],
+			      (unsigned long)ktep->val[9],
+			      (unsigned long)ktep->val[10]);
+		break;
+	case XFS_DIR_KTRACE_G_DUL:
+		qprintf("forw=%ld, cnt=%ld, 0x%08lx - 0x%08lx",
+			      (long)ktep->val[7],
+			      (long)ktep->val[8],
+			      (unsigned long)ktep->val[9],
+			      (unsigned long)ktep->val[10]);
+		break;
+	case XFS_DIR_KTRACE_G_DUE:
+		qprintf("entry hashval 0x%08lx", (unsigned long)ktep->val[7]);
+		break;
+	case XFS_DIR_KTRACE_G_DUC:
+		cookie = (__psunsigned_t)ktep->val[7];
+		cookie <<= 32;
+		cookie |= (__psunsigned_t)ktep->val[8];
+		hash = XFS_DA_COOKIE_HASH(mp, cookie);
+		qprintf("b/e/h=%ld/%ld/0x%08x",
+		    (long)XFS_DA_COOKIE_BNO(mp, cookie),
+		    (long)XFS_DA_COOKIE_ENTRY(mp, cookie),
+		    hash);
+		break;
+	default:
+		qprintf("unknown dir trace record format");
+		break;
+	}
+	return 1;
+}
+#endif
+
+#ifdef XFS_DIR2_TRACE
+/*
+ * Print a xfs v2 directory trace buffer entry.
+ */
+static int
+xfs_dir2_trace_entry(ktrace_entry_t *ktep)
+{
+	char	    *cp;
+	int	     i;
+	int	     len;
+
+	if (!ktep->val[0])
+		return 0;
+	cp = (char *)&ktep->val[10];
+	qprintf("%s: '", (char *)ktep->val[1]);
+	len = min((__psint_t)ktep->val[9], (__psint_t)sizeof(ktep->val[10])*6);
+	for (i = 0; i < len; i++)
+		qprintf("%c", cp[i]);
+	qprintf("'(%ld)", (long)ktep->val[9]);
+	if ((__psunsigned_t)ktep->val[0] != XFS_DIR2_KTRACE_ARGS_BIBII)
+		qprintf(" hashval 0x%llx inumber %lld dp 0x%p tp 0x%p check %d",
+			(__uint64_t)(unsigned long)ktep->val[2],
+			(__int64_t)(unsigned long)ktep->val[3],
+			ktep->val[4], ktep->val[5],
+			(int)(__psint_t)ktep->val[6]);
+	switch ((__psunsigned_t)ktep->val[0]) {
+	case XFS_DIR2_KTRACE_ARGS:
+		break;
+	case XFS_DIR2_KTRACE_ARGS_B:
+		qprintf(" bp 0x%p", ktep->val[7]);
+		break;
+	case XFS_DIR2_KTRACE_ARGS_BB:
+		qprintf(" lbp 0x%p dbp 0x%p", ktep->val[7], ktep->val[8]);
+		break;
+	case XFS_DIR2_KTRACE_ARGS_BIBII:
+		qprintf(" dp 0x%p tp 0x%p srcbp 0x%p srci %d dstbp 0x%p dsti %d count %d",
+			ktep->val[2], ktep->val[3], ktep->val[4],
+			(int)(__psint_t)ktep->val[5], ktep->val[6],
+			(int)(__psint_t)ktep->val[7],
+			(int)(__psint_t)ktep->val[8]);
+		break;
+	case XFS_DIR2_KTRACE_ARGS_DB:
+		qprintf(" db 0x%x bp 0x%p",
+			(xfs_dir2_db_t)(unsigned long)ktep->val[7],
+			ktep->val[8]);
+		break;
+	case XFS_DIR2_KTRACE_ARGS_I:
+		qprintf(" i 0x%lx", (unsigned long)ktep->val[7]);
+		break;
+	case XFS_DIR2_KTRACE_ARGS_S:
+		qprintf(" s 0x%x", (int)(__psint_t)ktep->val[7]);
+		break;
+	case XFS_DIR2_KTRACE_ARGS_SB:
+		qprintf(" s 0x%x bp 0x%p", (int)(__psint_t)ktep->val[7],
+			ktep->val[8]);
+		break;
+	default:
+		qprintf("unknown dirv2 trace record format");
+		break;
+	}
+	return 1;
+}
+#endif
+
+/*
+ * Print an efd log item.
+ */
+static void
+xfs_efd_item_print(xfs_efd_log_item_t *efdp, int summary)
+{
+	int		i;
+	xfs_extent_t	*ep;
+
+	if (summary) {
+		kdb_printf("Extent Free Done: ID 0x%Lx nextents %d (at 0x%p)\n",
+				efdp->efd_format.efd_efi_id,
+				efdp->efd_format.efd_nextents, efdp);
+		return;
+	}
+	kdb_printf("size %d nextents %d next extent %d efip 0x%p\n",
+		efdp->efd_format.efd_size, efdp->efd_format.efd_nextents,
+		efdp->efd_next_extent, efdp->efd_efip);
+	kdb_printf("efi_id 0x%Lx\n", efdp->efd_format.efd_efi_id);
+	kdb_printf("efd extents:\n");
+	ep = &(efdp->efd_format.efd_extents[0]);
+	for (i = 0; i < efdp->efd_next_extent; i++, ep++) {
+		kdb_printf("    block %Lx len %d\n",
+			ep->ext_start, ep->ext_len);
+	}
+}
+
+/*
+ * Print an efi log item.
+ */
+static void
+xfs_efi_item_print(xfs_efi_log_item_t *efip, int summary)
+{
+	int		i;
+	xfs_extent_t	*ep;
+	static char *efi_flags[] = {
+		"recovered",	/* 0x1 */
+		"committed",	/* 0x2 */
+		"cancelled",	/* 0x4 */
+		0,
+		};
+
+	if (summary) {
+		kdb_printf("Extent Free Intention: ID 0x%Lx nextents %d (at 0x%p)\n",
+				efip->efi_format.efi_id,
+				efip->efi_format.efi_nextents, efip);
+		return;
+	}
+	kdb_printf("size %d nextents %d next extent %d\n",
+		efip->efi_format.efi_size, efip->efi_format.efi_nextents,
+		efip->efi_next_extent);
+	kdb_printf("id %Lx", efip->efi_format.efi_id);
+	printflags(efip->efi_flags, efi_flags, "flags :");
+	kdb_printf("\n");
+	kdb_printf("efi extents:\n");
+	ep = &(efip->efi_format.efi_extents[0]);
+	for (i = 0; i < efip->efi_next_extent; i++, ep++) {
+		kdb_printf("    block %Lx len %d\n",
+			ep->ext_start, ep->ext_len);
+	}
+}
+
+/*
+ * Format inode "format" into a static buffer & return it.
+ */
+static char *
+xfs_fmtformat(xfs_dinode_fmt_t f)
+{
+	static char *t[] = {
+		"dev",
+		"local",
+		"extents",
+		"btree",
+		"uuid"
+	};
+
+	return t[f];
+}
+
+/*
+ * Format fsblock number into a static buffer & return it.
+ */
+static char *
+xfs_fmtfsblock(xfs_fsblock_t bno, xfs_mount_t *mp)
+{
+	static char rval[50];
+
+	if (bno == NULLFSBLOCK)
+		sprintf(rval, "NULLFSBLOCK");
+	else if (ISNULLSTARTBLOCK(bno))
+		sprintf(rval, "NULLSTARTBLOCK(%Ld)", STARTBLOCKVAL(bno));
+	else if (mp)
+		sprintf(rval, "%Ld[%x:%x]", (xfs_dfsbno_t)bno,
+			XFS_FSB_TO_AGNO(mp, bno), XFS_FSB_TO_AGBNO(mp, bno));
+	else
+		sprintf(rval, "%Ld", (xfs_dfsbno_t)bno);
+	return rval;
+}
+
+/*
+ * Format inode number into a static buffer & return it.
+ */
+static char *
+xfs_fmtino(xfs_ino_t ino, xfs_mount_t *mp)
+{
+	static char rval[50];
+
+	if (mp)
+		sprintf(rval, "%llu[%x:%x:%x]",
+			(unsigned long long) ino,
+			XFS_INO_TO_AGNO(mp, ino),
+			XFS_INO_TO_AGBNO(mp, ino),
+			XFS_INO_TO_OFFSET(mp, ino));
+	else
+		sprintf(rval, "%llu", (unsigned long long) ino);
+	return rval;
+}
+
+/*
+ * Format an lsn for printing into a static buffer & return it.
+ */
+static char *
+xfs_fmtlsn(xfs_lsn_t *lsnp)
+{
+	uint		*wordp;
+	uint		*word2p;
+	static char	buf[20];
+
+	wordp = (uint *)lsnp;
+	word2p = wordp++;
+	sprintf(buf, "[%u:%u]", *wordp, *word2p);
+
+	return buf;
+}
+
+/*
+ * Format file mode into a static buffer & return it.
+ */
+static char *
+xfs_fmtmode(int m)
+{
+	static char rval[16];
+
+	sprintf(rval, "%c%c%c%c%c%c%c%c%c%c%c%c%c",
+		"?fc?dxb?r?l?S?m?"[(m & S_IFMT) >> 12],
+		m & S_ISUID ? 'u' : '-',
+		m & S_ISGID ? 'g' : '-',
+		m & S_ISVTX ? 'v' : '-',
+		m & S_IRUSR ? 'r' : '-',
+		m & S_IWUSR ? 'w' : '-',
+		m & S_IXUSR ? 'x' : '-',
+		m & S_IRGRP ? 'r' : '-',
+		m & S_IWGRP ? 'w' : '-',
+		m & S_IXGRP ? 'x' : '-',
+		m & S_IROTH ? 'r' : '-',
+		m & S_IWOTH ? 'w' : '-',
+		m & S_IXOTH ? 'x' : '-');
+	return rval;
+}
+
+/*
+ * Format a size into a static buffer & return it.
+ */
+static char *
+xfs_fmtsize(size_t i)
+{
+	static char rval[20];
+
+	/* size_t is 32 bits in 32-bit kernel, 64 bits in 64-bit kernel */
+	sprintf(rval, "0x%lx", (unsigned long) i);
+	return rval;
+}
+
+/*
+ * Format a uuid into a static buffer & return it.
+ */
+static char *
+xfs_fmtuuid(uuid_t *uu)
+{
+	static char rval[40];
+	char	*o	  = rval;
+	char	*i	  = (unsigned char*)uu;
+	int	 b;
+
+	for (b=0;b<16;b++) {
+	    o+=sprintf(o, "%02x", *i++);
+	    if (b==3||b==5||b==7||b==9) *o++='-';
+	}
+	*o='\0';
+
+	return rval;
+}
+
+/*
+ * Print an inode log item.
+ */
+static void
+xfs_inode_item_print(xfs_inode_log_item_t *ilip, int summary)
+{
+	static char *ili_flags[] = {
+		"hold",		/* 0x1 */
+		"iolock excl",	/* 0x2 */
+		"iolock shrd",	/* 0x4 */
+		0
+		};
+	static char *ilf_fields[] = {
+		"core",		/* 0x001 */
+		"ddata",	/* 0x002 */
+		"dexts",	/* 0x004 */
+		"dbroot",	/* 0x008 */
+		"dev",		/* 0x010 */
+		"uuid",		/* 0x020 */
+		"adata",	/* 0x040 */
+		"aext",		/* 0x080 */
+		"abroot",	/* 0x100 */
+		0
+		};
+
+	if (summary) {
+		kdb_printf("inode 0x%p logged %d ",
+			ilip->ili_inode, ilip->ili_logged);
+		printflags(ilip->ili_flags, ili_flags, "flags:");
+		printflags(ilip->ili_format.ilf_fields, ilf_fields, "format:");
+		printflags(ilip->ili_last_fields, ilf_fields, "lastfield:");
+		kdb_printf("\n");
+		return;
+	}
+	kdb_printf("inode 0x%p ino 0x%llu pushbuf %d logged %d flags: ",
+		ilip->ili_inode, (unsigned long long) ilip->ili_format.ilf_ino,
+		ilip->ili_pushbuf_flag, ilip->ili_logged);
+	printflags(ilip->ili_flags, ili_flags, NULL);
+	kdb_printf("\n");
+	kdb_printf("ilock recur %d iolock recur %d ext buf 0x%p\n",
+		ilip->ili_ilock_recur, ilip->ili_iolock_recur,
+		ilip->ili_extents_buf);
+#ifdef XFS_TRANS_DEBUG
+	kdb_printf("root bytes %d root orig 0x%x\n",
+		ilip->ili_root_size, ilip->ili_orig_root);
+#endif
+	kdb_printf("size %d ", ilip->ili_format.ilf_size);
+	printflags(ilip->ili_format.ilf_fields, ilf_fields, "fields:");
+	printflags(ilip->ili_last_fields, ilf_fields, " last fields: ");
+	kdb_printf("\n");
+	kdb_printf(" flush lsn %s last lsn %s\n",
+		xfs_fmtlsn(&(ilip->ili_flush_lsn)),
+		xfs_fmtlsn(&(ilip->ili_last_lsn)));
+	kdb_printf("dsize %d, asize %d, rdev 0x%x\n",
+		ilip->ili_format.ilf_dsize,
+		ilip->ili_format.ilf_asize,
+		ilip->ili_format.ilf_u.ilfu_rdev);
+	kdb_printf("blkno 0x%Lx len 0x%x boffset 0x%x\n",
+		ilip->ili_format.ilf_blkno,
+		ilip->ili_format.ilf_len,
+		ilip->ili_format.ilf_boffset);
+}
+
+/*
+ * Print a dquot log item.
+ */
+/* ARGSUSED */
+static void
+xfs_dquot_item_print(xfs_dq_logitem_t *lip, int summary)
+{
+	kdb_printf("dquot 0x%p\n",
+		lip->qli_dquot);
+
+}
+
+/*
+ * Print a quotaoff log item.
+ */
+/* ARGSUSED */
+static void
+xfs_qoff_item_print(xfs_qoff_logitem_t *lip, int summary)
+{
+	kdb_printf("start qoff item 0x%p flags 0x%x\n",
+		lip->qql_start_lip, lip->qql_format.qf_flags);
+
+}
+
+/*
+ * Print buffer full of inodes.
+ */
+static void
+xfs_inodebuf(xfs_buf_t *bp)
+{
+	xfs_dinode_t *di;
+	int n, i;
+
+	n = XFS_BUF_COUNT(bp) >> 8;
+	for (i = 0; i < n; i++) {
+		di = (xfs_dinode_t *)xfs_buf_offset(bp,
+					i * 256);
+		xfs_prdinode(di, 0, ARCH_CONVERT);
+	}
+}
+
+#ifdef XFS_RW_TRACE
+/*
+ * Print iomap entry trace.
+ */
+static void
+xfs_iomap_enter_trace_entry(ktrace_entry_t *ktep)
+{
+	qprintf("ip 0x%p size 0x%x%x offset 0x%x%x count 0x%x\n",
+		ktep->val[1],
+		(unsigned int)(long)ktep->val[2],
+		(unsigned int)(long)ktep->val[3],
+		(unsigned int)(long)ktep->val[4],
+		(unsigned int)(long)ktep->val[5],
+		(unsigned int)(long)ktep->val[6]);
+	qprintf("io new size 0x%x%x\n",
+		(unsigned int)(long)ktep->val[7],
+		(unsigned int)(long)ktep->val[8]);
+}
+
+/*
+ * Print iomap map trace.
+ */
+static void
+xfs_iomap_map_trace_entry(ktrace_entry_t *ktep)
+{
+	static char *bmapi_flags[] = {
+		"read",		/* BMAPI_READ */
+		"write",	/* BMAPI_WRITE */
+		"allocate",	/* BMAPI_ALLOCATE */
+		"unwritten",	/* BMAPI_UNWRITTEN */
+		"ignstate",	/* BMAPI_IGNSTATE */
+		"direct",	/* BMAPI_DIRECT */
+		"mmap",		/* BMAPI_MMAP */
+		"sync",		/* BMAPI_SYNC */
+		"trylock",	/* BMAPI_TRYLOCK */
+		"device",	/* BMAPI_DEVICE */
+		0
+	};
+
+	qprintf("ip 0x%p size 0x%x%x offset 0x%x%x count 0x%x\n",
+		ktep->val[1],
+		(unsigned int)(long)ktep->val[2],
+		(unsigned int)(long)ktep->val[3],
+		(unsigned int)(long)ktep->val[4],
+		(unsigned int)(long)ktep->val[5],
+		(unsigned int)(long)ktep->val[6]);
+	printflags((__psint_t)ktep->val[7], bmapi_flags, "bmapi flags");
+	qprintf("iomap off 0x%x%x delta 0x%x bsize 0x%x bno 0x%x\n",
+		(unsigned int)(long)ktep->val[8],
+		(unsigned int)(long)ktep->val[9],
+		(unsigned int)(long)ktep->val[10],
+		(unsigned int)(long)ktep->val[11],
+		(unsigned int)(long)ktep->val[12]);
+	qprintf("imap off 0x%x count 0x%x block 0x%x\n",
+		(unsigned int)(long)ktep->val[13],
+		(unsigned int)(long)ktep->val[14],
+		(unsigned int)(long)ktep->val[15]);
+}
+
+/*
+ * Print itrunc entry trace.
+ */
+static void
+xfs_itrunc_trace_entry(ktrace_entry_t   *ktep)
+{
+	qprintf("ip 0x%p size 0x%x%x flag %ld new size 0x%x%x\n",
+		ktep->val[1],
+		(unsigned int)(long)ktep->val[2],
+		(unsigned int)(long)ktep->val[3],
+		(long)ktep->val[4],
+		(unsigned int)(long)ktep->val[5],
+		(unsigned int)(long)ktep->val[6]);
+	qprintf("toss start 0x%x%x toss finish 0x%x%x cpu id %ld\n",
+		(unsigned int)(long)ktep->val[7],
+		(unsigned int)(long)ktep->val[8],
+		(unsigned int)(long)ktep->val[9],
+		(unsigned int)(long)ktep->val[10],
+		(long)ktep->val[11]);
+}
+
+/*
+ * Print bunmap entry trace.
+ */
+static void
+xfs_bunmap_trace_entry(ktrace_entry_t   *ktep)
+{
+	static char *bunmapi_flags[] = {
+		"write",	/* 0x01 */
+		"delay",	/* 0x02 */
+		"entire",       /* 0x04 */
+		"metadata",     /* 0x08 */
+		"exact",	/* 0x10 */
+		"attrfork",     /* 0x20 */
+		"async",	/* 0x40 */
+		"rsvblocks",    /* 0x80 */
+		0
+	};
+
+	qprintf("ip 0x%p size 0x%x%x bno 0x%x%x len 0x%x cpu id %ld\n",
+		ktep->val[1],
+		(unsigned int)(long)ktep->val[2],
+		(unsigned int)(long)ktep->val[3],
+		(unsigned int)(long)ktep->val[4],
+		(unsigned int)(long)ktep->val[5],
+		(unsigned int)(long)ktep->val[6],
+		(long)ktep->val[8]);
+	qprintf("ra 0x%p ", ktep->val[9]);
+	printflags((__psint_t)ktep->val[7], bunmapi_flags, "flags");
+	qprintf("\n");
+}
+
+/*
+ * Print inval_cached_pages entry trace.
+ */
+static void
+xfs_inval_cached_trace_entry(ktrace_entry_t     *ktep)
+{
+	qprintf("ip 0x%p offset 0x%x%x len 0x%x%x first 0x%x%x last 0x%x%x\n",
+		ktep->val[1],
+		(unsigned int)(long)ktep->val[2],
+		(unsigned int)(long)ktep->val[3],
+		(unsigned int)(long)ktep->val[4],
+		(unsigned int)(long)ktep->val[5],
+		(unsigned int)(long)ktep->val[6],
+		(unsigned int)(long)ktep->val[7],
+		(unsigned int)(long)ktep->val[8],
+		(unsigned int)(long)ktep->val[9]);
+}
+#endif
+
+
+/*
+ * Print disk inode.
+ */
+static void
+xfs_prdinode(xfs_dinode_t *di, int coreonly, int convert)
+{
+	xfs_prdinode_core(&di->di_core, convert);
+	if (!coreonly)
+		kdb_printf("next_unlinked 0x%x u@0x%p\n",
+			INT_GET(di->di_next_unlinked, convert),
+			&di->di_u);
+}
+
+/*
+ * Print disk inode core.
+ */
+static void
+xfs_prdinode_core(xfs_dinode_core_t *dip, int convert)
+{
+	static char *diflags[] = {
+		"realtime",		/* XFS_DIFLAG_REALTIME */
+		"prealloc",		/* XFS_DIFLAG_PREALLOC */
+		"newrtbm",		/* XFS_DIFLAG_NEWRTBM */
+		"immutable",		/* XFS_DIFLAG_IMMUTABLE */
+		"append",		/* XFS_DIFLAG_APPEND */
+		"sync",			/* XFS_DIFLAG_SYNC */
+		"noatime",		/* XFS_DIFLAG_NOATIME */
+		"nodump",		/* XFS_DIFLAG_NODUMP */
+		NULL
+	};
+
+	kdb_printf("magic 0x%x mode 0%o (%s) version 0x%x format 0x%x (%s)\n",
+		INT_GET(dip->di_magic, convert),
+		INT_GET(dip->di_mode, convert),
+		xfs_fmtmode(INT_GET(dip->di_mode, convert)),
+		INT_GET(dip->di_version, convert),
+		INT_GET(dip->di_format, convert),
+		xfs_fmtformat(
+		    (xfs_dinode_fmt_t)INT_GET(dip->di_format, convert)));
+	kdb_printf("nlink %d uid %d gid %d projid %d flushiter %u\n",
+		INT_GET(dip->di_nlink, convert),
+		INT_GET(dip->di_uid, convert),
+		INT_GET(dip->di_gid, convert),
+		(uint)INT_GET(dip->di_projid, convert),
+		(uint)INT_GET(dip->di_flushiter, convert));
+	kdb_printf("atime %u:%u mtime %ud:%u ctime %u:%u\n",
+		INT_GET(dip->di_atime.t_sec, convert),
+		INT_GET(dip->di_atime.t_nsec, convert),
+		INT_GET(dip->di_mtime.t_sec, convert),
+		INT_GET(dip->di_mtime.t_nsec, convert),
+		INT_GET(dip->di_ctime.t_sec, convert),
+		INT_GET(dip->di_ctime.t_nsec, convert));
+	kdb_printf("size %Ld ", INT_GET(dip->di_size, convert));
+	kdb_printf("nblocks %Ld extsize 0x%x nextents 0x%x anextents 0x%x\n",
+		INT_GET(dip->di_nblocks, convert),
+		INT_GET(dip->di_extsize, convert),
+		INT_GET(dip->di_nextents, convert),
+		INT_GET(dip->di_anextents, convert));
+	kdb_printf("forkoff %d aformat 0x%x (%s) dmevmask 0x%x dmstate 0x%x ",
+		INT_GET(dip->di_forkoff, convert),
+		INT_GET(dip->di_aformat, convert),
+		xfs_fmtformat(
+		    (xfs_dinode_fmt_t)INT_GET(dip->di_aformat, convert)),
+		INT_GET(dip->di_dmevmask, convert),
+		INT_GET(dip->di_dmstate, convert));
+	printflags(INT_GET(dip->di_flags, convert), diflags, "flags");
+	kdb_printf("gen 0x%x\n", INT_GET(dip->di_gen, convert));
+}
+
+#ifdef XFS_RW_TRACE
+/*
+ * Print read/write entry trace.
+ */
+static void
+xfs_rw_enter_trace_entry(ktrace_entry_t *ktep)
+{
+	qprintf("ip 0x%p size 0x%x%x ptr 0x%p size %lu\n",
+		ktep->val[1],
+		(unsigned int)(long)ktep->val[2],
+		(unsigned int)(long)ktep->val[3],
+		ktep->val[4],
+		(unsigned long)ktep->val[5]);
+	qprintf("io offset 0x%x%x ioflags 0x%x new size 0x%x%x\n",
+		(unsigned int)(long)ktep->val[6],
+		(unsigned int)(long)ktep->val[7],
+		(unsigned int)(long)ktep->val[8],
+		(unsigned int)(long)ktep->val[9],
+		(unsigned int)(long)ktep->val[10]);
+}
+
+/*
+ * Print page write/release trace.
+ */
+static void
+xfs_page_trace_entry(ktrace_entry_t *ktep)
+{
+	qprintf("ip 0x%p inode 0x%p 0x%p page 0x%p\n",
+		ktep->val[1], ktep->val[2], ktep->val[3], ktep->val[4]);
+	qprintf("mask 0x%x di_size 0x%x%x isize 0x%x%x offset 0x%x%x\n",
+		(unsigned int)(long)ktep->val[5],
+		(unsigned int)(long)ktep->val[6],
+		(unsigned int)(long)ktep->val[7],
+		(unsigned int)(long)ktep->val[8],
+		(unsigned int)(long)ktep->val[9],
+		(unsigned int)(long)ktep->val[10],
+		(unsigned int)(long)ktep->val[11]);
+	qprintf("delalloc %d unmapped %d unwritten %d\n",
+		(unsigned int)(long)ktep->val[12],
+		(unsigned int)(long)ktep->val[13],
+		(unsigned int)(long)ktep->val[14]);
+}
+
+/*
+ * Print read/write trace entry.
+ */
+static int
+xfs_rw_trace_entry(ktrace_entry_t *ktep)
+{
+	switch ( (long)ktep->val[0] ) {
+	case XFS_READ_ENTER:
+		qprintf("READ ENTER:\n");
+		xfs_rw_enter_trace_entry(ktep);
+		break;
+	case XFS_WRITE_ENTER:
+		qprintf("WRITE ENTER:\n");
+		xfs_rw_enter_trace_entry(ktep);
+		break;
+	case XFS_SENDFILE_ENTER:
+		qprintf("SENDFILE ENTER:\n");
+		xfs_rw_enter_trace_entry(ktep);
+		break;
+	case XFS_IOMAP_READ_ENTER:
+		qprintf("IOMAP READ ENTER:\n");
+		xfs_iomap_enter_trace_entry(ktep);
+		break;
+	case XFS_IOMAP_WRITE_ENTER:
+		qprintf("IOMAP WRITE ENTER:\n");
+		xfs_iomap_enter_trace_entry(ktep);
+		break;
+	case XFS_IOMAP_WRITE_NOSPACE:
+		qprintf("IOMAP WRITE NOSPACE:\n");
+		xfs_iomap_enter_trace_entry(ktep);
+		break;
+	case XFS_IOMAP_READ_MAP:
+		qprintf("IOMAP READ MAP:\n");
+		xfs_iomap_map_trace_entry(ktep);
+		break;
+	case XFS_IOMAP_WRITE_MAP:
+		qprintf("IOMAP WRITE MAP:\n");
+		xfs_iomap_map_trace_entry(ktep);
+		break;
+	case XFS_ITRUNC_START:
+		qprintf("ITRUNC START:\n");
+		xfs_itrunc_trace_entry(ktep);
+		break;
+	case XFS_ITRUNC_FINISH1:
+		qprintf("ITRUNC FINISH1:\n");
+		xfs_itrunc_trace_entry(ktep);
+		break;
+	case XFS_ITRUNC_FINISH2:
+		qprintf("ITRUNC FINISH2:\n");
+		xfs_itrunc_trace_entry(ktep);
+		break;
+	case XFS_CTRUNC1:
+		qprintf("CTRUNC1:\n");
+		xfs_ctrunc_trace_entry(ktep);
+		break;
+	case XFS_CTRUNC2:
+		qprintf("CTRUNC2:\n");
+		xfs_ctrunc_trace_entry(ktep);
+		break;
+	case XFS_CTRUNC3:
+		qprintf("CTRUNC3:\n");
+		xfs_ctrunc_trace_entry(ktep);
+		break;
+	case XFS_CTRUNC4:
+		qprintf("CTRUNC4:\n");
+		xfs_ctrunc_trace_entry(ktep);
+		break;
+	case XFS_CTRUNC5:
+		qprintf("CTRUNC5:\n");
+		xfs_ctrunc_trace_entry(ktep);
+		break;
+	case XFS_CTRUNC6:
+		qprintf("CTRUNC6:\n");
+		xfs_ctrunc_trace_entry(ktep);
+		break;
+	case XFS_BUNMAPI:
+		qprintf("BUNMAPI:\n");
+		xfs_bunmap_trace_entry(ktep);
+		break;
+	case XFS_INVAL_CACHED:
+		qprintf("INVAL CACHED:\n");
+		xfs_inval_cached_trace_entry(ktep);
+		break;
+	case XFS_DIORD_ENTER:
+		qprintf("DIORD ENTER:\n");
+		xfs_rw_enter_trace_entry(ktep);
+		break;
+	case XFS_DIOWR_ENTER:
+		qprintf("DIOWR ENTER:\n");
+		xfs_rw_enter_trace_entry(ktep);
+		break;
+	case XFS_WRITEPAGE_ENTER:
+		qprintf("PAGE WRITE:\n");
+		xfs_page_trace_entry(ktep);
+		break;
+	case XFS_RELEASEPAGE_ENTER:
+		qprintf("PAGE RELEASE:\n");
+		xfs_page_trace_entry(ktep);
+		break;
+	case XFS_IOMAP_ALLOC_ENTER:
+		qprintf("ALLOC ENTER:\n");
+		xfs_iomap_enter_trace_entry(ktep);
+		break;
+	case XFS_IOMAP_ALLOC_MAP:
+		qprintf("ALLOC MAP:\n");
+		xfs_iomap_map_trace_entry(ktep);
+		break;
+	case XFS_IOMAP_UNWRITTEN:
+		qprintf("UNWRITTEN:\n");
+		xfs_iomap_enter_trace_entry(ktep);
+		break;
+
+	default:
+		qprintf("UNKNOWN RW TRACE\n");
+		return 0;
+	}
+
+	return 1;
+}
+#endif
+
+/*
+ * Print xfs extent list for a fork.
+ */
+static void
+xfs_xexlist_fork(xfs_inode_t *ip, int whichfork)
+{
+	int nextents, i;
+	xfs_ifork_t *ifp;
+	xfs_bmbt_irec_t irec;
+
+	ifp = XFS_IFORK_PTR(ip, whichfork);
+	if (ifp->if_flags & XFS_IFEXTENTS) {
+		nextents = ifp->if_bytes / sizeof(xfs_bmbt_rec_64_t);
+		kdb_printf("inode 0x%p %cf extents 0x%p nextents 0x%x\n",
+			ip, "da"[whichfork], ifp->if_u1.if_extents, nextents);
+		for (i = 0; i < nextents; i++) {
+			xfs_bmbt_get_all(&ifp->if_u1.if_extents[i], &irec);
+			kdb_printf(
+		"%d: startoff %Ld startblock %s blockcount %Ld flag %d\n",
+			i, irec.br_startoff,
+			xfs_fmtfsblock(irec.br_startblock, ip->i_mount),
+			irec.br_blockcount, irec.br_state);
+		}
+	}
+}
+
+static void
+xfs_xnode_fork(char *name, xfs_ifork_t *f)
+{
+	static char *tab_flags[] = {
+		"inline",	/* XFS_IFINLINE */
+		"extents",	/* XFS_IFEXTENTS */
+		"broot",	/* XFS_IFBROOT */
+		NULL
+	};
+	int *p;
+
+	kdb_printf("%s fork", name);
+	if (f == NULL) {
+		kdb_printf(" empty\n");
+		return;
+	} else
+		kdb_printf("\n");
+	kdb_printf(" bytes %s ", xfs_fmtsize(f->if_bytes));
+	kdb_printf("real_bytes %s lastex 0x%x u1:%s 0x%p\n",
+		xfs_fmtsize(f->if_real_bytes), f->if_lastex,
+		f->if_flags & XFS_IFINLINE ? "data" : "extents",
+		f->if_flags & XFS_IFINLINE ?
+			f->if_u1.if_data :
+			(char *)f->if_u1.if_extents);
+	kdb_printf(" broot 0x%p broot_bytes %s ext_max %d ",
+		f->if_broot, xfs_fmtsize(f->if_broot_bytes), f->if_ext_max);
+	printflags(f->if_flags, tab_flags, "flags");
+	kdb_printf("\n");
+	kdb_printf(" u2");
+	for (p = (int *)&f->if_u2;
+	     p < (int *)((char *)&f->if_u2 + XFS_INLINE_DATA);
+	     p++)
+		kdb_printf(" 0x%x", *p);
+	kdb_printf("\n");
+}
+
+/*
+ * Command-level xfs-idbg functions.
+ */
+
+/*
+ * Print xfs allocation group freespace header.
+ */
+static void
+xfsidbg_xagf(xfs_agf_t *agf)
+{
+	kdb_printf("magicnum 0x%x versionnum 0x%x seqno 0x%x length 0x%x\n",
+		INT_GET(agf->agf_magicnum, ARCH_CONVERT),
+		INT_GET(agf->agf_versionnum, ARCH_CONVERT),
+		INT_GET(agf->agf_seqno, ARCH_CONVERT),
+		INT_GET(agf->agf_length, ARCH_CONVERT));
+	kdb_printf("roots b 0x%x c 0x%x levels b %d c %d\n",
+		INT_GET(agf->agf_roots[XFS_BTNUM_BNO], ARCH_CONVERT),
+		INT_GET(agf->agf_roots[XFS_BTNUM_CNT], ARCH_CONVERT),
+		INT_GET(agf->agf_levels[XFS_BTNUM_BNO], ARCH_CONVERT),
+		INT_GET(agf->agf_levels[XFS_BTNUM_CNT], ARCH_CONVERT));
+	kdb_printf("flfirst %d fllast %d flcount %d freeblks %d longest %d\n",
+		INT_GET(agf->agf_flfirst, ARCH_CONVERT),
+		INT_GET(agf->agf_fllast, ARCH_CONVERT),
+		INT_GET(agf->agf_flcount, ARCH_CONVERT),
+		INT_GET(agf->agf_freeblks, ARCH_CONVERT),
+		INT_GET(agf->agf_longest, ARCH_CONVERT));
+}
+
+/*
+ * Print xfs allocation group inode header.
+ */
+static void
+xfsidbg_xagi(xfs_agi_t *agi)
+{
+	int	i;
+	int	j;
+
+	kdb_printf("magicnum 0x%x versionnum 0x%x seqno 0x%x length 0x%x\n",
+		INT_GET(agi->agi_magicnum, ARCH_CONVERT),
+		INT_GET(agi->agi_versionnum, ARCH_CONVERT),
+		INT_GET(agi->agi_seqno, ARCH_CONVERT),
+		INT_GET(agi->agi_length, ARCH_CONVERT));
+	kdb_printf("count 0x%x root 0x%x level 0x%x\n",
+		INT_GET(agi->agi_count, ARCH_CONVERT),
+		INT_GET(agi->agi_root, ARCH_CONVERT),
+		INT_GET(agi->agi_level, ARCH_CONVERT));
+	kdb_printf("freecount 0x%x newino 0x%x dirino 0x%x\n",
+		INT_GET(agi->agi_freecount, ARCH_CONVERT),
+		INT_GET(agi->agi_newino, ARCH_CONVERT),
+		INT_GET(agi->agi_dirino, ARCH_CONVERT));
+
+	kdb_printf("unlinked buckets\n");
+	for (i = 0; i < XFS_AGI_UNLINKED_BUCKETS; i++) {
+		for (j = 0; j < 4; j++, i++) {
+			kdb_printf("0x%08x ",
+				INT_GET(agi->agi_unlinked[i], ARCH_CONVERT));
+		}
+		kdb_printf("\n");
+	}
+}
+
+#ifdef XFS_ALLOC_TRACE
+/*
+ * Print out the last "count" entries in the allocation trace buffer.
+ */
+static void
+xfsidbg_xalatrace(int count)
+{
+	ktrace_entry_t  *ktep;
+	ktrace_snap_t   kts;
+	int	     nentries;
+	int	     skip_entries;
+
+	if (xfs_alloc_trace_buf == NULL) {
+		qprintf("The xfs alloc trace buffer is not initialized\n");
+		return;
+	}
+	nentries = ktrace_nentries(xfs_alloc_trace_buf);
+	if (count == -1) {
+		count = nentries;
+	}
+	if ((count <= 0) || (count > nentries)) {
+		qprintf("Invalid count.  There are %d entries.\n", nentries);
+		return;
+	}
+
+	ktep = ktrace_first(xfs_alloc_trace_buf, &kts);
+	if (count != nentries) {
+		/*
+		 * Skip the total minus the number to look at minus one
+		 * for the entry returned by ktrace_first().
+		 */
+		skip_entries = nentries - count - 1;
+		ktep = ktrace_skip(xfs_alloc_trace_buf, skip_entries, &kts);
+		if (ktep == NULL) {
+			qprintf("Skipped them all\n");
+			return;
+		}
+	}
+	while (ktep != NULL) {
+		if (xfs_alloc_trace_entry(ktep))
+			qprintf("\n");
+		ktep = ktrace_next(xfs_alloc_trace_buf, &kts);
+	}
+}
+
+/*
+ * Print out all the entries in the alloc trace buf corresponding
+ * to the given block number.
+ */
+static void
+xfsidbg_xalbtrace(xfs_agblock_t bno)
+{
+	ktrace_entry_t  *ktep;
+	ktrace_snap_t   kts;
+
+	if (xfs_alloc_trace_buf == NULL) {
+		qprintf("The xfs alloc trace buffer is not initialized\n");
+		return;
+	}
+
+	ktep = ktrace_first(xfs_alloc_trace_buf, &kts);
+	while (ktep != NULL) {
+		switch ((__psint_t)ktep->val[0]) {
+		case XFS_ALLOC_KTRACE_ALLOC:
+		case XFS_ALLOC_KTRACE_FREE:
+			if (bno >= (xfs_agblock_t)((__psint_t)ktep->val[5]) &&
+			    bno < (xfs_agblock_t)((__psint_t)ktep->val[5]) +
+				  (xfs_extlen_t)((__psint_t)ktep->val[13])) {
+				(void)xfs_alloc_trace_entry(ktep);
+				qprintf("\n");
+			}
+			break;
+		}
+		ktep = ktrace_next(xfs_alloc_trace_buf, &kts);
+	}
+}
+
+/*
+ * Print out all the entries in the alloc trace buf corresponding
+ * to the given allocation group.
+ */
+static void
+xfsidbg_xalgtrace(xfs_agnumber_t agno)
+{
+	ktrace_entry_t  *ktep;
+	ktrace_snap_t   kts;
+
+	if (xfs_alloc_trace_buf == NULL) {
+		qprintf("The xfs alloc trace buffer is not initialized\n");
+		return;
+	}
+
+	ktep = ktrace_first(xfs_alloc_trace_buf, &kts);
+	while (ktep != NULL) {
+		if (  (__psint_t)ktep->val[0] &&
+		      ((xfs_agnumber_t)((__psint_t)ktep->val[4])) == agno ) {
+			(void)xfs_alloc_trace_entry(ktep);
+			qprintf("\n");
+		}
+		ktep = ktrace_next(xfs_alloc_trace_buf, &kts);
+	}
+}
+#endif
+
+/*
+ * Print an allocation argument structure for XFS.
+ */
+static void
+xfsidbg_xalloc(xfs_alloc_arg_t *args)
+{
+	kdb_printf("tp 0x%p mp 0x%p agbp 0x%p pag 0x%p fsbno %s\n",
+		args->tp, args->mp, args->agbp, args->pag,
+		xfs_fmtfsblock(args->fsbno, args->mp));
+	kdb_printf("agno 0x%x agbno 0x%x minlen 0x%x maxlen 0x%x mod 0x%x\n",
+		args->agno, args->agbno, args->minlen, args->maxlen, args->mod);
+	kdb_printf("prod 0x%x minleft 0x%x total 0x%x alignment 0x%x\n",
+		args->prod, args->minleft, args->total, args->alignment);
+	kdb_printf("minalignslop 0x%x len 0x%x type %s otype %s wasdel %d\n",
+		args->minalignslop, args->len, xfs_alloctype[args->type],
+		xfs_alloctype[args->otype], args->wasdel);
+	kdb_printf("wasfromfl %d isfl %d userdata %d\n",
+		args->wasfromfl, args->isfl, args->userdata);
+}
+
+#ifdef XFS_ALLOC_TRACE
+/*
+ * Print out all the entries in the alloc trace buf corresponding
+ * to the given mount point.
+ */
+static void
+xfsidbg_xalmtrace(xfs_mount_t *mp)
+{
+	ktrace_entry_t	*ktep;
+	ktrace_snap_t	kts;
+
+	if (xfs_alloc_trace_buf == NULL) {
+		kdb_printf("The xfs alloc trace buffer is not initialized\n");
+		return;
+	}
+
+	ktep = ktrace_first(xfs_alloc_trace_buf, &kts);
+	while (ktep != NULL) {
+		if ((__psint_t)ktep->val[0] &&
+		    (xfs_mount_t *)ktep->val[3] == mp) {
+			(void)xfs_alloc_trace_entry(ktep);
+			kdb_printf("\n");
+		}
+		ktep = ktrace_next(xfs_alloc_trace_buf, &kts);
+	}
+}
+
+/*
+ * Print out all the entries in the alloc trace buf corresponding
+ * to the given entry type.
+ */
+static void
+xfsidbg_xalttrace(int tag)
+{
+	ktrace_entry_t  *ktep;
+	ktrace_snap_t   kts;
+
+	if (xfs_alloc_trace_buf == NULL) {
+		qprintf("The xfs alloc trace buffer is not initialized\n");
+		return;
+	}
+
+	ktep = ktrace_first(xfs_alloc_trace_buf, &kts);
+	while (ktep != NULL) {
+		if ((__psint_t)ktep->val[0] &&
+		    ((long)ktep->val[0] & 0xffffL) == (long)tag) {
+			(void)xfs_alloc_trace_entry(ktep);
+			qprintf("\n");
+		}
+		ktep = ktrace_next(xfs_alloc_trace_buf, &kts);
+	}
+}
+#endif
+
+/*
+ * Set xtra argument, used by xchksum.
+ */
+static void
+xfsidbg_xarg(int xarg)
+{
+	if (xarg == -1)
+		qprintf("xargument: %d\n", xargument);
+	else
+		xargument = xarg;
+}       /* xfsidbg_xarg */
+
+/*
+ * Print an attr_list() context structure.
+ */
+static void
+xfsidbg_xattrcontext(xfs_attr_list_context_t *context)
+{
+	static char *attr_arg_flags[] = {
+		"DONTFOLLOW",	/* 0x0001 */
+		"?",		/* 0x0002 */
+		"?",		/* 0x0004 */
+		"?",		/* 0x0008 */
+		"CREATE",	/* 0x0010 */
+		"?",		/* 0x0020 */
+		"?",		/* 0x0040 */
+		"?",		/* 0x0080 */
+		"?",		/* 0x0100 */
+		"?",		/* 0x0200 */
+		"?",		/* 0x0400 */
+		"?",		/* 0x0800 */
+		"KERNOTIME",	/* 0x1000 */
+		NULL
+	};
+
+	kdb_printf("dp 0x%p, dupcnt %d, resynch %d",
+		    context->dp, context->dupcnt, context->resynch);
+	printflags((__psunsigned_t)context->flags, attr_arg_flags, ", flags");
+	kdb_printf("\ncursor h/b/o 0x%x/0x%x/%d -- p/p/i 0x%x/0x%x/0x%x\n",
+			  context->cursor->hashval, context->cursor->blkno,
+			  context->cursor->offset, context->cursor->pad1,
+			  context->cursor->pad2, context->cursor->initted);
+	kdb_printf("alist 0x%p, bufsize 0x%x, count %d, firstu 0x%x\n",
+		       context->alist, context->bufsize, context->count,
+		       context->firstu);
+}
+
+/*
+ * Print attribute leaf block.
+ */
+static void
+xfsidbg_xattrleaf(xfs_attr_leafblock_t *leaf)
+{
+	xfs_attr_leaf_hdr_t *h;
+	xfs_da_blkinfo_t *i;
+	xfs_attr_leaf_map_t *m;
+	xfs_attr_leaf_entry_t *e;
+	xfs_attr_leaf_name_local_t *l;
+	xfs_attr_leaf_name_remote_t *r;
+	int j, k;
+
+	h = &leaf->hdr;
+	i = &h->info;
+	kdb_printf("hdr info forw 0x%x back 0x%x magic 0x%x\n",
+		i->forw, i->back, i->magic);
+	kdb_printf("hdr count %d usedbytes %d firstused %d holes %d\n",
+		INT_GET(h->count, ARCH_CONVERT),
+		INT_GET(h->usedbytes, ARCH_CONVERT),
+		INT_GET(h->firstused, ARCH_CONVERT), h->holes);
+	for (j = 0, m = h->freemap; j < XFS_ATTR_LEAF_MAPSIZE; j++, m++) {
+		kdb_printf("hdr freemap %d base %d size %d\n",
+			j, INT_GET(m->base, ARCH_CONVERT),
+			INT_GET(m->size, ARCH_CONVERT));
+	}
+	for (j = 0, e = leaf->entries; j < INT_GET(h->count, ARCH_CONVERT); j++, e++) {
+		kdb_printf("[%2d] hash 0x%x nameidx %d flags 0x%x",
+			j, INT_GET(e->hashval, ARCH_CONVERT),
+			INT_GET(e->nameidx, ARCH_CONVERT), e->flags);
+		if (e->flags & XFS_ATTR_LOCAL)
+			kdb_printf("LOCAL ");
+		if (e->flags & XFS_ATTR_ROOT)
+			kdb_printf("ROOT ");
+		if (e->flags & XFS_ATTR_SECURE)
+			kdb_printf("SECURE ");
+		if (e->flags & XFS_ATTR_INCOMPLETE)
+			kdb_printf("INCOMPLETE ");
+		k = ~(XFS_ATTR_LOCAL | XFS_ATTR_ROOT |
+			XFS_ATTR_SECURE | XFS_ATTR_INCOMPLETE);
+		if ((e->flags & k) != 0)
+			kdb_printf("0x%x", e->flags & k);
+		kdb_printf(">\n     name \"");
+		if (e->flags & XFS_ATTR_LOCAL) {
+			l = XFS_ATTR_LEAF_NAME_LOCAL(leaf, j);
+			for (k = 0; k < l->namelen; k++)
+				kdb_printf("%c", l->nameval[k]);
+			kdb_printf("\"(%d) value \"", l->namelen);
+			for (k = 0; (k < INT_GET(l->valuelen, ARCH_CONVERT)) && (k < 32); k++)
+				kdb_printf("%c", l->nameval[l->namelen + k]);
+			if (k == 32)
+				kdb_printf("...");
+			kdb_printf("\"(%d)\n",
+				INT_GET(l->valuelen, ARCH_CONVERT));
+		} else {
+			r = XFS_ATTR_LEAF_NAME_REMOTE(leaf, j);
+			for (k = 0; k < r->namelen; k++)
+				kdb_printf("%c", r->name[k]);
+			kdb_printf("\"(%d) value blk 0x%x len %d\n",
+				    r->namelen,
+				    INT_GET(r->valueblk, ARCH_CONVERT),
+				    INT_GET(r->valuelen, ARCH_CONVERT));
+		}
+	}
+}
+
+/*
+ * Print a shortform attribute list.
+ */
+static void
+xfsidbg_xattrsf(xfs_attr_shortform_t *s)
+{
+	xfs_attr_sf_hdr_t *sfh;
+	xfs_attr_sf_entry_t *sfe;
+	int i, j;
+
+	sfh = &s->hdr;
+	kdb_printf("hdr count %d\n", INT_GET(sfh->count, ARCH_CONVERT));
+	for (i = 0, sfe = s->list; i < INT_GET(sfh->count, ARCH_CONVERT); i++) {
+		kdb_printf("entry %d namelen %d name \"", i, sfe->namelen);
+		for (j = 0; j < sfe->namelen; j++)
+			kdb_printf("%c", sfe->nameval[j]);
+		kdb_printf("\" valuelen %d value \"", INT_GET(sfe->valuelen, ARCH_CONVERT));
+		for (j = 0; (j < INT_GET(sfe->valuelen, ARCH_CONVERT)) && (j < 32); j++)
+			kdb_printf("%c", sfe->nameval[sfe->namelen + j]);
+		if (j == 32)
+			kdb_printf("...");
+		kdb_printf("\"\n");
+		sfe = XFS_ATTR_SF_NEXTENTRY(sfe);
+	}
+}
+
+#ifdef XFS_ATTR_TRACE
+/*
+ * Print out the last "count" entries in the attribute trace buffer.
+ */
+static void
+xfsidbg_xattrtrace(int count)
+{
+	ktrace_entry_t  *ktep;
+	ktrace_snap_t   kts;
+	int	     nentries;
+	int	     skip_entries;
+
+	if (xfs_attr_trace_buf == NULL) {
+		qprintf("The xfs attribute trace buffer is not initialized\n");
+		return;
+	}
+	nentries = ktrace_nentries(xfs_attr_trace_buf);
+	if (count == -1) {
+		count = nentries;
+	}
+	if ((count <= 0) || (count > nentries)) {
+		qprintf("Invalid count.  There are %d entries.\n", nentries);
+		return;
+	}
+
+	ktep = ktrace_first(xfs_attr_trace_buf, &kts);
+	if (count != nentries) {
+		/*
+		 * Skip the total minus the number to look at minus one
+		 * for the entry returned by ktrace_first().
+		 */
+		skip_entries = nentries - count - 1;
+		ktep = ktrace_skip(xfs_attr_trace_buf, skip_entries, &kts);
+		if (ktep == NULL) {
+			qprintf("Skipped them all\n");
+			return;
+		}
+	}
+	while (ktep != NULL) {
+		xfs_attr_trace_entry(ktep);
+		ktep = ktrace_next(xfs_attr_trace_buf, &kts);
+	}
+}
+#endif
+
+/*
+ * Print xfs bmap internal record
+ */
+static void
+xfsidbg_xbirec(xfs_bmbt_irec_t *r)
+{
+	kdb_printf(
+	"startoff %Ld startblock %Lx blockcount %Ld state %Ld\n",
+		(__uint64_t)r->br_startoff,
+		(__uint64_t)r->br_startblock,
+		(__uint64_t)r->br_blockcount,
+		(__uint64_t)r->br_state);
+}
+
+#ifdef XFS_BLI_TRACE
+/*
+ * Print out the buf log item trace for the given buf log item.
+ */
+static void
+xfsidbg_xblitrace(xfs_buf_log_item_t *bip)
+{
+	ktrace_entry_t  *ktep;
+	ktrace_snap_t   kts;
+	uint64_t	flags;
+	static char *xbli_flags[] = {
+		"hold",		/* 0x01 */
+		"dirty",	/* 0x02 */
+		"stale",	/* 0x04 */
+		"logged",	/* 0x08 */
+		0
+		};
+	static char *xli_flags[] = {
+		"in ail",       /* 0x1 */
+		0
+		};
+
+	if (bip->bli_trace == NULL) {
+		qprintf("The bli trace buffer is not initialized\n");
+		return;
+	}
+
+	ktep = ktrace_first(bip->bli_trace, &kts);
+	while (ktep != NULL) {
+		qprintf("%s bp 0x%p flags ",
+			(char *)ktep->val[0], ktep->val[1]);
+		printflags((__psint_t)(ktep->val[2]), xbli_flags, "xbli");
+		qprintf("\n");
+		qprintf("recur %ld refcount %ld blkno 0x%lx bcount 0x%lx\n",
+			(long)ktep->val[3], (long)ktep->val[4],
+			(unsigned long)ktep->val[5],
+			(unsigned long)ktep->val[6]);
+		flags = (((uint64_t)(unsigned long)ktep->val[7] << 32) &
+					0xFFFFFFFF00000000ULL) |
+			(((uint64_t)(unsigned long)ktep->val[8]) &
+					0x00000000FFFFFFFFULL);
+		qprintf("bp flags ");
+		printflags(flags, pb_flag_vals, 0);
+		qprintf("\n");
+		qprintf("fspriv 0x%p fspriv2 0x%p pincount %ld iodone 0x%p\n",
+			ktep->val[9], ktep->val[10],
+			(long)ktep->val[11], ktep->val[12]);
+		qprintf("lockval %ld lid 0x%lx log item flags ",
+			(long)ktep->val[13], (unsigned long)ktep->val[14]);
+		printflags((__psint_t)(ktep->val[15]), xli_flags, "xli");
+		qprintf("\n");
+
+		ktep = ktrace_next(bip->bli_trace, &kts);
+	}
+}
+#endif
+
+/*
+ * Print a bmap alloc argument structure for XFS.
+ */
+static void
+xfsidbg_xbmalla(xfs_bmalloca_t *a)
+{
+	kdb_printf("tp 0x%p ip 0x%p eof %d prevp 0x%p\n",
+		a->tp, a->ip, a->eof, a->prevp);
+	kdb_printf("gotp 0x%p firstblock %s alen %d total %d\n",
+		a->gotp, xfs_fmtfsblock(a->firstblock, a->ip->i_mount),
+		a->alen, a->total);
+	kdb_printf("off %s wasdel %d userdata %d minlen %d\n",
+		xfs_fmtfsblock(a->off, a->ip->i_mount), a->wasdel,
+		a->userdata, a->minlen);
+	kdb_printf("minleft %d low %d rval %s aeof %d\n",
+		a->minleft, a->low, xfs_fmtfsblock(a->rval, a->ip->i_mount),
+		a->aeof);
+}
+
+#ifdef XFS_BMAP_TRACE
+/*
+ * Print out the last "count" entries in the bmap btree trace buffer.
+ * The "a" is for "all" inodes.
+ */
+static void
+xfsidbg_xbmatrace(int count)
+{
+	ktrace_entry_t  *ktep;
+	ktrace_snap_t   kts;
+	int	     nentries;
+	int	     skip_entries;
+
+	if (xfs_bmbt_trace_buf == NULL) {
+		qprintf("The xfs bmap btree trace buffer is not initialized\n");		return;
+	}
+	nentries = ktrace_nentries(xfs_bmbt_trace_buf);
+	if (count == -1) {
+		count = nentries;
+	}
+	if ((count <= 0) || (count > nentries)) {
+		qprintf("Invalid count.  There are %d entries.\n", nentries);
+		return;
+	}
+
+	ktep = ktrace_first(xfs_bmbt_trace_buf, &kts);
+	if (count != nentries) {
+		/*
+		 * Skip the total minus the number to look at minus one
+		 * for the entry returned by ktrace_first().
+		 */
+		skip_entries = nentries - count - 1;
+		ktep = ktrace_skip(xfs_bmbt_trace_buf, skip_entries, &kts);
+		if (ktep == NULL) {
+			qprintf("Skipped them all\n");
+			return;
+		}
+	}
+	while (ktep != NULL) {
+		if (xfs_bmbt_trace_entry(ktep))
+			qprintf("\n");
+		ktep = ktrace_next(xfs_bmbt_trace_buf, &kts);
+	}
+}
+
+/*
+ * Print out the bmap btree trace buffer attached to the given inode.
+ */
+static void
+xfsidbg_xbmitrace(xfs_inode_t *ip)
+{
+	ktrace_entry_t  *ktep;
+	ktrace_snap_t   kts;
+
+	if (ip->i_btrace == NULL) {
+		qprintf("The inode trace buffer is not initialized\n");
+		return;
+	}
+
+	ktep = ktrace_first(ip->i_btrace, &kts);
+	while (ktep != NULL) {
+		if (xfs_bmbt_trace_entry(ktep))
+			qprintf("\n");
+		ktep = ktrace_next(ip->i_btrace, &kts);
+	}
+}
+
+/*
+ * Print out all the entries in the bmap btree trace buf corresponding
+ * to the given inode.  The "s" is for "single" inode.
+ */
+static void
+xfsidbg_xbmstrace(xfs_inode_t *ip)
+{
+	ktrace_entry_t  *ktep;
+	ktrace_snap_t   kts;
+
+	if (xfs_bmbt_trace_buf == NULL) {
+		qprintf("The xfs bmap btree trace buffer is not initialized\n");		return;
+	}
+
+	ktep = ktrace_first(xfs_bmbt_trace_buf, &kts);
+	while (ktep != NULL) {
+		if ((xfs_inode_t *)(ktep->val[2]) == ip) {
+			if (xfs_bmbt_trace_entry(ktep))
+				qprintf("\n");
+		}
+		ktep = ktrace_next(xfs_bmbt_trace_buf, &kts);
+	}
+}
+#endif
+
+/*
+ * Print xfs bmap record
+ */
+static void
+xfsidbg_xbrec(xfs_bmbt_rec_64_t *r)
+{
+	xfs_bmbt_irec_t	irec;
+
+	xfs_bmbt_get_all((xfs_bmbt_rec_t *)r, &irec);
+	kdb_printf("startoff %Ld startblock %Lx blockcount %Ld flag %d\n",
+		irec.br_startoff, (__uint64_t)irec.br_startblock,
+		irec.br_blockcount, irec.br_state);
+}
+
+/*
+ * Print an xfs in-inode bmap btree root (data fork).
+ */
+static void
+xfsidbg_xbroot(xfs_inode_t *ip)
+{
+	xfs_broot(ip, &ip->i_df);
+}
+
+/*
+ * Print an xfs in-inode bmap btree root (attribute fork).
+ */
+static void
+xfsidbg_xbroota(xfs_inode_t *ip)
+{
+	if (ip->i_afp)
+		xfs_broot(ip, ip->i_afp);
+}
+
+/*
+ * Print xfs btree cursor.
+ */
+static void
+xfsidbg_xbtcur(xfs_btree_cur_t *c)
+{
+	int l;
+
+	kdb_printf("tp 0x%p mp 0x%p\n",
+		c->bc_tp,
+		c->bc_mp);
+	if (c->bc_btnum == XFS_BTNUM_BMAP) {
+		kdb_printf("rec.b ");
+		xfsidbg_xbirec(&c->bc_rec.b);
+	} else if (c->bc_btnum == XFS_BTNUM_INO) {
+		kdb_printf("rec.i startino 0x%x freecount 0x%x free %Lx\n",
+			c->bc_rec.i.ir_startino, c->bc_rec.i.ir_freecount,
+			c->bc_rec.i.ir_free);
+	} else {
+		kdb_printf("rec.a startblock 0x%x blockcount 0x%x\n",
+			c->bc_rec.a.ar_startblock,
+			c->bc_rec.a.ar_blockcount);
+	}
+	kdb_printf("bufs");
+	for (l = 0; l < c->bc_nlevels; l++)
+		kdb_printf(" 0x%p", c->bc_bufs[l]);
+	kdb_printf("\n");
+	kdb_printf("ptrs");
+	for (l = 0; l < c->bc_nlevels; l++)
+		kdb_printf(" 0x%x", c->bc_ptrs[l]);
+	kdb_printf("  ra");
+	for (l = 0; l < c->bc_nlevels; l++)
+		kdb_printf(" %d", c->bc_ra[l]);
+	kdb_printf("\n");
+	kdb_printf("nlevels %d btnum %s blocklog %d\n",
+		c->bc_nlevels,
+		c->bc_btnum == XFS_BTNUM_BNO ? "bno" :
+		(c->bc_btnum == XFS_BTNUM_CNT ? "cnt" :
+		 (c->bc_btnum == XFS_BTNUM_BMAP ? "bmap" : "ino")),
+		c->bc_blocklog);
+	if (c->bc_btnum == XFS_BTNUM_BMAP) {
+		kdb_printf("private forksize 0x%x whichfork %d ip 0x%p flags %d\n",
+			c->bc_private.b.forksize,
+			c->bc_private.b.whichfork,
+			c->bc_private.b.ip,
+			c->bc_private.b.flags);
+		kdb_printf("private firstblock %s flist 0x%p allocated 0x%x\n",
+			xfs_fmtfsblock(c->bc_private.b.firstblock, c->bc_mp),
+			c->bc_private.b.flist,
+			c->bc_private.b.allocated);
+	} else if (c->bc_btnum == XFS_BTNUM_INO) {
+		kdb_printf("private agbp 0x%p agno 0x%x\n",
+			c->bc_private.i.agbp,
+			c->bc_private.i.agno);
+	} else {
+		kdb_printf("private agbp 0x%p agno 0x%x\n",
+			c->bc_private.a.agbp,
+			c->bc_private.a.agno);
+	}
+}
+
+/*
+ * Figure out what kind of xfs block the buffer contains,
+ * and invoke a print routine.
+ */
+static void
+xfsidbg_xbuf(xfs_buf_t *bp)
+{
+	xfsidbg_xbuf_real(bp, 0);
+}
+
+/*
+ * Figure out what kind of xfs block the buffer contains,
+ * and invoke a print routine (if asked to).
+ */
+static void
+xfsidbg_xbuf_real(xfs_buf_t *bp, int summary)
+{
+	void *d;
+	xfs_agf_t *agf;
+	xfs_agi_t *agi;
+	xfs_sb_t *sb;
+	xfs_alloc_block_t *bta;
+	xfs_bmbt_block_t *btb;
+	xfs_inobt_block_t *bti;
+	xfs_attr_leafblock_t *aleaf;
+	xfs_dir_leafblock_t *dleaf;
+	xfs_da_intnode_t *node;
+	xfs_dinode_t *di;
+	xfs_disk_dquot_t *dqb;
+	xfs_dir2_block_t *d2block;
+	xfs_dir2_data_t *d2data;
+	xfs_dir2_leaf_t *d2leaf;
+	xfs_dir2_free_t *d2free;
+
+	d = XFS_BUF_PTR(bp);
+	if (INT_GET((agf = d)->agf_magicnum, ARCH_CONVERT) == XFS_AGF_MAGIC) {
+		if (summary) {
+			kdb_printf("freespace hdr for AG %d (at 0x%p)\n",
+				INT_GET(agf->agf_seqno, ARCH_CONVERT), agf);
+		} else {
+			kdb_printf("buf 0x%p agf 0x%p\n", bp, agf);
+			xfsidbg_xagf(agf);
+		}
+	} else if (INT_GET((agi = d)->agi_magicnum, ARCH_CONVERT) == XFS_AGI_MAGIC) {
+		if (summary) {
+			kdb_printf("Inode hdr for AG %d (at 0x%p)\n",
+			       INT_GET(agi->agi_seqno, ARCH_CONVERT), agi);
+		} else {
+			kdb_printf("buf 0x%p agi 0x%p\n", bp, agi);
+			xfsidbg_xagi(agi);
+		}
+	} else if (INT_GET((bta = d)->bb_magic, ARCH_CONVERT) == XFS_ABTB_MAGIC) {
+		if (summary) {
+			kdb_printf("Alloc BNO Btree blk, level %d (at 0x%p)\n",
+				       INT_GET(bta->bb_level, ARCH_CONVERT), bta);
+		} else {
+			kdb_printf("buf 0x%p abtbno 0x%p\n", bp, bta);
+			xfs_btalloc(bta, XFS_BUF_COUNT(bp));
+		}
+	} else if (INT_GET((bta = d)->bb_magic, ARCH_CONVERT) == XFS_ABTC_MAGIC) {
+		if (summary) {
+			kdb_printf("Alloc COUNT Btree blk, level %d (at 0x%p)\n",
+				       INT_GET(bta->bb_level, ARCH_CONVERT), bta);
+		} else {
+			kdb_printf("buf 0x%p abtcnt 0x%p\n", bp, bta);
+			xfs_btalloc(bta, XFS_BUF_COUNT(bp));
+		}
+	} else if (INT_GET((btb = d)->bb_magic, ARCH_CONVERT) == XFS_BMAP_MAGIC) {
+		if (summary) {
+			kdb_printf("Bmap Btree blk, level %d (at 0x%p)\n",
+				      INT_GET(btb->bb_level, ARCH_CONVERT), btb);
+		} else {
+			kdb_printf("buf 0x%p bmapbt 0x%p\n", bp, btb);
+			xfs_btbmap(btb, XFS_BUF_COUNT(bp));
+		}
+	} else if (INT_GET((bti = d)->bb_magic, ARCH_CONVERT) == XFS_IBT_MAGIC) {
+		if (summary) {
+			kdb_printf("Inode Btree blk, level %d (at 0x%p)\n",
+				       INT_GET(bti->bb_level, ARCH_CONVERT), bti);
+		} else {
+			kdb_printf("buf 0x%p inobt 0x%p\n", bp, bti);
+			xfs_btino(bti, XFS_BUF_COUNT(bp));
+		}
+	} else if (INT_GET((aleaf = d)->hdr.info.magic, ARCH_CONVERT) == XFS_ATTR_LEAF_MAGIC) {
+		if (summary) {
+			kdb_printf("Attr Leaf, 1st hash 0x%x (at 0x%p)\n",
+				      INT_GET(aleaf->entries[0].hashval, ARCH_CONVERT), aleaf);
+		} else {
+			kdb_printf("buf 0x%p attr leaf 0x%p\n", bp, aleaf);
+			xfsidbg_xattrleaf(aleaf);
+		}
+	} else if (INT_GET((dleaf = d)->hdr.info.magic, ARCH_CONVERT) == XFS_DIR_LEAF_MAGIC) {
+		if (summary) {
+			kdb_printf("Dir Leaf, 1st hash 0x%x (at 0x%p)\n",
+				     dleaf->entries[0].hashval, dleaf);
+		} else {
+			kdb_printf("buf 0x%p dir leaf 0x%p\n", bp, dleaf);
+			xfsidbg_xdirleaf(dleaf);
+		}
+	} else if (INT_GET((node = d)->hdr.info.magic, ARCH_CONVERT) == XFS_DA_NODE_MAGIC) {
+		if (summary) {
+			kdb_printf("Dir/Attr Node, level %d, 1st hash 0x%x (at 0x%p)\n",
+			      node->hdr.level, node->btree[0].hashval, node);
+		} else {
+			kdb_printf("buf 0x%p dir/attr node 0x%p\n", bp, node);
+			xfsidbg_xdanode(node);
+		}
+	} else if (INT_GET((di = d)->di_core.di_magic, ARCH_CONVERT) == XFS_DINODE_MAGIC) {
+		if (summary) {
+			kdb_printf("Disk Inode (at 0x%p)\n", di);
+		} else {
+			kdb_printf("buf 0x%p dinode 0x%p\n", bp, di);
+			xfs_inodebuf(bp);
+		}
+	} else if (INT_GET((sb = d)->sb_magicnum, ARCH_CONVERT) == XFS_SB_MAGIC) {
+		if (summary) {
+			kdb_printf("Superblock (at 0x%p)\n", sb);
+		} else {
+			kdb_printf("buf 0x%p sb 0x%p\n", bp, sb);
+			/* SB in a buffer - we need to convert */
+			xfsidbg_xsb(sb, 1);
+		}
+	} else if ((dqb = d)->d_magic == XFS_DQUOT_MAGIC) {
+#define XFSIDBG_DQTYPESTR(d)     \
+	((INT_GET((d)->d_flags, ARCH_CONVERT) & XFS_DQ_USER) ? "USR" : \
+	((INT_GET((d)->d_flags, ARCH_CONVERT) & XFS_DQ_GROUP) ? "GRP" : "???"))
+		kdb_printf("Quota blk starting ID [%d], type %s at 0x%p\n",
+			INT_GET(dqb->d_id, ARCH_CONVERT), XFSIDBG_DQTYPESTR(dqb), dqb);
+
+	} else if (INT_GET((d2block = d)->hdr.magic, ARCH_CONVERT) == XFS_DIR2_BLOCK_MAGIC) {
+		if (summary) {
+			kdb_printf("Dir2 block (at 0x%p)\n", d2block);
+		} else {
+			kdb_printf("buf 0x%p dir2 block 0x%p\n", bp, d2block);
+			xfs_dir2data((void *)d2block, XFS_BUF_COUNT(bp));
+		}
+	} else if (INT_GET((d2data = d)->hdr.magic, ARCH_CONVERT) == XFS_DIR2_DATA_MAGIC) {
+		if (summary) {
+			kdb_printf("Dir2 data (at 0x%p)\n", d2data);
+		} else {
+			kdb_printf("buf 0x%p dir2 data 0x%p\n", bp, d2data);
+			xfs_dir2data((void *)d2data, XFS_BUF_COUNT(bp));
+		}
+	} else if (INT_GET((d2leaf = d)->hdr.info.magic, ARCH_CONVERT) == XFS_DIR2_LEAF1_MAGIC) {
+		if (summary) {
+			kdb_printf("Dir2 leaf(1) (at 0x%p)\n", d2leaf);
+		} else {
+			kdb_printf("buf 0x%p dir2 leaf 0x%p\n", bp, d2leaf);
+			xfs_dir2leaf(d2leaf, XFS_BUF_COUNT(bp));
+		}
+	} else if (INT_GET(d2leaf->hdr.info.magic, ARCH_CONVERT) == XFS_DIR2_LEAFN_MAGIC) {
+		if (summary) {
+			kdb_printf("Dir2 leaf(n) (at 0x%p)\n", d2leaf);
+		} else {
+			kdb_printf("buf 0x%p dir2 leaf 0x%p\n", bp, d2leaf);
+			xfs_dir2leaf(d2leaf, XFS_BUF_COUNT(bp));
+		}
+	} else if (INT_GET((d2free = d)->hdr.magic, ARCH_CONVERT) == XFS_DIR2_FREE_MAGIC) {
+		if (summary) {
+			kdb_printf("Dir2 free (at 0x%p)\n", d2free);
+		} else {
+			kdb_printf("buf 0x%p dir2 free 0x%p\n", bp, d2free);
+			xfsidbg_xdir2free(d2free);
+		}
+	} else {
+		kdb_printf("buf 0x%p unknown 0x%p\n", bp, d);
+	}
+}
+
+#ifdef XFS_BMAP_TRACE
+/*
+ * Print out the last "count" entries in the bmap extent trace buffer.
+ * The "a" is for "all" inodes.
+ */
+static void
+xfsidbg_xbxatrace(int count)
+{
+	ktrace_entry_t  *ktep;
+	ktrace_snap_t   kts;
+	int	     nentries;
+	int	     skip_entries;
+
+	if (xfs_bmap_trace_buf == NULL) {
+		qprintf("The xfs bmap extent trace buffer is not initialized\n");
+		return;
+	}
+	nentries = ktrace_nentries(xfs_bmap_trace_buf);
+	if (count == -1) {
+		count = nentries;
+	}
+	if ((count <= 0) || (count > nentries)) {
+		qprintf("Invalid count.  There are %d entries.\n", nentries);
+		return;
+	}
+
+	ktep = ktrace_first(xfs_bmap_trace_buf, &kts);
+	if (count != nentries) {
+		/*
+		 * Skip the total minus the number to look at minus one
+		 * for the entry returned by ktrace_first().
+		 */
+		skip_entries = nentries - count - 1;
+		ktep = ktrace_skip(xfs_bmap_trace_buf, skip_entries, &kts);
+		if (ktep == NULL) {
+			qprintf("Skipped them all\n");
+			return;
+		}
+	}
+	while (ktep != NULL) {
+		if (xfs_bmap_trace_entry(ktep))
+			qprintf("\n");
+		ktep = ktrace_next(xfs_bmap_trace_buf, &kts);
+	}
+}
+
+/*
+ * Print out the bmap extent trace buffer attached to the given inode.
+ */
+static void
+xfsidbg_xbxitrace(xfs_inode_t *ip)
+{
+	ktrace_entry_t  *ktep;
+	ktrace_snap_t   kts;
+	if (ip->i_xtrace == NULL) {
+		qprintf("The inode trace buffer is not initialized\n");
+		return;
+	}
+
+	ktep = ktrace_first(ip->i_xtrace, &kts);
+	while (ktep != NULL) {
+		if (xfs_bmap_trace_entry(ktep))
+			qprintf("\n");
+		ktep = ktrace_next(ip->i_xtrace, &kts);
+	}
+}
+
+/*
+ * Print out all the entries in the bmap extent trace buf corresponding
+ * to the given inode.  The "s" is for "single" inode.
+ */
+static void
+xfsidbg_xbxstrace(xfs_inode_t *ip)
+{
+	ktrace_entry_t  *ktep;
+	ktrace_snap_t   kts;
+
+	if (xfs_bmap_trace_buf == NULL) {
+		qprintf("The xfs bmap extent trace buffer is not initialized\n");
+		return;
+	}
+
+	ktep = ktrace_first(xfs_bmap_trace_buf, &kts);
+	while (ktep != NULL) {
+		if ((xfs_inode_t *)(ktep->val[3]) == ip) {
+			if (xfs_bmap_trace_entry(ktep))
+				qprintf("\n");
+		}
+		ktep = ktrace_next(xfs_bmap_trace_buf, &kts);
+	}
+}
+#endif
+
+#ifdef XFS_ILOCK_TRACE
+/*
+ * Print out the ilock trace buffer attached to the given inode.
+ */
+static void
+xfsidbg_xilock_trace_entry(ktrace_entry_t *ktep)
+{
+	static char	*xiflags[] = {
+		"IOLOCK_EXCL",
+		"IOLOCK_SHAR",
+		"ILOCK_EXCL",
+		"ILOCK_SHAR",
+		"IUNLK_NONOT",
+		0
+	};
+
+	if ((__psint_t)ktep->val[0] &&
+	     (__psint_t)ktep->val[7] == 0) {
+		 printflags((__psint_t)ktep->val[2], xiflags,"Flags ");
+		if ((__psint_t)ktep->val[1] == 1)
+			 qprintf("LOCK\n");
+		else if ((__psint_t)ktep->val[1] == 2)
+			 qprintf("LOCK SHARED\n");
+		else if ((__psint_t)ktep->val[1] == 3)
+			 qprintf("UNLOCK\n");
+		qprintf("ip 0x%p %llx %ld\n",
+			ktep->val[0],
+			(unsigned long long)((xfs_inode_t*)ktep->val[0])->i_ino,
+			(long)ktep->val[6]);
+		qprintf("raddr 0x%p\n", ktep->val[3]);
+		qprintf("  Pid %ld, cpu %ld\n",
+			 (long)ktep->val[5],
+			 (long)ktep->val[4]);
+		qprintf("-----------------------\n");
+
+	} else if ((__psint_t)ktep->val[7] == 1) {
+		if ((__psint_t)ktep->val[1] == 1)
+			qprintf("FlushLOCK ");
+		else if ((__psint_t)ktep->val[1] == 2)
+			qprintf("FlushTRYLOCK %ld ",
+				(long)ktep->val[2]);
+		else if ((__psint_t)ktep->val[1] == 3)
+			qprintf("FlushUNLOCK ");
+		else if ((__psint_t)ktep->val[1] == 4)
+			qprintf("FlushInode 0x%p",
+				ktep->val[2]);
+		else if ((__psint_t)ktep->val[1] == 5)
+			qprintf("FlushInodeInt ");
+		else     qprintf("FlushUNKNOWN ");
+		qprintf("ip 0x%p ino %llx @ %ld\n",
+			ktep->val[0],
+			(unsigned long long)((xfs_inode_t*)ktep->val[0])->i_ino,
+			(long)ktep->val[6]);
+		qprintf("raddr 0x%p\n", ktep->val[3]);
+		qprintf("  Pid %ld, cpu %ld\n",
+			(long)ktep->val[5],
+			(long)ktep->val[4]);
+		qprintf("-----------------------\n");
+	}
+}
+
+static void
+xfsidbg_xilock_trace(xfs_inode_t *ip)
+{
+	static char *xiflags[] = {
+		"IOLOCK_EXCL",
+		"IOLOCK_SHAR",
+		"ILOCK_EXCL",
+		"ILOCK_SHAR",
+		"IUNLK_NONOT",
+		0
+	};
+
+	ktrace_entry_t  *ktep;
+	ktrace_snap_t   kts;
+	if (ip->i_lock_trace == NULL) {
+		qprintf("The inode ilock trace buffer is not initialized\n");
+		return;
+	}
+
+	ktep = ktrace_first(ip->i_lock_trace, &kts);
+	while (ktep != NULL) {
+		 if ((__psint_t)ktep->val[0] &&
+		     (__psint_t)ktep->val[7] == 0) {
+			 printflags((__psint_t)ktep->val[2], xiflags,"Flags ");
+			 if ((__psint_t)ktep->val[1] == 1)
+				 qprintf("LOCK\n");
+			 else if ((__psint_t)ktep->val[1] == 2)
+				 qprintf("LOCK SHARED\n");
+			 else if ((__psint_t)ktep->val[1] == 3)
+				 qprintf("UNLOCK\n");
+			qprintf("ip 0x%p %lld %ld\n",
+				ktep->val[0], (unsigned long long)
+				((xfs_inode_t*)ktep->val[0])->i_ino,
+				(long)ktep->val[6]);
+			 qprintf("raddr 0x%p\n", ktep->val[3]);
+			 qprintf("  Pid %ld, cpu %ld\n",
+				 (long)ktep->val[5],
+				 (long)ktep->val[4]);
+			 qprintf("-----------------------\n");
+		 } else if ((__psint_t)ktep->val[7] == 1) {
+			if ((__psint_t)ktep->val[1] == 1)
+				qprintf("LOCK ");
+			else if ((__psint_t)ktep->val[1] == 2)
+				qprintf("TRYLOCK %ld ",
+					(long)ktep->val[2]);
+			else if ((__psint_t)ktep->val[1] == 3)
+				qprintf("UNLOCK ");
+			else     qprintf("UNKNOWN ");
+			qprintf("ip 0x%p %lld %ld\n",
+				ktep->val[0], (unsigned long long)
+				((xfs_inode_t*)ktep->val[0])->i_ino,
+				(long)ktep->val[6]);
+			qprintf("raddr 0x%p\n", ktep->val[3]);
+			qprintf("  Pid %ld, cpu %ld\n",
+				(long)ktep->val[5],
+				(long)ktep->val[4]);
+			qprintf("-----------------------\n");
+		 }
+
+		 ktep = ktrace_next(ip->i_lock_trace, &kts);
+	}
+}
+
+/*
+ * Print out the last "count" entries in the inode lock trace buffer.
+ * The "a" is for "all" entries.
+ */
+static void
+xfsidbg_xailock_trace(int count)
+{
+	ktrace_entry_t  *ktep;
+	ktrace_snap_t   kts;
+	int	     nentries;
+	int	     skip_entries;
+
+	if (xfs_ilock_trace_buf == NULL) {
+		qprintf("The xfs inode lock trace buffer is not initialized\n");		return;
+	}
+	nentries = ktrace_nentries(xfs_ilock_trace_buf);
+	if (count == -1) {
+		count = nentries;
+	}
+	if ((count <= 0) || (count > nentries)) {
+		qprintf("Invalid count.  There are %d entries.\n", nentries);
+		return;
+	}
+
+	ktep = ktrace_first(xfs_ilock_trace_buf, &kts);
+	if (count != nentries) {
+		/*
+		 * Skip the total minus the number to look at minus one
+		 * for the entry returned by ktrace_first().
+		 */
+		skip_entries = nentries - count - 1;
+		ktep = ktrace_skip(xfs_ilock_trace_buf, skip_entries, &kts);
+		if (ktep == NULL) {
+			qprintf("Skipped them all\n");
+			return;
+		}
+	}
+	while (ktep != NULL) {
+		xfsidbg_xilock_trace_entry(ktep);
+		ktep = ktrace_next(xfs_ilock_trace_buf, &kts);
+	}
+}
+#endif
+
+/*
+ * Compute & print buffer's checksum.
+ */
+static void
+xfsidbg_xchksum(uint *addr)
+{
+	uint i, chksum = 0;
+
+	if (((__psint_t)addr) == ((__psint_t)-1)) {
+		qprintf("USAGE xchksum <address>\n");
+		qprintf("       length is set with xarg\n");
+	} else {
+		for (i=0; i<xargument; i++) {
+			chksum ^= *addr;
+			addr++;
+		}
+		qprintf("chksum (0x%x)  length (%d)\n", chksum, xargument);
+	}
+}       /* xfsidbg_xchksum */
+
+/*
+ * Print an xfs_da_args structure.
+ */
+static void
+xfsidbg_xdaargs(xfs_da_args_t *n)
+{
+	char *ch;
+	int i;
+
+	kdb_printf(" name \"");
+	for (i = 0; i < n->namelen; i++) {
+		kdb_printf("%c", n->name[i]);
+	}
+	kdb_printf("\"(%d) value ", n->namelen);
+	if (n->value) {
+		kdb_printf("\"");
+		ch = n->value;
+		for (i = 0; (i < n->valuelen) && (i < 32); ch++, i++) {
+			switch(*ch) {
+			case '\n':	kdb_printf("\n");		break;
+			case '\b':	kdb_printf("\b");		break;
+			case '\t':	kdb_printf("\t");		break;
+			default:	kdb_printf("%c", *ch);	break;
+			}
+		}
+		if (i == 32)
+			kdb_printf("...");
+		kdb_printf("\"(%d)\n", n->valuelen);
+	} else {
+		kdb_printf("(NULL)(%d)\n", n->valuelen);
+	}
+	kdb_printf(" hashval 0x%x whichfork %d flags <",
+		  (uint_t)n->hashval, n->whichfork);
+	if (n->flags & ATTR_ROOT)
+		kdb_printf("ROOT ");
+	if (n->flags & ATTR_SECURE)
+		kdb_printf("SECURE ");
+	if (n->flags & ATTR_CREATE)
+		kdb_printf("CREATE ");
+	if (n->flags & ATTR_REPLACE)
+		kdb_printf("REPLACE ");
+	if (n->flags & XFS_ATTR_INCOMPLETE)
+		kdb_printf("INCOMPLETE ");
+	i = ~(ATTR_ROOT | ATTR_SECURE |
+		ATTR_CREATE | ATTR_REPLACE | XFS_ATTR_INCOMPLETE);
+	if ((n->flags & i) != 0)
+		kdb_printf("0x%x", n->flags & i);
+	kdb_printf(">\n");
+	kdb_printf(" rename %d justcheck %d addname %d oknoent %d\n",
+		  n->rename, n->justcheck, n->addname, n->oknoent);
+	kdb_printf(" leaf: blkno %d index %d rmtblkno %d rmtblkcnt %d\n",
+		  n->blkno, n->index, n->rmtblkno, n->rmtblkcnt);
+	kdb_printf(" leaf2: blkno %d index %d rmtblkno %d rmtblkcnt %d\n",
+		  n->blkno2, n->index2, n->rmtblkno2, n->rmtblkcnt2);
+	kdb_printf(" inumber %llu dp 0x%p firstblock 0x%p flist 0x%p\n",
+		  (unsigned long long) n->inumber,
+		  n->dp, n->firstblock, n->flist);
+	kdb_printf(" trans 0x%p total %d\n",
+		  n->trans, n->total);
+}
+
+/*
+ * Print a da buffer structure.
+ */
+static void
+xfsidbg_xdabuf(xfs_dabuf_t *dabuf)
+{
+	int	i;
+
+	kdb_printf("nbuf %d dirty %d bbcount %d data 0x%p bps",
+		dabuf->nbuf, dabuf->dirty, dabuf->bbcount, dabuf->data);
+	for (i = 0; i < dabuf->nbuf; i++)
+		kdb_printf(" %d:0x%p", i, dabuf->bps[i]);
+	kdb_printf("\n");
+#ifdef XFS_DABUF_DEBUG
+	kdb_printf(" ra 0x%x prev 0x%x next 0x%x dev 0x%x blkno 0x%x\n",
+		dabuf->ra, dabuf->prev, dabuf->next, dabuf->dev, dabuf->blkno);
+#endif
+}
+
+/*
+ * Print a directory/attribute internal node block.
+ */
+static void
+xfsidbg_xdanode(xfs_da_intnode_t *node)
+{
+	xfs_da_node_hdr_t *h;
+	xfs_da_blkinfo_t *i;
+	xfs_da_node_entry_t *e;
+	int j;
+
+	h = &node->hdr;
+	i = &h->info;
+	kdb_printf("hdr info forw 0x%x back 0x%x magic 0x%x\n",
+		INT_GET(i->forw, ARCH_CONVERT), INT_GET(i->back, ARCH_CONVERT), INT_GET(i->magic, ARCH_CONVERT));
+	kdb_printf("hdr count %d level %d\n",
+		INT_GET(h->count, ARCH_CONVERT), INT_GET(h->level, ARCH_CONVERT));
+	for (j = 0, e = node->btree; j < INT_GET(h->count, ARCH_CONVERT); j++, e++) {
+		kdb_printf("btree %d hashval 0x%x before 0x%x\n",
+			j, (uint_t)INT_GET(e->hashval, ARCH_CONVERT), INT_GET(e->before, ARCH_CONVERT));
+	}
+}
+
+/*
+ * Print an xfs_da_state_blk structure.
+ */
+static void
+xfsidbg_xdastate(xfs_da_state_t *s)
+{
+	xfs_da_state_blk_t *eblk;
+
+	kdb_printf("args 0x%p mp 0x%p blocksize %u node_ents %u inleaf %u\n",
+		s->args, s->mp, s->blocksize, s->node_ents, s->inleaf);
+	if (s->args)
+		xfsidbg_xdaargs(s->args);
+
+	kdb_printf("path:  ");
+	xfs_dastate_path(&s->path);
+
+	kdb_printf("altpath:  ");
+	xfs_dastate_path(&s->altpath);
+
+	eblk = &s->extrablk;
+	kdb_printf("extra: valid %d, after %d\n", s->extravalid, s->extraafter);
+	kdb_printf(" bp 0x%p blkno 0x%x ", eblk->bp, eblk->blkno);
+	kdb_printf("index %d hashval 0x%x\n", eblk->index, (uint_t)eblk->hashval);
+}
+
+/*
+ * Print a directory leaf block.
+ */
+static void
+xfsidbg_xdirleaf(xfs_dir_leafblock_t *leaf)
+{
+	xfs_dir_leaf_hdr_t *h;
+	xfs_da_blkinfo_t *i;
+	xfs_dir_leaf_map_t *m;
+	xfs_dir_leaf_entry_t *e;
+	xfs_dir_leaf_name_t *n;
+	int j, k;
+	xfs_ino_t ino;
+
+	h = &leaf->hdr;
+	i = &h->info;
+	kdb_printf("hdr info forw 0x%x back 0x%x magic 0x%x\n",
+		INT_GET(i->forw, ARCH_CONVERT), INT_GET(i->back, ARCH_CONVERT), INT_GET(i->magic, ARCH_CONVERT));
+	kdb_printf("hdr count %d namebytes %d firstused %d holes %d\n",
+		INT_GET(h->count, ARCH_CONVERT), INT_GET(h->namebytes, ARCH_CONVERT), INT_GET(h->firstused, ARCH_CONVERT), h->holes);
+	for (j = 0, m = h->freemap; j < XFS_DIR_LEAF_MAPSIZE; j++, m++) {
+		kdb_printf("hdr freemap %d base %d size %d\n",
+			j, INT_GET(m->base, ARCH_CONVERT), INT_GET(m->size, ARCH_CONVERT));
+	}
+	for (j = 0, e = leaf->entries; j < INT_GET(h->count, ARCH_CONVERT); j++, e++) {
+		n = XFS_DIR_LEAF_NAMESTRUCT(leaf, INT_GET(e->nameidx, ARCH_CONVERT));
+		XFS_DIR_SF_GET_DIRINO_ARCH(&n->inumber, &ino, ARCH_CONVERT);
+		kdb_printf("leaf %d hashval 0x%x nameidx %d inumber %llu ",
+			j, (uint_t)INT_GET(e->hashval, ARCH_CONVERT),
+			INT_GET(e->nameidx, ARCH_CONVERT),
+			(unsigned long long)ino);
+		kdb_printf("namelen %d name \"", e->namelen);
+		for (k = 0; k < e->namelen; k++)
+			kdb_printf("%c", n->name[k]);
+		kdb_printf("\"\n");
+	}
+}
+
+/*
+ * Print a directory v2 data block, single or multiple.
+ */
+static void
+xfs_dir2data(void *addr, int size)
+{
+	xfs_dir2_data_t *db;
+	xfs_dir2_block_t *bb;
+	xfs_dir2_data_hdr_t *h;
+	xfs_dir2_data_free_t *m;
+	xfs_dir2_data_entry_t *e;
+	xfs_dir2_data_unused_t *u;
+	xfs_dir2_leaf_entry_t *l=NULL;
+	int j, k;
+	char *p;
+	char *t;
+	xfs_dir2_block_tail_t *tail=NULL;
+
+	db = (xfs_dir2_data_t *)addr;
+	bb = (xfs_dir2_block_t *)addr;
+	h = &db->hdr;
+	kdb_printf("hdr magic 0x%x (%s)\nhdr bestfree", INT_GET(h->magic, ARCH_CONVERT),
+		INT_GET(h->magic, ARCH_CONVERT) == XFS_DIR2_DATA_MAGIC ? "DATA" :
+			(INT_GET(h->magic, ARCH_CONVERT) == XFS_DIR2_BLOCK_MAGIC ? "BLOCK" : ""));
+	for (j = 0, m = h->bestfree; j < XFS_DIR2_DATA_FD_COUNT; j++, m++) {
+		kdb_printf(" %d: 0x%x@0x%x", j, INT_GET(m->length, ARCH_CONVERT), INT_GET(m->offset, ARCH_CONVERT));
+	}
+	kdb_printf("\n");
+	if (INT_GET(h->magic, ARCH_CONVERT) == XFS_DIR2_DATA_MAGIC)
+		t = (char *)db + size;
+	else {
+		/* XFS_DIR2_BLOCK_TAIL_P */
+		tail = (xfs_dir2_block_tail_t *)
+		       ((char *)bb + size - sizeof(xfs_dir2_block_tail_t));
+		l = XFS_DIR2_BLOCK_LEAF_P_ARCH(tail, ARCH_CONVERT);
+		t = (char *)l;
+	}
+	for (p = (char *)(h + 1); p < t; ) {
+		u = (xfs_dir2_data_unused_t *)p;
+		if (u->freetag == XFS_DIR2_DATA_FREE_TAG) {
+			kdb_printf("0x%lx unused freetag 0x%x length 0x%x tag 0x%x\n",
+				(unsigned long) (p - (char *)addr),
+				INT_GET(u->freetag, ARCH_CONVERT),
+				INT_GET(u->length, ARCH_CONVERT),
+				INT_GET(*XFS_DIR2_DATA_UNUSED_TAG_P_ARCH(u, ARCH_CONVERT), ARCH_CONVERT));
+			p += INT_GET(u->length, ARCH_CONVERT);
+			continue;
+		}
+		e = (xfs_dir2_data_entry_t *)p;
+		kdb_printf("0x%lx entry inumber %llu namelen %d name \"",
+			(unsigned long) (p - (char *)addr),
+			(unsigned long long) INT_GET(e->inumber, ARCH_CONVERT),
+			e->namelen);
+		for (k = 0; k < e->namelen; k++)
+			kdb_printf("%c", e->name[k]);
+		kdb_printf("\" tag 0x%x\n", INT_GET(*XFS_DIR2_DATA_ENTRY_TAG_P(e), ARCH_CONVERT));
+		p += XFS_DIR2_DATA_ENTSIZE(e->namelen);
+	}
+	if (INT_GET(h->magic, ARCH_CONVERT) == XFS_DIR2_DATA_MAGIC)
+		return;
+	for (j = 0; j < INT_GET(tail->count, ARCH_CONVERT); j++, l++) {
+		kdb_printf("0x%lx leaf %d hashval 0x%x address 0x%x (byte 0x%x)\n",
+			(unsigned long) ((char *)l - (char *)addr), j,
+			(uint_t)INT_GET(l->hashval, ARCH_CONVERT),
+			INT_GET(l->address, ARCH_CONVERT),
+			/* XFS_DIR2_DATAPTR_TO_BYTE */
+			INT_GET(l->address, ARCH_CONVERT) << XFS_DIR2_DATA_ALIGN_LOG);
+	}
+	kdb_printf("0x%lx tail count %d\n",
+		(unsigned long) ((char *)tail - (char *)addr),
+		INT_GET(tail->count, ARCH_CONVERT));
+}
+
+static void
+xfs_dir2leaf(xfs_dir2_leaf_t *leaf, int size)
+{
+	xfs_dir2_leaf_hdr_t *h;
+	xfs_da_blkinfo_t *i;
+	xfs_dir2_leaf_entry_t *e;
+	xfs_dir2_data_off_t *b;
+	xfs_dir2_leaf_tail_t *t;
+	int j;
+
+	h = &leaf->hdr;
+	i = &h->info;
+	e = leaf->ents;
+	kdb_printf("hdr info forw 0x%x back 0x%x magic 0x%x\n",
+		INT_GET(i->forw, ARCH_CONVERT), INT_GET(i->back, ARCH_CONVERT), INT_GET(i->magic, ARCH_CONVERT));
+	kdb_printf("hdr count %d stale %d\n", INT_GET(h->count, ARCH_CONVERT), INT_GET(h->stale, ARCH_CONVERT));
+	for (j = 0; j < INT_GET(h->count, ARCH_CONVERT); j++, e++) {
+		kdb_printf("0x%lx ent %d hashval 0x%x address 0x%x (byte 0x%x)\n",
+			(unsigned long) ((char *)e - (char *)leaf), j,
+			(uint_t)INT_GET(e->hashval, ARCH_CONVERT),
+			INT_GET(e->address, ARCH_CONVERT),
+			/* XFS_DIR2_DATAPTR_TO_BYTE */
+			INT_GET(e->address, ARCH_CONVERT) << XFS_DIR2_DATA_ALIGN_LOG);
+	}
+	if (INT_GET(i->magic, ARCH_CONVERT) == XFS_DIR2_LEAFN_MAGIC)
+		return;
+	/* XFS_DIR2_LEAF_TAIL_P */
+	t = (xfs_dir2_leaf_tail_t *)((char *)leaf + size - sizeof(*t));
+	b = XFS_DIR2_LEAF_BESTS_P_ARCH(t, ARCH_CONVERT);
+	for (j = 0; j < INT_GET(t->bestcount, ARCH_CONVERT); j++, b++) {
+		kdb_printf("0x%lx best %d 0x%x\n",
+			(unsigned long) ((char *)b - (char *)leaf), j,
+			INT_GET(*b, ARCH_CONVERT));
+	}
+	kdb_printf("tail bestcount %d\n", INT_GET(t->bestcount, ARCH_CONVERT));
+}
+
+/*
+ * Print a shortform directory.
+ */
+static void
+xfsidbg_xdirsf(xfs_dir_shortform_t *s)
+{
+	xfs_dir_sf_hdr_t *sfh;
+	xfs_dir_sf_entry_t *sfe;
+	xfs_ino_t ino;
+	int i, j;
+
+	sfh = &s->hdr;
+	XFS_DIR_SF_GET_DIRINO_ARCH(&sfh->parent, &ino, ARCH_CONVERT);
+	kdb_printf("hdr parent %llu", (unsigned long long)ino);
+	kdb_printf(" count %d\n", sfh->count);
+	for (i = 0, sfe = s->list; i < sfh->count; i++) {
+		XFS_DIR_SF_GET_DIRINO_ARCH(&sfe->inumber, &ino, ARCH_CONVERT);
+		kdb_printf("entry %d inumber %llu", i, (unsigned long long)ino);
+		kdb_printf(" namelen %d name \"", sfe->namelen);
+		for (j = 0; j < sfe->namelen; j++)
+			kdb_printf("%c", sfe->name[j]);
+		kdb_printf("\"\n");
+		sfe = XFS_DIR_SF_NEXTENTRY(sfe);
+	}
+}
+
+/*
+ * Print a shortform v2 directory.
+ */
+static void
+xfsidbg_xdir2sf(xfs_dir2_sf_t *s)
+{
+	xfs_dir2_sf_hdr_t *sfh;
+	xfs_dir2_sf_entry_t *sfe;
+	xfs_ino_t ino;
+	int i, j;
+
+	sfh = &s->hdr;
+	ino = XFS_DIR2_SF_GET_INUMBER_ARCH(s, &sfh->parent, ARCH_CONVERT);
+	kdb_printf("hdr count %d i8count %d parent %llu\n",
+		sfh->count, sfh->i8count, (unsigned long long) ino);
+	for (i = 0, sfe = XFS_DIR2_SF_FIRSTENTRY(s); i < sfh->count; i++) {
+		ino = XFS_DIR2_SF_GET_INUMBER_ARCH(s, XFS_DIR2_SF_INUMBERP(sfe), ARCH_CONVERT);
+		kdb_printf("entry %d inumber %llu offset 0x%x namelen %d name \"",
+			i, (unsigned long long) ino,
+			XFS_DIR2_SF_GET_OFFSET_ARCH(sfe, ARCH_CONVERT),
+			sfe->namelen);
+		for (j = 0; j < sfe->namelen; j++)
+			kdb_printf("%c", sfe->name[j]);
+		kdb_printf("\"\n");
+		sfe = XFS_DIR2_SF_NEXTENTRY(s, sfe);
+	}
+}
+
+/*
+ * Print a node-form v2 directory freemap block.
+ */
+static void
+xfsidbg_xdir2free(xfs_dir2_free_t *f)
+{
+	int	i;
+
+	kdb_printf("hdr magic 0x%x firstdb %d nvalid %d nused %d\n",
+		INT_GET(f->hdr.magic, ARCH_CONVERT), INT_GET(f->hdr.firstdb, ARCH_CONVERT), INT_GET(f->hdr.nvalid, ARCH_CONVERT), INT_GET(f->hdr.nused, ARCH_CONVERT));
+	for (i = 0; i < INT_GET(f->hdr.nvalid, ARCH_CONVERT); i++) {
+		kdb_printf("entry %d db %d count %d\n",
+			i, i + INT_GET(f->hdr.firstdb, ARCH_CONVERT), INT_GET(f->bests[i], ARCH_CONVERT));
+	}
+}
+
+#ifdef XFS_DIR_TRACE
+/*
+ * Print out the last "count" entries in the directory trace buffer.
+ */
+static void
+xfsidbg_xdirtrace(int count)
+{
+	ktrace_entry_t  *ktep;
+	ktrace_snap_t   kts;
+	int	     nentries;
+	int	     skip_entries;
+
+	if (xfs_dir_trace_buf == NULL) {
+		qprintf("The xfs directory trace buffer is not initialized\n");
+		return;
+	}
+	nentries = ktrace_nentries(xfs_dir_trace_buf);
+	if (count == -1) {
+		count = nentries;
+	}
+	if ((count <= 0) || (count > nentries)) {
+		qprintf("Invalid count.  There are %d entries.\n", nentries);
+		return;
+	}
+
+	ktep = ktrace_first(xfs_dir_trace_buf, &kts);
+	if (count != nentries) {
+		/*
+		 * Skip the total minus the number to look at minus one
+		 * for the entry returned by ktrace_first().
+		 */
+		skip_entries = nentries - count - 1;
+		ktep = ktrace_skip(xfs_dir_trace_buf, skip_entries, &kts);
+		if (ktep == NULL) {
+			qprintf("Skipped them all\n");
+			return;
+		}
+	}
+	while (ktep != NULL) {
+		if (xfs_dir_trace_entry(ktep))
+			qprintf("\n");
+		ktep = ktrace_next(xfs_dir_trace_buf, &kts);
+	}
+}
+#endif
+
+#ifdef XFS_DIR2_TRACE
+/*
+ * Print out the last "count" entries in the directory v2 trace buffer.
+ */
+static void
+xfsidbg_xdir2atrace(int count)
+{
+	ktrace_entry_t  *ktep;
+	ktrace_snap_t   kts;
+	int	     nentries;
+	int	     skip_entries;
+
+	if (xfs_dir2_trace_buf == NULL) {
+		qprintf("The xfs dirv2 trace buffer is not initialized\n");
+		return;
+	}
+	nentries = ktrace_nentries(xfs_dir2_trace_buf);
+	if (count == -1) {
+		count = nentries;
+	}
+	if ((count <= 0) || (count > nentries)) {
+		qprintf("Invalid count.  There are %d entries.\n", nentries);
+		return;
+	}
+
+	ktep = ktrace_first(xfs_dir2_trace_buf, &kts);
+	if (count != nentries) {
+		/*
+		 * Skip the total minus the number to look at minus one
+		 * for the entry returned by ktrace_first().
+		 */
+		skip_entries = nentries - count - 1;
+		ktep = ktrace_skip(xfs_dir2_trace_buf, skip_entries, &kts);
+		if (ktep == NULL) {
+			qprintf("Skipped them all\n");
+			return;
+		}
+	}
+	while (ktep != NULL) {
+		if (xfs_dir2_trace_entry(ktep))
+			qprintf("\n");
+		ktep = ktrace_next(xfs_dir2_trace_buf, &kts);
+	}
+}
+
+/*
+ * Print out the directory v2 trace buffer attached to the given inode.
+ */
+static void
+xfsidbg_xdir2itrace(xfs_inode_t *ip)
+{
+	ktrace_entry_t  *ktep;
+	ktrace_snap_t   kts;
+
+	if (ip->i_dir_trace == NULL) {
+		qprintf("The inode trace buffer is not initialized\n");
+		return;
+	}
+
+	ktep = ktrace_first(ip->i_dir_trace, &kts);
+	while (ktep != NULL) {
+		if (xfs_dir2_trace_entry(ktep))
+			qprintf("\n");
+		ktep = ktrace_next(ip->i_dir_trace, &kts);
+	}
+}
+#endif
+
+/*
+ * Print xfs extent list.
+ */
+static void
+xfsidbg_xexlist(xfs_inode_t *ip)
+{
+	xfs_xexlist_fork(ip, XFS_DATA_FORK);
+	if (XFS_IFORK_Q(ip))
+		xfs_xexlist_fork(ip, XFS_ATTR_FORK);
+}
+
+/*
+ * Print an xfs free-extent list.
+ */
+static void
+xfsidbg_xflist(xfs_bmap_free_t *flist)
+{
+	xfs_bmap_free_item_t	*item;
+
+	kdb_printf("flist@0x%p: first 0x%p count %d low %d\n", flist,
+		flist->xbf_first, flist->xbf_count, flist->xbf_low);
+	for (item = flist->xbf_first; item; item = item->xbfi_next) {
+		kdb_printf("item@0x%p: startblock %Lx blockcount %d", item,
+			(xfs_dfsbno_t)item->xbfi_startblock,
+			item->xbfi_blockcount);
+	}
+}
+
+/*
+ * Print out the help messages for these functions.
+ */
+static void
+xfsidbg_xhelp(void)
+{
+	struct xif	*p;
+
+	for (p = xfsidbg_funcs; p->name; p++)
+		kdb_printf("%-16s %s %s\n", p->name, p->args, p->help);
+}
+
+/*
+ * Print out an XFS in-core log structure.
+ */
+static void
+xfsidbg_xiclog(xlog_in_core_t *iclog)
+{
+	int i;
+	static char *ic_flags[] = {
+		"ACTIVE",	/* 0x0001 */
+		"WANT_SYNC",	/* 0x0002 */
+		"SYNCING",	/* 0X0004 */
+		"DONE_SYNC",	/* 0X0008 */
+		"DO_CALLBACK",	/* 0X0010 */
+		"CALLBACK",	/* 0X0020 */
+		"DIRTY",	/* 0X0040 */
+		"IOERROR",	/* 0X0080 */
+		"NOTUSED",	/* 0X8000 */
+		0
+	};
+
+	kdb_printf("xlog_in_core/header at 0x%p/0x%p\n",
+		iclog, iclog->hic_data);
+	kdb_printf("magicno: %x  cycle: %d  version: %d  lsn: 0x%Lx\n",
+		INT_GET(iclog->ic_header.h_magicno, ARCH_CONVERT), INT_GET(iclog->ic_header.h_cycle, ARCH_CONVERT),
+		INT_GET(iclog->ic_header.h_version, ARCH_CONVERT), INT_GET(iclog->ic_header.h_lsn, ARCH_CONVERT));
+	kdb_printf("tail_lsn: 0x%Lx  len: %d  prev_block: %d  num_ops: %d\n",
+		INT_GET(iclog->ic_header.h_tail_lsn, ARCH_CONVERT), INT_GET(iclog->ic_header.h_len, ARCH_CONVERT),
+		INT_GET(iclog->ic_header.h_prev_block, ARCH_CONVERT), INT_GET(iclog->ic_header.h_num_logops, ARCH_CONVERT));
+	kdb_printf("cycle_data: ");
+	for (i=0; i<(iclog->ic_size>>BBSHIFT); i++) {
+		kdb_printf("%x  ", INT_GET(iclog->ic_header.h_cycle_data[i], ARCH_CONVERT));
+	}
+	kdb_printf("\n");
+	kdb_printf("size: %d\n", INT_GET(iclog->ic_header.h_size, ARCH_CONVERT));
+	kdb_printf("\n");
+	kdb_printf("--------------------------------------------------\n");
+	kdb_printf("data: 0x%p  &forcesema: 0x%p  next: 0x%p bp: 0x%p\n",
+		iclog->ic_datap, &iclog->ic_forcesema, iclog->ic_next,
+		iclog->ic_bp);
+	kdb_printf("log: 0x%p  callb: 0x%p  callb_tail: 0x%p  roundoff: %d\n",
+		iclog->ic_log, iclog->ic_callback, iclog->ic_callback_tail,
+		iclog->ic_roundoff);
+	kdb_printf("size: %d (OFFSET: %d) trace: 0x%p refcnt: %d bwritecnt: %d",
+		iclog->ic_size, iclog->ic_offset,
+#ifdef XFS_LOG_TRACE
+		iclog->ic_trace,
+#else
+		NULL,
+#endif
+		iclog->ic_refcnt, iclog->ic_bwritecnt);
+	if (iclog->ic_state & XLOG_STATE_ALL)
+		printflags(iclog->ic_state, ic_flags, " state:");
+	else
+		kdb_printf(" state: INVALID 0x%x", iclog->ic_state);
+	kdb_printf("\n");
+}	/* xfsidbg_xiclog */
+
+
+/*
+ * Print all incore logs.
+ */
+static void
+xfsidbg_xiclogall(xlog_in_core_t *iclog)
+{
+    xlog_in_core_t *first_iclog = iclog;
+
+    do {
+	xfsidbg_xiclog(iclog);
+	kdb_printf("=================================================\n");
+	iclog = iclog->ic_next;
+    } while (iclog != first_iclog);
+}	/* xfsidbg_xiclogall */
+
+/*
+ * Print out the callback structures attached to an iclog.
+ */
+static void
+xfsidbg_xiclogcb(xlog_in_core_t *iclog)
+{
+	xfs_log_callback_t	*cb;
+	kdb_symtab_t		 symtab;
+
+	for (cb = iclog->ic_callback; cb != NULL; cb = cb->cb_next) {
+
+		if (kdbnearsym((unsigned long)cb->cb_func, &symtab)) {
+			unsigned long offval;
+
+			offval = (unsigned long)cb->cb_func - symtab.sym_start;
+
+			if (offval)
+				kdb_printf("func = %s+0x%lx",
+							symtab.sym_name,
+							offval);
+			else
+				kdb_printf("func = %s", symtab.sym_name);
+		} else
+			kdb_printf("func = ?? 0x%p", (void *)cb->cb_func);
+
+		kdb_printf(" arg 0x%p next 0x%p\n", cb->cb_arg, cb->cb_next);
+	}
+}
+
+#ifdef XFS_LOG_TRACE
+/*
+ * Print trace from incore log.
+ */
+static void
+xfsidbg_xiclogtrace(xlog_in_core_t *iclog)
+{
+	ktrace_entry_t  *ktep;
+	ktrace_snap_t   kts;
+	ktrace_t	*kt = iclog->ic_trace;
+
+	qprintf("iclog->ic_trace 0x%p\n", kt);
+	ktep = ktrace_first(kt, &kts);
+	while (ktep != NULL) {
+		switch ((__psint_t)ktep->val[0]) {
+		    case XLOG_TRACE_GRAB_FLUSH: {
+			    qprintf("grabbing semaphore\n");
+			    break;
+		    }
+		    case XLOG_TRACE_REL_FLUSH: {
+			    qprintf("releasing semaphore\n");
+			    break;
+		    }
+		    case XLOG_TRACE_SLEEP_FLUSH: {
+			    qprintf("sleeping on semaphore\n");
+			    break;
+		    }
+		    case XLOG_TRACE_WAKE_FLUSH: {
+			    qprintf("waking up on semaphore\n");
+			    break;
+		    }
+		    default: {
+		    }
+		}
+		ktep = ktrace_next(kt, &kts);
+	}
+}       /* xfsidbg_xiclogtrace */
+#endif
+
+/*
+ * Print all of the inodes attached to the given mount structure.
+ */
+static void
+xfsidbg_xinodes(xfs_mount_t *mp)
+{
+	xfs_inode_t	*ip;
+
+	kdb_printf("xfs_mount at 0x%p\n", mp);
+	ip = mp->m_inodes;
+	if (ip != NULL) {
+		do {
+			if (ip->i_mount == NULL) {
+				ip = ip->i_mnext;
+				continue;
+			}
+			kdb_printf("\n");
+			xfsidbg_xnode(ip);
+			ip = ip->i_mnext;
+		} while (ip != mp->m_inodes);
+	}
+	kdb_printf("\nEnd of Inodes\n");
+}
+
+static void
+xfsidbg_delayed_blocks(xfs_mount_t *mp)
+{
+	xfs_inode_t	*ip;
+	unsigned int	total = 0;
+	unsigned int	icount = 0;
+
+	ip = mp->m_inodes;
+	if (ip != NULL) {
+		do {
+			if (ip->i_mount == NULL) {
+				ip = ip->i_mnext;
+				continue;
+			}
+			if (ip->i_delayed_blks) {
+				total += ip->i_delayed_blks;
+				icount++;
+			}
+			ip = ip->i_mnext;
+		} while (ip != mp->m_inodes);
+	}
+	kdb_printf("delayed blocks total: %d in %d inodes\n", total, icount);
+}
+
+static void
+xfsidbg_xinodes_quiesce(xfs_mount_t *mp)
+{
+	xfs_inode_t	*ip;
+
+	kdb_printf("xfs_mount at 0x%p\n", mp);
+	ip = mp->m_inodes;
+	if (ip != NULL) {
+		do {
+			if (ip->i_mount == NULL) {
+				ip = ip->i_mnext;
+				continue;
+			}
+			if (!(ip->i_flags & XFS_IQUIESCE)) {
+				kdb_printf("ip 0x%p not quiesced\n", ip);
+			}
+			ip = ip->i_mnext;
+		} while (ip != mp->m_inodes);
+	}
+	kdb_printf("\nEnd of Inodes\n");
+}
+
+static char *
+xfsidbg_get_cstate(int state)
+{
+	switch(state) {
+	case  XLOG_STATE_COVER_IDLE:
+		return("idle");
+	case  XLOG_STATE_COVER_NEED:
+		return("need");
+	case  XLOG_STATE_COVER_DONE:
+		return("done");
+	case  XLOG_STATE_COVER_NEED2:
+		return("need2");
+	case  XLOG_STATE_COVER_DONE2:
+		return("done2");
+	default:
+		return("unknown");
+	}
+}
+
+/*
+ * Print out an XFS log structure.
+ */
+static void
+xfsidbg_xlog(xlog_t *log)
+{
+	int rbytes;
+	int wbytes;
+	static char *t_flags[] = {
+		"CHKSUM_MISMATCH",	/* 0x01 */
+		"ACTIVE_RECOVERY",	/* 0x02 */
+		"RECOVERY_NEEDED",	/* 0x04 */
+		"IO_ERROR",		/* 0x08 */
+		0
+	};
+
+	kdb_printf("xlog at 0x%p\n", log);
+	kdb_printf("&flushsm: 0x%p  flushcnt: %d tic_cnt: %d	 tic_tcnt: %d  \n",
+		&log->l_flushsema, log->l_flushcnt,
+		log->l_ticket_cnt, log->l_ticket_tcnt);
+	kdb_printf("freelist: 0x%p  tail: 0x%p	ICLOG: 0x%p  \n",
+		log->l_freelist, log->l_tail, log->l_iclog);
+	kdb_printf("&icloglock: 0x%p  tail_lsn: %s  last_sync_lsn: %s \n",
+		&log->l_icloglock, xfs_fmtlsn(&log->l_tail_lsn),
+		xfs_fmtlsn(&log->l_last_sync_lsn));
+	kdb_printf("mp: 0x%p  xbuf: 0x%p  roundoff: %d  l_covered_state: %s \n",
+		log->l_mp, log->l_xbuf, log->l_roundoff,
+			xfsidbg_get_cstate(log->l_covered_state));
+	kdb_printf("flags: ");
+	printflags(log->l_flags, t_flags,"log");
+	kdb_printf("  logBBstart: %lld logsize: %d logBBsize: %d\n",
+		(long long) log->l_logBBstart,
+		log->l_logsize,log->l_logBBsize);
+	kdb_printf("curr_cycle: %d  prev_cycle: %d  curr_block: %d  prev_block: %d\n",
+	     log->l_curr_cycle, log->l_prev_cycle, log->l_curr_block,
+	     log->l_prev_block);
+	kdb_printf("iclog_bak: 0x%p  iclog_size: 0x%x (%d)  num iclogs: %d\n",
+		log->l_iclog_bak, log->l_iclog_size, log->l_iclog_size,
+		log->l_iclog_bufs);
+	kdb_printf("l_iclog_hsize %d l_iclog_heads %d\n",
+		log->l_iclog_hsize, log->l_iclog_heads);
+	kdb_printf("l_sectbb_log %u l_sectbb_mask %u\n",
+		log->l_sectbb_log, log->l_sectbb_mask);
+	kdb_printf("&grant_lock: 0x%p  resHeadQ: 0x%p  wrHeadQ: 0x%p\n",
+		&log->l_grant_lock, log->l_reserve_headq, log->l_write_headq);
+	kdb_printf("GResCycle: %d  GResBytes: %d  GWrCycle: %d  GWrBytes: %d\n",
+		log->l_grant_reserve_cycle, log->l_grant_reserve_bytes,
+		log->l_grant_write_cycle, log->l_grant_write_bytes);
+	rbytes = log->l_grant_reserve_bytes + log->l_roundoff;
+	wbytes = log->l_grant_write_bytes + log->l_roundoff;
+	qprintf("GResBlocks: %d GResRemain: %d  GWrBlocks: %d GWrRemain: %d\n",
+	       rbytes / BBSIZE, rbytes % BBSIZE,
+	       wbytes / BBSIZE, wbytes % BBSIZE);
+#ifdef XFS_LOG_TRACE
+	qprintf("trace: 0x%p  grant_trace: use xlog value\n", log->l_trace);
+#endif
+}	/* xfsidbg_xlog */
+
+#ifdef XFS_LOG_TRACE
+/*
+ * Print grant trace for a log.
+ */
+static void
+xfsidbg_xlog_granttrace(xlog_t *log)
+{
+	ktrace_entry_t  *ktep;
+	ktrace_snap_t   kts;
+	ktrace_t	*kt;
+
+	if (((__psint_t)log) == ((__psint_t)-1)) {
+		qprintf("Usage: xl_grtr <log>\n");
+		return;
+	}
+	if ((kt = log->l_grant_trace))
+		qprintf("log->l_grant_trace 0x%p\n", kt);
+	else {
+		qprintf("log->l_grant_trace is empty!\n");
+		return;
+	}
+	ktep = ktrace_first(kt, &kts);
+	while (ktep != NULL) {
+		qprintf("%s\n", (char *)ktep->val[11]);
+		qprintf("  tic:0x%p resQ:0x%p wrQ:0x%p ",
+			ktep->val[0], ktep->val[1], ktep->val[2]);
+		qprintf("  GrResC:%ld GrResB:%ld GrWrC:%ld GrWrB:%ld \n",
+			(long)ktep->val[3], (long)ktep->val[4],
+			(long)ktep->val[5], (long)ktep->val[6]);
+		qprintf("  HeadC:%ld HeadB:%ld TailC:%ld TailB:%ld\n",
+			(long)ktep->val[7], (long)ktep->val[8],
+			(long)ktep->val[9], (long)ktep->val[10]);
+		ktep = ktrace_next(kt, &kts);
+	}
+}       /* xfsidbg_xlog_granttrace */
+#endif
+
+/*
+ * Print out an XFS recovery transaction
+ */
+static void
+xfsidbg_xlog_ritem(xlog_recover_item_t *item)
+{
+	int i = XLOG_MAX_REGIONS_IN_ITEM;
+
+	kdb_printf("(xlog_recover_item 0x%p) ", item);
+	kdb_printf("next: 0x%p prev: 0x%p type: %d cnt: %d ttl: %d\n",
+		item->ri_next, item->ri_prev, ITEM_TYPE(item), item->ri_cnt,
+		item->ri_total);
+	for ( ; i > 0; i--) {
+		if (!item->ri_buf[XLOG_MAX_REGIONS_IN_ITEM-i].i_addr)
+			break;
+		kdb_printf("a: 0x%p l: %d ",
+			item->ri_buf[XLOG_MAX_REGIONS_IN_ITEM-i].i_addr,
+			item->ri_buf[XLOG_MAX_REGIONS_IN_ITEM-i].i_len);
+	}
+	kdb_printf("\n");
+}	/* xfsidbg_xlog_ritem */
+
+/*
+ * Print out an XFS recovery transaction
+ */
+static void
+xfsidbg_xlog_rtrans(xlog_recover_t *trans)
+{
+	xlog_recover_item_t *rip, *first_rip;
+
+	kdb_printf("(xlog_recover 0x%p) ", trans);
+	kdb_printf("tid: %x type: %d items: %d ttid: 0x%x  ",
+		trans->r_log_tid, trans->r_theader.th_type,
+		trans->r_theader.th_num_items, trans->r_theader.th_tid);
+	kdb_printf("itemq: 0x%p\n", trans->r_itemq);
+	if (trans->r_itemq) {
+		rip = first_rip = trans->r_itemq;
+		do {
+			kdb_printf("(recovery item: 0x%p) ", rip);
+			kdb_printf("type: %d cnt: %d total: %d\n",
+				ITEM_TYPE(rip), rip->ri_cnt, rip->ri_total);
+			rip = rip->ri_next;
+		} while (rip != first_rip);
+	}
+}	/* xfsidbg_xlog_rtrans */
+
+static void
+xfsidbg_xlog_buf_logitem(xlog_recover_item_t *item)
+{
+	xfs_buf_log_format_t	*buf_f;
+	int			i, j;
+	int			bit;
+	int			nbits;
+	unsigned int		*data_map;
+	unsigned int		map_size;
+	int			size;
+
+	buf_f = (xfs_buf_log_format_t *)item->ri_buf[0].i_addr;
+	if (buf_f->blf_flags & XFS_BLI_INODE_BUF) {
+		kdb_printf("\tINODE BUF <blkno=0x%Lx, len=0x%x>\n",
+			buf_f->blf_blkno, buf_f->blf_len);
+	} else if (buf_f->blf_flags & (XFS_BLI_UDQUOT_BUF | XFS_BLI_GDQUOT_BUF)) {
+		kdb_printf("\tDQUOT BUF <blkno=0x%Lx, len=0x%x>\n",
+			buf_f->blf_blkno, buf_f->blf_len);
+	} else {
+		data_map = buf_f->blf_data_map;
+		map_size = buf_f->blf_map_size;
+		kdb_printf("\tREG BUF <blkno=0x%Lx, len=0x%x map 0x%p size %d>\n",
+			buf_f->blf_blkno, buf_f->blf_len, data_map, map_size);
+		bit = 0;
+		i = 0;  /* 0 is the buf format structure */
+		while (1) {
+			bit = xfs_next_bit(data_map, map_size, bit);
+			if (bit == -1)
+				break;
+			nbits = xfs_contig_bits(data_map, map_size, bit);
+			size = ((uint)bit << XFS_BLI_SHIFT)+(nbits<<XFS_BLI_SHIFT);
+			kdb_printf("\t\tlogbuf.i_addr 0x%p, size 0x%x\n",
+				item->ri_buf[i].i_addr, size);
+			kdb_printf("\t\t\t\"");
+			for (j=0; j<8 && j<size; j++) {
+				kdb_printf("%02x", ((char *)item->ri_buf[i].i_addr)[j]);
+			}
+			kdb_printf("...\"\n");
+			i++;
+			bit += nbits;
+		}
+
+	}
+}
+
+/*
+ * Print out an ENTIRE XFS recovery transaction
+ */
+static void
+xfsidbg_xlog_rtrans_entire(xlog_recover_t *trans)
+{
+	xlog_recover_item_t *item, *first_rip;
+
+	kdb_printf("(Recovering Xact 0x%p) ", trans);
+	kdb_printf("tid: %x type: %d nitems: %d ttid: 0x%x  ",
+		trans->r_log_tid, trans->r_theader.th_type,
+		trans->r_theader.th_num_items, trans->r_theader.th_tid);
+	kdb_printf("itemq: 0x%p\n", trans->r_itemq);
+	if (trans->r_itemq) {
+		item = first_rip = trans->r_itemq;
+		do {
+			/*
+			   kdb_printf("(recovery item: 0x%x) ", item);
+			   kdb_printf("type: %d cnt: %d total: %d\n",
+				   item->ri_type, item->ri_cnt, item->ri_total);
+				   */
+			if ((ITEM_TYPE(item) == XFS_LI_BUF) ||
+			    (ITEM_TYPE(item) == XFS_LI_6_1_BUF) ||
+			    (ITEM_TYPE(item) == XFS_LI_5_3_BUF)) {
+				kdb_printf("BUF:");
+				xfsidbg_xlog_buf_logitem(item);
+			} else if ((ITEM_TYPE(item) == XFS_LI_INODE) ||
+				   (ITEM_TYPE(item) == XFS_LI_6_1_INODE) ||
+				   (ITEM_TYPE(item) == XFS_LI_5_3_INODE)) {
+				kdb_printf("INODE:\n");
+			} else if (ITEM_TYPE(item) == XFS_LI_EFI) {
+				kdb_printf("EFI:\n");
+			} else if (ITEM_TYPE(item) == XFS_LI_EFD) {
+				kdb_printf("EFD:\n");
+			} else if (ITEM_TYPE(item) == XFS_LI_DQUOT) {
+				kdb_printf("DQUOT:\n");
+			} else if ((ITEM_TYPE(item) == XFS_LI_QUOTAOFF)) {
+				kdb_printf("QUOTAOFF:\n");
+			} else {
+				kdb_printf("UNKNOWN LOGITEM 0x%x\n", ITEM_TYPE(item));
+			}
+			item = item->ri_next;
+		} while (item != first_rip);
+	}
+}	/* xfsidbg_xlog_rtrans */
+
+/*
+ * Print out an XFS ticket structure.
+ */
+static void
+xfsidbg_xlog_tic(xlog_ticket_t *tic)
+{
+	static char *t_flags[] = {
+		"INIT",		/* 0x1 */
+		"PERM_RES",	/* 0x2 */
+		"IN_Q",		/* 0x4 */
+		0
+	};
+
+	kdb_printf("xlog_ticket at 0x%p\n", tic);
+	kdb_printf("next: 0x%p  prev: 0x%p  tid: 0x%x  \n",
+		tic->t_next, tic->t_prev, tic->t_tid);
+	kdb_printf("curr_res: %d  unit_res: %d  ocnt: %d  cnt: %d\n",
+		tic->t_curr_res, tic->t_unit_res, (int)tic->t_ocnt,
+		(int)tic->t_cnt);
+	kdb_printf("clientid: %c  \n", tic->t_clientid);
+	printflags(tic->t_flags, t_flags,"ticket");
+	kdb_printf("\n");
+}	/* xfsidbg_xlog_tic */
+
+/*
+ * Print out a single log item.
+ */
+static void
+xfsidbg_xlogitem(xfs_log_item_t *lip)
+{
+	xfs_log_item_t	*bio_lip;
+	static char *lid_type[] = {
+		"???",		/* 0 */
+		"5-3-buf",	/* 1 */
+		"5-3-inode",	/* 2 */
+		"efi",		/* 3 */
+		"efd",		/* 4 */
+		"iunlink",	/* 5 */
+		"6-1-inode",	/* 6 */
+		"6-1-buf",	/* 7 */
+		"inode",	/* 8 */
+		"buf",		/* 9 */
+		"dquot",	/* 10 */
+		0
+		};
+	static char *li_flags[] = {
+		"in ail",	/* 0x1 */
+		0
+		};
+
+	kdb_printf("type %s mountp 0x%p flags ",
+		lid_type[lip->li_type - XFS_LI_5_3_BUF + 1],
+		lip->li_mountp);
+	printflags((uint)(lip->li_flags), li_flags,"log");
+	kdb_printf("\n");
+	kdb_printf("ail forw 0x%p ail back 0x%p lsn %s\ndesc %p ops 0x%p",
+		lip->li_ail.ail_forw, lip->li_ail.ail_back,
+		xfs_fmtlsn(&(lip->li_lsn)), lip->li_desc, lip->li_ops);
+	kdb_printf(" iodonefunc &0x%p\n", lip->li_cb);
+	if (lip->li_type == XFS_LI_BUF) {
+		bio_lip = lip->li_bio_list;
+		if (bio_lip != NULL) {
+			kdb_printf("iodone list:\n");
+		}
+		while (bio_lip != NULL) {
+			kdb_printf("item 0x%p func 0x%p\n",
+				bio_lip, bio_lip->li_cb);
+			bio_lip = bio_lip->li_bio_list;
+		}
+	}
+	switch (lip->li_type) {
+	case XFS_LI_BUF:
+		xfs_buf_item_print((xfs_buf_log_item_t *)lip, 0);
+		break;
+	case XFS_LI_INODE:
+		xfs_inode_item_print((xfs_inode_log_item_t *)lip, 0);
+		break;
+	case XFS_LI_EFI:
+		xfs_efi_item_print((xfs_efi_log_item_t *)lip, 0);
+		break;
+	case XFS_LI_EFD:
+		xfs_efd_item_print((xfs_efd_log_item_t *)lip, 0);
+		break;
+	case XFS_LI_DQUOT:
+		xfs_dquot_item_print((xfs_dq_logitem_t *)lip, 0);
+		break;
+	case XFS_LI_QUOTAOFF:
+		xfs_qoff_item_print((xfs_qoff_logitem_t *)lip, 0);
+		break;
+
+	default:
+		kdb_printf("Unknown item type %d\n", lip->li_type);
+		break;
+	}
+}
+
+/*
+ * Print out a summary of the AIL hanging off of a mount struct.
+ */
+static void
+xfsidbg_xaildump(xfs_mount_t *mp)
+{
+	xfs_log_item_t *lip;
+	static char *lid_type[] = {
+		"???",		/* 0 */
+		"5-3-buf",	/* 1 */
+		"5-3-inode",	/* 2 */
+		"efi",		/* 3 */
+		"efd",		/* 4 */
+		"iunlink",	/* 5 */
+		"6-1-inode",	/* 6 */
+		"6-1-buf",	/* 7 */
+		"inode",	/* 8 */
+		"buf",		/* 9 */
+		"dquot",	/* 10 */
+		0
+		};
+	static char *li_flags[] = {
+		"in ail",	/* 0x1 */
+		0
+		};
+	int count;
+
+	if ((mp->m_ail.ail_forw == NULL) ||
+	    (mp->m_ail.ail_forw == (xfs_log_item_t *)&mp->m_ail)) {
+		kdb_printf("AIL is empty\n");
+		return;
+	}
+	kdb_printf("AIL for mp 0x%p, oldest first\n", mp);
+	lip = (xfs_log_item_t*)mp->m_ail.ail_forw;
+	for (count = 0; lip; count++) {
+		kdb_printf("[%d] type %s ", count,
+			      lid_type[lip->li_type - XFS_LI_5_3_BUF + 1]);
+		printflags((uint)(lip->li_flags), li_flags, "flags:");
+		kdb_printf("  lsn %s\n   ", xfs_fmtlsn(&(lip->li_lsn)));
+		switch (lip->li_type) {
+		case XFS_LI_BUF:
+			xfs_buf_item_print((xfs_buf_log_item_t *)lip, 1);
+			break;
+		case XFS_LI_INODE:
+			xfs_inode_item_print((xfs_inode_log_item_t *)lip, 1);
+			break;
+		case XFS_LI_EFI:
+			xfs_efi_item_print((xfs_efi_log_item_t *)lip, 1);
+			break;
+		case XFS_LI_EFD:
+			xfs_efd_item_print((xfs_efd_log_item_t *)lip, 1);
+			break;
+		case XFS_LI_DQUOT:
+			xfs_dquot_item_print((xfs_dq_logitem_t *)lip, 1);
+			break;
+		case XFS_LI_QUOTAOFF:
+			xfs_qoff_item_print((xfs_qoff_logitem_t *)lip, 1);
+			break;
+		default:
+			kdb_printf("Unknown item type %d\n", lip->li_type);
+			break;
+		}
+
+		if (lip->li_ail.ail_forw == (xfs_log_item_t*)&mp->m_ail) {
+			lip = NULL;
+		} else {
+			lip = lip->li_ail.ail_forw;
+		}
+	}
+}
+
+/*
+ * Print xfs mount structure.
+ */
+static void
+xfsidbg_xmount(xfs_mount_t *mp)
+{
+	static char *xmount_flags[] = {
+		"WSYNC",	/* 0x0001 */
+		"INO64",	/* 0x0002 */
+		"RQCHK",	/* 0x0004 */
+		"FSCLEAN",	/* 0x0008 */
+		"FSSHUTDN",	/* 0x0010 */
+		"NOATIME",	/* 0x0020 */
+		"RETERR",	/* 0x0040 */
+		"NOALIGN",	/* 0x0080 */
+		"UNSHRD",	/* 0x0100 */
+		"RGSTRD",	/* 0x0200 */
+		"NORECVR",	/* 0x0400 */
+		"SHRD",		/* 0x0800 */
+		"IOSZ",		/* 0x1000 */
+		"OSYNC",	/* 0x2000 */
+		"NOUUID",	/* 0x4000 */
+		"32BIT",	/* 0x8000 */
+		"NOLOGFLUSH",	/* 0x10000 */
+		0
+	};
+
+	static char *quota_flags[] = {
+		"UQ",		/* 0x0001 */
+		"UQE",		/* 0x0002 */
+		"UQCHKD",	/* 0x0004 */
+		"PQ",		/* 0x0008 (IRIX ondisk) */
+		"GQE",		/* 0x0010 */
+		"GQCHKD",	/* 0x0020 */
+		"GQ",		/* 0x0040 */
+		"UQACTV",	/* 0x0080 */
+		"GQACTV",	/* 0x0100 */
+		"QMAYBE",	/* 0x0200 */
+		0
+	};
+
+	kdb_printf("xfs_mount at 0x%p\n", mp);
+	kdb_printf("vfsp 0x%p tid 0x%x ail_lock 0x%p &ail 0x%p\n",
+		XFS_MTOVFS(mp), mp->m_tid, &mp->m_ail_lock, &mp->m_ail);
+	kdb_printf("ail_gen 0x%x &sb 0x%p\n",
+		mp->m_ail_gen, &mp->m_sb);
+	kdb_printf("sb_lock 0x%p sb_bp 0x%p dev 0x%x logdev 0x%x rtdev 0x%x\n",
+		&mp->m_sb_lock, mp->m_sb_bp,
+		mp->m_ddev_targp ? mp->m_ddev_targp->pbr_dev : 0,
+		mp->m_logdev_targp ? mp->m_logdev_targp->pbr_dev : 0,
+		mp->m_rtdev_targp ? mp->m_rtdev_targp->pbr_dev : 0);
+	kdb_printf("bsize %d agfrotor %d agirotor %d ihash 0x%p ihsize %d\n",
+		mp->m_bsize, mp->m_agfrotor, mp->m_agirotor,
+		mp->m_ihash, mp->m_ihsize);
+	kdb_printf("inodes 0x%p ilock 0x%p ireclaims 0x%x\n",
+		mp->m_inodes, &mp->m_ilock, mp->m_ireclaims);
+	kdb_printf("readio_log 0x%x readio_blocks 0x%x ",
+		mp->m_readio_log, mp->m_readio_blocks);
+	kdb_printf("writeio_log 0x%x writeio_blocks 0x%x\n",
+		mp->m_writeio_log, mp->m_writeio_blocks);
+	kdb_printf("logbufs %d logbsize %d LOG 0x%p\n", mp->m_logbufs,
+		mp->m_logbsize, mp->m_log);
+	kdb_printf("rsumlevels 0x%x rsumsize 0x%x rbmip 0x%p rsumip 0x%p\n",
+		mp->m_rsumlevels, mp->m_rsumsize, mp->m_rbmip, mp->m_rsumip);
+	kdb_printf("rootip 0x%p\n", mp->m_rootip);
+	kdb_printf("dircook_elog %d blkbit_log %d blkbb_log %d agno_log %d\n",
+		mp->m_dircook_elog, mp->m_blkbit_log, mp->m_blkbb_log,
+		mp->m_agno_log);
+	kdb_printf("agino_log %d nreadaheads %d inode cluster size %d\n",
+		mp->m_agino_log, mp->m_nreadaheads,
+		mp->m_inode_cluster_size);
+	kdb_printf("blockmask 0x%x blockwsize 0x%x blockwmask 0x%x\n",
+		mp->m_blockmask, mp->m_blockwsize, mp->m_blockwmask);
+	kdb_printf("alloc_mxr[lf,nd] %d %d alloc_mnr[lf,nd] %d %d\n",
+		mp->m_alloc_mxr[0], mp->m_alloc_mxr[1],
+		mp->m_alloc_mnr[0], mp->m_alloc_mnr[1]);
+	kdb_printf("bmap_dmxr[lfnr,ndnr] %d %d bmap_dmnr[lfnr,ndnr] %d %d\n",
+		mp->m_bmap_dmxr[0], mp->m_bmap_dmxr[1],
+		mp->m_bmap_dmnr[0], mp->m_bmap_dmnr[1]);
+	kdb_printf("inobt_mxr[lf,nd] %d %d inobt_mnr[lf,nd] %d %d\n",
+		mp->m_inobt_mxr[0], mp->m_inobt_mxr[1],
+		mp->m_inobt_mnr[0], mp->m_inobt_mnr[1]);
+	kdb_printf("ag_maxlevels %d bm_maxlevels[d,a] %d %d in_maxlevels %d\n",
+		mp->m_ag_maxlevels, mp->m_bm_maxlevels[0],
+		mp->m_bm_maxlevels[1], mp->m_in_maxlevels);
+	kdb_printf("perag 0x%p &peraglock 0x%p &growlock 0x%p\n",
+		mp->m_perag, &mp->m_peraglock, &mp->m_growlock);
+	printflags(mp->m_flags, xmount_flags,"flags");
+	kdb_printf("ialloc_inos %d ialloc_blks %d litino %d\n",
+		mp->m_ialloc_inos, mp->m_ialloc_blks, mp->m_litino);
+	kdb_printf("dir_node_ents %u attr_node_ents %u\n",
+		mp->m_dir_node_ents, mp->m_attr_node_ents);
+	kdb_printf("attroffset %d maxicount %Ld inoalign_mask %d\n",
+		mp->m_attroffset, mp->m_maxicount, mp->m_inoalign_mask);
+	kdb_printf("resblks %Ld resblks_avail %Ld\n", mp->m_resblks,
+		mp->m_resblks_avail);
+#if XFS_BIG_INUMS
+	kdb_printf(" inoadd %llx\n", (unsigned long long) mp->m_inoadd);
+#else
+	kdb_printf("\n");
+#endif
+	if (mp->m_quotainfo)
+		kdb_printf("quotainfo 0x%p (uqip = 0x%p, gqip = 0x%p)\n",
+			mp->m_quotainfo,
+			mp->m_quotainfo->qi_uquotaip,
+			mp->m_quotainfo->qi_gquotaip);
+	else
+		kdb_printf("quotainfo NULL\n");
+	printflags(mp->m_qflags, quota_flags,"quotaflags");
+	kdb_printf("\n");
+	kdb_printf("dalign %d swidth %d sinoalign %d attr_magicpct %d dir_magicpct %d\n",
+		mp->m_dalign, mp->m_swidth, mp->m_sinoalign,
+		mp->m_attr_magicpct, mp->m_dir_magicpct);
+	kdb_printf("mk_sharedro %d inode_quiesce %d sectbb_log %d\n",
+		mp->m_mk_sharedro, mp->m_inode_quiesce, mp->m_sectbb_log);
+	kdb_printf("dirversion %d dirblkfsbs %d &dirops 0x%p\n",
+		mp->m_dirversion, mp->m_dirblkfsbs, &mp->m_dirops);
+	kdb_printf("dirblksize %d dirdatablk 0x%Lx dirleafblk 0x%Lx dirfreeblk 0x%Lx\n",
+		mp->m_dirblksize,
+		(xfs_dfiloff_t)mp->m_dirdatablk,
+		(xfs_dfiloff_t)mp->m_dirleafblk,
+		(xfs_dfiloff_t)mp->m_dirfreeblk);
+	kdb_printf("chsize %d chash 0x%p\n",
+		mp->m_chsize, mp->m_chash);
+	kdb_printf("m_frozen %d m_active_trans %d\n",
+		mp->m_frozen, mp->m_active_trans.counter);
+	if (mp->m_fsname != NULL)
+		kdb_printf("mountpoint \"%s\"\n", mp->m_fsname);
+	else
+		kdb_printf("No name!!!\n");
+
+}
+
+static void
+xfsidbg_xihash(xfs_mount_t *mp)
+{
+	xfs_ihash_t	*ih;
+	int		i;
+	int		j;
+	int		total;
+	int		numzeros;
+	xfs_inode_t	*ip;
+	int		*hist;
+	int		hist_bytes = mp->m_ihsize * sizeof(int);
+	int		hist2[21];
+
+	hist = (int *) kmalloc(hist_bytes, GFP_KERNEL);
+
+	if (hist == NULL) {
+		kdb_printf("xfsidbg_xihash: kmalloc(%d) failed!\n",
+							hist_bytes);
+		return;
+	}
+
+	for (i = 0; i < mp->m_ihsize; i++) {
+		ih = mp->m_ihash + i;
+		j = 0;
+		for (ip = ih->ih_next; ip != NULL; ip = ip->i_next)
+			j++;
+		hist[i] = j;
+	}
+
+	numzeros = total = 0;
+
+	for (i = 0; i < 21; i++)
+		hist2[i] = 0;
+
+	for (i = 0; i < mp->m_ihsize; i++)  {
+		kdb_printf("%d ", hist[i]);
+		total += hist[i];
+		numzeros += hist[i] == 0 ? 1 : 0;
+		if (hist[i] > 20)
+			j = 20;
+		else
+			j = hist[i];
+
+		if (! (j <= 20)) {
+			kdb_printf("xfsidbg_xihash: (j > 20)/%d @ line # %d\n",
+							j, __LINE__);
+			return;
+		}
+
+		hist2[j]++;
+	}
+
+	kdb_printf("\n");
+
+	kdb_printf("total inodes = %d, average length = %d, adjusted average = %d \n",
+		total, total / mp->m_ihsize,
+		total / (mp->m_ihsize - numzeros));
+
+	for (i = 0; i < 21; i++)  {
+		kdb_printf("%d - %d , ", i, hist2[i]);
+	}
+	kdb_printf("\n");
+	kfree(hist);
+}
+
+/*
+ * Command to print xfs inodes: kp xnode <addr>
+ */
+static void
+xfsidbg_xnode(xfs_inode_t *ip)
+{
+	static char *tab_flags[] = {
+		"grio",		/* XFS_IGRIO */
+		"uiosize",	/* XFS_IUIOSZ */
+		"quiesce",	/* XFS_IQUIESCE */
+		"reclaim",	/* XFS_IRECLAIM */
+		"stale",	/* XFS_ISTALE */
+		NULL
+	};
+
+	kdb_printf("hash 0x%p next 0x%p prevp 0x%p mount 0x%p\n",
+		ip->i_hash,
+		ip->i_next,
+		ip->i_prevp,
+		ip->i_mount);
+	kdb_printf("mnext 0x%p mprev 0x%p vnode 0x%p \n",
+		ip->i_mnext,
+		ip->i_mprev,
+		XFS_ITOV_NULL(ip));
+	kdb_printf("dev %x ino %s\n",
+		ip->i_mount->m_dev,
+		xfs_fmtino(ip->i_ino, ip->i_mount));
+	kdb_printf("blkno 0x%llx len 0x%x boffset 0x%x\n",
+		(long long) ip->i_blkno,
+		ip->i_len,
+		ip->i_boffset);
+	kdb_printf("transp 0x%p &itemp 0x%p\n",
+		ip->i_transp,
+		ip->i_itemp);
+	kdb_printf("&lock 0x%p &iolock 0x%p ",
+		&ip->i_lock,
+		&ip->i_iolock);
+	kdb_printf("&flock 0x%p (%d) pincount 0x%x\n",
+		&ip->i_flock, valusema(&ip->i_flock),
+		xfs_ipincount(ip));
+	kdb_printf("udquotp 0x%p gdquotp 0x%p\n",
+		ip->i_udquot, ip->i_gdquot);
+	kdb_printf("new_size %Ld\n", ip->i_iocore.io_new_size);
+	printflags((int)ip->i_flags, tab_flags, "flags");
+	kdb_printf("\n");
+	kdb_printf("update_core %d update size %d\n",
+		(int)(ip->i_update_core), (int) ip->i_update_size);
+	kdb_printf("gen 0x%x delayed blks %d",
+		ip->i_gen,
+		ip->i_delayed_blks);
+#ifdef XFS_BMAP_TRACE
+	qprintf(" bmap_trace 0x%p\n", ip->i_xtrace);
+#endif
+#ifdef XFS_BMBT_TRACE
+	qprintf(" bmbt trace 0x%p\n", ip->i_btrace);
+#endif  
+#ifdef XFS_RW_TRACE
+	qprintf(" rw trace 0x%p\n", ip->i_rwtrace);
+#endif  
+#ifdef XFS_ILOCK_TRACE
+	qprintf(" ilock trace 0x%p\n", ip->i_lock_trace);
+#endif
+#ifdef XFS_DIR2_TRACE
+	qprintf(" dir trace 0x%p\n", ip->i_dir_trace);
+#endif  
+	kdb_printf("\n");
+	kdb_printf("chash 0x%p cnext 0x%p cprev 0x%p\n",
+		ip->i_chash,
+		ip->i_cnext,
+		ip->i_cprev);
+	xfs_xnode_fork("data", &ip->i_df);
+	xfs_xnode_fork("attr", ip->i_afp);
+	kdb_printf("\n");
+	xfs_prdinode_core(&ip->i_d, ARCH_NOCONVERT);
+}
+
+static void
+xfsidbg_xcore(xfs_iocore_t *io)
+{
+	kdb_printf("io_obj 0x%p io_flags 0x%x io_mount 0x%p\n",
+			io->io_obj, io->io_flags, io->io_mount);
+	kdb_printf("new_size %Lx\n", io->io_new_size);
+}
+
+/*
+ * Command to print xfs inode cluster hash table: kp xchash <addr>
+ */
+static void
+xfsidbg_xchash(xfs_mount_t *mp)
+{
+	int		i;
+	xfs_chash_t	*ch;
+
+	kdb_printf("m_chash 0x%p size %d\n",
+		mp->m_chash, mp->m_chsize);
+	for (i = 0; i < mp->m_chsize; i++) {
+		ch = mp->m_chash + i;
+		kdb_printf("[%3d] ch 0x%p chashlist 0x%p\n", i, ch, ch->ch_list);
+		xfsidbg_xchashlist(ch->ch_list);
+	}
+}
+
+/*
+ * Command to print xfs inode cluster hash list: kp xchashlist <addr>
+ */
+static void
+xfsidbg_xchashlist(xfs_chashlist_t *chl)
+{
+	xfs_inode_t	*ip;
+
+	while (chl != NULL) {
+		kdb_printf("hashlist inode 0x%p blkno %lld buf 0x%p",
+		       chl->chl_ip, (long long) chl->chl_blkno, chl->chl_buf);
+
+		kdb_printf("\n");
+
+		/* print inodes on chashlist */
+		ip = chl->chl_ip;
+		do {
+			kdb_printf("0x%p ", ip);
+			ip = ip->i_cnext;
+		} while (ip != chl->chl_ip);
+		kdb_printf("\n");
+
+		chl=chl->chl_next;
+	}
+}
+
+/*
+ * Print xfs per-ag data structures for filesystem.
+ */
+static void
+xfsidbg_xperag(xfs_mount_t *mp)
+{
+	xfs_agnumber_t	agno;
+	xfs_perag_t	*pag;
+	int		busy;
+
+	pag = mp->m_perag;
+	for (agno = 0; agno < mp->m_sb.sb_agcount; agno++, pag++) {
+		kdb_printf("ag %d f_init %d i_init %d\n",
+			agno, pag->pagf_init, pag->pagi_init);
+		if (pag->pagf_init)
+			kdb_printf(
+	"    f_levels[b,c] %d,%d f_flcount %d f_freeblks %d f_longest %d\n"
+	"    f__metadata %d\n",
+				pag->pagf_levels[XFS_BTNUM_BNOi],
+				pag->pagf_levels[XFS_BTNUM_CNTi],
+				pag->pagf_flcount, pag->pagf_freeblks,
+				pag->pagf_longest, pag->pagf_metadata);
+		if (pag->pagi_init)
+			kdb_printf("    i_freecount %d i_inodeok %d\n",
+				pag->pagi_freecount, pag->pagi_inodeok);
+		if (pag->pagf_init) {
+			for (busy = 0; busy < XFS_PAGB_NUM_SLOTS; busy++) {
+				if (pag->pagb_list[busy].busy_length != 0) {
+					kdb_printf(
+		"	 %04d: start %d length %d tp 0x%p\n",
+					    busy,
+					    pag->pagb_list[busy].busy_start,
+					    pag->pagb_list[busy].busy_length,
+					    pag->pagb_list[busy].busy_tp);
+				}
+			}
+		}
+	}
+}
+
+#ifdef CONFIG_XFS_QUOTA
+static void
+xfsidbg_xqm()
+{
+	if (xfs_Gqm == NULL) {
+		kdb_printf("NULL XQM!!\n");
+		return;
+	}
+
+	kdb_printf("usrhtab 0x%p\tgrphtab 0x%p\tndqfree 0x%x\thashmask 0x%x\n",
+		xfs_Gqm->qm_usr_dqhtable,
+		xfs_Gqm->qm_grp_dqhtable,
+		xfs_Gqm->qm_dqfreelist.qh_nelems,
+		xfs_Gqm->qm_dqhashmask);
+	kdb_printf("&freelist 0x%p, totaldquots 0x%x nrefs 0x%x\n",
+		&xfs_Gqm->qm_dqfreelist,
+		atomic_read(&xfs_Gqm->qm_totaldquots),
+		xfs_Gqm->qm_nrefs);
+}
+#endif
+
+static void
+xfsidbg_xqm_diskdq(xfs_disk_dquot_t *d)
+{
+	kdb_printf("magic 0x%x\tversion 0x%x\tID 0x%x (%d)\t\n",
+		INT_GET(d->d_magic, ARCH_CONVERT),
+		INT_GET(d->d_version, ARCH_CONVERT),
+		INT_GET(d->d_id, ARCH_CONVERT),
+		INT_GET(d->d_id, ARCH_CONVERT));
+	kdb_printf("bhard 0x%llx\tbsoft 0x%llx\tihard 0x%llx\tisoft 0x%llx\n",
+		(unsigned long long)INT_GET(d->d_blk_hardlimit, ARCH_CONVERT),
+		(unsigned long long)INT_GET(d->d_blk_softlimit, ARCH_CONVERT),
+		(unsigned long long)INT_GET(d->d_ino_hardlimit, ARCH_CONVERT),
+		(unsigned long long)INT_GET(d->d_ino_softlimit, ARCH_CONVERT));
+	kdb_printf("bcount 0x%llx icount 0x%llx\n",
+		(unsigned long long)INT_GET(d->d_bcount, ARCH_CONVERT),
+		(unsigned long long)INT_GET(d->d_icount, ARCH_CONVERT));
+	kdb_printf("btimer 0x%x itimer 0x%x \n",
+		(int)INT_GET(d->d_btimer, ARCH_CONVERT),
+		(int)INT_GET(d->d_itimer, ARCH_CONVERT));
+}
+
+static void
+xfsidbg_xqm_dquot(xfs_dquot_t *dqp)
+{
+	static char *qflags[] = {
+		"USR",
+		"GRP",
+		"LCKD",
+		"FLKD",
+		"DIRTY",
+		"WANT",
+		"INACT",
+		"MARKER",
+		0
+	};
+	kdb_printf("mount 0x%p hash 0x%p gdquotp 0x%p HL_next 0x%p HL_prevp 0x%p\n",
+		dqp->q_mount,
+		dqp->q_hash,
+		dqp->q_gdquot,
+		dqp->HL_NEXT,
+		dqp->HL_PREVP);
+	kdb_printf("MPL_next 0x%p MPL_prevp 0x%p FL_next 0x%p FL_prev 0x%p\n",
+		dqp->MPL_NEXT,
+		dqp->MPL_PREVP,
+		dqp->dq_flnext,
+		dqp->dq_flprev);
+
+	kdb_printf("nrefs 0x%x, res_bcount %d, ",
+		dqp->q_nrefs, (int) dqp->q_res_bcount);
+	printflags(dqp->dq_flags, qflags, "flags:");
+	kdb_printf("\nblkno 0x%llx\tboffset 0x%x\n",
+		(unsigned long long) dqp->q_blkno, (int) dqp->q_bufoffset);
+	kdb_printf("qlock 0x%p  flock 0x%p (%s) pincount 0x%x\n",
+		&dqp->q_qlock,
+		&dqp->q_flock,
+		(valusema(&dqp->q_flock) <= 0) ? "LCK" : "UNLKD",
+		dqp->q_pincount);
+#ifdef XFS_DQUOT_TRACE
+	qprintf("dqtrace 0x%p\n", dqp->q_trace);
+#endif
+	kdb_printf("disk-dquot 0x%p\n", &dqp->q_core);
+	xfsidbg_xqm_diskdq(&dqp->q_core);
+
+}
+
+
+#define XQMIDBG_LIST_PRINT(l, NXT) \
+{ \
+	  xfs_dquot_t	*dqp;\
+	  int i = 0; \
+	  kdb_printf("[#%d dquots]\n", (int) (l)->qh_nelems); \
+	  for (dqp = (l)->qh_next; dqp != NULL; dqp = dqp->NXT) {\
+	   kdb_printf( \
+	      "\t%d. [0x%p] \"%d (%s)\"\t blks = %d, inos = %d refs = %d\n", \
+			 ++i, dqp, (int) INT_GET(dqp->q_core.d_id, ARCH_CONVERT), \
+			 DQFLAGTO_TYPESTR(dqp),      \
+			 (int) INT_GET(dqp->q_core.d_bcount, ARCH_CONVERT), \
+			 (int) INT_GET(dqp->q_core.d_icount, ARCH_CONVERT), \
+			 (int) dqp->q_nrefs); }\
+	  kdb_printf("\n"); \
+}
+
+static void
+xfsidbg_xqm_dqattached_inos(xfs_mount_t	*mp)
+{
+	xfs_inode_t	*ip;
+	int		n = 0;
+
+	ip = mp->m_inodes;
+	do {
+		if (ip->i_mount == NULL) {
+			ip = ip->i_mnext;
+			continue;
+		}
+		if (ip->i_udquot || ip->i_gdquot) {
+			n++;
+			kdb_printf("inode = 0x%p, ino %d: udq 0x%p, gdq 0x%p\n",
+				ip, (int)ip->i_ino, ip->i_udquot, ip->i_gdquot);
+		}
+		ip = ip->i_mnext;
+	} while (ip != mp->m_inodes);
+	kdb_printf("\nNumber of inodes with dquots attached: %d\n", n);
+}
+
+#ifdef CONFIG_XFS_QUOTA
+static void
+xfsidbg_xqm_freelist_print(xfs_frlist_t *qlist, char *title)
+{
+	xfs_dquot_t *dq;
+	int i = 0;
+	kdb_printf("%s (#%d)\n", title, (int) qlist->qh_nelems);
+	FOREACH_DQUOT_IN_FREELIST(dq, qlist) {
+		kdb_printf("\t%d.\t\"%d (%s:0x%p)\"\t bcnt = %d, icnt = %d "
+		       "refs = %d\n",
+		       ++i, (int) INT_GET(dq->q_core.d_id, ARCH_CONVERT),
+		       DQFLAGTO_TYPESTR(dq), dq,
+		       (int) INT_GET(dq->q_core.d_bcount, ARCH_CONVERT),
+		       (int) INT_GET(dq->q_core.d_icount, ARCH_CONVERT),
+		       (int) dq->q_nrefs);
+	}
+}
+
+static void
+xfsidbg_xqm_freelist(void)
+{
+	if (xfs_Gqm) {
+		xfsidbg_xqm_freelist_print(&(xfs_Gqm->qm_dqfreelist), "Freelist");
+	} else
+		kdb_printf("NULL XQM!!\n");
+}
+
+static void
+xfsidbg_xqm_htab(void)
+{
+	int		i;
+	xfs_dqhash_t	*h;
+
+	if (xfs_Gqm == NULL) {
+		kdb_printf("NULL XQM!!\n");
+		return;
+	}
+	for (i = 0; i <= xfs_Gqm->qm_dqhashmask; i++) {
+		h = &xfs_Gqm->qm_usr_dqhtable[i];
+		if (h->qh_next) {
+			kdb_printf("USR %d: ", i);
+			XQMIDBG_LIST_PRINT(h, HL_NEXT);
+		}
+	}
+	for (i = 0; i <= xfs_Gqm->qm_dqhashmask; i++) {
+		h = &xfs_Gqm->qm_grp_dqhtable[i];
+		if (h->qh_next) {
+			kdb_printf("GRP %d: ", i);
+			XQMIDBG_LIST_PRINT(h, HL_NEXT);
+		}
+	}
+}
+#endif
+
+#ifdef XFS_DQUOT_TRACE
+static int
+xfsidbg_xqm_pr_dqentry(ktrace_entry_t *ktep)
+{
+	static char *xdq_flags[] = {
+		"USR",	  /* 0x1 */
+		"PRJ",	  /* 0x2 */
+		"LCKD",	 /* 0x4 */
+		"GRP",	  /* 0x8 */
+		"FLOCKD",       /* 0x08 */
+		"DIRTY",	/* 0x10 */
+		"WANT",	 /* 0x20 */
+		"INACT",	/* 0x40 */
+		"MARKER",       /* 0x80 */
+		0
+	};
+
+	if ((__psint_t)ktep->val[0] == 0)
+		return 0;
+	switch ((__psint_t)ktep->val[0]) {
+	      case DQUOT_KTRACE_ENTRY:
+		qprintf("[%ld] %s\t",
+			(long)ktep->val[12], /* pid */
+			(char *)ktep->val[1]);
+		printflags((__psint_t)ktep->val[3], xdq_flags,"flgs ");
+		qprintf("\nnrefs = %u, "
+			"flags = 0x%x, "
+			"id = %d, "
+			"res_bc = 0x%x\n"
+			"bcnt = 0x%x [0x%x | 0x%x], "
+			"icnt = 0x%x [0x%x | 0x%x]\n"
+			"@ %ld\n",
+			(unsigned int)(long)ktep->val[2], /* nrefs */
+			(unsigned int)(long)ktep->val[3], /* flags */
+			(unsigned int)(long)ktep->val[11], /* ID */
+			(unsigned int)(long)ktep->val[4], /* res_bc */
+			(unsigned int)(long)ktep->val[5], /* bcnt */
+			(unsigned int)(long)ktep->val[8], /* bsoft */
+			(unsigned int)(long)ktep->val[7], /* bhard */
+			(unsigned int)(long)ktep->val[6], /* icnt */
+			(unsigned int)(long)ktep->val[10], /* isoft */
+			(unsigned int)(long)ktep->val[9], /* ihard */
+			(long) ktep->val[13] /* time */
+			);
+		break;
+
+	      default:
+		qprintf("unknown dqtrace record\n");
+		break;
+	}
+	return (1);
+}
+
+void
+xfsidbg_xqm_dqtrace(xfs_dquot_t *dqp)
+{
+	ktrace_entry_t	*ktep;
+	ktrace_snap_t	kts;
+
+	if (dqp->q_trace == NULL) {
+		qprintf("The xfs dquot trace buffer is not initialized\n");
+		return;
+	}
+	qprintf("xdqtrace dquot 0x%p\n", dqp);
+
+	ktep = ktrace_first(dqp->q_trace, &kts);
+	while (ktep != NULL) {
+		if (xfsidbg_xqm_pr_dqentry(ktep))
+			qprintf("---------------------------------\n");
+		ktep = ktrace_next(dqp->q_trace, &kts);
+	}
+}
+#endif
+
+static void
+xfsidbg_xqm_mplist(xfs_mount_t *mp)
+{
+	if (mp->m_quotainfo == NULL) {
+		kdb_printf("NULL quotainfo\n");
+		return;
+	}
+
+	XQMIDBG_LIST_PRINT(&(mp->m_quotainfo->qi_dqlist), MPL_NEXT);
+
+}
+
+
+static void
+xfsidbg_xqm_qinfo(xfs_mount_t *mp)
+{
+	if (mp == NULL || mp->m_quotainfo == NULL) {
+		kdb_printf("NULL quotainfo\n");
+		return;
+	}
+
+	kdb_printf("uqip 0x%p, gqip 0x%p, &pinlock 0x%p &dqlist 0x%p\n",
+		mp->m_quotainfo->qi_uquotaip,
+		mp->m_quotainfo->qi_gquotaip,
+		&mp->m_quotainfo->qi_pinlock,
+		&mp->m_quotainfo->qi_dqlist);
+
+	kdb_printf("nreclaims %d, btmlimit 0x%x, itmlimit 0x%x, RTbtmlim 0x%x\n",
+		(int)mp->m_quotainfo->qi_dqreclaims,
+		(int)mp->m_quotainfo->qi_btimelimit,
+		(int)mp->m_quotainfo->qi_itimelimit,
+		(int)mp->m_quotainfo->qi_rtbtimelimit);
+
+	kdb_printf("bwarnlim 0x%x, iwarnlim 0x%x, &qofflock 0x%p, "
+		"chunklen 0x%x, dqperchunk 0x%x\n",
+		(int)mp->m_quotainfo->qi_bwarnlimit,
+		(int)mp->m_quotainfo->qi_iwarnlimit,
+		&mp->m_quotainfo->qi_quotaofflock,
+		(int)mp->m_quotainfo->qi_dqchunklen,
+		(int)mp->m_quotainfo->qi_dqperchunk);
+}
+
+static void
+xfsidbg_xqm_tpdqinfo(xfs_trans_t *tp)
+{
+	xfs_dqtrx_t	*qa, *q;
+	int		i,j;
+
+	kdb_printf("dqinfo 0x%p\n", tp->t_dqinfo);
+	if (! tp->t_dqinfo)
+		return;
+	kdb_printf("USR: \n");
+	qa = tp->t_dqinfo->dqa_usrdquots;
+	for (j = 0; j < 2; j++) {
+		for (i = 0; i < XFS_QM_TRANS_MAXDQS; i++) {
+			if (qa[i].qt_dquot == NULL)
+				break;
+			q = &qa[i];
+			kdb_printf(
+  "\"%d\"[0x%p]: bres %d, bres-used %d, bdelta %d, del-delta %d, icnt-delta %d\n",
+				(int) q->qt_dquot->q_core.d_id,
+				q->qt_dquot,
+				(int) q->qt_blk_res,
+				(int) q->qt_blk_res_used,
+				(int) q->qt_bcount_delta,
+				(int) q->qt_delbcnt_delta,
+				(int) q->qt_icount_delta);
+		}
+		if (j == 0) {
+			qa = tp->t_dqinfo->dqa_grpdquots;
+			kdb_printf("GRP: \n");
+		}
+	}
+
+}
+
+#ifdef XFS_RW_TRACE
+/*
+ * Print out the read/write trace buffer attached to the given inode.
+ */
+static void
+xfsidbg_xrwtrace(xfs_inode_t *ip)
+{
+	ktrace_entry_t	*ktep;
+	ktrace_snap_t	kts;
+	int		nentries;
+	int		skip_entries;
+	int		count = xargument;
+
+	if (ip->i_rwtrace == NULL) {
+		qprintf("The inode trace buffer is not initialized\n");
+		return;
+	}
+	qprintf("i_rwtrace = 0x%p\n", ip->i_rwtrace);
+
+	nentries = ktrace_nentries(ip->i_rwtrace);
+	if (count == -1) {
+		count = nentries;
+	}
+	if ((count <= 0) || (count > nentries)) {
+		qprintf("Invalid count.  There are %d entries.\n", nentries);
+		return;
+	}
+
+	ktep = ktrace_first(ip->i_rwtrace, &kts);
+	if (count != nentries) {
+		/*
+		 * Skip the total minus the number to look at minus one
+		 * for the entry returned by ktrace_first().
+		 */
+		skip_entries = count - nentries - 1;
+		ktep = ktrace_skip(ip->i_rwtrace, skip_entries, &kts);
+		if (ktep == NULL) {
+			qprintf("Skipped them all\n");
+			return;
+		}
+	}
+	while (ktep != NULL) {
+		if (xfs_rw_trace_entry(ktep))
+			qprintf("\n");
+		ktep = ktrace_next(ip->i_rwtrace, &kts);
+	}
+}
+#endif
+
+/*
+ * Print xfs superblock.
+ */
+static void
+xfsidbg_xsb(xfs_sb_t *sbp, int convert)
+{
+	xfs_arch_t arch=convert?ARCH_CONVERT:ARCH_NOCONVERT;
+
+	kdb_printf(convert?"<converted>\n":"<unconverted>\n");
+
+	kdb_printf("magicnum 0x%x blocksize 0x%x dblocks %Ld rblocks %Ld\n",
+		INT_GET(sbp->sb_magicnum, arch), INT_GET(sbp->sb_blocksize, arch),
+		INT_GET(sbp->sb_dblocks, arch), INT_GET(sbp->sb_rblocks, arch));
+	kdb_printf("rextents %Ld uuid %s logstart %s\n",
+		INT_GET(sbp->sb_rextents, arch),
+		xfs_fmtuuid(&sbp->sb_uuid),
+		xfs_fmtfsblock(INT_GET(sbp->sb_logstart, arch), NULL));
+	kdb_printf("rootino %s ",
+		xfs_fmtino(INT_GET(sbp->sb_rootino, arch), NULL));
+	kdb_printf("rbmino %s ",
+		xfs_fmtino(INT_GET(sbp->sb_rbmino, arch), NULL));
+	kdb_printf("rsumino %s\n",
+		xfs_fmtino(INT_GET(sbp->sb_rsumino, arch), NULL));
+	kdb_printf("rextsize 0x%x agblocks 0x%x agcount 0x%x rbmblocks 0x%x\n",
+		INT_GET(sbp->sb_rextsize, arch),
+		INT_GET(sbp->sb_agblocks, arch),
+		INT_GET(sbp->sb_agcount, arch),
+		INT_GET(sbp->sb_rbmblocks, arch));
+	kdb_printf("logblocks 0x%x versionnum 0x%x sectsize 0x%x inodesize 0x%x\n",
+		INT_GET(sbp->sb_logblocks, arch),
+		INT_GET(sbp->sb_versionnum, arch),
+		INT_GET(sbp->sb_sectsize, arch),
+		INT_GET(sbp->sb_inodesize, arch));
+	kdb_printf("inopblock 0x%x blocklog 0x%x sectlog 0x%x inodelog 0x%x\n",
+		INT_GET(sbp->sb_inopblock, arch),
+		INT_GET(sbp->sb_blocklog, arch),
+		INT_GET(sbp->sb_sectlog, arch),
+		INT_GET(sbp->sb_inodelog, arch));
+	kdb_printf("inopblog %d agblklog %d rextslog %d inprogress %d imax_pct %d\n",
+		INT_GET(sbp->sb_inopblog, arch),
+		INT_GET(sbp->sb_agblklog, arch),
+		INT_GET(sbp->sb_rextslog, arch),
+		INT_GET(sbp->sb_inprogress, arch),
+		INT_GET(sbp->sb_imax_pct, arch));
+	kdb_printf("icount %Lx ifree %Lx fdblocks %Lx frextents %Lx\n",
+		INT_GET(sbp->sb_icount, arch),
+		INT_GET(sbp->sb_ifree, arch),
+		INT_GET(sbp->sb_fdblocks, arch),
+		INT_GET(sbp->sb_frextents, arch));
+	kdb_printf("uquotino %s ", xfs_fmtino(INT_GET(sbp->sb_uquotino, arch), NULL));
+	kdb_printf("gquotino %s ", xfs_fmtino(INT_GET(sbp->sb_gquotino, arch), NULL));
+	kdb_printf("qflags 0x%x flags 0x%x shared_vn %d inoaligmt %d\n",
+		INT_GET(sbp->sb_qflags, arch), INT_GET(sbp->sb_flags, arch), INT_GET(sbp->sb_shared_vn, arch),
+		INT_GET(sbp->sb_inoalignmt, arch));
+	kdb_printf("unit %d width %d dirblklog %d\n",
+		INT_GET(sbp->sb_unit, arch), INT_GET(sbp->sb_width, arch), INT_GET(sbp->sb_dirblklog, arch));
+	kdb_printf("log sunit %d\n", INT_GET(sbp->sb_logsunit, arch));
+}
+
+
+/*
+ * Print out an XFS transaction structure.  Print summaries for
+ * each of the items.
+ */
+static void
+xfsidbg_xtp(xfs_trans_t *tp)
+{
+	xfs_log_item_chunk_t	*licp;
+	xfs_log_item_desc_t	*lidp;
+	xfs_log_busy_chunk_t	*lbcp;
+	int			i;
+	int			chunk;
+	static char *xtp_flags[] = {
+		"dirty",	/* 0x1 */
+		"sb_dirty",	/* 0x2 */
+		"perm_log_res",	/* 0x4 */
+		"sync",		 /* 0x08 */
+		"dq_dirty",     /* 0x10 */
+		0
+		};
+	static char *lid_flags[] = {
+		"dirty",	/* 0x1 */
+		"pinned",	/* 0x2 */
+		"sync unlock",	/* 0x4 */
+		"buf stale",	/* 0x8 */
+		0
+		};
+
+	kdb_printf("tp 0x%p type ", tp);
+	switch (tp->t_type) {
+	case XFS_TRANS_SETATTR_NOT_SIZE: kdb_printf("SETATTR_NOT_SIZE");break;
+	case XFS_TRANS_SETATTR_SIZE:	kdb_printf("SETATTR_SIZE");	break;
+	case XFS_TRANS_INACTIVE:	kdb_printf("INACTIVE");		break;
+	case XFS_TRANS_CREATE:		kdb_printf("CREATE");		break;
+	case XFS_TRANS_CREATE_TRUNC:	kdb_printf("CREATE_TRUNC");	break;
+	case XFS_TRANS_TRUNCATE_FILE:	kdb_printf("TRUNCATE_FILE");	break;
+	case XFS_TRANS_REMOVE:		kdb_printf("REMOVE");		break;
+	case XFS_TRANS_LINK:		kdb_printf("LINK");		break;
+	case XFS_TRANS_RENAME:		kdb_printf("RENAME");		break;
+	case XFS_TRANS_MKDIR:		kdb_printf("MKDIR");		break;
+	case XFS_TRANS_RMDIR:		kdb_printf("RMDIR");		break;
+	case XFS_TRANS_SYMLINK:		kdb_printf("SYMLINK");		break;
+	case XFS_TRANS_SET_DMATTRS:	kdb_printf("SET_DMATTRS");	break;
+	case XFS_TRANS_GROWFS:		kdb_printf("GROWFS");		break;
+	case XFS_TRANS_STRAT_WRITE:	kdb_printf("STRAT_WRITE");	break;
+	case XFS_TRANS_DIOSTRAT:	kdb_printf("DIOSTRAT");		break;
+	case XFS_TRANS_WRITE_SYNC:	kdb_printf("WRITE_SYNC");	break;
+	case XFS_TRANS_WRITEID:		kdb_printf("WRITEID");		break;
+	case XFS_TRANS_ADDAFORK:	kdb_printf("ADDAFORK");		break;
+	case XFS_TRANS_ATTRINVAL:	kdb_printf("ATTRINVAL");	break;
+	case XFS_TRANS_ATRUNCATE:	kdb_printf("ATRUNCATE");	break;
+	case XFS_TRANS_ATTR_SET:	kdb_printf("ATTR_SET");		break;
+	case XFS_TRANS_ATTR_RM:		kdb_printf("ATTR_RM");		break;
+	case XFS_TRANS_ATTR_FLAG:	kdb_printf("ATTR_FLAG");	break;
+	case XFS_TRANS_CLEAR_AGI_BUCKET:kdb_printf("CLEAR_AGI_BUCKET");	break;
+	case XFS_TRANS_QM_SBCHANGE:	kdb_printf("QM_SBCHANGE");	break;
+	case XFS_TRANS_QM_QUOTAOFF:	kdb_printf("QM_QUOTAOFF");	break;
+	case XFS_TRANS_QM_DQALLOC:	kdb_printf("QM_DQALLOC");	break;
+	case XFS_TRANS_QM_SETQLIM:	kdb_printf("QM_SETQLIM");	break;
+	case XFS_TRANS_QM_DQCLUSTER:	kdb_printf("QM_DQCLUSTER");	break;
+	case XFS_TRANS_QM_QINOCREATE:	kdb_printf("QM_QINOCREATE");	break;
+	case XFS_TRANS_QM_QUOTAOFF_END:	kdb_printf("QM_QOFF_END");	break;
+	case XFS_TRANS_SB_UNIT:		kdb_printf("SB_UNIT");		break;
+	case XFS_TRANS_FSYNC_TS:	kdb_printf("FSYNC_TS");		break;
+	case XFS_TRANS_GROWFSRT_ALLOC:	kdb_printf("GROWFSRT_ALLOC");	break;
+	case XFS_TRANS_GROWFSRT_ZERO:	kdb_printf("GROWFSRT_ZERO");	break;
+	case XFS_TRANS_GROWFSRT_FREE:	kdb_printf("GROWFSRT_FREE");	break;
+	default:			kdb_printf("0x%x", tp->t_type);	break;
+	}
+	kdb_printf(" mount 0x%p\n", tp->t_mountp);
+	kdb_printf("flags ");
+	printflags(tp->t_flags, xtp_flags,"xtp");
+	kdb_printf("\n");
+	kdb_printf("callback 0x%p forw 0x%p back 0x%p\n",
+		&tp->t_logcb, tp->t_forw, tp->t_back);
+	kdb_printf("log res %d block res %d block res used %d\n",
+		tp->t_log_res, tp->t_blk_res, tp->t_blk_res_used);
+	kdb_printf("rt res %d rt res used %d\n", tp->t_rtx_res,
+		tp->t_rtx_res_used);
+	kdb_printf("ticket 0x%lx lsn %s commit_lsn %s\n",
+		(unsigned long) tp->t_ticket,
+		xfs_fmtlsn(&tp->t_lsn),
+		xfs_fmtlsn(&tp->t_commit_lsn));
+	kdb_printf("callback 0x%p callarg 0x%p\n",
+		tp->t_callback, tp->t_callarg);
+	kdb_printf("icount delta %ld ifree delta %ld\n",
+		tp->t_icount_delta, tp->t_ifree_delta);
+	kdb_printf("blocks delta %ld res blocks delta %ld\n",
+		tp->t_fdblocks_delta, tp->t_res_fdblocks_delta);
+	kdb_printf("rt delta %ld res rt delta %ld\n",
+		tp->t_frextents_delta, tp->t_res_frextents_delta);
+	kdb_printf("ag freeblks delta %ld ag flist delta %ld ag btree delta %ld\n",
+		tp->t_ag_freeblks_delta, tp->t_ag_flist_delta,
+		tp->t_ag_btree_delta);
+	kdb_printf("dblocks delta %ld agcount delta %ld imaxpct delta %ld\n",
+		tp->t_dblocks_delta, tp->t_agcount_delta, tp->t_imaxpct_delta);
+	kdb_printf("rextsize delta %ld rbmblocks delta %ld\n",
+		tp->t_rextsize_delta, tp->t_rbmblocks_delta);
+	kdb_printf("rblocks delta %ld rextents delta %ld rextslog delta %ld\n",
+		tp->t_rblocks_delta, tp->t_rextents_delta,
+		tp->t_rextslog_delta);
+	kdb_printf("dqinfo 0x%p\n", tp->t_dqinfo);
+	kdb_printf("log items:\n");
+	licp = &tp->t_items;
+	chunk = 0;
+	while (licp != NULL) {
+		if (XFS_LIC_ARE_ALL_FREE(licp)) {
+			licp = licp->lic_next;
+			chunk++;
+			continue;
+		}
+		for (i = 0; i < licp->lic_unused; i++) {
+			if (XFS_LIC_ISFREE(licp, i)) {
+				continue;
+			}
+
+			lidp = XFS_LIC_SLOT(licp, i);
+			kdb_printf("\n");
+			kdb_printf("chunk %d index %d item 0x%p size %d\n",
+				chunk, i, lidp->lid_item, lidp->lid_size);
+			kdb_printf("flags ");
+			printflags(lidp->lid_flags, lid_flags,"lic");
+			kdb_printf("\n");
+			xfsidbg_xlogitem(lidp->lid_item);
+		}
+		chunk++;
+		licp = licp->lic_next;
+	}
+
+	kdb_printf("log busy free %d, list:\n", tp->t_busy_free);
+	lbcp = &tp->t_busy;
+	chunk = 0;
+	while (lbcp != NULL) {
+		kdb_printf("Chunk %d at 0x%p next 0x%p free 0x%08x unused %d\n",
+			chunk, lbcp, lbcp->lbc_next, lbcp->lbc_free,
+			lbcp->lbc_unused);
+		for (i = 0; i < XFS_LBC_NUM_SLOTS; i++) {
+			kdb_printf("  %02d: ag %d idx %d\n",
+				i,
+				lbcp->lbc_busy[i].lbc_ag,
+				lbcp->lbc_busy[i].lbc_idx);
+		}
+		lbcp = lbcp->lbc_next;
+	}
+}
+
+static void
+xfsidbg_xtrans_res(
+	xfs_mount_t	*mp)
+{
+	xfs_trans_reservations_t	*xtrp;
+
+	xtrp = &mp->m_reservations;
+	kdb_printf("write: %d\ttruncate: %d\trename: %d\n",
+		xtrp->tr_write, xtrp->tr_itruncate, xtrp->tr_rename);
+	kdb_printf("link: %d\tremove: %d\tsymlink: %d\n",
+		xtrp->tr_link, xtrp->tr_remove, xtrp->tr_symlink);
+	kdb_printf("create: %d\tmkdir: %d\tifree: %d\n",
+		xtrp->tr_create, xtrp->tr_mkdir, xtrp->tr_ifree);
+	kdb_printf("ichange: %d\tgrowdata: %d\tswrite: %d\n",
+		xtrp->tr_ichange, xtrp->tr_growdata, xtrp->tr_swrite);
+	kdb_printf("addafork: %d\twriteid: %d\tattrinval: %d\n",
+		xtrp->tr_addafork, xtrp->tr_writeid, xtrp->tr_attrinval);
+	kdb_printf("attrset: %d\tattrrm: %d\tclearagi: %d\n",
+		xtrp->tr_attrset, xtrp->tr_attrrm, xtrp->tr_clearagi);
+	kdb_printf("growrtalloc: %d\tgrowrtzero: %d\tgrowrtfree: %d\n",
+		xtrp->tr_growrtalloc, xtrp->tr_growrtzero, xtrp->tr_growrtfree);
+}
+
+module_init(xfsidbg_init)
+module_exit(xfsidbg_exit)
