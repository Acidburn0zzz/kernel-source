From: Jeff Mahoney <jeffm@suse.com>
Subject: ocfs2: revert mainline code to OCFS 1.2 branch from Oracle

 Oracle has requested that we ship the old version of OCFS2 to ease support
 load on them. This version has the features Oracle needs and has undergone
 extensive QA with them.

 This code is the branch that Oracle intends us to use for CODE10. If mainline
 changes, the code produced after this patch is applied must not. So, if you
 merge in an upstream patch, please revert it by remerging this patch or
 by adding a patch that explicitly reverts it before applying this one.

 Thanks.

 -Jeff

 >From: Mark Fasheh <mark.fasheh@oracle.com>
 >Subject: Re: [oracle-dev] OCFS2 Code Update for SP3 - 2005-12-13
 >
 >Ok, here's another update against 2.6.16-rc1-git3-20060123171059. The
 >typedef changes are all in. I completely missed the static declarations, but
 >anyway, which ones are giving you guys trouble? If you want, we can still
 >change that but I'd like some more specifics -- those patches are harder for
 >to find in our history as several rounds went in at different times.
 >
 >One thing of note is that the attached patch reverts the userspace
 >clustering -- I assume you're just going to have to re-apply that with the
 >new structure names in place anyway.
 >       --Mark

 fs/ocfs2/Makefile               |    4 
 fs/ocfs2/aio.c                  |  388 +++++++++++++
 fs/ocfs2/aio.h                  |   37 +
 fs/ocfs2/alloc.c                |    4 
 fs/ocfs2/alloc.h                |    2 
 fs/ocfs2/aops.c                 |  321 +----------
 fs/ocfs2/aops.h                 |   41 -
 fs/ocfs2/buffer_head_io.c       |  125 +++-
 fs/ocfs2/buffer_head_io.h       |   21 
 fs/ocfs2/cluster/Makefile       |    5 
 fs/ocfs2/cluster/heartbeat.c    |  104 ---
 fs/ocfs2/cluster/heartbeat.h    |    5 
 fs/ocfs2/cluster/masklog.c      |  261 +++++----
 fs/ocfs2/cluster/masklog.h      |   20 
 fs/ocfs2/cluster/net_proc.c     |  389 +++++++++++++
 fs/ocfs2/cluster/nodemanager.c  |  136 ++++
 fs/ocfs2/cluster/sys.c          |  124 ----
 fs/ocfs2/cluster/sys.h          |   33 -
 fs/ocfs2/cluster/tcp.c          |   58 +-
 fs/ocfs2/cluster/tcp.h          |    4 
 fs/ocfs2/cluster/tcp_internal.h |   32 -
 fs/ocfs2/cluster/ver.c          |   12 
 fs/ocfs2/dir.c                  |    6 
 fs/ocfs2/dlm/dlmast.c           |    4 
 fs/ocfs2/dlm/dlmcommon.h        |   35 +
 fs/ocfs2/dlm/dlmconvert.c       |   12 
 fs/ocfs2/dlm/dlmdebug.c         |  333 +++++++++++
 fs/ocfs2/dlm/dlmdebug.h         |    2 
 fs/ocfs2/dlm/dlmdomain.c        |   12 
 fs/ocfs2/dlm/dlmdomain.h        |    1 
 fs/ocfs2/dlm/dlmfsver.c         |    9 
 fs/ocfs2/dlm/dlmlock.c          |   28 
 fs/ocfs2/dlm/dlmmaster.c        |   66 +-
 fs/ocfs2/dlm/dlmrecovery.c      |   97 ++-
 fs/ocfs2/dlm/dlmthread.c        |   26 
 fs/ocfs2/dlm/dlmver.c           |    9 
 fs/ocfs2/dlm/userdlm.c          |    2 
 fs/ocfs2/dlmglue.c              |  872 ++++++++++++------------------
 fs/ocfs2/dlmglue.h              |   43 -
 fs/ocfs2/extent_map.c           |  113 +--
 fs/ocfs2/extent_map.h           |   11 
 fs/ocfs2/file.c                 | 1151 ++++++++++++++++++----------------------
 fs/ocfs2/file.h                 |   28 
 fs/ocfs2/heartbeat.c            |    3 
 fs/ocfs2/inode.c                |   23 
 fs/ocfs2/inode.h                |   38 -
 fs/ocfs2/journal.c              |   44 -
 fs/ocfs2/journal.h              |   16 
 fs/ocfs2/mmap.c                 |  718 ++++++++++++++++++++++++
 fs/ocfs2/mmap.h                 |  127 ++++
 fs/ocfs2/namei.c                |    5 
 fs/ocfs2/ocfs2.h                |   48 -
 fs/ocfs2/ocfs2_lockid.h         |    4 
 fs/ocfs2/proc.c                 |  461 ++++++++++++++++
 fs/ocfs2/proc.h                 |   34 +
 fs/ocfs2/super.c                |  108 ++-
 fs/ocfs2/symlink.c              |  248 ++++++++
 fs/ocfs2/sysfile.c              |    6 
 fs/ocfs2/uptodate.c             |   12 
 fs/ocfs2/uptodate.h             |    2 
 fs/ocfs2/ver.c                  |   14 
 fs/ocfs2/ver.h                  |    1 
 fs/ocfs2/vote.c                 |   15 
 63 files changed, 4667 insertions(+), 2246 deletions(-)

Signed-off-by: Jeff Mahoney <jeffm@suse.com>

diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/aio.c linux-2.6.15-ocfs2/fs/ocfs2/aio.c
--- linux-2.6.15-2/fs/ocfs2/aio.c	1969-12-31 19:00:00.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/aio.c	2006-02-06 17:59:23.000000000 -0500
@@ -0,0 +1,388 @@
+/* -*- mode: c; c-basic-offset: 8; -*-
+ * vim: noexpandtab sw=8 ts=8 sts=0:
+ *
+ * aio.c
+ *
+ * aio read and write
+ *
+ * Copyright (C) 2002, 2004, 2005 Oracle.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public
+ * License as published by the Free Software Foundation; either
+ * version 2 of the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public
+ * License along with this program; if not, write to the
+ * Free Software Foundation, Inc., 59 Temple Place - Suite 330,
+ * Boston, MA 021110-1307, USA.
+ */
+
+#include <linux/fs.h>
+#include <linux/types.h>
+#include <linux/slab.h>
+#include <linux/highmem.h>
+#include <linux/pagemap.h>
+#include <linux/uio.h>
+
+#define MLOG_MASK_PREFIX ML_FILE_IO|ML_AIO
+#include <cluster/masklog.h>
+
+#include "ocfs2.h"
+
+#include "aio.h"
+#include "alloc.h"
+#include "dir.h"
+#include "dlmglue.h"
+#include "extent_map.h"
+#include "file.h"
+#include "sysfile.h"
+#include "inode.h"
+#include "mmap.h"
+#include "suballoc.h"
+
+struct ocfs2_kiocb_private {
+	struct ocfs2_kiocb_private	*kp_teardown_next;
+	struct ocfs2_super			*kp_osb;
+	unsigned			kp_have_alloc_sem:1,
+					kp_have_write_locks:1;
+	struct inode			*kp_inode;
+	struct ocfs2_buffer_lock_ctxt	kp_ctxt;
+	struct ocfs2_write_lock_info	kp_info;
+};
+
+static void okp_teardown(struct ocfs2_kiocb_private *okp)
+{
+	mlog(0, "okp %p\n", okp);
+
+	BUG_ON(okp->kp_inode == NULL);
+
+	if (okp->kp_info.wl_unlock_ctxt)
+		ocfs2_unlock_buffer_inodes(&okp->kp_ctxt);
+	if (okp->kp_have_alloc_sem)
+		up_read(&OCFS2_I(okp->kp_inode)->ip_alloc_sem);
+
+	iput(okp->kp_inode);
+	kfree(okp);
+}
+
+void okp_teardown_from_list(void *data)
+{
+	struct ocfs2_super *osb = data;
+	struct ocfs2_kiocb_private *okp, *next;
+
+	for (okp = xchg(&osb->osb_okp_teardown_next, NULL); okp != NULL;
+	     okp = next) {
+
+		next = okp->kp_teardown_next;
+		okp_teardown(okp);
+	}
+}
+
+/*
+ * This releases the dlm locks we held across an aio operation and frees the
+ * space we were tracking them in.
+ *
+ * While aio operations are in flight they have a vfsmnt reference for the file
+ * which prevents unmount.  This dtor gets called *after* that ref is dropped,
+ * however, so we have to make sure to account for pending work we have here in
+ * the unmount path.  The race starts when aio does its fputs, before it calls
+ * dtor which queues work, so just synchronizing with the work queue could miss
+ * that first phase.  So unmount first waits for the pending count to drop.
+ * Then it has to wait for keventd to finish the work freeing the okps.
+ *
+ * _dtor can be called from just about any context and lock teardown is
+ * anything but interrupt safe.  We used to hand the okps to
+ * okp_teardown_from_list with a normal list_head and irq masking lock but we
+ * want to avoid masking interrupts so it was shifted to the {cmp,}xchg() and
+ * atomic_t.
+ *
+ * Adding to the singly linked ->next list is only a little tricky.  We have to
+ * watch for races between sampling the head to assign ->next in the inserting
+ * okp and a new head being written before we point the head to the inserting
+ * okp.
+ */
+static void ocfs2_ki_dtor(struct kiocb *iocb)
+{
+	struct ocfs2_kiocb_private *next, *okp = iocb->private;
+	struct ocfs2_super *osb = okp->kp_osb;
+
+	mlog(0, "iocb %p okp %p\n", iocb, okp);
+
+	/* okp_alloc only assigns the iocb->private and ->ki_dtor pointers if
+	 * it was able to alloc the okp and get an inode reference */
+	BUG_ON(okp == NULL);
+	BUG_ON(okp->kp_inode == NULL);
+
+	/* we had better not try to work with this iocb again */
+	iocb->private = NULL;
+
+	 /* once this cmpxchg succeeds the okp can be freed so we have to be
+	  * careful not to deref it when testing success */
+	do {
+		next = osb->osb_okp_teardown_next;
+		okp->kp_teardown_next = next;
+	} while (cmpxchg(&osb->osb_okp_teardown_next, next, okp) != next);
+
+	schedule_work(&osb->osb_okp_teardown_work);
+
+	if (atomic_dec_and_test(&osb->osb_okp_pending))
+		wake_up(&osb->osb_okp_pending_wq);
+}
+
+/* see ocfs2_ki_dtor() */
+void ocfs2_wait_for_okp_destruction(struct ocfs2_super *osb)
+{
+	/* first wait for okps to enter the work queue */
+	wait_event(osb->osb_okp_pending_wq,
+		   atomic_read(&osb->osb_okp_pending) == 0);
+	/*
+	 * then wait for keventd to finish with all its work, including ours.
+	 *
+	 * XXX this makes me very nervous.  what if our work blocks keventd
+	 * during an unlock and the unlock can only proceed if keventd
+	 * can get to some more work that the dlm might have queued?
+	 * do we push any dlm work to keventd?
+	 */
+	flush_scheduled_work();
+}
+
+/* just to stop sys_io_cancel() from spewing to the console when it sees an
+ * iocb without ki_cancel */
+static int ocfs2_ki_cancel(struct kiocb *iocb, struct io_event *ev)
+{
+	mlog(0, "iocb %p\n", iocb);
+	aio_put_req(iocb);
+	return -EAGAIN;
+}
+
+static struct ocfs2_kiocb_private *okp_alloc(struct kiocb *iocb)
+{
+	struct inode *inode = iocb->ki_filp->f_dentry->d_inode;
+	struct ocfs2_kiocb_private *okp;
+	struct ocfs2_super *osb;
+
+	okp = kcalloc(1, sizeof(*okp), GFP_KERNEL);
+	if (okp == NULL) {
+		okp = ERR_PTR(-ENOMEM);
+		goto out;
+	}
+
+	/* our dtor only gets registerd if we can guarantee that it holds
+	 * a reference to the inode */
+	okp->kp_inode = igrab(inode);
+	if (okp->kp_inode == NULL) {
+		kfree(okp);
+		okp = ERR_PTR(-EINVAL);
+		goto out;
+	}
+	/* unmount syncs with work using this ref before destroying the osb */
+	osb = OCFS2_SB(inode->i_sb);
+	okp->kp_osb = osb;
+
+	iocb->private = okp;
+	iocb->ki_dtor = ocfs2_ki_dtor;
+	iocb->ki_cancel = ocfs2_ki_cancel;
+	INIT_BUFFER_LOCK_CTXT(&okp->kp_ctxt);
+
+	atomic_inc(&osb->osb_okp_pending);
+out:
+	mlog(0, "iocb %p returning %p\n", iocb, okp);
+	return okp;
+}
+
+/* The DLM supports a minimal notion of AIO lock acquiry.  Instead of testing
+ * the iocb or current-> like kernel fs/block paths tend to, it takes an
+ * explicit callback which it calls when a lock state attempt makes forward
+ * progress.  It would be better if it worked with the native
+ * kernel AIO mechanics */
+static void ocfs2_aio_kick(int status, unsigned long data)
+{
+	struct kiocb *iocb = (struct kiocb *)data;
+	/* XXX worry about racing with ki_cancel once we set it */
+	mlog(0, "iocb %p\n", iocb);
+	kick_iocb(iocb);
+}
+
+/* this is called as iocb->ki_retry so it is careful to only repeat
+ * what is needed */
+ssize_t ocfs2_file_aio_read(struct kiocb *iocb, char __user *buf, size_t count,
+			    loff_t pos)
+{
+	struct ocfs2_kiocb_private *okp = iocb->private;
+	struct file *filp = iocb->ki_filp;
+	struct inode *inode = filp->f_dentry->d_inode;
+	struct ocfs2_backing_inode *target_binode;
+	ssize_t ret, ret2;
+	sigset_t blocked, oldset;
+
+	/*
+	 * The DLM doesn't block waiting for network traffic or anything, it
+	 * modifies state and calls our callback when things have changed.
+	 * However, it still likes to check signals and return ERESTARTSYS.
+	 * The AIO core does not appreciate ERESTARTSYS as its semantics are
+	 * not exactly clear for submission, etc.  So we block signals and
+	 * ensure that the DLM won't notice them.  The caller, particularly
+	 * sys_io_getevents(), will eventually check signals before sleeping
+	 * and so things should still work as expected, if perhaps with
+	 * slightly higher signal delivery latency.
+	 */
+	sigfillset(&blocked);
+	ret = sigprocmask(SIG_BLOCK, &blocked, &oldset);
+	if (ret < 0) {
+		mlog_errno(ret);
+		goto out;
+	}
+
+	mlog(0, "iocb %p okp %p\n", iocb, okp);
+
+	if (okp == NULL) {
+		okp = okp_alloc(iocb);
+		if (IS_ERR(okp)) {
+			ret = PTR_ERR(okp);
+			mlog_errno(ret);
+			goto setmask;
+		}
+
+		ret = ocfs2_setup_io_locks(inode->i_sb, inode, buf, count,
+					   &okp->kp_ctxt, &target_binode);
+		if (ret < 0) {
+			mlog_errno(ret);
+			goto setmask;
+		}
+
+		okp->kp_ctxt.b_cb = ocfs2_aio_kick;
+		okp->kp_ctxt.b_cb_data = (unsigned long)iocb;
+		target_binode->ba_lock_data = filp->f_flags & O_DIRECT ? 0 : 1;
+	}
+
+	/* this might return EIOCBRETRY and we'll come back again to
+	 * continue the locking.  It's harmless to call it once it has
+	 * returned success.. */
+	okp->kp_info.wl_unlock_ctxt = 1; /* re-use the write info path */
+	ret = ocfs2_lock_buffer_inodes(&okp->kp_ctxt, NULL);
+	if (ret < 0) {
+		if (ret != -EIOCBRETRY)
+			mlog_errno(ret);
+		goto setmask;
+	}
+
+	/* hold the ip_alloc_sem across the op */
+	if (!okp->kp_have_alloc_sem) {
+		down_read(&OCFS2_I(inode)->ip_alloc_sem);
+		okp->kp_have_alloc_sem = 1;
+	}
+
+	ret = generic_file_aio_read(iocb, buf, count, pos);
+
+setmask:
+	ret2 = sigprocmask(SIG_SETMASK, &oldset, NULL);
+	if (ret2 < 0) {
+		mlog_errno(ret2);
+		if (ret == 0)
+			ret = ret2;
+	}
+
+out:
+	/* ki_dtor will always be called eventually, no tear down here */
+	mlog(0, "iocb %p returning %lld\n", iocb, (long long)ret);
+	return ret;
+}
+
+/* this is called as iocb->ki_retry so it is careful to only repeat
+ * what is needed */
+ssize_t ocfs2_file_aio_write(struct kiocb *iocb, const char __user *buf,
+			     size_t count, loff_t pos)
+{
+	struct ocfs2_kiocb_private *okp = iocb->private;
+	struct file *filp = iocb->ki_filp;
+	struct inode *inode = filp->f_dentry->d_inode;
+	ssize_t ret = 0, ret2;
+	sigset_t blocked, oldset;
+	struct iovec local_iov = { .iov_base = (void __user *)buf,
+				   .iov_len = count };
+
+	/* explained up in ocfs2_file_aio_read() */
+	sigfillset(&blocked);
+	ret = sigprocmask(SIG_BLOCK, &blocked, &oldset);
+	if (ret < 0) {
+		mlog_errno(ret);
+		goto out;
+	}
+
+	mlog(0, "iocb %p okp %p\n", iocb, okp);
+
+	if (okp == NULL) {
+		okp = okp_alloc(iocb);
+		if (IS_ERR(okp)) {
+			ret = PTR_ERR(okp);
+			mlog_errno(ret);
+			goto up_io;
+		}
+
+		okp->kp_ctxt.b_cb = ocfs2_aio_kick;
+		okp->kp_ctxt.b_cb_data = (unsigned long)iocb;
+	}
+
+	if (!okp->kp_have_write_locks) {
+		ret = ocfs2_write_lock_maybe_extend(filp, buf, count,
+						    &iocb->ki_pos,
+						    &okp->kp_info,
+						    &okp->kp_ctxt);
+		okp->kp_have_write_locks = 1;
+		if (okp->kp_info.wl_extended) {
+			/*
+			 * this is not a particularly nice place to do this but
+			 * extending aio in ocfs2 is not yet a priority.  it
+			 * means that we'll write zeros in the buffered case
+			 * before then over-writing them with the real op.  It
+			 * also sleeps in the aio submission context.
+			 */
+			ocfs2_file_finish_extension(inode,
+						    !okp->kp_info.wl_newsize,
+						    okp->kp_info.wl_do_direct_io);
+			okp->kp_info.wl_extended = 0;
+		}
+		if (ret) {
+			mlog_errno(ret);
+			goto up_io;
+		}
+	}
+
+	/* hold the ip_alloc_sem across the op */
+	if (!okp->kp_have_alloc_sem) {
+		down_read(&OCFS2_I(inode)->ip_alloc_sem);
+		okp->kp_have_alloc_sem = 1;
+	}
+
+up_io:
+	/*
+	 * never hold i_mutex when we leave this function, nor when we call
+	 * g_f_a_w().  we've done all extending and inode field updating under
+	 * the i_mutex and we hold the ip_alloc_sem for reading across the ops.
+	 * ocfs2_direct_IO calls blockdev_direct_IO with NO_LOCKING.
+	 */
+	if (okp->kp_info.wl_have_i_mutex) {
+		mutex_unlock(&inode->i_mutex);
+		okp->kp_info.wl_have_i_mutex = 0;
+	}
+	if (ret == 0)
+		ret = generic_file_aio_write_nolock(iocb, &local_iov, 1,
+						    &iocb->ki_pos);
+
+	ret2 = sigprocmask(SIG_SETMASK, &oldset, NULL);
+	if (ret2 < 0) {
+		mlog_errno(ret2);
+		if (ret == 0)
+			ret = ret2;
+	}
+out:
+	/* ki_dtor will always be called eventually, no tear down here */
+	mlog(0, "iocb %p returning %lld\n", iocb, (long long)ret);
+	return ret;
+}
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/aio.h linux-2.6.15-ocfs2/fs/ocfs2/aio.h
--- linux-2.6.15-2/fs/ocfs2/aio.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/aio.h	2006-02-06 17:59:23.000000000 -0500
@@ -0,0 +1,37 @@
+/* -*- mode: c; c-basic-offset: 8; -*-
+ * vim: noexpandtab sw=8 ts=8 sts=0:
+ *
+ * aio.h
+ *
+ * Function prototypes
+ *
+ * Copyright (C) 2002, 2004, 2005 Oracle.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public
+ * License as published by the Free Software Foundation; either
+ * version 2 of the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public
+ * License along with this program; if not, write to the
+ * Free Software Foundation, Inc., 59 Temple Place - Suite 330,
+ * Boston, MA 021110-1307, USA.
+ */
+
+#ifndef OCFS2_AIO_H
+#define OCFS2_AIO_H
+
+ssize_t ocfs2_file_aio_write(struct kiocb *iocb, const char __user *buf,
+			     size_t count, loff_t pos);
+ssize_t ocfs2_file_aio_read(struct kiocb *iocb, char __user *buf, size_t count,
+			    loff_t pos);
+
+void okp_teardown_from_list(void *data);
+void ocfs2_wait_for_okp_destruction(struct ocfs2_super *osb);
+
+#endif /* OCFS2_AIO_H */
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/alloc.c linux-2.6.15-ocfs2/fs/ocfs2/alloc.c
--- linux-2.6.15-2/fs/ocfs2/alloc.c	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/alloc.c	2006-02-06 17:59:23.000000000 -0500
@@ -92,8 +92,6 @@ static int ocfs2_find_new_last_ext_blk(s
 				       struct buffer_head *old_last_eb,
 				       struct buffer_head **new_last_eb);
 
-static void ocfs2_free_truncate_context(struct ocfs2_truncate_context *tc);
-
 static int ocfs2_extent_contig(struct inode *inode,
 			       struct ocfs2_extent_rec *ext,
 			       u64 blkno)
@@ -2020,7 +2018,7 @@ bail:
 	return status;
 }
 
-static void ocfs2_free_truncate_context(struct ocfs2_truncate_context *tc)
+void ocfs2_free_truncate_context(struct ocfs2_truncate_context *tc)
 {
 	if (tc->tc_ext_alloc_inode) {
 		if (tc->tc_ext_alloc_locked)
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/alloc.h linux-2.6.15-ocfs2/fs/ocfs2/alloc.h
--- linux-2.6.15-2/fs/ocfs2/alloc.h	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/alloc.h	2006-02-06 17:59:23.000000000 -0500
@@ -70,6 +70,8 @@ struct ocfs2_truncate_context {
 	struct buffer_head *tc_last_eb_bh;
 };
 
+void ocfs2_free_truncate_context(struct ocfs2_truncate_context *tc);
+
 int ocfs2_prepare_truncate(struct ocfs2_super *osb,
 			   struct inode *inode,
 			   struct buffer_head *fe_bh,
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/aops.c linux-2.6.15-ocfs2/fs/ocfs2/aops.c
--- linux-2.6.15-2/fs/ocfs2/aops.c	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/aops.c	2006-02-06 17:59:23.000000000 -0500
@@ -31,13 +31,11 @@
 #include "ocfs2.h"
 
 #include "alloc.h"
-#include "aops.h"
 #include "dlmglue.h"
 #include "extent_map.h"
 #include "file.h"
 #include "inode.h"
 #include "journal.h"
-#include "super.h"
 #include "symlink.h"
 
 #include "buffer_head_io.h"
@@ -133,15 +131,17 @@ bail:
 static int ocfs2_get_block(struct inode *inode, sector_t iblock,
 			   struct buffer_head *bh_result, int create)
 {
-	int err = 0;
-	u64 p_blkno, past_eof;
+	int err = -EIO;
+	u64 vbo = 0;
+	u64 p_blkno;
 
 	mlog_entry("(0x%p, %llu, 0x%p, %d)\n", inode,
 		   (unsigned long long)iblock, bh_result, create);
 
-	if (OCFS2_I(inode)->ip_flags & OCFS2_INODE_SYSTEM_FILE)
+	if (OCFS2_I(inode)->ip_flags & OCFS2_INODE_SYSTEM_FILE) {
 		mlog(ML_NOTICE, "get_block on system inode 0x%p (%lu)\n",
 		     inode, inode->i_ino);
+	}
 
 	if (S_ISLNK(inode->i_mode)) {
 		/* this always does I/O for some reason. */
@@ -149,17 +149,22 @@ static int ocfs2_get_block(struct inode 
 		goto bail;
 	}
 
+	vbo = (u64)iblock << inode->i_sb->s_blocksize_bits;
+
 	/* this can happen if another node truncs after our extend! */
 	spin_lock(&OCFS2_I(inode)->ip_lock);
-	if (iblock >= ocfs2_clusters_to_blocks(inode->i_sb,
-					       OCFS2_I(inode)->ip_clusters))
+	if (iblock >=
+	    ocfs2_clusters_to_blocks(inode->i_sb,
+				     OCFS2_I(inode)->ip_clusters)) {
+		spin_unlock(&OCFS2_I(inode)->ip_lock);
 		err = -EIO;
-	spin_unlock(&OCFS2_I(inode)->ip_lock);
-	if (err)
 		goto bail;
+	}
+	spin_unlock(&OCFS2_I(inode)->ip_lock);
 
 	err = ocfs2_extent_map_get_blocks(inode, iblock, 1, &p_blkno,
 					  NULL);
+
 	if (err) {
 		mlog(ML_ERROR, "Error %d from get_blocks(0x%p, %llu, 1, "
 		     "%"MLFu64", NULL)\n", err, inode,
@@ -169,6 +174,8 @@ static int ocfs2_get_block(struct inode 
 
 	map_bh(bh_result, inode->i_sb, p_blkno);
 
+	err = 0;
+
 	if (bh_result->b_blocknr == 0) {
 		err = -EIO;
 		mlog(ML_ERROR, "iblock = %llu p_blkno = %"MLFu64" "
@@ -176,11 +183,22 @@ static int ocfs2_get_block(struct inode 
 		     p_blkno, OCFS2_I(inode)->ip_blkno);
 	}
 
-	past_eof = ocfs2_blocks_for_bytes(inode->i_sb, i_size_read(inode));
-	mlog(0, "Inode %lu, past_eof = %"MLFu64"\n", inode->i_ino, past_eof);
+	if (vbo < OCFS2_I(inode)->ip_mmu_private)
+		goto bail;
+	if (!create)
+		goto bail;
+	if (vbo != OCFS2_I(inode)->ip_mmu_private) {
+		mlog(ML_ERROR, "Uh-oh, vbo = %"MLFi64", i_size = %lld, "
+		     "mmu = %lld, inode = %"MLFu64"\n", vbo,
+		     i_size_read(inode), OCFS2_I(inode)->ip_mmu_private,
+		     OCFS2_I(inode)->ip_blkno);
+		BUG();
+		err = -EIO;
+		goto bail;
+	}
 
-	if (create && (iblock >= past_eof))
-		set_buffer_new(bh_result);
+	set_buffer_new(bh_result);
+	OCFS2_I(inode)->ip_mmu_private += inode->i_sb->s_blocksize;
 
 bail:
 	if (err < 0)
@@ -192,75 +210,17 @@ bail:
 
 static int ocfs2_readpage(struct file *file, struct page *page)
 {
-	struct inode *inode = page->mapping->host;
-	loff_t start = (loff_t)page->index << PAGE_CACHE_SHIFT;
-	int ret, unlock = 1;
+	int ret;
 
 	mlog_entry("(0x%p, %lu)\n", file, (page ? page->index : 0));
 
-	ret = ocfs2_meta_lock_with_page(inode, NULL, NULL, 0, page);
-	if (ret != 0) {
-		if (ret == AOP_TRUNCATED_PAGE)
-			unlock = 0;
-		mlog_errno(ret);
-		goto out;
-	}
-
-	down_read(&OCFS2_I(inode)->ip_alloc_sem);
-
-	/*
-	 * i_size might have just been updated as we grabed the meta lock.  We
-	 * might now be discovering a truncate that hit on another node.
-	 * block_read_full_page->get_block freaks out if it is asked to read
-	 * beyond the end of a file, so we check here.  Callers
-	 * (generic_file_read, fault->nopage) are clever enough to check i_size
-	 * and notice that the page they just read isn't needed.
-	 *
-	 * XXX sys_readahead() seems to get that wrong?
-	 */
-	if (start >= i_size_read(inode)) {
-		char *addr = kmap(page);
-		memset(addr, 0, PAGE_SIZE);
-		flush_dcache_page(page);
-		kunmap(page);
-		SetPageUptodate(page);
-		ret = 0;
-		goto out_alloc;
-	}
-
-	ret = ocfs2_data_lock_with_page(inode, 0, page);
-	if (ret != 0) {
-		if (ret == AOP_TRUNCATED_PAGE)
-			unlock = 0;
-		mlog_errno(ret);
-		goto out_alloc;
-	}
-
 	ret = block_read_full_page(page, ocfs2_get_block);
-	unlock = 0;
 
-	ocfs2_data_unlock(inode, 0);
-out_alloc:
-	up_read(&OCFS2_I(inode)->ip_alloc_sem);
-	ocfs2_meta_unlock(inode, 0);
-out:
-	if (unlock)
-		unlock_page(page);
 	mlog_exit(ret);
+
 	return ret;
 }
 
-/* Note: Because we don't support holes, our allocation has
- * already happened (allocation writes zeros to the file data)
- * so we don't have to worry about ordered writes in
- * ocfs2_writepage.
- *
- * ->writepage is called during the process of invalidating the page cache
- * during blocked lock processing.  It can't block on any cluster locks
- * to during block mapping.  It's relying on the fact that the block
- * mapping can't have disappeared under the dirty pages that it is
- * being asked to write back.
- */
 static int ocfs2_writepage(struct page *page, struct writeback_control *wbc)
 {
 	int ret;
@@ -274,207 +234,32 @@ static int ocfs2_writepage(struct page *
 	return ret;
 }
 
-/*
- * ocfs2_prepare_write() can be an outer-most ocfs2 call when it is called
- * from loopback.  It must be able to perform its own locking around
- * ocfs2_get_block().
- */
-int ocfs2_prepare_write(struct file *file, struct page *page,
-			unsigned from, unsigned to)
+static int ocfs2_prepare_write(struct file *file, struct page *page,
+		unsigned from, unsigned to)
 {
-	struct inode *inode = page->mapping->host;
 	int ret;
 
 	mlog_entry("(0x%p, 0x%p, %u, %u)\n", file, page, from, to);
 
-	ret = ocfs2_meta_lock_with_page(inode, NULL, NULL, 0, page);
-	if (ret != 0) {
-		mlog_errno(ret);
-		goto out;
-	}
-
-	down_read(&OCFS2_I(inode)->ip_alloc_sem);
-
-	ret = block_prepare_write(page, from, to, ocfs2_get_block);
-
-	up_read(&OCFS2_I(inode)->ip_alloc_sem);
+	ret = cont_prepare_write(page, from, to, ocfs2_get_block,
+		&(OCFS2_I(page->mapping->host)->ip_mmu_private));
 
-	ocfs2_meta_unlock(inode, 0);
-out:
 	mlog_exit(ret);
-	return ret;
-}
 
-/* Taken from ext3. We don't necessarily need the full blown
- * functionality yet, but IMHO it's better to cut and paste the whole
- * thing so we can avoid introducing our own bugs (and easily pick up
- * their fixes when they happen) --Mark */
-static int walk_page_buffers(	handle_t *handle,
-				struct buffer_head *head,
-				unsigned from,
-				unsigned to,
-				int *partial,
-				int (*fn)(	handle_t *handle,
-						struct buffer_head *bh))
-{
-	struct buffer_head *bh;
-	unsigned block_start, block_end;
-	unsigned blocksize = head->b_size;
-	int err, ret = 0;
-	struct buffer_head *next;
-
-	for (	bh = head, block_start = 0;
-		ret == 0 && (bh != head || !block_start);
-	    	block_start = block_end, bh = next)
-	{
-		next = bh->b_this_page;
-		block_end = block_start + blocksize;
-		if (block_end <= from || block_start >= to) {
-			if (partial && !buffer_uptodate(bh))
-				*partial = 1;
-			continue;
-		}
-		err = (*fn)(handle, bh);
-		if (!ret)
-			ret = err;
-	}
 	return ret;
 }
 
-struct ocfs2_journal_handle *ocfs2_start_walk_page_trans(struct inode *inode,
-							 struct page *page,
-							 unsigned from,
-							 unsigned to)
-{
-	struct ocfs2_super *osb = OCFS2_SB(inode->i_sb);
-	struct ocfs2_journal_handle *handle = NULL;
-	int ret = 0;
-
-	handle = ocfs2_start_trans(osb, NULL, OCFS2_INODE_UPDATE_CREDITS);
-	if (!handle) {
-		ret = -ENOMEM;
-		mlog_errno(ret);
-		goto out;
-	}
-
-	if (ocfs2_should_order_data(inode)) {
-		ret = walk_page_buffers(handle->k_handle,
-					page_buffers(page),
-					from, to, NULL,
-					ocfs2_journal_dirty_data);
-		if (ret < 0) 
-			mlog_errno(ret);
-	}
-out:
-	if (ret) {
-		if (handle)
-			ocfs2_commit_trans(handle);
-		handle = ERR_PTR(ret);
-	}
-	return handle;
-}
-
 static int ocfs2_commit_write(struct file *file, struct page *page,
 			      unsigned from, unsigned to)
 {
-	int ret, extending = 0, locklevel = 0;
-	loff_t new_i_size;
-	struct buffer_head *di_bh = NULL;
-	struct inode *inode = page->mapping->host;
-	struct ocfs2_journal_handle *handle = NULL;
+	int ret;
 
 	mlog_entry("(0x%p, 0x%p, %u, %u)\n", file, page, from, to);
 
-	/* NOTE: ocfs2_file_aio_write has ensured that it's safe for
-	 * us to sample inode->i_size here without the metadata lock:
-	 *
-	 * 1) We're currently holding the inode alloc lock, so no
-	 *    nodes can change it underneath us.
-	 *
-	 * 2) We've had to take the metadata lock at least once
-	 *    already to check for extending writes, hence insuring
-	 *    that our current copy is also up to date.
-	 */
-	new_i_size = ((loff_t)page->index << PAGE_CACHE_SHIFT) + to;
-	if (new_i_size > i_size_read(inode)) {
-		extending = 1;
-		locklevel = 1;
-	}
-
-	ret = ocfs2_meta_lock_with_page(inode, NULL, &di_bh, locklevel, page);
-	if (ret != 0) {
-		mlog_errno(ret);
-		goto out;
-	}
-
-	ret = ocfs2_data_lock_with_page(inode, 1, page);
-	if (ret != 0) {
-		mlog_errno(ret);
-		goto out_unlock_meta;
-	}
-
-	if (extending) {
-		handle = ocfs2_start_walk_page_trans(inode, page, from, to);
-		if (IS_ERR(handle)) {
-			ret = PTR_ERR(handle);
-			handle = NULL;
-			goto out_unlock_data;
-		}
-
-		/* Mark our buffer early. We'd rather catch this error up here
-		 * as opposed to after a successful commit_write which would
-		 * require us to set back inode->i_size. */
-		ret = ocfs2_journal_access(handle, inode, di_bh,
-					   OCFS2_JOURNAL_ACCESS_WRITE);
-		if (ret < 0) {
-			mlog_errno(ret);
-			goto out_commit;
-		}
-	}
-
-	/* might update i_size */
 	ret = generic_commit_write(file, page, from, to);
-	if (ret < 0) {
-		mlog_errno(ret);
-		goto out_commit;
-	}
-
-	if (extending) {
-		loff_t size = (u64) i_size_read(inode);
-		struct ocfs2_dinode *di =
-			(struct ocfs2_dinode *)di_bh->b_data;
-
-		/* ocfs2_mark_inode_dirty is too heavy to use here. */
-		inode->i_blocks = ocfs2_align_bytes_to_sectors(size);
-		inode->i_ctime = inode->i_mtime = CURRENT_TIME;
-
-		di->i_size = cpu_to_le64(size);
-		di->i_ctime = di->i_mtime = 
-				cpu_to_le64(inode->i_mtime.tv_sec);
-		di->i_ctime_nsec = di->i_mtime_nsec = 
-				cpu_to_le32(inode->i_mtime.tv_nsec);
-
-		ret = ocfs2_journal_dirty(handle, di_bh);
-		if (ret < 0) {
-			mlog_errno(ret);
-			goto out_commit;
-		}
-	}
-
-	BUG_ON(extending && (i_size_read(inode) != new_i_size));
-
-out_commit:
-	if (handle)
-		ocfs2_commit_trans(handle);
-out_unlock_data:
-	ocfs2_data_unlock(inode, 1);
-out_unlock_meta:
-	ocfs2_meta_unlock(inode, locklevel);
-out:
-	if (di_bh)
-		brelse(di_bh);
 
 	mlog_exit(ret);
+
 	return ret;
 }
 
@@ -592,26 +377,6 @@ bail:
 	return ret;
 }
 
-/* 
- * ocfs2_dio_end_io is called by the dio core when a dio is finished.  We're
- * particularly interested in the aio/dio case.  Like the core uses
- * i_alloc_sem, we use the rw_lock DLM lock to protect io on one node from
- * truncation on another.
- */
-static void ocfs2_dio_end_io(struct kiocb *iocb,
-			     loff_t offset,
-			     ssize_t bytes,
-			     void *private)
-{
-	struct inode *inode = iocb->ki_filp->f_dentry->d_inode;
-
-	/* this io's submitter should not have unlocked this before we could */
-	BUG_ON(!ocfs2_iocb_is_rw_locked(iocb));
-	ocfs2_iocb_clear_rw_locked(iocb);
-	up_read(&inode->i_alloc_sem);
-	ocfs2_rw_unlock(inode, 0);
-}
-
 static ssize_t ocfs2_direct_IO(int rw,
 			       struct kiocb *iocb,
 			       const struct iovec *iov,
@@ -623,11 +388,13 @@ static ssize_t ocfs2_direct_IO(int rw,
 	int ret;
 
 	mlog_entry_void();
+
+	/* blockdev_direct_IO checks alignment for us, using */
 	ret = blockdev_direct_IO_no_locking(rw, iocb, inode,
 					    inode->i_sb->s_bdev, iov, offset,
-					    nr_segs, 
-					    ocfs2_direct_IO_get_blocks,
-					    ocfs2_dio_end_io);
+					    nr_segs, ocfs2_direct_IO_get_blocks,
+					    NULL);
+
 	mlog_exit(ret);
 	return ret;
 }
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/aops.h linux-2.6.15-ocfs2/fs/ocfs2/aops.h
--- linux-2.6.15-2/fs/ocfs2/aops.h	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/aops.h	1969-12-31 19:00:00.000000000 -0500
@@ -1,41 +0,0 @@
-/* -*- mode: c; c-basic-offset: 8; -*-
- * vim: noexpandtab sw=8 ts=8 sts=0:
- *
- * Copyright (C) 2002, 2004, 2005 Oracle.  All rights reserved.
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public
- * License as published by the Free Software Foundation; either
- * version 2 of the License, or (at your option) any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * You should have received a copy of the GNU General Public
- * License along with this program; if not, write to the
- * Free Software Foundation, Inc., 59 Temple Place - Suite 330,
- * Boston, MA 021110-1307, USA.
- */
-
-#ifndef OCFS2_AOPS_H
-#define OCFS2_AOPS_H
-
-int ocfs2_prepare_write(struct file *file, struct page *page,
-			unsigned from, unsigned to);
-
-struct ocfs2_journal_handle *ocfs2_start_walk_page_trans(struct inode *inode,
-							 struct page *page,
-							 unsigned from,
-							 unsigned to);
-
-/* all ocfs2_dio_end_io()'s fault */
-#define ocfs2_iocb_is_rw_locked(iocb) \
-	test_bit(0, (unsigned long *)&iocb->private)
-#define ocfs2_iocb_set_rw_locked(iocb) \
-	set_bit(0, (unsigned long *)&iocb->private)
-#define ocfs2_iocb_clear_rw_locked(iocb) \
-	clear_bit(0, (unsigned long *)&iocb->private)
-
-#endif /* OCFS2_FILE_H */
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/buffer_head_io.c linux-2.6.15-ocfs2/fs/ocfs2/buffer_head_io.c
--- linux-2.6.15-2/fs/ocfs2/buffer_head_io.c	2006-02-06 18:15:15.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/buffer_head_io.c	2006-02-06 17:59:23.000000000 -0500
@@ -39,53 +39,109 @@
 
 #include "buffer_head_io.h"
 
-int ocfs2_write_block(struct ocfs2_super *osb, struct buffer_head *bh,
-		      struct inode *inode)
+void ocfs2_end_buffer_io_sync(struct buffer_head *bh,
+			      int uptodate)
+{
+	if (!uptodate)
+		mlog_errno(-EIO);
+
+	if (uptodate)
+		set_buffer_uptodate(bh);
+	else
+		clear_buffer_uptodate(bh);
+	unlock_buffer(bh);
+}
+
+int ocfs2_write_blocks(struct ocfs2_super *osb, struct buffer_head *bhs[],
+		       int nr, struct inode *inode)
 {
-	int ret = 0;
+	int status = 0;
+	int i;
+	struct buffer_head *bh;
 
-	mlog_entry("(bh->b_blocknr = %llu, inode=%p)\n",
-		   (unsigned long long)bh->b_blocknr, inode);
+	mlog_entry("(bh[0]->b_blocknr = %llu, nr=%d, inode=%p)\n",
+		   (unsigned long long)bhs[0]->b_blocknr, nr, inode);
 
-	BUG_ON(bh->b_blocknr < OCFS2_SUPER_BLOCK_BLKNO);
-	BUG_ON(buffer_jbd(bh));
+	if (osb == NULL || osb->sb == NULL || bhs == NULL) {
+		status = -EINVAL;
+		mlog_errno(status);
+		goto bail;
+	}
 
 	/* No need to check for a soft readonly file system here. non
 	 * journalled writes are only ever done on system files which
 	 * can get modified during recovery even if read-only. */
 	if (ocfs2_is_hard_readonly(osb)) {
-		ret = -EROFS;
-		goto out;
+		status = -EROFS;
+		goto bail;
 	}
 
-	mutex_lock(&OCFS2_I(inode)->ip_io_mutex);
+	if (inode)
+		down(&OCFS2_I(inode)->ip_io_sem);
+	for (i = 0 ; i < nr ; i++) {
+		bh = bhs[i];
+		if (bh == NULL) {
+			if (inode)
+				up(&OCFS2_I(inode)->ip_io_sem);
+			status = -EIO;
+			mlog_errno(status);
+			goto bail;
+		}
+
+		if (unlikely(bh->b_blocknr < OCFS2_SUPER_BLOCK_BLKNO)) {
+			BUG();
+			status = -EIO;
+			mlog_errno(status);
+			goto bail;
+		}
 
-	lock_buffer(bh);
-	set_buffer_uptodate(bh);
+		if (unlikely(buffer_jbd(bh))) {
+			/* What are you thinking?! */
+			mlog(ML_ERROR, "trying to write a jbd managed bh "
+				       "(blocknr = %llu), nr=%d\n",
+			     (unsigned long long)bh->b_blocknr, nr);
+			BUG();
+		}
 
-	/* remove from dirty list before I/O. */
-	clear_buffer_dirty(bh);
+		lock_buffer(bh);
 
-	get_bh(bh); /* for end_buffer_write_sync() */                   
-	bh->b_end_io = end_buffer_write_sync;
-	submit_bh(WRITE, bh);
+		set_buffer_uptodate(bh);
 
-	wait_on_buffer(bh);
+		/* remove from dirty list before I/O. */
+		clear_buffer_dirty(bh);
 
-	if (buffer_uptodate(bh)) {
-		ocfs2_set_buffer_uptodate(inode, bh);
-	} else {
-		/* We don't need to remove the clustered uptodate
-		 * information for this bh as it's not marked locally
-		 * uptodate. */
-		ret = -EIO;
-		brelse(bh);
+		bh->b_end_io = ocfs2_end_buffer_io_sync;
+		submit_bh(WRITE, bh);
 	}
 
-	mutex_unlock(&OCFS2_I(inode)->ip_io_mutex);
-out:
-	mlog_exit(ret);
-	return ret;
+	for (i = (nr - 1) ; i >= 0; i--) {
+		bh = bhs[i];
+
+		wait_on_buffer(bh);
+
+		if (!buffer_uptodate(bh)) {
+			/* Status won't be cleared from here on out,
+			 * so we can safely record this and loop back
+			 * to cleanup the other buffers. Don't need to
+			 * remove the clustered uptodate information
+			 * for this bh as it's not marked locally
+			 * uptodate. */
+			status = -EIO;
+			brelse(bh);
+			bhs[i] = NULL;
+			continue;
+		}
+
+		if (inode)
+			ocfs2_set_buffer_uptodate(inode, bh);
+	}
+	if (inode)
+		up(&OCFS2_I(inode)->ip_io_sem);
+
+bail:
+
+	mlog_exit(status);
+	return status;
 }
 
 int ocfs2_read_blocks(struct ocfs2_super *osb, u64 block, int nr,
@@ -125,13 +181,13 @@ int ocfs2_read_blocks(struct ocfs2_super
 		flags &= ~OCFS2_BH_CACHED;
 
 	if (inode)
-		mutex_lock(&OCFS2_I(inode)->ip_io_mutex);
+		down(&OCFS2_I(inode)->ip_io_sem);
 	for (i = 0 ; i < nr ; i++) {
 		if (bhs[i] == NULL) {
 			bhs[i] = sb_getblk(sb, block++);
 			if (bhs[i] == NULL) {
 				if (inode)
-					mutex_unlock(&OCFS2_I(inode)->ip_io_mutex);
+					up(&OCFS2_I(inode)->ip_io_sem);
 				status = -EIO;
 				mlog_errno(status);
 				goto bail;
@@ -182,8 +238,7 @@ int ocfs2_read_blocks(struct ocfs2_super
 #endif
 			}
 			clear_buffer_uptodate(bh);
-			get_bh(bh); /* for end_buffer_read_sync() */
-			bh->b_end_io = end_buffer_read_sync;
+			bh->b_end_io = ocfs2_end_buffer_io_sync;
 			if (flags & OCFS2_BH_READAHEAD)
 				submit_bh(READA, bh);
 			else
@@ -220,7 +275,7 @@ int ocfs2_read_blocks(struct ocfs2_super
 			ocfs2_set_buffer_uptodate(inode, bh);
 	}
 	if (inode)
-		mutex_unlock(&OCFS2_I(inode)->ip_io_mutex);
+		up(&OCFS2_I(inode)->ip_io_sem);
 
 	mlog(ML_BH_IO, "block=(%"MLFu64"), nr=(%d), cached=%s\n", block, nr,
 	     (!(flags & OCFS2_BH_CACHED) || ignore_cache) ? "no" : "yes");
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/buffer_head_io.h linux-2.6.15-ocfs2/fs/ocfs2/buffer_head_io.h
--- linux-2.6.15-2/fs/ocfs2/buffer_head_io.h	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/buffer_head_io.h	2006-02-06 17:59:23.000000000 -0500
@@ -31,15 +31,20 @@
 void ocfs2_end_buffer_io_sync(struct buffer_head *bh,
 			     int uptodate);
 
+/* Yosh made me do it. */
+static inline int ocfs2_write_block(struct ocfs2_super         *osb,
+				    struct buffer_head  *bh,
+				    struct inode        *inode);
 static inline int ocfs2_read_block(struct ocfs2_super          *osb,
 				   u64                  off,
 				   struct buffer_head **bh,
 				   int                  flags,
 				   struct inode        *inode);
 
-int ocfs2_write_block(struct ocfs2_super          *osb,
-		      struct buffer_head  *bh,
-		      struct inode        *inode);
+int ocfs2_write_blocks(struct ocfs2_super          *osb,
+		       struct buffer_head  *bh[],
+		       int                  nr,
+		       struct inode        *inode);
 int ocfs2_read_blocks(struct ocfs2_super          *osb,
 		      u64                  block,
 		      int                  nr,
@@ -51,6 +56,16 @@ int ocfs2_read_blocks(struct ocfs2_super
 #define OCFS2_BH_CACHED            1
 #define OCFS2_BH_READAHEAD         8	/* use this to pass READA down to submit_bh */
 
+static inline int ocfs2_write_block(struct ocfs2_super * osb, struct buffer_head *bh,
+				    struct inode *inode)
+{
+	int status;
+
+	status = ocfs2_write_blocks(osb, &bh, 1, inode);
+
+	return status;
+}
+
 static inline int ocfs2_read_block(struct ocfs2_super * osb, u64 off,
 				   struct buffer_head **bh, int flags,
 				   struct inode *inode)
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/cluster/heartbeat.c linux-2.6.15-ocfs2/fs/ocfs2/cluster/heartbeat.c
--- linux-2.6.15-2/fs/ocfs2/cluster/heartbeat.c	2006-02-06 18:15:15.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/cluster/heartbeat.c	2006-02-06 18:05:55.000000000 -0500
@@ -77,7 +77,7 @@ unsigned int o2hb_dead_threshold = O2HB_
  * No locking or otherwise interesting code is required for reading
  * o2hb_dead_threshold as it can't change once regions are active and
  * it's not interesting to anyone until then anyway. */
-static void o2hb_dead_threshold_set(unsigned int threshold)
+void o2hb_dead_threshold_set(unsigned int threshold)
 {
 	if (threshold > O2HB_MIN_DEAD_THRESHOLD) {
 		spin_lock(&o2hb_live_lock);
@@ -917,9 +918,8 @@ static int o2hb_thread(void *data)
 		elapsed_msec = o2hb_elapsed_msecs(&before_hb, &after_hb);
 
 		mlog(0, "start = %lu.%lu, end = %lu.%lu, msec = %u\n",
-		     before_hb.tv_sec, (unsigned long) before_hb.tv_usec,
-		     after_hb.tv_sec, (unsigned long) after_hb.tv_usec,
-		     elapsed_msec);
+		     before_hb.tv_sec, before_hb.tv_usec,
+		     after_hb.tv_sec, after_hb.tv_usec, elapsed_msec);
 
 		if (elapsed_msec < reg->hr_timeout_ms) {
 			/* the kthread api has blocked signals for us so no
@@ -970,8 +970,7 @@ void o2hb_init(void)
 }
 
 /* if we're already in a callback then we're already serialized by the sem */
-static void o2hb_fill_node_map_from_callback(unsigned long *map,
-					     unsigned bytes)
+void o2hb_fill_node_map_from_callback(unsigned long *map, unsigned bytes)
 {
 	BUG_ON(bytes < (BITS_TO_LONGS(O2NM_MAX_NODES) * sizeof(unsigned long)));
 
@@ -1533,81 +1532,6 @@ static void o2hb_heartbeat_group_drop_it
 	config_item_put(item);
 }
 
-struct o2hb_heartbeat_group_attribute {
-	struct configfs_attribute attr;
-	ssize_t (*show)(struct o2hb_heartbeat_group *, char *);
-	ssize_t (*store)(struct o2hb_heartbeat_group *, const char *, size_t);
-};
-
-static ssize_t o2hb_heartbeat_group_show(struct config_item *item,
-					 struct configfs_attribute *attr,
-					 char *page)
-{
-	struct o2hb_heartbeat_group *reg = to_o2hb_heartbeat_group(to_config_group(item));
-	struct o2hb_heartbeat_group_attribute *o2hb_heartbeat_group_attr =
-		container_of(attr, struct o2hb_heartbeat_group_attribute, attr);
-	ssize_t ret = 0;
-
-	if (o2hb_heartbeat_group_attr->show)
-		ret = o2hb_heartbeat_group_attr->show(reg, page);
-	return ret;
-}
-
-static ssize_t o2hb_heartbeat_group_store(struct config_item *item,
-					  struct configfs_attribute *attr,
-					  const char *page, size_t count)
-{
-	struct o2hb_heartbeat_group *reg = to_o2hb_heartbeat_group(to_config_group(item));
-	struct o2hb_heartbeat_group_attribute *o2hb_heartbeat_group_attr =
-		container_of(attr, struct o2hb_heartbeat_group_attribute, attr);
-	ssize_t ret = -EINVAL;
-
-	if (o2hb_heartbeat_group_attr->store)
-		ret = o2hb_heartbeat_group_attr->store(reg, page, count);
-	return ret;
-}
-
-static ssize_t o2hb_heartbeat_group_threshold_show(struct o2hb_heartbeat_group *group,
-						     char *page)
-{
-	return sprintf(page, "%u\n", o2hb_dead_threshold);
-}
-
-static ssize_t o2hb_heartbeat_group_threshold_store(struct o2hb_heartbeat_group *group,
-						    const char *page,
-						    size_t count)
-{
-	unsigned long tmp;
-	char *p = (char *)page;
-
-	tmp = simple_strtoul(p, &p, 10);
-	if (!p || (*p && (*p != '\n')))
-                return -EINVAL;
-
-	/* this will validate ranges for us. */
-	o2hb_dead_threshold_set((unsigned int) tmp);
-
-	return count;
-}
-
-static struct o2hb_heartbeat_group_attribute o2hb_heartbeat_group_attr_threshold = {
-	.attr	= { .ca_owner = THIS_MODULE,
-		    .ca_name = "dead_threshold",
-		    .ca_mode = S_IRUGO | S_IWUSR },
-	.show	= o2hb_heartbeat_group_threshold_show,
-	.store	= o2hb_heartbeat_group_threshold_store,
-};
-
-static struct configfs_attribute *o2hb_heartbeat_group_attrs[] = {
-	&o2hb_heartbeat_group_attr_threshold.attr,
-	NULL,
-};
-
-static struct configfs_item_operations o2hb_hearbeat_group_item_ops = {
-	.show_attribute		= o2hb_heartbeat_group_show,
-	.store_attribute	= o2hb_heartbeat_group_store,
-};
-
 static struct configfs_group_operations o2hb_heartbeat_group_group_ops = {
 	.make_item	= o2hb_heartbeat_group_make_item,
 	.drop_item	= o2hb_heartbeat_group_drop_item,
@@ -1615,8 +1539,6 @@ static struct configfs_group_operations 
 
 static struct config_item_type o2hb_heartbeat_group_type = {
 	.ct_group_ops	= &o2hb_heartbeat_group_group_ops,
-	.ct_item_ops	= &o2hb_hearbeat_group_item_ops,
-	.ct_attrs	= o2hb_heartbeat_group_attrs,
 	.ct_owner	= THIS_MODULE,
 };
 
@@ -1778,6 +1700,23 @@ int o2hb_check_local_node_heartbeating(v
 }
 EXPORT_SYMBOL_GPL(o2hb_check_local_node_heartbeating);
 
+/* Makes sure our local node is configured with a node number, and is
+ * heartbeating. */
+int o2hb_check_local_node_heartbeating_from_callback(void)
+{
+	u8 node_num;
+
+	/* if this node was set then we have networking */
+	node_num = o2nm_this_node();
+	if (node_num == O2NM_MAX_NODES) {
+		mlog(ML_HEARTBEAT, "this node has not been configured.\n");
+		return 0;
+	}
+
+	return o2hb_check_node_heartbeating_from_callback(node_num);
+}
+EXPORT_SYMBOL_GPL(o2hb_check_local_node_heartbeating_from_callback);
+
 /*
  * this is just a hack until we get the plumbing which flips file systems
  * read only and drops the hb ref instead of killing the node dead.
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/cluster/heartbeat.h linux-2.6.15-ocfs2/fs/ocfs2/cluster/heartbeat.h
--- linux-2.6.15-2/fs/ocfs2/cluster/heartbeat.h	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/cluster/heartbeat.h	2006-02-06 17:59:23.000000000 -0500
@@ -40,6 +40,9 @@ extern unsigned int o2hb_dead_threshold;
 #define O2HB_MIN_DEAD_THRESHOLD	  2
 #define O2HB_MAX_WRITE_TIMEOUT_MS (O2HB_REGION_TIMEOUT_MS * (o2hb_dead_threshold - 1))
 
+/* Always use this to set o2hb_dead_threshold */
+void o2hb_dead_threshold_set(unsigned int threshold);
+
 #define O2HB_CB_MAGIC		0x51d1e4ec
 
 /* callback stuff */
@@ -73,10 +76,12 @@ int o2hb_register_callback(struct o2hb_c
 int o2hb_unregister_callback(struct o2hb_callback_func *hc);
 void o2hb_fill_node_map(unsigned long *map,
 			unsigned bytes);
+void o2hb_fill_node_map_from_callback(unsigned long *map, unsigned bytes);
 void o2hb_init(void);
 int o2hb_check_node_heartbeating(u8 node_num);
 int o2hb_check_node_heartbeating_from_callback(u8 node_num);
 int o2hb_check_local_node_heartbeating(void);
+int o2hb_check_local_node_heartbeating_from_callback(void);
 void o2hb_stop_all_regions(void);
 
 #endif /* O2CLUSTER_HEARTBEAT_H */
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/cluster/Makefile linux-2.6.15-ocfs2/fs/ocfs2/cluster/Makefile
--- linux-2.6.15-2/fs/ocfs2/cluster/Makefile	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/cluster/Makefile	2006-02-06 17:59:23.000000000 -0500
@@ -1,4 +1,5 @@
 obj-$(CONFIG_OCFS2_FS) += ocfs2_nodemanager.o
 
-ocfs2_nodemanager-objs := heartbeat.o masklog.o sys.o nodemanager.o \
-	quorum.o tcp.o ver.o
+ocfs2_nodemanager-objs := nodemanager.o heartbeat.o tcp.o net_proc.o \
+	masklog.o quorum.o ver.o
+
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/cluster/masklog.c linux-2.6.15-ocfs2/fs/ocfs2/cluster/masklog.c
--- linux-2.6.15-2/fs/ocfs2/cluster/masklog.c	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/cluster/masklog.c	2006-02-06 17:38:14.000000000 -0500
@@ -33,134 +33,195 @@ EXPORT_SYMBOL_GPL(mlog_and_bits);
 struct mlog_bits mlog_not_bits = MLOG_BITS_RHS(MLOG_INITIAL_NOT_MASK);
 EXPORT_SYMBOL_GPL(mlog_not_bits);
 
-static ssize_t mlog_mask_show(u64 mask, char *buf)
+static char *mlog_bit_names[MLOG_MAX_BITS];
+
+static void *mlog_name_from_pos(loff_t *caller_pos)
+{
+	loff_t pos = *caller_pos;
+	while (pos < ARRAY_SIZE(mlog_bit_names) && mlog_bit_names[pos] == NULL)
+		pos++;
+
+	if (pos >= ARRAY_SIZE(mlog_bit_names))
+		return NULL;
+
+	*caller_pos = pos;
+	return &mlog_bit_names[pos];
+}
+
+static void *mlog_seq_start(struct seq_file *seq, loff_t *pos)
 {
+	return mlog_name_from_pos(pos);
+}
+
+static void *mlog_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+{
+	(*pos)++;
+	return mlog_name_from_pos(pos);
+}
+
+static int mlog_seq_show(struct seq_file *seq, void *v)
+{
+	char **name = v;
+	int bit = name - mlog_bit_names;
 	char *state;
 
-	if (__mlog_test_u64(mask, mlog_and_bits))
+	if (__mlog_test_u64((u64)1 << bit, mlog_and_bits))
 		state = "allow";
-	else if (__mlog_test_u64(mask, mlog_not_bits))
+	else if (__mlog_test_u64((u64)1 << bit, mlog_not_bits))
 		state = "deny";
 	else
 		state = "off";
 
-	return snprintf(buf, PAGE_SIZE, "%s\n", state);
+	seq_printf(seq, "%s %s\n", *name, state);
+	return 0;
 }
 
-static ssize_t mlog_mask_store(u64 mask, const char *buf, size_t count)
+static void mlog_seq_stop(struct seq_file *p, void *v)
 {
-	if (!strnicmp(buf, "allow", 5)) {
-		__mlog_set_u64(mask, mlog_and_bits);
-		__mlog_clear_u64(mask, mlog_not_bits);
-	} else if (!strnicmp(buf, "deny", 4)) {
-		__mlog_set_u64(mask, mlog_not_bits);
-		__mlog_clear_u64(mask, mlog_and_bits);
-	} else if (!strnicmp(buf, "off", 3)) {
-		__mlog_clear_u64(mask, mlog_not_bits);
-		__mlog_clear_u64(mask, mlog_and_bits);
-	} else
-		return -EINVAL;
-
-	return count;
 }
 
-struct mlog_attribute {
-	struct attribute attr;
-	u64 mask;
+static struct seq_operations mlog_seq_ops = {
+	.start = mlog_seq_start,
+	.next = mlog_seq_next,
+	.stop = mlog_seq_stop,
+	.show = mlog_seq_show,
 };
 
-#define to_mlog_attr(_attr) container_of(_attr, struct mlog_attribute, attr)
+static int mlog_fop_open(struct inode *inode, struct file *file)
+{
+	return seq_open(file, &mlog_seq_ops);
+}
 
-#define define_mask(_name) {			\
-	.attr = {				\
-		.name = #_name,			\
-		.mode = S_IRUGO | S_IWUSR,	\
-	},					\
-	.mask = ML_##_name,			\
-}
-
-static struct mlog_attribute mlog_attrs[MLOG_MAX_BITS] = {
-	define_mask(ENTRY),
-	define_mask(EXIT),
-	define_mask(TCP),
-	define_mask(MSG),
-	define_mask(SOCKET),
-	define_mask(HEARTBEAT),
-	define_mask(HB_BIO),
-	define_mask(DLMFS),
-	define_mask(DLM),
-	define_mask(DLM_DOMAIN),
-	define_mask(DLM_THREAD),
-	define_mask(DLM_MASTER),
-	define_mask(DLM_RECOVERY),
-	define_mask(AIO),
-	define_mask(JOURNAL),
-	define_mask(DISK_ALLOC),
-	define_mask(SUPER),
-	define_mask(FILE_IO),
-	define_mask(EXTENT_MAP),
-	define_mask(DLM_GLUE),
-	define_mask(BH_IO),
-	define_mask(UPTODATE),
-	define_mask(NAMEI),
-	define_mask(INODE),
-	define_mask(VOTE),
-	define_mask(DCACHE),
-	define_mask(CONN),
-	define_mask(QUORUM),
-	define_mask(EXPORT),
-	define_mask(ERROR),
-	define_mask(NOTICE),
-	define_mask(KTHREAD),
-};
+static ssize_t mlog_fop_write(struct file *filp, const char __user *buf,
+			      size_t count, loff_t *pos)
+{
+	char *name;
+	char str[32], *mask, *val;
+	unsigned i, masklen, namelen;
 
-static struct attribute *mlog_attr_ptrs[MLOG_MAX_BITS] = {NULL, };
+	if (count == 0)
+		return 0;
 
-static ssize_t mlog_show(struct kobject *obj, struct attribute *attr,
-			 char *buf)
-{
-	struct mlog_attribute *mlog_attr = to_mlog_attr(attr);
+	/* count at least mask + space + 3 for "off" */
+	if (*pos != 0 || count < 5 || count >= sizeof(str))
+		return -EINVAL;
 
-	return mlog_mask_show(mlog_attr->mask, buf);
-}
+	if (copy_from_user(str, buf, count))
+		return -EFAULT;
 
-static ssize_t mlog_store(struct kobject *obj, struct attribute *attr,
-			  const char *buf, size_t count)
-{
-	struct mlog_attribute *mlog_attr = to_mlog_attr(attr);
+	str[count] = '\0';
 
-	return mlog_mask_store(mlog_attr->mask, buf, count);
-}
+	mask = str;
+	val = strchr(str, ' ');
+	if (val == NULL)
+		return -EINVAL;
+	*val = '\0';
+	val++;
 
-static struct sysfs_ops mlog_attr_ops = {
-	.show  = mlog_show,
-	.store = mlog_store,
-};
+	if (strlen(val) == 0)
+		return -EINVAL;
 
-static struct kobj_type mlog_ktype = {
-	.default_attrs = mlog_attr_ptrs,
-	.sysfs_ops     = &mlog_attr_ops,
-};
+	masklen = strlen(mask);
 
-static struct kset mlog_kset = {
-	.kobj   = {.name = "logmask", .ktype = &mlog_ktype},
-};
+	for (i = 0; i < ARRAY_SIZE(mlog_bit_names); i++) {
+		name = mlog_bit_names[i];
 
-int mlog_sys_init(struct subsystem *o2cb_subsys)
-{
-	int i = 0;
+		if (name == NULL)
+			continue;
+
+		namelen = strlen(name);
 
-	while (mlog_attrs[i].attr.mode) {
-		mlog_attr_ptrs[i] = &mlog_attrs[i].attr;
-		i++;
+		if (namelen != masklen
+		    || strnicmp(mask, name, namelen))
+			continue;
+		break;
 	}
-	mlog_attr_ptrs[i] = NULL;
+	if (i == ARRAY_SIZE(mlog_bit_names))
+		return -EINVAL;
 
-	mlog_kset.subsys = o2cb_subsys;
-	return kset_register(&mlog_kset);
+	if (!strnicmp(val, "allow", 5)) {
+		__mlog_set_u64((u64)1 << i, mlog_and_bits);
+		__mlog_clear_u64((u64)1 << i, mlog_not_bits);
+	} else if (!strnicmp(val, "deny", 4)) {
+		__mlog_set_u64((u64)1 << i, mlog_not_bits);
+		__mlog_clear_u64((u64)1 << i, mlog_and_bits);
+	} else if (!strnicmp(val, "off", 3)) {
+		__mlog_clear_u64((u64)1 << i, mlog_not_bits);
+		__mlog_clear_u64((u64)1 << i, mlog_and_bits);
+	} else
+		return -EINVAL;
+
+	*pos += count;
+	return count;
 }
 
-void mlog_sys_shutdown(void)
-{
-	kset_unregister(&mlog_kset);
+static struct file_operations mlog_seq_fops = {
+	.owner = THIS_MODULE,
+	.open = mlog_fop_open,
+	.read = seq_read,
+	.write = mlog_fop_write,
+	.llseek = seq_lseek,
+	.release = seq_release,
+};
+
+#define set_a_string(which) do {					\
+	struct mlog_bits _bits = {{0,}, };				\
+	int _bit;							\
+	__mlog_set_u64(ML_##which, _bits);				\
+	_bit = find_first_bit(_bits.words, MLOG_MAX_BITS);		\
+	mlog_bit_names[_bit] = #which;					\
+} while (0)
+
+#define LOGMASK_PROC_NAME "log_mask"
+
+void mlog_remove_proc(struct proc_dir_entry *parent)
+{
+	remove_proc_entry(LOGMASK_PROC_NAME, parent);
+}
+
+int mlog_init_proc(struct proc_dir_entry *parent)
+{
+	struct proc_dir_entry *p;
+
+	set_a_string(ENTRY);
+	set_a_string(EXIT);
+	set_a_string(TCP);
+	set_a_string(MSG);
+	set_a_string(SOCKET);
+	set_a_string(HEARTBEAT);
+	set_a_string(HB_BIO);
+	set_a_string(DLMFS);
+	set_a_string(DLM);
+	set_a_string(DLM_DOMAIN);
+	set_a_string(DLM_THREAD);
+	set_a_string(DLM_MASTER);
+	set_a_string(DLM_RECOVERY);
+	set_a_string(AIO);
+	set_a_string(JOURNAL);
+	set_a_string(DISK_ALLOC);
+	set_a_string(SUPER);
+	set_a_string(FILE_IO);
+	set_a_string(EXTENT_MAP);
+	set_a_string(DLM_GLUE);
+	set_a_string(BH_IO);
+	set_a_string(UPTODATE);
+	set_a_string(NAMEI);
+	set_a_string(INODE);
+	set_a_string(VOTE);
+	set_a_string(DCACHE);
+	set_a_string(CONN);
+	set_a_string(QUORUM);
+	set_a_string(EXPORT);
+	set_a_string(ERROR);
+	set_a_string(NOTICE);
+	set_a_string(KTHREAD);
+
+	p = create_proc_entry(LOGMASK_PROC_NAME, S_IRUGO, parent);
+	if (p == NULL)
+		return -ENOMEM;
+
+	p->proc_fops = &mlog_seq_fops;
+
+	return 0;
 }
+EXPORT_SYMBOL_GPL(mlog_init_proc);
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/cluster/masklog.h linux-2.6.15-ocfs2/fs/ocfs2/cluster/masklog.h
--- linux-2.6.15-2/fs/ocfs2/cluster/masklog.h	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/cluster/masklog.h	2006-02-06 17:38:14.000000000 -0500
@@ -198,10 +198,8 @@ extern struct mlog_bits mlog_and_bits, m
 } while (0)
 
 #define mlog_errno(st) do {						\
-	int _st = (st);							\
-	if (_st != -ERESTARTSYS && _st != -EINTR &&			\
-	    _st != AOP_TRUNCATED_PAGE)					\
-		mlog(ML_ERROR, "status = %lld\n", (long long)_st);	\
+	if ((st) != -ERESTARTSYS && (st) != -EINTR)			\
+		mlog(ML_ERROR, "status = %lld\n", (long long)(st));	\
 } while (0)
 
 #define mlog_entry(fmt, args...) do {					\
@@ -212,10 +210,11 @@ extern struct mlog_bits mlog_and_bits, m
 	mlog(ML_ENTRY, "ENTRY:\n");					\
 } while (0)
 
-/*
- * We disable this for sparse.
+/* We disable this for old compilers since they don't have support for
+ * __builtin_types_compatible_p.
  */
-#if !defined(__CHECKER__)
+#if (__GNUC__ > 3 || (__GNUC__ == 3 && __GNUC_MINOR__ >= 1)) && \
+    !defined(__CHECKER__)
 #define mlog_exit(st) do {						     \
 	if (__builtin_types_compatible_p(typeof(st), unsigned long))	     \
 		mlog(ML_EXIT, "EXIT: %lu\n", (unsigned long) (st));	     \
@@ -266,9 +265,8 @@ extern struct mlog_bits mlog_and_bits, m
 #define MLFx64 "lx"
 #endif
 
-#include <linux/kobject.h>
-#include <linux/sysfs.h>
-int mlog_sys_init(struct subsystem *o2cb_subsys);
-void mlog_sys_shutdown(void);
+#include <linux/proc_fs.h>
+int mlog_init_proc(struct proc_dir_entry *parent);
+void mlog_remove_proc(struct proc_dir_entry *parent);
 
 #endif /* O2CLUSTER_MASKLOG_H */
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/cluster/net_proc.c linux-2.6.15-ocfs2/fs/ocfs2/cluster/net_proc.c
--- linux-2.6.15-2/fs/ocfs2/cluster/net_proc.c	1969-12-31 19:00:00.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/cluster/net_proc.c	2006-02-06 17:59:23.000000000 -0500
@@ -0,0 +1,389 @@
+/* -*- mode: c; c-basic-offset: 8; -*-
+ * vim: noexpandtab sw=8 ts=8 sts=0:
+ *
+ * Copyright (C) 2005 Oracle.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public
+ * License as published by the Free Software Foundation; either
+ * version 2 of the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public
+ * License along with this program; if not, write to the
+ * Free Software Foundation, Inc., 59 Temple Place - Suite 330,
+ * Boston, MA 021110-1307, USA.
+ */
+
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/slab.h>
+#include <linux/idr.h>
+#include <linux/kref.h>
+#include <linux/seq_file.h>
+
+#include <asm/uaccess.h>
+
+#include "tcp.h"
+#include "nodemanager.h"
+#define MLOG_MASK_PREFIX ML_TCP
+#include "masklog.h"
+
+#include "tcp_internal.h"
+
+static spinlock_t o2net_proc_lock = SPIN_LOCK_UNLOCKED;
+
+static LIST_HEAD(sock_containers);
+static LIST_HEAD(send_tracking);
+
+void o2net_proc_add_nst(struct o2net_send_tracking *nst)
+{
+	spin_lock(&o2net_proc_lock);
+	list_add(&nst->st_net_proc_item, &send_tracking);
+	spin_unlock(&o2net_proc_lock);
+}
+void o2net_proc_del_nst(struct o2net_send_tracking *nst)
+{
+	spin_lock(&o2net_proc_lock);
+	if (!list_empty(&nst->st_net_proc_item))
+		list_del_init(&nst->st_net_proc_item);
+	spin_unlock(&o2net_proc_lock);
+}
+
+static struct o2net_send_tracking *next_nst(struct o2net_send_tracking *nst_start)
+{
+	struct o2net_send_tracking *nst, *ret = NULL;
+
+	assert_spin_locked(&o2net_proc_lock);
+
+	list_for_each_entry(nst, &nst_start->st_net_proc_item,
+			    st_net_proc_item) {
+		/* discover the head of the list */
+		if (&nst->st_net_proc_item == &send_tracking)
+			break;
+
+		/* use st_task to detect real nsts in the list */
+		if (nst->st_task != NULL) {
+			ret = nst;
+			break;
+		}
+	}
+
+	return ret;
+}
+
+static void *nst_seq_start(struct seq_file *seq, loff_t *pos)
+{
+	struct o2net_send_tracking *nst, *dummy_nst = seq->private;
+
+	spin_lock(&o2net_proc_lock);
+	nst = next_nst(dummy_nst);
+	spin_unlock(&o2net_proc_lock);
+
+	return nst;
+}
+
+static void *nst_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+{
+	struct o2net_send_tracking *nst, *dummy_nst = seq->private;
+
+	spin_lock(&o2net_proc_lock);
+	nst = next_nst(dummy_nst);
+	list_del_init(&dummy_nst->st_net_proc_item);
+	if (nst)
+		list_add(&dummy_nst->st_net_proc_item,
+			 &nst->st_net_proc_item);
+	spin_unlock(&o2net_proc_lock);
+
+	return nst; /* unused, just needs to be null when done */
+}
+
+static int nst_seq_show(struct seq_file *seq, void *v)
+{
+	struct o2net_send_tracking *nst, *dummy_nst = seq->private;
+
+	spin_lock(&o2net_proc_lock);
+	nst = next_nst(dummy_nst);
+
+	if (nst != NULL) {
+		/* get_task_comm isn't exported.  oh well. */
+		seq_printf(seq, "%p:\n"
+			   "  pid:          %lu\n"
+			   "  tgid:         %lu\n"
+			   "  process name: %s\n"
+			   "  node:         %u\n"
+			   "  sc:           %p\n"
+			   "  message type: %u\n"
+			   "  message key:  0x%08x\n"
+			   "  sock acquiry: %lu.%lu\n"
+			   "  send start:   %lu.%lu\n"
+			   "  wait start:   %lu.%lu\n",
+			   nst, (unsigned long)nst->st_task->pid,
+			   (unsigned long)nst->st_task->tgid,
+			   nst->st_task->comm, nst->st_node,
+			   nst->st_sc, nst->st_msg_type, nst->st_msg_key,
+			   nst->st_sock_time.tv_sec, nst->st_sock_time.tv_usec,
+			   nst->st_send_time.tv_sec, nst->st_send_time.tv_usec,
+			   nst->st_status_time.tv_sec,
+			   nst->st_status_time.tv_usec);
+	}
+
+	spin_unlock(&o2net_proc_lock);
+
+	return 0;
+}
+
+static void nst_seq_stop(struct seq_file *seq, void *v)
+{
+}
+
+static struct seq_operations nst_seq_ops = {
+	.start = nst_seq_start,
+	.next = nst_seq_next,
+	.stop = nst_seq_stop,
+	.show = nst_seq_show,
+};
+
+static int nst_fop_open(struct inode *inode, struct file *file)
+{
+	struct o2net_send_tracking *dummy_nst;
+	struct seq_file *seq;
+	int ret;
+
+	dummy_nst = kmalloc(sizeof(struct o2net_send_tracking), GFP_KERNEL);
+	if (dummy_nst == NULL) {
+		ret = -ENOMEM;
+		goto out;
+	}
+	dummy_nst->st_task = NULL;
+
+	ret = seq_open(file, &nst_seq_ops);
+	if (ret)
+		goto out;
+
+	seq = file->private_data;
+	seq->private = dummy_nst;
+	o2net_proc_add_nst(dummy_nst);
+
+	dummy_nst = NULL;
+
+out:
+	kfree(dummy_nst);
+	return ret;
+}
+
+static int nst_fop_release(struct inode *inode, struct file *file)
+{
+	struct seq_file *seq = file->private_data;
+	struct o2net_send_tracking *dummy_nst = seq->private;
+
+	o2net_proc_del_nst(dummy_nst);
+	return seq_release_private(inode, file);
+}
+
+static struct file_operations nst_seq_fops = {
+	.owner = THIS_MODULE,
+	.open = nst_fop_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = nst_fop_release,
+};
+
+void o2net_proc_add_sc(struct o2net_sock_container *sc)
+{
+	spin_lock(&o2net_proc_lock);
+	list_add(&sc->sc_net_proc_item, &sock_containers);
+	spin_unlock(&o2net_proc_lock);
+}
+
+void o2net_proc_del_sc(struct o2net_sock_container *sc)
+{
+	spin_lock(&o2net_proc_lock);
+	list_del_init(&sc->sc_net_proc_item);
+	spin_unlock(&o2net_proc_lock);
+}
+
+static struct o2net_sock_container *next_sc(struct o2net_sock_container *sc_start)
+{
+	struct o2net_sock_container *sc, *ret = NULL;
+
+	assert_spin_locked(&o2net_proc_lock);
+
+	list_for_each_entry(sc, &sc_start->sc_net_proc_item, sc_net_proc_item) {
+		/* discover the head of the list miscast as a sc */
+		if (&sc->sc_net_proc_item == &sock_containers)
+			break;
+
+		/* use sc_page to detect real scs in the list */
+		if (sc->sc_page != NULL) {
+			ret = sc;
+			break;
+		}
+	}
+
+	return ret;
+}
+
+static void *sc_seq_start(struct seq_file *seq, loff_t *pos)
+{
+	struct o2net_sock_container *sc, *dummy_sc = seq->private;
+
+	spin_lock(&o2net_proc_lock);
+	sc = next_sc(dummy_sc);
+	spin_unlock(&o2net_proc_lock);
+
+	return sc;
+}
+
+static void *sc_seq_next(struct seq_file *seq, void *v, loff_t *pos)
+{
+	struct o2net_sock_container *sc, *dummy_sc = seq->private;
+
+	spin_lock(&o2net_proc_lock);
+	sc = next_sc(dummy_sc);
+	list_del_init(&dummy_sc->sc_net_proc_item);
+	if (sc)
+		list_add(&dummy_sc->sc_net_proc_item, &sc->sc_net_proc_item);
+	spin_unlock(&o2net_proc_lock);
+
+	return sc; /* unused, just needs to be null when done */
+}
+
+static int sc_seq_show(struct seq_file *seq, void *v)
+{
+	struct o2net_sock_container *sc, *dummy_sc = seq->private;
+
+	spin_lock(&o2net_proc_lock);
+	sc = next_sc(dummy_sc);
+
+	if (sc != NULL) {
+/* netdev 1, world 0 */
+#ifdef INET_SK_RETURNS_INET_OPT
+		struct inet_opt *inet = NULL;
+#else
+		struct inet_sock *inet = NULL;
+#endif
+		__be32 saddr = 0, daddr = 0;
+		__be16 sport = 0, dport = 0;
+
+		if (sc->sc_sock) {
+			inet = inet_sk(sc->sc_sock->sk);
+			/* the stack's structs aren't sparse endian clean */
+			saddr = (__force __be32)inet->saddr;
+			daddr = (__force __be32)inet->daddr;
+			sport = (__force __be16)inet->sport;
+			dport = (__force __be16)inet->dport;
+		}
+
+		/* XXX sigh, inet-> doesn't have sparse annotation so any
+		 * use of it here generates a warning with -Wbitwise */
+		seq_printf(seq, "%p:\n"
+			   "  krefs:           %d\n"
+			   "  sock:            %u.%u.%u.%u:%u -> %u.%u.%u.%u:%u\n"
+			   "  remote node:     %s\n"
+			   "  page off:        %zu\n",
+			   sc, atomic_read(&sc->sc_kref.refcount),
+			   NIPQUAD(saddr), inet ? ntohs(sport) : 0,
+			   NIPQUAD(daddr), inet ? ntohs(dport) : 0,
+			   sc->sc_node->nd_name, sc->sc_page_off);
+	}
+
+
+	spin_unlock(&o2net_proc_lock);
+
+	return 0;
+}
+
+static void sc_seq_stop(struct seq_file *seq, void *v)
+{
+}
+
+static struct seq_operations sc_seq_ops = {
+	.start = sc_seq_start,
+	.next = sc_seq_next,
+	.stop = sc_seq_stop,
+	.show = sc_seq_show,
+};
+
+static int sc_fop_open(struct inode *inode, struct file *file)
+{
+	struct o2net_sock_container *dummy_sc;
+	struct seq_file *seq;
+	int ret;
+
+	dummy_sc = kmalloc(sizeof(struct o2net_sock_container), GFP_KERNEL);
+	if (dummy_sc == NULL) {
+		ret = -ENOMEM;
+		goto out;
+	}
+	dummy_sc->sc_page = NULL;
+
+	ret = seq_open(file, &sc_seq_ops);
+	if (ret)
+		goto out;
+
+	seq = file->private_data;
+	seq->private = dummy_sc;
+	o2net_proc_add_sc(dummy_sc);
+
+	dummy_sc = NULL;
+
+out:
+	kfree(dummy_sc);
+	return ret;
+}
+
+static int sc_fop_release(struct inode *inode, struct file *file)
+{
+	struct seq_file *seq = file->private_data;
+	struct o2net_sock_container *dummy_sc = seq->private;
+
+	o2net_proc_del_sc(dummy_sc);
+	return seq_release_private(inode, file);
+}
+
+static struct file_operations sc_seq_fops = {
+	.owner = THIS_MODULE,
+	.open = sc_fop_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = sc_fop_release,
+};
+
+#define SC_PROC_NAME "sock_containers"
+#define NST_PROC_NAME "send_tracking"
+
+void o2net_proc_exit(struct proc_dir_entry *parent)
+{
+	remove_proc_entry(SC_PROC_NAME, parent);
+	remove_proc_entry(NST_PROC_NAME, parent);
+}
+EXPORT_SYMBOL_GPL(o2net_proc_exit);
+
+int o2net_proc_init(struct proc_dir_entry *parent)
+{
+	struct proc_dir_entry *proc_sc = NULL, *proc_nst = NULL;
+	int ret;
+
+	proc_sc = create_proc_entry(SC_PROC_NAME, S_IRUGO, parent);
+	proc_nst = create_proc_entry(NST_PROC_NAME, S_IRUGO, parent);
+
+	if (proc_sc && proc_nst) {
+		ret = 0;
+		proc_sc->proc_fops = &sc_seq_fops;
+		proc_nst->proc_fops = &nst_seq_fops;
+	} else {
+		ret = -ENOMEM;
+		if (proc_sc)
+			remove_proc_entry(SC_PROC_NAME, parent);
+		if (proc_nst)
+			remove_proc_entry(NST_PROC_NAME, parent);
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(o2net_proc_init);
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/cluster/nodemanager.c linux-2.6.15-ocfs2/fs/ocfs2/cluster/nodemanager.c
--- linux-2.6.15-2/fs/ocfs2/cluster/nodemanager.c	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/cluster/nodemanager.c	2006-02-06 18:00:00.000000000 -0500
@@ -22,6 +22,7 @@
 #include <linux/kernel.h>
 #include <linux/module.h>
 #include <linux/sysctl.h>
+#include <linux/proc_fs.h>
 #include <linux/configfs.h>
 
 #include "endian.h"
@@ -29,7 +30,6 @@
 #include "nodemanager.h"
 #include "heartbeat.h"
 #include "masklog.h"
-#include "sys.h"
 #include "ver.h"
 
 /* for now we operate under the assertion that there can be only one
@@ -653,7 +653,7 @@ static struct config_group *o2nm_cluster
 	struct config_group *o2hb_group = NULL, *ret = NULL;
 	void *defs = NULL;
 
-	/* this runs under the parent dir's i_mutex; there can be only
+	/* this runs under the parent dir's i_sem; there can be only
 	 * one caller in here at a time */
 	if (o2nm_single_cluster)
 		goto out; /* ENOSPC */
@@ -730,6 +730,18 @@ static struct o2nm_cluster_group o2nm_cl
 	},
 };
 
+#define O2NM_PROC_PATH "fs/ocfs2_nodemanager"
+static struct proc_dir_entry *o2nm_proc;
+
+#define O2NM_VERSION_PROC_NAME "interface_revision"
+#define O2NM_HB_DEAD_THRESHOLD_NAME "hb_dead_threshold"
+
+static void o2nm_remove_proc(struct proc_dir_entry *parent)
+{
+	remove_proc_entry(O2NM_VERSION_PROC_NAME, parent);
+	remove_proc_entry(O2NM_HB_DEAD_THRESHOLD_NAME, parent);
+}
+
 static void __exit exit_o2nm(void)
 {
 	if (ocfs2_table_header)
@@ -737,12 +749,105 @@ static void __exit exit_o2nm(void)
 
 	/* XXX sync with hb callbacks and shut down hb? */
 	o2net_unregister_hb_callbacks();
-	configfs_unregister_subsystem(&o2nm_cluster_group.cs_subsys);
-	o2cb_sys_shutdown();
+	configfs_unregister_subsystem(&o2nm_cluster_group.cs_subsys);
+	o2nm_remove_proc(o2nm_proc);
+	mlog_remove_proc(o2nm_proc);
+	o2net_proc_exit(o2nm_proc);
+	remove_proc_entry(O2NM_PROC_PATH, NULL);
 
 	o2net_exit();
 }
 
+static int o2nm_proc_read_uint(char *page, char **start, off_t off,
+			       int count, int *eof, unsigned int data)
+{
+	int len;
+
+	len = sprintf(page, "%u\n", data);
+	if (len < 0)
+		return len;
+
+	if (len <= off + count)
+		*eof = 1;
+
+	*start = page + off;
+
+	len -= off;
+
+	if (len > count)
+		len = count;
+
+	if (len < 0)
+		len = 0;
+
+	return len;
+}
+
+static int o2nm_proc_version(char *page, char **start, off_t off,
+			     int count, int *eof, void *data)
+{
+	return o2nm_proc_read_uint(page, start, off, count, eof,
+				   O2NM_API_VERSION);
+}
+
+static int o2nm_proc_threshold(char *page, char **start, off_t off,
+			       int count, int *eof, void *data)
+{
+	return o2nm_proc_read_uint(page, start, off, count, eof,
+				   o2hb_dead_threshold);
+}
+
+static int o2nm_proc_write_threshold(struct file *file,
+				     const char __user *buffer,
+				     unsigned long count, void *data)
+{
+	char buf[32];
+	char *p = buf;
+	unsigned long tmp;
+
+	if (count > ARRAY_SIZE(buf) - 1)
+		count = ARRAY_SIZE(buf) - 1;
+
+	if (copy_from_user(buf, buffer, count))
+		return -EFAULT;
+
+	buf[ARRAY_SIZE(buf) - 1] = '\0';
+
+	tmp = simple_strtoul(p, &p, 10);
+	if (!p || (*p && (*p != '\n')))
+                return -EINVAL;
+
+	/* this will validate ranges for us. */
+	o2hb_dead_threshold_set((unsigned int) tmp);
+
+	return count;
+}
+
+static int o2nm_init_proc(struct proc_dir_entry *parent)
+{
+	struct proc_dir_entry *p;
+
+	p = create_proc_read_entry(O2NM_VERSION_PROC_NAME,
+				   S_IFREG | S_IRUGO,
+				   parent,
+				   o2nm_proc_version,
+				   NULL);
+	if (!p)
+		return -ENOMEM;
+
+	p = create_proc_read_entry(O2NM_HB_DEAD_THRESHOLD_NAME,
+				   S_IFREG | S_IRUGO | S_IWUSR, parent,
+				   o2nm_proc_threshold,
+				   NULL);
+	if (!p) {
+		remove_proc_entry(O2NM_VERSION_PROC_NAME, parent);
+		return -ENOMEM;
+	}
+	p->write_proc = o2nm_proc_write_threshold;
+
+	return 0;
+}
+
 static int __init init_o2nm(void)
 {
 	int ret = -1;
@@ -771,10 +877,29 @@ static int __init init_o2nm(void)
 		goto out_callbacks;
 	}
 
-	ret = o2cb_sys_init();
-	if (!ret)
+	o2nm_proc = proc_mkdir(O2NM_PROC_PATH, NULL);
+	if (o2nm_proc == NULL) {
+		ret = -ENOMEM; /* shrug */
+		goto out_subsys;
+	}
+
+	ret = mlog_init_proc(o2nm_proc);
+	if (ret)
+		goto out_remove;
+
+	ret = o2net_proc_init(o2nm_proc);
+	if (ret)
+		goto out_mlog;
+
+	ret = o2nm_init_proc(o2nm_proc);
+	if (ret == 0)
 		goto out;
 
+out_mlog:
+	mlog_remove_proc(o2nm_proc);
+out_remove:
+	remove_proc_entry(O2NM_PROC_PATH, NULL);
+out_subsys:
 	configfs_unregister_subsystem(&o2nm_cluster_group.cs_subsys);
 out_callbacks:
 	o2net_unregister_hb_callbacks();
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/cluster/sys.c linux-2.6.15-ocfs2/fs/ocfs2/cluster/sys.c
--- linux-2.6.15-2/fs/ocfs2/cluster/sys.c	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/cluster/sys.c	1969-12-31 19:00:00.000000000 -0500
@@ -1,124 +0,0 @@
-/* -*- mode: c; c-basic-offset: 8; -*-
- * vim: noexpandtab sw=8 ts=8 sts=0:
- *
- * sys.c
- *
- * OCFS2 cluster sysfs interface
- *
- * Copyright (C) 2005 Oracle.  All rights reserved.
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public
- * License as published by the Free Software Foundation,
- * version 2 of the License.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * You should have received a copy of the GNU General Public
- * License along with this program; if not, write to the
- * Free Software Foundation, Inc., 59 Temple Place - Suite 330,
- * Boston, MA 021110-1307, USA.
- *
- */
-
-#include <linux/kernel.h>
-#include <linux/module.h>
-#include <linux/kobject.h>
-#include <linux/sysfs.h>
-
-#include "ocfs2_nodemanager.h"
-#include "masklog.h"
-#include "sys.h"
-
-struct o2cb_attribute {
-	struct attribute	attr;
-	ssize_t (*show)(char *buf);
-	ssize_t (*store)(const char *buf, size_t count);
-};
-
-#define O2CB_ATTR(_name, _mode, _show, _store)	\
-struct o2cb_attribute o2cb_attr_##_name = __ATTR(_name, _mode, _show, _store)
-
-#define to_o2cb_subsys(k) container_of(to_kset(k), struct subsystem, kset)
-#define to_o2cb_attr(_attr) container_of(_attr, struct o2cb_attribute, attr)
-
-static ssize_t o2cb_interface_revision_show(char *buf)
-{
-	return snprintf(buf, PAGE_SIZE, "%u\n", O2NM_API_VERSION);
-}
-
-static O2CB_ATTR(interface_revision, S_IFREG | S_IRUGO, o2cb_interface_revision_show, NULL);
-
-static struct attribute *o2cb_attrs[] = {
-	&o2cb_attr_interface_revision.attr,
-	NULL,
-};
-
-static ssize_t
-o2cb_show(struct kobject * kobj, struct attribute * attr, char * buffer);
-static ssize_t
-o2cb_store(struct kobject * kobj, struct attribute * attr,
-	   const char * buffer, size_t count);
-static struct sysfs_ops o2cb_sysfs_ops = {
-	.show	= o2cb_show,
-	.store	= o2cb_store,
-};
-
-static struct kobj_type o2cb_subsys_type = {
-	.default_attrs	= o2cb_attrs,
-	.sysfs_ops	= &o2cb_sysfs_ops,
-};
-
-/* gives us o2cb_subsys */
-static decl_subsys(o2cb, NULL, NULL);
-
-static ssize_t
-o2cb_show(struct kobject * kobj, struct attribute * attr, char * buffer)
-{
-	struct o2cb_attribute *o2cb_attr = to_o2cb_attr(attr);
-	struct subsystem *sbs = to_o2cb_subsys(kobj);
-
-	BUG_ON(sbs != &o2cb_subsys);
-
-	if (o2cb_attr->show)
-		return o2cb_attr->show(buffer);
-	return -EIO;
-}
-
-static ssize_t
-o2cb_store(struct kobject * kobj, struct attribute * attr,
-	     const char * buffer, size_t count)
-{
-	struct o2cb_attribute *o2cb_attr = to_o2cb_attr(attr);
-	struct subsystem *sbs = to_o2cb_subsys(kobj);
-
-	BUG_ON(sbs != &o2cb_subsys);
-
-	if (o2cb_attr->store)
-		return o2cb_attr->store(buffer, count);
-	return -EIO;
-}
-
-void o2cb_sys_shutdown(void)
-{
-	mlog_sys_shutdown();
-	subsystem_unregister(&o2cb_subsys);
-}
-
-int o2cb_sys_init(void)
-{
-	int ret;
-
-	o2cb_subsys.kset.kobj.ktype = &o2cb_subsys_type;
-	ret = subsystem_register(&o2cb_subsys);
-	if (ret)
-		return ret;
-
-	ret = mlog_sys_init(&o2cb_subsys);
-	if (ret)
-		subsystem_unregister(&o2cb_subsys);
-	return ret;
-}
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/cluster/sys.h linux-2.6.15-ocfs2/fs/ocfs2/cluster/sys.h
--- linux-2.6.15-2/fs/ocfs2/cluster/sys.h	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/cluster/sys.h	1969-12-31 19:00:00.000000000 -0500
@@ -1,33 +0,0 @@
-/* -*- mode: c; c-basic-offset: 8; -*-
- * vim: noexpandtab sw=8 ts=8 sts=0:
- *
- * sys.h
- *
- * Function prototypes for o2cb sysfs interface
- *
- * Copyright (C) 2005 Oracle.  All rights reserved.
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public
- * License as published by the Free Software Foundation,
- * version 2 of the License.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
- * General Public License for more details.
- *
- * You should have received a copy of the GNU General Public
- * License along with this program; if not, write to the
- * Free Software Foundation, Inc., 59 Temple Place - Suite 330,
- * Boston, MA 021110-1307, USA.
- *
- */
-
-#ifndef O2CLUSTER_SYS_H
-#define O2CLUSTER_SYS_H
-
-void o2cb_sys_shutdown(void);
-int o2cb_sys_init(void);
-
-#endif /* O2CLUSTER_SYS_H */
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/cluster/tcp.c linux-2.6.15-ocfs2/fs/ocfs2/cluster/tcp.c
--- linux-2.6.15-2/fs/ocfs2/cluster/tcp.c	2006-02-06 18:15:15.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/cluster/tcp.c	2006-02-06 17:59:23.000000000 -0500
@@ -183,7 +183,16 @@ static int o2net_prep_nsw(struct o2net_n
 			break;
 		}
 		spin_lock(&nn->nn_lock);
+#ifndef IDR_GET_NEW_RETURNS_ID
 		ret = idr_get_new(&nn->nn_status_idr, nsw, &nsw->ns_id);
+#else
+		/* old semantics */
+		nsw->ns_id = idr_get_new(&nn->nn_status_idr, nsw);
+		if (nsw->ns_id < 0)
+			ret = -EAGAIN;
+		else
+			ret = 0;
+#endif
 		if (ret == 0)
 			list_add_tail(&nsw->ns_node_item,
 				      &nn->nn_status_list);
@@ -281,6 +290,7 @@ static void sc_kref_release(struct kref 
 	o2nm_node_put(sc->sc_node);
 	sc->sc_node = NULL;
 
+	o2net_proc_del_sc(sc);
 	kfree(sc);
 }
 
@@ -321,6 +331,7 @@ static struct o2net_sock_container *sc_a
 
 	ret = sc;
 	sc->sc_page = page;
+	o2net_proc_add_sc(sc);
 	sc = NULL;
 	page = NULL;
 
@@ -875,6 +886,13 @@ int o2net_send_message_vec(u32 msg_type,
 	struct o2net_status_wait nsw = {
 		.ns_node_item = LIST_HEAD_INIT(nsw.ns_node_item),
 	};
+	struct o2net_send_tracking nst = {
+		.st_net_proc_item = LIST_HEAD_INIT(nst.st_net_proc_item),
+		.st_task = current,
+		.st_msg_type = msg_type,
+		.st_msg_key = key,
+		.st_node = target_node,
+	};
 
 	if (o2net_wq == NULL) {
 		mlog(0, "attempt to tx without o2netd running\n");
@@ -900,6 +918,9 @@ int o2net_send_message_vec(u32 msg_type,
 		goto out;
 	}
 
+	o2net_proc_add_nst(&nst);
+
+	do_gettimeofday(&nst.st_sock_time);
 	ret = wait_event_interruptible(nn->nn_sc_wq,
 				       o2net_tx_can_proceed(nn, &sc, &error));
 	if (!ret && error)
@@ -907,6 +928,8 @@ int o2net_send_message_vec(u32 msg_type,
 	if (ret)
 		goto out;
 
+	nst.st_sc = sc;
+
 	veclen = caller_veclen + 1;
 	vec = kmalloc(sizeof(struct kvec) * veclen, GFP_ATOMIC);
 	if (vec == NULL) {
@@ -934,6 +957,7 @@ int o2net_send_message_vec(u32 msg_type,
 
 	msg->msg_num = cpu_to_be32(nsw.ns_id);
 
+	do_gettimeofday(&nst.st_send_time);
 	/* finally, convert the message header to network byte-order
 	 * and send */
 	ret = o2net_send_tcp_msg(sc->sc_sock, vec, veclen,
@@ -945,6 +969,7 @@ int o2net_send_message_vec(u32 msg_type,
 	}
 
 	/* wait on other node's handler */
+	do_gettimeofday(&nst.st_status_time);
 	wait_event(nsw.ns_wq, o2net_nsw_completed(nn, &nsw));
 
 	/* Note that we avoid overwriting the callers status return
@@ -957,6 +982,7 @@ int o2net_send_message_vec(u32 msg_type,
 	mlog(0, "woken, returning system status %d, user status %d\n",
 	     ret, nsw.ns_status);
 out:
+	o2net_proc_del_nst(&nst); /* must be before dropping sc and node */
 	if (sc)
 		sc_put(sc);
 	if (vec)
@@ -1285,16 +1311,14 @@ static void o2net_idle_timer(unsigned lo
 	mlog(ML_NOTICE, "here are some times that might help debug the "
 	     "situation: (tmr %ld.%ld now %ld.%ld dr %ld.%ld adv "
 	     "%ld.%ld:%ld.%ld func (%08x:%u) %ld.%ld:%ld.%ld)\n",
-	     sc->sc_tv_timer.tv_sec, (long) sc->sc_tv_timer.tv_usec, 
-	     now.tv_sec, (long) now.tv_usec,
-	     sc->sc_tv_data_ready.tv_sec, (long) sc->sc_tv_data_ready.tv_usec,
-	     sc->sc_tv_advance_start.tv_sec,
-	     (long) sc->sc_tv_advance_start.tv_usec,
-	     sc->sc_tv_advance_stop.tv_sec,
-	     (long) sc->sc_tv_advance_stop.tv_usec,
+	     sc->sc_tv_timer.tv_sec, sc->sc_tv_timer.tv_usec, 
+	     now.tv_sec, now.tv_usec,
+	     sc->sc_tv_data_ready.tv_sec, sc->sc_tv_data_ready.tv_usec, 
+	     sc->sc_tv_advance_start.tv_sec, sc->sc_tv_advance_start.tv_usec, 
+	     sc->sc_tv_advance_stop.tv_sec, sc->sc_tv_advance_stop.tv_usec, 
 	     sc->sc_msg_key, sc->sc_msg_type,
-	     sc->sc_tv_func_start.tv_sec, (long) sc->sc_tv_func_start.tv_usec,
-	     sc->sc_tv_func_stop.tv_sec, (long) sc->sc_tv_func_stop.tv_usec);
+	     sc->sc_tv_func_start.tv_sec, sc->sc_tv_func_start.tv_usec,
+	     sc->sc_tv_func_stop.tv_sec, sc->sc_tv_func_stop.tv_usec);
 
 	o2net_sc_queue_work(sc, &sc->sc_shutdown_work);
 }
@@ -1526,6 +1550,22 @@ int o2net_register_hb_callbacks(void)
 
 /* ------------------------------------------------------------ */
 
+#ifdef MISSING_SOCK_CREATE_LITE
+static inline int sock_create_lite(int family, int type, int protocol,
+				   struct socket **res)
+{
+	struct socket *sock = sock_alloc();
+	int ret = 0;
+
+	if (sock == NULL)
+		ret = -ENOMEM;
+
+	*res = sock;
+
+	return ret;
+}
+#endif /* MISSING_SOCK_CREATE_LITE */
+
 static int o2net_accept_one(struct socket *sock)
 {
 	int ret, slen;
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/cluster/tcp.h linux-2.6.15-ocfs2/fs/ocfs2/cluster/tcp.h
--- linux-2.6.15-2/fs/ocfs2/cluster/tcp.h	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/cluster/tcp.h	2006-02-06 17:59:23.000000000 -0500
@@ -110,4 +110,8 @@ void o2net_exit(void);
 int o2net_proc_init(struct proc_dir_entry *parent);
 void o2net_proc_exit(struct proc_dir_entry *parent);
 
+struct o2net_send_tracking;
+void o2net_proc_add_nst(struct o2net_send_tracking *nst);
+void o2net_proc_del_nst(struct o2net_send_tracking *nst);
+
 #endif /* O2CLUSTER_TCP_H */
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/cluster/tcp_internal.h linux-2.6.15-ocfs2/fs/ocfs2/cluster/tcp_internal.h
--- linux-2.6.15-2/fs/ocfs2/cluster/tcp_internal.h	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/cluster/tcp_internal.h	2006-02-06 17:59:23.000000000 -0500
@@ -22,6 +22,8 @@
 #ifndef O2CLUSTER_TCP_INTERNAL_H
 #define O2CLUSTER_TCP_INTERNAL_H
 
+#define O2NET_MAX_CONNECT_ATTEMPTS	5
+
 #define O2NET_MSG_MAGIC           ((u16)0xfa55)
 #define O2NET_MSG_STATUS_MAGIC    ((u16)0xfa56)
 #define O2NET_MSG_KEEP_REQ_MAGIC  ((u16)0xfa57)
@@ -38,17 +40,7 @@
 #define O2NET_KEEPALIVE_DELAY_SECS	5
 #define O2NET_IDLE_TIMEOUT_SECS		10
 
-/* 
- * This version number represents quite a lot, unfortunately.  It not
- * only represents the raw network message protocol on the wire but also
- * locking semantics of the file system using the protocol.  It should 
- * be somewhere else, I'm sure, but right now it isn't.
- *
- * New in version 2:
- * 	- full 64 bit i_size in the metadata lock lvbs
- * 	- introduction of "rw" lock and pushing meta/data locking down
- */
-#define O2NET_PROTOCOL_VERSION 2ULL
+#define O2NET_PROTOCOL_VERSION 1ULL
 struct o2net_handshake {
 	__be64	protocol_version;
 	__be64	connector_id;
@@ -134,6 +126,8 @@ struct o2net_sock_container {
 	void			(*sc_state_change)(struct sock *sk);
 	void			(*sc_data_ready)(struct sock *sk, int bytes);
 
+	struct list_head	sc_net_proc_item;
+
 	struct timeval 		sc_tv_timer;
 	struct timeval 		sc_tv_data_ready;
 	struct timeval 		sc_tv_advance_start;
@@ -171,4 +165,20 @@ struct o2net_status_wait {
 	struct list_head	ns_node_item;
 };
 
+/* just for state dumps */
+struct o2net_send_tracking {
+	struct list_head		st_net_proc_item;
+	struct task_struct		*st_task;
+	struct o2net_sock_container	*st_sc;
+	u32				st_msg_type;
+	u32				st_msg_key;
+	u8				st_node;
+	struct timeval			st_sock_time;
+	struct timeval			st_send_time;
+	struct timeval			st_status_time;
+};
+
+void o2net_proc_add_sc(struct o2net_sock_container *sc);
+void o2net_proc_del_sc(struct o2net_sock_container *sc);
+
 #endif /* O2CLUSTER_TCP_INTERNAL_H */
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/cluster/ver.c linux-2.6.15-ocfs2/fs/ocfs2/cluster/ver.c
--- linux-2.6.15-2/fs/ocfs2/cluster/ver.c	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/cluster/ver.c	2006-02-06 17:59:23.000000000 -0500
@@ -28,9 +28,13 @@
 
 #include "ver.h"
 
-#define CLUSTER_BUILD_VERSION "1.3.3"
-
-#define VERSION_STR "OCFS2 Node Manager " CLUSTER_BUILD_VERSION
+#define CLUSTER_BUILD_VERSION	"1.2.0-SLES"
+#define CLUSTER_BUILD_DATE	"Tue Jan 24 14:31:42 PST 2006"
+#define CLUSTER_BUILD_MD5	"sles"
+
+#define VERSION_STR "OCFS2 Node Manager " \
+	CLUSTER_BUILD_VERSION " " CLUSTER_BUILD_DATE \
+	" (build " CLUSTER_BUILD_MD5 ")"
 
 void cluster_print_version(void)
 {
@@ -39,4 +43,6 @@ void cluster_print_version(void)
 
 MODULE_DESCRIPTION(VERSION_STR);
 
+#ifdef MODULE_VERSION
 MODULE_VERSION(CLUSTER_BUILD_VERSION);
+#endif
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/dir.c linux-2.6.15-ocfs2/fs/ocfs2/dir.c
--- linux-2.6.15-2/fs/ocfs2/dir.c	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/dir.c	2006-02-06 17:59:23.000000000 -0500
@@ -356,9 +356,9 @@ int ocfs2_do_extend_dir(struct super_blo
 	spin_unlock(&OCFS2_I(dir)->ip_lock);
 
 	if (extend) {
-		status = ocfs2_do_extend_allocation(OCFS2_SB(sb), dir, 1,
-						    parent_fe_bh, handle,
-						    data_ac, meta_ac, NULL);
+		status = ocfs2_extend_allocation(OCFS2_SB(sb), dir, 1,
+						 parent_fe_bh, handle,
+						 data_ac, meta_ac, NULL);
 		BUG_ON(status == -EAGAIN);
 		if (status < 0) {
 			mlog_errno(status);
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/dlm/dlmast.c linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmast.c
--- linux-2.6.15-2/fs/ocfs2/dlm/dlmast.c	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmast.c	2006-02-06 17:59:23.000000000 -0500
@@ -91,7 +91,7 @@ static int dlm_should_cancel_bast(struct
 	return 0;
 }
 
-static void __dlm_queue_ast(struct dlm_ctxt *dlm, struct dlm_lock *lock)
+void __dlm_queue_ast(struct dlm_ctxt *dlm, struct dlm_lock *lock)
 {
 	mlog_entry_void();
 
@@ -149,7 +149,7 @@ void dlm_queue_ast(struct dlm_ctxt *dlm,
 }
 
 
-static void __dlm_queue_bast(struct dlm_ctxt *dlm, struct dlm_lock *lock)
+void __dlm_queue_bast(struct dlm_ctxt *dlm, struct dlm_lock *lock)
 {
 	mlog_entry_void();
 
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/dlm/dlmcommon.h linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmcommon.h
--- linux-2.6.15-2/fs/ocfs2/dlm/dlmcommon.h	2006-02-06 18:15:15.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmcommon.h	2006-02-06 17:59:23.000000000 -0500
@@ -652,14 +656,19 @@ void dlm_commit_pending_cancel(struct dl
 void dlm_commit_pending_unlock(struct dlm_lock_resource *res,
 			       struct dlm_lock *lock);
 
+void dlm_shuffle_lists(struct dlm_ctxt *dlm,
+		       struct dlm_lock_resource *res);
 int dlm_launch_thread(struct dlm_ctxt *dlm);
 void dlm_complete_thread(struct dlm_ctxt *dlm);
+void dlm_flush_asts(struct dlm_ctxt *dlm);
+int dlm_flush_lockres_asts(struct dlm_ctxt *dlm, struct dlm_lock_resource *res);
 int dlm_launch_recovery_thread(struct dlm_ctxt *dlm);
 void dlm_complete_recovery_thread(struct dlm_ctxt *dlm);
 void dlm_wait_for_recovery(struct dlm_ctxt *dlm);
 int dlm_is_node_dead(struct dlm_ctxt *dlm, u8 node);
 int dlm_wait_for_node_death(struct dlm_ctxt *dlm, u8 node, int timeout);
 
+void dlm_get(struct dlm_ctxt *dlm);
 void dlm_put(struct dlm_ctxt *dlm);
 struct dlm_ctxt *dlm_grab(struct dlm_ctxt *dlm);
 int dlm_domain_fully_joined(struct dlm_ctxt *dlm);
@@ -692,7 +702,9 @@ struct dlm_lock_resource *dlm_new_lockre
 					  const char *name,
 					  unsigned int namelen);
 
+void __dlm_queue_ast(struct dlm_ctxt *dlm, struct dlm_lock *lock);
 void dlm_queue_ast(struct dlm_ctxt *dlm, struct dlm_lock *lock);
+void __dlm_queue_bast(struct dlm_ctxt *dlm, struct dlm_lock *lock);
 void dlm_queue_bast(struct dlm_ctxt *dlm, struct dlm_lock *lock);
 void dlm_do_local_ast(struct dlm_ctxt *dlm,
 		      struct dlm_lock_resource *res,
@@ -732,13 +744,17 @@ void __dlm_print_one_lock_resource(struc
 
 u8 dlm_nm_this_node(struct dlm_ctxt *dlm);
 void dlm_kick_thread(struct dlm_ctxt *dlm, struct dlm_lock_resource *res);
+void __dlm_kick_thread(struct dlm_ctxt *dlm, struct dlm_lock_resource *res);
 void __dlm_dirty_lockres(struct dlm_ctxt *dlm, struct dlm_lock_resource *res);
 
 
 int dlm_nm_init(struct dlm_ctxt *dlm);
 int dlm_heartbeat_init(struct dlm_ctxt *dlm);
+void __dlm_hb_node_down(struct dlm_ctxt *dlm, int idx);
 void dlm_hb_node_down_cb(struct o2nm_node *node, int idx, void *data);
 void dlm_hb_node_up_cb(struct o2nm_node *node, int idx, void *data);
+int dlm_hb_node_dead(struct dlm_ctxt *dlm, int node);
+int __dlm_hb_node_dead(struct dlm_ctxt *dlm, int node);
 
 int dlm_lockres_is_dirty(struct dlm_ctxt *dlm, struct dlm_lock_resource *res);
 int dlm_migrate_lockres(struct dlm_ctxt *dlm,
@@ -766,6 +782,7 @@ int dlm_dispatch_assert_master(struct dl
 			       int ignore_higher,
 			       u8 request_from,
 			       u32 flags);
+void dlm_assert_master_worker(struct dlm_work_item *item, void *data);
 
 
 int dlm_send_one_lockres(struct dlm_ctxt *dlm,
@@ -776,6 +793,11 @@ int dlm_send_one_lockres(struct dlm_ctxt
 void dlm_move_lockres_to_recovery_list(struct dlm_ctxt *dlm,
 				       struct dlm_lock_resource *res);
 
+void dlm_init_lockres(struct dlm_ctxt *dlm,
+		      struct dlm_lock_resource *res,
+		      const char *name,
+		      unsigned int namelen);
+
 /* will exit holding res->spinlock, but may drop in function */
 void __dlm_wait_on_lockres_flags(struct dlm_lock_resource *res, int flags);
 void __dlm_wait_on_lockres_flags_set(struct dlm_lock_resource *res, int flags);
@@ -792,11 +814,24 @@ static inline void __dlm_wait_on_lockres
 int dlm_init_mle_cache(void);
 void dlm_destroy_mle_cache(void);
 void dlm_hb_event_notify_attached(struct dlm_ctxt *dlm, int idx, int node_up);
+int dlm_do_assert_master(struct dlm_ctxt *dlm,
+			 const char *lockname,
+			 unsigned int namelen,
+			 void *nodemap,
+			 u32 flags);
+int dlm_do_migrate_request(struct dlm_ctxt *dlm,
+			   struct dlm_lock_resource *res,
+			   u8 master,
+			   u8 new_master,
+			   struct dlm_node_iter *iter);
 void dlm_clean_master_list(struct dlm_ctxt *dlm,
 			   u8 dead_node);
 int dlm_lock_basts_flushed(struct dlm_ctxt *dlm, struct dlm_lock *lock);
 
 
+int dlm_dump_all_mles(const char __user *data, unsigned int len);
+
+
 static inline const char * dlm_lock_mode_name(int mode)
 {
 	switch (mode) {
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/dlm/dlmdebug.c linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmdebug.c
--- linux-2.6.15-2/fs/ocfs2/dlm/dlmdebug.c	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmdebug.c	2006-02-06 17:38:14.000000000 -0500
@@ -30,6 +30,7 @@
 #include <linux/utsname.h>
 #include <linux/sysctl.h>
 #include <linux/spinlock.h>
+#include <linux/proc_fs.h>
 
 #include "cluster/heartbeat.h"
 #include "cluster/nodemanager.h"
@@ -45,6 +46,176 @@
 #define MLOG_MASK_PREFIX ML_DLM
 #include "cluster/masklog.h"
 
+static int dlm_dump_all_lock_resources(const char __user *data,
+					unsigned int len);
+static void dlm_dump_purge_list(struct dlm_ctxt *dlm);
+static int dlm_dump_all_purge_lists(const char __user *data, unsigned int len);
+static int dlm_trigger_migration(const char __user *data, unsigned int len);
+static int dlm_dump_one_lock_resource(const char __user *data,
+				       unsigned int len);
+
+static int dlm_parse_domain_and_lockres(char *buf, unsigned int len,
+					struct dlm_ctxt **dlm,
+					struct dlm_lock_resource **res);
+
+typedef int (dlm_debug_func_t)(const char __user *data, unsigned int len);
+
+struct dlm_debug_funcs
+{
+	char key;
+	dlm_debug_func_t *func;
+};
+
+static struct dlm_debug_funcs dlm_debug_map[] = {
+	{ 'r', dlm_dump_all_lock_resources },
+	{ 'R', dlm_dump_one_lock_resource },
+	{ 'm', dlm_dump_all_mles },
+	{ 'p', dlm_dump_all_purge_lists  },
+	{ 'M', dlm_trigger_migration },
+};
+static int dlm_debug_map_sz = (sizeof(dlm_debug_map) /
+			       sizeof(struct dlm_debug_funcs));
+
+static ssize_t write_dlm_debug(struct file *file, const char __user *buf,
+			       size_t count, loff_t *ppos)
+{
+	int i;
+	char c;
+	dlm_debug_func_t *fn;
+	int ret;
+
+	mlog(0, "(%p, %p, %u, %lld)\n",
+		  file, buf, (unsigned int)count, (long long)*ppos);
+	ret = 0;
+	if (count<=0)
+		goto done;
+
+	ret = -EFAULT;
+	if (get_user(c, buf))
+		goto done;
+
+	ret = count;
+	for (i=0; i < dlm_debug_map_sz; i++) {
+		struct dlm_debug_funcs *d = &dlm_debug_map[i];
+		if (c == d->key) {
+			fn = d->func;
+			if (fn)
+				ret = (fn)(buf, count);
+			goto done;
+		}
+	}
+done:
+	return ret;
+}
+
+static struct file_operations dlm_debug_operations = {
+	.write          = write_dlm_debug,
+};
+
+#define OCFS2_DLM_PROC_PATH "fs/ocfs2_dlm"
+#define DLM_DEBUG_PROC_NAME "debug"
+static struct proc_dir_entry *ocfs2_dlm_proc;
+
+void dlm_remove_proc(void)
+{
+	if (ocfs2_dlm_proc) {
+		remove_proc_entry(DLM_DEBUG_PROC_NAME, ocfs2_dlm_proc);
+		remove_proc_entry(OCFS2_DLM_PROC_PATH, NULL);
+	}
+}
+
+void dlm_init_proc(void)
+{
+	struct proc_dir_entry *entry;
+
+	ocfs2_dlm_proc = proc_mkdir(OCFS2_DLM_PROC_PATH, NULL);
+	if (!ocfs2_dlm_proc) {
+		mlog_errno(-ENOMEM);
+		return;
+	}
+
+	entry = create_proc_entry(DLM_DEBUG_PROC_NAME, S_IWUSR,
+				  ocfs2_dlm_proc);
+	if (entry)
+		entry->proc_fops = &dlm_debug_operations;
+}
+
+/* lock resource printing is usually very important (printed
+ * right before a BUG in some cases), but we'd like to be
+ * able to shut it off if needed, hence the KERN_NOTICE level */
+static int dlm_dump_all_lock_resources(const char __user *data,
+				       unsigned int len)
+{
+	struct dlm_ctxt *dlm;
+	struct list_head *iter;
+
+	mlog(ML_NOTICE, "dumping ALL dlm state for node %s\n",
+		  system_utsname.nodename);
+	spin_lock(&dlm_domain_lock);
+	list_for_each(iter, &dlm_domains) {
+		dlm = list_entry (iter, struct dlm_ctxt, list);
+		dlm_dump_lock_resources(dlm);
+	}
+	spin_unlock(&dlm_domain_lock);
+	return len;
+}
+
+static int dlm_dump_one_lock_resource(const char __user *data,
+				       unsigned int len)
+{
+	struct dlm_ctxt *dlm;
+	struct dlm_lock_resource *res;
+	char *buf = NULL;
+	int ret = -EINVAL;
+	int tmpret;
+
+	if (len >= PAGE_SIZE-1) {
+		mlog(ML_ERROR, "user passed too much data: %d bytes\n", len);
+		goto leave;
+	}
+	if (len < 5) {
+		mlog(ML_ERROR, "user passed too little data: %d bytes\n", len);
+		goto leave;
+	}
+	buf = kmalloc(len+1, GFP_KERNEL);
+	if (!buf) {
+		mlog(ML_ERROR, "could not alloc %d bytes\n", len+1);
+		ret = -ENOMEM;
+		goto leave;
+	}
+	if (strncpy_from_user(buf, data, len) < len) {
+		mlog(ML_ERROR, "failed to get all user data.  done.\n");
+		goto leave;
+	}
+	buf[len]='\0';
+	mlog(0, "got this data from user: %s\n", buf);
+
+	if (*buf != 'R') {
+		mlog(0, "bad data\n");
+		goto leave;
+	}
+
+	tmpret = dlm_parse_domain_and_lockres(buf, len, &dlm, &res);
+	if (tmpret < 0) {
+		mlog(0, "bad data\n");
+		goto leave;
+	}
+
+	mlog(ML_NOTICE, "struct dlm_ctxt: %s, node=%u, key=%u\n",
+		dlm->name, dlm->node_num, dlm->key);
+
+	dlm_print_one_lock_resource(res);
+	dlm_lockres_put(res);
+	dlm_put(dlm);
+	ret = len;
+
+leave:
+	if (buf)
+		kfree(buf);
+	return ret;
+}
+
+
 void dlm_print_one_lock_resource(struct dlm_lock_resource *res)
 {
 	mlog(ML_NOTICE, "lockres: %.*s, owner=%u, state=%u\n",
@@ -108,6 +279,7 @@ void __dlm_print_one_lock_resource(struc
 	}
 }
 
+
 void dlm_print_one_lock(struct dlm_lock *lockid)
 {
 	dlm_print_one_lock_resource(lockid->lockres);
@@ -139,6 +311,165 @@ void dlm_dump_lock_resources(struct dlm_
 	spin_unlock(&dlm->spinlock);
 }
 
+static void dlm_dump_purge_list(struct dlm_ctxt *dlm)
+{
+	struct list_head *iter;
+	struct dlm_lock_resource *lockres;
+
+	mlog(ML_NOTICE, "Purge list for DLM Domain \"%s\"\n", dlm->name);
+	mlog(ML_NOTICE, "Last_used\tName\n");
+
+	spin_lock(&dlm->spinlock);
+	list_for_each(iter, &dlm->purge_list) {
+		lockres = list_entry(iter, struct dlm_lock_resource, purge);
+
+		spin_lock(&lockres->spinlock);
+		mlog(ML_NOTICE, "%lu\t%.*s\n", lockres->last_used,
+		       lockres->lockname.len, lockres->lockname.name);
+		spin_unlock(&lockres->spinlock);
+	}
+	spin_unlock(&dlm->spinlock);
+}
+
+static int dlm_dump_all_purge_lists(const char __user *data, unsigned int len)
+{
+	struct dlm_ctxt *dlm;
+	struct list_head *iter;
+
+	spin_lock(&dlm_domain_lock);
+	list_for_each(iter, &dlm_domains) {
+		dlm = list_entry (iter, struct dlm_ctxt, list);
+		dlm_dump_purge_list(dlm);
+	}
+	spin_unlock(&dlm_domain_lock);
+	return len;
+}
+
+static int dlm_parse_domain_and_lockres(char *buf, unsigned int len,
+					struct dlm_ctxt **dlm,
+					struct dlm_lock_resource **res)
+{
+	char *resname;
+	char *domainname;
+	char *tmp;
+	int ret = -EINVAL;
+
+	*dlm = NULL;
+	*res = NULL;
+
+	tmp = buf;
+	tmp++;
+	if (*tmp != ' ') {
+		mlog(0, "bad data\n");
+		goto leave;
+	}
+	tmp++;
+	domainname = tmp;
+
+	while (*tmp) {
+		if (*tmp == ' ')
+			break;
+		tmp++;
+	}
+	if (!*tmp || !*(tmp+1)) {
+		mlog(0, "bad data\n");
+		goto leave;
+	}
+
+	*tmp = '\0';  // null term the domainname
+	tmp++;
+	resname = tmp;
+	while (*tmp) {
+		if (*tmp == '\n' ||
+		    *tmp == ' ' ||
+		    *tmp == '\r') {
+			*tmp = '\0';
+			break;
+		}
+		tmp++;
+	}
+
+	mlog(0, "now looking up domain %s, lockres %s\n",
+	       domainname, resname);
+	spin_lock(&dlm_domain_lock);
+	*dlm = __dlm_lookup_domain(domainname);
+	spin_unlock(&dlm_domain_lock);
+
+	if (!dlm_grab(*dlm)) {
+		mlog(ML_ERROR, "bad dlm!\n");
+		*dlm = NULL;
+		goto leave;
+	}
+
+	*res = dlm_lookup_lockres(*dlm, resname, strlen(resname));
+	if (!*res) {
+		mlog(ML_ERROR, "bad lockres!\n");
+		dlm_put(*dlm);
+		*dlm = NULL;
+		goto leave;
+	}
+
+	mlog(0, "found dlm=%p, lockres=%p\n", *dlm, *res);
+	ret = 0;
+
+leave:
+	return ret;
+}
+
+static int dlm_trigger_migration(const char __user *data, unsigned int len)
+{
+	struct dlm_lock_resource *res;
+	struct dlm_ctxt *dlm;
+	char *buf = NULL;
+	int ret = -EINVAL;
+	int tmpret;
+
+	if (len >= PAGE_SIZE-1) {
+		mlog(ML_ERROR, "user passed too much data: %d bytes\n", len);
+		goto leave;
+	}
+	if (len < 5) {
+		mlog(ML_ERROR, "user passed too little data: %d bytes\n", len);
+		goto leave;
+	}
+	buf = kmalloc(len+1, GFP_KERNEL);
+	if (!buf) {
+		mlog(ML_ERROR, "could not alloc %d bytes\n", len+1);
+		ret = -ENOMEM;
+		goto leave;
+	}
+	if (strncpy_from_user(buf, data, len) < len) {
+		mlog(ML_ERROR, "failed to get all user data.  done.\n");
+		goto leave;
+	}
+	buf[len]='\0';
+	mlog(0, "got this data from user: %s\n", buf);
+
+	if (*buf != 'M') {
+		mlog(0, "bad data\n");
+		goto leave;
+	}
+
+	tmpret = dlm_parse_domain_and_lockres(buf, len, &dlm, &res);
+	if (tmpret < 0) {
+		mlog(0, "bad data\n");
+		goto leave;
+	}
+	tmpret = dlm_migrate_lockres(dlm, res, O2NM_MAX_NODES);
+	mlog(0, "dlm_migrate_lockres returned %d\n", tmpret);
+	if (tmpret < 0)
+		mlog(ML_ERROR, "failed to migrate %.*s: %d\n",
+		     res->lockname.len, res->lockname.name, tmpret);
+	dlm_lockres_put(res);
+	dlm_put(dlm);
+	ret = len;
+
+leave:
+	if (buf)
+		kfree(buf);
+	return ret;
+}
+
 static const char *dlm_errnames[] = {
 	[DLM_NORMAL] =			"DLM_NORMAL",
 	[DLM_GRANTED] =			"DLM_GRANTED",
@@ -229,6 +560,7 @@ static const char *dlm_errmsgs[] = {
 	[DLM_MAXSTATS] = 		"invalid error number",
 };
 
+
 const char *dlm_errmsg(enum dlm_status err)
 {
 	if (err >= DLM_MAXSTATS || err < 0)
@@ -244,3 +576,4 @@ const char *dlm_errname(enum dlm_status 
 	return dlm_errnames[err];
 }
 EXPORT_SYMBOL_GPL(dlm_errname);
+
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/dlm/dlmdebug.h linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmdebug.h
--- linux-2.6.15-2/fs/ocfs2/dlm/dlmdebug.h	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmdebug.h	2006-02-06 17:38:14.000000000 -0500
@@ -25,6 +25,8 @@
 #ifndef DLMDEBUG_H
 #define DLMDEBUG_H
 
+void dlm_remove_proc(void);
+void dlm_init_proc(void);
 void dlm_dump_lock_resources(struct dlm_ctxt *dlm);
 
 #endif
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/dlm/dlmdomain.c linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmdomain.c
--- linux-2.6.15-2/fs/ocfs2/dlm/dlmdomain.c	2006-02-06 18:15:15.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmdomain.c	2006-02-06 17:59:23.000000000 -0500
@@ -163,7 +163,7 @@ static struct dlm_ctxt * __dlm_lookup_do
 }
 
 /* For null terminated domain strings ONLY */
-static struct dlm_ctxt * __dlm_lookup_domain(const char *domain)
+struct dlm_ctxt * __dlm_lookup_domain(const char *domain)
 {
 	assert_spin_locked(&dlm_domain_lock);
 
@@ -266,6 +266,13 @@ struct dlm_ctxt *dlm_grab(struct dlm_ctx
 	return target;
 }
 
+void dlm_get(struct dlm_ctxt *dlm)
+{
+	spin_lock(&dlm_domain_lock);
+	__dlm_get(dlm);
+	spin_unlock(&dlm_domain_lock);
+}
+
 int dlm_domain_fully_joined(struct dlm_ctxt *dlm)
 {
 	int ret;
@@ -1469,11 +1476,14 @@ static int __init dlm_init(void)
 		return -1;
 	}
 
+	dlm_init_proc();
+
 	return 0;
 }
 
 static void __exit dlm_exit (void)
 {
+	dlm_remove_proc();
 	dlm_unregister_net_handlers();
 	dlm_destroy_mle_cache();
 }
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/dlm/dlmdomain.h linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmdomain.h
--- linux-2.6.15-2/fs/ocfs2/dlm/dlmdomain.h	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmdomain.h	2006-02-06 17:38:14.000000000 -0500
@@ -28,6 +28,7 @@
 extern spinlock_t dlm_domain_lock;
 extern struct list_head dlm_domains;
 
+struct dlm_ctxt * __dlm_lookup_domain(const char *domain);
 int dlm_joined(struct dlm_ctxt *dlm);
 int dlm_shutting_down(struct dlm_ctxt *dlm);
 void dlm_fire_domain_eviction_callbacks(struct dlm_ctxt *dlm,
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/dlm/dlmfsver.c linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmfsver.c
--- linux-2.6.15-2/fs/ocfs2/dlm/dlmfsver.c	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmfsver.c	2006-02-06 17:59:23.000000000 -0500
@@ -28,9 +28,12 @@
 
 #include "dlmfsver.h"
 
-#define DLM_BUILD_VERSION "1.3.3"
+#define DLM_BUILD_VERSION	"1.2.0-SLES"
+#define DLM_BUILD_DATE	"Tue Jan 24 14:31:42 PST 2006"
+#define DLM_BUILD_MD5	"sles"
 
-#define VERSION_STR "OCFS2 DLMFS " DLM_BUILD_VERSION
+#define VERSION_STR "OCFS2 DLMFS " \
+	DLM_BUILD_VERSION " " DLM_BUILD_DATE " (build " DLM_BUILD_MD5 ")"
 
 void dlmfs_print_version(void)
 {
@@ -39,4 +42,6 @@ void dlmfs_print_version(void)
 
 MODULE_DESCRIPTION(VERSION_STR);
 
+#ifdef MODULE_VERSION
 MODULE_VERSION(DLM_BUILD_VERSION);
+#endif
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/dlm/dlmmaster.c linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmmaster.c
--- linux-2.6.15-2/fs/ocfs2/dlm/dlmmaster.c	2006-02-06 18:15:15.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmmaster.c	2006-02-06 17:59:23.000000000 -0500
@@ -48,7 +48,6 @@
 #include "dlmapi.h"
 #include "dlmcommon.h"
 #include "dlmdebug.h"
-#include "dlmdomain.h"
 
 #define MLOG_MASK_PREFIX (ML_DLM|ML_DLM_MASTER)
 #include "cluster/masklog.h"
@@ -89,19 +88,16 @@ struct dlm_master_list_entry
 	} u;
 };
 
-static void dlm_mle_node_down(struct dlm_ctxt *dlm,
-			      struct dlm_master_list_entry *mle,
-			      struct o2nm_node *node,
-			      int idx);
-static void dlm_mle_node_up(struct dlm_ctxt *dlm,
-			    struct dlm_master_list_entry *mle,
-			    struct o2nm_node *node,
-			    int idx);
-
-static void dlm_assert_master_worker(struct dlm_work_item *item, void *data);
-static int dlm_do_assert_master(struct dlm_ctxt *dlm, const char *lockname,
-				unsigned int namelen, void *nodemap,
-				u32 flags);
+void dlm_print_one_mle(struct dlm_master_list_entry *mle);
+
+void dlm_mle_node_down(struct dlm_ctxt *dlm,
+		       struct dlm_master_list_entry *mle,
+		       struct o2nm_node *node,
+		       int idx);
+void dlm_mle_node_up(struct dlm_ctxt *dlm,
+		     struct dlm_master_list_entry *mle,
+		     struct o2nm_node *node,
+		     int idx);
 
 static inline int dlm_mle_equal(struct dlm_ctxt *dlm,
 				struct dlm_master_list_entry *mle,
@@ -127,9 +123,6 @@ static inline int dlm_mle_equal(struct d
 	return 1;
 }
 
-#if 0
-/* Code here is included but defined out as it aids debugging */
-
 void dlm_print_one_mle(struct dlm_master_list_entry *mle)
 {
 	int i = 0, refs;
@@ -164,6 +157,9 @@ void dlm_print_one_mle(struct dlm_master
 		  namelen, namelen, name);
 }
 
+			      
+static void dlm_dump_mles(struct dlm_ctxt *dlm);
+
 static void dlm_dump_mles(struct dlm_ctxt *dlm)
 {
 	struct dlm_master_list_entry *mle;
@@ -179,6 +175,9 @@ static void dlm_dump_mles(struct dlm_ctx
 	spin_unlock(&dlm->master_lock);
 }
 
+extern spinlock_t dlm_domain_lock;
+extern struct list_head dlm_domains;
+
 int dlm_dump_all_mles(const char __user *data, unsigned int len)
 {
 	struct list_head *iter;
@@ -195,8 +194,6 @@ int dlm_dump_all_mles(const char __user 
 }
 EXPORT_SYMBOL_GPL(dlm_dump_all_mles);
 
-#endif  /*  0  */
-
 
 static kmem_cache_t *dlm_mle_cache = NULL;
 
@@ -428,9 +425,8 @@ void dlm_hb_event_notify_attached(struct
 	}
 }
 
-static void dlm_mle_node_down(struct dlm_ctxt *dlm,
-			      struct dlm_master_list_entry *mle,
-			      struct o2nm_node *node, int idx)
+void dlm_mle_node_down(struct dlm_ctxt *dlm, struct dlm_master_list_entry *mle,
+		       struct o2nm_node *node, int idx)
 {
 	spin_lock(&mle->spinlock);
 
@@ -442,9 +438,8 @@ static void dlm_mle_node_down(struct dlm
 	spin_unlock(&mle->spinlock);
 }
 
-static void dlm_mle_node_up(struct dlm_ctxt *dlm,
-			    struct dlm_master_list_entry *mle,
-			    struct o2nm_node *node, int idx)
+void dlm_mle_node_up(struct dlm_ctxt *dlm, struct dlm_master_list_entry *mle,
+		     struct o2nm_node *node, int idx)
 {
 	spin_lock(&mle->spinlock);
 
@@ -587,9 +582,8 @@ void dlm_lockres_put(struct dlm_lock_res
 	kref_put(&res->refs, dlm_lockres_release);
 }
 
-static void dlm_init_lockres(struct dlm_ctxt *dlm,
-			     struct dlm_lock_resource *res,
-			     const char *name, unsigned int namelen)
+void dlm_init_lockres(struct dlm_ctxt *dlm, struct dlm_lock_resource *res,
+		      const char *name, unsigned int namelen)
 {
 	char *qname;
 
@@ -810,7 +804,7 @@ wait:
 			     dlm->name, res->lockname.len, 
 			     res->lockname.name, blocked);
 			dlm_print_one_lock_resource(res);
-			/* dlm_print_one_mle(mle); */
+			dlm_print_one_mle(mle);
 			tries = 0;
 		}
 		goto redo_request;
@@ -1463,9 +1457,9 @@ send_response:
  * can periodically run all locks owned by this node
  * and re-assert across the cluster...
  */
-static int dlm_do_assert_master(struct dlm_ctxt *dlm, const char *lockname,
-				unsigned int namelen, void *nodemap,
-				u32 flags)
+int dlm_do_assert_master(struct dlm_ctxt *dlm, const char *lockname,
+			 unsigned int namelen, void *nodemap,
+			 u32 flags)
 {
 	struct dlm_assert_master assert;
 	int to, tmpret;
@@ -1721,7 +1715,7 @@ int dlm_dispatch_assert_master(struct dl
 	return 0;
 }
 
-static void dlm_assert_master_worker(struct dlm_work_item *item, void *data)
+void dlm_assert_master_worker(struct dlm_work_item *item, void *data)
 {
 	struct dlm_ctxt *dlm = data;
 	int ret = 0;
@@ -2242,10 +2236,8 @@ static u8 dlm_pick_migration_target(stru
 
 /* this is called by the new master once all lockres
  * data has been received */
-static int dlm_do_migrate_request(struct dlm_ctxt *dlm,
-				  struct dlm_lock_resource *res,
-				  u8 master, u8 new_master,
-				  struct dlm_node_iter *iter)
+int dlm_do_migrate_request(struct dlm_ctxt *dlm, struct dlm_lock_resource *res,
+			   u8 master, u8 new_master, struct dlm_node_iter *iter)
 {
 	struct dlm_migrate_request migrate;
 	int ret, status = 0;
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/dlm/dlmrecovery.c linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmrecovery.c
--- linux-2.6.15-2/fs/ocfs2/dlm/dlmrecovery.c	2006-02-06 18:15:15.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmrecovery.c	2006-02-06 17:59:23.000000000 -0500
@@ -58,15 +58,15 @@ static void dlm_do_local_recovery_cleanu
 static int dlm_recovery_thread(void *data);
 void dlm_complete_recovery_thread(struct dlm_ctxt *dlm);
 int dlm_launch_recovery_thread(struct dlm_ctxt *dlm);
-static void dlm_kick_recovery_thread(struct dlm_ctxt *dlm);
-static int dlm_do_recovery(struct dlm_ctxt *dlm);
+void dlm_kick_recovery_thread(struct dlm_ctxt *dlm);
+int dlm_do_recovery(struct dlm_ctxt *dlm);
 
-static int dlm_pick_recovery_master(struct dlm_ctxt *dlm);
+int dlm_pick_recovery_master(struct dlm_ctxt *dlm);
 static int dlm_remaster_locks(struct dlm_ctxt *dlm, u8 dead_node);
-static int dlm_init_recovery_area(struct dlm_ctxt *dlm, u8 dead_node);
-static int dlm_request_all_locks(struct dlm_ctxt *dlm,
-				 u8 request_from, u8 dead_node);
-static void dlm_destroy_recovery_area(struct dlm_ctxt *dlm, u8 dead_node);
+int dlm_init_recovery_area(struct dlm_ctxt *dlm, u8 dead_node);
+int dlm_request_all_locks(struct dlm_ctxt *dlm,
+			  u8 request_from, u8 dead_node);
+void dlm_destroy_recovery_area(struct dlm_ctxt *dlm, u8 dead_node);
 
 static inline int dlm_num_locks_in_lockres(struct dlm_lock_resource *res);
 static void dlm_init_migratable_lockres(struct dlm_migratable_lockres *mres,
@@ -165,7 +165,7 @@ void dlm_dispatch_work(void *data)
  * RECOVERY THREAD
  */
 
-static void dlm_kick_recovery_thread(struct dlm_ctxt *dlm)
+void dlm_kick_recovery_thread(struct dlm_ctxt *dlm)
 {
 	/* wake the recovery thread
 	 * this will wake the reco thread in one of three places
@@ -316,7 +334,7 @@ static void dlm_end_recovery(struct dlm_
 	wake_up(&dlm->reco.event);
 }
 
-static int dlm_do_recovery(struct dlm_ctxt *dlm)
+int dlm_do_recovery(struct dlm_ctxt *dlm)
 {
 	int status = 0;
 	int ret;
@@ -587,7 +605,7 @@ leave:
 	return status;
 }
 
-static int dlm_init_recovery_area(struct dlm_ctxt *dlm, u8 dead_node)
+int dlm_init_recovery_area(struct dlm_ctxt *dlm, u8 dead_node)
 {
 	int num=0;
 	struct dlm_reco_node_data *ndata;
@@ -621,7 +639,7 @@ static int dlm_init_recovery_area(struct
 	return 0;
 }
 
-static void dlm_destroy_recovery_area(struct dlm_ctxt *dlm, u8 dead_node)
+void dlm_destroy_recovery_area(struct dlm_ctxt *dlm, u8 dead_node)
 {
 	struct list_head *iter, *iter2;
 	struct dlm_reco_node_data *ndata;
@@ -638,8 +656,7 @@ static void dlm_destroy_recovery_area(st
 	}
 }
 
-static int dlm_request_all_locks(struct dlm_ctxt *dlm, u8 request_from,
-				 u8 dead_node)
+int dlm_request_all_locks(struct dlm_ctxt *dlm, u8 request_from, u8 dead_node)
 {
 	struct dlm_lock_request lr;
 	enum dlm_status ret;
@@ -1862,8 +1879,8 @@ static void dlm_do_local_recovery_cleanu
 		bucket = &(dlm->resources[i]);
 		list_for_each(iter, bucket) {
 			res = list_entry (iter, struct dlm_lock_resource, list);
- 			/* always prune any $RECOVERY entries for dead nodes,
- 			 * otherwise hangs can occur during later recovery */
+			/* always prune any $RECOVERY entries for dead nodes,
+			 * otherwise hangs can occur during later recovery */
 			if (dlm_is_recovery_lock(res->lockname.name,
 						 res->lockname.len)) {
 				spin_lock(&res->spinlock);
@@ -1871,7 +1888,7 @@ static void dlm_do_local_recovery_cleanu
 					if (lock->ml.node == dead_node) {
 						mlog(0, "AHA! there was "
 						     "a $RECOVERY lock for dead "
-						     "node %u (%s)!\n",
+						     "node %u (%s)!\n", 
 						     dead_node, dlm->name);
 						list_del_init(&lock->list);
 						dlm_lock_put(lock);
@@ -1880,7 +1897,7 @@ static void dlm_do_local_recovery_cleanu
 				}
 				spin_unlock(&res->spinlock);
 				continue;
-			}			
+			}
 			spin_lock(&res->spinlock);
 			/* zero the lvb if necessary */
 			dlm_revalidate_lvb(dlm, res, dead_node);
@@ -1896,7 +1913,7 @@ static void dlm_do_local_recovery_cleanu
 
 }
 
-static void __dlm_hb_node_down(struct dlm_ctxt *dlm, int idx)
+void __dlm_hb_node_down(struct dlm_ctxt *dlm, int idx)
 {
 	assert_spin_locked(&dlm->spinlock);
 
@@ -1974,6 +1991,22 @@ void dlm_hb_node_up_cb(struct o2nm_node 
 	dlm_put(dlm);
 }
 
+int __dlm_hb_node_dead(struct dlm_ctxt *dlm, int node)
+{
+	if (test_bit(node, dlm->recovery_map))
+		return 1;
+	return 0;
+}
+
+int dlm_hb_node_dead(struct dlm_ctxt *dlm, int node)
+{
+	int ret;
+	spin_lock(&dlm->spinlock);
+	ret = __dlm_hb_node_dead(dlm, node);
+	spin_unlock(&dlm->spinlock);
+	return ret;
+}
+
 static void dlm_reco_ast(void *astdata)
 {
 	struct dlm_ctxt *dlm = astdata;
@@ -2003,7 +2036,7 @@ static void dlm_reco_unlock_ast(void *as
  * so each time a recovery master is needed, the entire cluster
  * will sync at this point.  if the new master dies, that will
  * be detected in dlm_do_recovery */
-static int dlm_pick_recovery_master(struct dlm_ctxt *dlm)
+int dlm_pick_recovery_master(struct dlm_ctxt *dlm)
 {
 	enum dlm_status ret;
 	struct dlm_lockstatus lksb;
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/dlm/dlmthread.c linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmthread.c
--- linux-2.6.15-2/fs/ocfs2/dlm/dlmthread.c	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmthread.c	2006-02-06 17:38:14.000000000 -0500
@@ -52,9 +52,10 @@
 #define MLOG_MASK_PREFIX (ML_DLM|ML_DLM_THREAD)
 #include "cluster/masklog.h"
 
-static int dlm_thread(void *data);
+extern spinlock_t dlm_domain_lock;
+extern struct list_head dlm_domains;
 
-static void dlm_flush_asts(struct dlm_ctxt *dlm);
+static int dlm_thread(void *data);
 
 #define dlm_lock_is_remote(dlm, lock)     ((lock)->ml.node != (dlm)->node_num)
 
@@ -235,8 +236,7 @@ static void dlm_run_purge_list(struct dl
 	spin_unlock(&dlm->spinlock);
 }
 
-static void dlm_shuffle_lists(struct dlm_ctxt *dlm,
-			      struct dlm_lock_resource *res)
+void dlm_shuffle_lists(struct dlm_ctxt *dlm, struct dlm_lock_resource *res)
 {
 	struct dlm_lock *lock, *target;
 	struct list_head *iter;
@@ -412,6 +412,15 @@ void dlm_kick_thread(struct dlm_ctxt *dl
 	wake_up(&dlm->dlm_thread_wq);
 }
 
+void __dlm_kick_thread(struct dlm_ctxt *dlm, struct dlm_lock_resource *res)
+{
+	mlog_entry("dlm=%p, res=%p\n", dlm, res);
+	if (res)
+		__dlm_dirty_lockres(dlm, res);
+
+	wake_up(&dlm->dlm_thread_wq);
+}
+
 void __dlm_dirty_lockres(struct dlm_ctxt *dlm, struct dlm_lock_resource *res)
 {
 	mlog_entry("dlm=%p, res=%p\n", dlm, res);
@@ -463,7 +472,14 @@ static int dlm_dirty_list_empty(struct d
 	return empty;
 }
 
-static void dlm_flush_asts(struct dlm_ctxt *dlm)
+int dlm_flush_lockres_asts(struct dlm_ctxt *dlm, struct dlm_lock_resource *res)
+{
+	dlm_flush_asts(dlm);
+	/* still need to implement dlm_flush_lockres_asts */
+	return 0;
+}
+
+void dlm_flush_asts(struct dlm_ctxt *dlm)
 {
 	int ret;
 	struct dlm_lock *lock;
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/dlm/dlmver.c linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmver.c
--- linux-2.6.15-2/fs/ocfs2/dlm/dlmver.c	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/dlm/dlmver.c	2006-02-06 17:59:23.000000000 -0500
@@ -28,9 +28,12 @@
 
 #include "dlmver.h"
 
-#define DLM_BUILD_VERSION "1.3.3"
+#define DLM_BUILD_VERSION	"1.2.0-SLES"
+#define DLM_BUILD_DATE	"Tue Jan 24 14:31:42 PST 2006"
+#define DLM_BUILD_MD5	"sles"
 
-#define VERSION_STR "OCFS2 DLM " DLM_BUILD_VERSION
+#define VERSION_STR "OCFS2 DLM " \
+	DLM_BUILD_VERSION " " DLM_BUILD_DATE " (build " DLM_BUILD_MD5 ")"
 
 void dlm_print_version(void)
 {
@@ -39,4 +42,6 @@ void dlm_print_version(void)
 
 MODULE_DESCRIPTION(VERSION_STR);
 
+#ifdef MODULE_VERSION
 MODULE_VERSION(DLM_BUILD_VERSION);
+#endif
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/dlmglue.c linux-2.6.15-ocfs2/fs/ocfs2/dlmglue.c
--- linux-2.6.15-2/fs/ocfs2/dlmglue.c	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/dlmglue.c	2006-02-06 17:59:23.000000000 -0500
@@ -30,7 +30,6 @@
 #include <linux/smp_lock.h>
 #include <linux/crc32.h>
 #include <linux/kthread.h>
-#include <linux/pagemap.h>
 #include <linux/debugfs.h>
 #include <linux/seq_file.h>
 
@@ -58,14 +57,6 @@
 
 #include "buffer_head_io.h"
 
-struct ocfs2_mask_waiter {
-	struct list_head	mw_item;
-	int			mw_status;
-	struct completion	mw_complete;
-	unsigned long		mw_mask;
-	unsigned long		mw_goal;
-};
-
 static void ocfs2_inode_ast_func(void *opaque);
 static void ocfs2_inode_bast_func(void *opaque,
 				  int level);
@@ -85,8 +76,6 @@ static int ocfs2_unblock_meta(struct ocf
 			      int *requeue);
 static int ocfs2_unblock_data(struct ocfs2_lock_res *lockres,
 			      int *requeue);
-static int ocfs2_unblock_inode_lock(struct ocfs2_lock_res *lockres,
-			      int *requeue);
 static int ocfs2_unblock_osb_lock(struct ocfs2_lock_res *lockres,
 				  int *requeue);
 typedef void (ocfs2_convert_worker_t)(struct ocfs2_lock_res *, int);
@@ -102,13 +91,6 @@ struct ocfs2_lock_res_ops {
 	int  (*unblock)(struct ocfs2_lock_res *, int *);
 };
 
-static struct ocfs2_lock_res_ops ocfs2_inode_rw_lops = {
-	.ast		= ocfs2_inode_ast_func,
-	.bast		= ocfs2_inode_bast_func,
-	.unlock_ast	= ocfs2_unlock_ast_func,
-	.unblock	= ocfs2_unblock_inode_lock,
-};
-
 static struct ocfs2_lock_res_ops ocfs2_inode_meta_lops = {
 	.ast		= ocfs2_inode_ast_func,
 	.bast		= ocfs2_inode_bast_func,
@@ -143,8 +125,7 @@ static struct ocfs2_lock_res_ops ocfs2_r
 static inline int ocfs2_is_inode_lock(struct ocfs2_lock_res *lockres)
 {
 	return lockres->l_type == OCFS2_LOCK_TYPE_META ||
-		lockres->l_type == OCFS2_LOCK_TYPE_DATA ||
-		lockres->l_type == OCFS2_LOCK_TYPE_RW;
+		lockres->l_type == OCFS2_LOCK_TYPE_DATA;
 }
 
 static inline int ocfs2_is_super_lock(struct ocfs2_lock_res *lockres)
@@ -175,9 +156,15 @@ static inline struct inode *ocfs2_lock_r
 static int ocfs2_lock_create(struct ocfs2_super *osb,
 			     struct ocfs2_lock_res *lockres,
 			     int level,
-			     int dlm_flags);
+			     int flags);
 static inline int ocfs2_may_continue_on_blocked_lock(struct ocfs2_lock_res *lockres,
 						     int wanted);
+static int ocfs2_cluster_lock(struct ocfs2_super *osb,
+			      struct ocfs2_lock_res *lockres,
+			      int level,
+			      int lkm_flags,
+			      ocfs2_lock_callback cb,
+			      unsigned long cb_data);
 static void ocfs2_cluster_unlock(struct ocfs2_super *osb,
 				 struct ocfs2_lock_res *lockres,
 				 int level);
@@ -200,26 +187,16 @@ static int ocfs2_meta_lock_update(struct
 				  struct buffer_head **bh);
 static void ocfs2_drop_osb_locks(struct ocfs2_super *osb);
 static inline int ocfs2_highest_compat_lock_level(int level);
+static int __ocfs2_downconvert_lock(struct ocfs2_super *osb,
+				    struct ocfs2_lock_res *lockres,
+				    int new_level,
+				    int lvb);
+static int __ocfs2_cancel_convert(struct ocfs2_super *osb,
+				  struct ocfs2_lock_res *lockres);
 static inline int ocfs2_can_downconvert_meta_lock(struct inode *inode,
 						  struct ocfs2_lock_res *lockres,
 						  int new_level);
 
-static char *ocfs2_lock_type_strings[] = {
-	[OCFS2_LOCK_TYPE_META] = "Meta",
-	[OCFS2_LOCK_TYPE_DATA] = "Data",
-	[OCFS2_LOCK_TYPE_SUPER] = "Super",
-	[OCFS2_LOCK_TYPE_RENAME] = "Rename",
-	/* Need to differntiate from [R]ename.. serializing writes is the
-	 * important job it does, anyway. */
-	[OCFS2_LOCK_TYPE_RW] = "Write/Read",
-};
-
-static char *ocfs2_lock_type_string(enum ocfs2_lock_type type)
-{
-	mlog_bug_on_msg(type >= OCFS2_NUM_LOCK_TYPES, "%d\n", type);
-	return ocfs2_lock_type_strings[type];
-}
-
 static void ocfs2_build_lock_name(enum ocfs2_lock_type type,
 				  u64 blkno,
 				  u32 generation,
@@ -294,7 +271,7 @@ void ocfs2_lock_res_init_once(struct ocf
 	spin_lock_init(&res->l_lock);
 	init_waitqueue_head(&res->l_event);
 	INIT_LIST_HEAD(&res->l_blocked_list);
-	INIT_LIST_HEAD(&res->l_mask_waiters);
+	INIT_LIST_HEAD(&res->l_flag_cb_list);
 }
 
 void ocfs2_inode_lock_res_init(struct ocfs2_lock_res *res,
@@ -303,21 +280,13 @@ void ocfs2_inode_lock_res_init(struct oc
 {
 	struct ocfs2_lock_res_ops *ops;
 
-	switch(type) {
-		case OCFS2_LOCK_TYPE_RW:
-			ops = &ocfs2_inode_rw_lops;
-			break;
-		case OCFS2_LOCK_TYPE_META:
-			ops = &ocfs2_inode_meta_lops;
-			break;
-		case OCFS2_LOCK_TYPE_DATA:
-			ops = &ocfs2_inode_data_lops;
-			break;
-		default:
-			mlog_bug_on_msg(1, "type: %d\n", type);
-			ops = NULL; /* thanks, gcc */
-			break;
-	};
+	BUG_ON(type != OCFS2_LOCK_TYPE_META &&
+	       type != OCFS2_LOCK_TYPE_DATA);
+
+	if (type == OCFS2_LOCK_TYPE_META)
+		ops = &ocfs2_inode_meta_lops;
+	else
+		ops = &ocfs2_inode_data_lops;
 
 	ocfs2_lock_res_init_common(OCFS2_SB(inode->i_sb), res, type,
 				   OCFS2_I(inode)->ip_blkno,
@@ -357,8 +326,8 @@ void ocfs2_lock_res_free(struct ocfs2_lo
 	mlog_bug_on_msg(!list_empty(&res->l_blocked_list),
 			"Lockres %s is on the blocked list\n",
 			res->l_name);
-	mlog_bug_on_msg(!list_empty(&res->l_mask_waiters),
-			"Lockres %s has mask waiters pending\n",
+	mlog_bug_on_msg(!list_empty(&res->l_flag_cb_list),
+			"Lockres %s has flag callbacks pending\n",
 			res->l_name);
 	mlog_bug_on_msg(spin_is_locked(&res->l_lock),
 			"Lockres %s is locked\n",
@@ -434,32 +403,35 @@ static inline int ocfs2_highest_compat_l
 	return new_level;
 }
 
-static void lockres_set_flags(struct ocfs2_lock_res *lockres,
-			      unsigned long newflags)
+/* XXX must be called with lockres->l_lock held */
+static void lockres_set_flags(struct ocfs2_lock_res *lockres, unsigned long newflags)
 {
 	struct list_head *pos, *tmp;
-	struct ocfs2_mask_waiter *mw;
+	struct ocfs2_lockres_flag_callback *fcb;
 
- 	assert_spin_locked(&lockres->l_lock);
+	assert_spin_locked(&lockres->l_lock);
 
 	lockres->l_flags = newflags;
 
-	list_for_each_safe(pos, tmp, &lockres->l_mask_waiters) {
-		mw = list_entry(pos, struct ocfs2_mask_waiter, mw_item);
-		if ((lockres->l_flags & mw->mw_mask) != mw->mw_goal)
+	list_for_each_safe(pos, tmp, &lockres->l_flag_cb_list) {
+		fcb = list_entry(pos, struct ocfs2_lockres_flag_callback,
+				 fc_lockres_item);
+		if ((lockres->l_flags & fcb->fc_flag_mask) !=
+		    fcb->fc_flag_goal)
 			continue;
 
-		list_del_init(&mw->mw_item);
-		mw->mw_status = 0;
-		complete(&mw->mw_complete);
+		list_del_init(&fcb->fc_lockres_item);
+		fcb->fc_cb(0, fcb->fc_data);
+		if (fcb->fc_free_once_called)
+			kfree(fcb);
 	}
 }
+
 static void lockres_or_flags(struct ocfs2_lock_res *lockres, unsigned long or)
 {
 	lockres_set_flags(lockres, lockres->l_flags | or);
 }
-static void lockres_clear_flags(struct ocfs2_lock_res *lockres,
-				unsigned long clear)
+static void lockres_clear_flags(struct ocfs2_lock_res *lockres, unsigned long clear)
 {
 	lockres_set_flags(lockres, lockres->l_flags & ~clear);
 }
@@ -527,7 +499,6 @@ static void ocfs2_inode_ast_func(void *o
 	struct ocfs2_lock_res *lockres = opaque;
 	struct inode *inode;
 	struct dlm_lockstatus *lksb;
-	unsigned long flags;
 
 	mlog_entry_void();
 
@@ -535,18 +506,18 @@ static void ocfs2_inode_ast_func(void *o
 
 	mlog(0, "AST fired for inode %"MLFu64", l_action = %u, type = %s\n",
 	     OCFS2_I(inode)->ip_blkno, lockres->l_action,
-	     ocfs2_lock_type_string(lockres->l_type));
+	     (lockres->l_type == OCFS2_LOCK_TYPE_META) ? "Meta" : "Data");
 
 	BUG_ON(!ocfs2_is_inode_lock(lockres));
 
-	spin_lock_irqsave(&lockres->l_lock, flags);
+	spin_lock(&lockres->l_lock);
 
 	lksb = &(lockres->l_lksb);
 	if (lksb->status != DLM_NORMAL) {
 		mlog(ML_ERROR, "ocfs2_inode_ast_func: lksb status value of %u "
 		     "on inode %"MLFu64"\n", lksb->status,
 		     OCFS2_I(inode)->ip_blkno);
-		spin_unlock_irqrestore(&lockres->l_lock, flags);
+		spin_unlock(&lockres->l_lock);
 		mlog_exit_void();
 		return;
 	}
@@ -571,14 +542,14 @@ static void ocfs2_inode_ast_func(void *o
 		BUG();
 	}
 
-	/* data and rw locking ignores refresh flag for now. */
-	if (lockres->l_type != OCFS2_LOCK_TYPE_META)
+	/* data locking ignores refresh flag for now. */
+	if (lockres->l_type == OCFS2_LOCK_TYPE_DATA)
 		lockres_clear_flags(lockres, OCFS2_LOCK_NEEDS_REFRESH);
 
 	/* set it to something invalid so if we get called again we
 	 * can catch it. */
 	lockres->l_action = OCFS2_AST_INVALID;
-	spin_unlock_irqrestore(&lockres->l_lock, flags);
+	spin_unlock(&lockres->l_lock);
 	wake_up(&lockres->l_event);
 
 	mlog_exit_void();
@@ -615,17 +586,16 @@ static void ocfs2_generic_bast_func(stru
 				    int level)
 {
 	int needs_downconvert;
-	unsigned long flags;
 
 	mlog_entry_void();
 
 	BUG_ON(level <= LKM_NLMODE);
 
-	spin_lock_irqsave(&lockres->l_lock, flags);
+	spin_lock(&lockres->l_lock);
 	needs_downconvert = ocfs2_generic_handle_bast(lockres, level);
 	if (needs_downconvert)
 		ocfs2_schedule_blocked_lock(osb, lockres);
-	spin_unlock_irqrestore(&lockres->l_lock, flags);
+	spin_unlock(&lockres->l_lock);
 
 	ocfs2_kick_vote_thread(osb);
 
@@ -649,7 +619,7 @@ static void ocfs2_inode_bast_func(void *
 	mlog(0, "BAST fired for inode %"MLFu64", blocking = %d, level = %d "
 	     "type = %s\n", OCFS2_I(inode)->ip_blkno, level,
 	     lockres->l_level,
-	     ocfs2_lock_type_string(lockres->l_type));
+	     (lockres->l_type == OCFS2_LOCK_TYPE_META) ? "Meta" : "Data");
 
 	ocfs2_generic_bast_func(osb, lockres, level);
 
@@ -660,14 +630,13 @@ static void ocfs2_generic_ast_func(struc
 				   int ignore_refresh)
 {
 	struct dlm_lockstatus *lksb = &lockres->l_lksb;
-	unsigned long flags;
 
-	spin_lock_irqsave(&lockres->l_lock, flags);
+	spin_lock(&lockres->l_lock);
 
 	if (lksb->status != DLM_NORMAL) {
 		mlog(ML_ERROR, "lockres %s: lksb status value of %u!\n",
 		     lockres->l_name, lksb->status);
-		spin_unlock_irqrestore(&lockres->l_lock, flags);
+		spin_unlock(&lockres->l_lock);
 		return;
 	}
 
@@ -691,7 +660,7 @@ static void ocfs2_generic_ast_func(struc
 	/* set it to something invalid so if we get called again we
 	 * can catch it. */
 	lockres->l_action = OCFS2_AST_INVALID;
-	spin_unlock_irqrestore(&lockres->l_lock, flags);
+	spin_unlock(&lockres->l_lock);
 
 	wake_up(&lockres->l_event);
 }
@@ -761,16 +730,14 @@ static void ocfs2_rename_bast_func(void 
 static inline void ocfs2_recover_from_dlm_error(struct ocfs2_lock_res *lockres,
 						int convert)
 {
-	unsigned long flags;
-
 	mlog_entry_void();
-	spin_lock_irqsave(&lockres->l_lock, flags);
+	spin_lock(&lockres->l_lock);
 	lockres_clear_flags(lockres, OCFS2_LOCK_BUSY);
 	if (convert)
 		lockres->l_action = OCFS2_AST_INVALID;
 	else
 		lockres->l_unlock_action = OCFS2_UNLOCK_INVALID;
-	spin_unlock_irqrestore(&lockres->l_lock, flags);
+	spin_unlock(&lockres->l_lock);
 
 	wake_up(&lockres->l_event);
 	mlog_exit_void();
@@ -783,33 +750,32 @@ static inline void ocfs2_recover_from_dl
 static int ocfs2_lock_create(struct ocfs2_super *osb,
 			     struct ocfs2_lock_res *lockres,
 			     int level,
-			     int dlm_flags)
+			     int flags)
 {
 	int ret = 0;
 	enum dlm_status status;
-	unsigned long flags;
 
 	mlog_entry_void();
 
 	mlog(0, "lock %s, level = %d, flags = %d\n", lockres->l_name, level,
-	     dlm_flags);
+	     flags);
 
-	spin_lock_irqsave(&lockres->l_lock, flags);
+	spin_lock(&lockres->l_lock);
 	if ((lockres->l_flags & OCFS2_LOCK_ATTACHED) ||
 	    (lockres->l_flags & OCFS2_LOCK_BUSY)) {
-		spin_unlock_irqrestore(&lockres->l_lock, flags);
+		spin_unlock(&lockres->l_lock);
 		goto bail;
 	}
 
 	lockres->l_action = OCFS2_AST_ATTACH;
 	lockres->l_requested = level;
 	lockres_or_flags(lockres, OCFS2_LOCK_BUSY);
-	spin_unlock_irqrestore(&lockres->l_lock, flags);
+	spin_unlock(&lockres->l_lock);
 
 	status = dlmlock(osb->dlm,
 			 level,
 			 &lockres->l_lksb,
-			 dlm_flags,
+			 flags,
 			 lockres->l_name,
 			 lockres->l_ops->ast,
 			 lockres,
@@ -830,12 +796,11 @@ bail:
 static inline int ocfs2_check_wait_flag(struct ocfs2_lock_res *lockres,
 					int flag)
 {
-	unsigned long flags;
 	int ret;
 
-	spin_lock_irqsave(&lockres->l_lock, flags);
+	spin_lock(&lockres->l_lock);
 	ret = lockres->l_flags & flag;
-	spin_unlock_irqrestore(&lockres->l_lock, flags);
+	spin_unlock(&lockres->l_lock);
 
 	return ret;
 }
@@ -854,6 +819,20 @@ static inline void ocfs2_wait_on_refresh
 		   !ocfs2_check_wait_flag(lockres, OCFS2_LOCK_REFRESHING));
 }
 
+static void lockres_add_flag_callback(struct ocfs2_lock_res *lockres,
+				      struct ocfs2_lockres_flag_callback *fcb,
+				      unsigned long mask, unsigned long goal)
+{
+	BUG_ON(!list_empty(&fcb->fc_lockres_item));
+	BUG_ON(fcb->fc_cb == NULL);
+
+	assert_spin_locked(&lockres->l_lock);
+
+	list_add_tail(&fcb->fc_lockres_item, &lockres->l_flag_cb_list);
+	fcb->fc_flag_mask = mask;
+	fcb->fc_flag_goal = goal;
+}
+
 /* predict what lock level we'll be dropping down to on behalf
  * of another node, and return true if the currently wanted
  * level will be compatible with it. */
@@ -865,85 +844,98 @@ static inline int ocfs2_may_continue_on_
 	return wanted <= ocfs2_highest_compat_lock_level(lockres->l_blocking);
 }
 
-static void ocfs2_init_mask_waiter(struct ocfs2_mask_waiter *mw)
+/* these are generic and could be used elsewhere */
+struct ocfs2_status_completion {
+	int			sc_status;
+	struct completion	sc_complete;
+};
+
+static void ocfs2_status_completion_cb(int rc, unsigned long data)
 {
-	INIT_LIST_HEAD(&mw->mw_item);
-	init_completion(&mw->mw_complete);
+	struct ocfs2_status_completion *sc;
+
+	sc = (struct ocfs2_status_completion *)data;
+	sc->sc_status = rc;
+	complete(&sc->sc_complete);
 }
 
-static int ocfs2_wait_for_mask(struct ocfs2_mask_waiter *mw)
+static int ocfs2_wait_for_status_completion(struct ocfs2_status_completion *sc)
 {
-	wait_for_completion(&mw->mw_complete);
+	wait_for_completion(&sc->sc_complete);
 	/* Re-arm the completion in case we want to wait on it again */
-	INIT_COMPLETION(mw->mw_complete);
-	return mw->mw_status;
+	INIT_COMPLETION(sc->sc_complete);
+	return sc->sc_status;
 }
 
-static void lockres_add_mask_waiter(struct ocfs2_lock_res *lockres,
-				    struct ocfs2_mask_waiter *mw,
-				    unsigned long mask,
-				    unsigned long goal)
+static void ocfs2_init_fcb(struct ocfs2_lockres_flag_callback *fcb,
+			   ocfs2_lock_callback cb,
+			   unsigned long cb_data,
+			   int stack_allocated)
 {
-	BUG_ON(!list_empty(&mw->mw_item));
-
-	assert_spin_locked(&lockres->l_lock);
-
-	list_add_tail(&mw->mw_item, &lockres->l_mask_waiters);
-	mw->mw_mask = mask;
-	mw->mw_goal = goal;
+	fcb->fc_cb = cb;
+	fcb->fc_data = cb_data;
+	fcb->fc_free_once_called = !stack_allocated;
+	INIT_LIST_HEAD(&fcb->fc_lockres_item);
 }
 
-/* returns 0 if the mw that was removed was already satisfied, -EBUSY
- * if the mask still hadn't reached its goal */
-static int lockres_remove_mask_waiter(struct ocfs2_lock_res *lockres,
-				      struct ocfs2_mask_waiter *mw)
+/* Init a stack allocated FCB and an ocfs2_status_completion together. */
+static void ocfs2_init_completion_fcb(struct ocfs2_lockres_flag_callback *fcb,
+				      struct ocfs2_status_completion *sc)
 {
-	unsigned long flags;
-	int ret = 0;
-
-	spin_lock_irqsave(&lockres->l_lock, flags);
-	if (!list_empty(&mw->mw_item)) {
-		if ((lockres->l_flags & mw->mw_mask) != mw->mw_goal)
-			ret = -EBUSY;
-
-		list_del_init(&mw->mw_item);
-		init_completion(&mw->mw_complete);
-	}
-	spin_unlock_irqrestore(&lockres->l_lock, flags);
-
-	return ret;
-
+	init_completion(&sc->sc_complete);
+	ocfs2_init_fcb(fcb, ocfs2_status_completion_cb, (unsigned long) sc, 1);
 }
 
 static int ocfs2_cluster_lock(struct ocfs2_super *osb,
 			      struct ocfs2_lock_res *lockres,
 			      int level,
 			      int lkm_flags,
-			      int arg_flags)
+			      ocfs2_lock_callback cb,
+			      unsigned long cb_data)
 {
-	struct ocfs2_mask_waiter mw;
+	struct ocfs2_lockres_flag_callback sync_fcb, *fcb;
+	struct ocfs2_status_completion sc;
 	enum dlm_status status;
-	int wait, catch_signals = !(osb->s_mount_opt & OCFS2_MOUNT_NOINTR);
-	int ret = 0; /* gcc doesn't realize wait = 1 guarantees ret is set */
-	unsigned long flags;
+	int ret;
+	int catch_signals = !(osb->s_mount_opt & OCFS2_MOUNT_NOINTR);
+	int sync = 1;
 
 	mlog_entry_void();
 
-	ocfs2_init_mask_waiter(&mw);
+	if (cb != NULL) {
+		fcb = kmalloc(sizeof(*fcb), GFP_NOFS);
+		if (fcb == NULL) {
+			ret = -ENOMEM;
+			goto out;
+		}
 
-again:
-	wait = 0;
+		ocfs2_init_fcb(fcb, cb, cb_data, 0);
 
+		/* A callback passed in means we'll assume async
+		 * behavior - no waiting on dlm operations will be
+		 * done here and the allocated fcb will call the
+		 * callback when done. */
+		sync = 0;
+	} else {
+		/* No callback passed which means the caller wants
+		 * synchronous behavior - we avoid kmalloc and use a
+		 * stack allocated fcb for this. The status completion
+		 * helpers defined above come in handy here. */
+		fcb = &sync_fcb;
+		ocfs2_init_completion_fcb(fcb, &sc);
+	}
+
+again:
 	if (catch_signals && signal_pending(current)) {
 		ret = -ERESTARTSYS;
 		goto out;
 	}
 
-	spin_lock_irqsave(&lockres->l_lock, flags);
+	spin_lock(&lockres->l_lock);
 
 	mlog_bug_on_msg(lockres->l_flags & OCFS2_LOCK_FREEING,
-			"Cluster lock called on freeing lockres %s! flags "
-			"0x%lx\n", lockres->l_name, lockres->l_flags);
+			"Cluster lock called on freeing lockres %s! flags 0x%lx\n",
+			lockres->l_name, lockres->l_flags);
 
 	/* We only compare against the currently granted level
 	 * here. If the lock is blocked waiting on a downconvert,
@@ -952,14 +944,14 @@ again:
 	    level > lockres->l_level) {
 		/* is someone sitting in dlm_lock? If so, wait on
 		 * them. */
-		lockres_add_mask_waiter(lockres, &mw, OCFS2_LOCK_BUSY, 0);
-		wait = 1;
+		lockres_add_flag_callback(lockres, fcb, OCFS2_LOCK_BUSY, 0);
+		ret = -EIOCBRETRY;
 		goto unlock;
 	}
 
 	if (!(lockres->l_flags & OCFS2_LOCK_ATTACHED)) {
 		/* lock has not been created yet. */
-		spin_unlock_irqrestore(&lockres->l_lock, flags);
+		spin_unlock(&lockres->l_lock);
 
 		ret = ocfs2_lock_create(osb, lockres, LKM_NLMODE, 0);
 		if (ret < 0) {
@@ -973,8 +965,8 @@ again:
 	    !ocfs2_may_continue_on_blocked_lock(lockres, level)) {
 		/* is the lock is currently blocked on behalf of
 		 * another node */
-		lockres_add_mask_waiter(lockres, &mw, OCFS2_LOCK_BLOCKED, 0);
-		wait = 1;
+		lockres_add_flag_callback(lockres, fcb, OCFS2_LOCK_BLOCKED, 0);
+		ret = -EIOCBRETRY;
 		goto unlock;
 	}
 
@@ -986,7 +978,7 @@ again:
 		lockres->l_action = OCFS2_AST_CONVERT;
 		lockres->l_requested = level;
 		lockres_or_flags(lockres, OCFS2_LOCK_BUSY);
-		spin_unlock_irqrestore(&lockres->l_lock, flags);
+		spin_unlock(&lockres->l_lock);
 
 		BUG_ON(level == LKM_IVMODE);
 		BUG_ON(level == LKM_NLMODE);
@@ -1032,31 +1024,26 @@ again:
 
 	ret = 0;
 unlock:
-	spin_unlock_irqrestore(&lockres->l_lock, flags);
+	spin_unlock(&lockres->l_lock);
 out:
-	/*
-	 * This is helping work around a lock inversion between the page lock
-	 * and dlm locks.  One path holds the page lock while calling aops
-	 * which block acquiring dlm locks.  The voting thread holds dlm
-	 * locks while acquiring page locks while down converting data locks.
-	 * This block is helping an aop path notice the inversion and back
-	 * off to unlock its page lock before trying the dlm lock again.
-	 */
-	if (wait && arg_flags & OCFS2_LOCK_NONBLOCK &&
-	    mw.mw_mask & (OCFS2_LOCK_BUSY|OCFS2_LOCK_BLOCKED)) {
-		wait = 0;
-		if (lockres_remove_mask_waiter(lockres, &mw))
-			ret = -EAGAIN;
-		else
-			goto again;
-	}
-	if (wait) {
-		ret = ocfs2_wait_for_mask(&mw);
+	/* Non-async callers will always wait here for dlm operations
+	 * to complete. We must be careful to re-initialize the
+	 * completion before looping back. */
+	if (ret == -EIOCBRETRY && sync) {
+		ret = ocfs2_wait_for_status_completion(&sc);
 		if (ret == 0)
 			goto again;
 		mlog_errno(ret);
 	}
 
+	/* Only free the async fcb on error. */
+	if (ret && ret != -EIOCBRETRY && !sync) {
+		mlog_bug_on_msg(!list_empty(&fcb->fc_lockres_item),
+				"Lockres %s, freeing flag callback in use\n",
+				lockres->l_name);
+		kfree(fcb);
+	}
+
 	mlog_exit(ret);
 	return ret;
 }
@@ -1065,30 +1052,14 @@ static void ocfs2_cluster_unlock(struct 
 				 struct ocfs2_lock_res *lockres,
 				 int level)
 {
-	unsigned long flags;
-
 	mlog_entry_void();
-	spin_lock_irqsave(&lockres->l_lock, flags);
+	spin_lock(&lockres->l_lock);
 	ocfs2_dec_holders(lockres, level);
 	ocfs2_vote_on_unlock(osb, lockres);
-	spin_unlock_irqrestore(&lockres->l_lock, flags);
+	spin_unlock(&lockres->l_lock);
 	mlog_exit_void();
 }
 
-static int ocfs2_create_new_inode_lock(struct inode *inode,
-				       struct ocfs2_lock_res *lockres)
-{
-	struct ocfs2_super *osb = OCFS2_SB(inode->i_sb);
-	unsigned long flags;
-
-	spin_lock_irqsave(&lockres->l_lock, flags);
-	BUG_ON(lockres->l_flags & OCFS2_LOCK_ATTACHED);
-	lockres_or_flags(lockres, OCFS2_LOCK_LOCAL);
-	spin_unlock_irqrestore(&lockres->l_lock, flags);
-
-	return ocfs2_lock_create(osb, lockres, LKM_EXMODE, LKM_LOCAL);
-}
-
 /* Grants us an EX lock on the data and metadata resources, skipping
  * the normal cluster directory lookup. Use this ONLY on newly created
  * inodes which other nodes can't possibly see, and which haven't been
@@ -1097,7 +1068,9 @@ static int ocfs2_create_new_inode_lock(s
  * with creating a new lock resource. */
 int ocfs2_create_new_inode_locks(struct inode *inode)
 {
-	int ret;
+	int status;
+	struct ocfs2_super *osb = OCFS2_SB(inode->i_sb);
+	struct ocfs2_lock_res *lockres;
 
 	BUG_ON(!inode);
 	BUG_ON(!ocfs2_inode_is_new(inode));
@@ -1114,77 +1087,40 @@ int ocfs2_create_new_inode_locks(struct 
 	 * on a resource which has an invalid one -- we'll set it
 	 * valid when we release the EX. */
 
-	ret = ocfs2_create_new_inode_lock(inode,
-					  &OCFS2_I(inode)->ip_rw_lockres);
-	if (ret) {
-		mlog_errno(ret);
-		goto bail;
-	}
+	lockres = &OCFS2_I(inode)->ip_meta_lockres;
 
-	ret = ocfs2_create_new_inode_lock(inode,
-					  &OCFS2_I(inode)->ip_meta_lockres);
-	if (ret) {
-		mlog_errno(ret);
-		goto bail;
-	}
+	spin_lock(&lockres->l_lock);
+	BUG_ON(lockres->l_flags & OCFS2_LOCK_ATTACHED);
+	lockres_or_flags(lockres, OCFS2_LOCK_LOCAL);
+	spin_unlock(&lockres->l_lock);
 
-	ret = ocfs2_create_new_inode_lock(inode,
-					  &OCFS2_I(inode)->ip_data_lockres);
-	if (ret) {
-		mlog_errno(ret);
+	status = ocfs2_lock_create(osb, lockres, LKM_EXMODE, LKM_LOCAL);
+	if (status < 0) {
+		mlog_errno(status);
 		goto bail;
 	}
 
-bail:
-	mlog_exit(ret);
-	return ret;
-}
-
-int ocfs2_rw_lock(struct inode *inode, int write)
-{
-	int status, level;
-	struct ocfs2_lock_res *lockres;
-
-	BUG_ON(!inode);
-
-	mlog_entry_void();
-
-	mlog(0, "inode %"MLFu64" take %s RW lock\n",
-	     OCFS2_I(inode)->ip_blkno,
-	     write ? "EXMODE" : "PRMODE");
-
-	lockres = &OCFS2_I(inode)->ip_rw_lockres;
+	lockres = &OCFS2_I(inode)->ip_data_lockres;
 
-	level = write ? LKM_EXMODE : LKM_PRMODE;
+	spin_lock(&lockres->l_lock);
+	BUG_ON(lockres->l_flags & OCFS2_LOCK_ATTACHED);
+	lockres_or_flags(lockres, OCFS2_LOCK_LOCAL);
+	spin_unlock(&lockres->l_lock);
 
-	status = ocfs2_cluster_lock(OCFS2_SB(inode->i_sb), lockres, level, 0,
-				    0);
-	if (status < 0)
+	status = ocfs2_lock_create(osb, lockres, LKM_EXMODE, LKM_LOCAL);
+	if (status < 0) {
 		mlog_errno(status);
+		goto bail;
+	}
 
+	status = 0;
+bail:
 	mlog_exit(status);
 	return status;
 }
 
-void ocfs2_rw_unlock(struct inode *inode, int write)
-{
-	int level = write ? LKM_EXMODE : LKM_PRMODE;
-	struct ocfs2_lock_res *lockres = &OCFS2_I(inode)->ip_rw_lockres;
-
-	mlog_entry_void();
-
-	mlog(0, "inode %"MLFu64" drop %s RW lock\n",
-	     OCFS2_I(inode)->ip_blkno,
-	     write ? "EXMODE" : "PRMODE");
-
-	ocfs2_cluster_unlock(OCFS2_SB(inode->i_sb), lockres, level);
-
-	mlog_exit_void();
-}
-
-int ocfs2_data_lock_full(struct inode *inode,
-			 int write,
-			 int arg_flags)
+int ocfs2_data_lock(struct inode *inode,
+		    int write)
 {
 	int status = 0, level;
 	struct ocfs2_lock_res *lockres;
@@ -1211,9 +1147,9 @@ int ocfs2_data_lock_full(struct inode *i
 
 	level = write ? LKM_EXMODE : LKM_PRMODE;
 
-	status = ocfs2_cluster_lock(OCFS2_SB(inode->i_sb), lockres, level,
-				    0, arg_flags);
-	if (status < 0 && status != -EAGAIN)
+	status = ocfs2_cluster_lock(OCFS2_SB(inode->i_sb), lockres, level, 0,
+				    NULL, 0);
+	if (status < 0)
 		mlog_errno(status);
 
 out:
@@ -1221,24 +1157,6 @@ out:
 	return status;
 }
 
-/* see ocfs2_meta_lock_with_page() */
-int ocfs2_data_lock_with_page(struct inode *inode,
-			      int write,
-			      struct page *page)
-{
-	int ret;
-
-	ret = ocfs2_data_lock_full(inode, write, OCFS2_LOCK_NONBLOCK);
-	if (ret == -EAGAIN) {
-		unlock_page(page);
-		if (ocfs2_data_lock(inode, write) == 0)
-			ocfs2_data_unlock(inode, write);
-		ret = AOP_TRUNCATED_PAGE;
-	}
-
-	return ret;
-}
-
 static void ocfs2_vote_on_unlock(struct ocfs2_super *osb,
 				 struct ocfs2_lock_res *lockres)
 {
@@ -1318,8 +1236,12 @@ static void __ocfs2_stuff_meta_lvb(struc
 
 	lvb = (struct ocfs2_meta_lvb *) lockres->l_lksb.lvb;
 
+	/* Setting this to zero will ensure that old versions of the
+	 * LVB code don't trust our information. */
+	lvb->lvb_old_seq   = cpu_to_be32(0);
 	lvb->lvb_version   = cpu_to_be32(OCFS2_LVB_VERSION);
-	lvb->lvb_isize	   = cpu_to_be64(i_size_read(inode));
+
+	lvb->lvb_isize     = cpu_to_be64(i_size_read(inode));
 	lvb->lvb_iclusters = cpu_to_be32(oi->ip_clusters);
 	lvb->lvb_iuid      = cpu_to_be32(inode->i_uid);
 	lvb->lvb_igid      = cpu_to_be32(inode->i_gid);
@@ -1361,6 +1283,9 @@ static void ocfs2_refresh_inode_from_lvb
 	oi->ip_clusters = be32_to_cpu(lvb->lvb_iclusters);
 	i_size_write(inode, be64_to_cpu(lvb->lvb_isize));
 
+	if (S_ISREG(inode->i_mode))
+		oi->ip_mmu_private = i_size_read(inode);
+
 	/* fast-symlinks are a special case */
 	if (S_ISLNK(inode->i_mode) && !oi->ip_clusters)
 		inode->i_blocks = 0;
@@ -1387,7 +1312,12 @@ static inline int ocfs2_meta_lvb_is_trus
 {
 	struct ocfs2_meta_lvb *lvb = (struct ocfs2_meta_lvb *) lockres->l_lksb.lvb;
 
-	if (be32_to_cpu(lvb->lvb_version) == OCFS2_LVB_VERSION)
+	/* Old OCFS2 versions stored a "sequence" in the lvb to
+	 * determine whether the information could be trusted. We
+	 * don't want to use an lvb populated from a node running the
+	 * old code, so check that sequence is not set. */
+	if (!lvb->lvb_old_seq &&
+	    be32_to_cpu(lvb->lvb_version) == OCFS2_LVB_VERSION)
 		return 1;
 	return 0;
 }
@@ -1401,20 +1331,19 @@ static inline int ocfs2_meta_lvb_is_trus
  *   ocfs2_complete_lock_res_refresh afterwards. */
 static int ocfs2_should_refresh_lock_res(struct ocfs2_lock_res *lockres)
 {
-	unsigned long flags;
-	int status = 0;
 
+	int status = 0;
 	mlog_entry_void();
 
 refresh_check:
-	spin_lock_irqsave(&lockres->l_lock, flags);
+	spin_lock(&lockres->l_lock);
 	if (!(lockres->l_flags & OCFS2_LOCK_NEEDS_REFRESH)) {
-		spin_unlock_irqrestore(&lockres->l_lock, flags);
+		spin_unlock(&lockres->l_lock);
 		goto bail;
 	}
 
 	if (lockres->l_flags & OCFS2_LOCK_REFRESHING) {
-		spin_unlock_irqrestore(&lockres->l_lock, flags);
+		spin_unlock(&lockres->l_lock);
 
 		ocfs2_wait_on_refreshing_lock(lockres);
 		goto refresh_check;
@@ -1422,7 +1351,7 @@ refresh_check:
 
 	/* Ok, I'll be the one to refresh this lock. */
 	lockres_or_flags(lockres, OCFS2_LOCK_REFRESHING);
-	spin_unlock_irqrestore(&lockres->l_lock, flags);
+	spin_unlock(&lockres->l_lock);
 
 	status = 1;
 bail:
@@ -1435,14 +1364,13 @@ bail:
 static inline void ocfs2_complete_lock_res_refresh(struct ocfs2_lock_res *lockres,
 						   int status)
 {
-	unsigned long flags;
 	mlog_entry_void();
 
-	spin_lock_irqsave(&lockres->l_lock, flags);
+	spin_lock(&lockres->l_lock);
 	lockres_clear_flags(lockres, OCFS2_LOCK_REFRESHING);
 	if (!status)
 		lockres_clear_flags(lockres, OCFS2_LOCK_NEEDS_REFRESH);
-	spin_unlock_irqrestore(&lockres->l_lock, flags);
+	spin_unlock(&lockres->l_lock);
 
 	wake_up(&lockres->l_event);
 
@@ -1528,6 +1456,21 @@ static int ocfs2_meta_lock_update(struct
 		ocfs2_refresh_inode(inode, fe);
 	}
 
+#ifdef OCFS2_DELETE_INODE_WORKAROUND
+	/* We might as well check this here - since the inode is now
+	 * locked, an up to date view will indicate whether this was
+	 * never actually orphaned -- i_nlink should be zero for an
+	 * orphaned inode. */
+	spin_lock(&oi->ip_lock);
+	if (inode->i_nlink &&
+	    oi->ip_flags & OCFS2_INODE_MAYBE_ORPHANED) {
+		mlog(0, "Inode %"MLFu64": clearing maybe_orphaned flag\n",
+		     oi->ip_blkno);
+		oi->ip_flags &= ~OCFS2_INODE_MAYBE_ORPHANED;
+	}
+	spin_unlock(&oi->ip_lock);
+#endif
+
 	status = 0;
 bail_refresh:
 	ocfs2_complete_lock_res_refresh(lockres, status);
@@ -1570,7 +1513,9 @@ int ocfs2_meta_lock_full(struct inode *i
 			 struct ocfs2_journal_handle *handle,
 			 struct buffer_head **ret_bh,
 			 int ex,
-			 int arg_flags)
+			 int flags,
+			 ocfs2_lock_callback cb,
+			 unsigned long cb_data)
 {
 	int status, level, dlm_flags, acquired;
 	struct ocfs2_lock_res *lockres;
@@ -1595,7 +1540,7 @@ int ocfs2_meta_lock_full(struct inode *i
 		goto bail;
 	}
 
-	if (!(arg_flags & OCFS2_META_LOCK_RECOVERY))
+	if (!(flags & OCFS2_META_LOCK_RECOVERY))
 		wait_event(osb->recovery_event,
 			   ocfs2_node_map_is_empty(osb, &osb->recovery_map));
 
@@ -1603,10 +1548,11 @@ int ocfs2_meta_lock_full(struct inode *i
 	lockres = &OCFS2_I(inode)->ip_meta_lockres;
 	level = ex ? LKM_EXMODE : LKM_PRMODE;
 	dlm_flags = 0;
-	if (arg_flags & OCFS2_META_LOCK_NOQUEUE)
+	if (flags & OCFS2_META_LOCK_NOQUEUE)
 		dlm_flags |= LKM_NOQUEUE;
 
-	status = ocfs2_cluster_lock(osb, lockres, level, dlm_flags, arg_flags);
+	status = ocfs2_cluster_lock(osb, lockres, level, dlm_flags, cb,
+				    cb_data);
 	if (status < 0) {
 		if (status != -EAGAIN && status != -EIOCBRETRY)
 			mlog_errno(status);
@@ -1620,7 +1566,7 @@ int ocfs2_meta_lock_full(struct inode *i
 	 * the lower dlm layers. The second time though, we've
 	 * committed to owning this lock so we don't allow signals to
 	 * abort the operation. */
-	if (!(arg_flags & OCFS2_META_LOCK_RECOVERY))
+	if (!(flags & OCFS2_META_LOCK_RECOVERY))
 		wait_event(osb->recovery_event,
 			   ocfs2_node_map_is_empty(osb, &osb->recovery_map));
 
@@ -1667,47 +1613,6 @@ bail:
 	return status;
 }
 
-/*
- * This is working around a lock inversion between tasks acquiring DLM locks
- * while holding a page lock and the vote thread which blocks dlm lock acquiry
- * while acquiring page locks.
- *
- * ** These _with_page variantes are only intended to be called from aop
- * methods that hold page locks and return a very specific *positive* error
- * code that aop methods pass up to the VFS -- test for errors with != 0. **
- *
- * The DLM is called such that it returns -EAGAIN if it would have blocked
- * waiting for the vote thread.  In that case we unlock our page so the vote
- * thread can make progress.  Once we've done this we have to return
- * AOP_TRUNCATED_PAGE so the aop method that called us can bubble that back up
- * into the VFS who will then immediately retry the aop call.
- *
- * We do a blocking lock and immediate unlock before returning, though, so that
- * the lock has a great chance of being cached on this node by the time the VFS
- * calls back to retry the aop.    This has a potential to livelock as nodes
- * ping locks back and forth, but that's a risk we're willing to take to avoid
- * the lock inversion simply.
- */
-int ocfs2_meta_lock_with_page(struct inode *inode,
-			      struct ocfs2_journal_handle *handle,
-			      struct buffer_head **ret_bh,
-			      int ex,
-			      struct page *page)
-{
-	int ret;
-
-	ret = ocfs2_meta_lock_full(inode, handle, ret_bh, ex,
-				   OCFS2_LOCK_NONBLOCK);
-	if (ret == -EAGAIN) {
-		unlock_page(page);
-		if (ocfs2_meta_lock(inode, handle, ret_bh, ex) == 0)
-			ocfs2_meta_unlock(inode, ex);
-		ret = AOP_TRUNCATED_PAGE;
-	}
-
-	return ret;
-}
-
 void ocfs2_meta_unlock(struct inode *inode,
 		       int ex)
 {
@@ -1740,7 +1645,7 @@ int ocfs2_super_lock(struct ocfs2_super 
 	if (ocfs2_is_hard_readonly(osb))
 		return -EROFS;
 
-	status = ocfs2_cluster_lock(osb, lockres, level, 0, 0);
+	status = ocfs2_cluster_lock(osb, lockres, level, 0, NULL, 0);
 	if (status < 0) {
 		mlog_errno(status);
 		goto bail;
@@ -1789,7 +1694,7 @@ int ocfs2_rename_lock(struct ocfs2_super
 	if (ocfs2_is_hard_readonly(osb))
 		return -EROFS;
 
-	status = ocfs2_cluster_lock(osb, lockres, LKM_EXMODE, 0, 0);
+	status = ocfs2_cluster_lock(osb, lockres, LKM_EXMODE, 0, NULL, 0);
 	if (status < 0)
 		mlog_errno(status);
 
@@ -2138,18 +2043,17 @@ void ocfs2_dlm_shutdown(struct ocfs2_sup
 static void ocfs2_unlock_ast_func(void *opaque, enum dlm_status status)
 {
 	struct ocfs2_lock_res *lockres = opaque;
-	unsigned long flags;
 
 	mlog_entry_void();
 
 	mlog(0, "UNLOCK AST called on lock %s, action = %d\n", lockres->l_name,
 	     lockres->l_unlock_action);
 
-	spin_lock_irqsave(&lockres->l_lock, flags);
+	spin_lock(&lockres->l_lock);
 	/* We tried to cancel a convert request, but it was already
 	 * granted. All we want to do here is clear our unlock
 	 * state. The wake_up call done at the bottom is redundant
-	 * (ocfs2_prepare_cancel_convert doesn't sleep on this) but doesn't
+	 * (__ocfs2_cancel_convert doesn't sleep on this) but doesn't
 	 * hurt anything anyway */
 	if (status == DLM_CANCELGRANT &&
 	    lockres->l_unlock_action == OCFS2_UNLOCK_CANCEL_CONVERT) {
@@ -2165,7 +2069,7 @@ static void ocfs2_unlock_ast_func(void *
 		mlog(ML_ERROR, "Dlm passes status %d for lock %s, "
 		     "unlock_action %d\n", status, lockres->l_name,
 		     lockres->l_unlock_action);
-		spin_unlock_irqrestore(&lockres->l_lock, flags);
+		spin_unlock(&lockres->l_lock);
 		return;
 	}
 
@@ -2184,13 +2088,65 @@ static void ocfs2_unlock_ast_func(void *
 	lockres_clear_flags(lockres, OCFS2_LOCK_BUSY);
 complete_unlock:
 	lockres->l_unlock_action = OCFS2_UNLOCK_INVALID;
-	spin_unlock_irqrestore(&lockres->l_lock, flags);
+	spin_unlock(&lockres->l_lock);
 
 	wake_up(&lockres->l_event);
 
 	mlog_exit_void();
 }
 
+/* BEWARE: called with lockres lock, and always drops it. Caller
+ * should not be calling us with a busy lock... */
+static int __ocfs2_drop_lock(struct ocfs2_super *osb,
+			     struct ocfs2_lock_res *lockres)
+{
+	int ret = 0;
+	enum dlm_status status;
+
+	if (lockres->l_flags & OCFS2_LOCK_BUSY)
+		mlog(ML_ERROR, "destroying busy lock: \"%s\"\n",
+		     lockres->l_name);
+	if (lockres->l_flags & OCFS2_LOCK_BLOCKED)
+		mlog(0, "destroying blocked lock: \"%s\"\n", lockres->l_name);
+
+	if (!(lockres->l_flags & OCFS2_LOCK_ATTACHED)) {
+		spin_unlock(&lockres->l_lock);
+		goto bail;
+	}
+
+	lockres_clear_flags(lockres, OCFS2_LOCK_ATTACHED);
+
+	/* make sure we never get here while waiting for an ast to
+	 * fire. */
+	BUG_ON(lockres->l_action != OCFS2_AST_INVALID);
+
+	/* is this necessary? */
+	lockres_or_flags(lockres, OCFS2_LOCK_BUSY);
+	lockres->l_unlock_action = OCFS2_UNLOCK_DROP_LOCK;
+	spin_unlock(&lockres->l_lock);
+
+	mlog(0, "lock %s\n", lockres->l_name);
+
+	status = dlmunlock(osb->dlm,
+			   &lockres->l_lksb,
+			   LKM_VALBLK,
+			   lockres->l_ops->unlock_ast,
+			   lockres);
+	if (status != DLM_NORMAL) {
+		ocfs2_log_dlm_error("dlmunlock", status, lockres);
+		mlog(ML_ERROR, "lockres flags: %lu\n", lockres->l_flags);
+		dlm_print_one_lock(lockres->l_lksb.lockid);
+		BUG();
+	}
+	mlog(0, "lock %s, successfull return from dlmunlock\n",
+	     lockres->l_name);
+
+	ocfs2_wait_on_busy_lock(lockres);
+bail:
+	mlog_exit(ret);
+	return ret;
+}
+
 typedef void (ocfs2_pre_drop_cb_t)(struct ocfs2_lock_res *, void *);
 
 struct drop_lock_cb {
@@ -2202,14 +2158,11 @@ static int ocfs2_drop_lock(struct ocfs2_
 			   struct ocfs2_lock_res *lockres,
 			   struct drop_lock_cb *dcb)
 {
-	enum dlm_status status;
-	unsigned long flags;
-
 	/* We didn't get anywhere near actually using this lockres. */
 	if (!(lockres->l_flags & OCFS2_LOCK_INITIALIZED))
-		goto out;
+		return 0;
 
-	spin_lock_irqsave(&lockres->l_lock, flags);
+	spin_lock(&lockres->l_lock);
 
 	mlog_bug_on_msg(!(lockres->l_flags & OCFS2_LOCK_FREEING),
 			"lockres %s, flags 0x%lx\n",
@@ -2221,58 +2174,22 @@ static int ocfs2_drop_lock(struct ocfs2_
 		     lockres->l_name, lockres->l_flags, lockres->l_action,
 		     lockres->l_unlock_action);
 
-		spin_unlock_irqrestore(&lockres->l_lock, flags);
+		spin_unlock(&lockres->l_lock);
 
 		/* XXX: Today we just wait on any busy
 		 * locks... Perhaps we need to cancel converts in the
 		 * future? */
 		ocfs2_wait_on_busy_lock(lockres);
 
-		spin_lock_irqsave(&lockres->l_lock, flags);
+		spin_lock(&lockres->l_lock);
 	}
 
 	if (dcb)
 		dcb->drop_func(lockres, dcb->drop_data);
 
-	if (lockres->l_flags & OCFS2_LOCK_BUSY)
-		mlog(ML_ERROR, "destroying busy lock: \"%s\"\n",
-		     lockres->l_name);
-	if (lockres->l_flags & OCFS2_LOCK_BLOCKED)
-		mlog(0, "destroying blocked lock: \"%s\"\n", lockres->l_name);
-
-	if (!(lockres->l_flags & OCFS2_LOCK_ATTACHED)) {
-		spin_unlock_irqrestore(&lockres->l_lock, flags);
-		goto out;
-	}
-
-	lockres_clear_flags(lockres, OCFS2_LOCK_ATTACHED);
-
-	/* make sure we never get here while waiting for an ast to
-	 * fire. */
-	BUG_ON(lockres->l_action != OCFS2_AST_INVALID);
-
-	/* is this necessary? */
-	lockres_or_flags(lockres, OCFS2_LOCK_BUSY);
-	lockres->l_unlock_action = OCFS2_UNLOCK_DROP_LOCK;
-	spin_unlock_irqrestore(&lockres->l_lock, flags);
-
-	mlog(0, "lock %s\n", lockres->l_name);
-
-	status = dlmunlock(osb->dlm, &lockres->l_lksb, LKM_VALBLK,
-			   lockres->l_ops->unlock_ast, lockres);
-	if (status != DLM_NORMAL) {
-		ocfs2_log_dlm_error("dlmunlock", status, lockres);
-		mlog(ML_ERROR, "lockres flags: %lu\n", lockres->l_flags);
-		dlm_print_one_lock(lockres->l_lksb.lockid);
-		BUG();
-	}
-	mlog(0, "lock %s, successfull return from dlmunlock\n",
-	     lockres->l_name);
-
-	ocfs2_wait_on_busy_lock(lockres);
-out:
-	mlog_exit(0);
-	return 0;
+	/* This will drop the spinlock for us. Dur de dur, at least we
+	 * keep the ugliness in one place :) */
+	return  __ocfs2_drop_lock(osb, lockres);
 }
 
 /* Mark the lockres as being dropped. It will no longer be
@@ -2284,26 +2201,26 @@ out:
 void ocfs2_mark_lockres_freeing(struct ocfs2_lock_res *lockres)
 {
 	int status;
-	struct ocfs2_mask_waiter mw;
-	unsigned long flags;
+	struct ocfs2_status_completion sc;
+	struct ocfs2_lockres_flag_callback fcb;
 
-	ocfs2_init_mask_waiter(&mw);
+	ocfs2_init_completion_fcb(&fcb, &sc);
 
-	spin_lock_irqsave(&lockres->l_lock, flags);
+	spin_lock(&lockres->l_lock);
 	lockres->l_flags |= OCFS2_LOCK_FREEING;
 	while (lockres->l_flags & OCFS2_LOCK_QUEUED) {
-		lockres_add_mask_waiter(lockres, &mw, OCFS2_LOCK_QUEUED, 0);
-		spin_unlock_irqrestore(&lockres->l_lock, flags);
+		lockres_add_flag_callback(lockres, &fcb, OCFS2_LOCK_QUEUED, 0);
+		spin_unlock(&lockres->l_lock);
 
 		mlog(0, "Waiting on lockres %s\n", lockres->l_name);
 
-		status = ocfs2_wait_for_mask(&mw);
+		status = ocfs2_wait_for_status_completion(&sc);
 		if (status)
 			mlog_errno(status);
 
-		spin_lock_irqsave(&lockres->l_lock, flags);
+		spin_lock(&lockres->l_lock);
 	}
-	spin_unlock_irqrestore(&lockres->l_lock, flags);
+	spin_unlock(&lockres->l_lock);
 }
 
 static void ocfs2_drop_osb_locks(struct ocfs2_super *osb)
@@ -2365,22 +2282,20 @@ int ocfs2_drop_inode_locks(struct inode 
 	if (err < 0 && !status)
 		status = err;
 
-	err = ocfs2_drop_lock(OCFS2_SB(inode->i_sb),
-			      &OCFS2_I(inode)->ip_rw_lockres,
-			      NULL);
-	if (err < 0)
-		mlog_errno(err);
-	if (err < 0 && !status)
-		status = err;
-
 	mlog_exit(status);
 	return status;
 }
 
-static void ocfs2_prepare_downconvert(struct ocfs2_lock_res *lockres,
-				      int new_level)
+/* called with the spinlock held, and WILL drop it. */
+static int __ocfs2_downconvert_lock(struct ocfs2_super *osb,
+				    struct ocfs2_lock_res *lockres,
+				    int new_level,
+				    int lvb)
 {
-	assert_spin_locked(&lockres->l_lock);
+	int ret, flags = LKM_CONVERT;
+	enum dlm_status status;
+
+	mlog_entry_void();
 
 	BUG_ON(lockres->l_blocking <= LKM_NLMODE);
 
@@ -2390,31 +2305,21 @@ static void ocfs2_prepare_downconvert(st
 		BUG();
 	}
 
-	mlog(0, "lock %s, new_level = %d, l_blocking = %d\n",
-	     lockres->l_name, new_level, lockres->l_blocking);
+	mlog(0, "lock %s, new_level = %d, l_blocking = %d, lvb = %d\n",
+	     lockres->l_name, new_level, lockres->l_blocking, lvb);
 
 	lockres->l_action = OCFS2_AST_DOWNCONVERT;
 	lockres->l_requested = new_level;
 	lockres_or_flags(lockres, OCFS2_LOCK_BUSY);
-}
-
-static int ocfs2_downconvert_lock(struct ocfs2_super *osb,
-				  struct ocfs2_lock_res *lockres,
-				  int new_level,
-				  int lvb)
-{
-	int ret, dlm_flags = LKM_CONVERT;
-	enum dlm_status status;
-
-	mlog_entry_void();
+	spin_unlock(&lockres->l_lock);
 
 	if (lvb)
-		dlm_flags |= LKM_VALBLK;
+		flags |= LKM_VALBLK;
 
 	status = dlmlock(osb->dlm,
 			 new_level,
 			 &lockres->l_lksb,
-			 dlm_flags,
+			 flags,
 			 lockres->l_name,
 			 lockres->l_ops->ast,
 			 lockres,
@@ -2432,23 +2337,16 @@ bail:
 	return ret;
 }
 
-/* returns 1 when the caller should unlock and call dlmunlock */
-static int ocfs2_prepare_cancel_convert(struct ocfs2_super *osb,
-				        struct ocfs2_lock_res *lockres)
+/* called with the spinlock held, and WILL drop it. */
+static int __ocfs2_cancel_convert(struct ocfs2_super *osb,
+				  struct ocfs2_lock_res *lockres)
 {
-	assert_spin_locked(&lockres->l_lock);
+	int ret;
+	enum dlm_status status;
 
 	mlog_entry_void();
-	mlog(0, "lock %s\n", lockres->l_name);
 
-	if (lockres->l_unlock_action == OCFS2_UNLOCK_CANCEL_CONVERT) {
-		/* If we're already trying to cancel a lock conversion
-		 * then just drop the spinlock and allow the caller to
-		 * requeue this lock. */
-
-		mlog(0, "Lockres %s, skip convert\n", lockres->l_name);
-		return 0;
-	}
+	mlog(0, "lock %s\n", lockres->l_name);
 
 	/* were we in a convert when we got the bast fire? */
 	BUG_ON(lockres->l_action != OCFS2_AST_CONVERT &&
@@ -2460,18 +2358,7 @@ static int ocfs2_prepare_cancel_convert(
 	mlog_bug_on_msg(!(lockres->l_flags & OCFS2_LOCK_BUSY),
 			"lock %s, invalid flags: 0x%lx\n",
 			lockres->l_name, lockres->l_flags);
-
-	return 1;
-}
-
-static int ocfs2_cancel_convert(struct ocfs2_super *osb,
-				struct ocfs2_lock_res *lockres)
-{
-	int ret;
-	enum dlm_status status;
-
-	mlog_entry_void();
-	mlog(0, "lock %s\n", lockres->l_name);
+	spin_unlock(&lockres->l_lock);
 
 	ret = 0;
 	status = dlmunlock(osb->dlm,
@@ -2491,6 +2378,25 @@ static int ocfs2_cancel_convert(struct o
 	return ret;
 }
 
+static int ocfs2_cancel_convert(struct ocfs2_super *osb,
+				struct ocfs2_lock_res *lockres)
+{
+	assert_spin_locked(&lockres->l_lock);
+
+	if (lockres->l_unlock_action == OCFS2_UNLOCK_CANCEL_CONVERT) {
+		/* If we're already trying to cancel a lock conversion
+		 * then just drop the spinlock and allow the caller to
+		 * requeue this lock. */
+		spin_unlock(&lockres->l_lock);
+
+		mlog(0, "Lockres %s, skip convert\n", lockres->l_name);
+		return 0;
+	}
+
+	/* this will drop the spinlock for us. */
+	return __ocfs2_cancel_convert(osb, lockres);
+}
+
 static inline int ocfs2_can_downconvert_meta_lock(struct inode *inode,
 						  struct ocfs2_lock_res *lockres,
 						  int new_level)
@@ -2523,13 +2429,11 @@ static int ocfs2_do_unblock_meta(struct 
 	int set_lvb = 0;
 	int ret = 0;
 	struct ocfs2_lock_res *lockres = &OCFS2_I(inode)->ip_meta_lockres;
-	unsigned long flags;
-
 	struct ocfs2_super *osb = OCFS2_SB(inode->i_sb);
 
 	mlog_entry_void();
 
-	spin_lock_irqsave(&lockres->l_lock, flags);
+	spin_lock(&lockres->l_lock);
 
 	BUG_ON(!(lockres->l_flags & OCFS2_LOCK_BLOCKED));
 
@@ -2541,13 +2445,9 @@ static int ocfs2_do_unblock_meta(struct 
 
 	if (lockres->l_flags & OCFS2_LOCK_BUSY) {
 		*requeue = 1;
-		ret = ocfs2_prepare_cancel_convert(osb, lockres);
-		spin_unlock_irqrestore(&lockres->l_lock, flags);
-		if (ret) {
-			ret = ocfs2_cancel_convert(osb, lockres);
-			if (ret < 0)
-				mlog_errno(ret);
-		}
+		ret = ocfs2_cancel_convert(osb, lockres);
+		if (ret < 0)
+			mlog_errno(ret);
 		goto leave;
 	}
 
@@ -2571,20 +2471,19 @@ static int ocfs2_do_unblock_meta(struct 
 			mlog(0, "lockres %s: downconverting stale lock!\n",
 			     lockres->l_name);
 
-		mlog(0, "calling ocfs2_downconvert_lock with l_level=%d, "
-		     "l_blocking=%d, new_level=%d\n",
-		     lockres->l_level, lockres->l_blocking, new_level);
-
-		ocfs2_prepare_downconvert(lockres, new_level);
-		spin_unlock_irqrestore(&lockres->l_lock, flags);
-		ret = ocfs2_downconvert_lock(osb, lockres, new_level, set_lvb);
+		mlog(0, "calling __ocfs2_downconvert_lock with "
+		     "l_level=%d, l_blocking=%d, new_level=%d\n",
+		     lockres->l_level, lockres->l_blocking,
+		     new_level);
+		ret = __ocfs2_downconvert_lock(osb, lockres, new_level,
+					       set_lvb);
 		goto leave;
 	}
 	if (!ocfs2_inode_fully_checkpointed(inode))
 		ocfs2_start_checkpoint(osb);
 
 	*requeue = 1;
-	spin_unlock_irqrestore(&lockres->l_lock, flags);
+	spin_unlock(&lockres->l_lock);
 	ret = 0;
 leave:
 	mlog_exit(ret);
@@ -2596,27 +2495,22 @@ static int ocfs2_generic_unblock_lock(st
 				      int *requeue,
 				      ocfs2_convert_worker_t *worker)
 {
-	unsigned long flags;
 	int blocking;
 	int new_level;
 	int ret = 0;
 
 	mlog_entry_void();
 
-	spin_lock_irqsave(&lockres->l_lock, flags);
+	spin_lock(&lockres->l_lock);
 
 	BUG_ON(!(lockres->l_flags & OCFS2_LOCK_BLOCKED));
 
 recheck:
 	if (lockres->l_flags & OCFS2_LOCK_BUSY) {
 		*requeue = 1;
-		ret = ocfs2_prepare_cancel_convert(osb, lockres);
-		spin_unlock_irqrestore(&lockres->l_lock, flags);
-		if (ret) {
-			ret = ocfs2_cancel_convert(osb, lockres);
-			if (ret < 0)
-				mlog_errno(ret);
-		}
+		ret = ocfs2_cancel_convert(osb, lockres);
+		if (ret < 0)
+			mlog_errno(ret);
 		goto leave;
 	}
 
@@ -2624,7 +2518,7 @@ recheck:
 	 * then requeue. */
 	if ((lockres->l_blocking == LKM_EXMODE)
 	    && (lockres->l_ex_holders || lockres->l_ro_holders)) {
-		spin_unlock_irqrestore(&lockres->l_lock, flags);
+		spin_unlock(&lockres->l_lock);
 		*requeue = 1;
 		ret = 0;
 		goto leave;
@@ -2634,7 +2528,7 @@ recheck:
 	 * requeue if we've got any EX holders */
 	if (lockres->l_blocking == LKM_PRMODE &&
 	    lockres->l_ex_holders) {
-		spin_unlock_irqrestore(&lockres->l_lock, flags);
+		spin_unlock(&lockres->l_lock);
 		*requeue = 1;
 		ret = 0;
 		goto leave;
@@ -2651,11 +2545,11 @@ recheck:
 	 * may sleep, so we save off a copy of what we're blocking as
 	 * it may change while we're not holding the spin lock. */
 	blocking = lockres->l_blocking;
-	spin_unlock_irqrestore(&lockres->l_lock, flags);
+	spin_unlock(&lockres->l_lock);
 
 	worker(lockres, blocking);
 
-	spin_lock_irqsave(&lockres->l_lock, flags);
+	spin_lock(&lockres->l_lock);
 	if (blocking != lockres->l_blocking) {
 		/* If this changed underneath us, then we can't drop
 		 * it just yet. */
@@ -2666,9 +2560,7 @@ downconvert:
 	*requeue = 0;
 	new_level = ocfs2_highest_compat_lock_level(lockres->l_blocking);
 
-	ocfs2_prepare_downconvert(lockres, new_level);
-	spin_unlock_irqrestore(&lockres->l_lock, flags);
-	ret = ocfs2_downconvert_lock(osb, lockres, new_level, 0);
+	ret = __ocfs2_downconvert_lock(osb, lockres, new_level, 0);
 leave:
 	mlog_exit(ret);
 	return ret;
@@ -2733,30 +2625,6 @@ int ocfs2_unblock_data(struct ocfs2_lock
 	return status;
 }
 
-static int ocfs2_unblock_inode_lock(struct ocfs2_lock_res *lockres,
-				    int *requeue)
-{
-	int status;
-	struct inode *inode;
-
-	mlog_entry_void();
-
-	mlog(0, "Unblock lockres %s\n", lockres->l_name);
-
-	inode  = ocfs2_lock_res_inode(lockres);
-
-	status = ocfs2_generic_unblock_lock(OCFS2_SB(inode->i_sb),
-					    lockres,
-					    requeue,
-					    NULL);
-	if (status < 0)
-		mlog_errno(status);
-
-	mlog_exit(status);
-	return status;
-}
-
-
 int ocfs2_unblock_meta(struct ocfs2_lock_res *lockres,
 		       int *requeue)
 {
@@ -2810,7 +2678,6 @@ void ocfs2_process_blocked_lock(struct o
 {
 	int status;
 	int requeue = 0;
-	unsigned long flags;
 
 	/* Our reference to the lockres in this function can be
 	 * considered valid until we remove the OCFS2_LOCK_QUEUED
@@ -2829,16 +2696,16 @@ void ocfs2_process_blocked_lock(struct o
 	 * still be marked with OCFS2_LOCK_FREEING after this check,
 	 * but short circuiting here will still save us some
 	 * performance. */
-	spin_lock_irqsave(&lockres->l_lock, flags);
+	spin_lock(&lockres->l_lock);
 	if (lockres->l_flags & OCFS2_LOCK_FREEING)
 		goto unqueue;
-	spin_unlock_irqrestore(&lockres->l_lock, flags);
+	spin_unlock(&lockres->l_lock);
 
 	status = lockres->l_ops->unblock(lockres, &requeue);
 	if (status < 0)
 		mlog_errno(status);
 
-	spin_lock_irqsave(&lockres->l_lock, flags);
+	spin_lock(&lockres->l_lock);
 unqueue:
 	if (lockres->l_flags & OCFS2_LOCK_FREEING || !requeue) {
 		lockres_clear_flags(lockres, OCFS2_LOCK_QUEUED);
@@ -2847,7 +2714,7 @@ unqueue:
 
 	mlog(0, "lockres %s, requeue = %s.\n", lockres->l_name,
 	     requeue ? "yes" : "no");
-	spin_unlock_irqrestore(&lockres->l_lock, flags);
+	spin_unlock(&lockres->l_lock);
 
 	mlog_exit_void();
 }
@@ -2891,8 +2758,9 @@ void ocfs2_dump_meta_lvb_info(u64 level,
 
 	mlog(level, "LVB information for %s (called from %s:%u):\n",
 	     lockres->l_name, function, line);
-	mlog(level, "version: %u, clusters: %u\n",
-	     be32_to_cpu(lvb->lvb_version), be32_to_cpu(lvb->lvb_iclusters));
+	mlog(level, "old_seq: %u, version: %u, clusters: %u\n",
+	     be32_to_cpu(lvb->lvb_old_seq), be32_to_cpu(lvb->lvb_version),
+	     be32_to_cpu(lvb->lvb_iclusters));
 	mlog(level, "size: %"MLFu64", uid %u, gid %u, mode 0x%x\n",
 	     be64_to_cpu(lvb->lvb_isize), be32_to_cpu(lvb->lvb_iuid),
 	     be32_to_cpu(lvb->lvb_igid), be16_to_cpu(lvb->lvb_imode));
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/dlmglue.h linux-2.6.15-ocfs2/fs/ocfs2/dlmglue.h
--- linux-2.6.15-2/fs/ocfs2/dlmglue.h	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/dlmglue.h	2006-02-06 17:59:23.000000000 -0500
@@ -27,30 +27,23 @@
 #ifndef DLMGLUE_H
 #define DLMGLUE_H
 
-#define OCFS2_LVB_VERSION 2
+#define OCFS2_LVB_VERSION 1
 
 struct ocfs2_meta_lvb {
+	__be32       lvb_old_seq;
 	__be32       lvb_version;
 	__be32       lvb_iclusters;
 	__be32       lvb_iuid;
 	__be32       lvb_igid;
+	__be16       lvb_imode;
+	__be16       lvb_inlink;
 	__be64       lvb_iatime_packed;
 	__be64       lvb_ictime_packed;
 	__be64       lvb_imtime_packed;
 	__be64       lvb_isize;
-	__be16       lvb_imode;
-	__be16       lvb_inlink;
-	__be32       lvb_reserved[3];
+	__be32       lvb_reserved[2];
 };
 
-/* ocfs2_meta_lock_full() and ocfs2_data_lock_full() 'arg_flags' flags */
-/* don't wait on recovery. */
-#define OCFS2_META_LOCK_RECOVERY	(0x01)
-/* Instruct the dlm not to queue ourselves on the other node. */
-#define OCFS2_META_LOCK_NOQUEUE		(0x02)
-/* don't block waiting for the vote thread, instead return -EAGAIN */
-#define OCFS2_LOCK_NONBLOCK		(0x04)
-
 int ocfs2_dlm_init(struct ocfs2_super *osb);
 void ocfs2_dlm_shutdown(struct ocfs2_super *osb);
 void ocfs2_lock_res_init_once(struct ocfs2_lock_res *res);
@@ -60,30 +53,24 @@ void ocfs2_inode_lock_res_init(struct oc
 void ocfs2_lock_res_free(struct ocfs2_lock_res *res);
 int ocfs2_create_new_inode_locks(struct inode *inode);
 int ocfs2_drop_inode_locks(struct inode *inode);
-int ocfs2_data_lock_full(struct inode *inode,
-			 int write,
-			 int arg_flags);
-#define ocfs2_data_lock(inode, write) ocfs2_data_lock_full(inode, write, 0)
-int ocfs2_data_lock_with_page(struct inode *inode,
-			      int write,
-			      struct page *page);
+int ocfs2_data_lock(struct inode *inode,
+		    int write);
 void ocfs2_data_unlock(struct inode *inode,
 		       int write);
-int ocfs2_rw_lock(struct inode *inode, int write);
-void ocfs2_rw_unlock(struct inode *inode, int write);
+/* don't wait on recovery. */
+#define OCFS2_META_LOCK_RECOVERY	(0x01)
+/* Instruct the dlm not to queue ourselves on the other node. */
+#define OCFS2_META_LOCK_NOQUEUE		(0x02)
 int ocfs2_meta_lock_full(struct inode *inode,
 			 struct ocfs2_journal_handle *handle,
 			 struct buffer_head **ret_bh,
 			 int ex,
-			 int arg_flags);
-int ocfs2_meta_lock_with_page(struct inode *inode,
-			      struct ocfs2_journal_handle *handle,
-			      struct buffer_head **ret_bh,
-			      int ex,
-			      struct page *page);
+			 int flags,
+			 ocfs2_lock_callback cb,
+			 unsigned long cb_data);
 /* 99% of the time we don't want to supply any additional flags --
  * those are for very specific cases only. */
-#define ocfs2_meta_lock(i, h, b, e) ocfs2_meta_lock_full(i, h, b, e, 0)
+#define ocfs2_meta_lock(i, h, b, e) ocfs2_meta_lock_full(i, h, b, e, 0, NULL, 0)
 void ocfs2_meta_unlock(struct inode *inode,
 		       int ex);
 int ocfs2_super_lock(struct ocfs2_super *osb,
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/extent_map.c linux-2.6.15-ocfs2/fs/ocfs2/extent_map.c
--- linux-2.6.15-2/fs/ocfs2/extent_map.c	2006-02-06 18:15:15.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/extent_map.c	2006-02-06 17:59:23.000000000 -0500
@@ -69,9 +69,6 @@ ocfs2_extent_map_lookup(struct ocfs2_ext
 			u32 cpos, u32 clusters,
 			struct rb_node ***ret_p,
 			struct rb_node **ret_parent);
-static int ocfs2_extent_map_insert(struct inode *inode,
-				   struct ocfs2_extent_rec *rec,
-				   int tree_depth);
 static int ocfs2_extent_map_insert_entry(struct ocfs2_extent_map *em,
 					 struct ocfs2_extent_map_entry *ent);
 static int ocfs2_extent_map_find_leaf(struct inode *inode,
@@ -179,27 +176,21 @@ static int ocfs2_extent_map_find_leaf(st
 				   le32_to_cpu(rec->e_clusters));
 
 			ret = -EBADR;
-			if (rec_end > OCFS2_I(inode)->ip_clusters) {
-				mlog_errno(ret);
+			if (rec_end > OCFS2_I(inode)->ip_clusters)
 				goto out_free;
-			}
 
 			if (rec_end <= cpos) {
 				ret = ocfs2_extent_map_insert(inode, rec,
 						le16_to_cpu(el->l_tree_depth));
-				if (ret && (ret != -EEXIST)) {
-					mlog_errno(ret);
+				if (ret && (ret != -EEXIST))
 					goto out_free;
-				}
 				continue;
 			}
 			if ((cpos + clusters) <= le32_to_cpu(rec->e_cpos)) {
 				ret = ocfs2_extent_map_insert(inode, rec,
 						le16_to_cpu(el->l_tree_depth));
-				if (ret && (ret != -EEXIST)) {
-					mlog_errno(ret);
+				if (ret && (ret != -EEXIST))
 					goto out_free;
-				}
 				continue;
 			}
 
@@ -213,10 +204,8 @@ static int ocfs2_extent_map_find_leaf(st
 			ret = -ESRCH;
 			if (!ocfs2_extent_rec_contains_clusters(rec,
 							        cpos,
-								clusters)) {
-				mlog_errno(ret);
+								clusters))
 				goto out_free;
-			}
 
 			/*
 			 * If we've already found a record, the el has
@@ -224,10 +213,8 @@ static int ocfs2_extent_map_find_leaf(st
 			 * EEEK!
 			 */
 			ret = -EBADR;
-			if (blkno) {
-				mlog_errno(ret);
+			if (blkno)
 				goto out_free;
-			}
 
 			blkno = le64_to_cpu(rec->e_blkno);
 		}
@@ -237,10 +224,8 @@ static int ocfs2_extent_map_find_leaf(st
 		 * in the branches, so we'd better have found someone
 		 */
 		ret = -EBADR;
-		if (!blkno) {
-			mlog_errno(ret);
+		if (!blkno)
 			goto out_free;
-		}
 
 		if (eb_bh) {
 			brelse(eb_bh);
@@ -249,10 +234,8 @@ static int ocfs2_extent_map_find_leaf(st
 		ret = ocfs2_read_block(OCFS2_SB(inode->i_sb),
 				       blkno, &eb_bh, OCFS2_BH_CACHED,
 				       inode);
-		if (ret) {
-			mlog_errno(ret);
+		if (ret)
 			goto out_free;
-		}
 		eb = (struct ocfs2_extent_block *)eb_bh->b_data;
 		if (!OCFS2_IS_VALID_EXTENT_BLOCK(eb)) {
 			OCFS2_RO_ON_INVALID_EXTENT_BLOCK(inode->i_sb, eb);
@@ -262,16 +245,15 @@ static int ocfs2_extent_map_find_leaf(st
 		el = &eb->h_list;
 	}
 
-	BUG_ON(el->l_tree_depth);
+	if (el->l_tree_depth)
+		BUG();
 
 	for (i = 0; i < le16_to_cpu(el->l_next_free_rec); i++) {
 		rec = &el->l_recs[i];
 		ret = ocfs2_extent_map_insert(inode, rec,
 					      le16_to_cpu(el->l_tree_depth));
-		if (ret) {
-			mlog_errno(ret);
+		if (ret)
 			goto out_free;
-		}
 	}
 
 	ret = 0;
@@ -316,7 +298,6 @@ static int ocfs2_extent_map_lookup_read(
 		ret = ocfs2_read_block(OCFS2_SB(inode->i_sb), blkno, &bh,
 				       OCFS2_BH_CACHED, inode);
 		if (ret) {
-			mlog_errno(ret);
 			if (bh)
 				brelse(bh);
 			return ret;
@@ -335,7 +316,6 @@ static int ocfs2_extent_map_lookup_read(
 				       OCFS2_I(inode)->ip_blkno, &bh,
 				       OCFS2_BH_CACHED, inode);
 		if (ret) {
-			mlog_errno(ret);
 			if (bh)
 				brelse(bh);
 			return ret;
@@ -351,20 +331,15 @@ static int ocfs2_extent_map_lookup_read(
 
 	ret = ocfs2_extent_map_find_leaf(inode, cpos, clusters, el);
 	brelse(bh);
-	if (ret) {
-		mlog_errno(ret);
+	if (ret)
 		return ret;
-	}
 
 	ent = ocfs2_extent_map_lookup(em, cpos, clusters, NULL, NULL);
-	if (!ent) {
-		ret = -ESRCH;
-		mlog_errno(ret);
-		return ret;
-	}
+	if (!ent)
+		return -ESRCH;
 
-	/* FIXME: Make sure this isn't a corruption */
-	BUG_ON(ent->e_tree_depth);
+	if (ent->e_tree_depth)
+		BUG();  /* FIXME: Make sure this isn't a corruption */
 
 	*ret_ent = ent;
 
@@ -422,7 +397,8 @@ static int ocfs2_extent_map_try_insert(s
 					  le32_to_cpu(rec->e_clusters), NULL,
 					  NULL);
 
-	BUG_ON(!old_ent);
+	if (!old_ent)
+		BUG();
 
 	ret = -EEXIST;
 	if (old_ent->e_tree_depth < tree_depth)
@@ -506,28 +482,21 @@ out_unlock:
 }
 
 
-static int ocfs2_extent_map_insert(struct inode *inode,
-				   struct ocfs2_extent_rec *rec,
-				   int tree_depth)
+int ocfs2_extent_map_insert(struct inode *inode, struct ocfs2_extent_rec *rec,
+			    int tree_depth)
 {
 	int ret;
 	struct ocfs2_em_insert_context ctxt = {0, };
 
 	if ((le32_to_cpu(rec->e_cpos) + le32_to_cpu(rec->e_clusters)) >
-	    OCFS2_I(inode)->ip_map.em_clusters) {
-		ret = -EBADR;
-		mlog_errno(ret);
-		return ret;
-	}
+	    OCFS2_I(inode)->ip_map.em_clusters)
+		return -EBADR;
 
 	/* Zero e_clusters means a truncated tail record.  It better be EOF */
 	if (!rec->e_clusters) {
 		if ((le32_to_cpu(rec->e_cpos) + le32_to_cpu(rec->e_clusters)) !=
-		    OCFS2_I(inode)->ip_map.em_clusters) {
-			ret = -EBADR;
-			mlog_errno(ret);
-			return ret;
-		}
+		    OCFS2_I(inode)->ip_map.em_clusters)
+			return -EBADR;
 
 		/* Ignore the truncated tail */
 		return 0;
@@ -536,10 +505,8 @@ static int ocfs2_extent_map_insert(struc
 	ret = -ENOMEM;
 	ctxt.new_ent = kmem_cache_alloc(ocfs2_em_ent_cachep,
 					GFP_KERNEL);
-	if (!ctxt.new_ent) {
-		mlog_errno(ret);
+	if (!ctxt.new_ent)
 		return ret;
-	}
 
 	ctxt.new_ent->e_rec = *rec;
 	ctxt.new_ent->e_tree_depth = tree_depth;
@@ -565,9 +532,6 @@ static int ocfs2_extent_map_insert(struc
 						  tree_depth, &ctxt);
 	} while (ret == -EAGAIN);
 
-	if (ret < 0)
-		mlog_errno(ret);
-
 	if (ctxt.left_ent)
 		kmem_cache_free(ocfs2_em_ent_cachep, ctxt.left_ent);
 	if (ctxt.right_ent)
@@ -666,15 +630,10 @@ int ocfs2_extent_map_append(struct inode
 
 	if (ret == -ENOENT)
 		ret = ocfs2_extent_map_insert(inode, rec, 0);
-	if (ret < 0)
-		mlog_errno(ret);
+
 	return ret;
 }
 
-#if 0
-/* Code here is included but defined out as it completes the extent
- * map api and may be used in the future. */
-
 /*
  * Look up the record containing this cluster offset.  This record is
  * part of the extent map.  Do not free it.  Any changes you make to
@@ -778,8 +737,6 @@ int ocfs2_extent_map_get_clusters(struct
 	return -ENOENT;
 }
 
-#endif  /*  0  */
-
 int ocfs2_extent_map_get_blocks(struct inode *inode,
 				u64 v_blkno, int count,
 				u64 *p_blkno, int *ret_count)
@@ -797,11 +754,8 @@ int ocfs2_extent_map_get_blocks(struct i
 	cpos = ocfs2_blocks_to_clusters(inode->i_sb, v_blkno);
 	clusters = ocfs2_blocks_to_clusters(inode->i_sb,
 					    (u64)count + bpc - 1);
-	if ((cpos + clusters) > OCFS2_I(inode)->ip_clusters) {
-		ret = -EINVAL;
-		mlog_errno(ret);
-		return ret;
-	}
+	if ((cpos + clusters) > OCFS2_I(inode)->ip_clusters)
+		return -EINVAL;
 
 	if ((cpos + clusters) > em->em_clusters) {
 		/*
@@ -814,21 +768,16 @@ int ocfs2_extent_map_get_blocks(struct i
 	}
 
 	ret = ocfs2_extent_map_lookup_read(inode, cpos, clusters, &ent);
-	if (ret) {
-		mlog_errno(ret);
+	if (ret)
 		return ret;
-	}
 
 	if (ent)
 	{
 		rec = &ent->e_rec;
 
 		/* We should never find ourselves straddling an interval */
-		if (!ocfs2_extent_rec_contains_clusters(rec, cpos, clusters)) {
-			ret = -ESRCH;
-			mlog_errno(ret);
-			return ret;
-		}
+		if (!ocfs2_extent_rec_contains_clusters(rec, cpos, clusters))
+			return -ESRCH;
 
 		boff = ocfs2_clusters_to_blocks(inode->i_sb, cpos -
 						le32_to_cpu(rec->e_cpos));
@@ -986,7 +935,7 @@ int __init init_ocfs2_extent_maps(void)
 	return 0;
 }
 
-void exit_ocfs2_extent_maps(void)
+void __exit exit_ocfs2_extent_maps(void)
 {
 	kmem_cache_destroy(ocfs2_em_ent_cachep);
 }
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/extent_map.h linux-2.6.15-ocfs2/fs/ocfs2/extent_map.h
--- linux-2.6.15-2/fs/ocfs2/extent_map.h	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/extent_map.h	2006-02-06 17:59:23.000000000 -0500
@@ -34,9 +34,16 @@ void exit_ocfs2_extent_maps(void);
  * in the process of being updated.
  */
 int ocfs2_extent_map_init(struct inode *inode);
-int ocfs2_extent_map_append(struct inode *inode,
-			    struct ocfs2_extent_rec *rec,
+int ocfs2_extent_map_insert(struct inode *inode, struct ocfs2_extent_rec *rec,
+			    int tree_depth);
+int ocfs2_extent_map_append(struct inode *inode, struct ocfs2_extent_rec *rec,
 			    u32 new_clusters);
+int ocfs2_extent_map_get_rec(struct inode *inode, u32 cpos,
+			     struct ocfs2_extent_rec **rec,
+			     int *tree_depth);
+int ocfs2_extent_map_get_clusters(struct inode *inode,
+				  u32 v_cpos, int count,
+			  	  u32 *p_cpos, int *ret_count);
 int ocfs2_extent_map_get_blocks(struct inode *inode,
 				u64 v_blkno, int count,
 				u64 *p_blkno, int *ret_count);
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/file.c linux-2.6.15-ocfs2/fs/ocfs2/file.c
--- linux-2.6.15-2/fs/ocfs2/file.c	2006-02-06 18:15:15.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/file.c	2006-02-06 17:59:39.000000000 -0500
@@ -23,7 +23,6 @@
  * Boston, MA 021110-1307, USA.
  */
 
-#include <linux/capability.h>
 #include <linux/fs.h>
 #include <linux/types.h>
 #include <linux/slab.h>
@@ -36,8 +35,8 @@
 
 #include "ocfs2.h"
 
+#include "aio.h"
 #include "alloc.h"
-#include "aops.h"
 #include "dir.h"
 #include "dlmglue.h"
 #include "extent_map.h"
@@ -51,7 +50,13 @@
 
 #include "buffer_head_io.h"
 
-static int ocfs2_sync_inode(struct inode *inode)
+static int ocfs2_zero_extend(struct inode *inode);
+static int ocfs2_orphan_for_truncate(struct ocfs2_super *osb,
+				     struct inode *inode,
+				     struct buffer_head *fe_bh,
+				     u64 new_i_size);
+
+int ocfs2_sync_inode(struct inode *inode)
 {
 	filemap_fdatawrite(inode->i_mapping);
 	return sync_mapping_buffers(inode->i_mapping);
@@ -98,6 +103,17 @@ static int ocfs2_file_release(struct ino
 		       file->f_dentry->d_name.name);
 
 	spin_lock(&oi->ip_lock);
+#ifdef OCFS2_DELETE_INODE_WORKAROUND
+	/* Do the sync *before* decrementing ip_open_count as
+	 * otherwise the voting code might allow this inode to be
+	 * wiped. */
+	if (oi->ip_open_count == 1 &&
+	    oi->ip_flags & OCFS2_INODE_MAYBE_ORPHANED) {
+		spin_unlock(&oi->ip_lock);
+		write_inode_now(inode, 1);
+		spin_lock(&oi->ip_lock);
+	}
+#endif
 	if (!--oi->ip_open_count)
 		oi->ip_flags &= ~OCFS2_INODE_OPEN_DIRECT;
 	spin_unlock(&oi->ip_lock);
@@ -132,55 +148,286 @@ bail:
 	return (err < 0) ? -EIO : 0;
 }
 
-int ocfs2_set_inode_size(struct ocfs2_journal_handle *handle,
-			 struct inode *inode,
-			 struct buffer_head *fe_bh,
-			 u64 new_i_size)
+static void ocfs2_update_inode_size(struct inode *inode,
+				    u64 new_size)
+{
+	i_size_write(inode, new_size);
+	inode->i_blocks = ocfs2_align_bytes_to_sectors(new_size);
+}
+
+void ocfs2_file_finish_extension(struct inode *inode,
+				 loff_t newsize,
+				 unsigned direct_extend)
 {
 	int status;
 
-	mlog_entry_void();
-	i_size_write(inode, new_i_size);
-	inode->i_blocks = ocfs2_align_bytes_to_sectors(new_i_size);
-	inode->i_ctime = inode->i_mtime = CURRENT_TIME;
+	mlog(0, "inode %"MLFu64", newsize = %lld, direct_extend = %u\n",
+	     OCFS2_I(inode)->ip_blkno, (long long)newsize, direct_extend);
 
-	status = ocfs2_mark_inode_dirty(handle, inode, fe_bh);
-	if (status < 0) {
-		mlog_errno(status);
+	ocfs2_update_inode_size(inode, newsize);
+
+#ifdef OCFS2_ORACORE_WORKAROUNDS
+	if (direct_extend) {
+		/*
+		 * This leaves dirty data in holes.
+		 * Caveat Emptor.
+		 */
+		OCFS2_I(inode)->ip_mmu_private = newsize;
+		return;
+	}
+#endif
+
+	status = ocfs2_zero_extend(inode);
+	/*
+	 * Don't overwrite the result of
+	 * generic_file_write
+	 */
+	if (status)
+		mlog(ML_ERROR, "Unable to pre-zero extension of inode "
+		     "(%d)\n", status);
+}
+
+static ssize_t ocfs2_file_write(struct file *filp,
+				const char __user *buf,
+				size_t count,
+				loff_t *ppos)
+{
+	struct iovec local_iov = { .iov_base = (void __user *)buf,
+				   .iov_len = count };
+	int ret = 0;
+	struct ocfs2_super *osb = NULL;
+	struct dentry *dentry = filp->f_dentry;
+	struct inode *inode = dentry->d_inode;
+	struct ocfs2_write_lock_info info = {0, };
+	DECLARE_BUFFER_LOCK_CTXT(ctxt);
+
+	mlog_entry("(0x%p, 0x%p, %u, '%.*s')\n", filp, buf,
+		   (unsigned int)count,
+		   filp->f_dentry->d_name.len,
+		   filp->f_dentry->d_name.name);
+
+	/* happy write of zero bytes */
+	if (count == 0) {
+		ret = 0;
+		goto bail;
+	}
+
+	if (!inode) {
+		mlog(0, "bad inode\n");
+		ret = -EIO;
+		goto bail;
+	}
+
+	osb = OCFS2_SB(inode->i_sb);
+
+	ret = ocfs2_write_lock_maybe_extend(filp, buf, count, ppos, &info,
+					    &ctxt);
+	if (ret)
+		goto bail;
+
+	down_read(&OCFS2_I(inode)->ip_alloc_sem);
+
+#ifdef OCFS2_ORACORE_WORKAROUNDS
+	if (osb->s_mount_opt & OCFS2_MOUNT_COMPAT_OCFS) {
+		unsigned int saved_flags = filp->f_flags;
+
+		if (info.wl_do_direct_io)
+			filp->f_flags |= O_DIRECT;
+		else
+			filp->f_flags &= ~O_DIRECT;
+
+		ret = generic_file_write_nolock(filp, &local_iov, 1, ppos);
+
+		filp->f_flags = saved_flags;
+	} else
+#endif
+		ret = generic_file_write_nolock(filp, &local_iov, 1, ppos);
+
+	up_read(&OCFS2_I(inode)->ip_alloc_sem);
+
+bail:
+	/* we might have to finish up extentions that were performed before
+	 * an error was returned by, say, data locking */
+	if (info.wl_extended)
+		ocfs2_file_finish_extension(inode, info.wl_newsize,
+					    info.wl_do_direct_io);
+	if (info.wl_unlock_ctxt)
+		ocfs2_unlock_buffer_inodes(&ctxt);
+	if (info.wl_have_i_mutex)
+		mutex_unlock(&inode->i_mutex);
+	mlog_exit(ret);
+
+	return ret;
+}
+
+static ssize_t ocfs2_file_read(struct file *filp,
+			       char __user *buf,
+			       size_t count,
+			       loff_t *ppos)
+{
+	int ret = 0;
+	struct ocfs2_super *osb = NULL;
+	struct dentry *dentry = filp->f_dentry;
+	struct inode *inode = dentry->d_inode;
+	struct ocfs2_backing_inode *target_binode;
+	DECLARE_BUFFER_LOCK_CTXT(ctxt);
+
+	mlog_entry("(0x%p, 0x%p, %u, '%.*s')\n", filp, buf,
+		   (unsigned int)count,
+		   filp->f_dentry->d_name.len,
+		   filp->f_dentry->d_name.name);
+
+	if (!inode) {
+		ret = -EINVAL;
+		mlog_errno(ret);
+		goto bail;
+	}
+
+	osb = OCFS2_SB(inode->i_sb);
+
+#ifdef OCFS2_ORACORE_WORKAROUNDS
+	if (osb->s_mount_opt & OCFS2_MOUNT_COMPAT_OCFS) {
+		if (filp->f_flags & O_DIRECT) {
+			int sector_size = 1 << osb->s_sectsize_bits;
+
+			if (((*ppos) & (sector_size - 1)) ||
+			    (count & (sector_size - 1)) ||
+			    ((unsigned long)buf & (sector_size - 1)) ||
+			    (i_size_read(inode) & (sector_size -1))) {
+				filp->f_flags &= ~O_DIRECT;
+			}
+		}
+	}
+#endif
+
+	ret = ocfs2_setup_io_locks(inode->i_sb, inode, buf, count, &ctxt,
+				   &target_binode);
+	if (ret < 0) {
+		mlog_errno(ret);
 		goto bail;
 	}
 
+	target_binode->ba_lock_data = (filp->f_flags & O_DIRECT) ? 0 : 1;
+
+	ret = ocfs2_lock_buffer_inodes(&ctxt, NULL);
+	if (ret < 0) {
+		mlog_errno(ret);
+		goto bail_unlock;
+	}
+
+	down_read(&OCFS2_I(inode)->ip_alloc_sem);
+
+	ret = generic_file_read(filp, buf, count, ppos);
+
+	up_read(&OCFS2_I(inode)->ip_alloc_sem);
+
+	if (ret == -EINVAL)
+		mlog(ML_ERROR, "Generic_file_read returned -EINVAL\n");
+
+bail_unlock:
+	ocfs2_unlock_buffer_inodes(&ctxt);
+
 bail:
-	mlog_exit(status);
-	return status;
+	mlog_exit(ret);
+
+	return ret;
 }
 
-static int ocfs2_simple_size_update(struct inode *inode,
-				    struct buffer_head *di_bh,
-				    u64 new_i_size)
+static ssize_t ocfs2_file_sendfile(struct file *in_file,
+				   loff_t *ppos,
+				   size_t count,
+				   read_actor_t actor,
+				   void *target)
 {
 	int ret;
-	struct ocfs2_super *osb = OCFS2_SB(inode->i_sb);
-	struct ocfs2_journal_handle *handle = NULL;
+	struct inode *inode = in_file->f_mapping->host;
 
-	handle = ocfs2_start_trans(osb, NULL,
-				   OCFS2_INODE_UPDATE_CREDITS);
-	if (handle == NULL) {
-		ret = -ENOMEM;
+	mlog_entry("inode %"MLFu64", ppos %lld, count = %u\n",
+		   OCFS2_I(inode)->ip_blkno, (long long) *ppos,
+		   (unsigned int) count);
+
+	/* Obviously, there is no user buffer to worry about here --
+	 * this simplifies locking, so no need to walk vmas a la
+	 * read/write. We take a simple set of cluster locks against
+	 * the inode and call generic_file_sendfile. */
+	ret = ocfs2_meta_lock(inode, NULL, NULL, 0);
+	if (ret < 0) {
 		mlog_errno(ret);
-		goto out;
+		goto bail;
+	}
+
+	ret = ocfs2_data_lock(inode, 0);
+	if (ret < 0) {
+		mlog_errno(ret);
+		goto bail_unlock_meta;
 	}
 
-	ret = ocfs2_set_inode_size(handle, inode, di_bh,
-				   new_i_size);
+	down_read(&OCFS2_I(inode)->ip_alloc_sem);
+
+	ret = generic_file_sendfile(in_file, ppos, count, actor, target);
 	if (ret < 0)
 		mlog_errno(ret);
 
-	ocfs2_commit_trans(handle);
-out:
+	up_read(&OCFS2_I(inode)->ip_alloc_sem);
+
+	ocfs2_data_unlock(inode, 0);
+bail_unlock_meta:
+	ocfs2_meta_unlock(inode, 0);
+
+bail:
+	mlog_exit(ret);
 	return ret;
 }
 
+struct file_operations ocfs2_fops = {
+	.read		= ocfs2_file_read,
+	.write		= ocfs2_file_write,
+	.sendfile	= ocfs2_file_sendfile,
+	.mmap		= ocfs2_mmap,
+	.fsync		= ocfs2_sync_file,
+	.release	= ocfs2_file_release,
+	.open		= ocfs2_file_open,
+	.aio_read	= ocfs2_file_aio_read,
+	.aio_write	= ocfs2_file_aio_write,
+};
+
+struct file_operations ocfs2_dops = {
+	.read		= generic_read_dir,
+	.readdir	= ocfs2_readdir,
+	.fsync		= ocfs2_sync_file,
+};
+
+int ocfs2_set_inode_size(struct ocfs2_journal_handle *handle,
+			 struct inode *inode,
+			 struct buffer_head *fe_bh,
+			 u64 new_i_size)
+{
+	int status, grow;
+
+	mlog_entry_void();
+
+	grow = new_i_size > inode->i_size;
+	i_size_write(inode, new_i_size);
+	inode->i_blocks = ocfs2_align_bytes_to_sectors(new_i_size);
+	inode->i_ctime = inode->i_mtime = CURRENT_TIME;
+
+	status = ocfs2_mark_inode_dirty(handle, inode, fe_bh);
+	if (status < 0) {
+		mlog_errno(status);
+		goto bail;
+	}
+
+	/* FIXME: I think this should all be in the caller */
+	spin_lock(&OCFS2_I(inode)->ip_lock);
+	if (!grow)
+		OCFS2_I(inode)->ip_mmu_private = i_size_read(inode);
+	spin_unlock(&OCFS2_I(inode)->ip_lock);
+
+bail:
+	mlog_exit(status);
+	return status;
+}
+
 static int ocfs2_orphan_for_truncate(struct ocfs2_super *osb,
 				     struct inode *inode,
 				     struct buffer_head *fe_bh,
@@ -211,13 +458,14 @@ out:
 	return status;
 }
 
-static int ocfs2_truncate_file(struct inode *inode,
-			       struct buffer_head *di_bh,
-			       u64 new_i_size)
+static int ocfs2_truncate_file(struct ocfs2_super *osb,
+			       u64 new_i_size,
+			       struct inode *inode)
 {
 	int status = 0;
 	struct ocfs2_dinode *fe = NULL;
-	struct ocfs2_super *osb = OCFS2_SB(inode->i_sb);
+	struct buffer_head *fe_bh = NULL;
+	struct ocfs2_journal_handle *handle = NULL;
 	struct ocfs2_truncate_context *tc = NULL;
 
 	mlog_entry("(inode = %"MLFu64", new_i_size = %"MLFu64"\n",
@@ -225,13 +473,19 @@ static int ocfs2_truncate_file(struct in
 
 	truncate_inode_pages(inode->i_mapping, new_i_size);
 
-	fe = (struct ocfs2_dinode *) di_bh->b_data;
+	status = ocfs2_read_block(osb, OCFS2_I(inode)->ip_blkno, &fe_bh,
+				  OCFS2_BH_CACHED, inode);
+	if (status < 0) {
+		mlog_errno(status);
+		goto bail;
+	}
+
+	fe = (struct ocfs2_dinode *) fe_bh->b_data;
 	if (!OCFS2_IS_VALID_DINODE(fe)) {
 		OCFS2_RO_ON_INVALID_DINODE(inode->i_sb, fe);
 		status = -EIO;
 		goto bail;
 	}
-
 	mlog_bug_on_msg(le64_to_cpu(fe->i_size) != i_size_read(inode),
 			"Inode %"MLFu64", inode i_size = %lld != di "
 			"i_size = %"MLFu64", i_flags = 0x%x\n",
@@ -262,7 +516,17 @@ static int ocfs2_truncate_file(struct in
 		     fe->i_clusters);
 		/* No allocation change is required, so lets fast path
 		 * this truncate. */
-		status = ocfs2_simple_size_update(inode, di_bh, new_i_size);
+		handle = ocfs2_start_trans(osb, NULL,
+					  OCFS2_INODE_UPDATE_CREDITS);
+		if (IS_ERR(handle)) {
+			status = PTR_ERR(handle);
+			handle = NULL;
+			mlog_errno(status);
+			goto bail;
+		}
+
+		status = ocfs2_set_inode_size(handle, inode, fe_bh,
+					      new_i_size);
 		if (status < 0)
 			mlog_errno(status);
 		goto bail;
@@ -280,19 +544,19 @@ static int ocfs2_truncate_file(struct in
 	 * change. Orphan the inode so that recovery can complete the
 	 * truncate if necessary. This does the task of marking
 	 * i_size. */
-	status = ocfs2_orphan_for_truncate(osb, inode, di_bh, new_i_size);
+	status = ocfs2_orphan_for_truncate(osb, inode, fe_bh, new_i_size);
 	if (status < 0) {
 		mlog_errno(status);
 		goto bail;
 	}
 
-	status = ocfs2_prepare_truncate(osb, inode, di_bh, &tc);
+	status = ocfs2_prepare_truncate(osb, inode, fe_bh, &tc);
 	if (status < 0) {
 		mlog_errno(status);
 		goto bail;
 	}
 
-	status = ocfs2_commit_truncate(osb, inode, di_bh, tc);
+	status = ocfs2_commit_truncate(osb, inode, fe_bh, tc);
 	if (status < 0) {
 		mlog_errno(status);
 		goto bail;
@@ -300,34 +564,74 @@ static int ocfs2_truncate_file(struct in
 
 	/* TODO: orphan dir cleanup here. */
 bail:
+	if (handle)
+		ocfs2_commit_trans(handle);
+
+	if (fe_bh)
+		brelse(fe_bh);
 
 	mlog_exit(status);
 	return status;
 }
 
+static int ocfs2_zero_extend(struct inode *inode)
+{
+	struct address_space *mapping = inode->i_mapping;
+	struct page *page;
+	u64 size = i_size_read(inode) - 1;
+	unsigned int offset;
+	int res = 0;
+
+	/* Start the zeroing of blocks */
+	if (i_size_read(inode) > OCFS2_I(inode)->ip_mmu_private) {
+		page = grab_cache_page(mapping,
+				       size >> PAGE_CACHE_SHIFT);
+		if (!page) {
+			res = -ENOMEM;
+			mlog_errno(res);
+			return res;
+		}
+		offset = (unsigned int)(size & (PAGE_CACHE_SIZE - 1)) + 1;
+		res = mapping->a_ops->prepare_write(NULL, page, offset,
+						    offset);
+		if (res < 0) {
+			mlog_errno(res);
+			goto bail_unlock;
+		}
+
+		res = mapping->a_ops->commit_write(NULL, page, offset, offset);
+		if (res < 0)
+			mlog_errno(res);
+
+bail_unlock:
+		unlock_page(page);
+		page_cache_release(page);
+		mark_inode_dirty(inode);
+	}
+
+	return res;
+}
+
 /*
  * extend allocation only here.
  * we'll update all the disk stuff, and oip->alloc_size
  *
  * expect stuff to be locked, a transaction started and enough data /
- * metadata reservations in the contexts.
- *
- * Will return -EAGAIN, and a reason if a restart is needed.
- * If passed in, *reason will always be set, even in error.
+ * metadata reservations in the contexts. I'll return -EAGAIN, if we
+ * run out of transaction credits, so the caller can restart us.
  */
-int ocfs2_do_extend_allocation(struct ocfs2_super *osb,
-			       struct inode *inode,
-			       u32 clusters_to_add,
-			       struct buffer_head *fe_bh,
-			       struct ocfs2_journal_handle *handle,
-			       struct ocfs2_alloc_context *data_ac,
-			       struct ocfs2_alloc_context *meta_ac,
-			       enum ocfs2_alloc_restarted *reason_ret)
+int ocfs2_extend_allocation(struct ocfs2_super *osb,
+			    struct inode *inode,
+			    u32 clusters_to_add,
+			    struct buffer_head *fe_bh,
+			    struct ocfs2_journal_handle *handle,
+			    struct ocfs2_alloc_context *data_ac,
+			    struct ocfs2_alloc_context *meta_ac,
+			    enum ocfs2_alloc_restarted *reason)
 {
 	int status = 0;
 	int free_extents;
 	struct ocfs2_dinode *fe = (struct ocfs2_dinode *) fe_bh->b_data;
-	enum ocfs2_alloc_restarted reason = RESTART_NONE;
 	u32 bit_off, num_bits;
 	u64 block;
 
@@ -348,14 +652,16 @@ int ocfs2_do_extend_allocation(struct oc
 	if (!free_extents && !meta_ac) {
 		mlog(0, "we haven't reserved any metadata!\n");
 		status = -EAGAIN;
-		reason = RESTART_META;
+		if (reason)
+			*reason = RESTART_META;
 		goto leave;
 	} else if ((!free_extents)
 		   && (ocfs2_alloc_context_bits_left(meta_ac)
 		       < ocfs2_extend_meta_needed(fe))) {
 		mlog(0, "filesystem is really fragmented...\n");
 		status = -EAGAIN;
-		reason = RESTART_META;
+		if (reason)
+			*reason = RESTART_META;
 		goto leave;
 	}
 
@@ -404,33 +710,57 @@ int ocfs2_do_extend_allocation(struct oc
 		mlog(0, "need to alloc once more, clusters = %u, wanted = "
 		     "%u\n", fe->i_clusters, clusters_to_add);
 		status = -EAGAIN;
-		reason = RESTART_TRANS;
+		if (reason)
+			*reason = RESTART_TRANS;
 	}
 
 leave:
 	mlog_exit(status);
-	if (reason_ret)
-		*reason_ret = reason;
 	return status;
 }
 
-static int ocfs2_extend_allocation(struct inode *inode,
-				   u32 clusters_to_add)
+/*
+ * Ok, this function is heavy on the goto's - we need to clean it up a
+ * bit.
+ *
+ * *bytes_extended is a measure of how much was added to
+ * dinode->i_size, NOT how much allocated was actually added to the
+ * file. It will always be correct, even when we return an error.
+ */
+int ocfs2_extend_file(struct ocfs2_super *osb,
+		      struct inode *inode,
+		      u64 new_i_size,
+		      u64 *bytes_extended)
 {
 	int status = 0;
 	int restart_func = 0;
 	int drop_alloc_sem = 0;
 	int credits, num_free_extents;
-	u32 prev_clusters;
+	u32 clusters_to_add;
+	u64 new_fe_size;
 	struct buffer_head *bh = NULL;
-	struct ocfs2_dinode *fe = NULL;
+	struct ocfs2_dinode *fe;
 	struct ocfs2_journal_handle *handle = NULL;
 	struct ocfs2_alloc_context *data_ac = NULL;
 	struct ocfs2_alloc_context *meta_ac = NULL;
 	enum ocfs2_alloc_restarted why;
-	struct ocfs2_super *osb = OCFS2_SB(inode->i_sb);
 
-	mlog_entry("(clusters_to_add = %u)\n", clusters_to_add);
+	mlog_entry("(Inode %"MLFu64" new_i_size=%"MLFu64")\n",
+		   OCFS2_I(inode)->ip_blkno, new_i_size);
+
+	*bytes_extended = 0;
+
+	/* setattr sometimes calls us like this. */
+	if (new_i_size == 0)
+		goto leave;
+
+restart_all:
+	handle = ocfs2_alloc_handle(osb);
+	if (handle == NULL) {
+		status = -ENOMEM;
+		mlog_errno(status);
+		goto leave;
+	}
 
 	status = ocfs2_read_block(osb, OCFS2_I(inode)->ip_blkno, &bh,
 				  OCFS2_BH_CACHED, inode);
@@ -445,21 +775,31 @@ static int ocfs2_extend_allocation(struc
 		status = -EIO;
 		goto leave;
 	}
+	mlog_bug_on_msg(i_size_read(inode) !=
+			(le64_to_cpu(fe->i_size) - *bytes_extended),
+			"Inode %"MLFu64" i_size = %lld, dinode i_size "
+			"= %"MLFu64", bytes_extended = %"MLFu64", new_i_size "
+			"= %"MLFu64"\n", OCFS2_I(inode)->ip_blkno,
+			i_size_read(inode), le64_to_cpu(fe->i_size),
+			*bytes_extended, new_i_size);
+	mlog_bug_on_msg(new_i_size < i_size_read(inode),
+			"Inode %"MLFu64", i_size = %lld, new sz = %"MLFu64"\n",
+			OCFS2_I(inode)->ip_blkno, i_size_read(inode),
+			new_i_size);
 
-restart_all:
-	BUG_ON(le32_to_cpu(fe->i_clusters) != OCFS2_I(inode)->ip_clusters);
+	if (i_size_read(inode) == new_i_size)
+  		goto leave;
+
+	clusters_to_add = ocfs2_clusters_for_bytes(osb->sb, new_i_size) -
+			  le32_to_cpu(fe->i_clusters);
 
-	mlog(0, "extend inode %"MLFu64", i_size = %lld, fe->i_clusters = %u, "
-	     "clusters_to_add = %u\n",
-	     OCFS2_I(inode)->ip_blkno, i_size_read(inode),
+	mlog(0, "extend inode %"MLFu64", new_i_size = %"MLFu64", "
+		"i_size = %lld, fe->i_clusters = %u, clusters_to_add = %u\n",
+	     OCFS2_I(inode)->ip_blkno, new_i_size, i_size_read(inode),
 	     fe->i_clusters, clusters_to_add);
 
-	handle = ocfs2_alloc_handle(osb);
-	if (handle == NULL) {
-		status = -ENOMEM;
-		mlog_errno(status);
-		goto leave;
-	}
+	if (!clusters_to_add)
+		goto do_start_trans;
 
 	num_free_extents = ocfs2_num_free_extents(osb,
 						  inode,
@@ -498,7 +838,7 @@ restart_all:
 	 * start_trans is important here -- always do it before! */
 	down_write(&OCFS2_I(inode)->ip_alloc_sem);
 	drop_alloc_sem = 1;
-
+do_start_trans:
 	credits = ocfs2_calc_extend_credits(osb->sb, fe, clusters_to_add);
 	handle = ocfs2_start_trans(osb, handle, credits);
 	if (IS_ERR(handle)) {
@@ -519,41 +859,57 @@ restarted_transaction:
 		goto leave;
 	}
 
-	prev_clusters = OCFS2_I(inode)->ip_clusters;
+	if (!clusters_to_add)
+		goto no_alloc;
 
-	status = ocfs2_do_extend_allocation(osb,
-					    inode,
-					    clusters_to_add,
-					    bh,
-					    handle,
-					    data_ac,
-					    meta_ac,
-					    &why);
+	status = ocfs2_extend_allocation(osb,
+					 inode,
+					 clusters_to_add,
+					 bh,
+					 handle,
+					 data_ac,
+					 meta_ac,
+					 &why);
 	if ((status < 0) && (status != -EAGAIN)) {
 		if (status != -ENOSPC)
 			mlog_errno(status);
 		goto leave;
 	}
 
-	status = ocfs2_journal_dirty(handle, bh);
-	if (status < 0) {
-		mlog_errno(status);
-		goto leave;
-	}
-
-	spin_lock(&OCFS2_I(inode)->ip_lock);
-	clusters_to_add -= (OCFS2_I(inode)->ip_clusters - prev_clusters);
-	spin_unlock(&OCFS2_I(inode)->ip_lock);
+	if (status == -EAGAIN && (new_i_size >
+	    ocfs2_clusters_to_bytes(osb->sb, le32_to_cpu(fe->i_clusters)))) {
 
-	if (why != RESTART_NONE && clusters_to_add) {
 		if (why == RESTART_META) {
-			mlog(0, "restarting function.\n");
+			mlog(0, "Inode %"MLFu64" restarting function.\n",
+			     OCFS2_I(inode)->ip_blkno);
 			restart_func = 1;
 		} else {
 			BUG_ON(why != RESTART_TRANS);
 
-			mlog(0, "restarting transaction.\n");
-			/* TODO: This can be more intelligent. */
+			new_fe_size = ocfs2_clusters_to_bytes(osb->sb,
+						le32_to_cpu(fe->i_clusters));
+			*bytes_extended += new_fe_size -
+					   le64_to_cpu(fe->i_size);
+			/* update i_size in case we crash after the
+			 * extend_trans */
+			fe->i_size = cpu_to_le64(new_fe_size);
+
+			fe->i_mtime = cpu_to_le64(CURRENT_TIME.tv_sec);
+			fe->i_mtime_nsec = cpu_to_le32(CURRENT_TIME.tv_nsec);
+
+			status = ocfs2_journal_dirty(handle, bh);
+			if (status < 0) {
+				mlog_errno(status);
+				goto leave;
+			}
+
+			clusters_to_add =
+				ocfs2_clusters_for_bytes(osb->sb,
+							 new_i_size)
+				- le32_to_cpu(fe->i_clusters);
+			mlog(0, "Inode %"MLFu64" restarting transaction.\n",
+			     OCFS2_I(inode)->ip_blkno);
+			/* TODO: This can be more intelligent. */
 			credits = ocfs2_calc_extend_credits(osb->sb,
 							    fe,
 							    clusters_to_add);
@@ -568,12 +924,34 @@ restarted_transaction:
 			goto restarted_transaction;
 		}
 	}
+	status = 0;
+
+no_alloc:
+	/* this may not be the end of our allocation so only update
+	 * i_size to what's appropriate. */
+	new_fe_size = ocfs2_clusters_to_bytes(osb->sb,
+					      le32_to_cpu(fe->i_clusters));
+	if (new_i_size < new_fe_size)
+		new_fe_size = new_i_size;
+
+	*bytes_extended += new_fe_size - le64_to_cpu(fe->i_size);
+	fe->i_size = cpu_to_le64(new_fe_size);
 
 	mlog(0, "fe: i_clusters = %u, i_size=%"MLFu64"\n",
-	     fe->i_clusters, fe->i_size);
+	     le32_to_cpu(fe->i_clusters), le64_to_cpu(fe->i_size));
+
 	mlog(0, "inode: ip_clusters=%u, i_size=%lld\n",
 	     OCFS2_I(inode)->ip_clusters, i_size_read(inode));
 
+	fe->i_ctime = fe->i_mtime = cpu_to_le64(CURRENT_TIME.tv_sec);
+	fe->i_ctime_nsec = fe->i_mtime_nsec = cpu_to_le32(CURRENT_TIME.tv_nsec);
+
+	status = ocfs2_journal_dirty(handle, bh);
+	if (status < 0) {
+		mlog_errno(status);
+		goto leave;
+	}
+
 leave:
 	if (drop_alloc_sem) {
 		up_write(&OCFS2_I(inode)->ip_alloc_sem);
@@ -591,158 +969,32 @@ leave:
 		ocfs2_free_alloc_context(meta_ac);
 		meta_ac = NULL;
 	}
-	if ((!status) && restart_func) {
-		restart_func = 0;
-		goto restart_all;
-	}
 	if (bh) {
 		brelse(bh);
 		bh = NULL;
 	}
+	if ((!status) && restart_func) {
+		restart_func = 0;
+		goto restart_all;
+	}
 
 	mlog_exit(status);
 	return status;
 }
 
-/* Some parts of this taken from generic_cont_expand, which turned out
- * to be too fragile to do exactly what we need without us having to
- * worry about recursive locking in ->commit_write(). */
-static int ocfs2_write_zero_page(struct inode *inode,
-				 u64 size)
-{
-	struct address_space *mapping = inode->i_mapping;
-	struct page *page;
-	unsigned long index;
-	unsigned int offset;
-	struct ocfs2_journal_handle *handle = NULL;
-	int ret;
-
-	offset = (size & (PAGE_CACHE_SIZE-1)); /* Within page */
-	/* ugh.  in prepare/commit_write, if from==to==start of block, we 
-	** skip the prepare.  make sure we never send an offset for the start
-	** of a block
-	*/
-	if ((offset & (inode->i_sb->s_blocksize - 1)) == 0) {
-		offset++;
-	}
-	index = size >> PAGE_CACHE_SHIFT;
-
-	page = grab_cache_page(mapping, index);
-	if (!page) {
-		ret = -ENOMEM;
-		mlog_errno(ret);
-		goto out;
-	}
-
-	ret = ocfs2_prepare_write(NULL, page, offset, offset);
-	if (ret < 0) {
-		mlog_errno(ret);
-		goto out_unlock;
-	}
-
-	if (ocfs2_should_order_data(inode)) {
-		handle = ocfs2_start_walk_page_trans(inode, page, offset,
-						     offset);
-		if (IS_ERR(handle)) {
-			ret = PTR_ERR(handle);
-			handle = NULL;
-			goto out_unlock;
-		}
-	}
-
-	/* must not update i_size! */
-	ret = block_commit_write(page, offset, offset);
-	if (ret < 0)
-		mlog_errno(ret);
-	else
-		ret = 0;
-
-	if (handle)
-		ocfs2_commit_trans(handle);
-out_unlock:
-	unlock_page(page);
-	page_cache_release(page);
-out:
-	return ret;
-}
-
-static int ocfs2_zero_extend(struct inode *inode,
-			     u64 zero_to_size)
-{
-	int ret = 0;
-	u64 start_off;
-	struct super_block *sb = inode->i_sb;
-
-	start_off = ocfs2_align_bytes_to_blocks(sb, i_size_read(inode));
-	while (start_off < zero_to_size) {
-		ret = ocfs2_write_zero_page(inode, start_off);
-		if (ret < 0) {
-			mlog_errno(ret);
-			goto out;
-		}
-
-		start_off += sb->s_blocksize;
-	}
-
-out:
-	return ret;
-}
-
-static int ocfs2_extend_file(struct inode *inode,
-			     struct buffer_head *di_bh,
-			     u64 new_i_size)
-{
-	int ret = 0;
-	u32 clusters_to_add;
-
-	/* setattr sometimes calls us like this. */
-	if (new_i_size == 0)
-		goto out;
-
-	if (i_size_read(inode) == new_i_size)
-  		goto out;
-	BUG_ON(new_i_size < i_size_read(inode));
-
-	clusters_to_add = ocfs2_clusters_for_bytes(inode->i_sb, new_i_size) - 
-		OCFS2_I(inode)->ip_clusters;
-
-	if (clusters_to_add) {
-		ret = ocfs2_extend_allocation(inode, clusters_to_add);
-		if (ret < 0) {
-			mlog_errno(ret);
-			goto out;
-		}
-
-		ret = ocfs2_zero_extend(inode, new_i_size);
-		if (ret < 0) {
-			mlog_errno(ret);
-			goto out;
-		}
-	} 
-
-	/* No allocation required, we just use this helper to
-	 * do a trivial update of i_size. */
-	ret = ocfs2_simple_size_update(inode, di_bh, new_i_size);
-	if (ret < 0) {
-		mlog_errno(ret);
-		goto out;
-	}
-
-out:
-	return ret;
-}
-
 int ocfs2_setattr(struct dentry *dentry, struct iattr *attr)
 {
-	int status = 0, size_change;
+	int status = 0;
+	u64 newsize, bytes_added;
 	struct inode *inode = dentry->d_inode;
 	struct super_block *sb = inode->i_sb;
 	struct ocfs2_super *osb = OCFS2_SB(sb);
 	struct buffer_head *bh = NULL;
 	struct ocfs2_journal_handle *handle = NULL;
 
-	mlog_entry("(0x%p, '%.*s')\n", dentry,
-	           dentry->d_name.len, dentry->d_name.name);
+	mlog_entry("(0x%p, '%.*s', inode %"MLFu64")\n", dentry,
+	           dentry->d_name.len, dentry->d_name.name,
+		   OCFS2_I(inode)->ip_blkno);
 
 	if (attr->ia_valid & ATTR_MODE)
 		mlog(0, "mode change: %d\n", attr->ia_mode);
@@ -766,33 +1018,60 @@ int ocfs2_setattr(struct dentry *dentry,
 	if (status)
 		return status;
 
-	size_change = S_ISREG(inode->i_mode) && attr->ia_valid & ATTR_SIZE;
-	if (size_change) {
-		status = ocfs2_rw_lock(inode, 1);
-		if (status < 0) {
-			mlog_errno(status);
-			goto bail;
-		}
-	}
+	newsize = attr->ia_size;
 
 	status = ocfs2_meta_lock(inode, NULL, &bh, 1);
 	if (status < 0) {
 		if (status != -ENOENT)
 			mlog_errno(status);
-		goto bail_unlock_rw;
+		goto bail;
 	}
 
-	if (size_change && attr->ia_size != i_size_read(inode)) {
-		if (i_size_read(inode) > attr->ia_size)
-			status = ocfs2_truncate_file(inode, bh, attr->ia_size);
+	if (S_ISREG(inode->i_mode) &&
+	    attr->ia_valid & ATTR_SIZE &&
+	    newsize != i_size_read(inode)) {
+		bytes_added = 0;
+
+		if (i_size_read(inode) > newsize)
+			status = ocfs2_truncate_file(osb, newsize, inode);
 		else
-			status = ocfs2_extend_file(inode, bh, attr->ia_size);
-		if (status < 0) {
+			status = ocfs2_extend_file(osb, inode, newsize,
+						   &bytes_added);
+		if (status < 0 && (!bytes_added)) {
 			if (status != -ENOSPC)
 				mlog_errno(status);
 			status = -ENOSPC;
 			goto bail_unlock;
 		}
+
+		/* partial extend, we continue with what we've got. */
+		if (status < 0
+		    && status != -ENOSPC
+		    && status != -EINTR
+		    && status != -ERESTARTSYS)
+			mlog(ML_ERROR,
+			     "status return of %d extending inode "
+			     "%"MLFu64"\n", status,
+			     OCFS2_I(inode)->ip_blkno);
+		status = 0;
+
+		newsize = bytes_added + i_size_read(inode);
+		if (bytes_added)
+			ocfs2_update_inode_size(inode, newsize);
+
+#ifdef OCFS2_ORACORE_WORKAROUNDS
+		spin_lock(&OCFS2_I(inode)->ip_lock);
+		if (OCFS2_I(inode)->ip_flags & OCFS2_INODE_OPEN_DIRECT) {
+			/* This is a total broken hack for O_DIRECT crack */
+			OCFS2_I(inode)->ip_mmu_private = i_size_read(inode);
+		}
+		spin_unlock(&OCFS2_I(inode)->ip_lock);
+#endif
+		status = ocfs2_zero_extend(inode);
+		if (status < 0) {
+			mlog_errno(status);
+			goto bail_unlock;
+		}
 	}
 
 	handle = ocfs2_start_trans(osb, NULL, OCFS2_INODE_UPDATE_CREDITS);
@@ -816,9 +1095,6 @@ bail_commit:
 	ocfs2_commit_trans(handle);
 bail_unlock:
 	ocfs2_meta_unlock(inode, 1);
-bail_unlock_rw:
-	if (size_change)
-		ocfs2_rw_unlock(inode, 1);
 bail:
 	if (bh)
 		brelse(bh);
@@ -856,361 +1132,6 @@ bail:
 	return err;
 }
 
-static int ocfs2_write_remove_suid(struct inode *inode)
-{
-	int ret;
-	struct buffer_head *bh = NULL;
-	struct ocfs2_inode_info *oi = OCFS2_I(inode);
-	struct ocfs2_journal_handle *handle;
-	struct ocfs2_super *osb = OCFS2_SB(inode->i_sb);
-	struct ocfs2_dinode *di;
-
-	mlog_entry("(Inode %"MLFu64", mode 0%o)\n", oi->ip_blkno,
-		   inode->i_mode);
-
-	handle = ocfs2_start_trans(osb, NULL, OCFS2_INODE_UPDATE_CREDITS);
-	if (handle == NULL) {
-		ret = -ENOMEM;
-		mlog_errno(ret);
-		goto out;
-	}
-
-	ret = ocfs2_read_block(osb, oi->ip_blkno, &bh, OCFS2_BH_CACHED, inode);
-	if (ret < 0) {
-		mlog_errno(ret);
-		goto out_trans;
-	}
-
-	ret = ocfs2_journal_access(handle, inode, bh,
-				   OCFS2_JOURNAL_ACCESS_WRITE);
-	if (ret < 0) {
-		mlog_errno(ret);
-		goto out_bh;
-	}
-
-	inode->i_mode &= ~S_ISUID;
-	if ((inode->i_mode & S_ISGID) && (inode->i_mode & S_IXGRP))
-		inode->i_mode &= ~S_ISGID;
-
-	di = (struct ocfs2_dinode *) bh->b_data;
-	di->i_mode = cpu_to_le16(inode->i_mode);
-
-	ret = ocfs2_journal_dirty(handle, bh);
-	if (ret < 0)
-		mlog_errno(ret);
-out_bh:
-	brelse(bh);
-out_trans:
-	ocfs2_commit_trans(handle);
-out:
-	mlog_exit(ret);
-	return ret;
-}
-
-static inline int ocfs2_write_should_remove_suid(struct inode *inode)
-{
-	mode_t mode = inode->i_mode;
-
-	if (!capable(CAP_FSETID)) {
-		if (unlikely(mode & S_ISUID))
-			return 1;
-
-		if (unlikely((mode & S_ISGID) && (mode & S_IXGRP)))
-			return 1;
-	}
-	return 0;
-}
-
-static ssize_t ocfs2_file_aio_write(struct kiocb *iocb,
-				    const char __user *buf,
-				    size_t count,
-				    loff_t pos)
-{
-	struct iovec local_iov = { .iov_base = (void __user *)buf,
-				   .iov_len = count };
-	int ret, rw_level = -1, meta_level = -1, have_alloc_sem = 0;
-	u32 clusters;
-	struct file *filp = iocb->ki_filp;
-	struct inode *inode = filp->f_dentry->d_inode;
-	loff_t newsize, saved_pos;
-#ifdef OCFS2_ORACORE_WORKAROUNDS
-	struct ocfs2_super *osb = OCFS2_SB(inode->i_sb);
-#endif
-
-	mlog_entry("(0x%p, 0x%p, %u, '%.*s')\n", filp, buf,
-		   (unsigned int)count,
-		   filp->f_dentry->d_name.len,
-		   filp->f_dentry->d_name.name);
-
-	/* happy write of zero bytes */
-	if (count == 0)
-		return 0;
-
-	if (!inode) {
-		mlog(0, "bad inode\n");
-		return -EIO;
-	}
-
-#ifdef OCFS2_ORACORE_WORKAROUNDS
-	/* ugh, work around some applications which open everything O_DIRECT +
-	 * O_APPEND and really don't mean to use O_DIRECT. */
-	if (osb->s_mount_opt & OCFS2_MOUNT_COMPAT_OCFS &&
-	    (filp->f_flags & O_APPEND) && (filp->f_flags & O_DIRECT)) 
-		filp->f_flags &= ~O_DIRECT;
-#endif
-
-	mutex_lock(&inode->i_mutex);
-	/* to match setattr's i_mutex -> i_alloc_sem -> rw_lock ordering */
-	if (filp->f_flags & O_DIRECT) {
-		have_alloc_sem = 1;
-		down_read(&inode->i_alloc_sem);
-	}
-
-	/* concurrent O_DIRECT writes are allowed */
-	rw_level = (filp->f_flags & O_DIRECT) ? 0 : 1;
-	ret = ocfs2_rw_lock(inode, rw_level);
-	if (ret < 0) {
-		rw_level = -1;
-		mlog_errno(ret);
-		goto out;
-	}
-
-	/* 
-	 * We sample i_size under a read level meta lock to see if our write
-	 * is extending the file, if it is we back off and get a write level
-	 * meta lock.
-	 */
-	meta_level = (filp->f_flags & O_APPEND) ? 1 : 0;
-	for(;;) {
-		ret = ocfs2_meta_lock(inode, NULL, NULL, meta_level);
-		if (ret < 0) {
-			meta_level = -1;
-			mlog_errno(ret);
-			goto out;
-		}
-
-		/* Clear suid / sgid if necessary. We do this here
-		 * instead of later in the write path because
-		 * remove_suid() calls ->setattr without any hint that
-		 * we may have already done our cluster locking. Since
-		 * ocfs2_setattr() *must* take cluster locks to
-		 * proceeed, this will lead us to recursively lock the
-		 * inode. There's also the dinode i_size state which
-		 * can be lost via setattr during extending writes (we
-		 * set inode->i_size at the end of a write. */
-		if (ocfs2_write_should_remove_suid(inode)) {
-			if (meta_level == 0) {
-				ocfs2_meta_unlock(inode, meta_level);
-				meta_level = 1;
-				continue;
-			}
-
-			ret = ocfs2_write_remove_suid(inode);
-			if (ret < 0) {
-				mlog_errno(ret);
-				goto out;
-			}
-		}
-
-		/* work on a copy of ppos until we're sure that we won't have
-		 * to recalculate it due to relocking. */
-		if (filp->f_flags & O_APPEND) {
-			saved_pos = i_size_read(inode);
-			mlog(0, "O_APPEND: inode->i_size=%llu\n", saved_pos);
-		} else {
-			saved_pos = iocb->ki_pos;
-		}
-		newsize = count + saved_pos;
-
-		mlog(0, "pos=%lld newsize=%lld cursize=%lld\n",
-		     (long long) saved_pos, (long long) newsize,
-		     (long long) i_size_read(inode));
-
-		/* No need for a higher level metadata lock if we're
-		 * never going past i_size. */
-		if (newsize <= i_size_read(inode))
-			break;
-
-		if (meta_level == 0) {
-			ocfs2_meta_unlock(inode, meta_level);
-			meta_level = 1;
-			continue;
-		}
-
-		spin_lock(&OCFS2_I(inode)->ip_lock);
-		clusters = ocfs2_clusters_for_bytes(inode->i_sb, newsize) -
-			OCFS2_I(inode)->ip_clusters;
-		spin_unlock(&OCFS2_I(inode)->ip_lock);
-
-		mlog(0, "Writing at EOF, may need more allocation: "
-		     "i_size = %lld, newsize = %lld, need %u clusters\n",
-		     (long long) i_size_read(inode), (long long) newsize,
-		     clusters);
-
-		/* We only want to continue the rest of this loop if
-		 * our extend will actually require more
-		 * allocation. */
-		if (!clusters)
-			break;
-
-		ret = ocfs2_extend_allocation(inode, clusters);
-		if (ret < 0) {
-			if (ret != -ENOSPC)
-				mlog_errno(ret);
-			goto out;
-		}
-
-		/* Fill any holes which would've been created by this
-		 * write. If we're O_APPEND, this will wind up
-		 * (correctly) being a noop. */
-		ret = ocfs2_zero_extend(inode, (u64) newsize - count);
-		if (ret < 0) {
-			mlog_errno(ret);
-			goto out;
-		}
-		break;
-	}
-
-	/* ok, we're done with i_size and alloc work */
-	iocb->ki_pos = saved_pos;
-	ocfs2_meta_unlock(inode, meta_level);
-	meta_level = -1;
-
-	/* communicate with ocfs2_dio_end_io */
-	ocfs2_iocb_set_rw_locked(iocb);
-
-#ifdef OCFS2_ORACORE_WORKAROUNDS
-	if (osb->s_mount_opt & OCFS2_MOUNT_COMPAT_OCFS &&
-	    filp->f_flags & O_DIRECT) {
-		unsigned int saved_flags = filp->f_flags;
-		int sector_size = 1 << osb->s_sectsize_bits;
-
-		if ((saved_pos & (sector_size - 1)) ||
-		    (count & (sector_size - 1)) ||
-		    ((unsigned long)buf & (sector_size - 1))) {
-			filp->f_flags |= O_SYNC;
-			filp->f_flags &= ~O_DIRECT;
-		}
-
-		ret = generic_file_aio_write_nolock(iocb, &local_iov, 1,
-						    &iocb->ki_pos);
-
-		filp->f_flags = saved_flags;
-	} else
-#endif
-		ret = generic_file_aio_write_nolock(iocb, &local_iov, 1,
-						    &iocb->ki_pos);
-
-	/* buffered aio wouldn't have proper lock coverage today */
-	BUG_ON(ret == -EIOCBQUEUED && !(filp->f_flags & O_DIRECT));
-
-	/* 
-	 * deep in g_f_a_w_n()->ocfs2_direct_IO we pass in a ocfs2_dio_end_io
-	 * function pointer which is called when o_direct io completes so that
-	 * it can unlock our rw lock.  (it's the clustered equivalent of
-	 * i_alloc_sem; protects truncate from racing with pending ios).
-	 * Unfortunately there are error cases which call end_io and others
-	 * that don't.  so we don't have to unlock the rw_lock if either an
-	 * async dio is going to do it in the future or an end_io after an
-	 * error has already done it.
-	 */
-	if (ret == -EIOCBQUEUED || !ocfs2_iocb_is_rw_locked(iocb)) {
-		rw_level = -1;
-		have_alloc_sem = 0;
-	}
-
-out:
-	if (meta_level != -1)
-		ocfs2_meta_unlock(inode, meta_level);
-	if (have_alloc_sem)
-		up_read(&inode->i_alloc_sem);
-	if (rw_level != -1) 
-		ocfs2_rw_unlock(inode, rw_level);
-	mutex_unlock(&inode->i_mutex);
-
-	mlog_exit(ret);
-	return ret;
-}
-
-static ssize_t ocfs2_file_aio_read(struct kiocb *iocb,
-				   char __user *buf,
-				   size_t count,
-				   loff_t pos)
-{
-	int ret = 0, rw_level = -1, have_alloc_sem = 0;
-	struct file *filp = iocb->ki_filp;
-	struct inode *inode = filp->f_dentry->d_inode;
-#ifdef OCFS2_ORACORE_WORKAROUNDS
-	struct ocfs2_super *osb = OCFS2_SB(inode->i_sb);
-#endif
-
-	mlog_entry("(0x%p, 0x%p, %u, '%.*s')\n", filp, buf,
-		   (unsigned int)count,
-		   filp->f_dentry->d_name.len,
-		   filp->f_dentry->d_name.name);
-
-	if (!inode) {
-		ret = -EINVAL;
-		mlog_errno(ret);
-		goto bail;
-	}
-
-#ifdef OCFS2_ORACORE_WORKAROUNDS
-	if (osb->s_mount_opt & OCFS2_MOUNT_COMPAT_OCFS) {
-		if (filp->f_flags & O_DIRECT) {
-			int sector_size = 1 << osb->s_sectsize_bits;
-
-			if ((pos & (sector_size - 1)) ||
-			    (count & (sector_size - 1)) ||
-			    ((unsigned long)buf & (sector_size - 1)) ||
-			    (i_size_read(inode) & (sector_size -1))) {
-				filp->f_flags &= ~O_DIRECT;
-			}
-		}
-	}
-#endif
-
-	/* 
-	 * buffered reads protect themselves in ->readpage().  O_DIRECT reads
-	 * need locks to protect pending reads from racing with truncate.
-	 */
-	if (filp->f_flags & O_DIRECT) {
-		down_read(&inode->i_alloc_sem);
-		have_alloc_sem = 1;
-
-		ret = ocfs2_rw_lock(inode, 0);
-		if (ret < 0) {
-			mlog_errno(ret);
-			goto bail;
-		}
-		rw_level = 0;
-		/* communicate with ocfs2_dio_end_io */
-		ocfs2_iocb_set_rw_locked(iocb);
-	}
-
-	ret = generic_file_aio_read(iocb, buf, count, iocb->ki_pos);
-	if (ret == -EINVAL)
-		mlog(ML_ERROR, "generic_file_aio_read returned -EINVAL\n");
-
-	/* buffered aio wouldn't have proper lock coverage today */
-	BUG_ON(ret == -EIOCBQUEUED && !(filp->f_flags & O_DIRECT));
-
-	/* see ocfs2_file_aio_write */
-	if (ret == -EIOCBQUEUED || !ocfs2_iocb_is_rw_locked(iocb)) {
-		rw_level = -1;
-		have_alloc_sem = 0;
-	}
-
-bail:
-	if (have_alloc_sem)
-		up_read(&inode->i_alloc_sem);
-	if (rw_level != -1) 
-		ocfs2_rw_unlock(inode, rw_level);
-	mlog_exit(ret);
-
-	return ret;
-}
-
 struct inode_operations ocfs2_file_iops = {
 	.setattr	= ocfs2_setattr,
 	.getattr	= ocfs2_getattr,
@@ -1220,21 +1141,3 @@ struct inode_operations ocfs2_special_fi
 	.setattr	= ocfs2_setattr,
 	.getattr	= ocfs2_getattr,
 };
-
-struct file_operations ocfs2_fops = {
-	.read		= do_sync_read,
-	.write		= do_sync_write,
-	.sendfile	= generic_file_sendfile,
-	.mmap		= ocfs2_mmap,
-	.fsync		= ocfs2_sync_file,
-	.release	= ocfs2_file_release,
-	.open		= ocfs2_file_open,
-	.aio_read	= ocfs2_file_aio_read,
-	.aio_write	= ocfs2_file_aio_write,
-};
-
-struct file_operations ocfs2_dops = {
-	.read		= generic_read_dir,
-	.readdir	= ocfs2_readdir,
-	.fsync		= ocfs2_sync_file,
-};
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/file.h linux-2.6.15-ocfs2/fs/ocfs2/file.h
--- linux-2.6.15-2/fs/ocfs2/file.h	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/file.h	2006-02-06 17:59:23.000000000 -0500
@@ -30,28 +30,34 @@ extern struct file_operations ocfs2_fops
 extern struct file_operations ocfs2_dops;
 extern struct inode_operations ocfs2_file_iops;
 extern struct inode_operations ocfs2_special_file_iops;
-struct ocfs2_alloc_context;
 
 enum ocfs2_alloc_restarted {
-	RESTART_NONE = 0,
-	RESTART_TRANS,
+	RESTART_TRANS = 0,
 	RESTART_META
 };
-int ocfs2_do_extend_allocation(struct ocfs2_super *osb,
-			       struct inode *inode,
-			       u32 clusters_to_add,
-			       struct buffer_head *fe_bh,
-			       struct ocfs2_journal_handle *handle,
-			       struct ocfs2_alloc_context *data_ac,
-			       struct ocfs2_alloc_context *meta_ac,
-			       enum ocfs2_alloc_restarted *reason);
+int ocfs2_extend_allocation(struct ocfs2_super *osb,
+			    struct inode *inode,
+			    u32 clusters_to_add,
+			    struct buffer_head *fe_bh,
+			    struct ocfs2_journal_handle *handle,
+			    struct ocfs2_alloc_context *data_ac,
+			    struct ocfs2_alloc_context *meta_ac,
+			    enum ocfs2_alloc_restarted *reason);
 int ocfs2_setattr(struct dentry *dentry, struct iattr *attr);
 int ocfs2_getattr(struct vfsmount *mnt, struct dentry *dentry,
 		  struct kstat *stat);
+int ocfs2_sync_inode(struct inode *inode);
+int ocfs2_extend_file(struct ocfs2_super *osb,
+		      struct inode *inode,
+		      u64 new_i_size,
+		      u64 *bytes_extended);
 
 int ocfs2_set_inode_size(struct ocfs2_journal_handle *handle,
 			 struct inode *inode,
 			 struct buffer_head *fe_bh,
 			 u64 new_i_size);
 
+void ocfs2_file_finish_extension(struct inode *inode, loff_t newsize,
+				 unsigned direct_extend);
+
 #endif /* OCFS2_FILE_H */
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/heartbeat.c linux-2.6.15-ocfs2/fs/ocfs2/heartbeat.c
--- linux-2.6.15-2/fs/ocfs2/heartbeat.c	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/heartbeat.c	2006-02-06 17:59:23.000000000 -0500
@@ -171,6 +171,9 @@ void ocfs2_clear_hb_callbacks(struct ocf
 {
 	int status;
 
+	if (osb->osb_hb_res == NULL)
+		return;
+
 	status = o2hb_unregister_callback(&osb->osb_hb_down);
 	if (status < 0)
 		mlog_errno(status);
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/inode.c linux-2.6.15-ocfs2/fs/ocfs2/inode.c
--- linux-2.6.15-2/fs/ocfs2/inode.c	2006-02-06 18:15:15.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/inode.c	2006-02-06 17:59:23.000000000 -0500
@@ -299,6 +299,7 @@ int ocfs2_populate_inode(struct inode *i
 		    inode->i_fop = &ocfs2_fops;
 		    inode->i_op = &ocfs2_file_iops;
 		    i_size_write(inode, le64_to_cpu(fe->i_size));
+		    OCFS2_I(inode)->ip_mmu_private = inode->i_size;
 		    break;
 	    case S_IFDIR:
 		    inode->i_op = &ocfs2_dir_iops;
@@ -319,8 +320,6 @@ int ocfs2_populate_inode(struct inode *i
 		    break;
 	}
 
-	ocfs2_inode_lock_res_init(&OCFS2_I(inode)->ip_rw_lockres,
-				  OCFS2_LOCK_TYPE_RW, inode);
 	ocfs2_inode_lock_res_init(&OCFS2_I(inode)->ip_meta_lockres,
 				  OCFS2_LOCK_TYPE_META, inode);
 	ocfs2_inode_lock_res_init(&OCFS2_I(inode)->ip_data_lockres,
@@ -747,9 +746,11 @@ static void ocfs2_cleanup_delete_inode(s
 {
 	mlog(0, "Cleanup inode %"MLFu64", sync = %d\n",
 	     OCFS2_I(inode)->ip_blkno, sync_data);
+#ifndef OCFS2_DELETE_INODE_WORKAROUND
 	if (sync_data)
 		write_inode_now(inode, 1);
 	truncate_inode_pages(&inode->i_data, 0);
+#endif
 }
 
 void ocfs2_delete_inode(struct inode *inode)
@@ -787,12 +788,7 @@ void ocfs2_delete_inode(struct inode *in
 
 	/* Lock down the inode. This gives us an up to date view of
 	 * it's metadata (for verification), and allows us to
-	 * serialize delete_inode votes. 
-	 *
-	 * Even though we might be doing a truncate, we don't take the
-	 * allocation lock here as it won't be needed - nobody will
-	 * have the file open.
-	 */
+	 * serialize delete_inode votes. */
 	status = ocfs2_meta_lock(inode, NULL, &di_bh, 1);
 	if (status < 0) {
 		if (status != -ENOENT)
@@ -861,7 +857,6 @@ void ocfs2_clear_inode(struct inode *ino
 
 	/* Do these before all the other work so that we don't bounce
 	 * the vote thread while waiting to destroy the locks. */
-	ocfs2_mark_lockres_freeing(&oi->ip_rw_lockres);
 	ocfs2_mark_lockres_freeing(&oi->ip_meta_lockres);
 	ocfs2_mark_lockres_freeing(&oi->ip_data_lockres);
 
@@ -885,7 +880,6 @@ void ocfs2_clear_inode(struct inode *ino
 	if (status < 0)
 		mlog_errno(status);
 
-	ocfs2_lock_res_free(&oi->ip_rw_lockres);
 	ocfs2_lock_res_free(&oi->ip_meta_lockres);
 	ocfs2_lock_res_free(&oi->ip_data_lockres);
 
@@ -903,10 +897,10 @@ void ocfs2_clear_inode(struct inode *ino
 			"Clear inode of %"MLFu64", inode is locked\n",
 			oi->ip_blkno);
 
-	mlog_bug_on_msg(!mutex_trylock(&oi->ip_io_mutex),
-			"Clear inode of %"MLFu64", io_mutex is locked\n",
+	mlog_bug_on_msg(down_trylock(&oi->ip_io_sem),
+			"Clear inode of %"MLFu64", io_sem is locked\n",
 			oi->ip_blkno);
-	mutex_unlock(&oi->ip_io_mutex);
+	up(&oi->ip_io_sem);
 
 	/*
 	 * down_trylock() returns 0, down_write_trylock() returns 1
@@ -1120,6 +1114,9 @@ void ocfs2_refresh_inode(struct inode *i
 
 	OCFS2_I(inode)->ip_clusters = le32_to_cpu(fe->i_clusters);
 	i_size_write(inode, le64_to_cpu(fe->i_size));
+	if (S_ISREG(inode->i_mode)) {
+		OCFS2_I(inode)->ip_mmu_private = i_size_read(inode);
+	}
 	inode->i_nlink = le16_to_cpu(fe->i_links_count);
 	inode->i_uid = le32_to_cpu(fe->i_uid);
 	inode->i_gid = le32_to_cpu(fe->i_gid);
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/inode.h linux-2.6.15-ocfs2/fs/ocfs2/inode.h
--- linux-2.6.15-2/fs/ocfs2/inode.h	2006-02-06 18:15:15.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/inode.h	2006-02-06 17:59:23.000000000 -0500
@@ -31,46 +31,46 @@ struct ocfs2_inode_info
 {
 	u64			ip_blkno;
 
-	struct ocfs2_lock_res		ip_rw_lockres;
-	struct ocfs2_lock_res		ip_meta_lockres;
-	struct ocfs2_lock_res		ip_data_lockres;
+	struct ocfs2_lock_res	ip_meta_lockres;
+	struct ocfs2_lock_res	ip_data_lockres;
 
 	/* protects allocation changes on this inode. */
-	struct rw_semaphore		ip_alloc_sem;
+	struct rw_semaphore	ip_alloc_sem;
 
 	/* These fields are protected by ip_lock */
-	spinlock_t			ip_lock;
-	u32				ip_open_count;
-	u32				ip_clusters;
-	struct ocfs2_extent_map		ip_map;
-	struct list_head		ip_io_markers;
-	int				ip_orphaned_slot;
+	spinlock_t		ip_lock;
+	u32			ip_open_count;
+	u32			ip_clusters;
+	loff_t			ip_mmu_private;
+	struct ocfs2_extent_map	ip_map;
+	struct list_head	ip_io_markers;
+	int			ip_orphaned_slot;
 
-	struct mutex			ip_io_mutex;
+	struct semaphore	ip_io_sem;
 
 	/* Used by the journalling code to attach an inode to a
-	 * handle.  These are protected by ip_io_mutex in order to lock
+	 * handle.  These are protected by ip_io_sem in order to lock
 	 * out other I/O to the inode until we either commit or
 	 * abort. */
-	struct list_head		ip_handle_list;
+	struct list_head	ip_handle_list;
 	struct ocfs2_journal_handle	*ip_handle;
 
-	u32				ip_flags; /* see below */
+	u32			ip_flags; /* see below */
 
 	/* protected by recovery_lock. */
-	struct inode			*ip_next_orphan;
+	struct inode		*ip_next_orphan;
 
-	u32				ip_dir_start_lookup;
+	u32			ip_dir_start_lookup;
 
 	/* next two are protected by trans_inc_lock */
 	/* which transaction were we created on? Zero if none. */
-	unsigned long			ip_created_trans;
+	unsigned long		ip_created_trans;
 	/* last transaction we were a part of. */
-	unsigned long			ip_last_trans;
+	unsigned long		ip_last_trans;
 
 	struct ocfs2_caching_info	ip_metadata_cache;
 
-	struct inode			vfs_inode;
+	struct inode		vfs_inode;
 };
 
 /*
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/journal.c linux-2.6.15-ocfs2/fs/ocfs2/journal.c
--- linux-2.6.15-2/fs/ocfs2/journal.c	2006-02-06 18:15:15.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/journal.c	2006-02-06 17:59:23.000000000 -0500
@@ -147,7 +147,8 @@ struct ocfs2_journal_handle *ocfs2_start
 
 	mlog_entry("(max_buffs = %d)\n", max_buffs);
 
-	BUG_ON(!osb || !osb->journal->j_journal);
+	if (!osb || !osb->journal->j_journal)
+		BUG();
 
 	if (ocfs2_is_hard_readonly(osb)) {
 		ret = -EROFS;
@@ -400,7 +401,7 @@ int ocfs2_journal_access(struct ocfs2_jo
 	 * j_trans_barrier for us. */
 	ocfs2_set_inode_lock_trans(OCFS2_SB(inode->i_sb)->journal, inode);
 
-	mutex_lock(&OCFS2_I(inode)->ip_io_mutex);
+	down(&OCFS2_I(inode)->ip_io_sem);
 	switch (type) {
 	case OCFS2_JOURNAL_ACCESS_CREATE:
 	case OCFS2_JOURNAL_ACCESS_WRITE:
@@ -415,7 +416,7 @@ int ocfs2_journal_access(struct ocfs2_jo
 		status = -EINVAL;
 		mlog(ML_ERROR, "Uknown access type!\n");
 	}
-	mutex_unlock(&OCFS2_I(inode)->ip_io_mutex);
+	up(&OCFS2_I(inode)->ip_io_sem);
 
 	if (status < 0)
 		mlog(ML_ERROR, "Error %d getting %d access to buffer!\n",
@@ -445,18 +446,6 @@ int ocfs2_journal_dirty(struct ocfs2_jou
 	return status;
 }
 
-int ocfs2_journal_dirty_data(handle_t *handle,
-			     struct buffer_head *bh)
-{
-	int err = journal_dirty_data(handle, bh);
-	if (err)
-		mlog_errno(err);
-	/* TODO: When we can handle it, abort the handle and go RO on
-	 * error here. */
-
-	return err;
-}
-
 /* We always assume you're adding a metadata lock at level 'ex' */
 int ocfs2_handle_add_lock(struct ocfs2_journal_handle *handle,
 			  struct inode *inode)
@@ -564,7 +553,7 @@ int ocfs2_journal_init(struct ocfs2_jour
 	 * changes in a live cluster so it can be considered an
 	 * exception to the rule. */
 	status = ocfs2_meta_lock_full(inode, NULL, &bh, 1,
-				      OCFS2_META_LOCK_RECOVERY);
+				      OCFS2_META_LOCK_RECOVERY, NULL, 0);
 	if (status < 0) {
 		if (status != -ERESTARTSYS)
 			mlog(ML_ERROR, "Could not get lock on journal!\n");
@@ -675,7 +664,8 @@ void ocfs2_journal_shutdown(struct ocfs2
 
 	mlog_entry_void();
 
-	BUG_ON(!osb);
+	if (!osb)
+		BUG();
 
 	journal = osb->journal;
 	if (!journal)
@@ -807,7 +797,8 @@ int ocfs2_journal_wipe(struct ocfs2_jour
 
 	mlog_entry_void();
 
-	BUG_ON(!journal);
+	if (!journal)
+		BUG();
 
 	status = journal_wipe(journal->j_journal, full);
 	if (status < 0) {
@@ -1073,10 +1064,10 @@ restart:
 					NULL);
 
 bail:
-	mutex_lock(&osb->recovery_lock);
+	down(&osb->recovery_lock);
 	if (!status &&
 	    !ocfs2_node_map_is_empty(osb, &osb->recovery_map)) {
-		mutex_unlock(&osb->recovery_lock);
+		up(&osb->recovery_lock);
 		goto restart;
 	}
 
@@ -1084,7 +1075,7 @@ bail:
 	mb(); /* sync with ocfs2_recovery_thread_running */
 	wake_up(&osb->recovery_event);
 
-	mutex_unlock(&osb->recovery_lock);
+	up(&osb->recovery_lock);
 
 	mlog_exit(status);
 	/* no one is callint kthread_stop() for us so the kthread() api
@@ -1099,7 +1090,7 @@ void ocfs2_recovery_thread(struct ocfs2_
 	mlog_entry("(node_num=%d, osb->node_num = %d)\n",
 		   node_num, osb->node_num);
 
-	mutex_lock(&osb->recovery_lock);
+	down(&osb->recovery_lock);
 	if (osb->disable_recovery)
 		goto out;
 
@@ -1121,7 +1112,7 @@ void ocfs2_recovery_thread(struct ocfs2_
 	}
 
 out:
-	mutex_unlock(&osb->recovery_lock);
+	up(&osb->recovery_lock);
 	wake_up(&osb->recovery_event);
 
 	mlog_exit_void();
@@ -1158,7 +1149,7 @@ static int ocfs2_replay_journal(struct o
 	SET_INODE_JOURNAL(inode);
 
 	status = ocfs2_meta_lock_full(inode, NULL, &bh, 1,
-				      OCFS2_META_LOCK_RECOVERY);
+				      OCFS2_META_LOCK_RECOVERY, NULL, 0);
 	if (status < 0) {
 		mlog(0, "status returned from ocfs2_meta_lock=%d\n", status);
 		if (status != -ERESTARTSYS)
@@ -1272,7 +1263,8 @@ static int ocfs2_recover_node(struct ocf
 
 	/* Should not ever be called to recover ourselves -- in that
 	 * case we should've called ocfs2_journal_load instead. */
-	BUG_ON(osb->node_num == node_num);
+	if (osb->node_num == node_num)
+		BUG();
 
 	slot_num = ocfs2_node_num_to_slot(si, node_num);
 	if (slot_num == OCFS2_INVALID_SLOT) {
@@ -1347,7 +1339,7 @@ static int ocfs2_trylock_journal(struct 
 	SET_INODE_JOURNAL(inode);
 
 	flags = OCFS2_META_LOCK_RECOVERY | OCFS2_META_LOCK_NOQUEUE;
-	status = ocfs2_meta_lock_full(inode, NULL, NULL, 1, flags);
+	status = ocfs2_meta_lock_full(inode, NULL, NULL, 1, flags, NULL, 0);
 	if (status < 0) {
 		if (status != -EAGAIN)
 			mlog_errno(status);
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/journal.h linux-2.6.15-ocfs2/fs/ocfs2/journal.h
--- linux-2.6.15-2/fs/ocfs2/journal.h	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/journal.h	2006-02-06 17:59:23.000000000 -0500
@@ -128,7 +128,7 @@ static inline int ocfs2_inode_is_new(str
 }
 
 static inline void ocfs2_inode_set_new(struct ocfs2_super *osb,
-				       struct inode *inode)
+				      struct inode *inode)
 {
 	spin_lock(&trans_inc_lock);
 	OCFS2_I(inode)->ip_created_trans = osb->journal->j_trans_id;
@@ -249,8 +249,6 @@ static inline void ocfs2_checkpoint_inod
  *                          buffer. Will have to call ocfs2_journal_dirty once
  *                          we've actually dirtied it. Type is one of . or .
  *  ocfs2_journal_dirty    - Mark a journalled buffer as having dirty data.
- *  ocfs2_journal_dirty_data - Indicate that a data buffer should go out before
- *                             the current handle commits.
  *  ocfs2_handle_add_lock  - Sometimes we need to delay lock release
  *                          until after a transaction has been completed. Use
  *                          ocfs2_handle_add_lock to indicate that a lock needs
@@ -264,11 +262,11 @@ static inline void ocfs2_checkpoint_inod
  * dirtied any buffers. */
 struct ocfs2_journal_handle *ocfs2_alloc_handle(struct ocfs2_super *osb);
 struct ocfs2_journal_handle *ocfs2_start_trans(struct ocfs2_super *osb,
-					       struct ocfs2_journal_handle *handle,
-					       int max_buffs);
-void			     ocfs2_commit_trans(struct ocfs2_journal_handle *handle);
-int			     ocfs2_extend_trans(struct ocfs2_journal_handle *handle,
-						int nblocks);
+					struct ocfs2_journal_handle *handle,
+					int max_buffs);
+void                 ocfs2_commit_trans(struct ocfs2_journal_handle *handle);
+int                  ocfs2_extend_trans(struct ocfs2_journal_handle *handle,
+					int nblocks);
 
 /*
  * Create access is for when we get a newly created buffer and we're
@@ -310,8 +308,6 @@ int                  ocfs2_journal_acces
  */
 int                  ocfs2_journal_dirty(struct ocfs2_journal_handle *handle,
 					 struct buffer_head *bh);
-int                  ocfs2_journal_dirty_data(handle_t *handle,
-					      struct buffer_head *bh);
 int                  ocfs2_handle_add_lock(struct ocfs2_journal_handle *handle,
 					   struct inode *inode);
 /*
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/Makefile linux-2.6.15-ocfs2/fs/ocfs2/Makefile
--- linux-2.6.15-2/fs/ocfs2/Makefile	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/Makefile	2006-02-06 17:59:23.000000000 -0500
@@ -1,10 +1,11 @@
 EXTRA_CFLAGS += -Ifs/ocfs2
 
-EXTRA_CFLAGS += -DCATCH_BH_JBD_RACES
+EXTRA_CFLAGS += -DCATCH_BH_JBD_RACES -DOCFS2_ORACORE_WORKAROUNDS -DOCFS2_CDSL
 
 obj-$(CONFIG_OCFS2_FS) += ocfs2.o
 
 ocfs2-objs := \
+	aio.o			\
 	alloc.o 		\
 	aops.o 			\
 	buffer_head_io.o	\
@@ -20,6 +21,7 @@ ocfs2-objs := \
 	localalloc.o 		\
 	mmap.o 			\
 	namei.o 		\
+	proc.o			\
 	slot_map.o 		\
 	suballoc.o 		\
 	super.o 		\
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/mmap.c linux-2.6.15-ocfs2/fs/ocfs2/mmap.c
--- linux-2.6.15-2/fs/ocfs2/mmap.c	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/mmap.c	2006-02-06 17:59:23.000000000 -0500
@@ -23,6 +23,7 @@
  * Boston, MA 021110-1307, USA.
  */
 
+#include <linux/sched.h>
 #include <linux/fs.h>
 #include <linux/types.h>
 #include <linux/slab.h>
@@ -37,41 +38,109 @@
 
 #include "ocfs2.h"
 
+#include "alloc.h"
 #include "dlmglue.h"
 #include "file.h"
 #include "inode.h"
+#include "journal.h"
 #include "mmap.h"
 
+#include "buffer_head_io.h"
+
+static inline u64 ocfs2_binode_blkno(struct ocfs2_backing_inode *binode);
+static inline struct rb_node * __ocfs2_buffer_lock_ctxt_root(
+	struct ocfs2_buffer_lock_ctxt *ctxt);
+static int ocfs2_buffer_lock_ctxt_insert(struct ocfs2_buffer_lock_ctxt *ctxt,
+					 struct inode *inode,
+					 struct ocfs2_backing_inode **binode_ret);
+static int ocfs2_fill_ctxt_from_buf(struct super_block *sb,
+				    struct inode *target_inode,
+				    char __user *buf,
+				    size_t size,
+				    struct ocfs2_buffer_lock_ctxt *ctxt);
+
 static struct page *ocfs2_nopage(struct vm_area_struct * area,
 				 unsigned long address,
 				 int *type)
 {
+	int status, tmpstat, locked;
 	struct inode *inode = area->vm_file->f_dentry->d_inode;
-	struct page *page = NOPAGE_SIGBUS;
+	struct page *page;
 	sigset_t blocked, oldset;
-	int ret;
+	DECLARE_IO_MARKER(io_marker);
 
-	mlog_entry("(inode %lu, address %lu)\n", inode->i_ino, address);
+	mlog_entry("(inode %lu, address %lu)\n", inode->i_ino,
+		   address);
 
-	/* The best way to deal with signals in this path is
-	 * to block them upfront, rather than allowing the
-	 * locking paths to return -ERESTARTSYS. */
-	sigfillset(&blocked);
-
-	/* We should technically never get a bad ret return
-	 * from sigprocmask */
-	ret = sigprocmask(SIG_BLOCK, &blocked, &oldset);
-	if (ret < 0) {
-		mlog_errno(ret);
-		goto out;
+	locked = ocfs2_is_in_io_marker_list(inode, current);
+
+	if (!locked) {
+		/* For lack of a better error... Unfortunately returns
+		 * from nopage aren't very expressive right now. */
+		page = NOPAGE_SIGBUS;
+
+		/* The best way to deal with signals in this path is
+		 * to block them upfront, rather than allowing the
+		 * locking paths to return -ERESTARTSYS. */
+		sigfillset(&blocked);
+
+		/* We should technically never get a bad status return
+		 * from sigprocmask */
+		status = sigprocmask(SIG_BLOCK, &blocked, &oldset);
+		if (status < 0) {
+			mlog_errno(status);
+			goto bail;
+		}
+
+		/* Since we don't allow shared writable, we need only
+		 * worry about read locking here. */
+		status = ocfs2_meta_lock(inode, NULL, NULL, 0);
+		if (status < 0) {
+			mlog_errno(status);
+
+			if (status == -ENOMEM)
+				page = NOPAGE_OOM;
+			goto bail_setmask;
+		}
+
+		status = ocfs2_data_lock(inode, 0);
+		if (status < 0) {
+			mlog_errno(status);
+
+			if (status == -ENOMEM)
+				page = NOPAGE_OOM;
+			goto bail_unlock;
+		}
+
+		tmpstat = sigprocmask(SIG_SETMASK, &oldset, NULL);
+		if (tmpstat < 0)
+			mlog_errno(tmpstat);
+
+		/* I'm not sure if we can somehow recurse back into
+		 * nopage or not, but this doesn't cost us anything,
+		 * so lets do it for now. */
+		ocfs2_add_io_marker(inode, &io_marker);
 	}
 
 	page = filemap_nopage(area, address, type);
 
-	ret = sigprocmask(SIG_SETMASK, &oldset, NULL);
-	if (ret < 0)
-		mlog_errno(ret);
-out:
+	if (!locked) {
+		ocfs2_del_io_marker(inode, &io_marker);
+		ocfs2_data_unlock(inode, 0);
+		ocfs2_meta_unlock(inode, 0);
+	}
+bail:
+	mlog_exit_ptr(page);
+	return page;
+
+bail_unlock:
+	ocfs2_meta_unlock(inode, 0);
+
+bail_setmask:
+	tmpstat = sigprocmask(SIG_SETMASK, &oldset, NULL);
+	if (tmpstat < 0)
+		mlog_errno(tmpstat);
+
 	mlog_exit_ptr(page);
 	return page;
 }
@@ -80,7 +149,8 @@ static struct vm_operations_struct ocfs2
 	.nopage = ocfs2_nopage,
 };
 
-int ocfs2_mmap(struct file *file, struct vm_area_struct *vma)
+int ocfs2_mmap(struct file *file,
+	       struct vm_area_struct *vma)
 {
 	/* We don't want to support shared writable mappings yet. */
 	if (((vma->vm_flags & VM_SHARED) || (vma->vm_flags & VM_MAYSHARE))
@@ -96,3 +166,613 @@ int ocfs2_mmap(struct file *file, struct
 	return 0;
 }
 
+static inline u64 ocfs2_binode_blkno(struct ocfs2_backing_inode *binode)
+{
+	struct inode *inode = binode->ba_inode;
+
+	BUG_ON(!inode);
+
+	return OCFS2_I(inode)->ip_blkno;
+}
+
+static inline struct rb_node * __ocfs2_buffer_lock_ctxt_root(
+	struct ocfs2_buffer_lock_ctxt *ctxt)
+{
+	return ctxt->b_inodes.rb_node;
+}
+
+static int ocfs2_buffer_lock_ctxt_insert(struct ocfs2_buffer_lock_ctxt *ctxt,
+					 struct inode *inode,
+					 struct ocfs2_backing_inode **binode_ret)
+{
+	u64 blkno;
+	struct ocfs2_backing_inode *tmp, *binode;
+	struct rb_node * parent = NULL;
+	struct rb_node ** p = &ctxt->b_inodes.rb_node;
+
+	BUG_ON(!ctxt);
+	BUG_ON(!inode);
+
+	blkno = OCFS2_I(inode)->ip_blkno;
+
+	while(*p) {
+		parent = *p;
+		tmp = rb_entry(parent, struct ocfs2_backing_inode, ba_node);
+
+		if (blkno < ocfs2_binode_blkno(tmp))
+			p = &(*p)->rb_left;
+		else if (blkno > ocfs2_binode_blkno(tmp))
+			p = &(*p)->rb_right;
+		else
+			return 0; /* Don't insert duplicates */
+	}
+
+	binode = kcalloc(1, sizeof(struct ocfs2_backing_inode), GFP_KERNEL);
+	if (!binode)
+		return -ENOMEM;
+	binode->ba_inode = inode;
+	ocfs2_init_io_marker(&binode->ba_task);
+
+	if (binode_ret)
+		*binode_ret = binode;
+
+	rb_link_node(&binode->ba_node, parent, p);
+	rb_insert_color(&binode->ba_node, &ctxt->b_inodes);
+
+	return 0;
+}
+
+static int ocfs2_fill_ctxt_from_buf(struct super_block *sb,
+				    struct inode *target_inode,
+				    char __user *buf,
+				    size_t size,
+				    struct ocfs2_buffer_lock_ctxt *ctxt)
+{
+	int status;
+	unsigned long start = (unsigned long)buf;
+	unsigned long end = start + size;
+	struct inode *inode;
+	struct mm_struct *mm = current->mm;
+	struct vm_area_struct *vma;
+
+	for (vma = find_vma(mm, start); vma; vma = vma->vm_next) {
+		if (end <= vma->vm_start)
+			break;
+		if (vma->vm_ops == &ocfs2_file_vm_ops) {
+			if (!vma->vm_file)
+				continue;
+			inode = vma->vm_file->f_dentry->d_inode;
+			if (inode->i_sb == sb &&
+			    inode != target_inode) {
+				status = ocfs2_buffer_lock_ctxt_insert(ctxt,
+								       inode,
+								       NULL);
+				if (status < 0)
+					goto bail;
+			}
+		}
+	}
+	status = 0;
+bail:
+	return status;
+}
+
+int ocfs2_setup_io_locks(struct super_block *sb,
+			 struct inode *target_inode,
+			 char __user *buf,
+			 size_t size,
+			 struct ocfs2_buffer_lock_ctxt *ctxt,
+			 struct ocfs2_backing_inode **target_binode)
+{
+	struct mm_struct *mm = current->mm;
+	int skip_sem = (current->flags & PF_DUMPCORE) || !mm;
+	int status;
+
+	if (!skip_sem)
+		down_read(&mm->mmap_sem);
+
+	BUG_ON(__ocfs2_buffer_lock_ctxt_root(ctxt));
+
+	/* We always insert target because it might not be backing part of the
+	 * buffer - but it needs to be in there so that it's lock gets ordered
+	 * with everything else */
+	status = ocfs2_buffer_lock_ctxt_insert(ctxt, target_inode,
+					       target_binode);
+
+	/* knfsd, which lacks an mm, may call us to do I/O. Since the buffer
+	 * is private to the kernel, there isn't any need to insert any other
+	 * locks, so we can skip it.
+	 *
+	 * The pile of duct tape and mixed nuts that is NFS 1, universe 0
+	 */
+	if (!status && mm) {
+		/* Now fill the tree with any inodes that back this
+		 * buffer. If target inode is in there, it will be
+		 * skipped over. */
+		status = ocfs2_fill_ctxt_from_buf(sb, target_inode, buf, size,
+						  ctxt);
+	}
+
+	if (!skip_sem)
+		up_read(&mm->mmap_sem);
+
+	if (status < 0) {
+		mlog_errno(status);
+		ocfs2_unlock_buffer_inodes(ctxt);
+		goto bail;
+	}
+
+	status = 0;
+bail:
+	return status;
+}
+
+/* starting from pos, which can be null for the first call, give the
+ * next buffer that needs unlocking.  we return null when there are none
+ * left or we see last_inode */
+static struct ocfs2_backing_inode *
+ocfs2_next_unlocked(struct ocfs2_buffer_lock_ctxt *ctxt,
+		    struct inode *last_inode,
+		    struct ocfs2_backing_inode *pos)
+{
+	struct ocfs2_backing_inode *binode = NULL;
+	struct rb_node *node = NULL;
+
+	if (pos == NULL) {
+		if (ctxt->b_next_unlocked)
+			binode = ctxt->b_next_unlocked;
+		else
+			node = rb_first(&ctxt->b_inodes);
+	} else
+		node = rb_next(&pos->ba_node);
+
+	if (node)
+		binode = rb_entry(node, struct ocfs2_backing_inode, ba_node);
+
+	if (binode && last_inode && binode->ba_inode == last_inode)
+		binode = NULL;
+
+	/* this is just an optimization to skip nodes in the tree
+	 * that we've already seen.  If we're moving from one we've locked
+	 * to one we haven't then we mark this node in the ctxt so that
+	 * we'll return to it in a future after, say, hitting last_inode
+	 * or EIOCBRETRY in lock_buffer_inodes */
+	if (pos && pos->ba_locked && binode)
+		ctxt->b_next_unlocked = binode;
+
+	return binode;
+}
+
+/* Will take locks on all inodes in the ctxt up until 'last_inode'. If
+ * last_inode is NULL, then we take locks on everything. We mark lock
+ * status on the context so we skip any that have already been
+ * locked. On error we will completely abort the context. */
+/* WARNING: If you get a failure case here, you *must* call
+ * "ocfs2_unlock_buffer_inodes" as we may have left a few inodes under
+ * cluster lock. */
+int ocfs2_lock_buffer_inodes(struct ocfs2_buffer_lock_ctxt *ctxt,
+			     struct inode *last_inode)
+{
+	int status, data_level;
+	struct ocfs2_backing_inode *binode = NULL;
+	struct inode *inode;
+
+	while((binode = ocfs2_next_unlocked(ctxt, last_inode, binode))) {
+		/* the tricksy caller might have locked inodes themselves
+		 * between calls. */
+		if (binode->ba_locked)
+			continue;
+		inode = binode->ba_inode;
+
+		if (!binode->ba_meta_locked) {
+			status = ocfs2_meta_lock_full(inode, NULL, NULL,
+						      binode->ba_lock_meta_level,
+						      0, ctxt->b_cb,
+						      ctxt->b_cb_data);
+
+			if (status < 0) {
+				if (status != -EIOCBRETRY)
+					mlog_errno(status);
+				goto bail;
+			}
+
+			binode->ba_meta_locked = 1;
+		}
+
+		/* ba_lock_data isn't set for direct io */
+		if (binode->ba_lock_data) {
+			data_level = binode->ba_lock_data_level;
+			status = ocfs2_data_lock(inode, data_level);
+			if (status < 0) {
+				if (status == -EIOCBRETRY)
+					goto bail;
+
+				/* clean up the metadata lock that we took
+				 * above
+				 */
+				ocfs2_meta_unlock(inode,
+						  binode->ba_lock_meta_level);
+				binode->ba_meta_locked = 0;
+
+				mlog_errno(status);
+				goto bail;
+			}
+		}
+		ocfs2_add_io_marker(inode, &binode->ba_task);
+		binode->ba_locked = 1;
+	}
+
+	status = 0;
+bail:
+	return status;
+}
+
+void ocfs2_unlock_buffer_inodes(struct ocfs2_buffer_lock_ctxt *ctxt)
+{
+	struct ocfs2_backing_inode *binode;
+	struct rb_node *node;
+
+	/* dlm locks don't mask ints.. this should be lower down */
+	BUG_ON(in_interrupt());
+
+	/* unlock in reverse order to minimize waking forward lockers */
+	while ((node = rb_last(&ctxt->b_inodes)) != NULL) {
+		binode = rb_entry(node, struct ocfs2_backing_inode, ba_node);
+
+		ocfs2_del_io_marker(binode->ba_inode, &binode->ba_task);
+
+		if (binode->ba_locked && binode->ba_lock_data)
+			ocfs2_data_unlock(binode->ba_inode,
+					  binode->ba_lock_data_level);
+
+		if (binode->ba_locked || binode->ba_meta_locked)
+			ocfs2_meta_unlock(binode->ba_inode,
+					  binode->ba_lock_meta_level);
+
+		rb_erase(node, &ctxt->b_inodes);
+		kfree(binode);
+	}
+
+	ctxt->b_next_unlocked = NULL;
+}
+
+static int ocfs2_write_remove_suid(struct inode *inode)
+{
+	int ret;
+	struct buffer_head *bh = NULL;
+	struct ocfs2_inode_info *oi = OCFS2_I(inode);
+	struct ocfs2_journal_handle *handle;
+	struct ocfs2_super *osb = OCFS2_SB(inode->i_sb);
+	struct ocfs2_dinode *di;
+
+	mlog_entry("(Inode %"MLFu64", mode 0%o)\n", oi->ip_blkno,
+		   inode->i_mode);
+
+	handle = ocfs2_start_trans(osb, NULL, OCFS2_INODE_UPDATE_CREDITS);
+	if (handle == NULL) {
+		ret = -ENOMEM;
+		mlog_errno(ret);
+		goto out;
+	}
+
+	ret = ocfs2_read_block(osb, oi->ip_blkno, &bh, OCFS2_BH_CACHED, inode);
+	if (ret < 0) {
+		mlog_errno(ret);
+		goto out_trans;
+	}
+
+	ocfs2_set_inode_lock_trans(osb->journal, inode);
+
+	ret = ocfs2_journal_access(handle, inode, bh,
+				   OCFS2_JOURNAL_ACCESS_WRITE);
+	if (ret < 0) {
+		mlog_errno(ret);
+		goto out_bh;
+	}
+
+	inode->i_mode &= ~S_ISUID;
+	if ((inode->i_mode & S_ISGID) && (inode->i_mode & S_IXGRP))
+		inode->i_mode &= ~S_ISGID;
+
+	di = (struct ocfs2_dinode *) bh->b_data;
+	di->i_mode = cpu_to_le16(inode->i_mode);
+
+	ret = ocfs2_journal_dirty(handle, bh);
+	if (ret < 0)
+		mlog_errno(ret);
+out_bh:
+	brelse(bh);
+out_trans:
+	ocfs2_commit_trans(handle);
+out:
+
+	mlog_exit(ret);
+	return ret;
+}
+
+static inline int ocfs2_write_should_remove_suid(struct inode *inode)
+{
+	mode_t mode = inode->i_mode;
+
+	if (!capable(CAP_FSETID)) {
+		if (unlikely(mode & S_ISUID))
+			return 1;
+
+		if (unlikely((mode & S_ISGID) && (mode & S_IXGRP)))
+			return 1;
+	}
+	return 0;
+}
+
+/*
+ * This builds up the locking state that will be used by a write.  both normal
+ * file writes and AIO writes come in through here.  This function does no
+ * teardown on its own.  The caller must examine the info struct to see if it
+ * needs to release locks or i_mutex, etc.  This function is also restartable in
+ * that it can return EIOCBRETRY if it would have blocked in the dlm.  It
+ * stores its partial progress in the info struct so the caller can call back
+ * in when it thinks the dlm won't block any more.  Thus, the caller must zero
+ * the info struct before calling in the first time.
+ */
+ssize_t ocfs2_write_lock_maybe_extend(struct file *filp,
+				      const char __user *buf,
+				      size_t count,
+				      loff_t *ppos,
+				      struct ocfs2_write_lock_info *info,
+				      struct ocfs2_buffer_lock_ctxt *ctxt)
+{
+	int ret = 0;
+	struct ocfs2_super *osb = NULL;
+	struct dentry *dentry = filp->f_dentry;
+	struct inode *inode = dentry->d_inode;
+	int status;
+	int level = filp->f_flags & O_APPEND;
+	loff_t saved_ppos;
+	u64 bytes_added = 0;
+
+	osb = OCFS2_SB(inode->i_sb);
+
+	/* the target inode is different from the other inodes.  in o_direct it
+	 * doesn't get a data lock and when appending it gets a level 1 meta
+	 * lock.  we use target_binode to set its flags accordingly */
+	if (info->wl_target_binode == NULL) {
+		ret = ocfs2_setup_io_locks(inode->i_sb, inode,
+					   (char __user *) buf,
+					   count, ctxt,
+					   &info->wl_target_binode);
+		if (ret < 0) {
+			BUG_ON(ret == -EIOCBRETRY);
+			mlog_errno(ret);
+			goto bail;
+		}
+	}
+
+	/* This will lock everyone in the context who's order puts
+	 * them before us. */
+	if (!info->wl_have_before) {
+		info->wl_unlock_ctxt = 1;
+		ret = ocfs2_lock_buffer_inodes(ctxt, inode);
+		if (ret < 0) {
+			if (ret != -EIOCBRETRY)
+				mlog_errno(ret);
+			goto bail;
+		}
+		info->wl_have_before = 1;
+		/* we're writing so get an ex data cluster lock */
+		info->wl_target_binode->ba_lock_data_level = 1;
+	}
+
+	if (!info->wl_have_i_mutex) {
+		mutex_lock(&inode->i_mutex);
+		info->wl_have_i_mutex = 1;
+	}
+
+lock:
+	if (!info->wl_have_target_meta) {
+		status = ocfs2_meta_lock(inode, NULL, NULL, level);
+		if (status < 0) {
+			mlog_errno(status);
+			ret = status;
+			goto bail;
+		}
+		info->wl_have_target_meta = 1;
+	}
+	/* to handle extending writes, we do a bit of our own locking
+	 * here, but we setup the ctxt do unlock for us (as well as
+	 * handle locking everything else. */
+	if (level)
+		info->wl_target_binode->ba_lock_meta_level = 1;
+
+	/* Clear suid / sgid if necessary. We do this here instead of
+	 * later in the write path because remove_suid() calls
+	 * ->setattr without any hint that we may have already done
+	 * our cluster locking. Since ocfs2_setattr() *must* take
+	 * cluster locks to proceeed, this will lead us to recursively
+	 * lock the inode. There's also the dinode i_size state which
+	 * can be lost via setattr during extending writes (we set
+	 * inode->i_size at the end of a write. */
+	if (ocfs2_write_should_remove_suid(inode)) {
+		if (!level) {
+			mlog(0, "inode %"MLFu64", had a PR, looping back for "
+			     "EX so we can remove SUID\n",
+			     OCFS2_I(inode)->ip_blkno);
+			ocfs2_meta_unlock(inode, level);
+			info->wl_have_target_meta = 0;
+			level = 1;
+			goto lock;
+		}
+
+		status = ocfs2_write_remove_suid(inode);
+		if (status < 0) {
+			mlog_errno(status);
+			ret = status;
+			ocfs2_meta_unlock(inode, level);
+			info->wl_have_target_meta = 0;
+			goto bail;
+		}
+	}
+
+	/* work on a copy of ppos until we're sure that we won't have
+	 * to recalculate it due to relocking. */
+	saved_ppos = *ppos;
+
+	if (filp->f_flags & O_APPEND) {
+		saved_ppos = i_size_read(inode);
+		mlog(0, "O_APPEND: inode->i_size=%llu\n", saved_ppos);
+
+#ifdef OCFS2_ORACORE_WORKAROUNDS
+		if (osb->s_mount_opt & OCFS2_MOUNT_COMPAT_OCFS) {
+			/* ugh, work around some applications which open
+			 * everything O_DIRECT + O_APPEND and really don't
+			 * mean to use O_DIRECT. */
+			filp->f_flags &= ~O_DIRECT;
+		}
+#endif
+	}
+
+	if (filp->f_flags & O_DIRECT) {
+#ifdef OCFS2_ORACORE_WORKAROUNDS
+		if (osb->s_mount_opt & OCFS2_MOUNT_COMPAT_OCFS) {
+			int sector_size = 1 << osb->s_sectsize_bits;
+
+			if ((saved_ppos & (sector_size - 1)) ||
+			    (count & (sector_size - 1)) ||
+			    ((unsigned long)buf & (sector_size - 1))) {
+				info->wl_do_direct_io = 0;
+				filp->f_flags |= O_SYNC;
+			} else {
+				info->wl_do_direct_io = 1;
+			}
+		} else
+#endif
+			info->wl_do_direct_io = 1;
+
+		mlog(0, "O_DIRECT\n");
+	}
+
+	info->wl_target_binode->ba_lock_data = info->wl_do_direct_io ? 0 : 1;
+
+	info->wl_newsize = count + saved_ppos;
+	if (filp->f_flags & O_APPEND)
+		info->wl_newsize = count + i_size_read(inode);
+
+	mlog(0, "ppos=%lld newsize=%"MLFu64" cursize=%lld\n", saved_ppos,
+	     info->wl_newsize, i_size_read(inode));
+
+	if (info->wl_newsize > i_size_read(inode)) {
+		if (!level) {
+			/* we want an extend, but need a higher
+			 * level cluster lock. */
+			mlog(0, "inode %"MLFu64", had a PR, looping back "
+			     "for EX\n", OCFS2_I(inode)->ip_blkno);
+			ocfs2_meta_unlock(inode, level);
+			info->wl_have_target_meta = 0;
+			level = 1;
+			goto lock;
+		}
+
+		mlog(0, "Writing at EOF, will need more allocation: "
+		     "i_size=%lld, need=%"MLFu64"\n", i_size_read(inode),
+		     info->wl_newsize);
+
+		/* If we extend AT ALL here then we update our state
+		 * and continue the write call, regardless of error --
+		 * this is basically a short write. */
+		status = ocfs2_extend_file(osb, inode, info->wl_newsize,
+					   &bytes_added);
+		if (status < 0 && (!bytes_added)) {
+			if (status != -ERESTARTSYS
+			    && status != -EINTR
+			    && status != -ENOSPC) {
+				mlog_errno(status);
+				mlog(ML_ERROR, "Failed to extend inode %"MLFu64
+				     " from %lld to %"MLFu64,
+				     OCFS2_I(inode)->ip_blkno,
+				     *ppos, info->wl_newsize);
+			}
+			ret = status;
+
+			info->wl_have_target_meta = 0;
+			ocfs2_meta_unlock(inode, level);
+			goto bail;
+		}
+
+		info->wl_extended = 1;
+
+		/* We need to recalulate newsize and count according
+		 * to what extend could give us. If we got the whole
+		 * extend then this doesn't wind up changing the
+		 * values. */
+		info->wl_newsize = i_size_read(inode) + bytes_added;
+		count = info->wl_newsize - saved_ppos;
+
+		if (status < 0
+		    && status != -ENOSPC
+		    && status != -EINTR
+		    && status != -ERESTARTSYS)
+			mlog(ML_ERROR, "status return of %d extending inode "
+			     "%"MLFu64"\n", status,
+			     OCFS2_I(inode)->ip_blkno);
+		status = 0;
+	}
+
+	/* we've got whatever cluster lock is appropriate now, so we
+	 * can stuff *ppos back. */
+	*ppos = saved_ppos;
+
+	if (!info->wl_do_direct_io && !info->wl_have_data_lock) {
+		status = ocfs2_data_lock(inode, 1);
+		if (status < 0) {
+			mlog_errno(status);
+			ret = status;
+
+			info->wl_have_target_meta = 0;
+			ocfs2_meta_unlock(inode, level);
+			goto bail;
+		}
+		info->wl_have_data_lock = 1;
+	}
+
+	/* Alright, fool the io locking stuff into thinking it's
+	 * handled our inode for us. We can now count on it to do the
+	 * unlock for us. */
+	info->wl_target_binode->ba_locked = 1;
+
+	/* This will lock everyone who's order puts them *after* our inode. */
+	ret = ocfs2_lock_buffer_inodes(ctxt, NULL);
+	if (ret < 0) {
+		if (ret != -EIOCBRETRY)
+			mlog_errno(ret);
+		goto bail;
+	}
+
+bail:
+	mlog_exit(ret);
+	return ret;
+}
+
+#if 0
+static void ocfs2_buffer_ctxt_debug(struct ocfs2_buffer_lock_ctxt *ctxt)
+{
+	struct ocfs2_backing_inode *binode;
+	struct inode *inode;
+	struct rb_node *node;
+
+	printk("(%u) ocfs2: buffer lock ctxt: direct io = %d\n",
+	       current->pid, ctxt->b_lock_direct);
+
+	node = rb_first(&ctxt->b_inodes);
+	while (node) {
+		binode = rb_entry(node, struct ocfs2_backing_inode, ba_node);
+		inode = binode->ba_inode;
+
+		printk("(%u) ocfs2: inode %llu, locked %d, is target? %s\n",
+		       current->pid, OCFS2_I(inode)->ip_blkno,
+		       binode->ba_locked,
+		       ocfs2_buffer_lock_is_target(ctxt, inode) ? "yes" :
+		       "no");
+
+		node = rb_next(node);
+	}
+}
+#endif
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/mmap.h linux-2.6.15-ocfs2/fs/ocfs2/mmap.h
--- linux-2.6.15-2/fs/ocfs2/mmap.h	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/mmap.h	2006-02-06 17:38:14.000000000 -0500
@@ -1,6 +1,131 @@
 #ifndef OCFS2_MMAP_H
 #define OCFS2_MMAP_H
 
-int ocfs2_mmap(struct file *file, struct vm_area_struct *vma);
+int ocfs2_mmap(struct file *file,
+	       struct vm_area_struct *vma);
+
+/* used by file_read/file_write and nopage to coordinate file
+ * locking. I keep this out of the dlmglue code, because quite frankly
+ * I don't like that we have to do this stuff. */
+struct ocfs2_io_marker {
+	struct list_head io_list;
+	struct task_struct *io_task;
+};
+
+#define __IOMARKER_INITIALIZER(name) {					\
+	.io_list      = { &(name).io_list, &(name).io_list },		\
+	.io_task      = NULL }
+
+#define DECLARE_IO_MARKER(name)						\
+	struct ocfs2_io_marker name = __IOMARKER_INITIALIZER(name)
+
+static inline void ocfs2_init_io_marker(struct ocfs2_io_marker *task)
+{
+	INIT_LIST_HEAD(&task->io_list);
+	task->io_task = NULL;
+}
+
+static inline void ocfs2_add_io_marker(struct inode *inode,
+				       struct ocfs2_io_marker *task)
+{
+	struct ocfs2_inode_info *oi = OCFS2_I(inode);
+
+	task->io_task = current;
+	spin_lock(&oi->ip_lock);
+	list_add(&task->io_list, &oi->ip_io_markers);
+	spin_unlock(&oi->ip_lock);
+}
+
+static inline void ocfs2_del_io_marker(struct inode *inode,
+				       struct ocfs2_io_marker *task)
+{
+	spin_lock(&OCFS2_I(inode)->ip_lock);
+	if (!list_empty(&task->io_list))
+		list_del_init(&task->io_list);
+	spin_unlock(&OCFS2_I(inode)->ip_lock);
+}
+
+static inline int ocfs2_is_in_io_marker_list(struct inode *inode,
+					   struct task_struct *task)
+{
+	int ret = 0;
+	struct ocfs2_inode_info *oi = OCFS2_I(inode);
+	struct list_head *p;
+	struct ocfs2_io_marker *tmp;
+
+	spin_lock(&oi->ip_lock);
+	list_for_each(p, &oi->ip_io_markers) {
+		tmp = list_entry(p, struct ocfs2_io_marker, io_list);
+		if (tmp->io_task == task) {
+			ret = 1;
+			break;
+		}
+	}
+	spin_unlock(&oi->ip_lock);
+
+	return ret;
+}
+
+struct ocfs2_backing_inode {
+	struct rb_node           ba_node;
+	struct inode            *ba_inode;
+	unsigned		 ba_meta_locked:1, 	/* meta is locked */
+				 ba_locked:1,		/* both are locked */
+				 ba_lock_data:1,	/* should lock data */
+				 ba_lock_meta_level:1,
+				 ba_lock_data_level:1;
+	struct ocfs2_io_marker   ba_task;
+};
+
+/* Used to manage the locks taken during I/O. */
+struct ocfs2_buffer_lock_ctxt {
+	struct rb_root			b_inodes;
+	struct ocfs2_backing_inode	*b_next_unlocked;
+	ocfs2_lock_callback		b_cb;
+	unsigned long			b_cb_data;
+};
+
+#define __BUFFERLOCK_INITIALIZER {					\
+	.b_inodes               = RB_ROOT,				\
+	.b_next_unlocked	= NULL,					\
+	.b_cb			= NULL,					\
+	.b_cb_data		= 0 }
+
+#define DECLARE_BUFFER_LOCK_CTXT(name)					\
+	struct ocfs2_buffer_lock_ctxt name = __BUFFERLOCK_INITIALIZER
+
+#define INIT_BUFFER_LOCK_CTXT(ctxt)	\
+	*(ctxt) = (struct ocfs2_buffer_lock_ctxt) __BUFFERLOCK_INITIALIZER
+
+int ocfs2_setup_io_locks(struct super_block *sb,
+			 struct inode *target_inode,
+			 char __user *buf,
+			 size_t size,
+			 struct ocfs2_buffer_lock_ctxt *ctxt,
+			 struct ocfs2_backing_inode **target_binode);
+
+int ocfs2_lock_buffer_inodes(struct ocfs2_buffer_lock_ctxt *ctxt,
+			     struct inode *last_inode);
+
+void ocfs2_unlock_buffer_inodes(struct ocfs2_buffer_lock_ctxt *ctxt);
+
+struct ocfs2_write_lock_info {
+	u64				wl_newsize;
+	unsigned			wl_extended:1,
+					wl_do_direct_io:1,
+					wl_have_i_mutex:1,
+					wl_unlock_ctxt:1,
+					wl_have_before:1,
+					wl_have_target_meta:1,
+					wl_have_data_lock:1;
+	struct ocfs2_backing_inode	*wl_target_binode;
+};
+
+ssize_t ocfs2_write_lock_maybe_extend(struct file *filp,
+				      const char __user *buf,
+				     size_t count,
+				      loff_t *ppos,
+				     struct ocfs2_write_lock_info *info,
+				     struct ocfs2_buffer_lock_ctxt *ctxt);
 
 #endif  /* OCFS2_MMAP_H */
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/namei.c linux-2.6.15-ocfs2/fs/ocfs2/namei.c
--- linux-2.6.15-2/fs/ocfs2/namei.c	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/namei.c	2006-02-06 17:59:23.000000000 -0500
@@ -1629,9 +1629,8 @@ static int ocfs2_symlink(struct inode *d
 	newsize = l - 1;
 	if (l > ocfs2_fast_symlink_chars(sb)) {
 		inode->i_op = &ocfs2_symlink_inode_operations;
-		status = ocfs2_do_extend_allocation(osb, inode, 1, new_fe_bh,
-						    handle, data_ac, NULL,
-						    NULL);
+		status = ocfs2_extend_allocation(osb, inode, 1, new_fe_bh,
+						 handle, data_ac, NULL, NULL);
 		if (status < 0) {
 			if (status != -ENOSPC && status != -EINTR) {
 				mlog(ML_ERROR, "Failed to extend file to "
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/ocfs2.h linux-2.6.15-ocfs2/fs/ocfs2/ocfs2.h
--- linux-2.6.15-2/fs/ocfs2/ocfs2.h	2006-02-06 18:15:15.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/ocfs2.h	2006-02-06 17:59:23.000000000 -0500
@@ -33,7 +33,6 @@
 #include <linux/rbtree.h>
 #include <linux/workqueue.h>
 #include <linux/kref.h>
-#include <linux/mutex.h>
 
 #include "cluster/nodemanager.h"
 #include "cluster/heartbeat.h"
@@ -110,13 +109,24 @@ struct ocfs2_lock_res_ops;
 
 typedef void (*ocfs2_lock_callback)(int status, unsigned long data);
 
+struct ocfs2_lockres_flag_callback {
+	struct list_head	fc_lockres_item;
+	unsigned		fc_free_once_called:1;
+
+	unsigned long		fc_flag_mask;
+	unsigned long		fc_flag_goal;
+
+	ocfs2_lock_callback	fc_cb;
+	unsigned long		fc_data;
+};
+
 struct ocfs2_lock_res {
 	void                    *l_priv;
 	struct ocfs2_lock_res_ops *l_ops;
 	spinlock_t               l_lock;
 
 	struct list_head         l_blocked_list;
-	struct list_head         l_mask_waiters;
+	struct list_head         l_flag_cb_list;
 
 	enum ocfs2_lock_type     l_type;
 	unsigned long		 l_flags;
@@ -173,7 +183,6 @@ enum ocfs2_mount_options
 	OCFS2_MOUNT_BARRIER = 1 << 1,	/* Use block barriers */
 	OCFS2_MOUNT_NOINTR  = 1 << 2,   /* Don't catch signals */
 	OCFS2_MOUNT_ERRORS_PANIC = 1 << 3, /* Panic on errors */
-	OCFS2_MOUNT_DATA_WRITEBACK = 1 << 4, /* No data ordering */
 #ifdef OCFS2_ORACORE_WORKAROUNDS
 	OCFS2_MOUNT_COMPAT_OCFS = 1 << 30, /* ocfs1 compatibility mode */
 #endif
@@ -234,7 +243,7 @@ struct ocfs2_super
 	struct proc_dir_entry *proc_sub_dir; /* points to /proc/fs/ocfs2/<maj_min> */
 
 	atomic_t vol_state;
-	struct mutex recovery_lock;
+	struct semaphore recovery_lock;
 	struct task_struct *recovery_thread_task;
 	int disable_recovery;
 	wait_queue_head_t checkpoint_event;
@@ -284,6 +293,12 @@ struct ocfs2_super
 
 	struct list_head	osb_net_handlers;
 
+	/* see ocfs2_ki_dtor() */
+	struct work_struct		osb_okp_teardown_work;
+	struct ocfs2_kiocb_private	*osb_okp_teardown_next;
+	atomic_t			osb_okp_pending;
+	wait_queue_head_t		osb_okp_pending_wq;
+
 	wait_queue_head_t		osb_mount_event;
 
 	/* Truncate log info */
@@ -295,15 +310,6 @@ struct ocfs2_super
 #define OCFS2_SB(sb)	    ((struct ocfs2_super *)(sb)->s_fs_info)
 #define OCFS2_MAX_OSB_ID             65536
 
-static inline int ocfs2_should_order_data(struct inode *inode)
-{
-	if (!S_ISREG(inode->i_mode))
-		return 0;
-	if (OCFS2_SB(inode->i_sb)->s_mount_opt & OCFS2_MOUNT_DATA_WRITEBACK)
-		return 0;
-	return 1;
-}
-
 /* set / clear functions because cluster events can make these happen
  * in parallel so we want the transitions to be atomic. this also
  * means that any future flags osb_flags must be protected by spinlock
@@ -420,13 +426,6 @@ static inline unsigned int ocfs2_cluster
 	return clusters;
 }
 
-static inline u64 ocfs2_blocks_for_bytes(struct super_block *sb,
-					 u64 bytes)
-{
-	bytes += sb->s_blocksize - 1;
-	return bytes >> sb->s_blocksize_bits;
-}
-
 static inline u64 ocfs2_clusters_to_bytes(struct super_block *sb,
 					  u32 clusters)
 {
@@ -443,15 +442,6 @@ static inline u64 ocfs2_align_bytes_to_c
 	return (u64)clusters << cl_bits;
 }
 
-static inline u64 ocfs2_align_bytes_to_blocks(struct super_block *sb,
-					      u64 bytes)
-{
-	u64 blocks;
-
-        blocks = ocfs2_blocks_for_bytes(sb, bytes);
-	return blocks << sb->s_blocksize_bits;
-}
-
 static inline unsigned long ocfs2_align_bytes_to_sectors(u64 bytes)
 {
 	return (unsigned long)((bytes + 511) >> 9);
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/ocfs2_lockid.h linux-2.6.15-ocfs2/fs/ocfs2/ocfs2_lockid.h
--- linux-2.6.15-2/fs/ocfs2/ocfs2_lockid.h	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/ocfs2_lockid.h	2006-02-06 17:38:14.000000000 -0500
@@ -40,7 +40,6 @@ enum ocfs2_lock_type {
 	OCFS2_LOCK_TYPE_DATA,
 	OCFS2_LOCK_TYPE_SUPER,
 	OCFS2_LOCK_TYPE_RENAME,
-	OCFS2_LOCK_TYPE_RW,
 	OCFS2_NUM_LOCK_TYPES
 };
 
@@ -60,9 +59,6 @@ static inline char ocfs2_lock_type_char(
 		case OCFS2_LOCK_TYPE_RENAME:
 			c = 'R';
 			break;
-		case OCFS2_LOCK_TYPE_RW:
-			c = 'W';
-			break;
 		default:
 			c = '\0';
 	}
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/proc.c linux-2.6.15-ocfs2/fs/ocfs2/proc.c
--- linux-2.6.15-2/fs/ocfs2/proc.c	1969-12-31 19:00:00.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/proc.c	2006-02-06 17:59:23.000000000 -0500
@@ -0,0 +1,461 @@
+/* -*- mode: c; c-basic-offset: 8; -*-
+ * vim: noexpandtab sw=8 ts=8 sts=0:
+ *
+ * proc.c
+ *
+ * proc interface
+ *
+ * Copyright (C) 2002, 2004 Oracle.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public
+ * License as published by the Free Software Foundation; either
+ * version 2 of the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public
+ * License along with this program; if not, write to the
+ * Free Software Foundation, Inc., 59 Temple Place - Suite 330,
+ * Boston, MA 021110-1307, USA.
+ */
+
+#include <linux/fs.h>
+#include <linux/types.h>
+#include <linux/slab.h>
+#include <linux/proc_fs.h>
+#include <linux/socket.h>
+
+#define MLOG_MASK_PREFIX ML_SUPER
+#include <cluster/masklog.h>
+
+#include "ocfs2.h"
+
+#include "proc.h"
+#include "alloc.h"
+#include "heartbeat.h"
+#include "inode.h"
+#include "journal.h"
+#include "ver.h"
+
+#define OCFS2_PROC_BASENAME    "fs/ocfs2"
+
+static struct proc_dir_entry *ocfs2_proc_root_dir = NULL; /* points to /proc/fs/ocfs2 */
+
+static int ocfs2_proc_version(char *page,
+			      char **start,
+			      off_t off,
+			      int count,
+			      int *eof,
+			      void *data);
+static int ocfs2_proc_nodenum(char *page,
+			      char **start,
+			      off_t off,
+			      int count,
+			      int *eof,
+			      void *data);
+static int ocfs2_proc_slotnum(char *page,
+			      char **start,
+			      off_t off,
+			      int count,
+			      int *eof,
+			      void *data);
+static int ocfs2_proc_nodename(char *page,
+			       char **start,
+			       off_t off,
+			       int count,
+			       int *eof,
+			       void *data);
+static int ocfs2_proc_uuid(char *page,
+			   char **start,
+			   off_t off,
+			   int count,
+			   int *eof,
+			   void *data);
+static int ocfs2_proc_statistics(char *page,
+				 char **start,
+				 off_t off,
+				 int count,
+				 int *eof,
+				 void *data);
+static int ocfs2_proc_device(char *page,
+			     char **start,
+			     off_t off,
+			     int count,
+			     int *eof,
+			     void *data);
+static int ocfs2_proc_nodes(char *page,
+			    char **start,
+			    off_t off,
+			    int count,
+			    int *eof,
+			    void *data);
+static int ocfs2_proc_alloc_stat(char *page,
+				 char **start,
+				 off_t off,
+				 int count,
+				 int *eof,
+				 void *data);
+static int ocfs2_proc_label(char *page,
+			    char **start,
+			    off_t off,
+			    int count,
+			    int *eof,
+			    void *data);
+
+typedef struct _ocfs2_proc_list
+{
+	char *name;
+	char *data;
+	int (*read_proc) (char *, char **, off_t, int, int *, void *);
+	write_proc_t *write_proc;
+	mode_t mode;
+} ocfs2_proc_list;
+
+static ocfs2_proc_list top_dir[] = {
+	{ "version", NULL, ocfs2_proc_version, NULL, S_IFREG | S_IRUGO, },
+	{ "nodename", NULL, ocfs2_proc_nodename, NULL, S_IFREG | S_IRUGO, },
+	{ NULL }
+};
+
+static ocfs2_proc_list sub_dir[] = {
+	{ "nodenum", NULL, ocfs2_proc_nodenum, NULL, S_IFREG | S_IRUGO, },
+	{ "uuid", NULL, ocfs2_proc_uuid, NULL, S_IFREG | S_IRUGO, },
+	{ "slotnum", NULL, ocfs2_proc_slotnum, NULL, S_IFREG | S_IRUGO, },
+	{ "statistics", NULL, ocfs2_proc_statistics, NULL, S_IFREG | S_IRUGO, },
+	{ "device", NULL, ocfs2_proc_device, NULL, S_IFREG | S_IRUGO, },
+	{ "nodes", NULL, ocfs2_proc_nodes, NULL, S_IFREG | S_IRUGO, },
+	{ "allocstat", NULL, ocfs2_proc_alloc_stat, NULL, S_IFREG | S_IRUGO, },
+	{ "label", NULL, ocfs2_proc_label, NULL, S_IFREG | S_IRUGO, },
+	{ NULL }
+};
+
+int ocfs2_proc_init(void)
+{
+	struct proc_dir_entry *parent = NULL;
+	ocfs2_proc_list *p;
+	struct proc_dir_entry* entry;
+
+	mlog_entry_void();
+
+	parent = proc_mkdir(OCFS2_PROC_BASENAME, NULL);
+	if (parent) {
+		ocfs2_proc_root_dir = parent;
+		for (p = top_dir; p->name; p++) {
+			entry = create_proc_read_entry(p->name, p->mode,
+						       parent, p->read_proc,
+						       p->data);
+			if (!entry)
+				return -EINVAL;
+			if (p->write_proc)
+				entry->write_proc = p->write_proc;
+
+			entry->owner = THIS_MODULE;
+		}
+	}
+
+	mlog_exit_void();
+	return 0;
+}
+
+void ocfs2_proc_deinit(void)
+{
+	struct proc_dir_entry *parent = ocfs2_proc_root_dir;
+	ocfs2_proc_list *p;
+
+	mlog_entry_void();
+
+	if (parent) {
+		for (p = top_dir; p->name; p++)
+			remove_proc_entry(p->name, parent);
+		remove_proc_entry(OCFS2_PROC_BASENAME, NULL);
+	}
+
+	mlog_exit_void();
+}
+
+static int ocfs2_proc_calc_metrics(char *page, char **start, off_t off,
+				   int count, int *eof, int len)
+{
+	mlog_entry_void();
+
+	if (len <= off + count)
+		*eof = 1;
+
+	*start = page + off;
+
+	len -= off;
+
+	if (len > count)
+		len = count;
+
+	if (len < 0)
+		len = 0;
+
+	mlog_exit_void();
+	return len;
+}
+
+static int ocfs2_proc_alloc_stat(char *page, char **start, off_t off,
+				 int count, int *eof, void *data)
+{
+	int len, ret;
+	char *la_state;
+	struct ocfs2_super *osb = data;
+
+	mlog_entry_void();
+
+#define ALLOC_STATS_HDR "%-25s %10u\n"
+
+	len = sprintf(page, "%s\n", "*** Disk Allocation Stats ***");
+
+	if (osb->local_alloc_state == OCFS2_LA_ENABLED)
+		la_state = "enabled";
+	else if (osb->local_alloc_state == OCFS2_LA_DISABLED)
+		la_state = "disabled";
+	else
+		la_state = "unused";
+
+	len += sprintf(page + len, "%-25s %10s\n", "Local Alloc", la_state);
+	len += sprintf(page + len, ALLOC_STATS_HDR, "Window Moves",
+		       atomic_read(&osb->alloc_stats.moves));
+	len += sprintf(page + len, ALLOC_STATS_HDR, "Local Allocs",
+		       atomic_read(&osb->alloc_stats.local_data));
+	len += sprintf(page + len, ALLOC_STATS_HDR, "Bitmap Allocs",
+		       atomic_read(&osb->alloc_stats.bitmap_data));
+	len += sprintf(page + len, ALLOC_STATS_HDR, "Block Group Allocs",
+		       atomic_read(&osb->alloc_stats.bg_allocs));
+	len += sprintf(page + len, ALLOC_STATS_HDR, "Block Group Adds",
+		       atomic_read(&osb->alloc_stats.bg_extends));
+
+	ret = ocfs2_proc_calc_metrics(page, start, off, count, eof, len);
+
+	mlog_exit(ret);
+
+	return ret;
+}
+
+static int ocfs2_proc_version(char *page, char **start, off_t off,
+			      int count, int *eof, void *data)
+{
+	int len;
+	int ret;
+
+	mlog_entry_void();
+
+        len = ocfs2_str_version(page);
+	ret = ocfs2_proc_calc_metrics(page, start, off, count, eof, len);
+
+	mlog_exit(ret);
+	return ret;
+}
+
+static int ocfs2_proc_nodenum(char *page, char **start, off_t off,
+			      int count, int *eof, void *data)
+{
+	int len;
+	int ret;
+	struct ocfs2_super *osb = data;
+
+	mlog_entry_void();
+
+	len = sprintf(page, "%d\n", osb->node_num);
+	ret = ocfs2_proc_calc_metrics(page, start, off, count, eof, len);
+
+	mlog_exit(ret);
+	return ret;
+}
+
+static int ocfs2_proc_slotnum(char *page, char **start, off_t off,
+			      int count, int *eof, void *data)
+{
+	int len;
+	int ret;
+	struct ocfs2_super *osb = data;
+
+	mlog_entry_void();
+
+	len = sprintf(page, "%d\n", osb->slot_num);
+	ret = ocfs2_proc_calc_metrics(page, start, off, count, eof, len);
+
+	mlog_exit(ret);
+	return ret;
+}
+
+static int ocfs2_proc_nodename(char *page, char **start, off_t off,
+			       int count, int *eof, void *data)
+{
+	int len;
+	int ret;
+	struct o2nm_node *node;
+
+	mlog_entry_void();
+
+	node = o2nm_get_node_by_num(o2nm_this_node());
+
+	if (node) {
+		len = sprintf(page, "%s\n", node->nd_name);
+		o2nm_node_put(node);
+	} else
+		len = sprintf(page, "(unknown)\n");
+
+	ret = ocfs2_proc_calc_metrics(page, start, off, count, eof, len);
+
+	mlog_exit(ret);
+	return ret;
+}
+
+void ocfs2_proc_add_volume(struct ocfs2_super * osb)
+{
+	char newdir[20];
+	struct proc_dir_entry *parent = NULL;
+	struct proc_dir_entry* entry;
+	ocfs2_proc_list *p;
+
+	mlog_entry_void();
+
+	snprintf(newdir, sizeof(newdir), "%u_%u",
+		 MAJOR(osb->sb->s_dev), MINOR(osb->sb->s_dev));
+	parent = proc_mkdir(newdir, ocfs2_proc_root_dir);
+	osb->proc_sub_dir = parent;
+
+	if (!parent) {
+		mlog_exit_void();
+		return;
+	}
+
+	for (p = sub_dir; p->name; p++) {
+		/* XXX: What do we do if
+		 * create_proc_read_entry fails?! */
+		entry = create_proc_read_entry(p->name, p->mode,
+					       parent, p->read_proc,
+					       (char *)osb);
+		if (entry) {
+			if (p->write_proc)
+				entry->write_proc = p->write_proc;
+
+			entry->owner = THIS_MODULE;
+		}
+	}
+
+	mlog_exit_void();
+}
+
+void ocfs2_proc_remove_volume(struct ocfs2_super * osb)
+{
+	ocfs2_proc_list *p;
+	char dir[20];
+
+	mlog_entry_void();
+
+	if (osb->proc_sub_dir) {
+		for (p = sub_dir; p->name; p++)
+			remove_proc_entry(p->name, osb->proc_sub_dir);
+
+		snprintf(dir, sizeof(dir), "%u_%u",
+			 MAJOR(osb->sb->s_dev), MINOR(osb->sb->s_dev));
+		remove_proc_entry(dir, ocfs2_proc_root_dir);
+	}
+
+	mlog_exit_void();
+}
+
+static int ocfs2_proc_uuid(char *page, char **start, off_t off,
+			   int count, int *eof, void *data)
+{
+	int len, ret;
+	struct ocfs2_super *osb = data;
+
+	mlog_entry_void();
+
+	len = sprintf(page, "%s\n", osb->uuid_str);
+	ret = ocfs2_proc_calc_metrics(page, start, off, count, eof, len);
+
+	mlog_exit(ret);
+	return ret;
+}
+
+static int ocfs2_proc_statistics(char *page, char **start, off_t off,
+				 int count, int *eof, void *data)
+{
+	int len;
+	int ret = 0;
+	struct ocfs2_super *osb = data;
+
+	mlog_entry_void();
+
+#define PROC_STATS				\
+  "Number of nodes          : %u\n"		\
+  "Cluster size             : %d\n"		\
+  "Volume size              : %"MLFu64"\n"	\
+  "Open Transactions:       : %u\n"
+
+	len = sprintf(page, PROC_STATS, osb->num_nodes, osb->s_clustersize,
+		      ocfs2_clusters_to_bytes(osb->sb, osb->num_clusters),
+		      atomic_read(&osb->journal->j_num_trans));
+
+	ret = ocfs2_proc_calc_metrics(page, start, off, count, eof, len);
+
+	mlog_exit(ret);
+	return ret;
+}
+
+static int ocfs2_proc_device(char *page, char **start, off_t off,
+			     int count, int *eof, void *data)
+{
+	int len;
+	int ret;
+	struct ocfs2_super *osb = data;
+
+	mlog_entry_void();
+
+	len = snprintf(page, sizeof(osb->dev_str), "%s\n", osb->dev_str);
+	ret = ocfs2_proc_calc_metrics(page, start, off, count, eof, len);
+
+	mlog_exit(ret);
+	return ret;
+}
+
+static int ocfs2_proc_nodes(char *page, char **start, off_t off,
+			    int count, int *eof, void *data)
+{
+	int len = 0;
+	int i;
+	int ret;
+	struct ocfs2_super *osb = data;
+	char mount;
+
+	mlog_entry_void();
+
+	if (osb) {
+		for (i = 0; i < OCFS2_NODE_MAP_MAX_NODES; i++) {
+			mount = ocfs2_node_map_test_bit(osb, &osb->mounted_map, i) ? 'M' : ' ';
+			len += sprintf(page + len, "%2d %c\n", i, mount);
+		}
+	}
+
+	ret = ocfs2_proc_calc_metrics(page, start, off, count, eof, len);
+
+	mlog_exit(ret);
+	return ret;
+}
+
+static int ocfs2_proc_label(char *page, char **start, off_t off,
+			    int count, int *eof, void *data)
+{
+	int len;
+	int ret;
+	struct ocfs2_super *osb = data;
+
+	mlog_entry_void();
+
+	len = sprintf(page, "%s\n", osb->vol_label);
+	ret = ocfs2_proc_calc_metrics(page, start, off, count, eof, len);
+
+	mlog_exit(ret);
+	return ret;
+}
+
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/proc.h linux-2.6.15-ocfs2/fs/ocfs2/proc.h
--- linux-2.6.15-2/fs/ocfs2/proc.h	1969-12-31 19:00:00.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/proc.h	2006-02-06 17:59:23.000000000 -0500
@@ -0,0 +1,34 @@
+/* -*- mode: c; c-basic-offset: 8; -*-
+ * vim: noexpandtab sw=8 ts=8 sts=0:
+ *
+ * proc.h
+ *
+ * Function prototypes
+ *
+ * Copyright (C) 2002, 2004 Oracle.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public
+ * License as published by the Free Software Foundation; either
+ * version 2 of the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public
+ * License along with this program; if not, write to the
+ * Free Software Foundation, Inc., 59 Temple Place - Suite 330,
+ * Boston, MA 021110-1307, USA.
+ */
+
+#ifndef OCFS2_PROC_H
+#define OCFS2_PROC_H
+
+void ocfs2_proc_add_volume(struct ocfs2_super *osb);
+void ocfs2_proc_deinit(void);
+int ocfs2_proc_init(void);
+void ocfs2_proc_remove_volume(struct ocfs2_super *osb);
+
+#endif /* OCFS2_PROC_H */
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/super.c linux-2.6.15-ocfs2/fs/ocfs2/super.c
--- linux-2.6.15-2/fs/ocfs2/super.c	2006-02-06 18:15:15.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/super.c	2006-02-06 17:59:23.000000000 -0500
@@ -46,6 +46,7 @@
 #include <cluster/masklog.h>
 
 #include "ocfs2.h"
+#include "aio.h"
 
 /* this should be the only file to include a version 1 header */
 #include "ocfs1_fs_compat.h"
@@ -59,6 +60,7 @@
 #include "journal.h"
 #include "localalloc.h"
 #include "namei.h"
+#include "proc.h"
 #include "slot_map.h"
 #include "super.h"
 #include "sysfile.h"
@@ -138,16 +140,21 @@ static struct super_operations ocfs2_sop
 	.remount_fs	= ocfs2_remount,
 };
 
+#ifdef OCFS2_ORACORE_WORKAROUNDS
+#define OCFS_SUPER_MAGIC		0xa156f7eb
+#endif
+
 enum {
 	Opt_barrier,
 	Opt_err_panic,
 	Opt_err_ro,
+#ifdef OCFS2_ORACORE_WORKAROUNDS
+	Opt_datavolume,
+#endif
 	Opt_intr,
 	Opt_nointr,
 	Opt_hb_none,
 	Opt_hb_local,
-	Opt_data_ordered,
-	Opt_data_writeback,
 	Opt_err,
 };
 
@@ -155,12 +162,13 @@ static match_table_t tokens = {
 	{Opt_barrier, "barrier=%u"},
 	{Opt_err_panic, "errors=panic"},
 	{Opt_err_ro, "errors=remount-ro"},
+#ifdef OCFS2_ORACORE_WORKAROUNDS
+	{Opt_datavolume, "datavolume"},
+#endif
 	{Opt_intr, "intr"},
 	{Opt_nointr, "nointr"},
 	{Opt_hb_none, OCFS2_HB_NONE},
 	{Opt_hb_local, OCFS2_HB_LOCAL},
-	{Opt_data_ordered, "data=ordered"},
-	{Opt_data_writeback, "data=writeback"},
 	{Opt_err, NULL}
 };
 
@@ -169,7 +177,7 @@ static match_table_t tokens = {
  */
 static void ocfs2_write_super(struct super_block *sb)
 {
-	if (mutex_trylock(&sb->s_lock) != 0)
+	if (!mutex_trylock(&sb->s_lock) == 0)
 		BUG();
 	sb->s_dirt = 0;
 }
@@ -369,20 +377,6 @@ static int ocfs2_remount(struct super_bl
 		goto out;
 	}
 
-	if ((osb->s_mount_opt & OCFS2_MOUNT_HB_LOCAL) !=
-	    (parsed_options & OCFS2_MOUNT_HB_LOCAL)) {
-		ret = -EINVAL;
-		mlog(ML_ERROR, "Cannot change heartbeat mode on remount\n");
-		goto out;
-	}
-
-	if ((osb->s_mount_opt & OCFS2_MOUNT_DATA_WRITEBACK) !=
-	    (parsed_options & OCFS2_MOUNT_DATA_WRITEBACK)) {
-		ret = -EINVAL;
-		mlog(ML_ERROR, "Cannot change data mode on remount\n");
-		goto out;
-	}
-
 	/* We're going to/from readonly mode. */
 	if ((*flags & MS_RDONLY) != (sb->s_flags & MS_RDONLY)) {
 		/* Lock here so the check of HARD_RO and the potential
@@ -555,7 +549,12 @@ static int ocfs2_fill_super(struct super
 	}
 	osb->s_mount_opt = parsed_opt;
 
-	sb->s_magic = OCFS2_SUPER_MAGIC;
+#ifdef OCFS2_ORACORE_WORKAROUNDS
+	if (osb->s_mount_opt & OCFS2_MOUNT_COMPAT_OCFS)
+		sb->s_magic = OCFS_SUPER_MAGIC;
+	else
+#endif
+		sb->s_magic = OCFS2_SUPER_MAGIC;
 
 	/* Hard readonly mode only if: bdev_read_only, MS_RDONLY,
 	 * heartbeat=none */
@@ -642,12 +641,9 @@ static int ocfs2_fill_super(struct super
 
 	ocfs2_complete_mount_recovery(osb);
 
-	printk("ocfs2: Mounting device (%u,%u) on (node %d, slot %d) with %s "
-	       "data mode.\n",
+	printk("ocfs2: Mounting device (%u,%u) on (node %d, slot %d)\n",
 	       MAJOR(sb->s_dev), MINOR(sb->s_dev), osb->node_num,
-	       osb->slot_num,
-	       osb->s_mount_opt & OCFS2_MOUNT_DATA_WRITEBACK ? "writeback" :
-	       "ordered");
+	       osb->slot_num);
 
 	atomic_set(&osb->vol_state, VOLUME_MOUNTED);
 	wake_up(&osb->osb_mount_event);
@@ -747,12 +743,17 @@ static int ocfs2_parse_options(struct su
 		case Opt_err_ro:
 			*mount_opt &= ~OCFS2_MOUNT_ERRORS_PANIC;
 			break;
-		case Opt_data_ordered:
-			*mount_opt &= ~OCFS2_MOUNT_DATA_WRITEBACK;
-			break;
-		case Opt_data_writeback:
-			*mount_opt |= OCFS2_MOUNT_DATA_WRITEBACK;
+#ifdef OCFS2_ORACORE_WORKAROUNDS
+		case Opt_datavolume:
+			if (is_remount) {
+				mlog(ML_ERROR, "Cannot specifiy datavolume "
+				     "on remount.\n");
+				status = 0;
+				goto bail;
+			}
+			*mount_opt |= OCFS2_MOUNT_COMPAT_OCFS;
 			break;
+#endif
 		default:
 			mlog(ML_ERROR,
 			     "Unrecognized mount option \"%s\" "
@@ -808,6 +809,9 @@ static int __init ocfs2_init(void)
 		mlog(ML_ERROR, "Unable to create ocfs2 debugfs root.\n");
 	}
 
+	/* Initialize the proc interface */
+	ocfs2_proc_init();
+
 leave:
 	if (status < 0) {
 		ocfs2_free_mem_caches();
@@ -832,10 +836,13 @@ static void __exit ocfs2_exit(void)
 		destroy_workqueue(ocfs2_wq);
 	}
 
-	debugfs_remove(ocfs2_debugfs_root);
-
 	ocfs2_free_mem_caches();
 
+	/* Deinit the proc interface */
+	ocfs2_proc_deinit();
+
+	debugfs_remove(ocfs2_debugfs_root);
+
 	unregister_filesystem(&ocfs2_fs_type);
 
 	exit_ocfs2_extent_maps();
@@ -888,7 +895,12 @@ static int ocfs2_statfs(struct super_blo
 	numbits = le32_to_cpu(bm_lock->id1.bitmap1.i_total);
 	freebits = numbits - le32_to_cpu(bm_lock->id1.bitmap1.i_used);
 
-	buf->f_type = OCFS2_SUPER_MAGIC;
+#ifdef OCFS2_ORACORE_WORKAROUNDS
+	if (osb->s_mount_opt & OCFS2_MOUNT_COMPAT_OCFS)
+		buf->f_type = OCFS_SUPER_MAGIC;
+	else
+#endif
+		buf->f_type = OCFS2_SUPER_MAGIC;
 	buf->f_bsize = sb->s_blocksize;
 	buf->f_namelen = OCFS2_MAX_FILENAME_LEN;
 	buf->f_blocks = ((sector_t) numbits) *
@@ -932,12 +944,12 @@ static void ocfs2_inode_init_once(void *
 		oi->ip_dir_start_lookup = 0;
 
 		init_rwsem(&oi->ip_alloc_sem);
-		mutex_init(&oi->ip_io_mutex);
+		init_MUTEX(&(oi->ip_io_sem));
 
 		oi->ip_blkno = 0ULL;
 		oi->ip_clusters = 0;
+		oi->ip_mmu_private = 0LL;
 
-		ocfs2_lock_res_init_once(&oi->ip_rw_lockres);
 		ocfs2_lock_res_init_once(&oi->ip_meta_lockres);
 		ocfs2_lock_res_init_once(&oi->ip_data_lockres);
 
@@ -1032,7 +1044,7 @@ static int ocfs2_mount_volume(struct sup
 	mlog_entry_void();
 
 	if (ocfs2_is_hard_readonly(osb))
-		goto leave;
+		goto out_add_proc;
 
 	status = ocfs2_fill_local_node_info(osb);
 	if (status < 0) {
@@ -1103,6 +1115,10 @@ static int ocfs2_mount_volume(struct sup
 	if (status < 0)
 		mlog_errno(status);
 
+out_add_proc:
+	/* Add proc entry for this volume */
+	ocfs2_proc_add_volume(osb);
+
 leave:
 	if (unlock_super)
 		ocfs2_super_unlock(osb, 1);
@@ -1131,15 +1147,17 @@ static void ocfs2_dismount_volume(struct
 	osb = OCFS2_SB(sb);
 	BUG_ON(!osb);
 
+	ocfs2_wait_for_okp_destruction(osb);
+
 	ocfs2_shutdown_local_alloc(osb);
 
 	ocfs2_truncate_log_shutdown(osb);
 
 	/* disable any new recovery threads and wait for any currently
 	 * running ones to exit. Do this before setting the vol_state. */
-	mutex_lock(&osb->recovery_lock);
+	down(&osb->recovery_lock);
 	osb->disable_recovery = 1;
-	mutex_unlock(&osb->recovery_lock);
+	up(&osb->recovery_lock);
 	wait_event(osb->recovery_event, !ocfs2_recovery_thread_running(osb));
 
 	/* At this point, we know that no more recovery threads can be
@@ -1151,6 +1169,9 @@ static void ocfs2_dismount_volume(struct
 
 	ocfs2_sync_blockdev(sb);
 
+	/* Remove the proc element for this volume */
+	ocfs2_proc_remove_volume(osb);
+
 	/* No dlm means we've failed during mount, so skip all the
 	 * steps which depended on that to complete. */
 	if (osb->dlm) {
@@ -1254,7 +1275,8 @@ static int ocfs2_initialize_super(struct
 	osb->sb = sb;
 	/* Save off for ocfs2_rw_direct */
 	osb->s_sectsize_bits = blksize_bits(sector_size);
-	BUG_ON(!osb->s_sectsize_bits);
+	if (!osb->s_sectsize_bits)
+		BUG();
 
 	osb->net_response_ids = 0;
 	spin_lock_init(&osb->net_response_lock);
@@ -1271,6 +1293,12 @@ static int ocfs2_initialize_super(struct
 	INIT_LIST_HEAD(&osb->vote_list);
 	spin_lock_init(&osb->osb_lock);
 
+	osb->osb_okp_teardown_next = NULL;
+	atomic_set(&osb->osb_okp_pending, 0);
+	init_waitqueue_head(&osb->osb_okp_pending_wq);
+	/* we sync with this work queue (and sb ref) on unmount */
+	INIT_WORK(&osb->osb_okp_teardown_work, okp_teardown_from_list, osb);
+
 	atomic_set(&osb->alloc_stats.moves, 0);
 	atomic_set(&osb->alloc_stats.local_data, 0);
 	atomic_set(&osb->alloc_stats.bitmap_data, 0);
@@ -1282,7 +1310,7 @@ static int ocfs2_initialize_super(struct
 	snprintf(osb->dev_str, sizeof(osb->dev_str), "%u,%u",
 		 MAJOR(osb->sb->s_dev), MINOR(osb->sb->s_dev));
 
-	mutex_init(&osb->recovery_lock);
+	init_MUTEX(&osb->recovery_lock);
 
 	osb->disable_recovery = 0;
 	osb->recovery_thread_task = NULL;
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/symlink.c linux-2.6.15-ocfs2/fs/ocfs2/symlink.c
--- linux-2.6.15-2/fs/ocfs2/symlink.c	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/symlink.c	2006-02-06 17:59:23.000000000 -0500
@@ -28,7 +28,7 @@
  *
  *  Portions Copyright (C) 2001 Compaq Computer Corporation
  *
- *  ocfs2 symlink handling code.
+ *  ocfs2 symlink handling code, including CDSL support
  *
  *  Copyright (C) 2004, 2005 Oracle.
  *
@@ -135,6 +135,248 @@ out:
 	return ret;
 }
 
+#ifdef OCFS2_CDSL
+
+struct ocfs2_symlink_ops {
+	const char *name;
+	const unsigned int len;
+	unsigned int (*subst_fn) (char *str, void *data);
+};
+
+/**
+ *** sym_hostname - Substitute system host name
+ *** @str: String for result
+ *** @len: Length of result buffer
+ ***
+ *** Returns: Length of hostname
+ ***/
+static unsigned int
+sym_hostname(char *str, void *data)
+{
+	  unsigned int l = strlen(system_utsname.nodename);
+
+	  if (str)
+		memcpy(str, system_utsname.nodename, l);
+
+	  return l;
+}
+
+/**
+ *** sym_machine - Substitute machine type
+ *** @str: String for result
+ *** @len: Length of result buffer
+ ***
+ *** Returns: Length of machine type
+ ***/
+
+static unsigned int
+sym_machine(char *str, void *data)
+{
+	unsigned int l = strlen(system_utsname.machine);
+
+	if (str)
+	       memcpy(str, system_utsname.machine, l);
+
+	return l;
+}
+
+/**
+ *** sym_os - Substitute OS name
+ *** @str: String for result
+ *** @len: Length of result buffer
+ ***
+ *** Returns: Length of OS name
+ ***/
+
+static unsigned int
+sym_os(char *str, void *data)
+{
+	unsigned int l = strlen(system_utsname.sysname);
+
+	if (str)
+	       memcpy(str, system_utsname.sysname, l);
+
+	return l;
+}
+
+/**
+ *** sym_nodenum - Substitute node number
+ *** @str: String for result
+ *** @len: Length of result buffer
+ ***
+ *** Returns: Length of  nodeNum
+ ***/
+
+static unsigned int
+sym_nodenum(char *str, void *data)
+{
+	unsigned int l;
+	char buf[10];
+	struct inode *inode = data;
+	struct ocfs2_super *osb = OCFS2_SB(inode->i_sb);
+
+	l = sprintf(buf, "%lu", (unsigned long)osb->node_num);
+
+	if (str) {
+	      memcpy(str, buf, l);
+	      str[l] = '\0';
+	}
+
+	return l;
+}
+
+static unsigned int
+sym_system(char *str, void *data)
+{
+	unsigned int ml = strlen(system_utsname.machine);
+	unsigned int sl = strlen(system_utsname.sysname);
+	unsigned int l = ml + sl + 1;
+
+	if (str) {
+	       memcpy(str, system_utsname.machine, ml);
+	       str[ml] = '_';
+	       memcpy(str + ml + 1, system_utsname.sysname, sl);
+	       str[l] = '\0';
+	};
+
+	return l;
+}
+
+static unsigned int
+sym_uid(char *str, void *data)
+{
+	unsigned int l;
+	char buf[10];
+
+	l = sprintf(buf, "%lu", (unsigned long)current->fsuid);
+
+	if (str) {
+	      memcpy(str, buf, l);
+	      str[l] = '\0';
+	}
+
+	return l;
+}
+
+static unsigned int
+sym_gid(char *str, void *data)
+{
+	unsigned int l;
+	char buf[10];
+
+	l = sprintf(buf, "%lu", (unsigned long)current->fsgid);
+
+	if (str) {
+	      memcpy(str, buf, l);
+	      str[l] = '\0';
+	}
+
+	return l;
+}
+
+static struct ocfs2_symlink_ops symlink_ops[] = {
+	{"hostname}", 9, sym_hostname},
+	{"mach}", 5, sym_machine},
+	{"os}", 3, sym_os},
+	{"nodenum}", 8, sym_nodenum},
+	{"sys}", 4, sym_system},
+	{"uid}", 4, sym_uid},
+	{"gid}", 4, sym_gid},
+	{NULL, 0, NULL}
+};
+
+
+/**
+ *** ocfs2_link_expand - Expand a context sensitive symlink
+ *** @ops: The symlink substitution operations table
+ *** @out: Buffer to place result in
+ *** @in: Buffer to get symlink from
+ ***
+ *** Returns: 0 or error code
+ ***/
+
+static void ocfs2_link_expand(struct ocfs2_symlink_ops *ops, char *out, char *in, struct inode *inode)
+{
+	unsigned int i;
+
+	while (*in) {
+		*out++ = *in;
+		if (*in++ != '{')
+			continue;
+
+		for (i = 0; ops[i].name; i++) {
+			if (memcmp(in, ops[i].name, ops[i].len) == 0) {
+				out--;
+				out += ops[i].subst_fn(out, inode);
+				in += ops[i].len;
+			}
+		}
+	}
+
+	*out = 0;
+}
+
+
+/**
+ *** ocfs2_link_size - Return expanded size required to store a symlink
+ *** @str: The symlink
+ *** @ops: The symlink substitution operations table
+ ***
+ *** Returns: The size of the expanded symlink.
+ ***/
+
+
+static unsigned int ocfs2_link_size(struct ocfs2_symlink_ops *ops, char *str, struct inode *inode)
+{
+	unsigned int len = 0;
+	unsigned int i;
+
+	while (*str) {
+		len++;
+		if (*str++ != '{')
+			continue;
+
+		for (i = 0; ops[i].name; i++) {
+			if (memcmp(str, ops[i].name, ops[i].len) == 0) {
+				len--;
+				len += ops[i].subst_fn(NULL, inode);
+				str += ops[i].len;
+				break;
+			}
+		}
+	}
+
+	return len + 1;
+}
+
+static inline int ocfs2_cdsl_follow_link(struct nameidata *nd,
+					 char *old_link,
+					 struct inode *inode)
+{
+	int status;
+	char *new_link;
+	unsigned int len;
+
+	len = ocfs2_link_size(symlink_ops, old_link, inode);
+	new_link = kmalloc(len, GFP_KERNEL);
+	if (new_link == NULL) {
+		status = -ENOMEM;
+		mlog_errno(status);
+		goto bail;
+	}
+
+	ocfs2_link_expand(symlink_ops, new_link, old_link, inode);
+
+	status = vfs_follow_link(nd, new_link);
+	if (status < 0)
+		mlog_errno(status);
+
+	kfree(new_link);
+bail:
+	return status;
+}
+#endif
+
 static void *ocfs2_follow_link(struct dentry *dentry,
 			       struct nameidata *nd)
 {
@@ -154,7 +396,11 @@ static void *ocfs2_follow_link(struct de
 		goto bail;
 	}
 
+#ifdef OCFS2_CDSL
+	status = ocfs2_cdsl_follow_link(nd, link, inode);
+#else
 	status = vfs_follow_link(nd, link);
+#endif
 	if (status)
 		mlog_errno(status);
 bail:
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/sysfile.c linux-2.6.15-ocfs2/fs/ocfs2/sysfile.c
--- linux-2.6.15-2/fs/ocfs2/sysfile.c	2006-02-06 18:15:15.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/sysfile.c	2006-02-06 17:59:23.000000000 -0500
@@ -77,7 +77,8 @@ struct inode *ocfs2_get_system_file_inod
 	if (arr && ((inode = *arr) != NULL)) {
 		/* get a ref in addition to the array ref */
 		inode = igrab(inode);
-		BUG_ON(!inode);
+		if (!inode)
+			BUG();
 
 		return inode;
 	}
@@ -88,7 +89,8 @@ struct inode *ocfs2_get_system_file_inod
 	/* add one more if putting into array for first time */
 	if (arr && inode) {
 		*arr = igrab(inode);
-		BUG_ON(!*arr);
+		if (!*arr)
+			BUG();
 	}
 	return inode;
 }
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/uptodate.c linux-2.6.15-ocfs2/fs/ocfs2/uptodate.c
--- linux-2.6.15-2/fs/ocfs2/uptodate.c	2006-02-06 18:15:15.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/uptodate.c	2006-02-06 17:38:13.000000000 -0500
@@ -388,7 +388,7 @@ out_free:
 	}
 }
 
-/* Item insertion is guarded by ip_io_mutex, so the insertion path takes
+/* Item insertion is guarded by ip_io_sem, so the insertion path takes
  * advantage of this by not rechecking for a duplicate insert during
  * the slow case. Additionally, if the cache needs to be bumped up to
  * a tree, the code will not recheck after acquiring the lock --
@@ -418,7 +418,7 @@ void ocfs2_set_buffer_uptodate(struct in
 	     (unsigned long long) bh->b_blocknr);
 
 	/* No need to recheck under spinlock - insertion is guarded by
-	 * ip_io_mutex */
+	 * ip_io_sem */
 	spin_lock(&oi->ip_lock);
 	if (ocfs2_insert_can_use_array(oi, ci)) {
 		/* Fast case - it's an array and there's a free
@@ -440,7 +440,7 @@ void ocfs2_set_buffer_uptodate(struct in
 
 /* Called against a newly allocated buffer. Most likely nobody should
  * be able to read this sort of metadata while it's still being
- * allocated, but this is careful to take ip_io_mutex anyway. */
+ * allocated, but this is careful to take ip_io_sem anyway. */
 void ocfs2_set_new_buffer_uptodate(struct inode *inode,
 				   struct buffer_head *bh)
 {
@@ -451,9 +451,9 @@ void ocfs2_set_new_buffer_uptodate(struc
 
 	set_buffer_uptodate(bh);
 
-	mutex_lock(&oi->ip_io_mutex);
+	down(&oi->ip_io_sem);
 	ocfs2_set_buffer_uptodate(inode, bh);
-	mutex_unlock(&oi->ip_io_mutex);
+	up(&oi->ip_io_sem);
 }
 
 /* Requires ip_lock. */
@@ -537,7 +537,7 @@ int __init init_ocfs2_uptodate_cache(voi
 	return 0;
 }
 
-void exit_ocfs2_uptodate_cache(void)
+void __exit exit_ocfs2_uptodate_cache(void)
 {
 	if (ocfs2_uptodate_cachep)
 		kmem_cache_destroy(ocfs2_uptodate_cachep);
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/uptodate.h linux-2.6.15-ocfs2/fs/ocfs2/uptodate.h
--- linux-2.6.15-2/fs/ocfs2/uptodate.h	2006-02-06 18:15:15.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/uptodate.h	2006-02-06 17:38:13.000000000 -0500
@@ -27,7 +27,7 @@
 #define OCFS2_UPTODATE_H
 
 int __init init_ocfs2_uptodate_cache(void);
-void exit_ocfs2_uptodate_cache(void);
+void __exit exit_ocfs2_uptodate_cache(void);
 
 void ocfs2_metadata_cache_init(struct inode *inode);
 void ocfs2_metadata_cache_purge(struct inode *inode);
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/ver.c linux-2.6.15-ocfs2/fs/ocfs2/ver.c
--- linux-2.6.15-2/fs/ocfs2/ver.c	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/ver.c	2006-02-06 17:59:23.000000000 -0500
@@ -29,15 +29,25 @@
 
 #include "ver.h"
 
-#define OCFS2_BUILD_VERSION "1.3.3"
+#define OCFS2_BUILD_VERSION	"1.2.0-SLES"
+#define OCFS2_BUILD_DATE	"Tue Jan 24 14:31:42 PST 2006"
+#define OCFS2_BUILD_MD5	"sles"
 
-#define VERSION_STR "OCFS2 " OCFS2_BUILD_VERSION
+#define VERSION_STR "OCFS2 " \
+	OCFS2_BUILD_VERSION " " OCFS2_BUILD_DATE " (build " OCFS2_BUILD_MD5 ")"
 
 void ocfs2_print_version(void)
 {
 	printk(KERN_INFO "%s\n", VERSION_STR);
 }
 
+int ocfs2_str_version(char *buf)
+{
+	return sprintf(buf, "%s\n", VERSION_STR);
+}
+
 MODULE_DESCRIPTION(VERSION_STR);
 
+#ifdef MODULE_VERSION
 MODULE_VERSION(OCFS2_BUILD_VERSION);
+#endif
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/ver.h linux-2.6.15-ocfs2/fs/ocfs2/ver.h
--- linux-2.6.15-2/fs/ocfs2/ver.h	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/ver.h	2006-02-06 17:38:14.000000000 -0500
@@ -27,5 +27,6 @@
 #define OCFS2_VER_H
 
 void ocfs2_print_version(void);
+int ocfs2_str_version(char *buf);
 
 #endif /* OCFS2_VER_H */
diff -ruNpX dontdiff linux-2.6.15-2/fs/ocfs2/vote.c linux-2.6.15-ocfs2/fs/ocfs2/vote.c
--- linux-2.6.15-2/fs/ocfs2/vote.c	2006-02-06 18:15:14.000000000 -0500
+++ linux-2.6.15-ocfs2/fs/ocfs2/vote.c	2006-02-06 17:59:23.000000000 -0500
@@ -344,6 +344,21 @@ static void ocfs2_process_dentry_request
 		if (new_nlink == 0) {
 			spin_lock(&oi->ip_lock);
 			oi->ip_flags |= OCFS2_INODE_MAYBE_ORPHANED;
+
+#ifdef OCFS2_DELETE_INODE_WORKAROUND
+			/* Do a sync now as we can't be sure whether
+			 * the inode will actually be orphaned or
+			 * not. We condition this on the open count as
+			 * otherwise, ocfs2_file_release will handle
+			 * it for us. */
+			if (!oi->ip_open_count) {
+				spin_unlock(&oi->ip_lock);
+				write_inode_now(inode, 1);
+				/* strange indentation past the
+				 * 'else', but I want to keep the non
+				 * hack code purty :) */
+			} else
+#endif
 			spin_unlock(&OCFS2_I(inode)->ip_lock);
 		}
 	}
