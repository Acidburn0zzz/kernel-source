From: Anil Keshavamurphy at Intel
Subject: [ia64] make kprobes use a separate notifier chain for DIE_PAGE_FAULT
Patch-mainline: not yet
References: 167612

Making notify_die traverse 4 notifiers will slow down the page fault
handler on ia64 by 10-20% in extreme cases. The original patch from
Anil Keshavamurphy addresses this for all architectures, but at this
stage we are going to use an ia64-only version of that patch.

Acked-by: okir@suse.de

 arch/ia64/mm/fault.c       |   49 ++++++++++++++++++++++++++++++++++++++++++++-
 include/asm-ia64/kdebug.h  |    3 ++
 include/asm-ia64/kprobes.h |    1 
 kernel/kprobes.c           |   30 +++++++++++++++++++++++++++
 4 files changed, 82 insertions(+), 1 deletion(-)

--- linux-2.6.17.orig/arch/ia64/mm/fault.c
+++ linux-2.6.17/arch/ia64/mm/fault.c
@@ -19,6 +19,53 @@
 
 extern void die (char *, struct pt_regs *, long);
 
+#ifdef CONFIG_KPROBES
+struct notifier_block *notify_page_fault_chain;
+static DEFINE_SPINLOCK(page_fault_notifier_lock);
+
+/* Hook to register for page fault notifications */
+int register_page_fault_notifier(struct notifier_block *nb)
+{
+	unsigned long flags;
+	int err = 0;
+
+	spin_lock_irqsave(&page_fault_notifier_lock, flags);
+	err = notifier_chain_register(&notify_page_fault_chain, nb);
+	spin_unlock_irqrestore(&page_fault_notifier_lock, flags);
+	return err;
+}
+
+int unregister_page_fault_notifier(struct notifier_block *nb)
+{
+	unsigned long flags;
+	int err = 0;
+
+	spin_lock_irqsave(&page_fault_notifier_lock, flags);
+	err = notifier_chain_unregister(&notify_page_fault_chain, nb);
+	spin_unlock_irqrestore(&page_fault_notifier_lock, flags);
+	return err;
+}
+
+static inline int notify_page_fault(enum die_val val, const char *str,
+			struct pt_regs *regs, long err, int trap, int sig)
+{
+	struct die_args args = {
+		.regs = regs,
+		.str = str,
+		.err = err,
+		.trapnr = trap,
+		.signr = sig
+	};
+	return notifier_call_chain(&notify_page_fault_chain, val, &args);
+}
+#else
+static inline int notify_page_fault(enum die_val val, const char *str,
+			struct pt_regs *regs, long err, int trap, int sig)
+{
+	return NOTIFY_DONE;
+}
+#endif
+
 /*
  * Return TRUE if ADDRESS points at a page in the kernel's mapped segment
  * (inside region 5, on ia64) and that page is present.
@@ -84,7 +131,7 @@ ia64_do_page_fault (unsigned long addres
 	/*
 	 * This is to handle the kprobes on user space access instructions
 	 */
-	if (notify_die(DIE_PAGE_FAULT, "page fault", regs, code, TRAP_BRKPT,
+	if (notify_page_fault(DIE_PAGE_FAULT, "page fault", regs, code, TRAP_BRKPT,
 					SIGSEGV) == NOTIFY_STOP)
 		return;
 
--- linux-2.6.17.orig/include/asm-ia64/kdebug.h
+++ linux-2.6.17/include/asm-ia64/kdebug.h
@@ -38,8 +38,11 @@ struct die_args {
 	int signr;
 };
 
+#define ARCH_HAVE_PAGE_FAULT_NOTIFIER
 extern int register_die_notifier(struct notifier_block *);
 extern int unregister_die_notifier(struct notifier_block *);
+extern int register_page_fault_notifier(struct notifier_block *);
+extern int unregister_page_fault_notifier(struct notifier_block *);
 extern struct atomic_notifier_head ia64die_chain;
 
 enum die_val {
--- linux-2.6.17.orig/include/asm-ia64/kprobes.h
+++ linux-2.6.17/include/asm-ia64/kprobes.h
@@ -82,6 +82,7 @@ struct kprobe_ctlblk {
 #define JPROBE_ENTRY(pentry)	(kprobe_opcode_t *)pentry
 
 #define ARCH_SUPPORTS_KRETPROBES
+#define ARCH_INACTIVE_KPROBE_COUNT 1
 
 #define SLOT0_OPCODE_SHIFT	(37)
 #define SLOT1_p1_OPCODE_SHIFT	(37 - (64-46))
--- linux-2.6.17.orig/kernel/kprobes.c
+++ linux-2.6.17/kernel/kprobes.c
@@ -52,6 +52,14 @@ DEFINE_MUTEX(kprobe_mutex);		/* Protects
 DEFINE_SPINLOCK(kretprobe_lock);	/* Protects kretprobe_inst_table */
 static DEFINE_PER_CPU(struct kprobe *, kprobe_instance) = NULL;
 
+#ifdef ARCH_HAVE_PAGE_FAULT_NOTIFIER
+static atomic_t kprobe_count = ATOMIC_INIT(0);
+static struct notifier_block kprobe_page_fault_nb = {
+	.notifier_call = kprobe_exceptions_notify,
+	.priority = 0x7fffffff /* we need to notified first */
+};
+#endif
+
 #ifdef __ARCH_WANT_KPROBES_INSN_SLOT
 /*
  * kprobe->ainsn.insn points to the copy of the instruction to be
@@ -464,6 +472,10 @@ static int __kprobes __register_kprobe(s
 	old_p = get_kprobe(p->addr);
 	if (old_p) {
 		ret = register_aggr_kprobe(old_p, p);
+#ifdef ARCH_HAVE_PAGE_FAULT_NOTIFIER
+		if (!ret)
+			atomic_inc(&kprobe_count);
+#endif
 		goto out;
 	}
 
@@ -474,6 +486,12 @@ static int __kprobes __register_kprobe(s
 	hlist_add_head_rcu(&p->hlist,
 		       &kprobe_table[hash_ptr(p->addr, KPROBE_HASH_BITS)]);
 
+#ifdef ARCH_HAVE_PAGE_FAULT_NOTIFIER
+	if (atomic_add_return(1, &kprobe_count) == \
+				(ARCH_INACTIVE_KPROBE_COUNT + 1))
+		register_page_fault_notifier(&kprobe_page_fault_nb);
+#endif
+
   	arch_arm_kprobe(p);
 
 out:
@@ -537,6 +555,18 @@ valid_p:
 		}
 		arch_remove_kprobe(p);
 	}
+
+#ifdef ARCH_HAVE_PAGE_FAULT_NOTIFIER
+	/* Call unregister_page_fault_notifier()
+	 * if no probes are active
+	 */
+	down(&kprobe_mutex);
+	if (atomic_add_return(-1, &kprobe_count) == \
+				ARCH_INACTIVE_KPROBE_COUNT)
+		unregister_page_fault_notifier(&kprobe_page_fault_nb);
+	up(&kprobe_mutex);
+#endif
+	return;
 }
 
 static struct notifier_block kprobe_exceptions_nb = {
