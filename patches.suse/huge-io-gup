diff -u linux/include/linux/hugetlb.h-o linux/include/linux/hugetlb.h
--- linux/include/linux/hugetlb.h-o	2004-06-10 10:18:31.000000000 +0200
+++ linux/include/linux/hugetlb.h	2004-06-10 10:58:06.000000000 +0200
@@ -14,7 +14,7 @@
 
 int hugetlb_sysctl_handler(struct ctl_table *, int, struct file *, void *, size_t *);
 int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *, struct vm_area_struct *);
-int follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *, struct page **, struct vm_area_struct **, unsigned long *, int *, int);
+int follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *, struct page **, struct vm_area_struct **, unsigned long *, int *, int, int);
 void zap_hugepage_range(struct vm_area_struct *, unsigned long, unsigned long);
 void unmap_hugepage_range(struct vm_area_struct *, unsigned long, unsigned long);
 int hugetlb_prefault(struct address_space *, struct vm_area_struct *);
diff -u linux/arch/i386/mm/hugetlbpage.c-o linux/arch/i386/mm/hugetlbpage.c
--- linux/arch/i386/mm/hugetlbpage.c-o	2004-06-10 10:18:36.000000000 +0200
+++ linux/arch/i386/mm/hugetlbpage.c	2004-06-10 10:57:43.000000000 +0200
@@ -30,6 +30,9 @@
 static struct list_head hugepage_freelists[MAX_NUMNODES];
 static spinlock_t htlbpage_lock = SPIN_LOCK_UNLOCKED;
 
+static int hugetlb_alloc_fault(struct mm_struct *mm, struct vm_area_struct *vma, 
+			       unsigned long addr, int flush);
+
 static void enqueue_huge_page(struct page *page)
 {
 	list_add(&page->lru,
@@ -164,13 +167,12 @@
 int
 follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
 		    struct page **pages, struct vm_area_struct **vmas,
-		    unsigned long *position, int *length, int i)
+		    unsigned long *position, int *length, int i, int write)
 {
 	unsigned long vpfn, vaddr = *position;
 	int remainder = *length;
 
-	WARN_ON(!is_vm_hugetlb_page(vma));
-
+	spin_lock(&mm->page_table_lock);
 	vpfn = vaddr/PAGE_SIZE;
 	while (vaddr < vma->vm_end && remainder) {
 
@@ -178,10 +180,22 @@
 			pte_t *pte;
 			struct page *page;
 
-			pte = huge_pte_offset(mm, vaddr);
-
-			if(!pte || pte_none(*pte))
-				return -EFAULT;
+			for (;;) { 
+				pte = huge_pte_offset(mm, vaddr);
+				if (pte && !pte_none(*pte))
+					break; 
+				if (!write) { 
+					spin_unlock(&mm->page_table_lock);
+					return -EFAULT;
+				}
+				switch (hugetlb_alloc_fault(mm, vma, vaddr, 0)) { 
+				case VM_FAULT_SIGBUS:
+					return -EFAULT;
+				case VM_FAULT_OOM:
+					return -ENOMEM; /* or better kill? */
+				} 
+				spin_lock(&mm->page_table_lock);
+			}
 
 			page = &pte_page(*pte)[vpfn % (HPAGE_SIZE/PAGE_SIZE)];
 
@@ -200,6 +214,8 @@
 		++i;
 	}
 
+	spin_unlock(&mm->page_table_lock);
+
 	*length = remainder;
 	*position = vaddr;
 
@@ -337,7 +353,7 @@
 /* page_table_lock hold on entry. */
 static int 
 hugetlb_alloc_fault(struct mm_struct *mm, struct vm_area_struct *vma, 
-			       unsigned long addr, int write_access)
+			       unsigned long addr, int flush)
 {
 		unsigned long idx;
 	int ret;
@@ -378,6 +394,7 @@
 			/* Instead of OOMing here could just transparently use
 			   small pages. */
 
+			if (flush) 
 			printk(KERN_INFO "%s[%d] ran out of huge pages. Killed.\n",
 			       current->comm, current->pid);
 			
@@ -404,7 +421,8 @@
  flush:
 	/* Don't need to flush other CPUs. They will just do a page
 	   fault and flush it lazily. */
-	__flush_tlb_one(addr);
+	if (flush) 
+		__flush_tlb_one(addr);
 	
  out:
 	spin_unlock(&mm->page_table_lock);
@@ -423,11 +441,11 @@
 	spin_lock(&mm->page_table_lock);	
 	pgd = pgd_offset(mm, address); 
 	if (pgd_none(*pgd)) 
-		return hugetlb_alloc_fault(mm, vma, address, write_access); 
+		return hugetlb_alloc_fault(mm, vma, address, 1); 
 
 	pmd = pmd_offset(pgd, address);
 	if (pmd_none(*pmd))
-		return hugetlb_alloc_fault(mm, vma, address, write_access); 
+		return hugetlb_alloc_fault(mm, vma, address, 1); 
 
 	BUG_ON(!pmd_large(*pmd)); 
 
diff -u linux/arch/ia64/mm/hugetlbpage.c-o linux/arch/ia64/mm/hugetlbpage.c
--- linux/arch/ia64/mm/hugetlbpage.c-o	2004-06-10 10:18:36.000000000 +0200
+++ linux/arch/ia64/mm/hugetlbpage.c	2004-06-10 11:07:10.000000000 +0200
@@ -184,25 +184,38 @@
 int
 follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
 		    struct page **pages, struct vm_area_struct **vmas,
-		    unsigned long *st, int *length, int i)
+		    unsigned long *st, int *length, int i, int write)
 {
-	pte_t *ptep, pte;
+	pte_t *pte;
 	unsigned long start = *st;
 	unsigned long pstart;
 	int len = *length;
 	struct page *page;
 
+	spin_lock(&mm->page_table_lock);
 	do {
 		pstart = start & HPAGE_MASK;
-		ptep = huge_pte_offset(mm, start);
-		if (!ptep) 
-			return -EFAULT;
-		pte = *ptep;
-		if (pte_none(pte))
-			return -EFAULT;
+
+		for (;;) { 
+			pte = huge_pte_offset(mm, start);
+			if (pte && !pte_none(*pte))
+				break; 
+			if (!write) { 
+				spin_unlock(&mm->page_table_lock);
+				return -EFAULT;
+			}
+			spin_unlock(&mm->page_table_lock); 
+			switch (arch_hugetlb_fault(mm, vma, start, write)) { 
+			case VM_FAULT_SIGBUS:
+				return -EFAULT;
+			case VM_FAULT_OOM:
+				return -ENOMEM; /* or better kill? */
+			} 
+			spin_lock(&mm->page_table_lock);
+		}
 
 back1:
-		page = pte_page(pte);
+		page = pte_page(*pte);
 		if (pages) {
 			page += ((start & ~HPAGE_MASK) >> PAGE_SHIFT);
 			get_page(page);
@@ -219,6 +232,7 @@
 	} while (len && start < vma->vm_end);
 	*length = len;
 	*st = start;
+	spin_unlock(&mm->page_table_lock);
 	return i;
 }
 
@@ -623,6 +637,9 @@
 	struct address_space *mapping;
 	int idx, ret = VM_FAULT_MINOR;
 
+	if (write_access && !(vma->vm_flags & VM_WRITE))
+		return VM_FAULT_SIGBUS;
+
 	spin_lock(&mm->page_table_lock);
 	pte = huge_pte_alloc(mm, addr & HPAGE_MASK);
 	if (!pte) {
diff -u linux/arch/ppc64/mm/hugetlbpage.c-o linux/arch/ppc64/mm/hugetlbpage.c
--- linux/arch/ppc64/mm/hugetlbpage.c-o	2004-06-10 10:18:36.000000000 +0200
+++ linux/arch/ppc64/mm/hugetlbpage.c	2004-06-10 10:58:04.000000000 +0200
@@ -381,7 +381,7 @@
 int
 follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
 		    struct page **pages, struct vm_area_struct **vmas,
-		    unsigned long *position, int *length, int i)
+		    unsigned long *position, int *length, int i, int write)
 {
 	unsigned long vpfn, vaddr = *position;
 	int remainder = *length;
diff -u linux/mm/memory.c-o linux/mm/memory.c
--- linux/mm/memory.c-o	2004-06-10 10:18:40.000000000 +0200
+++ linux/mm/memory.c	2004-06-10 10:57:57.000000000 +0200
@@ -750,7 +750,7 @@
 
 		if (is_vm_hugetlb_page(vma)) {
 			i = follow_hugetlb_page(mm, vma, pages, vmas,
-						&start, &len, i);
+						&start, &len, i, !!(flags & VM_WRITE));
 			if (i < 0) 
 				return i;
 			continue;
