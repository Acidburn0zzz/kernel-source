Index: linux-2.6.5/include/linux/hugetlb.h
===================================================================
--- linux-2.6.5.orig/include/linux/hugetlb.h
+++ linux-2.6.5/include/linux/hugetlb.h
@@ -14,7 +14,7 @@ static inline int is_vm_hugetlb_page(str
 
 int hugetlb_sysctl_handler(struct ctl_table *, int, struct file *, void *, size_t *);
 int copy_hugetlb_page_range(struct mm_struct *, struct mm_struct *, struct vm_area_struct *);
-int follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *, struct page **, struct vm_area_struct **, unsigned long *, int *, int);
+int follow_hugetlb_page(struct mm_struct *, struct vm_area_struct *, struct page **, struct vm_area_struct **, unsigned long *, int *, int, int);
 void zap_hugepage_range(struct vm_area_struct *, unsigned long, unsigned long);
 void unmap_hugepage_range(struct vm_area_struct *, unsigned long, unsigned long);
 int hugetlb_prefault(struct address_space *, struct vm_area_struct *);
@@ -68,7 +68,7 @@ static inline unsigned long hugetlb_tota
 	return 0;
 }
 
-#define follow_hugetlb_page(m,v,p,vs,a,b,i)	({ BUG(); 0; })
+#define follow_hugetlb_page(m,v,p,vs,a,b,i,w)	({ BUG(); 0; })
 #define follow_huge_addr(mm, vma, addr, write)	0
 #define copy_hugetlb_page_range(src, dst, vma)	({ BUG(); 0; })
 #define hugetlb_prefault(mapping, vma)		({ BUG(); 0; })
Index: linux-2.6.5/arch/i386/mm/hugetlbpage.c
===================================================================
--- linux-2.6.5.orig/arch/i386/mm/hugetlbpage.c
+++ linux-2.6.5/arch/i386/mm/hugetlbpage.c
@@ -30,6 +30,9 @@ static long    htlbzone_pages[MAX_NUMNOD
 static struct list_head hugepage_freelists[MAX_NUMNODES];
 static spinlock_t htlbpage_lock = SPIN_LOCK_UNLOCKED;
 
+static int hugetlb_alloc_fault(struct mm_struct *mm, struct vm_area_struct *vma, 
+			       unsigned long addr, int flush);
+
 static void enqueue_huge_page(struct page *page)
 {
 	list_add(&page->lru,
@@ -164,13 +167,12 @@ nomem:
 int
 follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
 		    struct page **pages, struct vm_area_struct **vmas,
-		    unsigned long *position, int *length, int i)
+		    unsigned long *position, int *length, int i, int write)
 {
 	unsigned long vpfn, vaddr = *position;
 	int remainder = *length;
 
-	WARN_ON(!is_vm_hugetlb_page(vma));
-
+	spin_lock(&mm->page_table_lock);
 	vpfn = vaddr/PAGE_SIZE;
 	while (vaddr < vma->vm_end && remainder) {
 
@@ -178,10 +180,22 @@ follow_hugetlb_page(struct mm_struct *mm
 			pte_t *pte;
 			struct page *page;
 
-			pte = huge_pte_offset(mm, vaddr);
-
-			if(!pte || pte_none(*pte))
-				return -EFAULT;
+			for (;;) { 
+				pte = huge_pte_offset(mm, vaddr);
+				if (pte && !pte_none(*pte))
+					break; 
+				if (!write) { 
+					spin_unlock(&mm->page_table_lock);
+					return -EFAULT;
+				}
+				switch (hugetlb_alloc_fault(mm, vma, vaddr, 0)) { 
+				case VM_FAULT_SIGBUS:
+					return -EFAULT;
+				case VM_FAULT_OOM:
+					return -ENOMEM; /* or better kill? */
+				} 
+				spin_lock(&mm->page_table_lock);
+			}
 
 			page = &pte_page(*pte)[vpfn % (HPAGE_SIZE/PAGE_SIZE)];
 
@@ -200,6 +214,8 @@ follow_hugetlb_page(struct mm_struct *mm
 		++i;
 	}
 
+	spin_unlock(&mm->page_table_lock);
+
 	*length = remainder;
 	*position = vaddr;
 
@@ -338,7 +354,7 @@ zap_hugepage_range(struct vm_area_struct
 /* page_table_lock hold on entry. */
 static int 
 hugetlb_alloc_fault(struct mm_struct *mm, struct vm_area_struct *vma, 
-			       unsigned long addr, int write_access)
+			       unsigned long addr, int flush)
 {
 		unsigned long idx;
 	int ret;
@@ -379,6 +395,7 @@ hugetlb_alloc_fault(struct mm_struct *mm
 			/* Instead of OOMing here could just transparently use
 			   small pages. */
 
+			if (flush) 
 			printk(KERN_INFO "%s[%d] ran out of huge pages. Killed.\n",
 			       current->comm, current->pid);
 			
@@ -405,7 +422,8 @@ hugetlb_alloc_fault(struct mm_struct *mm
  flush:
 	/* Don't need to flush other CPUs. They will just do a page
 	   fault and flush it lazily. */
-	__flush_tlb_one(addr);
+	if (flush) 
+		__flush_tlb_one(addr);
 	
  out:
 	spin_unlock(&mm->page_table_lock);
@@ -424,11 +442,11 @@ int arch_hugetlb_fault(struct mm_struct 
 	spin_lock(&mm->page_table_lock);	
 	pgd = pgd_offset(mm, address); 
 	if (pgd_none(*pgd)) 
-		return hugetlb_alloc_fault(mm, vma, address, write_access); 
+		return hugetlb_alloc_fault(mm, vma, address, 1); 
 
 	pmd = pmd_offset(pgd, address);
 	if (pmd_none(*pmd))
-		return hugetlb_alloc_fault(mm, vma, address, write_access); 
+		return hugetlb_alloc_fault(mm, vma, address, 1); 
 
 	BUG_ON(!pmd_large(*pmd)); 
 
Index: linux-2.6.5/arch/ia64/mm/hugetlbpage.c
===================================================================
--- linux-2.6.5.orig/arch/ia64/mm/hugetlbpage.c
+++ linux-2.6.5/arch/ia64/mm/hugetlbpage.c
@@ -184,25 +184,38 @@ nomem:
 int
 follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
 		    struct page **pages, struct vm_area_struct **vmas,
-		    unsigned long *st, int *length, int i)
+		    unsigned long *st, int *length, int i, int write)
 {
-	pte_t *ptep, pte;
+	pte_t *pte;
 	unsigned long start = *st;
 	unsigned long pstart;
 	int len = *length;
 	struct page *page;
 
+	spin_lock(&mm->page_table_lock);
 	do {
 		pstart = start & HPAGE_MASK;
-		ptep = huge_pte_offset(mm, start);
-		if (!ptep) 
-			return -EFAULT;
-		pte = *ptep;
-		if (pte_none(pte))
-			return -EFAULT;
+
+		for (;;) { 
+			pte = huge_pte_offset(mm, start);
+			if (pte && !pte_none(*pte))
+				break; 
+			if (!write) { 
+				spin_unlock(&mm->page_table_lock);
+				return -EFAULT;
+			}
+			spin_unlock(&mm->page_table_lock); 
+			switch (arch_hugetlb_fault(mm, vma, start, write)) { 
+			case VM_FAULT_SIGBUS:
+				return -EFAULT;
+			case VM_FAULT_OOM:
+				return -ENOMEM; /* or better kill? */
+			} 
+			spin_lock(&mm->page_table_lock);
+		}
 
 back1:
-		page = pte_page(pte);
+		page = pte_page(*pte);
 		if (pages) {
 			page += ((start & ~HPAGE_MASK) >> PAGE_SHIFT);
 			get_page(page);
@@ -219,6 +232,7 @@ back1:
 	} while (len && start < vma->vm_end);
 	*length = len;
 	*st = start;
+	spin_unlock(&mm->page_table_lock);
 	return i;
 }
 
@@ -623,6 +637,9 @@ int arch_hugetlb_fault(struct mm_struct 
 	struct address_space *mapping;
 	int idx, ret = VM_FAULT_MINOR;
 
+	if (write_access && !(vma->vm_flags & VM_WRITE))
+		return VM_FAULT_SIGBUS;
+
 	spin_lock(&mm->page_table_lock);
 	pte = huge_pte_alloc(mm, addr & HPAGE_MASK);
 	if (!pte) {
Index: linux-2.6.5/arch/ppc64/mm/hugetlbpage.c
===================================================================
--- linux-2.6.5.orig/arch/ppc64/mm/hugetlbpage.c
+++ linux-2.6.5/arch/ppc64/mm/hugetlbpage.c
@@ -370,7 +370,7 @@ int copy_hugetlb_page_range(struct mm_st
 int
 follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
 		    struct page **pages, struct vm_area_struct **vmas,
-		    unsigned long *position, int *length, int i)
+		    unsigned long *position, int *length, int i, int write)
 {
 	unsigned long vpfn, vaddr = *position;
 	int remainder = *length;
Index: linux-2.6.5/mm/memory.c
===================================================================
--- linux-2.6.5.orig/mm/memory.c
+++ linux-2.6.5/mm/memory.c
@@ -756,7 +756,7 @@ int get_user_pages(struct task_struct *t
 
 		if (is_vm_hugetlb_page(vma)) {
 			i = follow_hugetlb_page(mm, vma, pages, vmas,
-						&start, &len, i);
+						&start, &len, i, !!(flags & VM_WRITE));
 			if (i < 0) 
 				return i;
 			continue;
