From: Nick Piggin <npiggin@suse.de>
Subject: Prevent the kernel from going OOM too early
References: 153803

There are a number of things that need to happen before we have reasonably
scanned all memory: reclaim_mapped threshold needs to be passed, then
referenced active list pages need to have their referenced bit cleared, then
they need to be moved to the inactive list, then they need to be reclaimed from
the inactive list. So it is not just a matter of scanning a set number of
pages.

Index: linux-2.6/mm/vmscan.c
===================================================================
--- linux-2.6.orig/mm/vmscan.c
+++ linux-2.6/mm/vmscan.c
@@ -1171,6 +1171,11 @@ done:
 	pagevec_release(&pvec);
 }
 
+static inline int zone_is_near_oom(struct zone *zone)
+{
+	return zone->pages_scanned >= (zone->nr_active + zone->nr_inactive)*4;
+}
+
 /*
  * This moves pages from the active list to the inactive list.
  *
@@ -1202,11 +1207,14 @@ refill_inactive_zone(struct zone *zone, 
 	struct pagevec pvec;
 	int reclaim_mapped = 0;
 
-	if (unlikely(sc->may_swap)) {
+	if (likely(sc->may_swap)) {
 		long mapped_ratio;
 		long distress;
 		long swap_tendency;
 
+		if (zone_is_near_oom(zone))
+			goto force_reclaim_mapped;
+
 		/*
 		 * `distress' is a measure of how much trouble we're having
 		 * reclaiming pages.  0 -> no problems.  100 -> great trouble.
@@ -1240,6 +1248,7 @@ refill_inactive_zone(struct zone *zone, 
 		 * memory onto the inactive list.
 		 */
 		if (swap_tendency >= 100)
+force_reclaim_mapped:
 			reclaim_mapped = 1;
 	}
 
@@ -1646,7 +1655,7 @@ scan:
 			if (zone->all_unreclaimable)
 				continue;
 			if (nr_slab == 0 && zone->pages_scanned >=
-				    (zone->nr_active + zone->nr_inactive) * 4)
+				    (zone->nr_active + zone->nr_inactive) * 6)
 				zone->all_unreclaimable = 1;
 			/*
 			 * If we've done a decent amount of scanning and
