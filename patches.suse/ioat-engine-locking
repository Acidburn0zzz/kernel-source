From: Intel
Subject: IOAT: Locking for dma_client->channels list
References: 142510

This was previously accessed without locking, and a race between pushing
pending work from net/core/dev.c and driver unloading could cause a
panic.

Acked-by: okir@suse.de

diff -u
--- linux-2.6.15-20060109195850.ioat.40/drivers/dma/dmaengine.c	2006-01-11 06:48:04.000000000 -0800
+++ linux-2.6.15-20060109195850.ioat.42/drivers/dma/dmaengine.c	2006-01-17 07:00:58.000000000 -0800
@@ -90,6 +90,7 @@ static struct dma_chan * dma_client_chan
 {
 	struct dma_device *device;
 	struct dma_chan *chan;
+	unsigned long flags;
 
 	/* Find a channel, any DMA engine will do */
 	list_for_each_entry(device, &dma_device_list, global_node) {
@@ -102,7 +103,9 @@ static struct dma_chan * dma_client_chan
 				kref_init(&chan->refcount);
 				INIT_RCU_HEAD(&chan->rcu);
 				chan->client = client;
+				spin_lock_irqsave(&client->lock, flags);
 				list_add_tail(&chan->client_node, &client->channels);
+				spin_unlock_irqrestore(&client->lock, flags);
 				return chan;
 			}
 		}
@@ -144,6 +147,7 @@ static void dma_chans_rebalance(void)
 {
 	struct dma_client *client;
 	struct dma_chan *chan;
+	unsigned long flags;
 
 	spin_lock(&dma_list_lock);
 	list_for_each_entry(client, &dma_client_list, global_node) {
@@ -158,8 +162,10 @@ static void dma_chans_rebalance(void)
 		}
 
 		while (client->chans_desired < client->chan_count) {
+			spin_lock_irqsave(&client->lock, flags);
 			chan = list_entry(client->channels.next, struct dma_chan, client_node);
 			list_del(&chan->client_node);
+			spin_unlock_irqrestore(&client->lock, flags);
 			client->chan_count--;
 			client->event_callback(client, chan, DMA_RESOURCE_REMOVED);
 			dma_client_chan_free(chan);
@@ -181,6 +187,7 @@ struct dma_client * dma_async_client_reg
 		return NULL;
 
 	INIT_LIST_HEAD(&client->channels);
+	spin_lock_init(&client->lock);
 
 	client->chans_desired = 0;
 	client->chan_count = 0;
@@ -202,13 +209,16 @@ struct dma_client * dma_async_client_reg
 void dma_async_client_unregister(struct dma_client *client)
 {
 	struct dma_chan *chan, *_chan;
+	unsigned long flags;
 
 	if (!client)
 		return;
 
+	spin_lock_irqsave(&client->lock, flags);
 	list_for_each_entry_safe(chan, _chan, &client->channels, client_node) {
 		dma_client_chan_free(chan);
 	}
+	spin_unlock_irqrestore(&client->lock, flags);
 
 	spin_lock(&dma_list_lock);
 	list_del(&client->global_node);
@@ -285,21 +295,24 @@ static void dma_async_device_cleanup(str
 void dma_async_device_unregister(struct dma_device* device)
 {
 	struct dma_chan *chan;
+	unsigned long flags;
+
+	spin_lock(&dma_list_lock);
+	list_del(&device->global_node);
+	spin_unlock(&dma_list_lock);
 
 	list_for_each_entry(chan, &device->channels, device_node) {
 		if (chan->client) {
+			spin_lock_irqsave(&chan->client->lock, flags);
 			list_del(&chan->client_node);
 			chan->client->chan_count--;
+			spin_unlock_irqrestore(&chan->client->lock, flags);
 			chan->client->event_callback(chan->client, chan, DMA_RESOURCE_REMOVED);
 			dma_client_chan_free(chan);
 		}
 		class_device_unregister(&chan->class_dev);
 	}
 
-	spin_lock(&dma_list_lock);
-	list_del(&device->global_node);
-	spin_unlock(&dma_list_lock);
-
 	dma_chans_rebalance();
 
 	kref_put(&device->refcount, dma_async_device_cleanup);
--- linux-2.6.15-20060109195850.ioat.40/net/core/dev.c	2006-01-11 06:48:14.000000000 -0800
+++ linux-2.6.15-20060109195850.ioat.42/net/core/dev.c	2006-01-17 07:01:27.000000000 -0800
@@ -1816,8 +1816,10 @@ out:
 	 * There may not be any more sk_buffs comming right now, so push
 	 * any pending DMA copies to hardware
 	 */
+	spin_lock(&net_dma_client->lock);
 	list_for_each_entry(chan, &net_dma_client->channels, client_node)
 		dma_async_memcpy_issue_pending(chan);
+	spin_unlock(&net_dma_client->lock);
 #endif
 	local_irq_enable();
 	return;
@@ -3275,6 +3277,7 @@ static void net_dma_rebalance(void)
 {
 	unsigned int cpu, i, n;
 	struct dma_chan *chan;
+	unsigned long flags;
 
 	lock_cpu_hotplug();
 
@@ -3288,6 +3291,7 @@ static void net_dma_rebalance(void)
 	i = 0;
 	cpu = first_cpu(cpu_online_map);
 
+	spin_lock_irqsave(&net_dma_client->lock, flags);
 	list_for_each_entry(chan, &net_dma_client->channels, client_node) {
 		/* cpus_clear(chan->cpumask); */
 		n = ((num_online_cpus() / net_dma_count) + (i < (num_online_cpus() % net_dma_count) ? 1 : 0));
@@ -3300,6 +3304,7 @@ static void net_dma_rebalance(void)
 		}
 		i++;
 	}
+	spin_unlock_irqrestore(&net_dma_client->lock, flags);
 
 	unlock_cpu_hotplug();
 }
diff -u
--- linux-2.6.15-20060109195850.ioat.40/include/linux/dmaengine.h	2006-01-11 06:48:04.000000000 -0800
+++ linux-2.6.15-20060109195850.ioat.42/include/linux/dmaengine.h	2006-01-17 07:00:58.000000000 -0800
@@ -118,6 +118,7 @@ typedef void (*dma_event_callback) (stru
  * @event_callback: func ptr to call when something happens
  * @chan_count: number of chans allocated
  * @chans_desired: number of chans requested. Can be +/- chan_count
+ * @lock: protects access to the channels list
  * @channels: the list of DMA channels allocated
  * @global_node: list_head for global dma_client_list
  */
@@ -126,6 +127,7 @@ struct dma_client {
 	unsigned int		chan_count;
 	unsigned int		chans_desired;
 
+	spinlock_t		lock;
 	struct list_head	channels;
 	struct list_head	global_node;
 };
