
From: Nick Piggin <nickpiggin@yahoo.com.au>

Minor cleanups from Ingo's patch including task_hot (do it right in
try_to_wake_up too).


Index: linux-2.6.5/kernel/sched.c
===================================================================
--- linux-2.6.5.orig/kernel/sched.c	2004-04-16 15:06:45.000000000 +0200
+++ linux-2.6.5/kernel/sched.c	2004-04-16 15:06:45.000000000 +0200
@@ -185,6 +185,8 @@
 	return BASE_TIMESLICE(p);
 }
 
+#define task_hot(p, now, sd) ((now) - (p)->timestamp < (sd)->cache_hot_time)
+
 /*
  * These are the runqueue data structures:
  */
@@ -695,13 +697,11 @@
  */
 static int try_to_wake_up(task_t * p, unsigned int state, int sync)
 {
+	int cpu, this_cpu, success = 0;
 	unsigned long flags;
-	int success = 0;
 	long old_state;
 	runqueue_t *rq;
-	int cpu, this_cpu;
 #ifdef CONFIG_SMP
-	unsigned long long now;
 	unsigned long load, this_load;
 	struct sched_domain *sd;
 	int new_cpu;
@@ -744,8 +744,6 @@
 	if (load > this_load + SCHED_LOAD_SCALE*2)
 		goto out_set_cpu;
 
-	now = sched_clock();
-
 	/*
 	 * Migrate the task to the waking domain.
 	 * Do not violate hard affinity.
@@ -753,7 +751,7 @@
 	for_each_domain(this_cpu, sd) {
 		if (!(sd->flags & SD_WAKE_AFFINE))
 			break;
-		if (rq->timestamp_last_tick - p->timestamp < sd->cache_hot_time)
+		if (task_hot(p, rq->timestamp_last_tick, sd))
 			break;
 
 		if (cpu_isset(cpu, sd->span))
@@ -765,22 +763,18 @@
 	new_cpu = wake_idle(new_cpu, p);
 	if (new_cpu != cpu && cpu_isset(new_cpu, p->cpus_allowed)) {
 		set_task_cpu(p, new_cpu);
-		goto repeat_lock_task;
-	}
-	goto out_activate;
-
-repeat_lock_task:
-	task_rq_unlock(rq, &flags);
-	rq = task_rq_lock(p, &flags);
-	old_state = p->state;
-	if (!(old_state & state))
-		goto out;
-
-	if (p->array)
-		goto out_running;
+		task_rq_unlock(rq, &flags);
+		/* might preempt at this point */
+		rq = task_rq_lock(p, &flags);
+		old_state = p->state;
+		if (!(old_state & state))
+			goto out;
+		if (p->array)
+			goto out_running;
 
-	this_cpu = smp_processor_id();
-	cpu = task_cpu(p);
+		this_cpu = smp_processor_id();
+		cpu = task_cpu(p);
+	}
 
 out_activate:
 #endif /* CONFIG_SMP */
@@ -1291,7 +1285,7 @@
 	/* Aggressive migration if we've failed balancing */
 	if (idle == NEWLY_IDLE ||
 			sd->nr_balance_failed < sd->cache_nice_tries) {
-		if (rq->timestamp_last_tick - p->timestamp < sd->cache_hot_time)
+		if (task_hot(p, rq->timestamp_last_tick, sd))
 			return 0;
 	}
 
@@ -1309,10 +1303,9 @@
 			unsigned long max_nr_move, struct sched_domain *sd,
 			enum idle_type idle)
 {
-	int idx;
-	int pulled = 0;
 	prio_array_t *array, *dst_array;
 	struct list_head *head, *curr;
+	int idx, pulled = 0;
 	task_t *tmp;
 
 	if (max_nr_move <= 0 || busiest->nr_running <= 1)
@@ -1401,10 +1394,8 @@
 		/* Tally up the load of all CPUs in the group */
 		avg_load = 0;
 		cpus_and(tmp, group->cpumask, cpu_online_map);
-		if (unlikely(cpus_empty(tmp))) {
-			WARN_ON(1);
-			return NULL;
-		}
+		if (unlikely(cpus_empty(tmp)))
+			goto nextgroup;
 
 		for_each_cpu_mask(i, tmp) {
 			/* Bias balancing toward cpus of our domain */
Index: linux-2.6.5/include/linux/sched.h
===================================================================
--- linux-2.6.5.orig/include/linux/sched.h	2004-04-16 15:06:45.000000000 +0200
+++ linux-2.6.5/include/linux/sched.h	2004-04-16 15:06:45.000000000 +0200
@@ -586,9 +586,9 @@
 	.cache_nice_tries	= 0,			\
 	.per_cpu_gain		= 15,			\
 	.flags			= SD_BALANCE_NEWIDLE	\
-				 | SD_WAKE_AFFINE	\
-				 | SD_WAKE_IDLE		\
-				 | SD_SHARE_CPUPOWER,	\
+				| SD_WAKE_AFFINE	\
+				| SD_WAKE_IDLE		\
+				| SD_SHARE_CPUPOWER,	\
 	.last_balance		= jiffies,		\
 	.balance_interval	= 1,			\
 	.nr_balance_failed	= 0,			\
