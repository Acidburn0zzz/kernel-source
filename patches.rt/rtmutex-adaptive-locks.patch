Subject: adaptive real-time lock support

From: Gregory Haskins <ghaskins@novell.com>

The Real Time patches to the Linux kernel converts the architecture
specific SMP-synchronization primitives commonly referred to as
"spinlocks" to an "RT mutex" implementation that support a priority
inheritance protocol, and priority-ordered wait queues.  The RT mutex
implementation allows tasks that would otherwise busy-wait for a
contended lock to be preempted by higher priority tasks without
compromising the integrity of critical sections protected by the lock.
The unintended side-effect is that the -rt kernel suffers from
significant degradation of IO throughput (disk and net) due to the
extra overhead associated with managing pi-lists and context switching.
This has been generally accepted as a price to pay for low-latency
preemption.

Adaptive real-time lock technology restores some of the benefits lost
in the conversion to RT mutexes by adaptively spinning or sleeping
while retaining both the priority inheritance protocol as well as the
preemptive nature of RT mutexes.  Essentially, the RT Mutex has been
modified to busy-wait under contention for a limited (and configurable)
time.  This works because most locks are typically held for very short
time spans.  Too often, by the time a task goes to sleep on a mutex,
the mutex is already being released on another CPU.  The effect (on SMP)
is that by polling a mutex for a limited time we reduce context switch
overhead by up to 90%, and therefore eliminate CPU cycles as well as
massive hot-spots in the scheduler / other bottlenecks in the
Kernel - even though we busy-wait (using CPU cycles) to poll the lock.

The net result is a significant performance boost (300%+) in many aspects
of the -rt kernel.  These performance increases apply to disk IO as well
as netperf UDP benchmarks, without compromising RT preemption latency.
For more complex applications, overall the I/O throughput seems to
approach the throughput on a PREEMPT_VOLUNTARY or PREEMPT_DESKTOP Kernel,
as is shipped by most distros.

Signed-off-by: Gregory Haskins <ghaskins@novell.com>
Signed-off-by: Peter Morreale <pmorreale@novell.com>
Signed-off-by: Sven Dietrich <sdietrich@novell.com>
---

 Documentation/adaptive-locks.txt |   38 ++++++++++++++
 include/linux/sched.h            |    2 -
 kernel/Kconfig.preempt           |   20 +++++++
 kernel/rtmutex.c                 |   20 +++++--
 kernel/rtmutex_adaptive.h        |  106 ++++++++++++++++++++++++++++++++++++++
 kernel/sched.c                   |   35 +++++++++----
 6 files changed, 203 insertions(+), 18 deletions(-)

diff --git a/Documentation/adaptive-locks.txt b/Documentation/adaptive-locks.txt
new file mode 100644
index 0000000..bc9abf0
--- /dev/null
+++ b/Documentation/adaptive-locks.txt
@@ -0,0 +1,38 @@
+ï»¿Adaptive real-time locks
+------------------------
+
+The Real Time patches to the Linux kernel converts the architecture
+specific SMP-synchronization primitives commonly referred to as
+"spinlocks" to an "RT mutex" implementation that support a priority
+inheritance protocol, and priority-ordered wait queues.  The RT mutex
+implementation allows tasks that would otherwise busy-wait for a
+contended lock to be preempted by higher priority tasks without
+compromising the integrity of critical sections protected by the lock.
+The unintended side-effect is that the -rt kernel suffers from
+significant degradation of IO throughput (disk and net) due to the
+extra overhead associated with managing pi-lists and context switching.
+This has been generally accepted as a price to pay for low-latency
+preemption.
+
+Adaptive real-time lock technology restores some of the benefits lost
+in the conversion to RT mutexes by adaptively spinning or sleeping
+while retaining both the priority inheritance protocol as well as the
+preemptive nature of RT mutexes.  Essentially, the RT Mutex has been
+modified to busy-wait under contention for a limited (and configurable)
+time.  This works because most locks are typically held for very short
+time spans.  Too often, by the time a task goes to sleep on a mutex,
+the mutex is already being released on another CPU.  The effect (on SMP)
+is that by polling a mutex for a limited time we reduce context switch
+overhead by up to 90%, and therefore eliminate CPU cycles as well as
+massive hot-spots in the scheduler / other bottlenecks in the
+Kernel - even though we busy-wait (using CPU cycles) to poll the lock.
+
+The net result is a significant performance boost (300%+) in many aspects
+of the -rt kernel.  These performance increases apply to disk IO as well
+as netperf UDP benchmarks, without compromising RT preemption latency.
+For more complex applications, overall the I/O throughput seems to
+approach the throughput on a PREEMPT_VOLUNTARY or PREEMPT_DESKTOP Kernel,
+as is shipped by most distros.
+
+
+
diff --git a/include/linux/sched.h b/include/linux/sched.h
index 7f0a58f..2b2ccbd 100644
--- a/include/linux/sched.h
+++ b/include/linux/sched.h
@@ -1080,10 +1080,8 @@ struct task_struct {
 	int lock_depth;		/* BKL lock depth */
 
 #ifdef CONFIG_SMP
-#ifdef __ARCH_WANT_UNLOCKED_CTXSW
 	int oncpu;
 #endif
-#endif
 
 	int prio, static_prio, normal_prio;
 #ifdef CONFIG_PREEMPT_RCU_BOOST
diff --git a/kernel/Kconfig.preempt b/kernel/Kconfig.preempt
index ebdda53..2077df7 100644
--- a/kernel/Kconfig.preempt
+++ b/kernel/Kconfig.preempt
@@ -171,3 +171,23 @@ config RTLOCK_LATERAL_STEAL
 	  to higher priority tasks.  This allows for a significant
 	  boost in throughput under certain circumstances at the expense
 	  of strict FIFO lock access.
+
+config ADAPTIVE_RTLOCK
+	bool "Adaptive real-time locks"
+	default y
+	depends on PREEMPT_RT && SMP
+	help
+	  PREEMPT_RT allows for greater determinism by transparently
+	  converting normal spinlock_ts into preemptible rtmutexes which
+	  sleep any waiters under contention.  However, in many cases the
+	  lock will be released in less time than it takes to context
+	  switch.  Therefore, the "sleep under contention" policy may also
+	  degrade throughput performance due to the extra context switches.
+
+	  This option alters the rtmutex derived spinlock_t replacement
+	  code to use an adaptive spin/sleep algorithm.  It will spin
+	  unless it determines it must sleep to avoid deadlock.  This
+	  offers a best of both worlds solution since we achieve both
+	  high-throughput and low-latency.
+
+	  If unsure, say Y.
diff --git a/kernel/rtmutex.c b/kernel/rtmutex.c
index 5cc9c2e..b0d10aa 100644
--- a/kernel/rtmutex.c
+++ b/kernel/rtmutex.c
@@ -7,6 +7,8 @@
  *  Copyright (C) 2005-2006 Timesys Corp., Thomas Gleixner <tglx@timesys.com>
  *  Copyright (C) 2005 Kihon Technologies Inc., Steven Rostedt
  *  Copyright (C) 2006 Esben Nielsen
+ *  Copyright (C) 2008 Novell, Inc., Sven Dietrich, Peter Morreale,
+ *                                   and Gregory Haskins
  *
  *  See Documentation/rt-mutex-design.txt for details.
  */
@@ -17,6 +19,7 @@
 #include <linux/hardirq.h>
 
 #include "rtmutex_common.h"
+#include "rtmutex_adaptive.h"
 
 #ifdef CONFIG_RTLOCK_LATERAL_STEAL
 int rtmutex_lateral_steal __read_mostly = 1;
@@ -693,6 +696,7 @@ rt_spin_lock_slowlock(struct rt_mutex *lock)
 {
 	struct rt_mutex_waiter waiter;
 	unsigned long saved_state, state, flags;
+	DECLARE_ADAPTIVE_WAITER(adaptive);
 
 	debug_rt_mutex_init_waiter(&waiter);
 	waiter.task = NULL;
@@ -738,6 +742,8 @@ rt_spin_lock_slowlock(struct rt_mutex *lock)
 				continue;
 		}
 
+		prepare_adaptive_wait(lock, &adaptive);
+
 		/*
 		 * Prevent schedule() to drop BKL, while waiting for
 		 * the lock ! We restore lock_depth when we come back.
@@ -749,11 +755,15 @@ rt_spin_lock_slowlock(struct rt_mutex *lock)
 
 		debug_rt_mutex_print_deadlock(&waiter);
 
-		update_current(TASK_UNINTERRUPTIBLE, &saved_state);
-		if (waiter.task)
-			schedule_rt_mutex(lock);
-		else
-			update_current(TASK_RUNNING_MUTEX, &saved_state);
+		/* adaptive_wait() returns 1 if we need to sleep */
+		if (adaptive_wait(lock, &waiter, &adaptive)) {
+			update_current(TASK_UNINTERRUPTIBLE, &saved_state);
+			if (waiter.task)
+				schedule_rt_mutex(lock);
+			else
+				update_current(TASK_RUNNING_MUTEX,
+					       &saved_state);
+		}
 
 		spin_lock_irqsave(&lock->wait_lock, flags);
 		current->flags |= saved_flags;
diff --git a/kernel/rtmutex_adaptive.h b/kernel/rtmutex_adaptive.h
new file mode 100644
index 0000000..8329e3c
--- /dev/null
+++ b/kernel/rtmutex_adaptive.h
@@ -0,0 +1,106 @@
+/*
+ * Adaptive RT lock support
+ *
+ * See Documentation/adaptive-locks.txt
+ *
+ * Copyright (C) 2008 Novell, Inc.,
+ *          Sven Dietrich, Peter Morreale, and Gregory Haskins
+ *
+ *  This program is free software; you can redistribute it and/or
+ *  modify it under the terms of the GNU General Public License
+ *  as published by the Free Software Foundation; version 2
+ *  of the License.
+ */
+
+#ifndef __KERNEL_RTMUTEX_ADAPTIVE_H
+#define __KERNEL_RTMUTEX_ADAPTIVE_H
+
+#include "rtmutex_common.h"
+
+
+#ifdef CONFIG_ADAPTIVE_RTLOCK
+struct adaptive_waiter {
+	struct task_struct *owner;
+};
+
+/*
+ * Adaptive-rtlocks will busywait when possible, and sleep only if
+ * necessary. Note that the busyloop looks racy, and it is....but we do
+ * not care. If we lose any races it simply means that we spin one more
+ * time before seeing that we need to break-out on the next iteration.
+ *
+ * We realize this is a relatively large function to inline, but note that
+ * it is only instantiated 1 or 2 times max, and it makes a measurable
+ * performance different to avoid the call.
+ *
+ * Returns 1 if we should sleep
+ *
+ */
+static inline int
+adaptive_wait(struct rt_mutex *lock, struct rt_mutex_waiter *waiter,
+	      struct adaptive_waiter *adaptive)
+{
+	int sleep = 0;
+
+	for (;;) {
+		/*
+		 * If the task was re-awoken, break out completely so we can
+		 * reloop through the lock-acquisition code.
+		 */
+		if (!waiter->task)
+			break;
+
+		/*
+		 * We need to break if the owner changed so we can reloop
+		 * and safely acquire the owner-pointer again with the
+		 * wait_lock held.
+		 */
+		if (adaptive->owner != rt_mutex_owner(lock))
+			break;
+
+		/*
+		 * Sleep if the owner is not running....we could deadlock
+		 * if they are waiting for our cpu, and it just doesn't
+		 * make sense to anyway.
+		 */
+		if (!adaptive->owner->oncpu) {
+			sleep = 1;
+			break;
+		}
+
+		cpu_relax();
+	}
+
+	put_task_struct(adaptive->owner);
+
+	return sleep;
+}
+
+static inline void
+prepare_adaptive_wait(struct rt_mutex *lock, struct adaptive_waiter *adaptive)
+{
+	/*
+	 * We must acquire/lock the owner pointer while holding
+	 * the wait_lock, or we risk racing against the owner
+	 * exiting.
+	 */
+	adaptive->owner = rt_mutex_owner(lock);
+	get_task_struct(adaptive->owner);
+}
+
+#define DECLARE_ADAPTIVE_WAITER(name)                       \
+     struct adaptive_waiter name = {                        \
+	.owner = NULL,                                      \
+}
+
+#else
+
+#define DECLARE_ADAPTIVE_WAITER(name)
+
+#define adaptive_wait(lock, waiter, busy) (1)
+#define prepare_adaptive_wait(lock, busy) do {} while (0)
+
+#endif /* CONFIG_ADAPTIVE_RTLOCK */
+
+
+#endif /* __KERNEL_RTMUTEX_ADAPTIVE_H */
diff --git a/kernel/sched.c b/kernel/sched.c
index 8b76b13..9c25c0f 100644
--- a/kernel/sched.c
+++ b/kernel/sched.c
@@ -832,18 +832,39 @@ static inline int task_current(struct rq *rq, struct task_struct *p)
 	return rq->curr == p;
 }
 
-#ifndef __ARCH_WANT_UNLOCKED_CTXSW
 static inline int task_running(struct rq *rq, struct task_struct *p)
 {
+#ifdef CONFIG_SMP
+	return p->oncpu;
+#else
 	return task_current(rq, p);
+#endif
 }
 
+#ifndef __ARCH_WANT_UNLOCKED_CTXSW
 static inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)
 {
+#ifdef CONFIG_SMP
+	/*
+	 * We can optimise this out completely for !SMP, because the
+	 * SMP rebalancing from interrupt is the only thing that cares
+	 * here.
+	 */
+	next->oncpu = 1;
+#endif
 }
 
 static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
 {
+#ifdef CONFIG_SMP
+	/*
+	 * After ->oncpu is cleared, the task can be moved to a different CPU.
+	 * We must ensure this doesn't happen until the switch is completely
+	 * finished.
+	 */
+	smp_wmb();
+	prev->oncpu = 0;
+#endif
 #ifdef CONFIG_DEBUG_SPINLOCK
 	/* this is a valid case when another task releases the spinlock */
 	rq->lock.owner = current;
@@ -859,14 +880,6 @@ static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
 }
 
 #else /* __ARCH_WANT_UNLOCKED_CTXSW */
-static inline int task_running(struct rq *rq, struct task_struct *p)
-{
-#ifdef CONFIG_SMP
-	return p->oncpu;
-#else
-	return task_current(rq, p);
-#endif
-}
 
 static inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)
 {
@@ -2189,7 +2202,7 @@ void sched_fork(struct task_struct *p, int clone_flags)
 	if (likely(sched_info_on()))
 		memset(&p->sched_info, 0, sizeof(p->sched_info));
 #endif
-#if defined(CONFIG_SMP) && defined(__ARCH_WANT_UNLOCKED_CTXSW)
+#if defined(CONFIG_SMP)
 	p->oncpu = 0;
 #endif
 #ifdef CONFIG_PREEMPT
@@ -5798,7 +5811,7 @@ void __cpuinit init_idle(struct task_struct *idle, int cpu)
 
 	spin_lock_irqsave(&rq->lock, flags);
 	rq->curr = rq->idle = idle;
-#if defined(CONFIG_SMP) && defined(__ARCH_WANT_UNLOCKED_CTXSW)
+#if defined(CONFIG_SMP)
 	idle->oncpu = 1;
 #endif
 	spin_unlock_irqrestore(&rq->lock, flags);
