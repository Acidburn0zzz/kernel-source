Subject: drivers-edac-new-amd64.patch (revision 108)
From: http://bluesmoke.svn.sourceforge.net/viewvc/*checkout*/bluesmoke/trunk/edac/patches/drivers-edac-new-amd64.patch?revision=108

Acked-by: Sven-Thorsten Dietrich <sdietrich@suse.de>

Index: linux-2.6.24-rc6-mm1/drivers/edac/amd64_edac.c
===================================================================
--- /dev/null	1970-01-01 00:00:00.000000000 +0000
+++ linux-2.6.24-rc6-mm1/drivers/edac/amd64_edac.c	2008-01-07 13:16:41.000000000 -0700
@@ -0,0 +1,4121 @@
+/*
+ * AMD64 class Memory Controller kernel module
+ *
+ * This file may be distributed under the terms of the
+ * GNU General Public License.
+ *
+ * Originally Written by Thayne Harbaugh Linux Networx (http://lnxi.com)
+ *
+ *      Changes by Douglas "norsk" Thompson  <dougthompson@xmission.com>:
+ *      	- K8 CPU Revision D and greater support
+ *
+ *      Changes by Dave Peterson <dsp@llnl.gov> <dave_peterson@pobox.com>:
+ *		- Module largely rewritten, with new (and hopefully correct)
+ *		code for dealing with node and chip select interleaving,
+ *		various code cleanup, and bug fixes
+ *		- Added support for memory hoisting using DRAM hole address
+ *		register
+ *
+ *	Changes by Douglas "norsk" Thompson <dougthompson@xmission.com>:
+ *		-K8 Rev (1207) revision support added, required Revision
+ *		specific mini-driver code to support Rev F as well as
+ *		prior revisions
+ *
+ *	Changes by Douglas "norsk" Thompson <dougthompson@xmission.com>:
+ *		-Family 10h revision support added. New PCI Device IDs,
+ *		indicating new changes. Actual registers modified
+ *		were slight, less than the Rev E to Rev F transition
+ *		but changing the PCI Device ID was the proper thing to
+ *		do, as it provides for almost automactic family
+ *		detection. The mods to Rev F required more family
+ *		information detection.
+ *
+ * This module is based on the following documents
+ * (available from http://www.amd.com/):
+ *
+ *	Title:	BIOS and Kernel Developer's Guide for AMD Athlon 64 and AMD
+ *		Opteron Processors
+ *	AMD publication #: 26094
+ *`	Revision: 3.26
+ *
+ *	Title:	BIOS and Kernel Developer's Guide for AMD NPT Family 0Fh
+ *		Processors
+ *	AMD publication #: 32559
+ *	Revision: 3.00
+ *	Issue Date: May 2006
+ *
+ *	Title:	BIOS and Kernel Developer's Guide (BKDG) For AMD Family 10h
+ *		Processors
+ *	AMD publication #: 31116
+ *	Revision: 3.00
+ *	Issue Date: September 07, 2007
+ *
+ * Sections in the first 2 documents are no longer in sync with each other.
+ * The Family 10h BKDG was totally re-written from scratch with a new
+ * presentation model.
+ * Therefore, comments that refer to a Document section might be off.
+ */
+
+#include <linux/module.h>
+#include <linux/ctype.h>
+#include <linux/init.h>
+#include <linux/pci.h>
+#include <linux/pci_ids.h>
+#include <linux/slab.h>
+#include <linux/mmzone.h>
+#include <linux/edac.h>
+#include "edac_core.h"
+
+#define amd64_printk(level, fmt, arg...) \
+	edac_printk(level, "amd64", fmt, ##arg)
+
+#define amd64_mc_printk(mci, level, fmt, arg...) \
+	edac_mc_chipset_printk(mci, level, "amd64", fmt, ##arg)
+
+/* Throughout the comments in this code, the following terms are used:
+ *
+ *	SysAddr, DramAddr, and InputAddr
+ *
+ *  These terms come directly from the amd64 documentation
+ * (AMD publication #26094).  They are defined as follows:
+ *
+ *     SysAddr:
+ *         This is a physical address generated by a CPU core or a device
+ *         doing DMA.  If generated by a CPU core, a SysAddr is the result of
+ *         a virtual to physical address translation by the CPU core's address
+ *         translation mechanism (MMU).
+ *
+ *     DramAddr:
+ *         A DramAddr is derived from a SysAddr by subtracting an offset that
+ *         depends on which node the SysAddr maps to and whether the SysAddr
+ *         is within a range affected by memory hoisting.  The DRAM Base
+ *         (section 3.4.4.1) and DRAM Limit (section 3.4.4.2) registers
+ *         determine which node a SysAddr maps to.
+ *
+ *         If the DRAM Hole Address Register (DHAR) is enabled and the SysAddr
+ *         is within the range of addresses specified by this register, then
+ *         a value x from the DHAR is subtracted from the SysAddr to produce a
+ *         DramAddr.  Here, x represents the base address for the node that
+ *         the SysAddr maps to plus an offset due to memory hoisting.  See
+ *         section 3.4.8 and the comments in get_dram_hole_info() and
+ *         sys_addr_to_dram_addr() below for more information.
+ *
+ *         If the SysAddr is not affected by the DHAR then a value y is
+ *         subtracted from the SysAddr to produce a DramAddr.  Here, y is the
+ *         base address for the node that the SysAddr maps to.  See section
+ *         3.4.4 and the comments in sys_addr_to_dram_addr() below for more
+ *         information.
+ *
+ *     InputAddr:
+ *         A DramAddr is translated to an InputAddr before being passed to the
+ *         memory controller for the node that the DramAddr is associated
+ *         with.  The memory controller then maps the InputAddr to a csrow.
+ *         If node interleaving is not in use, then the InputAddr has the same
+ *         value as the DramAddr.  Otherwise, the InputAddr is produced by
+ *         discarding the bits used for node interleaving from the DramAddr.
+ *         See section 3.4.4 for more information.
+ *
+ *         The memory controller for a given node uses its DRAM CS Base and
+ *         DRAM CS Mask registers to map an InputAddr to a csrow.  See
+ *         sections 3.5.4 and 3.5.5 for more information.
+ */
+
+/*
+ * Alter this version for the K8 module when modifications are made
+ */
+#define EDAC_AMD64_VERSION	" Ver: 3.0.1 " __DATE__
+#define EDAC_MOD_STR		"amd64_edac"
+
+/* PCI Device IDs we look for
+ *   Rev F and prior
+ */
+#ifndef PCI_DEVICE_ID_AMD_K8_NB_ADDRMAP
+#define PCI_DEVICE_ID_AMD_K8_NB_ADDRMAP		0x1101
+#endif				/* PCI_DEVICE_ID_AMD_K8_NB_ADDRMAP */
+
+#ifndef PCI_DEVICE_ID_AMD_K8_NB_MEMCTL
+#define PCI_DEVICE_ID_AMD_K8_NB_MEMCTL		0x1102
+#endif				/* PCI_DEVICE_ID_AMD_K8_NB_MEMCTL */
+
+#ifndef PCI_DEVICE_ID_AMD_K8_NB_MISC
+#define PCI_DEVICE_ID_AMD_K8_NB_MISC		0x1103
+#endif				/* PCI_DEVICE_ID_AMD_K8_NB_MISC */
+
+/* PCI Device IDs for Family 10h - Quad Core */
+#ifndef PCI_DEVICE_ID_AMD_F10_NB_ADDRMAP
+#define PCI_DEVICE_ID_AMD_F10_NB_ADDRMAP	0x1201
+#endif				/* PCI_DEVICE_ID_AMD_F10_NB_ADDRMAP */
+
+#ifndef PCI_DEVICE_ID_AMD_F10_NB_MEMCTL
+#define PCI_DEVICE_ID_AMD_F10_NB_MEMCTL		0x1202
+#endif				/* PCI_DEVICE_ID_AMD_F10_NB_MEMCTL */
+
+#ifndef PCI_DEVICE_ID_AMD_F10_NB_MISC
+#define PCI_DEVICE_ID_AMD_F10_NB_MISC		0x1203
+#endif				/* PCI_DEVICE_ID_AMD_F10_NB_MISC */
+
+/* PCI Device IDs for Family 11h */
+#ifndef PCI_DEVICE_ID_AMD_F11_NB_ADDRMAP
+#define PCI_DEVICE_ID_AMD_F11_NB_ADDRMAP	0x1301
+#endif				/* PCI_DEVICE_ID_AMD_F11_NB_ADDRMAP */
+
+#ifndef PCI_DEVICE_ID_AMD_F11_NB_MEMCTL
+#define PCI_DEVICE_ID_AMD_F11_NB_MEMCTL		0x1302
+#endif				/* PCI_DEVICE_ID_AMD_F11_NB_MEMCTL */
+
+#ifndef PCI_DEVICE_ID_AMD_F11_NB_MISC
+#define PCI_DEVICE_ID_AMD_F11_NB_MISC		0x1303
+#endif				/* PCI_DEVICE_ID_AMD_F11_NB_MISC */
+
+
+/* Extended Model from CPUID, for CPU Revision numbers */
+#define OPTERON_CPU_LE_REV_C    0
+#define OPTERON_CPU_REV_D       1
+#define OPTERON_CPU_REV_E       2
+
+/* Unknown Extended Model value */
+#define OPTERON_CPU_REV_X	3
+
+/* NPT processors have the following Extended Models */
+#define OPTERON_CPU_REV_F       4
+#define OPTERON_CPU_REV_FA	5
+
+/* Hardware limit on ChipSelect rows per MC
+ * and Processors per system
+ */
+#define CHIPSELECT_COUNT	8
+#define DRAM_REG_COUNT		8
+
+/* K8 register addresses - device 0 Function 1 - Address Map */
+#define K8_DRAM_BASE_LOW	0x40	/* Function 1: DRAM Base Register (8 x 32b
+				 * interlaced with K8_DRAM_LIMIT_LOW)
+				 *
+				 * 31:16 DRAM base address reg bits[39:24]
+				 * 15:11 reserved
+				 * 10:8  interleave enable
+				 *  7:2  reserved
+				 *  1    write enable
+				 *  0    read enable
+				 *
+				 */
+
+#define K8_DRAM_LIMIT_LOW	0x44	/* Function 1: DRAM Limit Register (8 x 32b
+				 * interlaced with K8_DRAM_BASE_LOW)
+				 *
+				 * 31:16 DRAM Limit addr 32:24
+				 * 15:11 reserved
+				 * 10:8  interleave select
+				 *  7:3  reserved
+				 *  2:0  destination node ID
+				 */
+
+#define K8_DHAR         0xf0	/* Function 1: DRAM Hole Address Register
+				 *
+				 * 31:24 DramHoleBase
+				 * 23:16 reserved
+				 * 15:8  DramHoleOffset
+				 *  7:1  reserved
+				 *    0  DramHoleValid
+				 */
+#define K8_DHAR_VALID		0x1
+#define K8_DHAR_BASE_MASK	0xff000000
+#define K8_DHAR_OFFSET_MASK	0x0000ff00
+
+#define F10_DRAM_BASE_HIGH	0x140	/* Function 1: DRAM Base register HIGH
+				 *
+				 * 7:0  Drambase[47:40] DRAM base address reg
+				 *	bits[47:40]
+				 */
+
+#define F10_DRAM_LIMIT_HIGH	0x144	/* Function 1: DRAM Limit Register HIGH
+				 *
+				 * 7:0  DRAM limit address register bits[47:40]
+				 */
+
+/* K8 register addresses - device 0 Function 2 - DRAM controller */
+#define K8_DCSB0		0x40
+#define K8_DCSB1		0x140
+				/* Function 2: DRAM Chip-Select Base (8 x 32b)
+				 *
+				 * For Rev E and prior
+				 * 31:21 Base addr high 35:25
+				 * 20:16 reserved
+				 * 15:9  Base addr low 19:13 (interlvd)
+				 *  8:1  reserved
+				 *  0    chip-select bank enable
+				 *
+				 * For Rev F (NPT) and later
+				 * 31:29 reserved
+				 * 28:19 Base address (36:27)
+				 * 18:14 reserved
+				 * 13:5  Base address (21:13)
+				 * 4:3   reserved
+				 * 2     TestFail
+				 * 1     Spare Rank
+				 * 0     CESenable
+				 *
+				 */
+#define K8_DCSB_CS_ENABLE	0x1
+#define K8_DCSB_NPT_SPARE	0x2
+#define K8_DCSB_NPT_TESTFAIL	0x4
+
+/* REV E: selects bits 31-21 and 15-9 from DCSB
+ * and the shift amount to form address
+ */
+#define REV_E_DCSB_BASE_BITS	(0xFFE0FE00ULL)
+#define REV_E_DCS_SHIFT		4
+#define REV_E_DCSM_SHIFT	0
+#define REV_E_DCSM_COUNT	8
+
+/* REV F: selects bits 28-19 and 13-5 from DCSB
+ * and the shift amount to form address
+ */
+#define REV_F_DCSB_BASE_BITS	(0x1FF83FE0ULL)
+#define REV_F_DCS_SHIFT		8
+#define REV_F_DCSM_SHIFT	1
+#define REV_F_DCSM_COUNT	4
+
+/* F10: selects bits 28-19 and 13-5 from DCSB
+ * and the shift amount to form address
+ */
+#define REV_F10_DCSB_BASE_BITS	(0x1FF83FE0ULL)
+#define REV_F10_DCS_SHIFT	8
+#define REV_F10_DCSM_SHIFT	1
+#define REV_F10_DCSM_COUNT	4
+
+#define K8_DCSM0		0x60
+#define K8_DCSM1		0x160
+				/* Function 2: DRAM Chip-Select Mask (8 x 32b)
+				 *
+				 * 31:30 reserved
+				 * 29:21 addr mask high 33:25
+				 * 20:16 reserved
+				 * 15:9  addr mask low  19:13
+				 *  8:0  reserved
+				 */
+
+/* REV E: selects bits 29-21 and 15-9 from DCSM */
+#define REV_E_DCSM_MASK_BITS 		0x3FE0FE00
+/* 	represents unused bits [24-20] and [12-0] */
+#define REV_E_DCS_NOTUSED_BITS		0x1f01fff
+
+/* REV F: selects bits 28-19 and 13-5 from DCSM */
+#define REV_F_DCSM_MASK_BITS 		0x1FF83FC0
+/* 	represents unused bits [26-22] and [12-0] */
+#define REV_F_DCS_NOTUSED_BITS		0x03c1fff
+
+/* REV F10: selects bits 28-19 and 13-5 from DCSM */
+#define REV_F10_DCSM_MASK_BITS 		0x1FF83FC0
+/* 	represents unused bits [26-22] and [12-0] */
+#define REV_F10_DCS_NOTUSED_BITS		0x03c1fff
+
+
+#define K8_DBAM		0x80	/* Function 2: DRAM Base Addr Mapping (32b) */
+
+#define K8_DCL		0x90	/* Function 2: DRAM configuration low reg (32b)
+				 *
+				 * Rev E and earlier CPUS:
+				 *
+				 * 31:28 reserved
+				 * 27:25 Bypass Max: 000b=respect
+				 * 24    Dissable receivers - no sockets
+				 * 23:20 x4 DIMMS
+				 * 19    32byte chunks
+				 * 18    Unbuffered
+				 * 17    ECC enabled
+				 * 16    128/64 bit (dual/single chan)
+				 * 15:14 R/W Queue bypass count
+				 * 13    Self refresh
+				 * 12    exit self refresh
+				 * 11    mem clear status
+				 * 10    DRAM enable
+				 *  9    reserved
+				 *  8    DRAM init
+				 *  7:4  reserved
+				 *  3    dis DQS hysteresis
+				 *  2    QFC enabled
+				 *  1    DRAM drive strength
+				 *  0    Digital Locked Loop disable
+				 *
+				 * Rev F and later CPUs:
+				 *
+				 * 31:20 reserved
+				 * 19    DIMM ECC Enable
+				 * 18:17 reserved
+				 * 16    Unbuffered DIMM
+				 * 15:12 x4 DIMMs
+				 * 11    Width128 bits
+				 * 10    burstLength32
+				 *  9    SelRefRateEn
+				 *  8    ParEn
+				 *  7    DramDrvWeak
+				 *  6    reserved
+				 * 5:4   DramTerm
+				 * 3:2   reserved
+				 *  1    ExitSelfRef
+				 *  0    InitDram
+				 */
+
+#define F10_DRAM_CTL_SELECT_LOW	0x110	/* device 0 Function 2 - DRAM
+				 * Controller SELECT LOW
+				 */
+
+#define F10_DRAM_CTL_SELECT_HIGH	0x114	/* device 0 Function 2 - DRAM
+				 * Controller SELECT HIGH
+				 */
+
+/* K8 register addresses - device 0 Function 3 - Misc Control */
+#define K8_NBCTL	0x40	/* Function 3: MCA NB Control (32b)
+				 *
+				 *  1    MCA UE Reporting
+				 *  0    MCA CE Reporting
+				 */
+/* Correctable ECC error reporting enable */
+#define K8_NBCTL_CECCEn		BIT(0)
+
+/* UnCorrectable ECC error reporting enable */
+#define K8_NBCTL_UECCEn		BIT(1)
+
+#define K8_NBCFG	0x44	/* Function 3: MCA NB Config (32b)
+				 *
+				 * 23    Chip-kill x4 ECC enable
+				 * 22    ECC enable
+				 */
+#define		K8_NBCFG_CHIPKILL	BIT(23)
+#define		K8_NBCFG_ECC_ENABLE	BIT(22)
+
+#define K8_NBSL		0x48	/* Function 3: MCA NB Status Low (32b)
+				 *
+				 * 31:24 Syndrome 15:8 chip-kill x4
+				 * 23:20 reserved
+				 * 19:16 Extended err code (F0fh and earlier)
+				 * 20:16 Extended err code (F10h and later)
+				 * 15:0  Err code
+				 */
+#define	EXTRACT_SYNDROME(x)		(((x) >> 15) & 0xff)
+#define EXTRACT_EXT_ERROR_CODE(x)	(((x) >> 16) & 0x1f)
+
+/* Start Family F10h: Normalized Extended Error Codes */
+#define F10_NBSL_EXT_ERR_RES		(0x0)
+#define F10_NBSL_EXT_ERR_CRC		(0x1)
+#define F10_NBSL_EXT_ERR_SYNC		(0x2)
+#define F10_NBSL_EXT_ERR_MST		(0x3)
+#define F10_NBSL_EXT_ERR_TGT		(0x4)
+#define F10_NBSL_EXT_ERR_GART		(0x5)
+#define F10_NBSL_EXT_ERR_RMW		(0x6)
+#define F10_NBSL_EXT_ERR_WDT		(0x7)
+#define F10_NBSL_EXT_ERR_ECC		(0x8)
+#define F10_NBSL_EXT_ERR_DEV		(0x9)
+#define F10_NBSL_EXT_ERR_LINK_DATA	(0xA)
+
+/* Next two are overloaded values */
+#define F10_NBSL_EXT_ERR_LINK_PROTO	(0xB)
+#define F10_NBSL_EXT_ERR_L3_PROTO	(0xB)
+
+#define F10_NBSL_EXT_ERR_NB_ARRAY	(0xC)
+#define F10_NBSL_EXT_ERR_DRAM_PARITY	(0xD)
+#define F10_NBSL_EXT_ERR_LINK_RETRY	(0xE)
+
+/* Next two are overloaded values */
+#define F10_NBSL_EXT_ERR_GART_WALK	(0xF)
+#define F10_NBSL_EXT_ERR_DEV_WALK	(0xF)
+
+/* 0x10 to 0x1B: Reserved */
+
+#define F10_NBSL_EXT_ERR_L3_DATA	(0x1C)
+#define F10_NBSL_EXT_ERR_L3_TAG		(0x1D)
+#define F10_NBSL_EXT_ERR_L3_LRU		(0x1E)
+/* End Family F10h: Extended Error Codes */
+
+/* Start K8: Normalized Extended Error Codes */
+#define K8_NBSL_EXT_ERR_ECC		(0x0)
+#define K8_NBSL_EXT_ERR_CRC		(0x1)
+#define K8_NBSL_EXT_ERR_SYNC		(0x2)
+#define K8_NBSL_EXT_ERR_MST		(0x3)
+#define K8_NBSL_EXT_ERR_TGT		(0x4)
+#define K8_NBSL_EXT_ERR_GART		(0x5)
+#define K8_NBSL_EXT_ERR_RMW		(0x6)
+#define K8_NBSL_EXT_ERR_WDT		(0x7)
+#define K8_NBSL_EXT_ERR_CHIPKILL_ECC	(0x8)
+#define K8_NBSL_EXT_ERR_DRAM_PARITY	(0xD)
+/* End K8: Extended Error Codes */
+
+
+/* Error Code */
+#define EXTRACT_ERROR_CODE(x)	((x) & 0xffff)
+#define		TEST_TLB_ERROR(x)	(((x) & 0xFFF0) == 0x0010)
+#define		TEST_MEM_ERROR(x)	(((x) & 0xFF00) == 0x0100)
+#define		TEST_BUS_ERROR(x)	(((x) & 0xF800) == 0x0800)
+#define 	EXTRACT_TT_CODE(x)	(((x) >> 2) & 0x3)
+#define 	EXTRACT_II_CODE(x)	(((x) >> 2) & 0x3)
+#define 	EXTRACT_LL_CODE(x)	(((x) >> 0) & 0x3)
+#define		EXTRACT_RRRR_CODE(x)	(((x) >> 4) & 0xf)
+#define 	EXTRACT_TO_CODE(x)	(((x) >> 8) & 0x1)
+#define		EXTRACT_PP_CODE(x)	(((x) >> 9) & 0x3)
+
+/* The following are for BUS type errors AFTER values have been
+ * normalized by shifting right
+ */
+#define K8_NBSL_PP_SRC		(0x0)
+#define K8_NBSL_PP_RES		(0x1)
+#define K8_NBSL_PP_OBS		(0x2)
+#define K8_NBSL_PP_GENERIC	(0x3)
+
+
+#define K8_NBSH		0x4C	/* Function 3: MCA NB Status High (32b)
+				 *
+				 * 31    Err valid
+				 * 30    Err overflow
+				 * 29    Uncorrected err
+				 * 28    Err enable
+				 * 27    Misc err reg valid
+				 * 26    Err addr valid
+				 * 25    proc context corrupt
+				 * 24:23 reserved
+				 * 22:15 Syndrome 7:0
+				 * 14    CE
+				 * 13    UE
+				 * 12:9  reserved
+				 *  8    err found by scrubber
+				 *  7    reserved
+				 *  6:4  Hyper-transport link number
+				 *    3:2  reserved= Rev F/Family 10h=Quad Core
+				 *  3    Err CPU 3	(F10)
+				 *  2    Err CPU 2	(F10)
+				 *  1    Err CPU 1	(Dual Core)
+				 *  0    Err CPU 0
+				 */
+
+#define K8_NBSH_VALID_BIT		BIT(31)
+#define K8_NBSH_OVERFLOW		BIT(30)
+#define K8_NBSH_UNCORRECTED_ERR		BIT(29)
+#define K8_NBSH_ERR_ENABLE		BIT(28)
+#define K8_NBSH_MISC_ERR_VALID		BIT(27)
+#define K8_NBSH_VALID_ERROR_ADDR	BIT(26)
+#define K8_NBSH_PCC			BIT(25)
+#define K8_NBSH_CECC			BIT(14)
+#define K8_NBSH_UECC			BIT(13)
+#define K8_NBSH_ERR_SCRUBER		BIT(8)
+
+#define EXTRACT_LDT_LINK(x) 		(((x) >> 4) & 0x7)
+#define EXTRACT_ERR_CPU_MAP(x)		((x) & 0xF)
+
+
+#define K8_NBEAL	0x50	/* Function 3: MCA NB err addr low (32b)
+				 *
+				 * 31:3  Err addr low 31:3
+				 *  2:0  reserved
+				 */
+
+#define K8_NBEAH	0x54	/* Function 3: MCA NB err addr high (32b)
+				 *
+				 * 31:8  reserved
+				 *  7:0  Err addr high 39:32
+				 */
+
+#define K8_SCRCTRL      0x58	/* Function 3: Memory scrub control register.
+				 *
+				 * 30:21 reserved
+				 * 20:16 dcache scrub
+				 * 15:13 reserved
+				 * 12:8  L2Scrub
+				 * 7:5   reserved
+				 * 4:0   dramscrub
+				 *
+				 */
+
+#define F10_ONLINE_SPARE	0xB0
+				/* Function 3: On-Line Spare Control Register */
+
+
+#define K8_NBCAP	0xE8	/* Function 3: MCA NB capabilities (32b)
+				 *
+				 * 31:9  reserved
+				 *  4    ChipKill S4ECD4ED capable
+				 *  3    SECDED capable
+				 */
+#define K8_NBCAP_CHIPKILL	4
+#define K8_NBCAP_SECDED		3
+
+				/* MSR's */
+
+				/*
+				 * K8_MSR_MCxCTL (64b)
+				 * (0x400,404,408,40C,410)
+				 * 63    Enable reporting source 63
+				 *  .
+				 *  .
+				 *  .
+				 *  2    Enable error source 2
+				 *  1    Enable error source 1
+				 *  0    Enable error source 0
+				 */
+
+				/*
+				 * K8_MSR_MCxSTAT (64b)
+				 * (0x401,405,409,40D,411)
+				 * 63    Error valid
+				 * 62    Status overflow
+				 * 61    UE
+				 * 60    Enabled error condition
+				 * 59    Misc register valid (not used)
+				 * 58    Err addr register valid
+				 * 57    Processor context corrupt
+				 * 56:32 Other information
+				 * 31:16 Model specific error code
+				 * 15:0  MCA err code
+				 */
+
+				/*
+				 * K8_MSR_MCxADDR (64b)
+				 * (0x402,406,40A,40E,412)
+				 * 63:48 reserved
+				 * 47:0  Address
+				 */
+
+				/*
+				 * K8_MSR_MCxMISC (64b)
+				 * (0x403,407,40B,40F,413)
+				 * Unused on Athlon64 and K8
+				 */
+
+#define K8_MSR_MCGCTL	0x017b	/* Machine Chk Global report ctl (64b)
+				 *
+				 * 31:5  reserved
+				 *  4    North Bridge
+				 *  3    Load/Store
+				 *  2    Bus Unit
+				 *  1    Instruction Cache
+				 *  0    Data Cache
+				 */
+#define K8_MSR_MCGCTL_NBE	BIT(4)
+
+#define K8_MSR_MC4CTL	0x0410	/* North Bridge Check report ctl (64b) */
+#define K8_MSR_MC4STAT	0x0411	/* North Bridge status (64b) */
+#define K8_MSR_MC4ADDR	0x0412	/* North Bridge Address (64b) */
+
+#define K8_MSR_TOP_MEM	0xC001001A	/* TOP_MEM */
+#define K8_MSR_TOP_MEM2	0xC001001D	/* TOP_MEM2 */
+
+static struct edac_pci_ctl_info *amd64_ctl_pci;
+static int report_gart_errors;
+module_param(report_gart_errors, int, 0644);
+
+/*
+ * AMD sets the first MC device at device ID 0x18. Subsequent ones go up
+ * from there.
+ */
+static inline int get_mc_node_id_from_pdev(struct pci_dev *pdev)
+{
+	/* Take whatever SLOT we are and subtract 0x18 to get this node ID */
+	return PCI_SLOT(pdev->devfn) - 0x18;
+}
+
+/* Ugly hack that allows module to compile when built as part of a 32-bit
+ * kernel.  Just in case anyone wants to run a 32-bit kernel on their Opteron.
+ */
+#ifndef MAXNODE
+#define MAXNODE 8
+#endif
+
+/* Lookup table for all possible MC control instances */
+static struct mem_ctl_info *mci_lookup[MAXNODE];
+
+/* Each entry holds the CPU revision of all CPU cores for the given node. */
+static int amd64_node_revision_table[MAXNODE] = { 0 };
+
+static inline int node_rev(int node_id)
+{
+	return amd64_node_revision_table[node_id];
+}
+
+static void store_node_revision(void *param)
+{
+	int node_id, revision;
+
+	/* Multiple CPU cores on the same node will all write their revision
+	 * number to the same array entry.  This is ok.  For a given node, all
+	 * CPU cores are on the same piece of silicon and share the same
+	 * revision number.
+	 */
+	node_id = (cpuid_ebx(1) >> 24) & 0x07;
+	revision = (cpuid_eax(1) >> 16) & 0x0f;
+	amd64_node_revision_table[node_id] = revision;
+}
+
+/* build_node_revision_table
+ *	Initialize amd64_node_revision_table.
+ */
+static void build_node_revision_table(void)
+{
+	static int initialized;
+
+	if (initialized)
+		return;
+
+	on_each_cpu(store_node_revision, NULL, 1, 1);
+	initialized = 1;
+}
+
+/*
+ * AMD Athlon64 and Opterons now have several 'Families'
+ * as defined by the PCI DEVICE IDs:
+ *
+ *	First Family:
+ *		K8
+ *	New Families denoted by leading "F"amily mark
+ *		F10
+ *		F11
+ */
+enum amd64_chipset_families {
+	K8_CPUS = 0,
+	F10_CPUS,
+	F11_CPUS,
+};
+
+/*
+ * Structure to hold dynamicly read status and error address registers
+ */
+struct amd64_error_info_regs {
+	u32 nbcfg;
+	u32 nbsh;
+	u32 nbsl;
+	u32 nbeah;
+	u32 nbeal;
+};
+
+struct amd64_error_info {
+	struct amd64_error_info_regs error_info;
+	int race_condition_detected;
+};
+
+struct amd64_pvt;
+
+/*
+ * Each of the PCI Device IDs types have their own set of hardware
+ * accessor function and per device encoding/decoding logic.
+ */
+struct low_ops {
+	int (*set_scrub_rate)(struct mem_ctl_info *mci, u32 *band_width);
+	int (*get_scrub_rate)(struct mem_ctl_info *mci, u32 *band_width);
+	u64 (*get_error_address)(struct mem_ctl_info *mci,
+			struct amd64_error_info *info);
+	void (*read_dram_base_limit)(struct amd64_pvt *pvt, int dram);
+	void (*read_misc_registers)(struct amd64_pvt *pvt);
+	void (*setup_dcsBase_dcsMask)(struct amd64_pvt *pvt);
+	void (*map_sysaddr_to_csrow)(struct mem_ctl_info *mci,
+					struct amd64_error_info *info,
+					u64 SystemAddr);
+};
+
+/*
+ * amd64 family unique informatoin
+ */
+struct amd64_family_type {
+	const char *ctl_name;
+	u16 addr_map;
+	u16 misc_ctl;
+	struct low_ops ops;
+};
+
+/*
+ * NO-OP K8 function for registers that don't exit in K8
+ *
+ * NO-OP K8 function for registers that don't exit in K8
+ */
+static void k8_noop_call(struct amd64_pvt *pvt)
+{
+}
+
+/*
+ * Forward references for the low_ops functions
+ *
+ * Common functions
+ */
+static int get_scrub_rate(struct mem_ctl_info *mci, u32 *bw);
+
+/* K8 functions */
+static int k8_set_scrub_rate(struct mem_ctl_info *mci, u32 *bw);
+static u64 k8_get_error_address(struct mem_ctl_info *mci,
+			struct amd64_error_info *info);
+static void k8_read_dram_base_limit(struct amd64_pvt *pvt, int dram);
+static void k8_setup_dcsBase_dcsMask(struct amd64_pvt *pvt);
+static void k8_map_sysaddr_to_csrow(struct mem_ctl_info *mci,
+					struct amd64_error_info *info,
+					u64 SystemAddr);
+
+/* F10 functions */
+static int f10_set_scrub_rate(struct mem_ctl_info *mci, u32 *bw);
+static u64 f10_get_error_address(struct mem_ctl_info *mci,
+			struct amd64_error_info *info);
+static void f10_read_dram_base_limit(struct amd64_pvt *pvt, int dram);
+static void f10_read_misc_registers(struct amd64_pvt *pvt);
+static void f10_setup_dcsBase_dcsMask(struct amd64_pvt *pvt);
+static void f10_map_sysaddr_to_csrow(struct mem_ctl_info *mci,
+					struct amd64_error_info *info,
+					u64 SystemAddr);
+
+/* F11 functions - FUTURE */
+
+/*
+ * There currently are 3 types type of MC device for AMD
+ * Athlon/Opterons:
+ *
+ * K8: That is the Athlon64 and Opteron CPUs. They all have the
+ * same PCI DEVICE ID, even though there is differences between
+ * the different Revisions (CG,D,E,F,future).
+ *
+ * F10: AMD Quad Core
+ *
+ * F11: ????
+ *
+ */
+static struct amd64_family_type amd64_family_types[] = {
+	[K8_CPUS] = {
+		.ctl_name = "Athlon64/Opteron",
+		.addr_map = PCI_DEVICE_ID_AMD_K8_NB_ADDRMAP,
+		.misc_ctl = PCI_DEVICE_ID_AMD_K8_NB_MISC,
+		.ops = {
+			.get_scrub_rate = get_scrub_rate,
+			.set_scrub_rate = k8_set_scrub_rate,
+			.get_error_address = k8_get_error_address,
+			.read_dram_base_limit = k8_read_dram_base_limit,
+			.read_misc_registers = k8_noop_call,
+			.setup_dcsBase_dcsMask = k8_setup_dcsBase_dcsMask,
+			.map_sysaddr_to_csrow = k8_map_sysaddr_to_csrow,
+		}
+	},
+	[F10_CPUS] = {
+		.ctl_name = "Family-F10h-Quad-Core",
+		.addr_map = PCI_DEVICE_ID_AMD_F10_NB_ADDRMAP,
+		.misc_ctl = PCI_DEVICE_ID_AMD_F10_NB_MISC,
+		.ops = {
+			.get_scrub_rate = get_scrub_rate,
+			.set_scrub_rate = f10_set_scrub_rate,
+			.get_error_address = f10_get_error_address,
+			.read_dram_base_limit = f10_read_dram_base_limit,
+			.read_misc_registers = f10_read_misc_registers,
+			.setup_dcsBase_dcsMask = f10_setup_dcsBase_dcsMask,
+			.map_sysaddr_to_csrow = f10_map_sysaddr_to_csrow,
+		}
+	},
+	[F11_CPUS] = {
+		.ctl_name = "Unknown-at-this-time",
+		.addr_map = PCI_DEVICE_ID_AMD_F11_NB_ADDRMAP,
+		.misc_ctl = PCI_DEVICE_ID_AMD_F11_NB_MISC,
+		.ops = {
+			.get_scrub_rate = get_scrub_rate,
+			.set_scrub_rate = f10_set_scrub_rate,
+			.get_error_address = f10_get_error_address,
+			.read_dram_base_limit = f10_read_dram_base_limit,
+			.read_misc_registers = f10_read_misc_registers,
+			.setup_dcsBase_dcsMask = f10_setup_dcsBase_dcsMask,
+			.map_sysaddr_to_csrow = f10_map_sysaddr_to_csrow,
+		}
+	},
+};
+
+
+
+/*
+ * Private to this module: data and register state
+ */
+struct amd64_pvt {
+	struct pci_dev *addr_map;
+	struct pci_dev *dram_ctl;
+	struct pci_dev *misc_ctl;
+
+	int mc_node_id;		/* MC index of this MC node */
+	int ext_model;		/* extended model value of this node */
+
+	struct low_ops	*ops;	/* pointer to per PCI Device ID func table */
+
+	/* Raw registers */
+	u32 dcl;		/* DRAM Configuration Low reg */
+	u32 nbcap;		/* North Bridge Capabilities */
+
+	/* DRAM CTL Base */
+	u32 dcsBase0[CHIPSELECT_COUNT];	/* DRAM CS Base Registers */
+	u32 dcsBase1[CHIPSELECT_COUNT];	/* DRAM CS Base Registers */
+	u32 dcsMask0[CHIPSELECT_COUNT];	/* DRAM CS Mask Registers */
+	u32 dcsMask1[CHIPSELECT_COUNT];	/* DRAM CS Mask Registers */
+
+	/* Decoded parts of DRAM BASE and LIMIT Registers */
+	u64 dram_base[DRAM_REG_COUNT];/* DRAM Base Reg */
+	u64 dram_limit[DRAM_REG_COUNT];/* DRAM Limit Reg */
+	u8 dram_IntlvSel[DRAM_REG_COUNT];
+	u8 dram_IntlvEn[DRAM_REG_COUNT];
+	u8 dram_DstNode[DRAM_REG_COUNT];
+	u8 dram_rw_en[DRAM_REG_COUNT];
+
+	/* The following fields are set at (load) run time, after Revision has
+	 * been determine, since the dcsBase and dcsMask registers vary
+	 * by CPU Revsion
+	 */
+	u32 dcsb_mask;		/* DCSB mask bits */
+	u32 dcsm_mask;		/* DCSM mask bits */
+	u32 num_dcsm;		/* Number of DCSM registers */
+	u32 dcs_mask_notused;	/* DCSM notused mask bits */
+	u32 dcs_shift;		/* DCSB and DCSM shift value */
+
+	/* On Rev E there are 8 DCSM registers,
+	 * On Rev F and later there are 4 DCSM registers.
+	 *
+	 * This field is set as a 0 (Rev E) or 1 (Rev F) to indicate
+	 * number of bits to shift the index for DCSM array look ups.
+	 */
+	u32 dcsm_shift_bit;
+
+	u32 dhar;
+	u32 dbam;		/* DRAM Base Address Mapping reg */
+
+	u64 top_mem;
+	u64 top_mem2;
+
+	/* F10 registers */
+	u32 dram_ctl_select_low;	/* DRAM Controller Select Low Reg */
+	u32 dram_ctl_select_high;	/* DRAM Controller Select High Reg */
+
+	/* sysfs storage area: Temp storage for when input
+	 * is received from sysfs
+	 */
+	struct amd64_error_info ctl_error_info;
+};
+
+
+/* Valid scrub rates for the K8 hardware memory scrubber. We map
+   the scrubbing bandwidth to a valid bit pattern. The 'set'
+   operation finds the 'matching- or higher value'.
+
+   FIXME: Produce a better mapping/linearisation.
+*/
+
+# define SDRATE_EOD 0xFFFFFFFF
+
+static struct scrubrate {
+	u32 scrubval;		/* bit pattern for scrub rate */
+	u32 bandwidth;		/* bandwidth consumed (bytes/sec) */
+} scrubrates[] = {
+	{ 0x00, 0UL},		/* Scrubbing Off */
+	{ 0x16, 761UL},		/* Slowest Rate: Family 10h and later */
+	{ 0x15, 1523UL},	/* Slowest Rate: Family 0Fh and prior */
+	{ 0x14, 3051UL},
+	{ 0x13, 6101UL},
+	{ 0x12, 12213UL},
+	{ 0x11, 24427UL},
+	{ 0x10, 48854UL},
+	{ 0x0F, 97650UL},
+	{ 0x0E, 195300UL},
+	{ 0x0D, 390720UL},
+	{ 0x0C, 781440UL},
+	{ 0x0B, 1560975UL},
+	{ 0x0A, 3121951UL},
+	{ 0x09, 6274509UL},
+	{ 0x08, 12284069UL},
+	{ 0x07, 25000000UL},
+	{ 0x06, 50000000UL},
+	{ 0x05, 100000000UL},
+	{ 0x04, 200000000UL},
+	{ 0x03, 400000000UL},
+	{ 0x02, 800000000UL},
+	{ 0x01, 1600000000UL},
+	{ 0x00, SDRATE_EOD}	/* End Of Data */
+};
+
+/* For future CPU version, verify the following as new 'slow' rates appear.
+ * And modify the necessary skip values for the supported CPU
+ */
+#define K8_SCRUB_SKIP_LOW	1
+#define K8_SCRUB_SKIP_HIGH	1
+#define F10_SCRUB_SKIP_LOW	(sizeof(scrubrates) / sizeof(struct scrubrate))
+#define F10_SCRUB_SKIP_HIGH	(sizeof(scrubrates) / sizeof(struct scrubrate))
+#define F11_SCRUB_SKIP_LOW	(sizeof(scrubrates) / sizeof(struct scrubrate))
+#define F11_SCRUB_SKIP_HIGH	(sizeof(scrubrates) / sizeof(struct scrubrate))
+
+
+/* Memory scrubber control interface. For the K8 memory scrubbing is
+   handled by hardware and can involve the cache and the dcache as
+   well as the main memory. This causes the "units" for the scrubbing
+   speed to vary from 64 byte blocks (sdram) over cache lines (cache)
+   to bits (dcache).
+
+   This is nasty, so we will use bandwidth in bytes/sec  for the setting.
+
+   Currently, we only do scrubbing of sdram - the caches are assumed
+   to be excercised always by running code and if the scrubber is done
+   in software on other archs, we might not have access to the
+   caches directly.
+*/
+
+/*
+ * search_set_scrub_rate
+ *
+ *	scan the scrub rate mapping table for a close or matching
+ *	bandwidth value to issue. If requested is too big, then
+ *	use last maxium value found.
+ */
+static int search_set_scrub_rate(struct pci_dev *ctl, u32 new_bw,
+				int skip_low, int skip_high)
+{
+	u32 scrubval;
+	int i;
+
+	/* map the configured rate (new_bw) to a value specific to
+	 * the AMD64 memory controller and apply to register.
+	 * Search for the first bandwidth entry that is greater or equal
+	 * than the setting requested and program that.
+	 * If at last entry, accept that as the maximum. This saves
+	 * the user from determing maximum empirically
+	 */
+	for (i = 0;; i++) {
+
+		/* skip entries within the skip_window, move to next */
+		if ( (i >= skip_low) && (i <= skip_high))
+			continue;
+
+		/* If we found the proper entry, use it */
+		if (scrubrates[i].bandwidth >= new_bw)
+			break;
+
+		/* If this is the LAST entry in table, use it */
+		if (scrubrates[i+1].bandwidth == SDRATE_EOD)
+			break;
+	}
+
+	scrubval = scrubrates[i].scrubval;
+	pci_write_bits32(ctl, K8_SCRCTRL, scrubval, 0x001F);
+
+	return 0;
+}
+
+/* K8 Set function */
+static int k8_set_scrub_rate(struct mem_ctl_info *mci, u32 *band_width)
+{
+	struct amd64_pvt *pvt = mci->pvt_info;
+
+	return search_set_scrub_rate(pvt->misc_ctl, *band_width,
+				K8_SCRUB_SKIP_LOW, K8_SCRUB_SKIP_HIGH);
+}
+
+/* F10 Set Scrub Function */
+static int f10_set_scrub_rate(struct mem_ctl_info *mci, u32 *band_width)
+{
+	struct amd64_pvt *pvt = mci->pvt_info;
+
+	return search_set_scrub_rate(pvt->misc_ctl, *band_width,
+				F10_SCRUB_SKIP_LOW, F10_SCRUB_SKIP_HIGH);
+}
+
+/*
+ * set_sdram_scrub_rate
+ *
+ *	Generic Set scrub rate function
+ */
+static int set_sdram_scrub_rate(struct mem_ctl_info *mci,
+				u32 *band_width)
+{
+	struct amd64_pvt *pvt = mci->pvt_info;
+
+	return (*(pvt->ops->set_scrub_rate))(mci,band_width);
+}
+
+
+/* get_scrub_rate
+ *
+ *	K8 GET Scrub rate function
+ *	F10 Get Scrub Rate
+ *	F11 Get Scrub Rate
+ */
+static int get_scrub_rate(struct mem_ctl_info *mci, u32 *bw)
+{
+	struct amd64_pvt *pvt;
+	u32 scrubval = 0;
+	int status = -1;
+	int i;
+
+	pvt = mci->pvt_info;
+
+	/* find the bandwidth matching the memory scrubber configuration
+	 * and return that value via *bw.
+	 */
+	pci_read_config_dword(pvt->misc_ctl, K8_SCRCTRL, &scrubval);
+	scrubval = scrubval & 0x001F;
+
+	edac_printk(KERN_DEBUG, EDAC_MC,
+		    "pci-read, sdram scrub control value: %d \n", scrubval);
+
+	for (i = 0; scrubrates[i].bandwidth != SDRATE_EOD; i++) {
+
+		if (scrubrates[i].scrubval == scrubval) {
+			*bw = scrubrates[i].bandwidth;
+			status = 0;
+			break;
+		}
+	}
+
+	/* the bit pattern is invalid - we might fix it
+	   by applying the slowest scrub rate as this is
+	   closest to the valid value, but we do not!
+	 */
+	if (scrubrates[i].bandwidth == SDRATE_EOD) {
+		edac_printk(KERN_WARNING, EDAC_MC,
+			    "Invalid sdram scrub control value: %d\n",
+			    scrubval);
+		status = -1;
+	}
+	return status;
+}
+
+/*
+ * get_sdram_scrub_rate
+ *
+ *	get the current value of the scrub rate register, then map
+ *	that value to a bytes/second value and return it
+ */
+static int get_sdram_scrub_rate(struct mem_ctl_info *mci, u32 *bw)
+{
+	struct amd64_pvt *pvt = mci->pvt_info;
+
+	return (*(pvt->ops->get_scrub_rate))(mci,bw);
+}
+
+/*
+ * Iterate over devices to find AMD related devices
+ */
+static struct pci_dev *pci_get_related_function(unsigned int vendor,
+						unsigned int device,
+						struct pci_dev *related)
+{
+	struct pci_dev *dev;
+
+	dev = NULL;
+
+	dev = pci_get_device(vendor, device, dev);
+	while (dev != NULL) {
+		if ((dev->bus->number == related->bus->number) &&
+		    (PCI_SLOT(dev->devfn) == PCI_SLOT(related->devfn)))
+			break;
+		dev = pci_get_device(vendor, device, dev);
+	}
+
+	return dev;
+}
+
+/* stolen from msr.c - the calls in msr.c could be exported */
+struct msr_command {
+	int cpu;
+	int err;
+	u32 reg;
+	u32 data[2];
+};
+
+static void smp_wrmsr(void *cmd_block)
+{
+	struct msr_command *cmd = cmd_block;
+	wrmsr(cmd->reg, cmd->data[0], cmd->data[1]);
+}
+
+static void smp_rdmsr(void *cmd_block)
+{
+	struct msr_command *cmd = cmd_block;
+	rdmsr(cmd->reg, cmd->data[0], cmd->data[1]);
+}
+
+static void do_wrmsr(int cpu, u32 reg, u32 eax, u32 edx)
+{
+	struct msr_command cmd;
+
+	cmd.cpu = raw_smp_processor_id();
+	cmd.reg = reg;
+	cmd.data[0] = eax;
+	cmd.data[1] = edx;
+	on_each_cpu(smp_wrmsr, &cmd, 1, 1);
+}
+
+static void do_rdmsr(int cpu, u32 reg, u32 * eax, u32 * edx)
+{
+	struct msr_command cmd;
+
+	cmd.cpu = raw_smp_processor_id();
+	cmd.reg = reg;
+	on_each_cpu(smp_rdmsr, &cmd, 1, 1);
+	*eax = cmd.data[0];
+	*edx = cmd.data[1];
+}
+
+/*
+ * FIXME - This is a large chunk of memory to suck up just to decode the
+ * syndrome.  It would be nice to discover a pattern in the syndromes that
+ * could be used to quickly identify the channel.  The big problems with
+ * this table is memory usage, lookup speed (could sort and binary search),
+ * correctness (there could be a transcription error).  A zero in any nibble
+ * for a syndrom is always channel 0, but that only decodes some of the
+ * syndromes.  Can anyone find any other patterns?
+ *
+ * The comment in the left column is the nibble that is in error.  The least
+ * significant nibble of the syndrome is the mask for the bits that are
+ * in error (need to be toggled) for the particular nibble.
+ */
+#define SYNDROME_TABLE_SIZE 270
+static const unsigned long syndromes_chan0[SYNDROME_TABLE_SIZE] = {
+	/*0 */ 0xe821, 0x7c32, 0x9413, 0xbb44, 0x5365, 0xc776, 0x2f57,
+	0xdd88, 0x35a9, 0xa1ba, 0x499b, 0x66cc, 0x8eed, 0x1afe, 0xf2df,
+	/*1 */ 0x5d31, 0xa612, 0xfb23, 0x9584, 0xc8b5, 0x3396, 0x6ea7,
+	0xeac8, 0xb7f9, 0x4cda, 0x11eb, 0x7f4c, 0x227d, 0xd95e, 0x846f,
+	/*2 */ 0x0001, 0x0002, 0x0003, 0x0004, 0x0005, 0x0006, 0x0007,
+	0x0008, 0x0009, 0x000a, 0x000b, 0x000c, 0x000d, 0x000e, 0x000f,
+	/*3 */ 0x2021, 0x3032, 0x1013, 0x4044, 0x6065, 0x7076, 0x5057,
+	0x8088, 0xa0a9, 0xb0ba, 0x909b, 0xc0cc, 0xe0ed, 0xf0fe, 0xd0df,
+	/*4 */ 0x5041, 0xa082, 0xf0c3, 0x9054, 0xc015, 0x30d6, 0x6097,
+	0xe0a8, 0xb0e9, 0x402a, 0x106b, 0x70fc, 0x20bd, 0xd07e, 0x803f,
+	/*5 */ 0xbe21, 0xd732, 0x6913, 0x2144, 0x9f65, 0xf676, 0x4857,
+	0x3288, 0x8ca9, 0xe5ba, 0x5b9b, 0x13cc, 0xaded, 0xc4fe, 0x7adf,
+	/*6 */ 0x4951, 0x8ea2, 0xc7f3, 0x5394, 0x1ac5, 0xdd36, 0x9467,
+	0xa1e8, 0xe8b9, 0x2f4a, 0x661b, 0xf27c, 0xbb2d, 0x7cde, 0x358f,
+	/*7 */ 0x74e1, 0x9872, 0xec93, 0xd6b4, 0xa255, 0x4ec6, 0x3a27,
+	0x6bd8, 0x1f39, 0xf3aa, 0x874b, 0xbd6c, 0xc98d, 0x251e, 0x51ff,
+	/*8 */ 0x15c1, 0x2a42, 0x3f83, 0xcef4, 0xdb35, 0xe4b6, 0xf177,
+	0x4758, 0x5299, 0x6d1a, 0x78db, 0x89ac, 0x9c6d, 0xa3ee, 0xb62f,
+	/*9 */ 0x3d01, 0x1602, 0x2b03, 0x8504, 0xb805, 0x9306, 0xae07,
+	0xca08, 0xf709, 0xdc0a, 0xe10b, 0x4f0c, 0x720d, 0x590e, 0x640f,
+	/*a */ 0x9801, 0xec02, 0x7403, 0x6b04, 0xf305, 0x8706, 0x1f07,
+	0xbd08, 0x2509, 0x510a, 0xc90b, 0xd60c, 0x4e0d, 0x3a0e, 0xa20f,
+	/*b */ 0xd131, 0x6212, 0xb323, 0x3884, 0xe9b5, 0x5a96, 0x8ba7,
+	0x1cc8, 0xcdf9, 0x7eda, 0xafeb, 0x244c, 0xf57d, 0x465e, 0x976f,
+	/*c */ 0xe1d1, 0x7262, 0x93b3, 0xb834, 0x59e5, 0xca56, 0x2b87,
+	0xdc18, 0x3dc9, 0xae7a, 0x4fab, 0x542c, 0x85fd, 0x164e, 0xf79f,
+	/*d */ 0x6051, 0xb0a2, 0xd0f3, 0x1094, 0x70c5, 0xa036, 0xc067,
+	0x20e8, 0x40b9, 0x904a, 0x601b, 0x307c, 0x502d, 0x80de, 0xe08f,
+	/*e */ 0xa4c1, 0xf842, 0x5c83, 0xe6f4, 0x4235, 0x1eb6, 0xba77,
+	0x7b58, 0xdf99, 0x831a, 0x27db, 0x9dac, 0x396d, 0x65ee, 0xc12f,
+	/*f */ 0x11c1, 0x2242, 0x3383, 0xc8f4, 0xd935, 0xeab6, 0xfb77,
+	0x4c58, 0x5d99, 0x6e1a, 0x7fdb, 0x84ac, 0x9562, 0xa6ee, 0xb72f,
+
+	/*20 */ 0xbe01, 0xd702, 0x6903, 0x2104, 0x9f05, 0xf606, 0x4807,
+	0x3208, 0x8c09, 0xe50a, 0x5b0b, 0x130c, 0xad0d, 0xc40e, 0x7a0f,
+	/*21 */ 0x4101, 0x8202, 0xc303, 0x5804, 0x1905, 0xda06, 0x9b07,
+	0xac08, 0xed09, 0x2e0a, 0x6f0b, 0x640c, 0xb50d, 0x760e, 0x370f
+};
+
+static const unsigned long syndromes_chan1[SYNDROME_TABLE_SIZE] = {
+	/*10 */ 0x45d1, 0x8a62, 0xcfb3, 0x5e34, 0x1be5, 0xd456, 0x9187,
+	0xa718, 0xe2c9, 0x2d7a, 0x68ab, 0xf92c, 0xbcfd, 0x734e, 0x369f,
+	/*11 */ 0x63e1, 0xb172, 0xd293, 0x14b4, 0x7755, 0xa5c6, 0xc627,
+	0x28d8, 0x4b39, 0x99aa, 0xfa4b, 0x3c6c, 0x5f8d, 0x8d1e, 0xeeff,
+	/*12 */ 0xb741, 0xd982, 0x6ec3, 0x2254, 0x9515, 0xfbd6, 0x4c97,
+	0x33a8, 0x84e9, 0xea2a, 0x5d6b, 0x11fc, 0xa6bd, 0xc87e, 0x7f3f,
+	/*13 */ 0xdd41, 0x6682, 0xbbc3, 0x3554, 0xe815, 0x53d6, 0xce97,
+	0x1aa8, 0xc7e9, 0x7c2a, 0xa1fb, 0x2ffc, 0xf2bd, 0x497e, 0x943f,
+	/*14 */ 0x2bd1, 0x3d62, 0x16b3, 0x4f34, 0x64e5, 0x7256, 0x5987,
+	0x8518, 0xaec9, 0xb87a, 0x93ab, 0xca2c, 0xe1fd, 0xf74e, 0xdc9f,
+	/*15 */ 0x83c1, 0xc142, 0x4283, 0xa4f4, 0x2735, 0x65b6, 0xe677,
+	0xf858, 0x7b99, 0x391a, 0xbadb, 0x5cac, 0xdf6d, 0x9dee, 0x1e2f,
+	/*16 */ 0x8fd1, 0xc562, 0x4ab3, 0xa934, 0x26e5, 0x6c56, 0xe387,
+	0xfe18, 0x71c9, 0x3b7a, 0xb4ab, 0x572c, 0xd8fd, 0x924e, 0x1d9f,
+	/*17 */ 0x4791, 0x89e2, 0xce73, 0x5264, 0x15f5, 0xdb86, 0x9c17,
+	0xa3b8, 0xe429, 0x2a5a, 0x6dcb, 0xf1dc, 0xb64d, 0x783e, 0x3faf,
+	/*18 */ 0x5781, 0xa9c2, 0xfe43, 0x92a4, 0xc525, 0x3b66, 0x6ce7,
+	0xe3f8, 0xb479, 0x4a3a, 0x1dbb, 0x715c, 0x26dd, 0xd89e, 0x8f1f,
+	/*19 */ 0xbf41, 0xd582, 0x6ac3, 0x2954, 0x9615, 0xfcd6, 0x4397,
+	0x3ea8, 0x81e9, 0xeb2a, 0x546b, 0x17fc, 0xa8bd, 0xc27e, 0x7d3f,
+	/*1a */ 0x9891, 0xe1e2, 0x7273, 0x6464, 0xf7f5, 0x8586, 0x1617,
+	0xb8b8, 0x2b29, 0x595a, 0xcacb, 0xdcdc, 0x4f4d, 0x3d3e, 0xaeaf,
+	/*1b */ 0xcce1, 0x4472, 0x8893, 0xfdb4, 0x3f55, 0xb9c6, 0x7527,
+	0x56d8, 0x9a39, 0x12aa, 0xde4b, 0xab6c, 0x678d, 0xef1e, 0x23ff,
+	/*1c */ 0xa761, 0xf9b2, 0x5ed3, 0xe214, 0x4575, 0x1ba6, 0xbcc7,
+	0x7328, 0xd449, 0x8a9a, 0x2dfb, 0x913c, 0x365d, 0x688e, 0xcfef,
+	/*1d */ 0xff61, 0x55b2, 0xaad3, 0x7914, 0x8675, 0x2ca6, 0xd3c7,
+	0x9e28, 0x6149, 0xcb9a, 0x34fb, 0xe73c, 0x185d, 0xb28e, 0x4def,
+	/*1e */ 0x5451, 0xa8a2, 0xfcf3, 0x9694, 0xc2c5, 0x3e36, 0x6a67,
+	0xebe8, 0xbfb9, 0x434a, 0x171b, 0x7d7c, 0x292d, 0xd5de, 0x818f,
+	/*1f */ 0x6fc1, 0xb542, 0xda83, 0x19f4, 0x7635, 0xacb6, 0xc377,
+	0x2e58, 0x4199, 0x9b1a, 0xf4db, 0x37ac, 0x586d, 0x82ee, 0xed2f,
+
+	/*22 */ 0xc441, 0x4882, 0x8cc3, 0xf654, 0x3215, 0xbed6, 0x7a97,
+	0x5ba8, 0x9fe9, 0x132a, 0xd76b, 0xadfc, 0x69bd, 0xe57e, 0x213f,
+	/*23 */ 0x7621, 0x9b32, 0xed13, 0xda44, 0xac65, 0x4176, 0x3757,
+	0x6f88, 0x19a9, 0xf4ba, 0x829b, 0xb5cc, 0xc3ed, 0x2efe, 0x58df
+};
+
+static int chan_from_chipkill_syndrome(unsigned long syndrome)
+{
+	int i;
+
+	debugf0("%s()\n", __func__);
+
+	for (i = 0; i < SYNDROME_TABLE_SIZE; i++) {
+		if (syndromes_chan0[i] == syndrome)
+			return 0;
+		if (syndromes_chan1[i] == syndrome)
+			return 1;
+	}
+
+	debugf0("%s(): syndrome(%lx) not found\n", __func__, syndrome);
+	return -1;
+}
+
+/*
+ * toString tables for the various error decoding values
+ */
+static const char *tt_msgs[] = {	/* transaction type */
+	"instruction",
+	"data",
+	"generic",
+	"reserved"
+};
+
+static const char *ll_msgs[] = {	/* cache level */
+	"L0",
+	"L1",
+	"L2",
+	"L3/generic"
+};
+
+static const char *rrrr_msgs[] = {
+	"generic",
+	"generic read",
+	"generic write",
+	"data read",
+	"data write",
+	"inst fetch",
+	"prefetch",
+	"evict",
+	"snoop",
+	"reserved RRRR= 9",
+	"reserved RRRR= 10",
+	"reserved RRRR= 11",
+	"reserved RRRR= 12",
+	"reserved RRRR= 13",
+	"reserved RRRR= 14",
+	"reserved RRRR= 15"
+};
+
+static const char *pp_msgs[] = {	/* participating processor */
+	"local node originated (SRC)",
+	"local node responded to request (RES)",
+	"local node observed as 3rd party (OBS)",
+	"generic"
+};
+
+static const char *to_msgs[] = {
+	"no timeout",
+	"timed out"
+};
+
+static const char *ii_msgs[] = {	/* memory or i/o */
+	"mem access",
+	"reserved",
+	"i/o access",
+	"generic"
+};
+
+/* Map the 5 bits of Extended Error code toString table
+ *	enhanced to support the new F10 error codes
+ *	esp, with L3 cache errors
+ */
+static const char *ext_msgs[] = {	/* extended error */
+	"K8 ECC error/F10 reserved",	/* 0_0000b */
+	"CRC error",			/* 0_0001b */
+	"sync error",			/* 0_0010b */
+	"mst abort",			/* 0_0011b */
+	"tgt abort",			/* 0_0100b */
+	"GART error",			/* 0_0101b */
+	"RMW error",			/* 0_0110b */
+	"Wdog timer error",		/* 0_0111b */
+	"F10-ECC/K8-Chipkill error",	/* 0_1000b */
+	"DEV Error",			/* 0_1001b */
+	"Link Data error",		/* 0_1010b */
+	"Link or L3 Protocol error",	/* 0_1011b */
+	"NB Array error",		/* 0_1100b */
+	"DRAM Parity error",		/* 0_1101b */
+	"Link Retry/GART Table Walk/DEV Table Walk error", /* 0_1110b */
+	"Res 0x0ff error",		/* 0_1111b */
+	"Res 0x100 error",		/* 1_0000b */
+	"Res 0x101 error",		/* 1_0001b */
+	"Res 0x102 error",		/* 1_0010b */
+	"Res 0x103 error",		/* 1_0011b */
+	"Res 0x104 error",		/* 1_0100b */
+	"Res 0x105 error",		/* 1_0101b */
+	"Res 0x106 error",		/* 1_0110b */
+	"Res 0x107 error",		/* 1_0111b */
+	"Res 0x108 error",		/* 1_1000b */
+	"Res 0x109 error",		/* 1_1001b */
+	"Res 0x10A error",		/* 1_1010b */
+	"Res 0x10B error",		/* 1_1011b */
+	"L3 Cache Data error",		/* 1_1100b */
+	"L3 CacheTag error",		/* 1_1101b */
+	"L3 Cache LRU error",		/* 1_1110b */
+	"Res 0x1FF error"		/* 1_1111b */
+};
+
+static const char *htlink_msgs[] = {
+	"none",
+	"1",
+	"2",
+	"1 2",
+	"3",
+	"1 3",
+	"2 3",
+	"1 2 3"
+};
+
+/*
+ * The DCSB and DCSM registers differ between Rev E and Rev F CPUs
+ * The following several functions intialize and extract information
+ * from this registers
+ */
+
+/*
+ * k8_set_dcsBase_dcsMask_rev_specific(pvt)
+ *
+ *	NOTE: CPU Revision Dependent code: Rev E and Rev F
+ *
+ *	Set the DCSB and DCSM mask values depending on the
+ *	CPU revision value.
+ *	Also set the shift factor for the DCSB and DCSM values
+ *
+ *	member dcs_mask_notused, REV E:
+ *
+ *	To find the max InputAddr for the csrow, start with the base
+ *	address and set all bits that are "don't care" bits in the test at
+ *	the start of section 3.5.4 (p. 84).
+ *
+ *	The "don't care" bits are all set bits in the mask and
+ *	all bits in the gaps between bit ranges [35-25] and [19-13].
+ *	The value REV_E_DCS_NOTUSED_BITS represents bits [24-20] and [12-0],
+ *	which are all bits in the above-mentioned gaps.
+ *
+ *	member dcs_mask_notused, REV F:
+ *
+ *	To find the max InputAddr for the csrow, start with the base
+ *	address and set all bits that are "don't care" bits in the test at
+ *	the start of NPT section 4.5.4 (p. 87).
+ *
+ *	The "don't care" bits are all set bits in the mask and
+ *	all bits in the gaps between bit ranges [36-27] and [21-13].
+ *	The value REV_F_DCS_NOTUSED_BITS represents bits [26-22] and [12-0],
+ *	which are all bits in the above-mentioned gaps.
+ */
+static void k8_set_dcsBase_dcsMask_rev_specific(struct amd64_pvt *pvt)
+{
+	if (pvt->ext_model >= OPTERON_CPU_REV_F) {
+		pvt->dcsb_mask = REV_F_DCSB_BASE_BITS;
+		pvt->dcsm_mask = REV_F_DCSM_MASK_BITS;
+		pvt->dcs_mask_notused = REV_F_DCS_NOTUSED_BITS;
+		pvt->dcs_shift = REV_F_DCS_SHIFT;
+		pvt->dcsm_shift_bit = REV_F_DCSM_SHIFT;
+		pvt->num_dcsm = REV_F_DCSM_COUNT;
+	} else {
+		pvt->dcsb_mask = REV_E_DCSB_BASE_BITS;
+		pvt->dcsm_mask = REV_E_DCSM_MASK_BITS;
+		pvt->dcs_mask_notused = REV_E_DCS_NOTUSED_BITS;
+		pvt->dcs_shift = REV_E_DCS_SHIFT;
+		pvt->dcsm_shift_bit = REV_E_DCSM_SHIFT;
+		pvt->num_dcsm = REV_E_DCSM_COUNT;
+	}
+}
+
+/*
+ * f10_set_dcsBase_dcsMask_rev_specific
+ *
+ *	setup F10 values of masks, shifts and counts for this family of CPUs
+ *	concerning the DRAM BASE and MASK registers
+ */
+static void f10_set_dcsBase_dcsMask_rev_specific(struct amd64_pvt *pvt)
+{
+		pvt->dcsb_mask = REV_F10_DCSB_BASE_BITS;
+		pvt->dcsm_mask = REV_F10_DCSM_MASK_BITS;
+		pvt->dcs_mask_notused = REV_F10_DCS_NOTUSED_BITS;
+		pvt->dcs_shift = REV_F10_DCS_SHIFT;
+		pvt->dcsm_shift_bit = REV_F10_DCSM_SHIFT;
+		pvt->num_dcsm = REV_F10_DCSM_COUNT;
+}
+
+/*
+ * map_to_dcs_mask
+ *
+ *	Map from a CSROW entry to the mask entry that operates on it
+ */
+static inline u32 map_to_dcs_mask(struct amd64_pvt *pvt, int csrow)
+{
+	return csrow >> pvt->dcsm_shift_bit;
+}
+
+/*
+ * get_dcsBase()
+ *
+ *	getter function to return the 'base' address the i'th CS entry
+ *	of the 'dct' DRAM controller
+ */
+static u32 get_dcsBase(struct amd64_pvt *pvt, int dct, int csrow)
+{
+	if (dct == 0)
+		return pvt->dcsBase0[csrow];
+	else
+		return pvt->dcsBase1[csrow];
+}
+
+/*
+ * get_dcsMask()
+ *
+ *	getter function to return the 'mask' address the i'th CS entry.
+ *	This getter function is needed because there different number
+ *	of DCSM registers on Rev E and prior vs Rev F and later
+ */
+static u32 get_dcsMask(struct amd64_pvt *pvt, int dct, int csrow)
+{
+	if (dct == 0)
+		return pvt->dcsMask0[map_to_dcs_mask(pvt,csrow)];
+	else
+		return pvt->dcsMask1[map_to_dcs_mask(pvt,csrow)];
+}
+
+/*
+ * base_from_dcsBase
+ *
+ *	Extract the DRAM CS base address from selected csrow register
+ */
+static u64 base_from_dcsBase(struct amd64_pvt *pvt, int csrow)
+{
+	return ((u64) (get_dcsBase(pvt, 0, csrow) & pvt->dcsb_mask)) <<
+				pvt->dcs_shift;
+}
+
+/*
+ * mask_from_dcsMask
+ *
+ *	Extract the Mask from the dcsBase0[csrow] entry
+ *	Depends on CPU Revision on how to extract this information
+ */
+static u64 mask_from_dcsMask(struct amd64_pvt *pvt, int csrow)
+{
+	u64 dcsm_bits, other_bits;
+	u64 mask;
+
+	/* Extract bits bits 29-21 and 15-9 from DCSM (section 3.5.5). */
+	dcsm_bits = get_dcsMask(pvt, 0, csrow) & pvt->dcsm_mask;
+
+	/* Set all bits except bits 33-25 and 19-13. */
+	other_bits = pvt->dcsm_mask;
+	other_bits = ~(other_bits << pvt->dcs_shift);
+
+	/* The extracted bits from DCSM belong in the spaces represented by
+	 * the cleared bits in other_bits.
+	 */
+	mask = (dcsm_bits << pvt->dcs_shift) | other_bits;
+
+	return mask;
+}
+
+/*
+ * k8_setup_dcsBase_dcsMask()
+ *
+ *	Setup the DCSB and DCSM arrays from hardware
+ */
+static void k8_setup_dcsBase_dcsMask(struct amd64_pvt *pvt)
+{
+	struct pci_dev *ctl = pvt->dram_ctl;
+	int cs;
+
+	/* Set the dcsb and dcsm mask bits and their shift value */
+	k8_set_dcsBase_dcsMask_rev_specific(pvt);
+
+	/* Retrieve the DRAM CS Base Address Registers from hardware */
+	for (cs = 0; cs < CHIPSELECT_COUNT; cs++) {
+		pci_read_config_dword(ctl, K8_DCSB0 + (cs * 4),
+					&pvt->dcsBase0[cs]);
+
+	}
+
+	/* The number of DCSMs differents at the Rev E/Rev F boundary
+	 * so we retrieve the number of registers defined for this processor
+	 */
+	for (cs = 0; cs < pvt->num_dcsm; cs++) {
+		pci_read_config_dword(ctl, K8_DCSM0 + (cs * 4),
+					&pvt->dcsMask0[cs]);
+	}
+
+	/* Debug dump the DCSB and DCSM registers */
+	for (cs = 0; cs < CHIPSELECT_COUNT; cs++) {
+		debugf1("  dcsBase0[%d]: 0x%8.8x  dcsMask0[%d]: 0x%x\n",
+			cs, get_dcsBase(pvt, 0, cs),
+			map_to_dcs_mask(pvt, cs), get_dcsMask(pvt, 0, cs));
+	}
+}
+
+/*
+ * f10_setup_dcsBase_dcsMask
+ *
+ *	F10 setup DCSB and DCSM
+ */
+static void f10_setup_dcsBase_dcsMask(struct amd64_pvt *pvt)
+{
+	struct pci_dev *ctl = pvt->dram_ctl;
+	int cs;
+
+	/* Set the dcsb and dcsm mask bits and their shift value */
+	f10_set_dcsBase_dcsMask_rev_specific(pvt);
+
+	/* Retrieve the DRAM CS Base Address Registers from hardware */
+	for (cs = 0; cs < CHIPSELECT_COUNT; cs++) {
+		pci_read_config_dword(ctl, K8_DCSB0 + (cs * 4),
+					&pvt->dcsBase0[cs]);
+		pci_read_config_dword(ctl, K8_DCSB1 + (cs * 4),
+					&pvt->dcsBase1[cs]);
+	}
+
+	/* The number of DCSMs differents at the Rev E/Rev F boundary
+	 * so we retrieve the number of registers defined for this processor
+	 */
+	for (cs = 0; cs < pvt->num_dcsm; cs++) {
+		pci_read_config_dword(ctl, K8_DCSM0 + (cs * 4),
+					&pvt->dcsMask0[cs]);
+		pci_read_config_dword(ctl, K8_DCSM1 + (cs * 4),
+					&pvt->dcsMask1[cs]);
+	}
+
+	/* Debug dump the DCSB and DCSM registers */
+	for (cs = 0; cs < CHIPSELECT_COUNT; cs++) {
+		debugf1("  dcsBase0[%d]: 0x%8.8x  dcsMask0[%d]: 0x%x\n",
+			cs, get_dcsBase(pvt, 0, cs),
+			map_to_dcs_mask(pvt, cs), get_dcsMask(pvt, 0, cs));
+		debugf1("  dcsBase1[%d]: 0x%8.8x  dcsMask1[%d]: 0x%x\n",
+			cs, get_dcsBase(pvt, 1, cs),
+			map_to_dcs_mask(pvt, cs), get_dcsMask(pvt, 1, cs));
+	}
+}
+
+/*
+ * get_base_and_limit()
+ *
+ * In *base and *limit, pass back the full 40-bit base and limit physical
+ * addresses for the node given by node_id.  This information is obtained from
+ * DRAM Base (section 3.4.4.1) and DRAM Limit (section 3.4.4.2) registers. The
+ * base and limit addresses are of type SysAddr, as defined at the start of
+ * section 3.4.4 (p. 70).  They are the lowest and highest physical addresses
+ * in the address range they represent.
+ */
+static void get_base_and_limit(struct amd64_pvt *pvt, int node_id,
+			       u64 * base, u64 * limit)
+{
+	*base = pvt->dram_base[node_id];
+	*limit = pvt->dram_limit[node_id];
+}
+
+/* Return 1 if the SysAddr given by sys_addr matches the base/limit associated
+ * with node_id
+ */
+static int base_limit_match(struct amd64_pvt *pvt, u64 sys_addr, int node_id)
+{
+	u64 base, limit, addr;
+
+	get_base_and_limit(pvt, node_id, &base, &limit);
+
+	/* The K8 treats this as a 40-bit value.  However, bits 63-40 will be
+	 * all ones if the most significant implemented address bit is 1.
+	 * Here we discard bits 63-40.  See section 3.4.2 of AMD publication
+	 * 24592: AMD x86-64 Architecture Programmer's Manual Volume 1
+	 * Application Programming.
+	 */
+	addr = sys_addr & 0x000000ffffffffffull;
+
+	return (addr >= base) && (addr <= limit);
+}
+
+/* find_mc_by_sys_addr
+ *
+ *	Attempt to map a SysAddr to a node.
+ *
+ *	On success, return a pointer to the mem_ctl_info structure for
+ *	the node that the SysAddr maps to.
+ *
+ *	On failure, return NULL
+ */
+static struct mem_ctl_info *find_mc_by_sys_addr(struct mem_ctl_info *mci,
+						u64 sys_addr)
+{
+	struct amd64_pvt *pvt;
+	int node_id;
+	u32 intlv_en, bits;
+
+	/* Here we use the DRAM Base (section 3.4.4.1) and DRAM Limit (section
+	 * 3.4.4.2) registers to map the SysAddr to a node ID.
+	 */
+
+	pvt = mci->pvt_info;
+
+	/* The value of this field should be the same for all DRAM Base
+	 * registers.  Therefore we arbitrarily choose to read it from the
+	 * register for node 0.
+	 */
+	intlv_en = pvt->dram_IntlvEn[0];
+
+	if (intlv_en == 0) {	/* node interleaving is disabled */
+		debugf2("%s(): node interleaving disabled\n", __func__);
+		for (node_id = 0; ; ) {
+			if (base_limit_match(pvt, sys_addr, node_id))
+				break;
+
+			if (++node_id >= DRAM_REG_COUNT) {
+				debugf2("%s(): sys_addr 0x%lx "
+					"does not match any node\n", __func__,
+					(unsigned long)sys_addr);
+				return NULL;
+			}
+		}
+
+		goto found;
+	}
+
+	if (unlikely((intlv_en != (0x01 << 8)) &&
+		     (intlv_en != (0x03 << 8)) && (intlv_en != (0x07 << 8)))) {
+		amd64_printk(KERN_WARNING,
+			  "%s(): junk value of 0x%x extracted from IntlvEn "
+			  "field of DRAM Base Register for node 0: This "
+			  "probably indicates a BIOS bug.\n", __func__,
+			  intlv_en);
+		return NULL;
+	}
+
+	/* If we get this far, node interleaving is enabled. */
+	debugf2("%s(): node interleaving enabled\n", __func__);
+	bits = (((u32) sys_addr) >> 12) & intlv_en;
+
+	for (node_id = 0; ; ) {
+		if ((pvt->dram_limit[node_id] & intlv_en) == bits)
+			break;	/* intlv_sel field matches */
+
+		if (++node_id >= DRAM_REG_COUNT) {
+			debugf2("%s(): sys_addr 0x%lx does not match any "
+				"node\n", __func__, (unsigned long)sys_addr);
+			return NULL;
+		}
+	}
+
+	/* sanity test for sys_addr */
+	if (unlikely(!base_limit_match(pvt, sys_addr, node_id))) {
+		amd64_printk(KERN_WARNING,
+			  "%s(): sys_addr 0x%lx falls outside base/limit "
+			  "address range for node %d with node interleaving "
+			  "enabled.\n", __func__, (unsigned long)sys_addr,
+			  node_id);
+		return NULL;
+	}
+
+found:
+	debugf2("%s(): sys_addr 0x%lx matches node %d\n", __func__,
+		(unsigned long)sys_addr, node_id);
+
+	return edac_mc_find(node_id);
+}
+
+/* Return the base value defined by the DRAM Base register for the node
+ * represented by mci.  This function returns the full 40-bit value despite
+ * the fact that the register only stores bits 39-24 of the value.  See
+ * section 3.4.4.1.
+ */
+static inline u64 get_dram_base(struct mem_ctl_info *mci)
+{
+	struct amd64_pvt *pvt = mci->pvt_info;
+
+	return pvt->dram_base[pvt->mc_node_id];
+}
+
+/* Obtain info from the DRAM Hole Address Register (section 3.4.8) for the
+ * node represented by mci.  Info is passed back in *hole_base, *hole_offset,
+ * and *hole_size.  Function returns 0 if info is valid or 1 if info is
+ * invalid.  Info may be invalid for either of the following reasons:
+ *
+ *     - The revision of the node is not E or greater.  In this case, the DRAM
+ *       Hole Address Register does not exist.
+ *     - The DramHoleValid bit is cleared in the DRAM Hole Address Register,
+ *       indicating that its contents are not valid.
+ *
+ * The values passed back in *hole_base, *hole_offset, and *hole_size are
+ * complete 32-bit values despite the fact that the bitfields in the DHAR
+ * only represent bits 31-24 of the base and offset values.
+ */
+static int get_dram_hole_info(struct mem_ctl_info *mci, u64 * hole_base,
+			      u64 * hole_offset, u64 * hole_size)
+{
+	struct amd64_pvt *pvt;
+	u64 base;
+
+	pvt = mci->pvt_info;
+
+	/*
+	 * Only Rev E and later have the DRAM Hole Address Register
+	 */
+	if (pvt->ext_model < OPTERON_CPU_REV_E) {
+		debugf1("  revision %d for node %d does not support DHAR\n",
+			pvt->ext_model, pvt->mc_node_id);
+		return 1;
+	}
+
+	/*
+	 * If the DRAM Hole Address Register is OFF, return that status
+	 */
+	if ((pvt->dhar & K8_DHAR_VALID) == 0) {
+		debugf1("  DramHoleValid bit cleared in DHAR for node %d\n",
+			pvt->mc_node_id);
+		return 1;	/* DramHoleValid bit is cleared */
+	}
+
+	/* +------------------+--------------------+--------------------+-----
+	 * | memory           | DRAM hole          | relocated          |
+	 * | [0, (x - 1)]     | [x, 0xffffffff]    | addresses from     |
+	 * |                  |                    | DRAM hole          |
+	 * |                  |                    | [0x100000000,      |
+	 * |                  |                    |  (0x100000000+     |
+	 * |                  |                    |   (0xffffffff-x))] |
+	 * +------------------+--------------------+--------------------+-----
+	 *
+	 * Above is a diagram of physical memory showing the DRAM hole and the
+	 * relocated addresses from the DRAM hole.  As shown, the DRAM hole
+	 * starts at address x (the base address) and extends through address
+	 * 0xffffffff.  The DRAM Hole Address Register (DHAR) relocates the
+	 * addresses in the hole so that they start at 0x100000000.
+	 */
+
+	base = pvt->dhar & K8_DHAR_BASE_MASK;
+
+	*hole_base = base;
+	*hole_offset = (pvt->dhar & K8_DHAR_OFFSET_MASK) << 16;
+	*hole_size = (0x1ull << 32) - base;
+
+	debugf1("  DHAR info for node %d base 0x%lx offset 0x%lx size 0x%lx\n",
+		pvt->mc_node_id, (unsigned long)*hole_base,
+		(unsigned long)*hole_offset, (unsigned long)*hole_size);
+
+	return 0;
+}
+
+/* Return the DramAddr that the SysAddr given by sys_addr maps to.  It is
+ * assumed that sys_addr maps to the node given by mci.
+ */
+static u64 sys_addr_to_dram_addr(struct mem_ctl_info *mci, u64 sys_addr)
+{
+	u64 dram_base, hole_base, hole_offset, hole_size, dram_addr;
+
+	/* The first part of section 3.4.4 (p. 70) shows how the DRAM Base
+	 * (section 3.4.4.1) and DRAM Limit (section 3.4.4.2) registers are
+	 * used to translate a SysAddr to a DramAddr.  If the DRAM Hole
+	 * Address Register (DHAR) is enabled, then it is also involved in
+	 * translating a SysAddr to a DramAddr.  Sections 3.4.8 and 3.5.8.2
+	 * describe the DHAR and how it is used for memory hoisting.  These
+	 * parts of the documentation are unclear.  I interpret them as
+	 * follows:
+	 *
+	 *     When node n receives a SysAddr, it processes the SysAddr as
+	 *     follows:
+	 *
+	 *         1.  It extracts the DRAMBase and DRAMLimit values from the
+	 *             DRAM Base and DRAM Limit registers for node n.  If the
+	 *             SysAddr is not within the range specified by the base
+	 *             and limit values, then node n ignores the Sysaddr
+	 *             (since it does not map to node n).  Otherwise continue
+	 *             to step 2 below.
+	 *
+	 *         2.  If the DramHoleValid bit of the DHAR for node n is
+	 *             clear, the DHAR is disabled so skip to step 3 below.
+	 *             Otherwise see if the SysAddr is within the range of
+	 *             relocated addresses (starting at 0x100000000) from the
+	 *             DRAM hole.  If not, skip to step 3 below.  Else get the
+	 *             value of the DramHoleOffset field from the DHAR.  To
+	 *             obtain the DramAddr, subtract the offset defined by
+	 *             this value from the SysAddr.
+	 *
+	 *         3.  Obtain the base address for node n from the DRAMBase
+	 *             field of the DRAM Base register for node n.  To obtain
+	 *             the DramAddr, subtract the base address from the
+	 *             SysAddr, as shown near the start of section 3.4.4
+	 *             (p. 70).
+	 */
+
+	dram_base = get_dram_base(mci);
+
+	if (!get_dram_hole_info(mci, &hole_base, &hole_offset, &hole_size)) {
+		if ((sys_addr >= (1ull << 32)) &&
+		    (sys_addr < ((1ull << 32) + hole_size))) {
+			/* use DHAR to translate SysAddr to DramAddr */
+			dram_addr = sys_addr - hole_offset;
+			debugf2("using DHAR to translate SysAddr 0x%lx to "
+				"DramAddr 0x%lx\n",
+				(unsigned long)sys_addr,
+				(unsigned long)dram_addr);
+			return dram_addr;
+		}
+	}
+
+	/* Translate the SysAddr to a DramAddr as shown near the start of
+	 * section 3.4.4 (p. 70).  Although sys_addr is a 64-bit value, the k8
+	 * only deals with 40-bit values.  Therefore we discard bits 63-40 of
+	 * sys_addr below.  If bit 39 of sys_addr is 1 then the bits we
+	 * discard are all 1s.  Otherwise the bits we discard are all 0s.  See
+	 * section 3.4.2 of AMD publication 24592: AMD x86-64 Architecture
+	 * Programmer's Manual Volume 1 Application Programming.
+	 */
+	dram_addr = (sys_addr & 0xffffffffffull) - dram_base;
+
+	debugf2("using DRAM Base register to translate SysAddr 0x%lx to "
+		"DramAddr 0x%lx\n", (unsigned long)sys_addr,
+		(unsigned long)dram_addr);
+	return dram_addr;
+}
+
+/* Parameter intlv_en is the value of the IntlvEn field from a DRAM Base
+ * register (section 3.4.4.1).  Return the number of bits from a SysAddr that
+ * are used for node interleaving.
+ */
+static int num_node_interleave_bits(unsigned intlv_en)
+{
+	static const int intlv_shift_table[] = { 0, 1, 0, 2, 0, 0, 0, 3 };
+	int n;
+
+	BUG_ON(intlv_en > 7);
+	n = intlv_shift_table[intlv_en];
+	debugf2("using %d bits for node interleave\n", n);
+	return n;
+}
+
+/* Translate the DramAddr given by dram_addr to an InputAddr and return the
+ * result.
+ */
+static u64 dram_addr_to_input_addr(struct mem_ctl_info *mci, u64 dram_addr)
+{
+	struct amd64_pvt *pvt;
+	int intlv_shift;
+	u64 input_addr;
+
+	pvt = mci->pvt_info;
+
+	/* Near the start of section 3.4.4 (p. 70), the k8 documentation gives
+	 * instructions for translating a DramAddr to an InputAddr.  Here we
+	 * are following these instructions.
+	 */
+	intlv_shift = num_node_interleave_bits(pvt->dram_IntlvEn[0]);
+	input_addr = ((dram_addr >> intlv_shift) & 0xffffff000ull) +
+	    (dram_addr & 0xfff);
+
+	debugf2("DramAddr 0x%lx translates to InputAddr 0x%lx\n",
+		(unsigned long)dram_addr, (unsigned long)input_addr);
+	return input_addr;
+}
+
+/* Translate the SysAddr represented by sys_addr to an InputAddr and return
+ * the result.  It is assumed that sys_addr maps to the node given by mci.
+ */
+static u64 sys_addr_to_input_addr(struct mem_ctl_info *mci, u64 sys_addr)
+{
+	u64 input_addr;
+
+	input_addr =
+	    dram_addr_to_input_addr(mci, sys_addr_to_dram_addr(mci, sys_addr));
+	debugf2("%s(): SysAdddr 0x%lx translates to InputAddr 0x%lx\n",
+		__func__, (unsigned long)sys_addr, (unsigned long)input_addr);
+	return input_addr;
+}
+
+/* input_addr is an InputAddr associated with the node given by mci.  Return
+ * the csrow that input_addr maps to, or -1 on failure (no csrow claims
+ * input_addr).
+ */
+static int input_addr_to_csrow(struct mem_ctl_info *mci, u64 input_addr)
+{
+	struct amd64_pvt *pvt;
+	int csrow;
+	u64 base, mask;
+
+	pvt = mci->pvt_info;
+
+	/* Here we use the DRAM CS Base (section 3.5.4) and DRAM CS Mask
+	 * (section 3.5.5) registers.  For each CS base/mask register pair,
+	 * test the condition shown near the start of section 3.5.4 (p. 84).
+	 */
+
+	for (csrow = 0; csrow < CHIPSELECT_COUNT; csrow++) {
+
+		if ((pvt->dcsBase0[csrow] & K8_DCSB_CS_ENABLE) == 0) {
+			debugf2("input_addr_to_csrow: CSBE bit is cleared "
+				"for csrow %d (node %d)\n",
+				csrow, pvt->mc_node_id);
+			continue;	/* CSBE bit is cleared, no csrow */
+		}
+
+		/* Get the base addr and the mask values for this csrow */
+		base = base_from_dcsBase(pvt, csrow);
+		mask = ~mask_from_dcsMask(pvt, csrow);
+
+		if ((input_addr & mask) == (base & mask)) {
+			debugf2("InputAddr 0x%lx matches csrow %d "
+				"(MC node %d)\n",
+				(unsigned long)input_addr,
+				csrow, pvt->mc_node_id);
+			return csrow;	/* success: csrow matches */
+		}
+	}
+
+	debugf2("no matching csrow for InputAddr 0x%lx (MC node %d)\n",
+		(unsigned long)input_addr, pvt->mc_node_id);
+	return -1;		/* failed to find matching csrow */
+}
+
+/* input_addr is an InputAddr associated with the node represented by mci.
+ * Translate input_addr to a DramAddr and return the result.
+ */
+static u64 input_addr_to_dram_addr(struct mem_ctl_info *mci, u64 input_addr)
+{
+	struct amd64_pvt *pvt;
+	int node_id, intlv_shift;
+	u64 bits, dram_addr;
+	u32 intlv_sel;
+
+	/* Near the start of section 3.4.4 (p. 70), the k8 documentation shows
+	 * how to translate a DramAddr to an InputAddr.  Here we reverse this
+	 * procedure.  When translating from a DramAddr to an InputAddr, the
+	 * bits used for node interleaving are discarded.  Here we recover
+	 * these bits from the IntlvSel field of the DRAM Limit register
+	 * (section 3.4.4.2) for the node that input_addr is associated with.
+	 */
+
+	pvt = mci->pvt_info;
+	node_id = pvt->mc_node_id;
+	BUG_ON((node_id < 0) || (node_id > 7));
+
+	intlv_shift = num_node_interleave_bits(pvt->dram_IntlvEn[0]);
+
+	if (intlv_shift == 0) {
+		debugf1("  node interleaving disabled:\n");
+		debugf1("    InputAddr 0x%lx translates "
+			"to DramAddr of same value\n",
+			(unsigned long)input_addr);
+		return input_addr;
+	}
+
+	bits = ((input_addr & 0xffffff000ull) << intlv_shift) +
+	    (input_addr & 0xfff);
+
+	intlv_sel = pvt->dram_IntlvSel[node_id] & ((1 << intlv_shift) - 1);
+	dram_addr = bits + (intlv_sel << 12);
+
+	debugf1("InputAddr 0x%lx translates to DramAddr 0x%lx "
+		"(%d node interleave bits)\n", (unsigned long)input_addr,
+		(unsigned long)dram_addr, intlv_shift);
+	return dram_addr;
+}
+
+/* dram_addr is a DramAddr that maps to the node represented by mci.  Convert
+ * dram_addr to a SysAddr and return the result.
+ */
+static u64 dram_addr_to_sys_addr(struct mem_ctl_info *mci, u64 dram_addr)
+{
+	struct amd64_pvt *pvt;
+	u64 hole_base, hole_offset, hole_size, base, limit, sys_addr;
+
+	pvt = mci->pvt_info;
+
+	if (!get_dram_hole_info(mci, &hole_base, &hole_offset, &hole_size)) {
+		if ((dram_addr >= hole_base) &&
+		    (dram_addr < (hole_base + hole_size))) {
+			/* use DHAR to translate DramAddr to SysAddr */
+			sys_addr = dram_addr + hole_offset;
+			debugf1("using DHAR to translate DramAddr 0x%lx to "
+				"SysAddr 0x%lx\n", (unsigned long)dram_addr,
+				(unsigned long)sys_addr);
+			return sys_addr;
+		}
+	}
+
+	get_base_and_limit(pvt, pvt->mc_node_id, &base, &limit);
+	sys_addr = dram_addr + base;
+
+	/* The sys_addr we have computed up to this point is a 40-bit value
+	 * because the k8 deals with 40-bit values.  However, the value we are
+	 * supposed to return is a full 64-bit physical address.  The AMD
+	 * x86-64 architecture specifies that the most significant implemented
+	 * address bit through bit 63 of a physical address must be either all
+	 * 0s or all 1s.  Therefore we sign-extend the 40-bit sys_addr to a
+	 * 64-bit value below.  See section 3.4.2 of AMD publication 24592:
+	 * AMD x86-64 Architecture Programmer's Manual Volume 1 Application
+	 * Programming.
+	 */
+	sys_addr |= ~((sys_addr & (1ull << 39)) - 1);
+
+	debugf1("  Using DRAM Base reg on node %d to translate\n",
+		pvt->mc_node_id);
+	debugf1("    DramAddr 0x%lx to SysAddr 0x%lx\n",
+		(unsigned long)dram_addr, (unsigned long)sys_addr);
+	return sys_addr;
+}
+
+/* input_addr is an InputAddr associated with the node given by mci.
+ * Translate input_addr to a SysAddr and return the result.
+ */
+static inline u64 input_addr_to_sys_addr(struct mem_ctl_info *mci,
+					 u64 input_addr)
+{
+	return dram_addr_to_sys_addr(mci,
+				     input_addr_to_dram_addr(mci, input_addr));
+}
+
+/* Find the minimum and maximum InputAddr values that map to the given csrow.
+ * Pass back these values in *input_addr_min and *input_addr_max.
+ */
+static void find_csrow_limits(struct mem_ctl_info *mci,
+			      int csrow,
+			      u64 * input_addr_min, u64 * input_addr_max)
+{
+	struct amd64_pvt *pvt;
+	u64 base, mask;
+
+	pvt = mci->pvt_info;
+	BUG_ON((csrow < 0) || (csrow >= CHIPSELECT_COUNT));
+
+	base = base_from_dcsBase(pvt, csrow);
+	mask = mask_from_dcsMask(pvt, csrow);
+
+	*input_addr_min = base & ~mask;
+	*input_addr_max = base | mask | pvt->dcs_mask_notused;
+}
+
+/*
+ * k8_get_error_address
+ *	extract from the hardware copies of the error register
+ *	the ERROR ADDRESS for the K8 and Family 0Fh CPUs
+ */
+static u64 k8_get_error_address(struct mem_ctl_info *mci,
+			struct amd64_error_info *info)
+{
+	return (((u64) (info->error_info.nbeah & 0xff)) << 32) +
+			(info->error_info.nbeal & ~0x03);
+}
+
+/*
+ * f10_get_error_address
+ *	extract from the hardware copies of the error register
+ *	the ERROR ADDRESS for Family 10h CPUs
+ */
+static u64 f10_get_error_address(struct mem_ctl_info *mci,
+			struct amd64_error_info *info)
+{
+	return (((u64) (info->error_info.nbeah & 0xffff)) << 32) +
+			(info->error_info.nbeal & ~0x01);
+}
+
+/*
+ * static u64 extract_error_address
+ *	Extract error address from MCA NB Address Low (section 3.6.4.5) and
+ *	MCA NB Address High (section 3.6.4.6) register values and return the
+ *	result. Address is located in the info structure (nbeah and nbeal)
+ *	the encoding is device specific.
+ */
+static u64 extract_error_address(struct mem_ctl_info *mci,
+			struct amd64_error_info *info)
+{
+	struct amd64_pvt *pvt = mci->pvt_info;
+
+	/* call the proper CPU type function to extract the address */
+	return pvt->ops->get_error_address(mci,info);
+}
+
+
+/*
+ * error_address_to_page_and_offset
+ *
+ *	Map the Error address to a PAGE and PAGE OFFSET
+ */
+static inline void error_address_to_page_and_offset(u64 error_address,
+						    u32 *page, u32 *offset)
+{
+	*page = (u32) (error_address >> PAGE_SHIFT);
+	*offset = ((u32) error_address) & ~PAGE_MASK;
+}
+
+/*
+ * amd64_error_info_valid
+ *
+ * Return 1 if hardware contains valid error information.
+ * Else return 0, no valid error information.
+ */
+static inline int amd64_error_info_valid(struct amd64_error_info_regs *regs)
+{
+	return ((regs->nbsh & K8_NBSH_VALID_BIT) != 0);
+}
+
+/*
+ * amd64_get_error_info_regs
+ *
+ *	read the North Bridge Status register HIGH and test the
+ *	VALID ERROR status bit in that register.
+ *
+ *	If set, proceed to read:
+ *		NBS Low
+ *		NBEA LOW
+ *		NBEA HIGH
+ *
+ *	and store data into error structure
+ *
+ * return 0: if hardware regs contains valid error info
+ * return 1: if no valid error is indicated
+ */
+static int amd64_get_error_info_regs(struct mem_ctl_info *mci,
+				  struct amd64_error_info_regs *regs)
+{
+	struct amd64_pvt *pvt;
+	struct pci_dev *ctl;
+
+	pvt = mci->pvt_info;
+	ctl = pvt->misc_ctl;
+
+	/* Reg the NB Status Register High to get ERROR VALID bit */
+	pci_read_config_dword(ctl, K8_NBSH, &regs->nbsh);
+
+	if (!amd64_error_info_valid(regs))
+		return 1;
+
+	/* valid error, read remaining error information registers */
+	pci_read_config_dword(ctl, K8_NBSL, &regs->nbsl);
+	pci_read_config_dword(ctl, K8_NBEAL, &regs->nbeal);
+	pci_read_config_dword(ctl, K8_NBEAH, &regs->nbeah);
+	pci_read_config_dword(ctl, K8_NBCFG, &regs->nbcfg);
+
+	return 0;
+}
+
+/*
+ * amd64_get_error_info
+ *
+ *	this function is called to retrieve the error data from hardware
+ *	and store it in the info structure.
+ */
+static void amd64_get_error_info(struct mem_ctl_info *mci,
+			      struct amd64_error_info *info)
+{
+	struct amd64_pvt *pvt;
+	struct amd64_error_info_regs regs;
+
+	pvt = mci->pvt_info;
+	info->race_condition_detected = 0;
+
+	if (amd64_get_error_info_regs(mci, &info->error_info))
+		return;
+
+	/*
+	 * Here's the problem with the K8's EDAC reporting:
+	 * There are four registers which report pieces of error
+	 * information.  These four registers are shared between
+	 * CEs and UEs.  Furthermore, contrary to what is stated in
+	 * the OBKG, the overflow bit is never used!  Every error
+	 * always updates the reporting registers.
+	 *
+	 * Can you see the race condition?  All four error reporting
+	 * registers must be read before a new error updates them!
+	 * There is no way to read all four registers atomically.  The
+	 * best than can be done is to detect that a race has occured
+	 * and then report the error without any kind of precision.
+	 *
+	 * What is still positive is that errors are
+	 * still reported and thus problems can still be detected -
+	 * just not localized because the syndrome and address are
+	 * spread out across registers.
+	 *
+	 * Grrrrr!!!!!  Here's hoping that AMD fixes this in some
+	 * future K8 rev. UEs and CEs should have separate
+	 * register sets with proper overflow bits that are used!
+	 * At very least the problem can be fixed by honoring the
+	 * ErrValid bit in 'nbsh' and not updating registers - just
+	 * set the overflow bit - unless the current error is CE
+	 * and the new error is UE which would be the only situation
+	 * for overwriting the current values.
+	 */
+
+	regs = info->error_info;
+
+	/* Use info from the second read - most current */
+	if (unlikely(amd64_get_error_info_regs(mci, &info->error_info)))
+		return;
+
+	/* clear the error */
+	pci_write_bits32(pvt->misc_ctl, K8_NBSH, 0, K8_NBSH_VALID_BIT);
+
+	/* Check for the possible race condition */
+	info->race_condition_detected =
+	    ((regs.nbsh != info->error_info.nbsh) ||
+	     (regs.nbsl != info->error_info.nbsl) ||
+	     (regs.nbeah != info->error_info.nbeah) ||
+	     (regs.nbeal != info->error_info.nbeal));
+}
+
+/*
+ * amd64_decode_gart_tlb_error
+ *
+ *	decode GART TBL Error code
+ */
+static inline void amd64_decode_gart_tlb_error(struct mem_ctl_info *mci,
+					 struct amd64_error_info *info)
+{
+	u32 err_code;
+	u32 ec_tt;		/* error code transaction type (2b) */
+	u32 ec_ll;		/* error code cache level (2b) */
+
+	err_code = EXTRACT_ERROR_CODE(info->error_info.nbsl);
+	ec_ll = EXTRACT_LL_CODE(err_code);
+	ec_tt = EXTRACT_TT_CODE(err_code);
+
+	amd64_mc_printk(mci, KERN_ERR,
+		     "GART TLB event: transaction type(%s), "
+		     "cache level(%s)\n", tt_msgs[ec_tt], ll_msgs[ec_ll]);
+}
+
+/*
+ * amd64_decode_mem_cache_error
+ *
+ *	decode Memory/CACHE Error code
+ */
+static inline void amd64_decode_mem_cache_error(struct mem_ctl_info *mci,
+				      struct amd64_error_info *info)
+{
+	u32 err_code;
+	u32 ec_rrrr;		/* error code memory transaction (4b) */
+	u32 ec_tt;		/* error code transaction type (2b) */
+	u32 ec_ll;		/* error code cache level (2b) */
+
+	err_code = EXTRACT_ERROR_CODE(info->error_info.nbsl);
+	ec_ll = EXTRACT_LL_CODE(err_code);
+	ec_tt = EXTRACT_TT_CODE(err_code);
+	ec_rrrr = EXTRACT_RRRR_CODE(err_code);
+
+	amd64_mc_printk(mci, KERN_ERR,
+		     "cache heirarchy error: memory transaction type(%s), "
+		     "transaction type(%s), cache level(%s)\n",
+		     rrrr_msgs[ec_rrrr], tt_msgs[ec_tt], ll_msgs[ec_ll]);
+}
+
+/* sys_addr is an error address (a SysAddr) extracted from the MCA NB Address
+ * Low (section 3.6.4.5) and MCA NB Address High (section 3.6.4.6) registers
+ * of a node that detected an ECC memory error.  mci represents the node that
+ * the error address maps to (possibly different from the node that detected
+ * the error).  Return the number of the csrow that sys_addr maps to, or -1 on
+ * error.
+ */
+static int sys_addr_to_csrow(struct mem_ctl_info *mci, u64 sys_addr)
+{
+	int csrow;
+
+	csrow = input_addr_to_csrow(mci, sys_addr_to_input_addr(mci, sys_addr));
+
+	if (csrow == -1)
+		amd64_mc_printk(mci, KERN_ERR,
+			     "Failed to translate InputAddr to csrow for "
+			     "address 0x%lx\n", (unsigned long)sys_addr);
+	return csrow;
+}
+
+
+/* Hack for the time being - Can we get this from BIOS?? */
+#define	CH0SPARE_RANK	0
+#define	CH1SPARE_RANK	1
+
+/*
+ * f10_read_online_spare_reg
+ *
+ *	read the ON-LINE SPARE Register for the specified MC
+ */
+static inline u32 f10_read_online_spare_reg(struct amd64_pvt *pvt)
+{
+	u32 spare;
+
+	pci_read_config_dword(pvt->misc_ctl, F10_ONLINE_SPARE, &spare);
+	return spare;
+}
+
+
+/*
+ * f10_scan_csrows
+ *
+ *	Iterate over the DRAM DCT "base" and "mask" register looking for
+ *	a SystemAddr match on the specified 'ChannelSelect' and 'NodeID'
+ *
+ * Return:
+ *	-1  NOT FOUND
+ *	0..csrow = Chip-Select Row
+ */
+static int f10_scan_node_csrows(u32 InputAddr, u32 NodeID, u32 ChannelSelect)
+{
+	struct mem_ctl_info *mci;
+	struct amd64_pvt *pvt;
+	u32 CSBase, CSMask, CSEn;
+	u32 OnLineSpareCTL;
+	u32 *dcsBase;
+	u32 *dcsMask;
+	u32 SwapDone;
+	u32 BadDramCs;
+	int CSFound = -1;	/* Assume NOT FOUND */
+	int Mask_idx;
+	int csrow;
+
+	/* get pointer to NodeID's structure, if NULL, we are done */
+	mci = mci_lookup[NodeID];
+	if (!mci)
+		return CSFound;
+
+	pvt = mci->pvt_info;
+
+	/* Which Channel is being examined */
+	if (ChannelSelect) {
+		dcsBase = pvt->dcsBase0;
+		dcsMask = pvt->dcsMask0;
+	} else {
+		dcsBase = pvt->dcsBase1;
+		dcsMask = pvt->dcsMask1;
+	}
+
+	/* Iterate over the DRAM DCTs looking for an addr match */
+	for (csrow = 0; csrow < CHIPSELECT_COUNT; csrow++) {
+
+		Mask_idx = csrow >> 1;
+
+		/* Retrieve the CS Base and Mask from 'NodeID' */
+		CSBase = dcsBase[csrow];
+		CSEn = CSBase & 0x00000001;
+		CSBase &= 0x1FF83FE0;
+
+		CSMask = dcsMask[Mask_idx];
+		CSMask = (CSMask | 0x0007C01F);
+		CSMask &= 0x1FFFFFFF;
+
+		if (CSEn && ((InputAddr & ~CSMask) == (CSBase & ~CSMask))) {
+			/* NOTE which CS to return */
+			CSFound = csrow;
+
+			/* Read online spare reg */
+			OnLineSpareCTL = f10_read_online_spare_reg(pvt);
+
+			if (ChannelSelect) {
+				SwapDone = (OnLineSpareCTL >> 3) & 0x00000001;
+				BadDramCs = (OnLineSpareCTL >> 8) & 0x00000007;
+				if (SwapDone && (csrow == BadDramCs))
+					csrow = CH1SPARE_RANK;
+			} else {
+				SwapDone = (OnLineSpareCTL >> 1) & 0x00000001;
+				BadDramCs = (OnLineSpareCTL >> 4) & 0x00000007;
+				if (SwapDone && (csrow == BadDramCs))
+					csrow = CH0SPARE_RANK;
+			}
+			break;
+		} /* if CSEn */
+	} /* for each CS */
+
+	return CSFound;
+}
+
+
+/*
+ * f10_translate_sysaddr_to_CS
+ *
+ *	Code straight from the AMD's BKDG, section 2.8.5
+ *	with interface changes
+ */
+static int f10_translate_sysaddr_to_CS(struct mem_ctl_info *mci,
+			u64 SystemAddr,
+			int *node_id,
+			int *channel_select)
+{
+	int CSFound, NodeID, Ilog;
+	int HiRangeSelected, DramRange;
+	u32 IntlvEn, IntlvSel;
+	u32 DramEn;
+	u32 HoleOffset, HoleEn;
+	u32 InputAddr, Temp;
+	u32 DctSelBaseAddr, DctSelIntLvAddr, DctGangEn, DctSelIntLvEn;
+	u32 DctSelHiRngEn, DctSelHi;
+	u32 ChannelSelect = 0;
+	u64 DramBaseLong, DramLimitLong;
+	u64 DctSelBaseOffsetLong, ChannelOffsetLong, ChannelAddrLong;
+
+	struct amd64_pvt *pvt = mci->pvt_info;
+
+	CSFound = -1;
+
+	/* Iterate over each of the possible DRAM reg entries on this MC */
+	for (DramRange = 0; DramRange < DRAM_REG_COUNT; DramRange++) {
+
+		/* Get the DRAM Base value for this DRAM instance:
+		 * To extract DRAM Base Low, DramEnable and Interleave
+		 */
+		DramBaseLong = pvt->dram_base[DramRange];
+		DramEn = pvt->dram_rw_en[DramRange];
+		IntlvEn = pvt->dram_IntlvEn[DramRange];
+
+		/* Get the DRAM Limit value for this DRAM instance */
+		DramLimitLong = pvt->dram_limit[DramRange];
+		NodeID = pvt->dram_DstNode[DramRange];
+		IntlvSel = pvt->dram_IntlvSel[DramRange];
+
+		/* This assume that one node's DHAR is the same as
+		 * all the other node's DHARs
+		 */
+		HoleEn = pvt->dhar;
+		HoleOffset = (HoleEn & 0x0000FF80);
+		HoleEn = (HoleEn & 0x00000003);
+
+		/* If DRAM is enabled AND the SystemAddr falls within START
+		 * and END of this DRAM segment, then do further processing
+		 */
+		if (DramEn && DramBaseLong <= SystemAddr &&
+						SystemAddr <= DramLimitLong) {
+			if (IntlvEn == 0 ||
+				IntlvSel == ((SystemAddr >> 12) & IntlvEn)) {
+				if (IntlvEn == 1)
+					Ilog = 1;
+				else
+					Ilog = 0;
+
+				/* Fetch the F10 DRAM Controller Select Low Reg */
+				Temp = pvt->dram_ctl_select_low;
+
+				DctSelHiRngEn = Temp & 1;
+				DctSelHi = (Temp >> 1) & 1;
+				DctSelIntLvEn = Temp & 4;
+				DctGangEn = Temp & 0x10;
+				DctSelIntLvAddr = (Temp >> 6) & 3;
+				DctSelBaseAddr = Temp & 0xFFFFF800;
+
+				DctSelBaseOffsetLong =
+						pvt->dram_ctl_select_high << 16;
+
+				/* Determrine if High Range is selected */
+				if (DctSelHiRngEn && (DctGangEn == 0) &&
+					((SystemAddr >> 27) >= (DctSelBaseAddr >> 11)))
+					HiRangeSelected = 1;
+				else
+					HiRangeSelected = 0;
+
+				/* Determine Channel */
+				if (DctGangEn)
+					ChannelSelect = 0;
+				else if (HiRangeSelected)
+					ChannelSelect = DctSelHi;
+				else if (DctSelIntLvEn && DctSelIntLvAddr == 0)
+					ChannelSelect = SystemAddr >> 6 & 1;
+				else if (DctSelIntLvEn &&
+					(DctSelIntLvAddr >> 1) & 1) {
+					Temp ^= SystemAddr >> 16 & 0x1F;
+					if (DctSelIntLvAddr & 1)
+						ChannelSelect = (SystemAddr >> 9 & 1) ^ Temp;
+					else
+						ChannelSelect = (SystemAddr >> 6 & 1) ^ Temp;
+				} else if (DctSelIntLvEn && IntlvEn & 4)
+					ChannelSelect = SystemAddr >> 15 & 1;
+				else if (DctSelIntLvEn && IntlvEn & 2)
+					ChannelSelect = SystemAddr >> 14 & 1;
+				else if (DctSelIntLvEn && IntlvEn & 1)
+					ChannelSelect = SystemAddr >> 13 & 1;
+				else if (DctSelIntLvEn)
+					ChannelSelect = SystemAddr >> 12 & 1;
+				else if (DctSelHiRngEn && DctGangEn == 0)
+					ChannelSelect = ~DctSelHi & 1;
+				else
+					ChannelSelect = 0;
+
+				/* Determine Base Address Offset to use */
+				if (HiRangeSelected) {
+					if ((!DctSelBaseAddr & 0xFFFF0000) &&
+					   (HoleEn & 1) &&
+					   (SystemAddr >= 0x100000000ULL))
+						ChannelOffsetLong = HoleOffset <<16;
+					else
+						ChannelOffsetLong = DctSelBaseOffsetLong;
+				} else {
+					if ((HoleEn & 1) &&
+						(SystemAddr >= 0x100000000ULL))
+						ChannelOffsetLong = HoleOffset << 16;
+					else
+						ChannelOffsetLong = DramBaseLong & 0xFFFFF8000000ULL;
+				}
+
+				/* Remove hoisting offset and normalize to
+				 * DRAM bus addresses
+				 */
+				ChannelAddrLong = (SystemAddr & 0x0000FFFFFFFFFFC0ULL) -
+						(ChannelOffsetLong & 0x0000FFFFFF800000ULL);
+
+				/* Remove Node ID (in case of Processor
+				 * interleaving
+				 */
+				Temp = ChannelAddrLong & 0xFC0;
+				ChannelAddrLong = (ChannelAddrLong >> Ilog &
+					0xFFFFFFFFF000ULL) | Temp;
+
+				/* Remove Channel interleave and hash */
+				if (DctSelIntLvEn && (DctSelHiRngEn == 0) && (DctGangEn == 0)) {
+					if (DctSelIntLvAddr != 1)
+						ChannelAddrLong =
+							(ChannelAddrLong >> 1) &
+							0xFFFFFFFFFFFFFFC0ULL;
+					else {
+						Temp = ChannelAddrLong & 0xFC0;
+						ChannelAddrLong =
+							((ChannelAddrLong &
+							0xFFFFFFFFFFFFC000ULL)
+							>> 1) | Temp;
+					}
+				}
+
+				/* Form a normalize InputAddr */
+				InputAddr = ChannelAddrLong >> 8;
+
+				/* Iterate over the DRAM DCTs looking for a
+				 * match for InputAddr on the selected NodeID
+				 */
+				CSFound = f10_scan_node_csrows(InputAddr,
+							NodeID, ChannelSelect);
+
+			} /* if Interleave */
+		} /* if DramEn... */
+
+		/* 'FOUND' Exit logic: return data */
+		if (CSFound >= 0) {
+			*node_id = NodeID;
+			*channel_select = ChannelSelect;
+			break;
+		}
+
+	} /* for each DramRange */
+
+	return CSFound;
+}
+
+/*
+ * f10_map_sysaddr_to_csrow
+ *
+ *	map a SystemAddress to NodeID, CSROW, Channel
+ */
+static void f10_map_sysaddr_to_csrow(struct mem_ctl_info *mci,
+					struct amd64_error_info *info,
+					u64 SystemAddress)
+{
+	u32 page, offset;
+	unsigned syndrome;
+	int channel_select = 0;
+	int node_id;
+	int csrow;
+
+	syndrome = EXTRACT_SYNDROME(info->error_info.nbsh);
+
+	csrow = f10_translate_sysaddr_to_CS(mci, SystemAddress, &node_id,
+					&channel_select);
+
+	/* If address is found, log and provide information
+	 * else if not found, log the event w/o info
+	 */
+	if (csrow >= 0) {
+		/* Make the CSROW to a PAGE and OFFSET */
+		error_address_to_page_and_offset(SystemAddress, &page, &offset);
+
+		/* Report the error */
+		edac_mc_handle_ce(mci, page, offset, syndrome, csrow,
+				  channel_select, EDAC_MOD_STR);
+	} else {
+		/* Report the error, but without any info */
+		edac_mc_handle_ce_no_info(mci, EDAC_MOD_STR);
+	}
+}
+
+/*
+ * k8_map_sysaddr_to_csrow
+ *
+ *	map a SystemAddress to NodeID, CSROW, Channel
+ */
+static void k8_map_sysaddr_to_csrow(struct mem_ctl_info *mci,
+					struct amd64_error_info *info,
+					u64 SystemAddress)
+{
+	struct mem_ctl_info *src_mci;
+	unsigned syndrome;
+	int channel, csrow;
+	u32 page, offset;
+
+	syndrome = EXTRACT_SYNDROME(info->error_info.nbsh);
+
+	/* CHIPKILL enabled */
+	if (info->error_info.nbcfg & K8_NBCFG_CHIPKILL) {
+		/* chipkill ecc mode */
+		syndrome += (info->error_info.nbsl >> 16) & 0xff00;
+		channel = chan_from_chipkill_syndrome(syndrome);
+
+		if (channel < 0) {
+			/* If the syndrome couldn't be found then the race
+			 * condition for error reporting registers likely
+			 * occurred.  There's alot more in doubt than just the
+			 * channel.  Might as well just log the error without
+			 * any info.
+			 */
+			amd64_mc_printk(mci, KERN_WARNING,
+				     "unknown syndrome 0x%x - possible error "
+				     "reporting race\n", syndrome);
+			edac_mc_handle_ce_no_info(mci, EDAC_MOD_STR);
+			return;
+		}
+	} else {
+		/* non-chipkill ecc mode
+		 *
+		 * The k8 documentation is unclear about how to determine the
+		 * channel number when using non-chipkill memory.  This method
+		 * was obtained from email communication with someone at AMD.
+		 */
+		channel = ((SystemAddress & BIT(3)) != 0);
+	}
+
+	/* Find out which node the error address belongs to.  This may be
+	 * different from the node that detected the error.
+	 */
+	src_mci = find_mc_by_sys_addr(mci, SystemAddress);
+	if (src_mci == NULL) {
+		amd64_mc_printk(mci, KERN_ERR,
+			     "failed to map error address 0x%lx to a node\n",
+			     (unsigned long)SystemAddress);
+		edac_mc_handle_ce_no_info(mci, EDAC_MOD_STR);
+		return;
+	}
+
+	/* Now map the SystemAddress to a CSROW */
+	csrow = sys_addr_to_csrow(src_mci, SystemAddress);
+	if (csrow < 0) {
+		/* Report the error, but without any info */
+		edac_mc_handle_ce_no_info(src_mci, EDAC_MOD_STR);
+	} else {
+		/* Make the CSROW to a PAGE and OFFSET */
+		error_address_to_page_and_offset(SystemAddress, &page, &offset);
+
+		/* Report the error */
+		edac_mc_handle_ce(src_mci, page, offset, syndrome, csrow,
+				  channel, EDAC_MOD_STR);
+	}
+}
+
+/*
+ * amd64_handle_ce
+ *
+ *	this routine is called to handle any Correctable Errors (CEs)
+ *	that have occurred. Check for valid ERROR ADDRESS and process
+ */
+static void amd64_handle_ce(struct mem_ctl_info *mci,
+				struct amd64_error_info *info)
+{
+	struct amd64_pvt *pvt = mci->pvt_info;
+	u64 SystemAddress;
+
+	/* Ensure that the Error Address is VALID */
+	if ((info->error_info.nbsh & K8_NBSH_VALID_ERROR_ADDR) == 0) {
+		amd64_mc_printk(mci, KERN_ERR,
+			"HW has no ERROR_ADDRESS available\n");
+		edac_mc_handle_ce_no_info(mci, EDAC_MOD_STR);
+		return;
+	}
+
+	/* Retrieve the error address and syndrome */
+	SystemAddress = extract_error_address(mci,info);
+
+	amd64_mc_printk(mci, KERN_ERR,
+		"CE ERROR_ADDRESS= 0x%lx\n", (long unsigned int) SystemAddress);
+
+	/* call the specific mapper */
+	pvt->ops->map_sysaddr_to_csrow(mci, info, SystemAddress);
+}
+
+/*
+ * amd64_handle_ue
+ *
+ *	this routine is called to handle any Un-correctable Errors (UEs)
+ */
+static void amd64_handle_ue(struct mem_ctl_info *mci,
+				struct amd64_error_info *info)
+{
+	int csrow;
+	u64 SystemAddress;
+	u32 page, offset;
+	struct mem_ctl_info *log_mci, *src_mci;
+
+	log_mci = mci;
+
+	/* Ensure that the Error Address is VALID */
+	if ((info->error_info.nbsh & K8_NBSH_VALID_ERROR_ADDR) == 0) {
+		amd64_mc_printk(mci, KERN_CRIT,
+			"HW has no ERROR_ADDRESS available\n");
+		edac_mc_handle_ue_no_info(log_mci, EDAC_MOD_STR);
+		return;
+	}
+
+	/* Retrive the error address */
+	SystemAddress = extract_error_address(mci,info);
+
+	/* Find out which node the error address belongs to.  This may be
+	 * different from the node that detected the error.
+	 */
+	src_mci = find_mc_by_sys_addr(mci, SystemAddress);
+	if (src_mci == NULL) {
+		amd64_mc_printk(mci, KERN_CRIT,
+			"ERROR ADDRESS (0x%lx) value NOT mapped to a MC\n",
+			(unsigned long)SystemAddress);
+		edac_mc_handle_ue_no_info(log_mci, EDAC_MOD_STR);
+		return;
+	}
+
+	log_mci = src_mci;
+
+	/* Map the SystemAddress to a 'csrow' */
+	csrow = sys_addr_to_csrow(log_mci, SystemAddress);
+	if (csrow < 0) {
+		amd64_mc_printk(mci, KERN_CRIT,
+			"ERROR_ADDRESS (0x%lx) value NOT mapped to 'csrow'\n",
+			(unsigned long)SystemAddress);
+		edac_mc_handle_ue_no_info(log_mci, EDAC_MOD_STR);
+	} else {
+		error_address_to_page_and_offset(SystemAddress, &page, &offset);
+		edac_mc_handle_ue(log_mci, page, offset, csrow, EDAC_MOD_STR);
+	}
+}
+
+/*
+ * amd64_decode_bus_error
+ *
+ *	Perform decode of the AMD64 "BUS" error code
+ */
+static void amd64_decode_bus_error(struct mem_ctl_info *mci,
+			     struct amd64_error_info *info)
+{
+	u32 err_code, ext_ec;
+	u32 ec_pp;		/* error code participating processor (2p) */
+	u32 ec_to;		/* error code timed out (1b) */
+	u32 ec_rrrr;		/* error code memory transaction (4b) */
+	u32 ec_ii;		/* error code memory or I/O (2b) */
+	u32 ec_ll;		/* error code cache level (2b) */
+
+	debugf0("MC%d: %s()\n", mci->mc_idx, __func__);
+
+	ext_ec = EXTRACT_EXT_ERROR_CODE(info->error_info.nbsl);
+	err_code = EXTRACT_ERROR_CODE(info->error_info.nbsl);
+
+	ec_ll = EXTRACT_LL_CODE(err_code);
+	ec_ii = EXTRACT_II_CODE(err_code);
+	ec_rrrr = EXTRACT_RRRR_CODE(err_code);
+	ec_to = EXTRACT_TO_CODE(err_code);
+	ec_pp = EXTRACT_PP_CODE(err_code);
+
+	/* Log AMD Specific information */
+	amd64_mc_printk(mci, KERN_ERR,
+		"BUS ERROR:\n"
+		"  time-out(%s) mem or i/o(%s)\n"
+		"  participating processor(%s)\n"
+		"  memory transaction type(%s)\n"
+		"  cache level(%s) Error Found by: %s\n",
+		to_msgs[ec_to],
+		ii_msgs[ec_ii],
+		pp_msgs[ec_pp],
+		rrrr_msgs[ec_rrrr],
+		ll_msgs[ec_ll],
+		(info->error_info.nbsh & K8_NBSH_ERR_SCRUBER) ?
+			"Scrubber" : "Normal Operation" );
+
+	/* If this was an 'observed' error, early out */
+	if (ec_pp == K8_NBSL_PP_OBS)
+		return;		/* We aren't the node involved */
+
+	/* Parse out the extended error code for ECC events */
+	switch (ext_ec) {
+	/* F10 changed to one Extended ECC error code */
+	case F10_NBSL_EXT_ERR_RES:		/* Reserved field */
+	case F10_NBSL_EXT_ERR_ECC:		/* F10 ECC ext err code */
+
+	/* K8 ext error codes, ECC and CHIPKILL ECC - uses same value as F10
+	case K8_NBSL_EXT_ERR_ECC:
+	case K8_NBSL_EXT_ERR_CHIPKILL_ECC:
+	*/
+		break;
+
+	default:
+		amd64_mc_printk(mci, KERN_ERR,
+			"NOT ECC: no special error handling for this error\n");
+		return;
+	}
+
+	/* CE or UE event? */
+	if (info->error_info.nbsh & K8_NBSH_CECC)
+		amd64_handle_ce(mci, info);
+	else if (info->error_info.nbsh & K8_NBSH_UECC)
+		amd64_handle_ue(mci, info);
+
+	/* If main error is CE then overflow must be CE.  If main error is UE
+	 * then overflow is unknown.  We'll call the overflow a CE - if
+	 * panic_on_ue is set then we're already panic'ed and won't arrive
+	 * here.  If panic_on_ue is not set then apparently someone doesn't
+	 * think that UE's are catastrophic.
+	 */
+	if (info->error_info.nbsh & K8_NBSH_OVERFLOW)
+		edac_mc_handle_ce_no_info(mci,
+			EDAC_MOD_STR " Error Overflow set");
+}
+
+/*
+ * amd64_process_error_info
+ *
+ *	process the error information of the AMD64 chipset
+ *
+ *	return:
+ *		1 if error found or
+ *		0 if error not found
+ */
+static int amd64_process_error_info(struct mem_ctl_info *mci,
+					struct amd64_error_info *info,
+					int handle_errors)
+{
+	struct amd64_pvt *pvt;
+	struct amd64_error_info_regs *regs;
+	u32 err_code, ext_ec;
+	int gart_tlb_error = 0;
+
+	pvt = mci->pvt_info;
+
+	/* check for a valid error state, if none, return */
+	if (!amd64_error_info_valid(&info->error_info))
+		return 0;
+
+	/* If caller doesn't want us to process the error, return */
+	if (!handle_errors)
+		return 1;
+
+	/* Check if there was a race condition that occurred during harvest */
+	if (info->race_condition_detected)
+		amd64_mc_printk(mci, KERN_WARNING,
+			"hardware read access race condition detected!\n");
+
+	regs = &info->error_info;
+
+	debugf1("NorthBridge ERROR: mci(0x%p) MC node(%d) "
+		"ErrAddr(0x%.8x-%.8x) nbsh(0x%.8x) nbsl(0x%.8x)\n",
+		mci, pvt->mc_node_id,
+		regs->nbeah, regs->nbeal, regs->nbsh, regs->nbsl);
+
+	/* Extract the error code from the hw register value */
+	err_code = EXTRACT_ERROR_CODE(regs->nbsl);
+
+	/* Determine which error type:
+	 *	1) GART errors - non-fatal, developmental events
+	 *	2) MEMORY errors
+	 *	3) BUS errors
+	 *	4) Unknown error
+	 */
+	if (TEST_TLB_ERROR(err_code)) {
+		/*
+		 * GART errors are intended to help graphics driver
+		 * developers to detect bad GART PTEs. It is recommended by
+		 * AMD to disable GART table walk error reporting by default[1]
+		 * (currently being disabled in mce_cpu_quirks()) and according
+		 * to the comment in mce_cpu_quirks(), such GART errors can be
+		 * incorrectly triggered. We may see these errors anyway and
+		 * unless requested by the user, they won't be reported.
+		 *
+		 * [1] section 13.10.1 on BIOS and Kernel Developers Guide for
+		 *     AMD NPT family 0Fh processors
+		 */
+		if (report_gart_errors == 0)
+			return 1;
+
+		/* Only if GART error reporting are requested should
+		 * we generate any logs. If not requested, we left above.
+		 */
+ 		gart_tlb_error = 1;
+
+		debugf1("GART TLB error\n");
+		amd64_decode_gart_tlb_error(mci, info);
+	} else if(TEST_MEM_ERROR(err_code)) {
+		debugf1("Memory/Cache error\n");
+		amd64_decode_mem_cache_error(mci, info);
+	} else if (TEST_BUS_ERROR(err_code)) {
+		debugf1("Bus (Link/DRAM) error\n");
+		amd64_decode_bus_error(mci, info);
+	} else {
+		/* shouldn't reach here! */
+		amd64_mc_printk(mci, KERN_WARNING,
+			     "%s(): unknown MCE error 0x%x\n", __func__,
+			     err_code);
+	}
+
+	/* fetch and log the extended error code */
+	ext_ec = EXTRACT_EXT_ERROR_CODE(regs->nbsl);
+	amd64_mc_printk(mci, KERN_ERR,
+		"Extended Error code: (0x%x) %s\n", ext_ec, ext_msgs[ext_ec]);
+
+	/* Range Check the extended error code for HyperTransport error */
+	if (		((ext_ec >= F10_NBSL_EXT_ERR_CRC &&
+			ext_ec <= F10_NBSL_EXT_ERR_TGT) ||
+			(ext_ec == F10_NBSL_EXT_ERR_RMW)) &&
+			EXTRACT_LDT_LINK(info->error_info.nbsh)) {
+
+		/* Log a message on HT error */
+		amd64_mc_printk(mci, KERN_ERR,
+			"Error on hypertransport link: %s\n",
+			htlink_msgs[
+			EXTRACT_LDT_LINK(info->error_info.nbsh)]);
+	}
+
+	/* Check the UE bit of the NB status high register, if set
+	 * Generate some logs.
+	 * If NOT a GART error, then process the event as a NO-INFO event.
+	 * If it was a GART error, skip that process.
+	 */
+	if (regs->nbsh & K8_NBSH_UNCORRECTED_ERR) {
+		amd64_mc_printk(mci, KERN_CRIT, "uncorrected error\n");
+		if (!gart_tlb_error)
+			edac_mc_handle_ue_no_info(mci, "UE bit is set\n");
+	}
+
+	/* Check for Processor Context Corrupt: Just note it */
+	if (regs->nbsh & K8_NBSH_PCC)
+		amd64_mc_printk(mci, KERN_CRIT,
+			"PCC (processor context corrupt) set\n");
+
+	return 1;
+}
+
+/*
+ * amd64_check
+ *
+ * The main polling 'check' function, called to perform the error checking
+ * and if an error is there, error processing:
+ */
+static void amd64_check(struct mem_ctl_info *mci)
+{
+	struct amd64_error_info info;
+
+	debugf3("%s()\n", __func__);
+	amd64_get_error_info(mci, &info);
+	amd64_process_error_info(mci, &info, 1);
+}
+
+/*
+ * reserve_mc_devices
+ *
+ *	reserve the various PCI devices composing this memory controller
+ *	which we need to retrieve data from in the processing of errors
+ */
+static int reserve_mc_devices(struct mem_ctl_info *mci, int mc_type_index)
+{
+	const struct amd64_family_type *amd64_dev =
+					&amd64_family_types[mc_type_index];
+	struct amd64_pvt *pvt;
+	struct pci_dev *pdev;
+
+	pdev = to_pci_dev(mci->dev);
+	pvt = mci->pvt_info;
+
+	/* save the pci device we use as an anchor in the private area */
+	pvt->dram_ctl = pdev;
+
+	/* Reserved the ADDRESS MAP Device:
+	 * The address mapping device provides a table that indicates which
+	 * physical address ranges are owned by which node.  Each node's
+	 * memory controller has memory controller addresses that begin at
+	 * 0x0.
+	 */
+	pvt->addr_map = pci_get_related_function(pdev->vendor,
+						amd64_dev->addr_map,
+						pdev);
+
+	if (pvt->addr_map == NULL) {
+		amd64_printk(KERN_ERR, "error address map device not found: "
+			  "vendor %x device 0x%x (broken BIOS?)\n",
+			  PCI_VENDOR_ID_AMD, amd64_dev->addr_map);
+		return 1;
+	}
+
+	debugf1("Addr Map device PCI Bus ID:\t%s\n", pci_name(pvt->addr_map));
+
+	/* Get the MISC Device */
+	pvt->misc_ctl = pci_get_related_function(pdev->vendor,
+						amd64_dev->misc_ctl,
+						pdev);
+
+	if (pvt->misc_ctl == NULL) {
+		/* Release the priviously reserved address map */
+		pci_dev_put(pvt->addr_map);
+		pvt->addr_map = NULL;
+		amd64_printk(KERN_ERR, "error miscellaneous device not found: "
+			  "vendor %x device 0x%x (broken BIOS?)\n",
+			  PCI_VENDOR_ID_AMD, amd64_dev->misc_ctl);
+		return 1;
+	}
+
+	debugf1("Misc device PCI Bus ID:\t%s\n", pci_name(pvt->misc_ctl));
+
+	return 0;
+}
+
+
+/*
+ * k8_read_dram_base_limit
+ *
+ *	Read the Base and Limit registers for K8 based Memory controllers
+ *	Extract fields from the 'raw' reg into separate data fields
+ *
+ *		Isolates: BASE, LIMIT, IntlvEn, IntlvSel, RW_EN
+ */
+static void k8_read_dram_base_limit(struct amd64_pvt *pvt, int dram)
+{
+	u32 low;
+	u32 off = dram << 3;
+
+	/* read the BASE register */
+	pci_read_config_dword(pvt->addr_map, K8_DRAM_BASE_LOW + off, &low);
+
+	/* Extract parts into separate data entries */
+	pvt->dram_base[dram] = ((u64) low & 0xFFFF0000) << 8;
+	pvt->dram_IntlvEn[dram] = (low >> 8) & 0x7;
+	pvt->dram_rw_en[dram] = (low & 0x3);
+
+	/* read the LIMIT register */
+	pci_read_config_dword(pvt->addr_map, K8_DRAM_LIMIT_LOW + off, &low);
+
+	/* Extract parts into separate data entries
+	 * Limit is the HIGHEST memory location of the region, so lower
+	 * 24-bit needs to be all ones
+	 */
+	pvt->dram_limit[dram] = (((u64) low & 0xFFFF0000) << 8) | 0x00FFFFFF;
+	pvt->dram_IntlvSel[dram] = (low >> 8) & 0x7;
+	pvt->dram_DstNode[dram] = (low & 0x7);
+}
+
+/*
+ * f10_read_dram_base_limit
+ *
+ *	Read the Base and Limit registers for F10 based Memory controllers
+ *	Extract fields from the 'raw' reg into separate data fields
+ *
+ *		Isolates: BASE, LIMIT, IntlvEn, IntlvSel, RW_EN
+ */
+static void f10_read_dram_base_limit(struct amd64_pvt *pvt, int dram)
+{
+	u32 low;
+	u32 high;
+	u32 off = dram << 3;
+
+	/* read the 'raw' BASE register */
+	pci_read_config_dword(pvt->addr_map, K8_DRAM_BASE_LOW + off, &low);
+	pci_read_config_dword(pvt->addr_map, F10_DRAM_BASE_HIGH + off, &high);
+
+	/* Extract parts into separate data entries */
+	pvt->dram_base[dram] = (((u64) high & 0x0000FFFF) << 40) |
+				(((u64) low & 0xFFFF0000) << 8);
+	pvt->dram_IntlvEn[dram] = (low >> 8) & 0x7;
+	pvt->dram_rw_en[dram] = (low & 0x3);
+
+	/* read the 'raw' LIMIT register */
+	pci_read_config_dword(pvt->addr_map, K8_DRAM_LIMIT_LOW + off, &low);
+	pci_read_config_dword(pvt->addr_map, F10_DRAM_LIMIT_HIGH + off, &high);
+
+	/* Extract parts into separate data entries
+	 * Limit is the HIGHEST memory location of the region, so lower
+	 * 24-bit needs to be all ones
+	 */
+	pvt->dram_limit[dram] = (((u64) high & 0x0000FFFF) << 40) |
+				(((u64) low & 0xFFFF0000) << 8) |
+				(0x00FFFFFF);
+	pvt->dram_IntlvSel[dram] = (low >> 8) & 0x7;
+	pvt->dram_DstNode[dram] = (low & 0x7);
+}
+
+/*
+ * f10_read_misc_registers
+ *	Read misc registers for the F10 that are NOT in the K8 series
+ */
+static void f10_read_misc_registers(struct amd64_pvt *pvt)
+{
+	pci_read_config_dword(pvt->dram_ctl, F10_DRAM_CTL_SELECT_LOW,
+					&pvt->dram_ctl_select_low);
+	pci_read_config_dword(pvt->dram_ctl, F10_DRAM_CTL_SELECT_HIGH,
+					&pvt->dram_ctl_select_high);
+}
+
+/*
+ * amd64_read_mc_registers
+ *
+ *	Retrieve the hardware registers of the memory controller
+ *	(this includes the 'Address Map' and 'Misc' device regs)
+ *	and cache that data in the private data area for this instance
+ */
+static void amd64_read_mc_registers(struct mem_ctl_info *mci)
+{
+	struct amd64_pvt *pvt = mci->pvt_info;
+	int dram;
+
+	debugf1("%s(MC node-id=%d): (ExtModel=%d) %s\n",
+		__func__, pvt->mc_node_id, pvt->ext_model,
+		(pvt->ext_model >= OPTERON_CPU_REV_F) ?
+		"Rev F or later" : "Rev E or earlier");
+
+	/* Retrieve the DRAM Base i'th and Limit i'th Registers */
+	for (dram = 0; dram < DRAM_REG_COUNT; dram++) {
+
+		/* call CPU specific READ function to get the DRAM Base and
+		 * Limit values from the Memory Controller
+		 */
+		pvt->ops->read_dram_base_limit(pvt, dram);
+
+		/* Only print out debug info on rows with both R and W
+		 * Enabled.  Normal processing, compiler should optimize
+		 * this whole debug output block away
+		 */
+		if (pvt->dram_rw_en[dram] != 0) {
+			debugf1("  DRAM_BASE[%d]: 0x%8.08x-%8.08x "
+				"DRAM_LIMIT:  0x%8.08x-%8.08x\n",
+				dram,
+				(u32)(pvt->dram_base[dram] >> 32),
+				(u32)(pvt->dram_base[dram] & 0xFFFFFFFF),
+				(u32)(pvt->dram_limit[dram] >> 32),
+				(u32)(pvt->dram_limit[dram] & 0xFFFFFFFF));
+			debugf1("   IntlvEn = %d %s %s "
+				"IntlvSel= %d DstNode= %d\n",
+				pvt->dram_IntlvEn[dram],
+				(pvt->dram_rw_en[dram] & 0x2) ? "W" : "!W",
+				(pvt->dram_rw_en[dram] & 0x1) ? "R" : "!R",
+				pvt->dram_IntlvSel[dram],
+				pvt->dram_DstNode[dram]);
+		}
+	}
+
+	/* Setup the DCSB and DCSM arrays from hardware */
+	pvt->ops->setup_dcsBase_dcsMask(pvt);
+
+	/* Read the DRAM Hole Address Register */
+	pci_read_config_dword(pvt->addr_map, K8_DHAR, &pvt->dhar);
+
+	/* Read in the DRAM Base Address Mapping and the
+	 * North Bridge Capabilities registers
+	 */
+	pci_read_config_dword(pvt->dram_ctl, K8_DBAM, &pvt->dbam);
+	pci_read_config_dword(pvt->misc_ctl, K8_NBCAP, &pvt->nbcap);
+
+	/* Read NEWER registers if available */
+	pvt->ops->read_misc_registers(pvt);
+
+	debugf1("  dhar: 0x%8.08x\n", pvt->dhar);
+	debugf1("  dbam: 0x%8.08x\n", pvt->dbam);
+	debugf1("  dcl:  0x%8.08x\n", pvt->dcl);
+	debugf1("  nbcap:0x%8.08x\n", pvt->nbcap);
+}
+
+/*
+ * dual_channel_active
+ *
+ *	NOTE: CPU Revision Dependent code
+ *
+ *	the DCL - DRAM Configuration Low Register contains various
+ *	configuration bits on memory.
+ *
+ *	BUT it is different between CG, D & E revs and the later
+ *	Rev F memory controllers (DDR vs DDR2)
+ *
+ * Return:
+ *	1 if dual channel mode is active.
+ *	0 if single channel mode is active.
+ */
+static int dual_channel_active(u32 dcl, int mc_device_index)
+{
+	int flag;
+	int ext_model = node_rev(mc_device_index);
+
+	if (ext_model >= OPTERON_CPU_REV_F) {
+		/* Rev F (NPT) and later */
+		flag = (dcl >> 11) & 0x1;
+	} else {
+		/* Rev E and earlier */
+		flag = (dcl >> 16) & 0x1;
+	}
+
+	return flag;
+}
+
+/*
+ * csrow_nr_pages
+ *
+ *	NOTE: CPU Revision Dependent code
+ *
+ *	Input:
+ *		csrow_nr  ChipSelect Row Number (0..CHIPSELECT_COUNT-1)
+ *		k8 private pointer to -->
+ *			DRAM Bank Address mapping register
+ *			node_id
+ *			DCL register where dual_channel_active is
+ *
+ *	The DBAM register consists of 4 sets of 4 bits each definitions:
+ *
+ *	Bits:		CSROWs
+ *	0-3		CSROWs 0 and 1
+ *	4-7		CSROWs 2 and 3
+ *	8-11		CSROWs 4 and 5
+ *	12-15		CSROWs 6 and 7
+ *
+ * 	Values range from: 0 to 15
+ *	The meanings of the values depends on CPU REV and dual-channel state
+ *
+ *	Various CPU revisions have different size definitions for this
+ *	4 bit values.
+ *	Therefore, we need to examine which CPU we are running on to
+ *	extract meaning for these bits:
+ *
+ *	REV CG and earlier have a given DIMM size definition
+ *	REV D & REV E share another set of size definitions
+ *
+ *	REV CG, D and E all have a common beginning set of size definitions,
+ *	but change the meanings midway into the table.
+ *
+ *	REV F has yet another set of size definitions, which has a totally
+ *	different starting point.
+ *
+ *	REV ? future revs? We won't know till we get the specs
+ *
+ *	The memory controller provides for total of only 8 CSROWs in its
+ *	current architecture. Each "pair" of CSROWs normally represents
+ *	just one (1) *	DIMM in single channel or just two (2) DIMMs
+ *	in dual channel mode.
+ *
+ *	The following code logic collapses the various tables for CSROW
+ *	based on CPU Rev number.
+ *	See the tables/algorithms in the respective BKDG manuals.
+ *
+ *	return:
+ *		The number of PAGE_SIZE pages on the specified CSROW number
+ *		This number incompasses
+ *
+ */
+static u32 csrow_nr_pages(int csrow_nr, struct amd64_pvt *pvt)
+{
+	u32 shift;
+	u32 nr_pages;
+	int ext_model = pvt->ext_model;
+
+	/* The math on this doesn't look right on the surface because x/2*4
+	 * can be simplified to x*2 but this expression makes use of the fact
+	 * that it is integral math where 1/2=0. This intermediate value
+	 * becomes the number of bits to shift the DBAM register to extract
+	 * the proper CSROW field.
+	 */
+	shift = (pvt->dbam >> ((csrow_nr / 2) * 4)) & 0xF;	/* PG88 */
+
+	debugf0("  %s(csrow=%d) DBAM index= %d\n", __func__, csrow_nr, shift);
+
+	/* First step is to calc the number of bits to shift a value of 1
+	 * left to indicate show many pages. Start with the DBAM value
+	 * as the starting bits, then proceed to adjust those shift
+	 * bits, based on CPU REV and the table. See BKDG on the DBAM
+	 */
+	if (ext_model >= OPTERON_CPU_REV_F) {
+		/* 27 shift, is 128Mib minimum DIMM size in REV F and later
+		 * upto 8 Gb, in a step function progression
+		 */
+		static u32 rev_f_shift[] = { 27, 28, 29, 29, 29, 30, 30, 31,
+			31, 32, 32, 33, 0, 0, 0, 0
+		};
+
+		/* REV F and greater section */
+		nr_pages = 1 << (rev_f_shift[shift] - PAGE_SHIFT);
+
+	} else {
+		/* REV E and less section This line is tricky.
+		 * It collapses the table used by revision D and later to one
+		 * that matches revision CG and earlier
+		 */
+		shift -= (ext_model >= OPTERON_CPU_REV_D) ?
+		    (shift > 8 ? 4 : (shift > 5 ? 3 : (shift > 2 ? 1 : 0))) : 0;
+
+		/* 25 shift, is 32MiB minimum DIMM size in REV E and prior
+		 */
+		nr_pages = 1 << (shift + 25 - PAGE_SHIFT);
+	}
+
+	/* If dual channel then double the memory size of single channel */
+	nr_pages <<= dual_channel_active(pvt->dcl, pvt->mc_node_id);
+
+	debugf0("    nr_pages= %u  dual_channel_active = %d\n",
+		nr_pages, dual_channel_active(pvt->dcl, pvt->mc_node_id));
+
+	return nr_pages;
+}
+
+/*
+ * determine_parity_enabled
+ *
+ *	NOTE: CPU Revision Dependent code
+ *
+ *	determine if Parity is Enabled
+ */
+static int determine_parity_enabled(struct amd64_pvt *pvt)
+{
+	int rc = 0;
+
+	if (pvt->ext_model >= OPTERON_CPU_REV_F) {
+		if (pvt->dcl & BIT(8))
+			rc = 1;
+	}
+
+	return rc;
+}
+
+/*
+ * determine_memory_type
+ *
+ *	NOTE: CPU Revision Dependent code
+ *
+ *	determine the memory type in operation on this controller
+ */
+static enum mem_type determine_memory_type(struct amd64_pvt *pvt)
+{
+	enum mem_type type;
+
+	if (pvt->ext_model >= OPTERON_CPU_REV_F) {
+		/* Rev F and later */
+		type = ((pvt->dcl >> 16) & 0x1) ? MEM_DDR2 : MEM_RDDR2;
+	} else {
+		/* Rev E and earlier */
+		type = ((pvt->dcl >> 18) & 0x1) ? MEM_DDR : MEM_RDDR;
+	}
+
+	debugf1("  Memory type is: %s\n",
+		(type == MEM_DDR2) ? "MEM_DDR2" :
+		(type == MEM_RDDR2) ? "MEM_RDDR2" :
+		(type == MEM_DDR) ? "MEM_DDR" : "MEM_RDDR");
+
+	return type;
+}
+
+/*
+ * determine_dram_type
+ *
+ *	NOTE: CPU Revision Dependent code
+ *
+ *	determine the DRAM type in operation
+ *	There are CHIPSELECT_COUNT  (8) and 2 CSROWS per DIMM, therefore
+ *	there are 4 Logical DIMMs possible, thus 4 bits in the
+ *	configuration register indicating whether there are
+ *	X4 or X8 devices, one per logical DIMM
+ */
+static enum dev_type determine_dram_type(struct amd64_pvt *pvt, int row)
+{
+	int bit;
+	enum dev_type type;
+
+	/* the starting bit depends on Revision value */
+	bit = (pvt->ext_model >= OPTERON_CPU_REV_F) ? 12 : 20;
+	type = ((pvt->dcl >> (bit + (row / 2))) & 0x01) ? DEV_X4 : DEV_X8;
+
+	debugf1("  DRAM type is: %s\n", (type == DEV_X4) ? "DEV-x4" : "DEV-x8");
+
+	return type;
+}
+
+/*
+ * determine_edac_cap
+ *
+ *	NOTE: CPU Revision Dependent code
+ *
+ *	determine if the DIMMs have ECC enabled
+ *	ECC is enabled ONLY if all the DIMMs are ECC capable
+ */
+static enum edac_type determine_edac_cap(struct amd64_pvt *pvt)
+{
+	int bit;
+	enum dev_type edac_cap = EDAC_NONE;
+
+	bit = (pvt->ext_model >= OPTERON_CPU_REV_F) ? 19 : 17;
+	if ((pvt->dcl >> bit) & 0x1) {
+		debugf1("  edac_type is: EDAC_FLAG_SECDED\n");
+		edac_cap = EDAC_FLAG_SECDED;
+	}
+
+	return edac_cap;
+}
+
+/*
+ * amd64_init_csrows
+ *
+ *	perform initialization on the array of csrow attribute instances
+ */
+static int amd64_init_csrows(struct mem_ctl_info *mci)
+{
+	struct csrow_info *csrow;
+	struct amd64_pvt *pvt;
+	int i;
+	int empty = 1;		/* start out assume the rows are empty */
+	u64 input_addr_min, input_addr_max, sys_addr;
+	u32 nbcfg;
+
+	pvt = mci->pvt_info;
+
+	/* read the North Bridge Configuration reg */
+	pci_read_config_dword(pvt->misc_ctl, K8_NBCFG, &nbcfg);
+
+	/* Iterate over the rows, looking for a valid set */
+	for (i = 0; i < CHIPSELECT_COUNT; i++) {
+		csrow = &mci->csrows[i];
+
+		/* Check the 'CS Enable' bit of this instance */
+		if ((pvt->dcsBase0[i] & K8_DCSB_CS_ENABLE) == 0) {
+			debugf1("csrow %d empty for node %d\n", i,
+				pvt->mc_node_id);
+			continue;	/* empty */
+		}
+
+		debugf1("csrow %d valid for MC node %d\n", i, pvt->mc_node_id);
+
+		empty = 0;
+		csrow->nr_pages = csrow_nr_pages(i, pvt);
+		find_csrow_limits(mci, i, &input_addr_min, &input_addr_max);
+		sys_addr = input_addr_to_sys_addr(mci, input_addr_min);
+		csrow->first_page = (u32) (sys_addr >> PAGE_SHIFT);
+		sys_addr = input_addr_to_sys_addr(mci, input_addr_max);
+		csrow->last_page = (u32) (sys_addr >> PAGE_SHIFT);
+		csrow->page_mask = ~mask_from_dcsMask(pvt, i);
+		csrow->grain = 8;	/* 8 bytes of resolution */
+
+		csrow->mtype = determine_memory_type(pvt);
+		csrow->dtype = determine_dram_type(pvt, i);
+
+		debugf1("  for MC node %d csrow %d:\n", pvt->mc_node_id, i);
+		debugf1("    input_addr_min: 0x%lx input_addr_max: 0x%lx\n",
+			(unsigned long)input_addr_min,
+			(unsigned long)input_addr_max);
+		debugf1("    sys_addr: 0x%lx  page_mask: 0x%lx\n",
+			(unsigned long)sys_addr, csrow->page_mask);
+		debugf1("    nr_pages: %u  first_page: 0x%lx "
+			"last_page: 0x%lx\n",
+			(unsigned)csrow->nr_pages,
+			csrow->first_page, csrow->last_page);
+
+		/* determine wheather:
+		 * 	CHIPKILL or JUST ECC or NO ECC is operating
+		 */
+		if (nbcfg & K8_NBCFG_ECC_ENABLE)
+			csrow->edac_mode =
+			    (nbcfg & K8_NBCFG_CHIPKILL) ?
+			    EDAC_S4ECD4ED : EDAC_SECDED;
+		else
+			csrow->edac_mode = EDAC_NONE;
+	}
+
+	return empty;
+}
+
+/*
+ * amd64_enable_ecc_error_reporting
+ *
+ *     enable the hardware to generated and detect error reporting
+ */
+static void amd64_enable_ecc_error_reporting(struct mem_ctl_info *mci)
+{
+	struct amd64_pvt *pvt = mci->pvt_info;
+	u32 mc4ctl_l = 0, mc4ctl_h = 0;
+
+	/* Enable the UE and CE detections */
+	pci_write_bits32(pvt->misc_ctl, K8_NBCTL,
+		K8_NBCTL_CECCEn | K8_NBCTL_UECCEn,
+		K8_NBCTL_CECCEn | K8_NBCTL_UECCEn);
+
+
+	/* Following is redundent */
+	do_rdmsr(pvt->mc_node_id, K8_MSR_MC4CTL, &mc4ctl_l, &mc4ctl_h);
+	mc4ctl_l |= K8_NBCTL_CECCEn | K8_NBCTL_UECCEn;
+	do_wrmsr(pvt->mc_node_id, K8_MSR_MC4CTL, mc4ctl_l, mc4ctl_h);
+	do_rdmsr(pvt->mc_node_id, K8_MSR_MC4CTL, &mc4ctl_l, &mc4ctl_h);
+}
+
+/*
+ * amd64_enable_nbe_error_reporting
+ *
+ *     enable the North Bridge Enable bank for hardware detect error reporting
+ */
+static void amd64_enable_nbe_error_reporting(struct mem_ctl_info *mci)
+{
+	struct amd64_pvt *pvt = mci->pvt_info;
+	u32 mcgctl_l = 0, mcgctl_h = 0;
+
+	/* Enable the North Bridge Enable (NBE) detection */
+	do_rdmsr(pvt->mc_node_id, K8_MSR_MCGCTL, &mcgctl_l, &mcgctl_h);
+	mcgctl_l |= K8_MSR_MCGCTL_NBE;
+	do_wrmsr(pvt->mc_node_id, K8_MSR_MCGCTL, mcgctl_l, mcgctl_h);
+	do_rdmsr(pvt->mc_node_id, K8_MSR_MCGCTL, &mcgctl_l, &mcgctl_h);
+}
+
+/*
+ * get_top_mem_regs
+ *
+ *	Retrive the 2 TOP of MEMORY values from the northbridge
+ *	1) Below 4 GB boundary
+ *	2) top of physical memory beyond 4 GB
+ */
+static void get_top_mem_regs(struct amd64_pvt *pvt)
+{
+	u32 low, high;
+
+	debugf0("%s()\n", __func__);
+	do_rdmsr(pvt->mc_node_id, K8_MSR_TOP_MEM, &low, &high);
+	pvt->top_mem = ((u64) high << 32) | low;
+	debugf0("  TOP_MEM=  0x%08x-%08x\n", high, low);
+
+	do_rdmsr(pvt->mc_node_id, K8_MSR_TOP_MEM2, &low, &high);
+	pvt->top_mem2 = ((u64) high << 32) | low;
+	debugf0("  TOP_MEM2= 0x%08x-%08x\n", high, low);
+}
+
+/* Forward reference */
+static int amd64_process_error_info(struct mem_ctl_info *mci,
+					struct amd64_error_info *info,
+					int handle_errors);
+
+/*
+ * amd64_nbea_store
+ *
+ * 	Accept a hex value and store it into the virutal error
+ *	register file, field:	nbeal and nbeah
+ *
+ *	Assume virtual error values have already been set for:
+ *		NBSL, NBSH and NBCFG
+ *
+ *	Then proceed to map the error values to a:
+ *		MC, CSROW and CHANNEL
+ */
+static ssize_t amd64_nbea_store(struct mem_ctl_info *mci,
+					const char *data, size_t count)
+{
+	struct amd64_pvt *pvt = mci->pvt_info;
+	u64 value;
+
+	if (isdigit(*data)) {
+		value = simple_strtoul(data, NULL, 16);
+
+		/* place the value into the virtual error packet */
+		pvt->ctl_error_info.error_info.nbeal = (u32) value;
+		pvt->ctl_error_info.error_info.nbeah = (u32) (value >> 32);
+		pvt->ctl_error_info.race_condition_detected = 0;
+
+		debugf0("%s() received NBEA= 0x%lx\n", __func__,
+			(unsigned long) value);
+
+		/* Process the Mapping request */
+		/* TODO: Add race preventation */
+		amd64_process_error_info(mci, &pvt->ctl_error_info, 1);
+
+		return count;
+	}
+	return 0;
+}
+
+/*
+ * amd64_nbea_show
+ *
+ *	display back what the last NBEA address was written
+ */
+static ssize_t amd64_nbea_show(struct mem_ctl_info *mci, char *data)
+{
+        struct amd64_pvt *pvt = mci->pvt_info;
+	u64 value;
+
+	value = pvt->ctl_error_info.error_info.nbeah;
+	value <<= 32;
+	value |= pvt->ctl_error_info.error_info.nbeal;
+
+        return sprintf(data, "%lx\n", (unsigned long) value);
+}
+
+/*
+ * amd64_nbsl_store
+ *
+ *	accept and store the NBSL value user desires
+ */
+static ssize_t amd64_nbsl_store(struct mem_ctl_info *mci,
+					const char *data, size_t count)
+{
+	struct amd64_pvt *pvt = mci->pvt_info;
+	u32 value;
+
+	if (isdigit(*data)) {
+		value = simple_strtoul(data, NULL, 16);
+
+		/* place the NBSL value into the virtual error packet */
+		pvt->ctl_error_info.error_info.nbsl = value;
+
+		debugf0("%s() received NBSL= 0x%x\n", __func__, value);
+
+		return count;
+	}
+	return 0;
+}
+
+/*
+ * amd64_nbsl_show
+ *
+ *	display back what the last NBSL value written
+ */
+static ssize_t amd64_nbsl_show(struct mem_ctl_info *mci, char *data)
+{
+        struct amd64_pvt *pvt = mci->pvt_info;
+	u32 value;
+
+	value = pvt->ctl_error_info.error_info.nbsl;
+
+        return sprintf(data, "%x\n", value);
+}
+
+/*
+ * amd64_nbsh_store
+ *
+ *	accept and store the NBSH value user desires
+ */
+static ssize_t amd64_nbsh_store(struct mem_ctl_info *mci,
+					const char *data, size_t count)
+{
+	struct amd64_pvt *pvt = mci->pvt_info;
+	u32 value;
+
+	if (isdigit(*data)) {
+		value = simple_strtoul(data, NULL, 16);
+
+		/* place the NBSL value into the virtual error packet */
+		pvt->ctl_error_info.error_info.nbsh = value;
+
+		debugf0("%s() received NBSL= 0x%x\n", __func__, value);
+
+		return count;
+	}
+	return 0;
+}
+
+/*
+ * amd64_nbsh_show
+ *
+ *	display back what the last NBSL value written
+ */
+static ssize_t amd64_nbsh_show(struct mem_ctl_info *mci, char *data)
+{
+        struct amd64_pvt *pvt = mci->pvt_info;
+	u32 value;
+
+	value = pvt->ctl_error_info.error_info.nbsh;
+
+        return sprintf(data, "%x\n", value);
+}
+
+/*
+ * amd64_nbcfg_store
+ *
+ *	accept and store the NBSL value user desires
+ */
+static ssize_t amd64_nbcfg_store(struct mem_ctl_info *mci,
+					const char *data, size_t count)
+{
+	struct amd64_pvt *pvt = mci->pvt_info;
+	u32 value;
+
+	if (isdigit(*data)) {
+		value = simple_strtoul(data, NULL, 16);
+
+		/* place the NBSL value into the virtual error packet */
+		pvt->ctl_error_info.error_info.nbcfg = value;
+
+		debugf0("%s() received NBCFG= 0x%x\n", __func__, value);
+
+		return count;
+	}
+	return 0;
+}
+
+/*
+ * amd64_nbcfg_show
+ *
+ *	display back what the last NBSL value written
+ */
+static ssize_t amd64_nbcfg_show(struct mem_ctl_info *mci, char *data)
+{
+        struct amd64_pvt *pvt = mci->pvt_info;
+	u32 value;
+
+	value = pvt->ctl_error_info.error_info.nbcfg;
+
+        return sprintf(data, "%x\n", value);
+}
+
+/*
+ * Per MC instance Attribute/Control data control structure
+ */
+static struct mcidev_sysfs_attribute amd64_mc_sysfs_ctls_attrs[] = {
+	{
+		.attr = {
+			.name = "ctl_nbea",
+			.mode = (S_IRUGO | S_IWUSR)
+		},
+		.show = amd64_nbea_show,
+		.store = amd64_nbea_store,
+	},
+
+	{
+		.attr = {
+			.name = "ctl_nbsl",
+			.mode = (S_IRUGO | S_IWUSR)
+		},
+		.show = amd64_nbsl_show,
+		.store = amd64_nbsl_store,
+	},
+
+	{
+		.attr = {
+			.name = "ctl_nbsh",
+			.mode = (S_IRUGO | S_IWUSR)
+		},
+		.show = amd64_nbsh_show,
+		.store = amd64_nbsh_store,
+	},
+
+	{
+		.attr = {
+			.name = "ctl_nbcfg",
+			.mode = (S_IRUGO | S_IWUSR)
+		},
+		.show = amd64_nbcfg_show,
+		.store = amd64_nbcfg_store,
+	},
+
+	/* End of List */
+	{
+		.attr = { .name = NULL}
+	}
+};
+
+/*
+ * amd64_set_mc_sysfs_attributes
+ */
+static void amd64_set_mc_sysfs_attributes(struct mem_ctl_info *mci)
+{
+	mci->mc_driver_sysfs_attributes = amd64_mc_sysfs_ctls_attrs;
+}
+
+/*
+ * amd64_probe_one_instance
+ *
+ *	probe function to determine if there is a DRAM Controller
+ *	device is there and to construct data tables
+ *	and read in data from those HW regs
+ */
+static int amd64_probe_one_instance(struct pci_dev *pdev, int mc_type_index)
+{
+	struct mem_ctl_info *mci;
+	struct amd64_pvt *pvt;
+	u32 dcl, dual_channel;
+	int parity_enable;
+	int node_id;
+
+	/* get the index of this MC */
+	node_id = get_mc_node_id_from_pdev(pdev);
+
+	debugf0("%s(mc_type_index=%d)\n", __func__, mc_type_index);
+	debugf0("DRAM MEM-CTL PCI Bus ID:       0000:%02u:%02x.%x\n",
+		pdev->bus->number,
+		PCI_SLOT(pdev->devfn), PCI_FUNC(pdev->devfn));
+
+	/* Determine CPU Rev of each CPU. On
+	 * Rev E and earlier we have to do the CPUID instruction.
+	 * Rev F and later we could just do a dword read from
+	 * Function 3 Offset FCh
+	 * TODO: Fix this function to do just that somehow
+	 */
+	build_node_revision_table();
+
+	/* We need to determine if we are single or dual channel operation
+	 * and use that information for calculating the size of the dynamic
+	 * instance tables in the 'mci' structure
+	 */
+	pci_read_config_dword(pdev, K8_DCL, &dcl);
+	dual_channel = dual_channel_active(dcl, node_id);
+
+	mci = edac_mc_alloc(sizeof(*pvt), CHIPSELECT_COUNT,
+				dual_channel + 1, node_id);
+	if (mci == NULL)
+		return -ENOMEM;
+
+	/* start filling in the mci data */
+	mci->dev = &pdev->dev;
+
+	/* fill in the private data as well */
+	pvt = mci->pvt_info;
+	pvt->dcl = dcl;
+	pvt->mc_node_id = node_id;
+	pvt->ext_model = node_rev(pvt->mc_node_id);
+
+	/* Reserved the other NB devices we use */
+	if (reserve_mc_devices(mci, mc_type_index))
+		goto fail0;
+
+	/* Capture the low level ops functions */
+	pvt->ops = &amd64_family_types[mc_type_index].ops;
+
+	/* Retrieve TOP_MEM and TOP_MEM2 */
+	get_top_mem_regs(pvt);
+
+	/* Retrieve the hardware registers, and cache them in the private area,
+	 * for this instance of memory controller
+	 */
+	amd64_read_mc_registers(mci);
+
+	/* Initialize state */
+	mci->mtype_cap = MEM_FLAG_DDR2 | MEM_FLAG_RDDR2;
+	mci->edac_ctl_cap = EDAC_FLAG_NONE;
+	mci->edac_cap = EDAC_FLAG_NONE;
+
+	/* Exam the capabilities of the northbridge */
+	if (pvt->nbcap & BIT(K8_NBCAP_SECDED))
+		mci->edac_ctl_cap |= EDAC_FLAG_SECDED;
+
+	if (pvt->nbcap & BIT(K8_NBCAP_CHIPKILL))
+		mci->edac_ctl_cap |= EDAC_FLAG_S4ECD4ED;
+
+	mci->edac_cap = determine_edac_cap(pvt);
+	if (mci->edac_cap & EDAC_FLAG_SECDED) {
+
+/*
+ *	NOT Correct:
+ *	TODO: Fix
+		if (dual_channel) {
+			debugf1("  DUAL-Channel: setting EDAC_FLAG_S4ECD4ED\n");
+			mci->edac_cap |= EDAC_FLAG_S4ECD4ED;
+		}
+ */
+	}
+
+	parity_enable = determine_parity_enabled(pvt);
+	debugf1("  Parity is %s\n", parity_enable ? "Enabled" : "Disabled");
+
+	mci->mod_name = EDAC_MOD_STR;
+	mci->mod_ver = EDAC_AMD64_VERSION;
+	mci->ctl_name = amd64_family_types[mc_type_index].ctl_name;
+	mci->dev_name = pci_name(pdev);
+	mci->edac_check = amd64_check;
+	mci->ctl_page_to_phys = NULL;
+
+	/* memory scrubber interface */
+	mci->set_sdram_scrub_rate = set_sdram_scrub_rate;
+	mci->get_sdram_scrub_rate = get_sdram_scrub_rate;
+
+	/* Now initialize control structures for all CSROWs present */
+	if (amd64_init_csrows(mci)) {
+		debugf1("Setting mci->edac_cap to EDAC_FLAG_NONE because\n");
+		debugf1("   amd64_init_csrows() returned NO csrows found\n");
+		mci->edac_cap = EDAC_FLAG_NONE;	/* no csrows found */
+	} else {
+		/* Enable the ECC reporting hardware */
+		amd64_enable_ecc_error_reporting(mci);
+		amd64_enable_nbe_error_reporting(mci);
+	}
+
+	/* Set SYSFS controls and attributes for this MC */
+	amd64_set_mc_sysfs_attributes(mci);
+
+	/* Finalize the registration with the EDAC Helper CORE */
+	if (edac_mc_add_mc(mci)) {
+		debugf1("%s(): failed edac_mc_add_mc()\n", __func__);
+		/* FIXME: perhaps some code should go here that disables error
+		 * reporting if we just enabled it
+		 */
+		goto fail1;
+	}
+
+	/* allocating ONE generic PCI control info */
+	if (!amd64_ctl_pci) {
+		amd64_ctl_pci = edac_pci_create_generic_ctl(&pdev->dev,
+						EDAC_MOD_STR);
+		if (!amd64_ctl_pci) {
+			printk(KERN_WARNING
+				"%s(): Unable to create PCI control\n",
+				__func__);
+			printk(KERN_WARNING
+				"%s(): PCI error report via EDAC not setup\n",
+				__func__);
+		}
+	}
+
+	/* Store the pointer to this control instance in our lookup table */
+	mci_lookup[node_id] = mci;
+
+	debugf1("%s(): success\n", __func__);
+	return 0;
+
+fail1:
+	pci_dev_put(pvt->addr_map);
+	pci_dev_put(pvt->misc_ctl);
+
+fail0:
+	edac_mc_free(mci);
+	return -ENODEV;
+}
+
+/*
+ * amd64_init_one_instance
+ *
+ *	initialize just one device
+ *
+ *	returns:
+ *		 count (>= 0), or
+ *		negative on error
+ */
+static int __devinit amd64_init_one_instance(struct pci_dev *pdev,
+				 const struct pci_device_id *mc_type)
+{
+	debugf0("%s(MC node=%d,mc_type='%s')\n",
+		__func__,
+		get_mc_node_id_from_pdev(pdev),
+		amd64_family_types[mc_type->driver_data].ctl_name);
+
+	/* wake up and enable device */
+	return pci_enable_device(pdev) ? -EIO :
+	    amd64_probe_one_instance(pdev, mc_type->driver_data);
+}
+
+/*
+ * amd64_remove_one_instance
+ *
+ *	remove just one device instance upon driver unloading
+ */
+static void __devexit amd64_remove_one_instance(struct pci_dev *pdev)
+{
+	struct mem_ctl_info *mci;
+	struct amd64_pvt *pvt;
+
+	debugf0("%s()\n", __func__);
+
+	/* Remove from EDAC CORE tracking list */
+	mci = edac_mc_del_mc(&pdev->dev);
+	if (mci == NULL)
+		return;
+
+	pvt = mci->pvt_info;
+
+	/* Release the PCI holds on these PCI devices */
+	pci_dev_put(pvt->addr_map);
+	pci_dev_put(pvt->misc_ctl);
+
+	/* clear the lookup table entry */
+	mci_lookup[pvt->mc_node_id] = NULL;
+
+	/* Free the EDAC CORE resources */
+	edac_mc_free(mci);
+}
+
+/*
+ * The 'pci_device_id' table.
+ *
+ *	This table is part of the interface for loading drivers for PCI
+ *	devices. The PCI core identifies what devices are on a system
+ *	during boot, and then inquiry this table to see if this driver
+ *	is for a given device found.
+ *
+ *	The PCI helpper functions walk this table and call the
+ *	'.probe' function of the 'pci_driver' table, for each
+ *	instance in this table
+ */
+static const struct pci_device_id amd64_pci_table[] __devinitdata = {
+	{
+		/* Athlon64/Opteron  Rev F and prior */
+		.vendor = PCI_VENDOR_ID_AMD,
+		.device = PCI_DEVICE_ID_AMD_K8_NB_MEMCTL,
+		.subvendor = PCI_ANY_ID,
+		.subdevice = PCI_ANY_ID,
+		.class = 0,
+		.class_mask = 0,
+		.driver_data = K8_CPUS
+	},
+	{
+		/* Barcelona, Family 10h */
+		.vendor = PCI_VENDOR_ID_AMD,
+		.device = PCI_DEVICE_ID_AMD_F10_NB_MEMCTL,
+		.subvendor = PCI_ANY_ID,
+		.subdevice = PCI_ANY_ID,
+		.class = 0,
+		.class_mask = 0,
+		.driver_data = F10_CPUS
+	},
+	{
+		/* Family 11h */
+		.vendor = PCI_VENDOR_ID_AMD,
+		.device = PCI_DEVICE_ID_AMD_F11_NB_MEMCTL,
+		.subvendor = PCI_ANY_ID,
+		.subdevice = PCI_ANY_ID,
+		.class = 0,
+		.class_mask = 0,
+		.driver_data = F11_CPUS
+	},
+	{0, }			/* 0 terminated list. */
+};
+MODULE_DEVICE_TABLE(pci, amd64_pci_table);
+
+/*
+ * The 'pci_driver' structure to define the name, probe and removal
+ * functions
+ */
+static struct pci_driver amd64_pci_driver = {
+	.name = EDAC_MOD_STR,
+	.probe = amd64_init_one_instance,
+	.remove = __devexit_p(amd64_remove_one_instance),
+	.id_table = amd64_pci_table,
+};
+
+/*
+ * edac_opstate_setup
+ *
+ *      ensure OP state is valid upon startup
+ */
+static void edac_opstate_setup(void)
+{
+	/* make sure error reporting method is sane */
+	switch (edac_op_state) {
+	case EDAC_OPSTATE_POLL:
+	case EDAC_OPSTATE_NMI:
+		break;
+	default:
+		edac_op_state = EDAC_OPSTATE_POLL;
+		break;
+	}
+}
+
+/*
+ * amd64_edac_init
+ *
+ *	Module ENTRY point after loading
+ */
+static int __init amd64_edac_init(void)
+{
+	/* setup the OPSTATE */
+	edac_opstate_setup();
+
+	return pci_register_driver(&amd64_pci_driver);
+}
+
+/*
+ * amd64_edac_exit
+ *
+ *	Module EXIT point prior to unloading
+ */
+static void __exit amd64_edac_exit(void)
+{
+	/* release the pci control structure */
+	if (amd64_ctl_pci)
+		edac_pci_release_generic_ctl(amd64_ctl_pci);
+
+	pci_unregister_driver(&amd64_pci_driver);
+}
+
+module_init(amd64_edac_init);
+module_exit(amd64_edac_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Linux Networx (http://lnxi.com) "
+	      "Thayne Harbaugh, Doug Thompson, Dave Peterson");
+MODULE_DESCRIPTION("MC support for AMD64 memory controllers - "
+		EDAC_AMD64_VERSION);
Index: linux-2.6.24-rc6-mm1/drivers/edac/Kconfig
===================================================================
--- linux-2.6.24-rc6-mm1.orig/drivers/edac/Kconfig	2008-01-03 14:49:17.000000000 -0700
+++ linux-2.6.24-rc6-mm1/drivers/edac/Kconfig	2008-01-03 14:49:43.000000000 -0700
@@ -51,6 +51,12 @@
 	  occurred so that a particular failing memory module can be
 	  replaced.  If unsure, select 'Y'.
 
+config EDAC_AMD64_OPTERON
+	tristate "AMD64 (Opteron, Athlon64) K8, F10h, F11h"
+	depends on EDAC_MM_EDAC && X86 && PCI
+	help
+	Support for error detection and correction on the AMD 64
+	Families of Memory Controllers (K8, F10h and F11h)
 
 config EDAC_AMD76X
 	tristate "AMD 76x (760, 762, 768)"
Index: linux-2.6.24-rc6-mm1/drivers/edac/Makefile
===================================================================
--- linux-2.6.24-rc6-mm1.orig/drivers/edac/Makefile	2008-01-03 14:49:17.000000000 -0700
+++ linux-2.6.24-rc6-mm1/drivers/edac/Makefile	2008-01-03 14:49:43.000000000 -0700
@@ -27,6 +27,7 @@
 obj-$(CONFIG_EDAC_I3000)		+= i3000_edac.o
 obj-$(CONFIG_EDAC_I82860)		+= i82860_edac.o
 obj-$(CONFIG_EDAC_R82600)		+= r82600_edac.o
+obj-$(CONFIG_EDAC_AMD64_OPTERON)	+= amd64_edac.o
 obj-$(CONFIG_EDAC_PASEMI)		+= pasemi_edac.o
 obj-$(CONFIG_EDAC_MPC85XX)		+= mpc85xx_edac.o
 obj-$(CONFIG_EDAC_MV64X60)		+= mv64x60_edac.o
