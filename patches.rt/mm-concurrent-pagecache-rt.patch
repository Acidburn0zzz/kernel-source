Subject: Linux-RT 2.6.27-RT
From: http://www.kernel.org/pub/linux/kernel/projects/rt/
Acked-by: Sven-Thorsten Dietrich <sdietrich@suse.de>
Subject: mm: -rt bits for concurrent pagecache


Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
---
 include/linux/pagemap.h |   62 ++++++++++++++++++++++++++++++++++++++++++++----
 mm/filemap.c            |   17 ++-----------
 2 files changed, 60 insertions(+), 19 deletions(-)

Index: linux-2.6.26-RT/include/linux/pagemap.h
===================================================================
--- linux-2.6.26-RT.orig/include/linux/pagemap.h
+++ linux-2.6.26-RT/include/linux/pagemap.h
@@ -14,6 +14,9 @@
 #include <linux/bitops.h>
 #include <linux/hardirq.h> /* for in_interrupt() */
 #include <linux/bit_spinlock.h>
+#include <linux/wait.h>
+#include <linux/hash.h>
+#include <linux/interrupt.h>
 #include <linux/page-states.h>
 
 /*
@@ -77,6 +80,26 @@ extern struct page * find_lock_page_nodi
 #define page_cache_release_nocheck(page)	put_page(page)
 void release_pages(struct page **pages, int nr, int cold);
 
+/*
+ * In order to wait for pages to become available there must be
+ * waitqueues associated with pages. By using a hash table of
+ * waitqueues where the bucket discipline is to maintain all
+ * waiters on the same queue and wake all when any of the pages
+ * become available, and for the woken contexts to check to be
+ * sure the appropriate page became available, this saves space
+ * at a cost of "thundering herd" phenomena during rare hash
+ * collisions.
+ */
+static inline wait_queue_head_t *page_waitqueue(struct page *page)
+{
+	const struct zone *zone = page_zone(page);
+
+	return &zone->wait_table[hash_ptr(page, zone->wait_table_bits)];
+}
+
+extern int __sleep_on_page(void *);
+
+#ifndef CONFIG_PREEMPT_RT
 static inline void lock_page_ref(struct page *page)
 {
 	bit_spin_lock(PG_nonewrefs, &page->flags);
@@ -93,29 +116,58 @@ static inline void wait_on_page_ref(stru
 	while (unlikely(test_bit(PG_nonewrefs, &page->flags)))
 		cpu_relax();
 }
+#else // CONFIG_PREEMPT_RT
+static inline void wait_on_page_ref(struct page *page)
+{
+	might_sleep();
+	if (unlikely(PageNoNewRefs(page))) {
+		DEFINE_WAIT_BIT(wait, &page->flags, PG_nonewrefs);
+		__wait_on_bit(page_waitqueue(page), &wait, __sleep_on_page,
+				TASK_UNINTERRUPTIBLE);
+	}
+}
+
+static inline void lock_page_ref(struct page *page)
+{
+	while (test_and_set_bit(PG_nonewrefs, &page->flags))
+		wait_on_page_ref(page);
+	__acquire(bitlock);
+	smp_wmb();
+}
+
+static inline void unlock_page_ref(struct page *page)
+{
+	VM_BUG_ON(!PageNoNewRefs(page));
+	smp_mb__before_clear_bit();
+	ClearPageNoNewRefs(page);
+	smp_mb__after_clear_bit();
+	__wake_up_bit(page_waitqueue(page), &page->flags, PG_nonewrefs);
+	__release(bitlock);
+}
+#endif // CONFIG_PREEMPT_RT
 
 #define lock_page_ref_irq(page)					\
 	do {							\
-		local_irq_disable();				\
+		local_irq_disable_nort();			\
 		lock_page_ref(page);				\
 	} while (0)
 
 #define unlock_page_ref_irq(page)				\
 	do {							\
 		unlock_page_ref(page);				\
-		local_irq_enable();				\
+		local_irq_enable_nort();			\
 	} while (0)
 
 #define lock_page_ref_irqsave(page, flags)			\
 	do {							\
-		local_irq_save(flags);				\
+		local_irq_save_nort(flags);			\
 		lock_page_ref(page);				\
 	} while (0)
 
 #define unlock_page_ref_irqrestore(page, flags)			\
 	do {							\
 		unlock_page_ref(page);				\
-		local_irq_restore(flags);			\
+		local_irq_restore_nort(flags);			\
 	} while (0)
 
 /*
@@ -166,7 +218,7 @@ static inline int page_cache_get_specula
 {
 	VM_BUG_ON(in_interrupt());
 
-#if !defined(CONFIG_SMP) && defined(CONFIG_CLASSIC_RCU)
+#if !defined(CONFIG_SMP) && defined(CONFIG_CLASSIC_RCU) && !defined(CONFIG_PREEMPT_RT)
 # ifdef CONFIG_PREEMPT
 	VM_BUG_ON(!in_atomic());
 # endif
Index: linux-2.6.26-RT/mm/filemap.c
===================================================================
--- linux-2.6.26-RT.orig/mm/filemap.c
+++ linux-2.6.26-RT/mm/filemap.c
@@ -611,21 +611,10 @@ EXPORT_SYMBOL(find_lock_page_nodiscard);
 
 #endif
 
-/*
- * In order to wait for pages to become available there must be
- * waitqueues associated with pages. By using a hash table of
- * waitqueues where the bucket discipline is to maintain all
- * waiters on the same queue and wake all when any of the pages
- * become available, and for the woken contexts to check to be
- * sure the appropriate page became available, this saves space
- * at a cost of "thundering herd" phenomena during rare hash
- * collisions.
- */
-static wait_queue_head_t *page_waitqueue(struct page *page)
+int __sleep_on_page(void *word)
 {
-	const struct zone *zone = page_zone(page);
-
-	return &zone->wait_table[hash_ptr(page, zone->wait_table_bits)];
+	schedule();
+	return 0;
 }
 
 static inline void wake_up_page(struct page *page, int bit)
