From slert-devel-bounces+sdietrich=novell.com@suse.de Fri Feb 15 18:53:00 2008
Return-path: <slert-devel-bounces+sdietrich=novell.com@suse.de>
Received: from Relay1.suse.de ([149.44.160.87]) by emea5-mh.id5.novell.com
	with ESMTP (TLS encrypted); Fri, 15 Feb 2008 18:53:00 +0100
Received: from Fourier.suse.de (fourier.suse.de [149.44.160.40]) by
	Relay1.suse.de (Postfix) with ESMTP id 9BCAD1C052A8 for
	<sdietrich@novell.com>; Fri, 15 Feb 2008 18:53:00 +0100 (CET)
Received: from fourier.suse.de (localhost [127.0.0.1]) by Fourier.suse.de
	(Postfix) with ESMTP id 976201E42782 for <sdietrich@novell.com>; Fri, 15
	Feb 2008 18:53:00 +0100 (CET)
Received: from Relay2.suse.de (relay2.suse.de [149.44.160.89]) by
	Fourier.suse.de (Postfix) with ESMTP id 5C8411E42782 for
	<slert-devel@mailman.suse.de>; Fri, 15 Feb 2008 18:52:58 +0100 (CET)
Received: by Relay2.suse.de (Postfix) id 5A0221C4C34D; Fri, 15 Feb 2008
	18:52:58 +0100 (CET)
Received: from relay2.suse.de (localhost [127.0.0.1]) by Relay2.suse.de
	(Postfix) with ESMTP id 4F1DD1C4C34A for <slert-devel@suse.de>; Fri, 15 Feb
	2008 18:52:58 +0100 (CET)
X-Virus-Scanned: by amavisd-new at relay2.suse.de
X-Spam-Score: -0.899
X-Spam-Level: 
X-Spam-Status: No, score=-0.899 tagged_above=-20 required=5
	tests=[BAYES_50=0.001, MY_LINUX=-1, RDNS_DYNAMIC=0.1]
Received: from mx1.suse.de ([195.135.220.2]) by relay2.suse.de
	(relay2.suse.de [149.44.160.89]) (amavisd-new, port 10025) with ESMTP id
	JZ6yi4oHAHo2 for <slert-devel@suse.de>; Fri, 15 Feb 2008 18:52:52 +0100
	(CET)
Received: from novell1.haskins.net (75-130-111-13.dhcp.oxfr.ma.charter.com
	[75.130.111.13]) by mx1.suse.de (Postfix) with ESMTP id 10D022D4A5 for
	<slert-devel@suse.de>; Fri, 15 Feb 2008 18:52:52 +0100 (CET)
Received: from novell1.haskins.net (localhost [127.0.0.1]) by
	novell1.haskins.net (Postfix) with ESMTP id 8E0473FC1DB for
	<slert-devel@suse.de>; Fri, 15 Feb 2008 12:25:54 -0500 (EST)
From: Gregory Haskins <ghaskins@novell.com>
To: slert-devel@suse.de
Date: Fri, 15 Feb 2008 12:25:54 -0500
Message-ID: <20080215172553.25573.3686.stgit@novell1.haskins.net>
In-Reply-To: <20080215172421.25573.85164.stgit@novell1.haskins.net>
References: <20080215172421.25573.85164.stgit@novell1.haskins.net>
User-Agent: StGIT/0.12.1
MIME-Version: 1.0
Content-Type: text/plain; charset="utf-8"
Subject: [slert-devel] [PATCH 04/10] Adaptive RT spinlock support
X-BeenThere: slert-devel@suse.de
X-Mailman-Version: 2.1.4
Precedence: list
List-Id: All things slert <slert-devel.suse.de>
List-Unsubscribe: <https://mailman.suse.de/mailman/listinfo/slert-devel>,
	<mailto:slert-devel-request@suse.de?subject=unsubscribe>
List-Archive: <https://mailman.suse.de/mailman/private/slert-devel>
List-Post: <mailto:slert-devel@suse.de>
List-Help: <mailto:slert-devel-request@suse.de?subject=help>
List-Subscribe: <https://mailman.suse.de/mailman/listinfo/slert-devel>,
	<mailto:slert-devel-request@suse.de?subject=subscribe>
Sender: slert-devel-bounces+sdietrich=novell.com@suse.de
Errors-To: slert-devel-bounces+sdietrich=novell.com@suse.de
X-Evolution-Source: imap://sdietrich@prv1-3.novell.com/
Content-Transfer-Encoding: 8bit

There are pros and cons when deciding between the two basic forms of
locking primitives (spinning vs sleeping).  Without going into great
detail on either one, we note that spinlocks have the advantage of
lower overhead for short hold locks.  However, they also have a
con in that they create indeterminate latencies since preemption
must traditionally be disabled while the lock is held to prevent deadlock.

Non-deterministic behavior is the antithesis of -rt.  Therefore, when
realtime is enabled, most contexts are converted to threads, and likewise
most spinlocks are converted to sleepable rt-mutex derived locks.  This
allows the holder of the lock to remain fully preemptible, thus reducing
a major source of latencies in the kernel.

However, converting what was once a true spinlock into a sleeping lock
may also decrease performance since the locks will now sleep under
contention.  Since the fundamental lock used to be a spinlock, it is
highly likely that that release is imminent, and therefore sleeping
only serves to cause context-thrashing.

Adaptive spinlocks use a hybrid approach to solve the problem.  They
spin when possible, and sleep when necessary (to avoid deadlock, etc).
This significantly improves many areas of the performance of the -rt
kernel.

Signed-off-by: Gregory Haskins <ghaskins@novell.com>
Signed-off-by: Peter Morreale <pmorreale@novell.com>
Signed-off-by: Sven Dietrich <sdietrich@novell.com>
---

 kernel/Kconfig.preempt    |   22 +++++++++++
 kernel/Makefile           |    1 
 kernel/rtmutex.c          |   19 +++++++--
 kernel/rtmutex_adaptive.c |   93 +++++++++++++++++++++++++++++++++++++++++++++
 kernel/rtmutex_adaptive.h |   51 +++++++++++++++++++++++++
 5 files changed, 181 insertions(+), 5 deletions(-)

diff --git a/kernel/Kconfig.preempt b/kernel/Kconfig.preempt
index 41a0d88..ef41b7e 100644
--- a/kernel/Kconfig.preempt
+++ b/kernel/Kconfig.preempt
@@ -183,6 +183,28 @@ config RCU_TRACE
 	  Say Y/M here if you want to enable RCU tracing in-kernel/module.
 	  Say N if you are unsure.
 
+config ADAPTIVE_RTSPINLOCK
+        bool "Adaptive real-time spinlocks"
+	default y
+	depends on PREEMPT_RT && SMP
+	help
+	 PREEMPT_RT allows for greater determinism by transparently
+	 converting normal spinlocks into preemptible rtmutexes which
+	 sleep any waiters under contention.  Normal spinlocks avoid
+	 deadlock by disabling preemption for the duration that the
+	 lock is held, which is bad for real-time latencies.  Rtmutexes
+	 avoid deadlock by sleeping the waiters and thus avoid the need
+	 to disable preemption on the owner side.  However, the extra
+	 overhead of sleeping can have a significant negative impact
+	 on throughput since spinlocks are typically short-hold locks.
+
+	 This option alters the rtspinlock code to use an adaptive
+	 spin/sleep algorithm.  It will spin unless it determines it must
+	 sleep to avoid deadlock.  This offers a best of both worlds
+	 solution since we achieve both high-throughput and low-latency.
+
+	 If unsure, say Y
+
 config SPINLOCK_BKL
 	bool "Old-Style Big Kernel Lock"
 	depends on (PREEMPT || SMP) && !PREEMPT_RT
diff --git a/kernel/Makefile b/kernel/Makefile
index f77599f..5d5ac64 100644
--- a/kernel/Makefile
+++ b/kernel/Makefile
@@ -27,6 +27,7 @@ ifeq ($(CONFIG_COMPAT),y)
 obj-$(CONFIG_FUTEX) += futex_compat.o
 endif
 obj-$(CONFIG_RT_MUTEXES) += rtmutex.o
+obj-$(CONFIG_ADAPTIVE_RTSPINLOCK) += rtmutex_adaptive.o
 obj-$(CONFIG_DEBUG_RT_MUTEXES) += rtmutex-debug.o
 obj-$(CONFIG_RT_MUTEX_TESTER) += rtmutex-tester.o
 obj-$(CONFIG_PREEMPT_RT) += rt.o
diff --git a/kernel/rtmutex.c b/kernel/rtmutex.c
index cb27b08..feb938f 100644
--- a/kernel/rtmutex.c
+++ b/kernel/rtmutex.c
@@ -7,6 +7,7 @@
  *  Copyright (C) 2005-2006 Timesys Corp., Thomas Gleixner <tglx@timesys.com>
  *  Copyright (C) 2005 Kihon Technologies Inc., Steven Rostedt
  *  Copyright (C) 2006 Esben Nielsen
+ *  Copyright (C) 2008 Novell, Inc.
  *
  *  See Documentation/rt-mutex-design.txt for details.
  */
@@ -17,6 +18,7 @@
 #include <linux/hardirq.h>
 
 #include "rtmutex_common.h"
+#include "rtmutex_adaptive.h"
 
 /*
  * lock->owner state tracking:
@@ -697,6 +699,7 @@ rt_spin_lock_slowlock(struct rt_mutex *lock)
 {
 	struct rt_mutex_waiter waiter;
 	unsigned long saved_state, state, flags;
+	DECLARE_ADAPTIVE_WAITER(adaptive);
 
 	debug_rt_mutex_init_waiter(&waiter);
 	waiter.task = NULL;
@@ -743,6 +746,8 @@ rt_spin_lock_slowlock(struct rt_mutex *lock)
 				continue;
 		}
 
+		prepare_adaptive_wait(lock, &adaptive);
+
 		/*
 		 * Prevent schedule() to drop BKL, while waiting for
 		 * the lock ! We restore lock_depth when we come back.
@@ -754,11 +759,15 @@ rt_spin_lock_slowlock(struct rt_mutex *lock)
 
 		debug_rt_mutex_print_deadlock(&waiter);
 
-		update_current(TASK_UNINTERRUPTIBLE, &saved_state);
-		if (waiter.task)
-			schedule_rt_mutex(lock);
-		else
-			update_current(TASK_RUNNING_MUTEX, &saved_state);
+		/* adaptive_wait() returns 1 if we need to sleep */
+		if (adaptive_wait(lock, &waiter, &adaptive)) {
+			update_current(TASK_UNINTERRUPTIBLE, &saved_state);
+			if (waiter.task)
+				schedule_rt_mutex(lock);
+			else
+				update_current(TASK_RUNNING_MUTEX,
+					       &saved_state);
+		}
 
 		spin_lock_irqsave(&lock->wait_lock, flags);
 		current->flags |= saved_flags;
diff --git a/kernel/rtmutex_adaptive.c b/kernel/rtmutex_adaptive.c
new file mode 100644
index 0000000..ddff53c
--- /dev/null
+++ b/kernel/rtmutex_adaptive.c
@@ -0,0 +1,93 @@
+/*
+ *  Adaptive RT spinlock support
+ *
+ *  There are pros and cons when deciding between the two basic forms of
+ *  locking primitives (spinning vs sleeping).  Without going into great
+ *  detail on either one, we note that spinlocks have the advantage of
+ *  lower overhead for short hold locks.  However, they also have a
+ *  con in that they create indeterminate latencies since preemption
+ *  must traditionally be disabled while the lock is held to prevent deadlock.
+ *
+ *  Non-deterministic behavior is the antithesis of -rt.  Therefore, when
+ *  realtime is enabled, most contexts are converted to threads, and likewise
+ *  most spinlocks are converted to sleepable rt-mutex derived locks.  This
+ *  allows the holder of the lock to remain fully preemptible, thus reducing
+ *  a major source of latencies in the kernel.
+ *
+ *  However, converting what was once a true spinlock into a sleeping lock
+ *  may also decrease performance since the locks will now sleep under
+ *  contention.  Since the fundamental lock used to be a spinlock, it is
+ *  highly likely that that release is imminent, and therefore sleeping
+ *  only serves to cause context-thrashing.
+ *
+ *  Adaptive spinlocks use a hybrid approach to solve the problem.  They
+ *  spin when possible, and sleep when necessary (to avoid deadlock, etc).
+ *  This significantly improves many areas of the performance of the -rt
+ *  kernel.
+ *
+ * Copyright (C) 2008 Novell, Inc.,
+ *          Sven Dietrich, Peter Morreale, and Gregory Haskins
+ *
+ */
+
+#include <linux/sched.h>
+#include "rtmutex_adaptive.h"
+
+/*
+ * Adaptive-spinlocks will busywait when possible, and sleep only if
+ * necessary. Note that the busyloop looks racy, and it is....but we do
+ * not care. If we lose any races it simply means that we spin one more
+ * time before seeing that we need to break-out on the next iteration.
+ */
+int
+adaptive_wait(struct rt_mutex *lock, struct rt_mutex_waiter *waiter,
+	      struct adaptive_waiter *adaptive)
+{
+	int sleep = 0;
+
+	for (;;) {
+		/*
+		 * If the task was re-awoken, break out completely so we can
+		 * reloop through the lock-acquisition code.
+		 */
+		if (!waiter->task)
+			break;
+
+		/*
+		 * We need to break if the owner changed so we can reloop
+		 * and safely acquire the owner-pointer again with the
+		 * wait_lock held.
+		 */
+		if (adaptive->owner != rt_mutex_owner(lock))
+			break;
+
+		/*
+		 * If we got here, presumably the lock ownership is still
+		 * current.  We will use it to our advantage to be able to
+		 * spin without disabling preemption...
+		 */
+
+		/*
+		 * .. sleep if the owner is not running..
+		 */
+		if (!adaptive->owner->se.on_rq) {
+			sleep = 1;
+			break;
+		}
+
+		/*
+		 * .. or is running on our own cpu (to prevent deadlock)
+		 */
+		if (task_cpu(adaptive->owner) == task_cpu(current)) {
+			sleep = 1;
+			break;
+		}
+
+		cpu_relax();
+	}
+
+	put_task_struct(adaptive->owner);
+
+	return sleep;
+}
+
diff --git a/kernel/rtmutex_adaptive.h b/kernel/rtmutex_adaptive.h
new file mode 100644
index 0000000..a136571
--- /dev/null
+++ b/kernel/rtmutex_adaptive.h
@@ -0,0 +1,51 @@
+/*
+ * Adaptive rtmutex support
+ *
+ * Copyright (C) 2008 Novell, Inc.,
+ *          Sven Dietrich, Peter Morreale, and Gregory Haskins
+ *
+ */
+
+#ifndef __KERNEL_RTMUTEX_ADAPTIVE_H
+#define __KERNEL_RTMUTEX_ADAPTIVE_H
+
+#include "rtmutex_common.h"
+
+
+#ifdef CONFIG_ADAPTIVE_RTSPINLOCK
+struct adaptive_waiter {
+	struct task_struct *owner;
+};
+
+/*
+ * Returns 1 if we should sleep
+ */
+int adaptive_wait(struct rt_mutex *lock, struct rt_mutex_waiter *waiter,
+		  struct adaptive_waiter *adative);
+
+static inline void
+prepare_adaptive_wait(struct rt_mutex *lock, struct adaptive_waiter *adaptive)
+{
+	/*
+	 * We must acquire/lock the owner pointer while holding
+	 * the wait_lock, or we risk racing against the owner
+	 * exiting.
+	 */
+	adaptive->owner = rt_mutex_owner(lock);
+	get_task_struct(adaptive->owner);
+}
+
+#define DECLARE_ADAPTIVE_WAITER(name) \
+     struct adaptive_waiter name = { .owner = NULL, }
+
+#else
+
+#define DECLARE_ADAPTIVE_WAITER(name) {}
+
+#define adaptive_wait(lock, waiter, busy) 1
+#define prepare_adaptive_wait(lock, busy) {}
+
+#endif /* CONFIG_ADAPTIVE_RTSPINLOCK */
+
+
+#endif /* __KERNEL_RTMUTEX_ADAPTIVE_H */

