From ghaskins@novell.com Wed Feb 13 16:37:24 2008
Return-path: <ghaskins@novell.com>
Received: from Leto.novell.com (leto.provo.novell.com [130.57.1.24]) by
	sinclair.provo.novell.com with ESMTP; Wed, 13 Feb 2008 16:37:24 -0700
Received: from novell1.haskins.net (75-130-111-13.dhcp.oxfr.ma.charter.com
	[75.130.111.13]) by Leto.novell.com (TMA SMTPRS 4.4.568.30) with ESMTP id
	<C0135449977@Leto.novell.com>; Wed, 13 Feb 2008 15:36:20 -0800
Received-SPF: none (Leto.novell.com: domain of ghaskins@novell.com does not
	designate any permitted senders)
X-IADB2: None
X-Bonded-Senders: None
X-Habeas: None
X-Cloudmark-Rating: 0 <127.0.50.0>
X-IDDB: None
X-Modus-ReverseDNS: OK
X-Modus-BlackList: 75.130.111.13=OK;ghaskins@novell.com=OK
X-Modus-RBL: 75.130.111.13=OK
X-Modus-Trusted: 75.130.111.13=NO
X-Modus-Audit: FALSE;0;0;0
Received: from novell1.haskins.net (localhost [127.0.0.1]) by
	novell1.haskins.net (Postfix) with ESMTP id 38D623FC1D7; Wed, 13 Feb 2008
	18:11:02 -0500 (EST)
From: Gregory Haskins <ghaskins@novell.com>
Subject: [PATCH 3/9] Adaptive RT spinlock support
To: pmorreale@novell.com, sdietrich@novell.com
Cc: ghaskins@novell.com
Date: Wed, 13 Feb 2008 18:11:02 -0500
Message-ID: <20080213231101.4771.9731.stgit@novell1.haskins.net>
In-Reply-To: <20080213230912.4771.22530.stgit@novell1.haskins.net>
References: <20080213230912.4771.22530.stgit@novell1.haskins.net>
User-Agent: StGIT/0.12.1
MIME-Version: 1.0
Content-Type: text/plain; charset="utf-8"
X-Evolution-Source: imap://sdietrich@prv1-3.novell.com/
Content-Transfer-Encoding: 8bit

There are pros and cons when deciding between the two basic forms of
locking primitives (spinning vs sleeping).  Without going into great
detail on either one, we note that spinlocks have the advantage of
lower overhead for short hold locks.  However, they also have a
con in that they create indeterminate latencies since preemption
must traditionally be disabled while the lock is held to prevent deadlock.

Non-deterministic behavior is the antithesis of -rt.  Therefore, when
realtime is enabled, most contexts are converted to threads, and likewise
most spinlocks are converted to sleepable rt-mutex derived locks.  This
allows the holder of the lock to remain fully preemptible, thus reducing
a major source of latencies in the kernel.

However, converting what was once a true spinlock into a sleeping lock
may also decrease performance since the locks will now sleep under
contention.  Since the fundamental lock used to be a spinlock, it is
highly likely that that release is imminent, and therefore sleeping
only serves to cause context-thrashing.

Adaptive spinlocks use a hybrid approach to solve the problem.  They
spin when possible, and sleep when necessary (to avoid deadlock, etc).
This significantly improves many areas of the performance of the -rt
kernel.

Signed-off-by: Gregory Haskins <ghaskins@novell.com>
Signed-off-by: Peter Morreale <pmorreale@novell.com>
Signed-off-by: Sven Dietrich <sdietrich@novell.com>
---

 kernel/Kconfig.preempt    |   22 ++++++++++++
 kernel/Makefile           |    1 +
 kernel/rtmutex.c          |   19 +++++++---
 kernel/rtmutex_adaptive.c |   86 +++++++++++++++++++++++++++++++++++++++++++++
 kernel/rtmutex_adaptive.h |   51 +++++++++++++++++++++++++++
 5 files changed, 174 insertions(+), 5 deletions(-)

diff --git a/kernel/Kconfig.preempt b/kernel/Kconfig.preempt
index 41a0d88..1c2a396 100644
--- a/kernel/Kconfig.preempt
+++ b/kernel/Kconfig.preempt
@@ -183,6 +183,28 @@ config RCU_TRACE
 	  Say Y/M here if you want to enable RCU tracing in-kernel/module.
 	  Say N if you are unsure.
 
+config ADAPTIVE_RTSPINLOCK
+        bool "Adaptive real-time spinlocks"
+	default y
+	depends on PREEMPT && SMP
+	help
+	 PREEMPT_RT allows for greater determinism by transparently
+	 converting normal spinlocks into preemptible rtmutexes which
+	 sleep any waiters under contention.  Normal spinlocks avoid
+	 deadlock by disabling preemption for the duration that the
+	 lock is held, which is bad for real-time latencies.  Rtmutexes
+	 avoid deadlock by sleeping the waiters and thus avoid the need
+	 to disable preemption on the owner side.  However, the extra
+	 overhead of sleeping can have a significant negative impact
+	 on throughput since spinlocks are typically short-hold locks.
+
+	 This option alters the rtspinlock code to use an adaptive
+	 spin/sleep algorithm.  It will spin unless it determines it must
+	 sleep to avoid deadlock.  This offers a best of both worlds
+	 solution since we achieve both high-throughput and low-latency.
+
+	 If unsure, say Y
+
 config SPINLOCK_BKL
 	bool "Old-Style Big Kernel Lock"
 	depends on (PREEMPT || SMP) && !PREEMPT_RT
diff --git a/kernel/Makefile b/kernel/Makefile
index f77599f..5d5ac64 100644
--- a/kernel/Makefile
+++ b/kernel/Makefile
@@ -27,6 +27,7 @@ ifeq ($(CONFIG_COMPAT),y)
 obj-$(CONFIG_FUTEX) += futex_compat.o
 endif
 obj-$(CONFIG_RT_MUTEXES) += rtmutex.o
+obj-$(CONFIG_ADAPTIVE_RTSPINLOCK) += rtmutex_adaptive.o
 obj-$(CONFIG_DEBUG_RT_MUTEXES) += rtmutex-debug.o
 obj-$(CONFIG_RT_MUTEX_TESTER) += rtmutex-tester.o
 obj-$(CONFIG_PREEMPT_RT) += rt.o
diff --git a/kernel/rtmutex.c b/kernel/rtmutex.c
index cb27b08..feb938f 100644
--- a/kernel/rtmutex.c
+++ b/kernel/rtmutex.c
@@ -7,6 +7,7 @@
  *  Copyright (C) 2005-2006 Timesys Corp., Thomas Gleixner <tglx@timesys.com>
  *  Copyright (C) 2005 Kihon Technologies Inc., Steven Rostedt
  *  Copyright (C) 2006 Esben Nielsen
+ *  Copyright (C) 2008 Novell, Inc.
  *
  *  See Documentation/rt-mutex-design.txt for details.
  */
@@ -17,6 +18,7 @@
 #include <linux/hardirq.h>
 
 #include "rtmutex_common.h"
+#include "rtmutex_adaptive.h"
 
 /*
  * lock->owner state tracking:
@@ -697,6 +699,7 @@ rt_spin_lock_slowlock(struct rt_mutex *lock)
 {
 	struct rt_mutex_waiter waiter;
 	unsigned long saved_state, state, flags;
+	DECLARE_ADAPTIVE_WAITER(adaptive);
 
 	debug_rt_mutex_init_waiter(&waiter);
 	waiter.task = NULL;
@@ -743,6 +746,8 @@ rt_spin_lock_slowlock(struct rt_mutex *lock)
 				continue;
 		}
 
+		prepare_adaptive_wait(lock, &adaptive);
+
 		/*
 		 * Prevent schedule() to drop BKL, while waiting for
 		 * the lock ! We restore lock_depth when we come back.
@@ -754,11 +759,15 @@ rt_spin_lock_slowlock(struct rt_mutex *lock)
 
 		debug_rt_mutex_print_deadlock(&waiter);
 
-		update_current(TASK_UNINTERRUPTIBLE, &saved_state);
-		if (waiter.task)
-			schedule_rt_mutex(lock);
-		else
-			update_current(TASK_RUNNING_MUTEX, &saved_state);
+		/* adaptive_wait() returns 1 if we need to sleep */
+		if (adaptive_wait(lock, &waiter, &adaptive)) {
+			update_current(TASK_UNINTERRUPTIBLE, &saved_state);
+			if (waiter.task)
+				schedule_rt_mutex(lock);
+			else
+				update_current(TASK_RUNNING_MUTEX,
+					       &saved_state);
+		}
 
 		spin_lock_irqsave(&lock->wait_lock, flags);
 		current->flags |= saved_flags;
diff --git a/kernel/rtmutex_adaptive.c b/kernel/rtmutex_adaptive.c
new file mode 100644
index 0000000..b92df1a
--- /dev/null
+++ b/kernel/rtmutex_adaptive.c
@@ -0,0 +1,86 @@
+/*
+ *  Adaptive RT spinlock support
+ *
+ *  There are pros and cons when deciding between the two basic forms of
+ *  locking primitives (spinning vs sleeping).  Without going into great
+ *  detail on either one, we note that spinlocks have the advantage of
+ *  lower overhead for short hold locks.  However, they also have a
+ *  con in that they create indeterminate latencies since preemption
+ *  must traditionally be disabled while the lock is held to prevent deadlock.
+ *
+ *  Non-deterministic behavior is the antithesis of -rt.  Therefore, when
+ *  realtime is enabled, most contexts are converted to threads, and likewise
+ *  most spinlocks are converted to sleepable rt-mutex derived locks.  This
+ *  allows the holder of the lock to remain fully preemptible, thus reducing
+ *  a major source of latencies in the kernel.
+ *
+ *  However, converting what was once a true spinlock into a sleeping lock
+ *  may also decrease performance since the locks will now sleep under
+ *  contention.  Since the fundamental lock used to be a spinlock, it is
+ *  highly likely that that release is imminent, and therefore sleeping
+ *  only serves to cause context-thrashing.
+ *
+ *  Adaptive spinlocks use a hybrid approach to solve the problem.  They
+ *  spin when possible, and sleep when necessary (to avoid deadlock, etc).
+ *  This significantly improves many areas of the performance of the -rt
+ *  kernel.
+ *
+ * Copyright (C) 2008 Novell, Inc.,
+ *          Sven Dietrich, Peter Morreale, and Gregory Haskins
+ *
+ */
+
+#include <linux/sched.h>
+#include "rtmutex_adaptive.h"
+
+/*
+ * Adaptive-spinlocks will busywait when possible, and sleep only if
+ * necessary. Note that the busyloop looks racy, and it is....but we do
+ * not care. If we lose any races it simply means that we spin one more
+ * time before seeing that we need to break-out on the next iteration.
+ */
+int
+adaptive_wait(struct rt_mutex *lock, struct rt_mutex_waiter *waiter,
+	      struct adaptive_waiter *adaptive)
+{
+	int sleep = 0;
+
+	for (;;) {
+		/*
+		 * If the task was re-awoken, break out completely so we can
+		 * reloop through the lock-acquisition code.
+		 */
+		if (!waiter->task)
+			break;
+
+		/*
+		 * We need to break if the owner changed so we can reloop
+		 * and safely acquire the owner-pointer again with the
+		 * wait_lock held.
+		 */
+		if (adaptive->owner != rt_mutex_owner(lock))
+			break;
+
+		/*
+		 * If we got here, presumably the lock ownership is still
+		 * current.  So sleep if the owner is not running, is running
+		 * on our own cpu (to prevent deadlock)
+		 */
+		if (!adaptive->owner->se.on_rq) {
+			sleep = 1;
+			break;
+		}
+
+		if (task_cpu(adaptive->owner) == task_cpu(current)) {
+			sleep = 1;
+			break;
+		}
+
+		cpu_relax();
+	}
+
+	put_task_struct(adaptive->owner);
+
+	return sleep;
+}
+
diff --git a/kernel/rtmutex_adaptive.h b/kernel/rtmutex_adaptive.h
new file mode 100644
index 0000000..a136571
--- /dev/null
+++ b/kernel/rtmutex_adaptive.h
@@ -0,0 +1,51 @@
+/*
+ * Adaptive rtmutex support
+ *
+ * Copyright (C) 2008 Novell, Inc.,
+ *          Sven Dietrich, Peter Morreale, and Gregory Haskins
+ *
+ */
+
+#ifndef __KERNEL_RTMUTEX_ADAPTIVE_H
+#define __KERNEL_RTMUTEX_ADAPTIVE_H
+
+#include "rtmutex_common.h"
+
+
+#ifdef CONFIG_ADAPTIVE_RTSPINLOCK
+struct adaptive_waiter {
+	struct task_struct *owner;
+};
+
+/*
+ * Returns 1 if we should sleep
+ */
+int adaptive_wait(struct rt_mutex *lock, struct rt_mutex_waiter *waiter,
+		  struct adaptive_waiter *adative);
+
+static inline void
+prepare_adaptive_wait(struct rt_mutex *lock, struct adaptive_waiter *adaptive)
+{
+	/*
+	 * We must acquire/lock the owner pointer while holding
+	 * the wait_lock, or we risk racing against the owner
+	 * exiting.
+	 */
+	adaptive->owner = rt_mutex_owner(lock);
+	get_task_struct(adaptive->owner);
+}
+
+#define DECLARE_ADAPTIVE_WAITER(name) \
+     struct adaptive_waiter name = { .owner = NULL, }
+
+#else
+
+#define DECLARE_ADAPTIVE_WAITER(name) {}
+
+#define adaptive_wait(lock, waiter, busy) 1
+#define prepare_adaptive_wait(lock, busy) {}
+
+#endif /* CONFIG_ADAPTIVE_RTSPINLOCK */
+
+
+#endif /* __KERNEL_RTMUTEX_ADAPTIVE_H */

