Subject: Big -rt patchset update


---
 Makefile              |    2 
 include/linux/sched.h |    3 
 kernel/fork.c         |    1 
 kernel/sched.c        |  361 ++++++++++++++++++++++++++++----------------------
 kernel/sched_debug.c  |    7 
 kernel/sched_fair.c   |   34 ++++
 kernel/sched_rt.c     |   14 -
 7 files changed, 249 insertions(+), 173 deletions(-)

--- linux-2.6.22.orig/include/linux/sched.h
+++ linux-2.6.22/include/linux/sched.h
@@ -44,6 +44,9 @@ struct sched_param {
 	int sched_priority;
 };
 
+#define DEFINE_PER_CPU_SHARED_ALIGNED(x, y) \
+		DEFINE_PER_CPU(x, y) ____cacheline_aligned_in_smp
+
 #include <asm/param.h>	/* for HZ */
 
 #include <linux/capability.h>
--- linux-2.6.22.orig/kernel/fork.c
+++ linux-2.6.22/kernel/fork.c
@@ -132,7 +132,6 @@ void __put_task_struct_cb(struct rcu_hea
 	BUG_ON(atomic_read(&tsk->usage));
 	WARN_ON(!(tsk->exit_state & (EXIT_DEAD | EXIT_ZOMBIE)));
 	WARN_ON(tsk == current);
-	sched_dead(tsk);
 
 	security_task_free(tsk);
 	free_uid(tsk->user);
--- linux-2.6.22.orig/kernel/sched.c
+++ linux-2.6.22/kernel/sched.c
@@ -23,13 +23,14 @@
  *  2007-05-05  Load balancing (smp-nice) and other improvements
  *              by Peter Williams
  *  2007-05-06  Interactivity improvements to CFS by Mike Galbraith
+ *  2007-07-01  Group scheduling enhancements by Srivatsa Vaddagiri
  */
 
 #include <linux/mm.h>
 #include <linux/module.h>
 #include <linux/nmi.h>
 #include <linux/init.h>
-#include <asm/uaccess.h>
+#include <linux/uaccess.h>
 #include <linux/highmem.h>
 #include <linux/smp_lock.h>
 #include <asm/mmu_context.h>
@@ -61,9 +62,9 @@
 #include <linux/kprobes.h>
 #include <linux/delayacct.h>
 #include <linux/reciprocal_div.h>
+#include <linux/unistd.h>
 
 #include <asm/tlb.h>
-#include <asm/unistd.h>
 
 /*
  * Scheduler clock - returns current time in nanosec units.
@@ -103,6 +104,9 @@ unsigned long long __attribute__((weak))
 #define NS_TO_JIFFIES(TIME)	((TIME) / (1000000000 / HZ))
 #define JIFFIES_TO_NS(TIME)	((TIME) * (1000000000 / HZ))
 
+#define NICE_0_LOAD		SCHED_LOAD_SCALE
+#define NICE_0_SHIFT		SCHED_LOAD_SHIFT
+
 #if (BITS_PER_LONG < 64)
 #define JIFFIES_TO_NS64(TIME) \
 	((unsigned long long)(TIME) * ((unsigned long) (1000000000 / HZ)))
@@ -127,6 +131,45 @@ unsigned long long __attribute__((weak))
 #define MIN_TIMESLICE		max(5 * HZ / 1000, 1)
 #define DEF_TIMESLICE		(100 * HZ / 1000)
 
+#ifdef CONFIG_SMP
+/*
+ * Divide a load by a sched group cpu_power : (load / sg->__cpu_power)
+ * Since cpu_power is a 'constant', we can use a reciprocal divide.
+ */
+static inline u32 sg_div_cpu_power(const struct sched_group *sg, u32 load)
+{
+	return reciprocal_divide(load, sg->reciprocal_cpu_power);
+}
+
+/*
+ * Each time a sched group cpu_power is changed,
+ * we must compute its reciprocal value
+ */
+static inline void sg_inc_cpu_power(struct sched_group *sg, u32 val)
+{
+	sg->__cpu_power += val;
+	sg->reciprocal_cpu_power = reciprocal_value(sg->__cpu_power);
+}
+#endif
+
+#define SCALE_PRIO(x, prio) \
+	max(x * (MAX_PRIO - prio) / (MAX_USER_PRIO / 2), MIN_TIMESLICE)
+
+/*
+ * static_prio_timeslice() scales user-nice values [ -20 ... 0 ... 19 ]
+ * to time slice values: [800ms ... 100ms ... 5ms]
+ */
+static unsigned int static_prio_timeslice(int static_prio)
+{
+	if (static_prio == NICE_TO_PRIO(19))
+		return 1;
+
+	if (static_prio < NICE_TO_PRIO(0))
+		return SCALE_PRIO(DEF_TIMESLICE * 4, static_prio);
+	else
+		return SCALE_PRIO(DEF_TIMESLICE, static_prio);
+}
+
 #define TASK_PREEMPTS_CURR(p, rq) \
 	((p)->prio < (rq)->curr->prio)
 
@@ -168,7 +211,7 @@ static inline int task_has_rt_policy(str
 /*
  * This is the priority-queue data structure of the RT scheduling class:
  */
-struct prio_array {
+struct rt_prio_array {
 	DECLARE_BITMAP(bitmap, MAX_RT_PRIO+1); /* include 1 bit for delimiter */
 	struct list_head queue[MAX_RT_PRIO];
 };
@@ -213,7 +256,7 @@ struct cfs_rq {
 
 /* Real-Time classes' related field in a runqueue: */
 struct rt_rq {
-	struct prio_array active;
+	struct rt_prio_array active;
 	int rt_load_balance_idx;
 	struct list_head *rt_load_balance_head, *rt_load_balance_curr;
 };
@@ -226,7 +269,7 @@ struct rt_rq {
  * acquire operations must be ordered by ascending &runqueue.
  */
 struct rq {
-	raw_spinlock_t lock;
+	raw_spinlock_t lock;	/* runqueue lock */
 
 	/*
 	 * nr_running and cpu_load should be in the same cacheline because
@@ -317,30 +360,14 @@ struct rq {
 	struct lock_class_key rq_lock_key;
 };
 
-static DEFINE_PER_CPU(struct rq, runqueues) ____cacheline_aligned_in_smp;
+static DEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);
 static DEFINE_MUTEX(sched_hotcpu_mutex);
 
 static inline void check_preempt_curr(struct rq *rq, struct task_struct *p)
 {
 	rq->curr->sched_class->check_preempt_curr(rq, p);
-}
-
-#define SCALE_PRIO(x, prio) \
-	max(x * (MAX_PRIO - prio) / (MAX_USER_PRIO / 2), MIN_TIMESLICE)
-
-/*
- * static_prio_timeslice() scales user-nice values [ -20 ... 0 ... 19 ]
- * to time slice values: [800ms ... 100ms ... 5ms]
- */
-static unsigned int static_prio_timeslice(int static_prio)
-{
-	if (static_prio == NICE_TO_PRIO(19))
-		return 1;
-
-	if (static_prio < NICE_TO_PRIO(0))
-		return SCALE_PRIO(DEF_TIMESLICE * 4, static_prio);
-	else
-		return SCALE_PRIO(DEF_TIMESLICE, static_prio);
+	if (p != rq->curr && p->prio < rq->curr->prio)
+		__trace_start_sched_wakeup(p);
 }
 
 static inline int cpu_of(struct rq *rq)
@@ -352,26 +379,6 @@ static inline int cpu_of(struct rq *rq)
 #endif
 }
 
-#ifdef CONFIG_SMP
-/*
- * Divide a load by a sched group cpu_power : (load / sg->__cpu_power)
- * Since cpu_power is a 'constant', we can use a reciprocal divide.
- */
-static inline u32 sg_div_cpu_power(const struct sched_group *sg, u32 load)
-{
-	return reciprocal_divide(load, sg->reciprocal_cpu_power);
-}
-
-/*
- * Each time a sched group cpu_power is changed,
- * we must compute its reciprocal value
- */
-static inline void sg_inc_cpu_power(struct sched_group *sg, u32 val)
-{
-	sg->__cpu_power += val;
-	sg->reciprocal_cpu_power = reciprocal_value(sg->__cpu_power);
-}
-#endif
 /*
  * Per-runqueue clock, as finegrained as the platform can give us:
  */
@@ -433,6 +440,23 @@ static inline unsigned long long rq_cloc
 #define task_rq(p)		cpu_rq(task_cpu(p))
 #define cpu_curr(cpu)		(cpu_rq(cpu)->curr)
 
+/*
+ * For kernel-internal use: high-speed (but slightly incorrect) per-cpu
+ * clock constructed from sched_clock():
+ */
+unsigned long long cpu_clock(int cpu)
+{
+	struct rq *rq = cpu_rq(cpu);
+	unsigned long long now;
+	unsigned long flags;
+
+	spin_lock_irqsave(&rq->lock, flags);
+	now = rq_clock(rq);
+	spin_unlock_irqrestore(&rq->lock, flags);
+
+	return now;
+}
+
 #ifdef CONFIG_FAIR_GROUP_SCHED
 /* Change a task's ->cfs_rq if it moves across CPUs */
 static inline void set_task_cfs_rq(struct task_struct *p)
@@ -662,9 +686,6 @@ void sched_clock_unstable_event(void)
 	task_rq_unlock(rq, &flags);
 }
 
-#define NICE_0_LOAD	SCHED_LOAD_SCALE
-#define NICE_0_SHIFT	SCHED_LOAD_SHIFT
-
 /*
  * resched_task - mark a task 'to be rescheduled now'.
  *
@@ -709,7 +730,6 @@ static void resched_cpu(int cpu)
 	resched_task(cpu_curr(cpu));
 	spin_unlock_irqrestore(&rq->lock, flags);
 }
-
 #else
 static inline void resched_task(struct task_struct *p)
 {
@@ -837,10 +857,10 @@ static void update_curr_load(struct rq *
 #define TIME_SLICE_NICE_ZERO DEF_TIMESLICE
 #define load_weight(lp) \
 	(((lp) * SCHED_LOAD_SCALE) / TIME_SLICE_NICE_ZERO)
-#define PRIO_TO_load_weight(prio) \
+#define PRIO_TO_LOAD_WEIGHT(prio) \
 	load_weight(static_prio_timeslice(prio))
-#define RTPRIO_TO_load_weight(rp) \
-	(PRIO_TO_load_weight(MAX_RT_PRIO) + load_weight(rp))
+#define RTPRIO_TO_LOAD_WEIGHT(rp) \
+	(PRIO_TO_LOAD_WEIGHT(MAX_RT_PRIO) + load_weight(rp))
 
 #define WEIGHT_IDLEPRIO		2
 #define WMULT_IDLEPRIO		(1 << 31)
@@ -853,7 +873,9 @@ static void update_curr_load(struct rq *
  *
  * The "10% effect" is relative and cumulative: from _any_ nice level,
  * if you go up 1 level, it's -10% CPU usage, if you go down 1 level
- * it's +10% CPU usage.
+ * it's +10% CPU usage. (to achieve that we use a multiplier of 1.25.
+ * If a task goes up by ~10% and another task goes down by ~10% then
+ * the relative distance between them is ~25%.)
  */
 static const int prio_to_weight[40] = {
 /* -20 */ 88818, 71054, 56843, 45475, 36380, 29104, 23283, 18626, 14901, 11921,
@@ -863,15 +885,22 @@ static const int prio_to_weight[40] = {
 /*  10 */   110,    87,    70,    56,    45,    36,    29,    23,    18,    15,
 };
 
+/*
+ * Inverse (2^32/x) values of the prio_to_weight[] array, precalculated.
+ *
+ * In cases where the weight does not change often, we can use the
+ * precalculated inverse to speed up arithmetics by turning divisions
+ * into multiplications:
+ */
 static const u32 prio_to_wmult[40] = {
-	48356,   60446,   75558,   94446,  118058,  147573,
-	184467,  230589,  288233,  360285,  450347,
-	562979,  703746,  879575, 1099582, 1374389,
-	717986, 2147483, 2684354, 3355443, 4194304,
-	5244160, 6557201, 8196502, 10250518, 12782640,
-	16025997, 19976592, 24970740, 31350126, 39045157,
-	49367440, 61356675, 76695844, 95443717, 119304647,
-	148102320, 186737708, 238609294, 286331153,
+/* -20 */     48356,     60446,     75558,     94446,    118058,
+/* -15 */    147573,    184467,    230589,    288233,    360285,
+/* -10 */    450347,    562979,    703746,    879575,   1099582,
+/*  -5 */   1374389,   1717986,   2147483,   2684354,   3355443,
+/*   0 */   4194304,   5244160,   6557201,   8196502,  10250518,
+/*   5 */  12782640,  16025997,  19976592,  24970740,  31350126,
+/*  10 */  39045157,  49367440,  61356675,  76695844,  95443717,
+/*  15 */ 119304647, 148102320, 186737708, 238609294, 286331153,
 };
 
 static inline void
@@ -902,6 +931,11 @@ static inline void dec_nr_running(struct
 
 static void activate_task(struct rq *rq, struct task_struct *p, int wakeup);
 
+/*
+ * runqueue iterator, to support SMP load-balancing between different
+ * scheduling classes, without having to expose their internal data
+ * structures to the load-balancing proper:
+ */
 struct rq_iterator {
 	void *arg;
 	struct task_struct *(*start)(void *);
@@ -1108,7 +1142,7 @@ pick_next_task(struct rq *rq, struct tas
 	}
 
 	class = sched_class_highest;
-	for (;;) {
+	for ( ; ; ) {
 		p = class->pick_next_task(rq, now);
 		if (p)
 			return p;
@@ -1120,14 +1154,18 @@ pick_next_task(struct rq *rq, struct tas
 	}
 }
 
-#ifdef CONFIG_SMP
 
 static inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)
 {
+#ifdef CONFIG_SMP
+	trace_change_sched_cpu(p, cpu);
 	task_thread_info(p)->cpu = cpu;
 	set_task_cfs_rq(p);
+#endif
 }
 
+#ifdef CONFIG_SMP
+
 void set_task_cpu(struct task_struct *p, unsigned int new_cpu)
 {
 	int old_cpu = task_cpu(p);
@@ -1148,9 +1186,7 @@ void set_task_cpu(struct task_struct *p,
 	if (p->se.sleep_start_fair)
 		p->se.sleep_start_fair -= fair_clock_offset;
 
-	task_thread_info(p)->cpu = new_cpu;
-
-	set_task_cfs_rq(p);
+	__set_task_cpu(p, new_cpu);
 }
 
 struct migration_req {
@@ -1398,7 +1434,7 @@ nextgroup:
 static struct task_struct * pick_rt_task(struct rq *src_rq, int this_cpu)
 {
 	struct list_head *head, *curr;
-	struct prio_array *array = &src_rq->rt.active;
+	struct rt_prio_array *array = &src_rq->rt.active;
 	struct task_struct *tmp;
 	int idx;
 
@@ -1499,6 +1535,14 @@ static void balance_rt_tasks(struct rq *
 			 * in another runqueue. (low likelyhood
 			 * but possible)
 			 */
+
+			/*
+			 * Update next so that we won't pick a task
+			 * on another cpu with a priority lower (or equal)
+			 * than the one we just picked.
+			 */
+			next = p;
+
 		}
 		spin_unlock(&src_rq->lock);
 	}
@@ -1634,9 +1678,9 @@ static int wake_idle(int cpu, struct tas
 				if (idle_cpu(i))
 					return i;
 			}
-		}
-		else
+		} else {
 			break;
+		}
 	}
 	return cpu;
 }
@@ -2013,11 +2057,6 @@ void fastcall wake_up_new_task(struct ta
 	task_rq_unlock(rq, &flags);
 }
 
-void sched_dead(struct task_struct *p)
-{
-	WARN_ON_ONCE(p->se.on_rq);
-}
-
 /**
  * prepare_task_switch - prepare to switch tasks
  * @rq: the runqueue preparing to switch
@@ -2174,9 +2213,7 @@ context_switch(struct rq *rq, struct tas
 	switch_to(prev, next, prev);
 
 	barrier();
-
 	trace_special_pid(prev->pid, PRIO(prev), PRIO(current));
-
 	/*
 	 * this_rq must be evaluated again because prev may have moved
 	 * CPUs since it called schedule(), thus the 'rq' on its stack
@@ -2666,7 +2703,7 @@ find_busiest_group(struct sched_domain *
 
 			rq = cpu_rq(i);
 
-			if (*sd_idle && !idle_cpu(i))
+			if (*sd_idle && rq->nr_running)
 				*sd_idle = 0;
 
 			/* Bias balancing toward cpus of our domain */
@@ -2688,9 +2725,11 @@ find_busiest_group(struct sched_domain *
 		/*
 		 * First idle cpu or the first cpu(busiest) in this sched group
 		 * is eligible for doing load balancing at this and above
-		 * domains.
+		 * domains. In the newly idle case, we will allow all the cpu's
+		 * to do the newly idle load balance.
 		 */
-		if (local_group && balance_cpu != this_cpu && balance) {
+		if (idle != CPU_NEWLY_IDLE && local_group &&
+		    balance_cpu != this_cpu && balance) {
 			*balance = 0;
 			goto ret;
 		}
@@ -3108,6 +3147,7 @@ load_balance_newidle(int this_cpu, struc
 	unsigned long imbalance;
 	int nr_moved = 0;
 	int sd_idle = 0;
+	int all_pinned = 0;
 	cpumask_t cpus = CPU_MASK_ALL;
 
 	/*
@@ -3146,10 +3186,11 @@ redo:
 		double_lock_balance(this_rq, busiest);
 		nr_moved = move_tasks(this_rq, this_cpu, busiest,
 					minus_1_or_zero(busiest->nr_running),
-					imbalance, sd, CPU_NEWLY_IDLE, NULL);
+					imbalance, sd, CPU_NEWLY_IDLE,
+					&all_pinned);
 		spin_unlock(&busiest->lock);
 
-		if (!nr_moved) {
+		if (unlikely(all_pinned)) {
 			cpu_clear(cpu_of(busiest), cpus);
 			if (!cpus_empty(cpus))
 				goto redo;
@@ -3253,7 +3294,7 @@ static void active_load_balance(struct r
 		schedstat_inc(sd, alb_cnt);
 
 		if (move_tasks(target_rq, target_cpu, busiest_rq, 1,
-			       RTPRIO_TO_load_weight(100), sd, CPU_IDLE,
+			       RTPRIO_TO_LOAD_WEIGHT(100), sd, CPU_IDLE,
 			       NULL))
 			schedstat_inc(sd, alb_pushed);
 		else
@@ -3409,12 +3450,12 @@ out:
  */
 static void run_rebalance_domains(struct softirq_action *h)
 {
-	int local_cpu = raw_smp_processor_id();
-	struct rq *local_rq = cpu_rq(local_cpu);
-	enum cpu_idle_type idle = local_rq->idle_at_tick ?
+	int this_cpu = raw_smp_processor_id();
+	struct rq *this_rq = cpu_rq(this_cpu);
+	enum cpu_idle_type idle = this_rq->idle_at_tick ?
 						CPU_IDLE : CPU_NOT_IDLE;
 
-	rebalance_domains(local_cpu, idle);
+	rebalance_domains(this_cpu, idle);
 
 #ifdef CONFIG_NO_HZ
 	/*
@@ -3422,13 +3463,13 @@ static void run_rebalance_domains(struct
 	 * balancing on behalf of the other idle cpus whose ticks are
 	 * stopped.
 	 */
-	if (local_rq->idle_at_tick &&
-	    atomic_read(&nohz.load_balancer) == local_cpu) {
+	if (this_rq->idle_at_tick &&
+	    atomic_read(&nohz.load_balancer) == this_cpu) {
 		cpumask_t cpus = nohz.cpu_mask;
 		struct rq *rq;
 		int balance_cpu;
 
-		cpu_clear(local_cpu, cpus);
+		cpu_clear(this_cpu, cpus);
 		for_each_cpu_mask(balance_cpu, cpus) {
 			/*
 			 * If this cpu gets work to do, stop the load balancing
@@ -3441,8 +3482,8 @@ static void run_rebalance_domains(struct
 			rebalance_domains(balance_cpu, SCHED_IDLE);
 
 			rq = cpu_rq(balance_cpu);
-			if (time_after(local_rq->next_balance, rq->next_balance))
-				local_rq->next_balance = rq->next_balance;
+			if (time_after(this_rq->next_balance, rq->next_balance))
+				this_rq->next_balance = rq->next_balance;
 		}
 	}
 #endif
@@ -3531,7 +3572,7 @@ static int balance_tasks(struct rq *this
 	return 0;
 }
 
-#endif	/* CONFIG_SMP */
+#endif
 
 DEFINE_PER_CPU(struct kernel_stat, kstat);
 
@@ -3696,6 +3737,8 @@ static inline void schedule_debug(struct
 
 	profile_hit(SCHED_PROFILING, __builtin_return_address(0));
 
+	schedstat_inc(this_rq(), sched_cnt);
+
 	trace_special_sym();
 
 	schedstat_inc(this_rq(), sched_cnt);
@@ -4192,73 +4235,85 @@ out:
 }
 EXPORT_SYMBOL(wait_for_completion_interruptible_timeout);
 
-#define	SLEEP_ON_VAR					\
-	unsigned long flags;				\
-	wait_queue_t wait;				\
-	init_waitqueue_entry(&wait, current);
-
-#define SLEEP_ON_HEAD					\
-	spin_lock_irqsave(&q->lock,flags);		\
-	__add_wait_queue(q, &wait);			\
+static inline void
+sleep_on_head(wait_queue_head_t *q, wait_queue_t *wait, unsigned long *flags)
+{
+	spin_lock_irqsave(&q->lock, *flags);
+	__add_wait_queue(q, wait);
 	spin_unlock(&q->lock);
+}
 
-#define	SLEEP_ON_TAIL					\
-	spin_lock_irq(&q->lock);			\
-	__remove_wait_queue(q, &wait);			\
-	spin_unlock_irqrestore(&q->lock, flags);
+static inline void
+sleep_on_tail(wait_queue_head_t *q, wait_queue_t *wait, unsigned long *flags)
+{
+	spin_lock_irq(&q->lock);
+	__remove_wait_queue(q, wait);
+	spin_unlock_irqrestore(&q->lock, *flags);
+}
 
-void fastcall __sched interruptible_sleep_on(wait_queue_head_t *q)
+void __sched interruptible_sleep_on(wait_queue_head_t *q)
 {
-	SLEEP_ON_VAR
+	unsigned long flags;
+	wait_queue_t wait;
+
+	init_waitqueue_entry(&wait, current);
 
 	current->state = TASK_INTERRUPTIBLE;
 
-	SLEEP_ON_HEAD
+	sleep_on_head(q, &wait, &flags);
 	schedule();
-	SLEEP_ON_TAIL
+	sleep_on_tail(q, &wait, &flags);
 }
 EXPORT_SYMBOL(interruptible_sleep_on);
 
-long fastcall __sched
+long __sched
 interruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)
 {
-	SLEEP_ON_VAR
+	unsigned long flags;
+	wait_queue_t wait;
+
+	init_waitqueue_entry(&wait, current);
 
 	current->state = TASK_INTERRUPTIBLE;
 
-	SLEEP_ON_HEAD
+	sleep_on_head(q, &wait, &flags);
 	timeout = schedule_timeout(timeout);
-	SLEEP_ON_TAIL
+	sleep_on_tail(q, &wait, &flags);
 
 	return timeout;
 }
 EXPORT_SYMBOL(interruptible_sleep_on_timeout);
 
-void fastcall __sched sleep_on(wait_queue_head_t *q)
+void __sched sleep_on(wait_queue_head_t *q)
 {
-	SLEEP_ON_VAR
+	unsigned long flags;
+	wait_queue_t wait;
+
+	init_waitqueue_entry(&wait, current);
 
 	current->state = TASK_UNINTERRUPTIBLE;
 
-	SLEEP_ON_HEAD
+	sleep_on_head(q, &wait, &flags);
 	schedule();
-	SLEEP_ON_TAIL
+	sleep_on_tail(q, &wait, &flags);
 }
 EXPORT_SYMBOL(sleep_on);
 
-long fastcall __sched sleep_on_timeout(wait_queue_head_t *q, long timeout)
+long __sched sleep_on_timeout(wait_queue_head_t *q, long timeout)
 {
-	SLEEP_ON_VAR
+	unsigned long flags;
+	wait_queue_t wait;
+
+	init_waitqueue_entry(&wait, current);
 
 	current->state = TASK_UNINTERRUPTIBLE;
 
-	SLEEP_ON_HEAD
+	sleep_on_head(q, &wait, &flags);
 	timeout = schedule_timeout(timeout);
-	SLEEP_ON_TAIL
+	sleep_on_tail(q, &wait, &flags);
 
 	return timeout;
 }
-
 EXPORT_SYMBOL(sleep_on_timeout);
 
 #ifdef CONFIG_RT_MUTEXES
@@ -4509,7 +4564,6 @@ __setscheduler(struct rq *rq, struct tas
 	switch (p->policy) {
 	case SCHED_NORMAL:
 	case SCHED_BATCH:
-	case SCHED_ISO:
 	case SCHED_IDLE:
 		p->sched_class = &fair_sched_class;
 		break;
@@ -4549,12 +4603,12 @@ recheck:
 		policy = oldpolicy = p->policy;
 	else if (policy != SCHED_FIFO && policy != SCHED_RR &&
 			policy != SCHED_NORMAL && policy != SCHED_BATCH &&
-			policy != SCHED_ISO && policy != SCHED_IDLE)
+			policy != SCHED_IDLE)
 		return -EINVAL;
 	/*
 	 * Valid priorities for SCHED_FIFO and SCHED_RR are
 	 * 1..MAX_USER_RT_PRIO-1, valid priority for SCHED_NORMAL,
-	 * SCHED_BATCH, SCHED_ISO and SCHED_IDLE is 0.
+	 * SCHED_BATCH and SCHED_IDLE is 0.
 	 */
 	if (param->sched_priority < 0 ||
 	    (p->mm && param->sched_priority > MAX_USER_RT_PRIO-1) ||
@@ -5174,7 +5228,6 @@ asmlinkage long sys_sched_get_priority_m
 		break;
 	case SCHED_NORMAL:
 	case SCHED_BATCH:
-	case SCHED_ISO:
 	case SCHED_IDLE:
 		ret = 0;
 		break;
@@ -5200,7 +5253,6 @@ asmlinkage long sys_sched_get_priority_m
 		break;
 	case SCHED_NORMAL:
 	case SCHED_BATCH:
-	case SCHED_ISO:
 	case SCHED_IDLE:
 		ret = 0;
 	}
@@ -5256,14 +5308,14 @@ static void show_task(struct task_struct
 	state = p->state ? __ffs(p->state) + 1 : 0;
 	printk("%-13.13s %c [%p]", p->comm,
 		state < sizeof(stat_nam) - 1 ? stat_nam[state] : '?', p);
-#if (BITS_PER_LONG == 32)
+#if BITS_PER_LONG == 32
 	if (0 && (state == TASK_RUNNING))
-		printk(" running ");
+		printk(" running  ");
 	else
-		printk(" %08lX ", thread_saved_pc(p));
+		printk(" %08lx ", thread_saved_pc(p));
 #else
 	if (0 && (state == TASK_RUNNING))
-		printk("  running task   ");
+		printk("  running task    ");
 	else
 		printk(" %016lx ", thread_saved_pc(p));
 #endif
@@ -5319,6 +5371,9 @@ void show_state_filter(unsigned long sta
 
 	touch_all_softlockup_watchdogs();
 
+#ifdef CONFIG_SCHED_DEBUG
+	sysrq_sched_debug_show();
+#endif
 	if (do_unlock)
 		read_unlock(&tasklist_lock);
 
@@ -5327,9 +5382,6 @@ void show_state_filter(unsigned long sta
 	 */
 	if (state_filter == -1)
 		debug_show_all_locks();
-#ifdef CONFIG_SCHED_DEBUG
-	sysrq_sched_debug_show();
-#endif
 }
 
 void __cpuinit init_idle_bootup_task(struct task_struct *idle)
@@ -5540,8 +5592,6 @@ static int migration_thread(void *data)
 		struct migration_req *req;
 		struct list_head *head;
 
-		try_to_freeze();
-
 		spin_lock_irq(&rq->lock);
 
 		if (cpu_is_offline(cpu)) {
@@ -5742,7 +5792,7 @@ static void migrate_dead_tasks(unsigned 
 	struct rq *rq = cpu_rq(dead_cpu);
 	struct task_struct *next;
 
-	for (;;) {
+	for ( ; ; ) {
 		if (!rq->nr_running)
 			break;
 		next = pick_next_task(rq, rq->curr, rq_clock(rq));
@@ -5772,10 +5822,9 @@ migration_call(struct notifier_block *nf
 
 	case CPU_UP_PREPARE:
 	case CPU_UP_PREPARE_FROZEN:
-		p = kthread_create(migration_thread, hcpu, "migration/%d",cpu);
+		p = kthread_create(migration_thread, hcpu, "migration/%d", cpu);
 		if (IS_ERR(p))
 			return NOTIFY_BAD;
-		p->flags |= PF_NOFREEZE;
 		kthread_bind(p, cpu);
 		/* Must be high prio: stop_machine expects to yield to it. */
 		rq = task_rq_lock(p, &flags);
@@ -6429,7 +6478,7 @@ static int build_sched_domains(const cpu
 	/*
 	 * Allocate the per-node list of sched groups
 	 */
-	sched_group_nodes = kzalloc(sizeof(struct sched_group*)*MAX_NUMNODES,
+	sched_group_nodes = kzalloc(sizeof(struct sched_group *)*MAX_NUMNODES,
 					   GFP_KERNEL);
 	if (!sched_group_nodes) {
 		printk(KERN_WARNING "Can not alloc sched group node list\n");
@@ -6448,8 +6497,8 @@ static int build_sched_domains(const cpu
 		cpus_and(nodemask, nodemask, *cpu_map);
 
 #ifdef CONFIG_NUMA
-		if (cpus_weight(*cpu_map)
-				> SD_NODES_PER_DOMAIN*cpus_weight(nodemask)) {
+		if (cpus_weight(*cpu_map) >
+				SD_NODES_PER_DOMAIN*cpus_weight(nodemask)) {
 			sd = &per_cpu(allnodes_domains, i);
 			*sd = SD_ALLNODES_INIT;
 			sd->span = *cpu_map;
@@ -6508,7 +6557,8 @@ static int build_sched_domains(const cpu
 		if (i != first_cpu(this_sibling_map))
 			continue;
 
-		init_sched_build_groups(this_sibling_map, cpu_map, &cpu_to_cpu_group);
+		init_sched_build_groups(this_sibling_map, cpu_map,
+					&cpu_to_cpu_group);
 	}
 #endif
 
@@ -6519,11 +6569,11 @@ static int build_sched_domains(const cpu
 		cpus_and(this_core_map, this_core_map, *cpu_map);
 		if (i != first_cpu(this_core_map))
 			continue;
-		init_sched_build_groups(this_core_map, cpu_map, &cpu_to_core_group);
+		init_sched_build_groups(this_core_map, cpu_map,
+					&cpu_to_core_group);
 	}
 #endif
 
-
 	/* Set up physical groups */
 	for (i = 0; i < MAX_NUMNODES; i++) {
 		cpumask_t nodemask = node_to_cpumask(i);
@@ -6538,7 +6588,8 @@ static int build_sched_domains(const cpu
 #ifdef CONFIG_NUMA
 	/* Set up node groups */
 	if (sd_allnodes)
-		init_sched_build_groups(*cpu_map, cpu_map, &cpu_to_allnodes_group);
+		init_sched_build_groups(*cpu_map, cpu_map,
+					&cpu_to_allnodes_group);
 
 	for (i = 0; i < MAX_NUMNODES; i++) {
 		/* Set up node groups */
@@ -6901,7 +6952,7 @@ void __init sched_init(void)
 	idle_sched_class.next = NULL;
 
 	for_each_possible_cpu(i) {
-		struct prio_array *array;
+		struct rt_prio_array *array;
 		struct rq *rq;
 
 		rq = cpu_rq(i);
@@ -7012,16 +7063,16 @@ void normalize_rt_tasks(void)
 
 	read_lock_irq(&tasklist_lock);
 	do_each_thread(g, p) {
-		p->se.fair_key = 0;
-		p->se.wait_runtime = 0;
-		p->se.wait_start_fair = 0;
-		p->se.wait_start = 0;
-		p->se.exec_start = 0;
-		p->se.sleep_start = 0;
-		p->se.sleep_start_fair = 0;
-		p->se.block_start = 0;
-		task_rq(p)->cfs.fair_clock = 0;
-		task_rq(p)->clock = 0;
+		p->se.fair_key			= 0;
+		p->se.wait_runtime		= 0;
+		p->se.wait_start_fair		= 0;
+		p->se.wait_start		= 0;
+		p->se.exec_start		= 0;
+		p->se.sleep_start		= 0;
+		p->se.sleep_start_fair		= 0;
+		p->se.block_start		= 0;
+		task_rq(p)->cfs.fair_clock	= 0;
+		task_rq(p)->clock		= 0;
 
 		if (!rt_task(p)) {
 			/*
--- linux-2.6.22.orig/kernel/sched_debug.c
+++ linux-2.6.22/kernel/sched_debug.c
@@ -16,8 +16,6 @@
 #include <linux/kallsyms.h>
 #include <linux/utsname.h>
 
-typedef void (*print_fn_t)(struct seq_file *m, unsigned int *classes);
-
 /*
  * This allows printing both to /proc/sched_debug and
  * to the console
@@ -173,7 +171,7 @@ static int sched_debug_show(struct seq_f
 	u64 now = ktime_to_ns(ktime_get());
 	int cpu;
 
-	SEQ_printf(m, "Sched Debug Version: v0.04, cfs-v20, %s %.*s\n",
+	SEQ_printf(m, "Sched Debug Version: v0.05, %s %.*s\n",
 		init_utsname()->release,
 		(int)strcspn(init_utsname()->version, " "),
 		init_utsname()->version);
@@ -264,7 +262,8 @@ void proc_sched_show_task(struct task_st
 
 		t0 = sched_clock();
 		t1 = sched_clock();
-		SEQ_printf(m, "%-25s:%20Ld\n", "clock-delta", (long long)(t1-t0));
+		SEQ_printf(m, "%-25s:%20Ld\n",
+			   "clock-delta", (long long)(t1-t0));
 	}
 }
 
--- linux-2.6.22.orig/kernel/sched_fair.c
+++ linux-2.6.22/kernel/sched_fair.c
@@ -199,12 +199,12 @@ __dequeue_entity(struct cfs_rq *cfs_rq, 
 	se->on_rq = 0;
 }
 
-static inline struct rb_node * first_fair(struct cfs_rq *cfs_rq)
+static inline struct rb_node *first_fair(struct cfs_rq *cfs_rq)
 {
 	return cfs_rq->rb_leftmost;
 }
 
-static struct sched_entity * __pick_next_entity(struct cfs_rq *cfs_rq)
+static struct sched_entity *__pick_next_entity(struct cfs_rq *cfs_rq)
 {
 	return rb_entry(first_fair(cfs_rq), struct sched_entity, run_node);
 }
@@ -356,6 +356,28 @@ update_stats_wait_start(struct cfs_rq *c
 }
 
 /*
+ * We calculate fair deltas here, so protect against the random effects
+ * of a multiplication overflow by capping it to the runtime limit:
+ */
+#if BITS_PER_LONG == 32
+static inline unsigned long
+calc_weighted(unsigned long delta, unsigned long weight, int shift)
+{
+	u64 tmp = (u64)delta * weight >> shift;
+
+	if (unlikely(tmp > sysctl_sched_runtime_limit*2))
+		return sysctl_sched_runtime_limit*2;
+	return tmp;
+}
+#else
+static inline unsigned long
+calc_weighted(unsigned long delta, unsigned long weight, int shift)
+{
+	return delta * weight >> shift;
+}
+#endif
+
+/*
  * Task is being enqueued - update stats:
  */
 static void
@@ -412,7 +434,8 @@ __update_stats_wait_end(struct cfs_rq *c
 #endif
 
 	if (unlikely(se->load.weight != NICE_0_LOAD))
-		delta_fair = (u64)delta_fair * se->load.weight >> NICE_0_SHIFT;
+		delta_fair = calc_weighted(delta_fair, se->load.weight,
+							NICE_0_SHIFT);
 
 	add_wait_runtime(cfs_rq, se, delta_fair);
 }
@@ -493,7 +516,8 @@ __enqueue_sleeper(struct cfs_rq *cfs_rq,
 						load + se->load.weight);
 
 	if (unlikely(se->load.weight != NICE_0_LOAD))
-		delta_fair = (u64)delta_fair * se->load.weight >> NICE_0_SHIFT;
+		delta_fair = calc_weighted(delta_fair, se->load.weight,
+							NICE_0_SHIFT);
 
 	prev_runtime = se->wait_runtime;
 	__add_wait_runtime(cfs_rq, se, delta_fair);
@@ -846,7 +870,7 @@ static void check_preempt_curr_fair(stru
 		__check_preempt_curr_fair(cfs_rq, &p->se, &curr->se, gran);
 }
 
-static struct task_struct * pick_next_task_fair(struct rq *rq, u64 now)
+static struct task_struct *pick_next_task_fair(struct rq *rq, u64 now)
 {
 	struct cfs_rq *cfs_rq = &rq->cfs;
 	struct sched_entity *se;
--- linux-2.6.22.orig/kernel/sched_rt.c
+++ linux-2.6.22/kernel/sched_rt.c
@@ -28,7 +28,7 @@ static inline void update_curr_rt(struct
 static void
 enqueue_task_rt(struct rq *rq, struct task_struct *p, int wakeup, u64 now)
 {
-	struct prio_array *array = &rq->rt.active;
+	struct rt_prio_array *array = &rq->rt.active;
 
 	list_add_tail(&p->run_list, array->queue + p->prio);
 	__set_bit(p->prio, array->bitmap);
@@ -42,7 +42,7 @@ enqueue_task_rt(struct rq *rq, struct ta
 static void
 dequeue_task_rt(struct rq *rq, struct task_struct *p, int sleep, u64 now)
 {
-	struct prio_array *array = &rq->rt.active;
+	struct rt_prio_array *array = &rq->rt.active;
 
 	update_curr_rt(rq, now);
 
@@ -59,7 +59,7 @@ dequeue_task_rt(struct rq *rq, struct ta
  */
 static void requeue_task_rt(struct rq *rq, struct task_struct *p)
 {
-	struct prio_array *array = &rq->rt.active;
+	struct rt_prio_array *array = &rq->rt.active;
 
 	list_move_tail(&p->run_list, array->queue + p->prio);
 }
@@ -79,9 +79,9 @@ static void check_preempt_curr_rt(struct
 		resched_task(rq->curr);
 }
 
-static struct task_struct * pick_next_task_rt(struct rq *rq, u64 now)
+static struct task_struct *pick_next_task_rt(struct rq *rq, u64 now)
 {
-	struct prio_array *array = &rq->rt.active;
+	struct rt_prio_array *array = &rq->rt.active;
 	struct task_struct *next;
 	struct list_head *queue;
 	int idx;
@@ -114,7 +114,7 @@ static void put_prev_task_rt(struct rq *
 static struct task_struct *load_balance_start_rt(void *arg)
 {
 	struct rq *rq = arg;
-	struct prio_array *array = &rq->rt.active;
+	struct rt_prio_array *array = &rq->rt.active;
 	struct list_head *head, *curr;
 	struct task_struct *p;
 	int idx;
@@ -140,7 +140,7 @@ static struct task_struct *load_balance_
 static struct task_struct *load_balance_next_rt(void *arg)
 {
 	struct rq *rq = arg;
-	struct prio_array *array = &rq->rt.active;
+	struct rt_prio_array *array = &rq->rt.active;
 	struct list_head *head, *curr;
 	struct task_struct *p;
 	int idx;
--- linux-2.6.22.orig/Makefile
+++ linux-2.6.22/Makefile
@@ -1,7 +1,7 @@
 VERSION = 2
 PATCHLEVEL = 6
 SUBLEVEL = 22
-EXTRAVERSION = .3-rc6
+EXTRAVERSION = .3-rc7
 NAME = Holy Dancing Manatees, Batman!
 
 # *DOCUMENTATION*
