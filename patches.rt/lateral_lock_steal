From slert-devel-bounces+sdietrich=novell.com@suse.de Fri Feb 15 18:53:23 2008
Return-path: <slert-devel-bounces+sdietrich=novell.com@suse.de>
Received: from Relay2.suse.de ([149.44.160.89]) by emea5-mh.id5.novell.com
	with ESMTP (TLS encrypted); Fri, 15 Feb 2008 18:53:23 +0100
Received: from Fourier.suse.de (fourier.suse.de [149.44.160.40]) by
	Relay2.suse.de (Postfix) with ESMTP id A414B1C4C871 for
	<sdietrich@novell.com>; Fri, 15 Feb 2008 18:53:23 +0100 (CET)
Received: from fourier.suse.de (localhost [127.0.0.1]) by Fourier.suse.de
	(Postfix) with ESMTP id 9F0301E4279D for <sdietrich@novell.com>; Fri, 15
	Feb 2008 18:53:23 +0100 (CET)
Received: from Relay1.suse.de (relay1.suse.de [149.44.160.87]) by
	Fourier.suse.de (Postfix) with ESMTP id 9636D1E42798 for
	<slert-devel@mailman.suse.de>; Fri, 15 Feb 2008 18:53:21 +0100 (CET)
Received: by Relay1.suse.de (Postfix) id 93CFE1C052B5; Fri, 15 Feb 2008
	18:53:21 +0100 (CET)
Received: from relay1.suse.de (localhost [127.0.0.1]) by Relay1.suse.de
	(Postfix) with ESMTP id 8874E1C052B4 for <slert-devel@suse.de>; Fri, 15 Feb
	2008 18:53:21 +0100 (CET)
X-Virus-Scanned: by amavisd-new at relay1.suse.de
X-Spam-Score: -2.499
X-Spam-Level: 
X-Spam-Status: No, score=-2.499 tagged_above=-20 required=5
	tests=[BAYES_00=-2.599, RDNS_DYNAMIC=0.1]
Received: from mx2.suse.de ([195.135.220.15]) by relay1.suse.de
	(relay1.suse.de [149.44.160.87]) (amavisd-new, port 10025) with ESMTP id
	2WZjxylC2TvJ for <slert-devel@suse.de>; Fri, 15 Feb 2008 18:53:15 +0100
	(CET)
Received: from novell1.haskins.net (75-130-111-13.dhcp.oxfr.ma.charter.com
	[75.130.111.13]) by mx2.suse.de (Postfix) with ESMTP id 7647A356F9 for
	<slert-devel@suse.de>; Fri, 15 Feb 2008 18:53:15 +0100 (CET)
Received: from novell1.haskins.net (localhost [127.0.0.1]) by
	novell1.haskins.net (Postfix) with ESMTP id 9EA513FC1DB for
	<slert-devel@suse.de>; Fri, 15 Feb 2008 12:26:26 -0500 (EST)
From: Gregory Haskins <ghaskins@novell.com>
To: slert-devel@suse.de
Date: Fri, 15 Feb 2008 12:26:26 -0500
Message-ID: <20080215172626.25573.58450.stgit@novell1.haskins.net>
In-Reply-To: <20080215172421.25573.85164.stgit@novell1.haskins.net>
References: <20080215172421.25573.85164.stgit@novell1.haskins.net>
User-Agent: StGIT/0.12.1
MIME-Version: 1.0
Content-Type: text/plain; charset="utf-8"
Subject: [slert-devel] [PATCH 10/10] allow rt-mutex lock-stealing to
	include lateral priority
X-BeenThere: slert-devel@suse.de
X-Mailman-Version: 2.1.4
Precedence: list
List-Id: All things slert <slert-devel.suse.de>
List-Unsubscribe: <https://mailman.suse.de/mailman/listinfo/slert-devel>,
	<mailto:slert-devel-request@suse.de?subject=unsubscribe>
List-Archive: <https://mailman.suse.de/mailman/private/slert-devel>
List-Post: <mailto:slert-devel@suse.de>
List-Help: <mailto:slert-devel-request@suse.de?subject=help>
List-Subscribe: <https://mailman.suse.de/mailman/listinfo/slert-devel>,
	<mailto:slert-devel-request@suse.de?subject=subscribe>
Sender: slert-devel-bounces+sdietrich=novell.com@suse.de
Errors-To: slert-devel-bounces+sdietrich=novell.com@suse.de
X-Evolution-Source: imap://sdietrich@prv1-3.novell.com/
Content-Transfer-Encoding: 8bit

The current logic only allows lock stealing to occur if the current task
is of higher priority than the pending owner. We can gain signficant
throughput improvements (200%+) by allowing the lock-stealing code to
include tasks of equal priority.  The theory is that the system will make
faster progress by allowing the task already on the CPU to take the lock
rather than waiting for the system to wake-up a different task.

This does add a degree of unfairness, yes.  But also note that the users
of these locks under non -rt environments have already been using unfair
raw spinlocks anyway so the tradeoff is probably worth it.

The way I like to think of this is that higher priority tasks should
clearly preempt, and lower priority tasks should clearly block.  However,
if tasks have an identical priority value, then we can think of the
scheduler decisions as the tie-breaking parameter. (e.g. tasks that the
scheduler picked to run first have a logically higher priority amoung tasks
of the same prio).  This helps to keep the system "primed" with tasks doing
useful work, and the end result is higher throughput.

Signed-off-by: Gregory Haskins <ghaskins@novell.com>
---

 kernel/Kconfig.preempt |    9 +++++++++
 kernel/rtmutex.c       |   31 +++++++++++++++++++++++--------
 2 files changed, 32 insertions(+), 8 deletions(-)

diff --git a/kernel/Kconfig.preempt b/kernel/Kconfig.preempt
index 654d56c..ad5d341 100644
--- a/kernel/Kconfig.preempt
+++ b/kernel/Kconfig.preempt
@@ -270,3 +270,12 @@ config SPINLOCK_BKL
 	  Say Y here if you are building a kernel for a desktop system.
 	  Say N if you are unsure.
 
+config RTSPINLOCK_LATERAL_STEAL
+        bool "Allow equal-priority lock stealing"
+	default y
+	depends on PREEMPT_RT
+	help
+	 This option alters the rtspinlock lock-stealing logic to allow
+	 equal priority tasks to preempt a pending owner in addition
+	 to higher priority tasks.  This allows for a significant
+	 boost in throughput at the expense of strict FIFO lock access.
diff --git a/kernel/rtmutex.c b/kernel/rtmutex.c
index 65c2eac..8614228 100644
--- a/kernel/rtmutex.c
+++ b/kernel/rtmutex.c
@@ -315,12 +315,27 @@ static int rt_mutex_adjust_prio_chain(struct task_struct *task,
 	return ret;
 }
 
+static inline int lock_is_stealable(struct task_struct *pendowner, int unfair)
+{
+#ifndef CONFIG_RTSPINLOCK_LATERAL_STEAL
+	if (current->prio >= pendowner->prio)
+#else
+	if (current->prio > pendowner->prio)
+		return 0;
+
+	if (!unfair && (current->prio == pendowner->prio))
+#endif
+		return 0;
+
+	return 1;
+}
+
 /*
  * Optimization: check if we can steal the lock from the
  * assigned pending owner [which might not have taken the
  * lock yet]:
  */
-static inline int try_to_steal_lock(struct rt_mutex *lock)
+static inline int try_to_steal_lock(struct rt_mutex *lock, int unfair)
 {
 	struct task_struct *pendowner = rt_mutex_owner(lock);
 	struct rt_mutex_waiter *next;
@@ -332,7 +347,7 @@ static inline int try_to_steal_lock(struct rt_mutex *lock)
 		return 1;
 
 	spin_lock(&pendowner->pi_lock);
-	if (current->prio >= pendowner->prio) {
+	if (!lock_is_stealable(pendowner, unfair)) {
 		spin_unlock(&pendowner->pi_lock);
 		return 0;
 	}
@@ -385,7 +400,7 @@ static inline int try_to_steal_lock(struct rt_mutex *lock)
  *
  * Must be called with lock->wait_lock held.
  */
-static int try_to_take_rt_mutex(struct rt_mutex *lock)
+static int try_to_take_rt_mutex(struct rt_mutex *lock, int unfair)
 {
 	/*
 	 * We have to be careful here if the atomic speedups are
@@ -408,7 +423,7 @@ static int try_to_take_rt_mutex(struct rt_mutex *lock)
 	 */
 	mark_rt_mutex_waiters(lock);
 
-	if (rt_mutex_owner(lock) && !try_to_steal_lock(lock))
+	if (rt_mutex_owner(lock) && !try_to_steal_lock(lock, unfair))
 		return 0;
 
 	/* We got the lock. */
@@ -729,7 +744,7 @@ rt_spin_lock_slowlock(struct rt_mutex *lock)
 		int saved_lock_depth = current->lock_depth;
 
 		/* Try to acquire the lock */
-		if (try_to_take_rt_mutex(lock))
+		if (try_to_take_rt_mutex(lock, 1))
 			break;
 		/*
 		 * waiter.task is NULL the first time we come here and
@@ -977,7 +992,7 @@ rt_mutex_slowlock(struct rt_mutex *lock, int state,
 	init_lists(lock);
 
 	/* Try to acquire the lock again: */
-	if (try_to_take_rt_mutex(lock)) {
+	if (try_to_take_rt_mutex(lock, 0)) {
 		spin_unlock_irqrestore(&lock->wait_lock, flags);
 		return 0;
 	}
@@ -998,7 +1013,7 @@ rt_mutex_slowlock(struct rt_mutex *lock, int state,
 		unsigned long saved_flags;
 
 		/* Try to acquire the lock: */
-		if (try_to_take_rt_mutex(lock))
+		if (try_to_take_rt_mutex(lock, 0))
 			break;
 
 		/*
@@ -1112,7 +1127,7 @@ rt_mutex_slowtrylock(struct rt_mutex *lock)
 
 		init_lists(lock);
 
-		ret = try_to_take_rt_mutex(lock);
+		ret = try_to_take_rt_mutex(lock, 0);
 		/*
 		 * try_to_take_rt_mutex() sets the lock waiters
 		 * bit unconditionally. Clean this up.

