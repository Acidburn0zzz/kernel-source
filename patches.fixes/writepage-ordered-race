From Andrew Morton

asynchronous writes via pdflush or kswapd can race with data=ordered
writes in both ext3 and reiserfs in such a way that a concurrent
synchronous write via fsync or O_SYNC won't wait on some pages.

Basically filesystem writepage functions need to leave a page dirty or
set it as writeback if they don't actually clean the page.  This fixes
__block_write_full_page to do that.

reiserfs is fixed by patches.suse/reiserfs-writepage-ordered-race

Index: linux.t/fs/fs-writeback.c
===================================================================
--- linux.t.orig/fs/fs-writeback.c	2004-04-16 15:33:22.890986644 -0400
+++ linux.t/fs/fs-writeback.c	2004-04-16 15:57:48.089852079 -0400
@@ -252,6 +252,7 @@ sync_sb_inodes(struct super_block *sb, s
 						struct inode, i_list);
 		struct address_space *mapping = inode->i_mapping;
 		struct backing_dev_info *bdi = mapping->backing_dev_info;
+		long pages_skipped;
 
 		if (bdi->memory_backed) {
 			if (sb == blockdev_superblock) {
@@ -299,6 +300,7 @@ sync_sb_inodes(struct super_block *sb, s
 
 		BUG_ON(inode->i_state & I_FREEING);
 		__iget(inode);
+		pages_skipped = wbc->pages_skipped;
 		__writeback_single_inode(inode, wbc);
 		if (wbc->sync_mode == WB_SYNC_HOLD) {
 			inode->dirtied_when = jiffies;
@@ -306,6 +308,13 @@ sync_sb_inodes(struct super_block *sb, s
 		}
 		if (current_is_pdflush())
 			writeback_release(bdi);
+		if (wbc->pages_skipped != pages_skipped) {
+			/*
+			 * writeback is not making progress due to locked
+			 * buffers.  Skip this inode for now.
+			 */
+			list_move(&inode->i_list, &sb->s_dirty);
+		}
 		spin_unlock(&inode_lock);
 		iput(inode);
 		spin_lock(&inode_lock);
Index: linux.t/fs/buffer.c
===================================================================
--- linux.t.orig/fs/buffer.c	2004-04-16 15:33:23.411833054 -0400
+++ linux.t/fs/buffer.c	2004-04-16 15:57:48.088852374 -0400
@@ -1810,14 +1810,18 @@ static int __block_write_full_page(struc
 		get_bh(bh);
 		if (!buffer_mapped(bh))
 			continue;
-		if (wbc->sync_mode != WB_SYNC_NONE) {
+		/*
+		 * If it's a fully non-blocking write attempt and we cannot
+		 * lock the buffer then redirty the page.  Note that this can
+		 * potentially cause a busy-wait loop from pdflush and kswapd
+		 * activity, but those code paths have their own higher-level
+		 * throttling.
+		 */
+		if (wbc->sync_mode != WB_SYNC_NONE || !wbc->nonblocking) {
 			lock_buffer(bh);
-		} else {
-			if (test_set_buffer_locked(bh)) {
-				if (buffer_dirty(bh))
-					__set_page_dirty_nobuffers(page);
-				continue;
-			}
+		} else if (test_set_buffer_locked(bh)) {
+			__set_page_dirty_nobuffers(page);
+			continue;
 		}
 		if (test_clear_buffer_dirty(bh)) {
 			if (!buffer_uptodate(bh))
@@ -1865,6 +1869,7 @@ done:
 		if (uptodate)
 			SetPageUptodate(page);
 		end_page_writeback(page);
+		wbc->pages_skipped++;	/* We didn't write this page */
 	}
 	return err;
 
Index: linux.t/mm/page-writeback.c
===================================================================
--- linux.t.orig/mm/page-writeback.c	2004-04-16 15:33:23.378842783 -0400
+++ linux.t/mm/page-writeback.c	2004-04-16 15:57:48.091851490 -0400
@@ -272,13 +272,13 @@ static void background_writeout(unsigned
 			break;
 		wbc.encountered_congestion = 0;
 		wbc.nr_to_write = MAX_WRITEBACK_PAGES;
+		wbc.pages_skipped = 0;
 		writeback_inodes(&wbc);
 		min_pages -= MAX_WRITEBACK_PAGES - wbc.nr_to_write;
-		if (wbc.nr_to_write > 0) {
+		if (wbc.nr_to_write > 0 || wbc.pages_skipped > 0) {
 			/* Wrote less than expected */
-			if (wbc.encountered_congestion)
-				blk_congestion_wait(WRITE, HZ/10);
-			else
+			blk_congestion_wait(WRITE, HZ/10);
+			if (!wbc.encountered_congestion)
 				break;
 		}
 	}
Index: linux.t/include/linux/writeback.h
===================================================================
--- linux.t.orig/include/linux/writeback.h	2004-04-16 15:33:14.723395147 -0400
+++ linux.t/include/linux/writeback.h	2004-04-16 15:57:48.090851785 -0400
@@ -39,6 +39,7 @@ struct writeback_control {
 					   older than this */
 	long nr_to_write;		/* Write this many pages, and decrement
 					   this for each page written */
+	long pages_skipped;		/* Pages which were not written */
 	int nonblocking;		/* Don't get stuck on request queues */
 	int encountered_congestion;	/* An output: a queue is full */
 	int for_kupdate;		/* A kupdate writeback */
