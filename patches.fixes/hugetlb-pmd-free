(This is a rediff of http://bugzilla.suse.de/attachment.cgi?id=13689)

there is a bug in arch_hugetlb_fault: with 3 level page table, a pmd
might point to a PTE page.  It happens if the virtual address for
hugetlb mmap is recycled from previously used normal page mmap.
free_pgtables() might not scrub the pmd entry on munmap and
arch_hugetlb_fault skips on any pmd presence regardless what type it is.

Other 3 functions: unmap_hugepage_range, copy_hugetlb_page_range,
follow_hugetlb_page also have the same problem. huge_pte_offset might
return a pmd that points to a PTE page.

Index: linux-2.6.5/arch/i386/mm/hugetlbpage.c
===================================================================
--- linux-2.6.5.orig/arch/i386/mm/hugetlbpage.c
+++ linux-2.6.5/arch/i386/mm/hugetlbpage.c
@@ -106,7 +106,8 @@ static pte_t *huge_pte_offset(struct mm_
 	pgd = pgd_offset(mm, addr);
 	if (pgd_none(*pgd))
 		return NULL;
-	pmd = pmd_offset(pgd, addr);
+	if (pgd_present(*pgd))
+		pmd = pmd_offset(pgd, addr);
 	return (pte_t *) pmd;
 }
 
@@ -146,12 +147,28 @@ int copy_hugetlb_page_range(struct mm_st
 	unsigned long end = vma->vm_end;
 
 	for (; addr < end; addr += HPAGE_SIZE) {
+		pmd_t *pmd;
+
 		src_pte = huge_pte_offset(src, addr);
-		if (!src_pte)
+
+		pmd = (pmd_t *)src_pte;
+		if (!src_pte || pte_none(*src_pte) || !pmd_large(*pmd))
 			continue;
+
 		dst_pte = huge_pte_alloc(dst, addr);
 		if (!dst_pte)
 			goto nomem;
+
+		pmd = (pmd_t *)dst_pte;
+
+		if (!pmd_none(*pmd) && !pmd_large(*pmd)) {
+			struct page * page;
+			page = pmd_page(*pmd);
+			dec_page_state(nr_page_table_pages);
+			page_cache_release(page);
+			pmd_clear(pmd);
+		}
+
 		entry = *src_pte;
 		ptepage = pte_page(entry);
 		get_page(ptepage);
@@ -188,8 +205,17 @@ follow_hugetlb_page(struct mm_struct *mm
 
 			for (;;) { 
 				pte = huge_pte_offset(mm, vaddr);
-				if (pte && !pte_none(*pte))
-					break; 
+				if (pte && !pte_none(*pte)) {
+					pmd_t *pmd = (pmd_t *)pte;
+					if (!pmd_large(*pmd)) {
+						page = pmd_page(*pmd);
+						dec_page_state(nr_page_table_pages);
+						page_cache_release(page);
+						pmd_clear(pmd);
+					}
+					else
+						break;
+				}
 				switch (hugetlb_alloc_fault(mm, vma, vaddr, 0)) { 
 				case VM_FAULT_SIGBUS:
 					return -EFAULT;
@@ -331,9 +357,20 @@ void unmap_hugepage_range(struct vm_area
 	BUG_ON(end & (HPAGE_SIZE - 1));
 
 	for (address = start; address < end; address += HPAGE_SIZE) {
+		pmd_t *pmd;
 		pte = huge_pte_offset(mm, address);
 		if (!pte || pte_none(*pte))
 			continue;
+
+		pmd = (pmd_t *) pte;
+		if (!pmd_none(*pmd) && !pmd_large(*pmd)) {
+			page = pmd_page(*pmd);
+			dec_page_state(nr_page_table_pages);
+			page_cache_release(page);
+			pmd_clear(pmd);
+			continue;
+		}
+
 		page = pte_page(*pte);
 		huge_page_release(page);
 		pte_clear(pte);
@@ -436,6 +473,7 @@ int arch_hugetlb_fault(struct mm_struct 
 { 
 	pmd_t *pmd;
 	pgd_t *pgd;
+	struct page *page;
 
 	if (write_access && !(vma->vm_flags & VM_WRITE))
 		return VM_FAULT_SIGBUS;
@@ -446,6 +484,14 @@ int arch_hugetlb_fault(struct mm_struct 
 		return hugetlb_alloc_fault(mm, vma, address, 1); 
 
 	pmd = pmd_offset(pgd, address);
+
+	if (!pmd_none(*pmd) && !pmd_large(*pmd)) {
+		page = pmd_page(*pmd);
+		dec_page_state(nr_page_table_pages);
+		page_cache_release(page);
+		pmd_clear(pmd);
+	}
+
 	if (pmd_none(*pmd))
 		return hugetlb_alloc_fault(mm, vma, address, 1); 
 
