From 894270adb77e62601e128b2143458254886f48fc Mon Sep 17 00:00:00 2001
From: Mike Anderson <andmike@linux.vnet.ibm.com>
Date: Tue, 14 Oct 2008 12:27:55 -0700
Subject: scsi: atomic blk timer deletes
References: LTC#48990, bnc#434105

Backport the mainline change that added atomic operations to blk timer
deletes. This addresses issues being seen when IO timeouts are racing with
IO completions.

Signed-off-by: Mike Anderson <andmike@linux.vnet.ibm.com>
Signed-off-by: Hannes Reinecke <hare@suse.de>
---
 block/blk-core.c        |   26 ++++++++++++++++++++++----
 block/blk-timeout.c     |   43 +++++++++++++++++++++----------------------
 block/blk.h             |   22 ++++++++++++++++++++++
 drivers/scsi/scsi.c     |    4 ++--
 drivers/scsi/scsi_lib.c |   23 -----------------------
 include/linux/blkdev.h  |    3 ++-
 6 files changed, 69 insertions(+), 52 deletions(-)

--- a/block/blk-core.c
+++ b/block/blk-core.c
@@ -911,6 +911,7 @@ EXPORT_SYMBOL(blk_start_queueing);
 void blk_requeue_request(struct request_queue *q, struct request *rq)
 {
 	blk_delete_timer(rq);
+	blk_clear_rq_complete(rq);
 	blk_add_trace_rq(q, rq, BLK_TA_REQUEUE);
 
 	if (blk_rq_tagged(rq))
@@ -1042,9 +1043,6 @@ void __blk_put_request(struct request_qu
 	if (unlikely(--req->ref_count))
 		return;
 
-	if (!blk_delete_timer(req))
-		return;
-
 	elv_completed_request(q, req);
 
 	/*
@@ -1713,7 +1711,7 @@ static struct notifier_block blk_cpu_not
  *     callback through blk_queue_softirq_done().
  **/
 
-void blk_complete_request(struct request *req)
+void __blk_complete_request(struct request *req)
 {
 	struct list_head *cpu_list;
 	unsigned long flags;
@@ -1728,6 +1726,24 @@ void blk_complete_request(struct request
 
 	local_irq_restore(flags);
 }
+
+/**
+ * blk_complete_request - end I/O on a request
+ * @req:      the request being processed
+ *
+ * Description:
+ *     Ends all I/O on a request. It does not handle partial completions,
+ *     unless the driver actually implements this in its completion callback
+ *     through requeueing. The actual completion happens out-of-order,
+ *     through a softirq handler. The user must have registered a completion
+ *     callback through blk_queue_softirq_done().
+ **/
+
+void blk_complete_request(struct request *req)
+{
+	if (!blk_mark_rq_complete(req))
+		__blk_complete_request(req);
+}
 EXPORT_SYMBOL(blk_complete_request);
 
 /*
@@ -1737,6 +1753,8 @@ static void end_that_request_last(struct
 {
 	struct gendisk *disk = req->rq_disk;
 
+	blk_delete_timer(req);
+
 	if (blk_rq_tagged(req))
 		blk_queue_end_tag(req->q, req);
 
--- a/block/blk.h
+++ b/block/blk.h
@@ -18,8 +18,30 @@ void __blk_queue_free_tags(struct reques
 void blk_unplug_work(struct work_struct *work);
 void blk_unplug_timeout(unsigned long data);
 void blk_rq_timed_out_timer(unsigned long data);
+void blk_delete_timer(struct request *);
 void blk_add_timer(struct request *);
 
+/*
+ * Internal atomic flags for request handling
+ */
+enum rq_atomic_flags {
+	REQ_ATOM_COMPLETE = 0,
+};
+
+/*
+ * EH timer and IO completion will both attempt to 'grab' the request, make
+ * sure that only one of them suceeds
+ */
+static inline int blk_mark_rq_complete(struct request *rq)
+{
+	return test_and_set_bit(REQ_ATOM_COMPLETE, &rq->atomic_flags);
+}
+
+static inline void blk_clear_rq_complete(struct request *rq)
+{
+	clear_bit(REQ_ATOM_COMPLETE, &rq->atomic_flags);
+}
+
 struct io_context *current_io_context(gfp_t gfp_flags, int node);
 
 int ll_back_merge_fn(struct request_queue *q, struct request *req,
--- a/block/blk-timeout.c
+++ b/block/blk-timeout.c
@@ -11,11 +11,8 @@
  * blk_delete_timer - Delete/cancel timer for a given function.
  * @req:	request that we are canceling timer for
  *
- * Return value:
- *     1 if we were able to detach the timer.  0 if we blew it, and the
- *     timer function has already started to run. Caller must hold queue lock.
  */
-int blk_delete_timer(struct request *req)
+void blk_delete_timer(struct request *req)
 {
 	struct request_queue *q = req->q;
 
@@ -23,22 +20,13 @@ int blk_delete_timer(struct request *req
 	 * Nothing to detach
 	 */
 	if (!q->rq_timed_out_fn || !req->deadline)
-		return 1;
-
-	/*
-	 * Not on the list, must have already been scheduled (or never added)
-	 */
-	if (list_empty(&req->timeout_list))
-		return 0;
+		return;
 
 	list_del_init(&req->timeout_list);
 
 	if (list_empty(&q->timeout_list))
 		del_timer(&q->timeout);
-
-	return 1;
 }
-EXPORT_SYMBOL_GPL(blk_delete_timer);
 
 static void blk_rq_timed_out(struct request *req)
 {
@@ -48,9 +36,10 @@ static void blk_rq_timed_out(struct requ
 	ret = q->rq_timed_out_fn(req);
 	switch (ret) {
 	case BLK_EH_HANDLED:
-		blk_complete_request(req);
+		__blk_complete_request(req);
 		break;
 	case BLK_EH_RESET_TIMER:
+		blk_clear_rq_complete(req);
 		blk_add_timer(req);
 		break;
 	case BLK_EH_NOT_HANDLED:
@@ -80,11 +69,11 @@ void blk_rq_timed_out_timer(unsigned lon
 			list_del_init(&rq->timeout_list);
 
 			/*
-			 * if rq->bio is now NULL, then IO completion did
-			 * run on this request and we simply raced to get here
+			 * Check if we raced with end io completion
 			 */
-			if (rq->bio)
-				blk_rq_timed_out(rq);
+			if (blk_mark_rq_complete(rq))
+				continue;
+			blk_rq_timed_out(rq);
 		}
 		if (!next_set) {
 			next = rq->deadline;
@@ -110,8 +99,9 @@ void blk_rq_timed_out_timer(unsigned lon
  */
 void blk_abort_request(struct request *req)
 {
-	if (!blk_delete_timer(req))
+	if (blk_mark_rq_complete(req))
 		return;
+	blk_delete_timer(req);
 	blk_rq_timed_out(req);
 }
 EXPORT_SYMBOL_GPL(blk_abort_request);
@@ -129,13 +119,22 @@ void blk_add_timer(struct request *req)
 	struct request_queue *q = req->q;
 	unsigned long expiry;
 
+	if (!q->rq_timed_out_fn)
+		return;
+
 	BUG_ON(!list_empty(&req->timeout_list));
+	BUG_ON(test_bit(REQ_ATOM_COMPLETE, &req->atomic_flags));
 
 	if (req->timeout)
 		req->deadline = jiffies + req->timeout;
-	else
+	else {
 		req->deadline = jiffies + q->rq_timeout;
-
+		/*
+		 * Some LLDs, like scsi, peek at the timeout to prevent
+		 * a command from being retried forever.
+		 */
+		req->timeout = q->rq_timeout;
+	}
 	list_add_tail(&req->timeout_list, &q->timeout_list);
 
 	/*
--- a/drivers/scsi/scsi.c
+++ b/drivers/scsi/scsi.c
@@ -678,7 +678,7 @@ int scsi_dispatch_cmd(struct scsi_cmnd *
 		 * suspend state.
 		 */
 
-		scsi_queue_retry(cmd, SCSI_MLQUEUE_DEVICE_BUSY);
+		scsi_queue_insert(cmd, SCSI_MLQUEUE_DEVICE_BUSY);
 
 		SCSI_LOG_MLQUEUE(3, printk("queuecommand : device blocked \n"));
 
@@ -758,7 +758,7 @@ int scsi_dispatch_cmd(struct scsi_cmnd *
 		    rtn != SCSI_MLQUEUE_TARGET_BUSY)
 			rtn = SCSI_MLQUEUE_HOST_BUSY;
 
-		scsi_queue_retry(cmd, rtn);
+		scsi_queue_insert(cmd, rtn);
 
 		SCSI_LOG_MLQUEUE(3,
 		    printk("queuecommand : request rejected\n"));
--- a/drivers/scsi/scsi_lib.c
+++ b/drivers/scsi/scsi_lib.c
@@ -173,29 +173,6 @@ int scsi_queue_insert(struct scsi_cmnd *
 }
 
 /**
- * scsi_queue_retry - Try inserting a command in the midlevel queue.
- *
- * @cmd:	command that we are adding to queue.
- * @reason:	why we are inserting command to queue.
- *
- * Notes:       This is very similar to scsi_queue_insert except that we
- *              call this function when we don't know if the blk layer timer
- *              is active or not. We could implement this either by calling
- *              blk_delete_timer and inserting in the midlevel queue if we
- *              successfully delete the timer OR setting appropriate result
- *              field in the cmd and letting it go through the normal done
- *              routines which will retry the command. For now, We call
- *              blk_delete_timer!
- */
-void scsi_queue_retry(struct scsi_cmnd *cmd, int reason)
-{
-	if (blk_delete_timer(cmd->request)) {
-		atomic_inc(&cmd->device->iodone_cnt);
-		scsi_queue_insert(cmd, reason);
-	}
-}
-
-/**
  * scsi_execute - insert request and wait for the result
  * @sdev:	scsi device
  * @cmd:	scsi command
--- a/include/linux/blkdev.h
+++ b/include/linux/blkdev.h
@@ -150,6 +150,7 @@ struct request {
 
 	unsigned int cmd_flags;
 	enum rq_cmd_type_bits cmd_type;
+	unsigned long atomic_flags;
 
 	/* Maintain bio traversal state for part by part I/O submission.
 	 * hard_* are block layer internals, no driver should touch them!
@@ -780,8 +781,8 @@ extern int blk_end_request_callback(stru
 				unsigned int nr_bytes,
 				int (drv_callback)(struct request *));
 extern void blk_complete_request(struct request *);
+extern void __blk_complete_request(struct request *);
 extern void blk_abort_request(struct request *);
-extern int blk_delete_timer(struct request *);
 
 /*
  * blk_end_request() takes bytes instead of sectors as a complete size.
