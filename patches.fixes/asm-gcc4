From: Michael Matz <matz@suse.de>
Subject: fix compilation with gcc4
Patch-mainline: no
References: none

fix compilation with gcc4

Acked-by: Michael Matz <matz@suse.de>

Index: linux-2.6.11/arch/i386/kernel/process.c
===================================================================
--- linux-2.6.11.orig/arch/i386/kernel/process.c
+++ linux-2.6.11/arch/i386/kernel/process.c
@@ -596,8 +596,8 @@ struct task_struct fastcall * __switch_t
 	 * Save away %fs and %gs. No need to save %es and %ds, as
 	 * those are always kernel segments while inside the kernel.
 	 */
-	asm volatile("movl %%fs,%0":"=m" (*(int *)&prev->fs));
-	asm volatile("movl %%gs,%0":"=m" (*(int *)&prev->gs));
+	savesegment(fs,prev->fs);
+	savesegment(gs,prev->gs);
 
 	/*
 	 * Restore %fs and %gs if needed.
Index: linux-2.6.11/include/asm-i386/system.h
===================================================================
--- linux-2.6.11.orig/include/asm-i386/system.h
+++ linux-2.6.11/include/asm-i386/system.h
@@ -81,25 +81,25 @@ static inline unsigned long _get_base(ch
 #define loadsegment(seg,value)			\
 	asm volatile("\n"			\
 		"1:\t"				\
-		"movl %0,%%" #seg "\n"		\
+		"mov %0,%%" #seg "\n"		\
 		"2:\n"				\
 		".section .fixup,\"ax\"\n"	\
 		"3:\t"				\
 		"pushl $0\n\t"			\
-		"popl %%" #seg "\n\t"		\
+		"pop %%" #seg "\n\t"		\
 		"jmp 2b\n"			\
 		".previous\n"			\
 		".section __ex_table,\"a\"\n\t"	\
 		".align 4\n\t"			\
 		".long 1b,3b\n"			\
 		".previous"			\
-		: :"m" (*(unsigned int *)&(value)))
+		: :"m" (*(unsigned short *)&(value)))
 
 /*
  * Save a segment register away
  */
 #define savesegment(seg, value) \
-	asm volatile("movl %%" #seg ",%0":"=m" (*(int *)&(value)))
+	asm volatile("mov %%" #seg ",%0":"=m" (*(short *)&(value)))
 
 /*
  * Clear and set 'TS' bit respectively
Index: linux-2.6.11/arch/i386/kernel/vm86.c
===================================================================
--- linux-2.6.11.orig/arch/i386/kernel/vm86.c
+++ linux-2.6.11/arch/i386/kernel/vm86.c
@@ -309,8 +309,8 @@ static void do_sys_vm86(struct kernel_vm
  */
 	info->regs32->eax = 0;
 	tsk->thread.saved_esp0 = tsk->thread.esp0;
-	asm volatile("movl %%fs,%0":"=m" (tsk->thread.saved_fs));
-	asm volatile("movl %%gs,%0":"=m" (tsk->thread.saved_gs));
+	savesegment(fs,tsk->thread.saved_fs);
+	savesegment(gs,tsk->thread.saved_gs);
 
 	tss = &per_cpu(init_tss, get_cpu());
 	tsk->thread.esp0 = (unsigned long) &info->VM86_TSS_ESP0;
Index: linux-2.6.11/arch/x86_64/kernel/process.c
===================================================================
--- linux-2.6.11.orig/arch/x86_64/kernel/process.c
+++ linux-2.6.11/arch/x86_64/kernel/process.c
@@ -390,10 +390,10 @@ int copy_thread(int nr, unsigned long cl
 	p->thread.fs = me->thread.fs;
 	p->thread.gs = me->thread.gs;
 
-	asm("movl %%gs,%0" : "=m" (p->thread.gsindex));
-	asm("movl %%fs,%0" : "=m" (p->thread.fsindex));
-	asm("movl %%es,%0" : "=m" (p->thread.es));
-	asm("movl %%ds,%0" : "=m" (p->thread.ds));
+	savesegment(gs,p->thread.gsindex);
+	savesegment(fs,p->thread.fsindex);
+	savesegment(es,p->thread.es);
+	savesegment(ds,p->thread.ds);
 
 	if (unlikely(me->thread.io_bitmap_ptr != NULL)) { 
 		p->thread.io_bitmap_ptr = kmalloc(IO_BITMAP_BYTES, GFP_KERNEL);
@@ -456,11 +456,11 @@ struct task_struct *__switch_to(struct t
 	 * Switch DS and ES.
 	 * This won't pick up thread selector changes, but I guess that is ok.
 	 */
-	asm volatile("movl %%es,%0" : "=m" (prev->es)); 
+	savesegment(es,prev->es);
 	if (unlikely(next->es | prev->es))
 		loadsegment(es, next->es); 
-	
-	asm volatile ("movl %%ds,%0" : "=m" (prev->ds)); 
+
+	savesegment(ds,prev->ds);
 	if (unlikely(next->ds | prev->ds))
 		loadsegment(ds, next->ds);
 
Index: linux-2.6.11/include/asm-x86_64/system.h
===================================================================
--- linux-2.6.11.orig/include/asm-x86_64/system.h
+++ linux-2.6.11/include/asm-x86_64/system.h
@@ -70,6 +70,9 @@ extern void load_gs_index(unsigned); 
 		".previous"			\
 		: :"r" (value), "r" (0))
 
+#define savesegment(seg, value) \
+	asm volatile("mov %%" #seg ",%0":"=m" (*(short *)&(value)))
+
 #define set_debug(value,register) \
                 __asm__("movq %0,%%db" #register  \
 		: /* no output */ \
Index: linux-2.6.11/arch/xen/i386/kernel/process.c
===================================================================
--- linux-2.6.11.orig/arch/xen/i386/kernel/process.c
+++ linux-2.6.11/arch/xen/i386/kernel/process.c
@@ -494,8 +494,8 @@ struct task_struct fastcall * __switch_t
 	 * Save away %fs and %gs. No need to save %es and %ds, as
 	 * those are always kernel segments while inside the kernel.
 	 */
-	asm volatile("movl %%fs,%0":"=m" (*(int *)&prev->fs));
-	asm volatile("movl %%gs,%0":"=m" (*(int *)&prev->gs));
+	savesegment(fs,prev->fs);
+	savesegment(gs,prev->gs);
 
 	/*
 	 * We clobber FS and GS here so that we avoid a GPF when restoring
Index: linux-2.6.11/include/asm-xen/asm-i386/system.h
===================================================================
--- linux-2.6.11.orig/include/asm-xen/asm-i386/system.h
+++ linux-2.6.11/include/asm-xen/asm-i386/system.h
@@ -84,25 +84,25 @@ static inline unsigned long _get_base(ch
 #define loadsegment(seg,value)			\
 	asm volatile("\n"			\
 		"1:\t"				\
-		"movl %0,%%" #seg "\n"		\
+		"mov %0,%%" #seg "\n"		\
 		"2:\n"				\
 		".section .fixup,\"ax\"\n"	\
 		"3:\t"				\
 		"pushl $0\n\t"			\
-		"popl %%" #seg "\n\t"		\
+		"pop %%" #seg "\n\t"		\
 		"jmp 2b\n"			\
 		".previous\n"			\
 		".section __ex_table,\"a\"\n\t"	\
 		".align 4\n\t"			\
 		".long 1b,3b\n"			\
 		".previous"			\
-		: :"m" (*(unsigned int *)&(value)))
+		: :"m" (*(unsigned short *)&(value)))
 
 /*
  * Save a segment register away
  */
 #define savesegment(seg, value) \
-	asm volatile("movl %%" #seg ",%0":"=m" (*(int *)&(value)))
+	asm volatile("mov %%" #seg ",%0":"=m" (*(short *)&(value)))
 
 /*
  * Clear and set 'TS' bit respectively
