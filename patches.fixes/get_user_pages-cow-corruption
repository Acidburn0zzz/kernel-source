--- 2.6.5-aa3/mm/objrmap.c.~1~	2004-04-14 11:51:32.000000000 +0200
+++ 2.6.5-aa3/mm/objrmap.c	2004-04-23 02:51:38.558818080 +0200
@@ -547,6 +547,25 @@ try_to_unmap_one(struct vm_area_struct *
 	if (unlikely(!spin_trylock(&mm->page_table_lock)))
 		return ret;
 
+	/*
+	 * While the page is pinned on the physical side we must
+	 * prevent unmapping the page or we'd invalidate the
+	 * prefaulting work that get_user_pages does, allowing
+	 * a swapout + swapin + COW to destroy the effect of the
+	 * get_user_pages users that want to write to the page.
+	 *
+	 * We've to do it here inside the page_table_lock to
+	 * serialize against get_user_pages's get_page() that
+	 * is run inside the page_table_lock atomically after
+	 * verifiying the page is mapped good in the pte.
+	 *
+	 * The mapcount is already serialized via the PG_maplock.
+	 */
+	if (PageSwapCache(page) && page->mapcount + 2 < page_count(page)) {
+		BUG_ON(PagePrivate(page));
+		goto out;
+	}
+
 	if (unlikely(vma->vm_flags & VM_NONLINEAR)) {
 		/*
 		 * If this was a false positive generated by a
--- 2.6.5-aa3/mm/vmscan.c.~1~	2004-04-14 11:51:32.000000000 +0200
+++ 2.6.5-aa3/mm/vmscan.c	2004-04-23 02:59:08.467421576 +0200
@@ -418,6 +418,15 @@ shrink_list(struct list_head *page_list,
 		 * The non-racy check for busy page.  It is critical to check
 		 * PageDirty _after_ making sure that the page is freeable and
 		 * not in use by anybody. 	(pagecache + us == 2)
+		 *
+		 * Like there is a dependency here on the number of VM
+		 * page->count references, another dependency is also in
+		 * try_to_unmap() to make sure not to unmap pages while
+		 * they're being pinned by get_user_pages() or other
+		 * paths that might write to the page. See the comment
+		 * in objrmap:try_to_unmap_one() for details.
+		 * In short if you change != 2, you've to change
+		 * objrmap:try_to_unmap_one too.
 		 */
 		if (page_count(page) != 2 || PageDirty(page)) {
 			spin_unlock_irq(&mapping->tree_lock);
