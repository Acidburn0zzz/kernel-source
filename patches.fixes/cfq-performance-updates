From: Jens Axboe <axboe@suse.de>
Subject: Performance enhancements for CFQ
Patch-mainline: 
References: 

- Detect tagging and disable slice idling if depth goes beyond 4
- Before deciding that no work is to be done, check the busy list. If that
  has work, we are not idle and must serve that (those) queues.
- Detect idling process issuing an async request, and expire the slice if
  it does.

Acked-by: 
Signed-off-by: 

diff -urp /mnt/kscratch/linux-2.6.16/block/cfq-iosched.c linux-2.6.16/block/cfq-iosched.c
--- /mnt/kscratch/linux-2.6.16/block/cfq-iosched.c	2006-05-31 14:41:36.000000000 +0200
+++ linux-2.6.16/block/cfq-iosched.c	2006-05-31 14:47:52.000000000 +0200
@@ -133,6 +133,7 @@ struct cfq_data {
 	mempool_t *crq_pool;
 
 	int rq_in_driver;
+	int hw_tag;
 
 	/*
 	 * schedule slice state info
@@ -664,6 +665,15 @@ static void cfq_activate_request(request
 	struct cfq_data *cfqd = q->elevator->elevator_data;
 
 	cfqd->rq_in_driver++;
+
+	/*
+	 * If the depth is larger 1, it really could be queueing. But lets
+	 * make the mark a little higher - idling could still be good for
+	 * low queueing, and a low queueing number could also just indicate
+	 * a SCSI mid layer like behaviour where limit+1 is often seen.
+	 */
+	if (!cfqd->hw_tag && cfqd->rq_in_driver > 4)
+		cfqd->hw_tag = 1;
 }
 
 static void cfq_deactivate_request(request_queue_t *q, struct request *rq)
@@ -882,6 +892,13 @@ static struct cfq_queue *cfq_set_active_
 		cfqq = list_entry_cfqq(cfqd->cur_rr.next);
 
 	/*
+	 * If no new queues are available, check if the busy list has some
+	 * before falling back to idle io.
+	 */
+	if (!cfqq && !list_empty(&cfqd->busy_rr))
+		cfqq = list_entry_cfqq(cfqd->busy_rr.next);
+
+	/*
 	 * if we have idle queues and no rt or be queues had pending
 	 * requests, either allow immediate service if the grace period
 	 * has passed or arm the idle grace timer
@@ -1461,7 +1478,8 @@ retry:
 		 * set ->slice_left to allow preemption for a new process
 		 */
 		cfqq->slice_left = 2 * cfqd->cfq_slice_idle;
-		cfq_mark_cfqq_idle_window(cfqq);
+		if (!cfqd->hw_tag)
+			cfq_mark_cfqq_idle_window(cfqq);
 		cfq_mark_cfqq_prio_changed(cfqq);
 		cfq_init_prio_data(cfqq);
 	}
@@ -1652,8 +1670,8 @@ cfq_update_idle_window(struct cfq_data *
 {
 	int enable_idle = cfq_cfqq_idle_window(cfqq);
 
-	if (!cic->ioc->task || !cfqd->cfq_slice_idle)
-		enable_idle = 0;
+	if (!cic->ioc->task || !cfqd->cfq_slice_idle || cfqd->hw_tag)
+                enable_idle = 0;
 	else if (sample_valid(cic->ttime_samples)) {
 		if (cic->ttime_mean > cfqd->cfq_slice_idle)
 			enable_idle = 0;
@@ -1743,14 +1761,24 @@ cfq_crq_enqueued(struct cfq_data *cfqd, 
 
 	cfqq->next_crq = cfq_choose_req(cfqd, cfqq->next_crq, crq);
 
+	cic = crq->io_context;
+
 	/*
 	 * we never wait for an async request and we don't allow preemption
 	 * of an async request. so just return early
 	 */
-	if (!cfq_crq_is_sync(crq))
+	if (!cfq_crq_is_sync(crq)) {
+		/*
+		 * sync process issued async request, if it's waiting
+		 * then expire it and kick rq handling.
+		 */
+		if (cic == cfqd->active_cic &&
+		    del_timer(&cfqd->idle_slice_timer)) {
+			cfq_slice_expired(cfqd, 0);
+			cfq_start_queueing(cfqd, cfqq);
+		}
 		return;
-
-	cic = crq->io_context;
+	}
 
 	cfq_update_io_thinktime(cfqd, cic);
 	cfq_update_io_seektime(cfqd, cic, crq);
